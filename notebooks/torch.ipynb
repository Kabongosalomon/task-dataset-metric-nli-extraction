{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import ipdb\n",
    "import spacy\n",
    "import torch\n",
    "import optuna\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import deque\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import RobertaTokenizer, BertModel, TransfoXLTokenizer, TransfoXLModel, AdamW\n",
    "from transformers import BigBirdTokenizer, BigBirdForSequenceClassification\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_path = \"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/5Neg10unk/twofoldwithunk/fold1/train.tsv\"\n",
    "# valid_path = \"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/5Neg10unk/twofoldwithunk/fold1/dev.tsv\"\n",
    "\n",
    "# train_path = \"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/new/60Neg800unk/twofoldwithunk/fold1/train.tsv\"\n",
    "# valid_path = \"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/new/60Neg800unk/twofoldwithunk/fold1/dev.tsv\"\n",
    "\n",
    "train_path = \"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/ibm/exp/few-shot-setup/NLP-TDMS/paperVersion/train.tsv\"\n",
    "valid_path = \"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/ibm/exp/few-shot-setup/NLP-TDMS/paperVersion/test.tsv\"\n",
    "output_path = \"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/ibm/exp/few-shot-setup/NLP-TDMS/paperVersion/torch/SciBert/\"\n",
    "\n",
    "model_pt_path = \"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/ibm/exp/few-shot-setup/NLP-TDMS/paperVersion/torch/SciBert/Model_SciBert_avg_metric_0.9001.pt\"\n",
    "\n",
    "N_EPOCHS = 3\n",
    "model_name = \"SciBert\"\n",
    "max_input_len = 512\n",
    "# output_path = \"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/new/60Neg800unk/twofoldwithunk/fold1/\"\n",
    "bs = 16\n",
    "\n",
    "processors = {\n",
    "      \"Bert\": [BertTokenizer, BertForSequenceClassification, \"bert-base-uncased\"],\n",
    "      \"SciBert\": [BertTokenizer, BertForSequenceClassification, \"allenai/scibert_scivocab_uncased\"],\n",
    "      \"XLNet\": [XLNetTokenizer, XLNetForSequenceClassification, \"xlnet-base-cased\"],\n",
    "      \"BigBird\": [BigBirdTokenizer, BigBirdForSequenceClassification, \"google/bigbird-roberta-base\"],\n",
    "      \"Longformer\": [LongformerTokenizer, LongformerForSequenceClassification, \"allenai/longformer-base-4096\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(train_path, \n",
    "                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])\n",
    "\n",
    "valid_df = pd.read_csv(valid_path, \n",
    "                   sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>D16-1036.pdf</td>\n",
       "      <td>unknow</td>\n",
       "      <td>Multi-view Response Selection for Human-Comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>D16-1036.pdf</td>\n",
       "      <td>question answering; SQuAD; F1</td>\n",
       "      <td>Multi-view Response Selection for Human-Comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>D16-1036.pdf</td>\n",
       "      <td>relation prediction; FB15K-237; H@1</td>\n",
       "      <td>Multi-view Response Selection for Human-Comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>D16-1036.pdf</td>\n",
       "      <td>word sense disambiguation; SemEval 2013; F1</td>\n",
       "      <td>Multi-view Response Selection for Human-Comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>D16-1036.pdf</td>\n",
       "      <td>language modeling; 1B Words / Google Billion W...</td>\n",
       "      <td>Multi-view Response Selection for Human-Comput...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label         title                                                TDM  \\\n",
       "0   True  D16-1036.pdf                                             unknow   \n",
       "1  False  D16-1036.pdf                      question answering; SQuAD; F1   \n",
       "2  False  D16-1036.pdf                relation prediction; FB15K-237; H@1   \n",
       "3  False  D16-1036.pdf        word sense disambiguation; SemEval 2013; F1   \n",
       "4  False  D16-1036.pdf  language modeling; 1B Words / Google Billion W...   \n",
       "\n",
       "                                             Context  \n",
       "0  Multi-view Response Selection for Human-Comput...  \n",
       "1  Multi-view Response Selection for Human-Comput...  \n",
       "2  Multi-view Response Selection for Human-Comput...  \n",
       "3  Multi-view Response Selection for Human-Comput...  \n",
       "4  Multi-view Response Selection for Human-Comput...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>1803.11175.pdf</td>\n",
       "      <td>sentiment analysis; SUBJ; Accuracy</td>\n",
       "      <td>Universal Sentence Encoder We present models f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>1803.11175.pdf</td>\n",
       "      <td>text classification; TREC; Error</td>\n",
       "      <td>Universal Sentence Encoder We present models f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>1803.11175.pdf</td>\n",
       "      <td>question answering; SQuAD; F1</td>\n",
       "      <td>Universal Sentence Encoder We present models f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>1803.11175.pdf</td>\n",
       "      <td>relation prediction; FB15K-237; H@1</td>\n",
       "      <td>Universal Sentence Encoder We present models f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>1803.11175.pdf</td>\n",
       "      <td>word sense disambiguation; SemEval 2013; F1</td>\n",
       "      <td>Universal Sentence Encoder We present models f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label           title                                          TDM  \\\n",
       "0   True  1803.11175.pdf           sentiment analysis; SUBJ; Accuracy   \n",
       "1   True  1803.11175.pdf             text classification; TREC; Error   \n",
       "2  False  1803.11175.pdf                question answering; SQuAD; F1   \n",
       "3  False  1803.11175.pdf          relation prediction; FB15K-237; H@1   \n",
       "4  False  1803.11175.pdf  word sense disambiguation; SemEval 2013; F1   \n",
       "\n",
       "                                             Context  \n",
       "0  Universal Sentence Encoder We present models f...  \n",
       "1  Universal Sentence Encoder We present models f...  \n",
       "2  Universal Sentence Encoder We present models f...  \n",
       "3  Universal Sentence Encoder We present models f...  \n",
       "4  Universal Sentence Encoder We present models f...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [SEP] [PAD] [UNK]\n"
     ]
    }
   ],
   "source": [
    "if model_name in processors.keys():\n",
    "    selected_processor = processors[model_name]\n",
    "else:\n",
    "    print(f\"Model not available check selected model only {list(processors.keys())} as supported\")\n",
    "    quit()\n",
    "\n",
    "if model_name == \"SciBert\":\n",
    "    tokenizer = selected_processor[0].from_pretrained(\"bert-base-uncased\")\n",
    "else:\n",
    "    tokenizer = selected_processor[0].from_pretrained(selected_processor[2])\n",
    "\n",
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 102 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximun sequence lenght 512\n"
     ]
    }
   ],
   "source": [
    "if model_name == \"SciBert\":\n",
    "    max_input_length = tokenizer.max_model_input_sizes[\"bert-base-uncased\"]\n",
    "else:\n",
    "    max_input_length = tokenizer.max_model_input_sizes[selected_processor[2]]\n",
    "\n",
    "if not max_input_length:\n",
    "    max_input_length = max_input_len\n",
    "\n",
    "print(f\"Maximun sequence lenght {max_input_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersNLI(Dataset):\n",
    "    def __init__(self, tokenizer, max_input_length):\n",
    "        self.label_dict = {'True': 0, 'False': 1} # Default {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length        \n",
    "        \n",
    "    def _truncate_seq_pair(self, tokens_a, tokens_b, max_length):\n",
    "        \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "        # This is a simple heuristic which will always truncate the longer sequence\n",
    "        # one token at a time. This makes more sense than truncating an equal percent\n",
    "        # of tokens from each, since if one sequence is very short then each token\n",
    "        # that's truncated likely contains more information than a longer sequence.\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= max_length:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop()\n",
    "            else:\n",
    "                tokens_b.pop()\n",
    "        \n",
    "    def load_data(self, df):\n",
    "        MAX_LEN = self.max_input_length\n",
    "        token_ids = []\n",
    "        mask_ids = []\n",
    "        seg_ids = []\n",
    "        y = []\n",
    "\n",
    "        premise_list = df['TDM'].to_list()           # df['sentence1'].to_list()\n",
    "        hypothesis_list = df['Context'].to_list()    # df['sentence2'].to_list()\n",
    "        label_list = df['label'].to_list()           # df['gold_label'].to_list()\n",
    "\n",
    "        for (premise, hypothesis, label) in tqdm(zip(premise_list, hypothesis_list, label_list), total=len(label_list)):\n",
    "            premise_id = self.tokenizer.encode(premise, add_special_tokens = False)\n",
    "            hypothesis_id = self.tokenizer.encode(hypothesis, add_special_tokens = False)\n",
    "            # ignore the warning as the ong sequence issuw is taken care of here \n",
    "            self._truncate_seq_pair(premise_id, hypothesis_id, MAX_LEN-3) # -3 to account for the special characters \n",
    "            \n",
    "            pair_token_ids = [self.tokenizer.cls_token_id] + premise_id \\\n",
    "                            + [self.tokenizer.sep_token_id] + hypothesis_id \\\n",
    "                            + [self.tokenizer.sep_token_id]\n",
    "            premise_len = len(premise_id)\n",
    "            hypothesis_len = len(hypothesis_id)\n",
    "\n",
    "            segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (hypothesis_len + 1))  # sentence 0 and sentence 1\n",
    "            attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values\n",
    "\n",
    "            token_ids.append(torch.tensor(pair_token_ids))\n",
    "            seg_ids.append(segment_ids)\n",
    "            mask_ids.append(attention_mask_ids)\n",
    "            # we have str(label) to have the key work proprely \n",
    "            y.append(self.label_dict[str(label)]) # y.append(self.label_dict[label]) \n",
    "            \n",
    "        token_ids = pad_sequence(token_ids, batch_first=True)\n",
    "        mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
    "        seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
    "        y = torch.tensor(y)\n",
    "        dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)\n",
    "\n",
    "        print(len(dataset))\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def get_train_data(self, train_df, batch_size=32, shuffle=True):\n",
    "        train_data = self.load_data(train_df)\n",
    "                    \n",
    "        train_loader = DataLoader(\n",
    "            train_data,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=batch_size\n",
    "            )\n",
    "\n",
    "        return train_loader\n",
    "\n",
    "    def get_valid_data(self, valid_df, batch_size=32, shuffle=True):\n",
    "        valid_data = self.load_data(valid_df)\n",
    "                    \n",
    "        valid_loader = DataLoader(\n",
    "            valid_data,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=batch_size\n",
    "            )\n",
    "\n",
    "        return valid_loader\n",
    "\n",
    "    def get_inference_data(self, test_df, batch_size=32, shuffle=False):\n",
    "        test_data = self.load_data(test_df)\n",
    "                    \n",
    "        test_loader = DataLoader(\n",
    "            test_data,\n",
    "            shuffle=shuffle,\n",
    "            batch_size=batch_size\n",
    "            )\n",
    "\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TDM_dataset = TransformersNLI(tokenizer, max_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_loader = TDM_dataset.get_train_data(train_df, batch_size=bs, shuffle=True)\n",
    "# valid_loader = TDM_dataset.get_valid_data(valid_df, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.exists(f'{output_path}train_loader_{bs}_seq_{max_input_length}.pth'):\n",
    "#     train_loader = torch.load(f'{output_path}train_loader_{bs}_seq_{max_input_length}.pth')\n",
    "# else:\n",
    "#     train_loader = TDM_dataset.get_train_data(train_df, batch_size=bs, shuffle=True)\n",
    "#     # Save dataloader\n",
    "#     torch.save(train_loader, f'{output_path}train_loader_{bs}_seq_{max_input_length}.pth')\n",
    "\n",
    "# if os.path.exists(f'{output_path}valid_loader_{bs}_seq_{max_input_length}.pth'):\n",
    "#     valid_loader = torch.load(f'{output_path}valid_loader_{bs}_seq_{max_input_length}.pth')\n",
    "# else:\n",
    "#     valid_loader = TDM_dataset.get_valid_data(valid_df, batch_size=bs, shuffle=True)\n",
    "#     # Save dataloader\n",
    "#     torch.save(valid_loader, f'{output_path}valid_loader_{bs}_seq_{max_input_length}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13071/13071 [01:28<00:00, 147.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13071\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(f'{output_path}train_loader_{bs}_seq_{max_input_length}.pth'):\n",
    "    train_loader = torch.load(f'{output_path}train_loader_{bs}_seq_{max_input_length}.pth')\n",
    "    # os.remove(f'{output_path}train_loader_{bs}_seq_{max_input_length}.pth')\n",
    "else:\n",
    "    train_loader = TDM_dataset.get_train_data(train_df, batch_size=bs, shuffle=True)\n",
    "    # Save dataloader\n",
    "    torch.save(train_loader, f'{output_path}train_loader_{bs}_seq_{max_input_length}.pth')\n",
    "\n",
    "if os.path.exists(f'{output_path}valid_loader_{bs}_seq_{max_input_length}.pth'):\n",
    "    # valid_loader = torch.load(f'{output_path}valid_loader_{bs}_seq_{max_input_length}.pth')\n",
    "    os.remove(f'{output_path}valid_loader_{bs}_seq_{max_input_length}.pth')\n",
    "# else:\n",
    "#     valid_loader = TDM_dataset.get_valid_data(valid_df, batch_size=bs, shuffle=True)\n",
    "#     # Save dataloader\n",
    "#     torch.save(valid_loader, f'{output_path}valid_loader_{bs}_seq_{max_input_length}.pth')\n",
    "\n",
    "# train_loader = TDM_dataset.get_train_data(train_df, batch_size=bs, shuffle=True)\n",
    "valid_loader = TDM_dataset.get_valid_data(valid_df, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "model = selected_processor[1].from_pretrained(\n",
    "                                selected_processor[2], num_labels=2)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n",
    "    model = nn.DataParallel(model)\n",
    "else:\n",
    "    print(f\"Device: {device}\")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "    'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "    'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 109,920,002 trainable parameters\n",
      "The model has 0 non-trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return (sum(p.numel() for p in model.parameters() if p.requires_grad), sum(p.numel() for p in model.parameters() if not p.requires_grad))\n",
    "\n",
    "print(f'The model has {count_parameters(model)[0]:,} trainable parameters')\n",
    "print(f'The model has {count_parameters(model)[1]:,} non-trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, epoch):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    train_loss = AverageMeter()\n",
    "    train_acc = AverageMeter()\n",
    "    train_macro_p = AverageMeter()\n",
    "    train_macro_r = AverageMeter()\n",
    "    train_macro_f1 = AverageMeter()\n",
    "    train_micro_p = AverageMeter()\n",
    "    train_micro_r = AverageMeter()\n",
    "    train_micro_f1 = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pair_token_ids = pair_token_ids.to(device)\n",
    "        mask_ids = mask_ids.to(device)\n",
    "        seg_ids = seg_ids.to(device)\n",
    "        labels = y.to(device)\n",
    "#         ipdb.set_trace()\n",
    "\n",
    "#         loss, prediction = model(pair_token_ids, \n",
    "#                             token_type_ids=seg_ids, \n",
    "#                             attention_mask=mask_ids, \n",
    "#                             labels=labels).values()          \n",
    "        \n",
    "        outputs = model(pair_token_ids, \n",
    "                        token_type_ids=seg_ids, \n",
    "                        attention_mask=mask_ids, \n",
    "                        labels=labels)\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        prediction = outputs.logits\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        prediction = torch.log_softmax(prediction, dim=1).argmax(dim=1)\n",
    "\n",
    "        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/len(labels)) # accuracy_score(labels.cpu(), prediction.cpu())\n",
    "        train_loss.update(loss.item())  \n",
    "        train_macro_p.update(precision_score(labels.cpu(), prediction.cpu(), average ='macro'))\n",
    "        train_macro_r.update(recall_score(labels.cpu(), prediction.cpu(), average ='macro'))\n",
    "        train_macro_f1.update(f1_score(labels.cpu(), prediction.cpu(), average ='macro'))\n",
    "        train_micro_p.update(precision_score(labels.cpu(), prediction.cpu(), average ='micro'))\n",
    "        train_micro_r.update(recall_score(labels.cpu(), prediction.cpu(), average ='micro'))\n",
    "        train_micro_f1.update(f1_score(labels.cpu(), prediction.cpu(), average ='micro'))\n",
    "        \n",
    "        if (batch_idx + 1) % 1000 == 0:\n",
    "            print(f\"[epoch {epoch+1}] [iter {(batch_idx + 1)}/{len(iterator)}]\")\n",
    "            print('------------------------------------------------------------')\n",
    "            print(f\"Train Accuracy Score: {train_acc.avg}; Train loss : {train_loss.avg}\")\n",
    "            print(f\"Macro Precision: {train_macro_p.avg}; Macro Recall : {train_macro_r.avg}; Macro F1 : {train_macro_f1.avg}\")\n",
    "            print(f\"Micro Precision: {train_micro_p.avg}; Micro Recall : {train_micro_r.avg}; Micro F1 : {train_micro_f1.avg}\")\n",
    "            print('------------------------------------------------------------')\n",
    "            \n",
    "    return train_loss.avg, train_acc.avg, train_macro_p.avg, train_macro_r.avg, train_macro_f1.avg, train_micro_p.avg, train_micro_r.avg, train_micro_f1.avg\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, optimizer):\n",
    "        \n",
    "    model.eval()\n",
    "    val_loss = AverageMeter()\n",
    "    val_acc = AverageMeter()\n",
    "    val_macro_p = AverageMeter()\n",
    "    val_macro_r = AverageMeter()\n",
    "    val_macro_f1 = AverageMeter()\n",
    "    val_micro_p = AverageMeter()\n",
    "    val_micro_r = AverageMeter()\n",
    "    val_micro_f1 = AverageMeter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "#             optimizer.zero_grad()\n",
    "            pair_token_ids = pair_token_ids.to(device)\n",
    "            mask_ids = mask_ids.to(device)\n",
    "            seg_ids = seg_ids.to(device)\n",
    "            labels = y.to(device)\n",
    "\n",
    "            outputs = model(pair_token_ids, \n",
    "                        token_type_ids=seg_ids, \n",
    "                        attention_mask=mask_ids, \n",
    "                        labels=labels)\n",
    "            \n",
    "            loss = outputs.loss\n",
    "            prediction = outputs.logits\n",
    "\n",
    "            prediction = torch.log_softmax(prediction, dim=1).argmax(dim=1)\n",
    "            \n",
    "            ipdb.set_trace()\n",
    "\n",
    "            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/len(labels)) # accuracy_score(labels.cpu(), prediction.cpu())\n",
    "            val_macro_p.update(precision_score(labels.cpu(), prediction.cpu(), average ='macro'))\n",
    "            val_macro_r.update(recall_score(labels.cpu(), prediction.cpu(), average ='macro'))\n",
    "            val_macro_f1.update(f1_score(labels.cpu(), prediction.cpu(), average ='macro'))\n",
    "            val_micro_p.update(precision_score(labels.cpu(), prediction.cpu(), average ='micro'))\n",
    "            val_micro_r.update(recall_score(labels.cpu(), prediction.cpu(), average ='micro'))\n",
    "            val_micro_f1.update(f1_score(labels.cpu(), prediction.cpu(), average ='micro'))\n",
    "            val_loss.update(loss.item())        \n",
    "\n",
    "    \n",
    "    val_macro_avg_p, val_macro_avg_r, val_macro_avg_f1 = val_macro_p.avg, val_macro_r.avg, val_macro_f1.avg \n",
    "    val_micro_avg_p, val_micro_avg_r, val_micro_avg_f1 = val_micro_p.avg, val_micro_r.avg, val_micro_f1.avg \n",
    "\n",
    "    print('------------------------------------------------------------')\n",
    "    print(f\"Validation Accuracy Score : {val_acc.avg}; Vadidation loss : {val_loss.avg}\")\n",
    "    print(f\"Macro Precision : {val_macro_avg_p}; Macro Recall : {val_macro_avg_r}; Macro F1 : {val_macro_avg_f1}\")\n",
    "    print(f\"Micro Precision : {val_micro_avg_p}; Micro Recall : {val_micro_avg_r}; Micro F1 : {val_micro_avg_f1}\")\n",
    "    print('------------------------------------------------------------')\n",
    "    \n",
    "    return val_loss.avg, val_acc.avg, val_macro_avg_p, val_macro_avg_r, val_macro_avg_f1, val_micro_avg_p, val_micro_avg_r, val_micro_avg_f1\n",
    "\n",
    "def predict_TDM_from_pdf(model, tokenizer, iterator, output_path):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "            pair_token_ids = pair_token_ids.to(device)\n",
    "            mask_ids = mask_ids.to(device)\n",
    "            seg_ids = seg_ids.to(device)\n",
    "            labels = y.to(device)\n",
    "\n",
    "            outputs = model(pair_token_ids, \n",
    "                        token_type_ids=seg_ids, \n",
    "                        attention_mask=mask_ids, \n",
    "                        labels=labels)\n",
    "        \n",
    "            loss = outputs.loss\n",
    "            prediction = outputs.logits\n",
    "\n",
    "            prediction_scalled = torch.sigmoid(prediction)\n",
    "            \n",
    "            with open(f\"{output_path}test_results.tsv\", \"a+\", encoding=\"utf-8\") as text_file:\n",
    "                for true, false in prediction_scalled.cpu():\n",
    "                    text_file.write(str(true.item())+\"\\t\"+str(false.item())+\"\\n\")\n",
    "\n",
    "def get_top_n_prediction_label(path_to_test_file, path_to_prediction_file, output_path, n = 5):\n",
    "    \"\"\"\n",
    "    This function return the label with the highest proba\n",
    "    \"\"\"\n",
    "    top5 = deque()\n",
    "    with open(f\"{path_to_test_file}\") as f:\n",
    "        txt_test_files = f.read().splitlines()\n",
    "    with open(f\"{path_to_prediction_file}\") as f:\n",
    "        txt_prediction_files = f.read().splitlines()\n",
    "    \n",
    "    for example, prediction in zip(txt_test_files, txt_prediction_files):\n",
    "        true_prob, false_prob = prediction.split(\"\\t\")\n",
    "        true_prob, false_prob = float(true_prob), float(false_prob)\n",
    "        if true_prob > false_prob:\n",
    "            label = example.split(\"\\t\")[2]\n",
    "            top5.append((label, true_prob))\n",
    "    results = deque(sorted(top5, key=lambda x: x[1] if x else x, reverse=False), n)\n",
    "    with open(f\"{output_path}test_top_{n}_tdm.tsv\", \"w+\", encoding=\"utf-8\") as text_file:\n",
    "        for tdm in results:\n",
    "            text_file.write(f\"{tdm[0]}\\t{tdm[1]}\\n\")\n",
    "    return results\n",
    "\n",
    "def write_evaluation_result(val_macro_avg_p, val_macro_avg_r, val_macro_avg_f1, val_micro_avg_p, val_micro_avg_r, val_micro_avg_f1, output_path):\n",
    "    with open(f\"{output_path}evaluation_tdm_results.tsv\", \"w+\", encoding=\"utf-8\") as text_file:\n",
    "        text_file.write(f\"Macro P\\tMacro R\\t Macro F1\\t Micro P\\t Micro R\\t Micro F1\\n\")\n",
    "        text_file.write(f\"{val_macro_avg_p}\\t{val_macro_avg_r}\\t{val_macro_avg_f1}\\t{val_micro_avg_p}\\t{val_micro_avg_r}\\t{val_micro_avg_f1}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_valid_loss = 0.30 #float('inf')\n",
    "# best_valid_f1 = 0.5\n",
    "\n",
    "# for epoch in range(N_EPOCHS):\n",
    "\n",
    "#     start_time = time.time()\n",
    "    \n",
    "# #     train_loss, train_acc, train_macro_avg_p, train_macro_avg_r, train_macro_avg_f1, train_micro_avg_p, train_micro_avg_r, train_micro_avg_f1 = train(model, train_loader, optimizer, epoch)\n",
    "#     valid_loss, valid_acc, val_macro_avg_p, val_macro_avg_r, val_macro_avg_f1, val_micro_avg_p, val_micro_avg_r, val_micro_avg_f1 = evaluate(model, valid_loader, optimizer)\n",
    "    \n",
    "#     end_time = time.time()\n",
    "\n",
    "#     epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "#     print(f'Epoch: {epoch+1:02} Final | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "#     print('------------------------------------------------------------')\n",
    "#     print(f\"Train Accuracy Score: {train_acc}; Train loss : {train_loss}\")\n",
    "#     print(f\"Macro Precision: {train_macro_avg_p}; Macro Recall : {train_macro_avg_r}; Macro F1 : {train_macro_avg_f1}\")\n",
    "#     print(f\"Micro Precision: {train_micro_avg_p}; Micro Recall : {train_micro_avg_r}; Micro F1 : {train_micro_avg_f1}\")\n",
    "#     print('------------------------------------------------------------')\n",
    "    \n",
    "#     valid_metric_avg = (val_macro_avg_p + val_macro_avg_r + val_macro_avg_f1+val_micro_avg_p + val_micro_avg_r + val_micro_avg_f1)/6\n",
    "    \n",
    "    \n",
    "    \n",
    "#     if valid_metric_avg > best_valid_metric_avg : #and abs(valid_loss - best_valid_loss) < 1e-1\n",
    "#         best_valid_metric_avg = valid_metric_avg\n",
    "#         print('Saving Model ...')\n",
    "#         torch.save(model.state_dict(), f'{output_path}Model_{model_name}_avg_metric_{str(best_valid_metric_avg)[:4]}.pt')\n",
    "#         print('****************************************************************************')\n",
    "#         print('best record: [epoch %d], [val loss %.5f], [val acc %.5f], [val avg. metric %.5f]' % (epoch, valid_loss, valid_acc, valid_metric_avg))\n",
    "#         print(f\"Macro Precision : {val_macro_avg_p}; Macro Recall : {val_macro_avg_r}; Macro F1 : {val_macro_avg_f1}\")\n",
    "#         print(f\"Micro Precision : {val_micro_avg_p}; Micro Recall : {val_micro_avg_r}; Micro F1 : {val_micro_avg_f1}\")\n",
    "#         print('****************************************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We'll then use the model to test the sentiment of some sequences. We tokenize the input sequence, trim it down to the maximum length, add the special tokens to either side, convert it to a tensor, add a fake batch dimension and then pass it through our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pt_path = \"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/ibm/exp/few-shot-setup/NLP-TDMS/paperVersion/torch/SciBert/Model_SciBert_avg_metric_0.9001.pt\"\n",
    "# model_pt_path = \"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/new/60Neg800unk/twofoldwithunk/fold1/Model_SciBert_avg_metric_0.95.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the best model\n",
    "# model.load_state_dict(torch.load('Model_f1_0.93.pt'))\n",
    "model.load_state_dict(torch.load(model_pt_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/817 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-29-b05de942b1ce>\u001b[0m(95)\u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     94 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 95 \u001b[0;31m            \u001b[0mval_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# accuracy_score(labels.cpu(), prediction.cpu())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     96 \u001b[0;31m            \u001b[0mval_macro_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  label\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** NameError: name 'label' is not defined\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-29-b05de942b1ce>\u001b[0m(96)\u001b[0;36mevaluate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     95 \u001b[0;31m            \u001b[0mval_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# accuracy_score(labels.cpu(), prediction.cpu())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 96 \u001b[0;31m            \u001b[0mval_macro_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     97 \u001b[0;31m            \u001b[0mval_macro_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  labels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  prediction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], device='cuda:0')\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  classification_report(labels.cpu(), prediction.cpu(), labels=[1, 0], output_dict=True)['macro avg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/home/kabenamualus/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/nfs/home/kabenamualus/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/nfs/home/kabenamualus/.local/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.4375, 'recall': 0.5, 'f1-score': 0.4666666666666667, 'support': 16}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  f1_score(labels.cpu(), prediction.cpu(), average ='macro'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SyntaxError: unexpected EOF while parsing\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  f1_score(labels.cpu(), prediction.cpu(), average ='macro')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4666666666666667\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  f1_score(labels.cpu(), prediction.cpu(), average ='macro', zero_division=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4666666666666667\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  f1_score(labels.cpu(), prediction.cpu(), average ='macro', zero_division=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4666666666666667\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  f1_score(labels.cpu(), prediction.cpu(), average ='macro')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4666666666666667\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  f1_score(labels.cpu(), prediction.cpu(), average ='macro')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4666666666666667\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  precision_score(labels.cpu(), prediction.cpu(), average ='micro')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  f1_score(labels.cpu(), prediction.cpu(), average ='micro')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.875\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/817 [05:12<?, ?it/s]\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-07045b604380>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalid_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_macro_avg_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_macro_avg_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_macro_avg_f1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_micro_avg_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_micro_avg_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_micro_avg_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-29-b05de942b1ce>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, iterator, optimizer)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mval_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# accuracy_score(labels.cpu(), prediction.cpu())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mval_macro_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mval_macro_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mval_macro_f1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-b05de942b1ce>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, iterator, optimizer)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0mval_acc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# accuracy_score(labels.cpu(), prediction.cpu())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mval_macro_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mval_macro_r\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mval_macro_f1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "valid_loss, valid_acc, val_macro_avg_p, val_macro_avg_r, val_macro_avg_f1, val_micro_avg_p, val_micro_avg_r, val_micro_avg_f1 = evaluate(model, valid_loader, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>1803.11175.pdf</td>\n",
       "      <td>sentiment analysis; SUBJ; Accuracy</td>\n",
       "      <td>Universal Sentence Encoder We present models f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>1803.11175.pdf</td>\n",
       "      <td>text classification; TREC; Error</td>\n",
       "      <td>Universal Sentence Encoder We present models f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>1803.11175.pdf</td>\n",
       "      <td>question answering; SQuAD; F1</td>\n",
       "      <td>Universal Sentence Encoder We present models f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>1803.11175.pdf</td>\n",
       "      <td>relation prediction; FB15K-237; H@1</td>\n",
       "      <td>Universal Sentence Encoder We present models f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>1803.11175.pdf</td>\n",
       "      <td>word sense disambiguation; SemEval 2013; F1</td>\n",
       "      <td>Universal Sentence Encoder We present models f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label           title                                          TDM  \\\n",
       "0   True  1803.11175.pdf           sentiment analysis; SUBJ; Accuracy   \n",
       "1   True  1803.11175.pdf             text classification; TREC; Error   \n",
       "2  False  1803.11175.pdf                question answering; SQuAD; F1   \n",
       "3  False  1803.11175.pdf          relation prediction; FB15K-237; H@1   \n",
       "4  False  1803.11175.pdf  word sense disambiguation; SemEval 2013; F1   \n",
       "\n",
       "                                             Context  \n",
       "0  Universal Sentence Encoder We present models f...  \n",
       "1  Universal Sentence Encoder We present models f...  \n",
       "2  Universal Sentence Encoder We present models f...  \n",
       "3  Universal Sentence Encoder We present models f...  \n",
       "4  Universal Sentence Encoder We present models f...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_df = pd.read_csv(\"../data/paperwithcode/new/60Neg800unk/twofoldwithunk/fold1/test_results.tsv\", \n",
    "#                    sep=\"\\t\", names=[\"true\", \"false\"])\n",
    "\n",
    "# test_df = pd.read_csv(\"../data/paperwithcode/new/jar/10Neg20unk/testOutput.tsv\", \n",
    "#                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])\n",
    "\n",
    "test_df = valid_df\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2655/2655 [00:39<00:00, 67.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2655\n"
     ]
    }
   ],
   "source": [
    "# test_loader = TDM_dataset.get_inference_data(test_df, batch_size=16, shuffle=False) # this shuffle should be false to preserve the order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(valid_path) as f:\n",
    "    list_prediction_inputs = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13071"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_prediction_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_text = list_prediction_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'true\\t1803.11175.pdf\\tsentiment analysis; SUBJ; Accuracy\\tUniversal Sentence Encoder We present models '"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output  : tensor([[0.4158, 0.6538]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "Outputs logits  : tensor([[-0.3399,  0.6357]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "encoded_review = tokenizer.encode_plus(\n",
    "  list_prediction_inputs[-200],\n",
    "  max_length=max_input_length,\n",
    "  add_special_tokens=True,\n",
    "  return_token_type_ids=False,\n",
    "  pad_to_max_length=True,\n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',\n",
    ")\n",
    "\n",
    "input_ids = encoded_review['input_ids'].to(device)\n",
    "attention_mask = encoded_review['attention_mask'].to(device)\n",
    "\n",
    "outputs = model(input_ids, attention_mask)\n",
    "\n",
    "prediction_scalled = torch.sigmoid(outputs.logits)\n",
    "# _, prediction = torch.max(output, dim=1)\n",
    "# print(f'Review text: {review_text}')\n",
    "\n",
    "print(f'Output  : {prediction_scalled}')\n",
    "print(f'Outputs logits  : {outputs.logits}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output  : tensor([[0.1245, 0.8405]], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
      "Outputs logits  : tensor([[-1.9507,  1.6618]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = iter(test_loader)\n",
    "# sample.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_TDM_from_pdf(model, tokenizer, sentence):\n",
    "#     model.eval()\n",
    "#     tokens = tokenizer.tokenize(sentence)\n",
    "#     tokens = tokens[:max_input_length-2]\n",
    "#     indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
    "#     tensor = torch.LongTensor(indexed).to(device)\n",
    "#     tensor = tensor.unsqueeze(0)\n",
    "#     prediction = torch.sigmoid(model(tensor))\n",
    "#     return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_TDM_from_pdf(model, tokenizer, iterator):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "            pair_token_ids = pair_token_ids.to(device)\n",
    "            mask_ids = mask_ids.to(device)\n",
    "            seg_ids = seg_ids.to(device)\n",
    "            labels = y.to(device)\n",
    "\n",
    "            loss, prediction = model(pair_token_ids, \n",
    "                                 token_type_ids=seg_ids, \n",
    "                                 attention_mask=mask_ids, \n",
    "                                 labels=labels).values()\n",
    "\n",
    "            prediction_scalled = torch.sigmoid(prediction)\n",
    "            \n",
    "            with open(\"test_results.tsv\", \"a+\", encoding=\"utf-8\") as text_file:\n",
    "                for true, false in prediction_scalled.cpu():\n",
    "                    text_file.write(str(true.item())+\"\\t\"+str(false.item())+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 166/166 [00:31<00:00,  5.35it/s]\n"
     ]
    }
   ],
   "source": [
    "predict_TDM_from_pdf(model, tokenizer, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def get_top_n_prediction_label(path_to_test_file, path_to_prediction_file, n = 5):\n",
    "    \"\"\"\n",
    "    This function return the label with the highest proba\n",
    "    \"\"\"\n",
    "    top5 = deque()\n",
    "    with open(f\"{path_to_test_file}\") as f:\n",
    "        txt_test_files = f.read().splitlines()\n",
    "    with open(f\"{path_to_prediction_file}\") as f:\n",
    "        txt_prediction_files = f.read().splitlines()\n",
    "    \n",
    "    highest = 0\n",
    "    for example, prediction in zip(txt_test_files, txt_prediction_files):\n",
    "        true_prob, false_prob = prediction.split(\"\\t\")\n",
    "        true_prob, false_prob = float(true_prob), float(false_prob)\n",
    "        if true_prob > false_prob:\n",
    "            label = example.split(\"\\t\")[2]\n",
    "            highest = true_prob\n",
    "            top5.append((label, true_prob))\n",
    "    return deque(sorted(top5, key=lambda x: x[1] if x else x, reverse=False), n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([('Image Clustering; Extended Yale-B; Accuracy', 0.8984434008598328)])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n_prediction_label(\n",
    "    path_to_test_file=\"../data/paperwithcode/new/jar/10Neg20unk/testOutput.tsv\",\n",
    "    path_to_prediction_file=\"test_results.tsv\", \n",
    "    n = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
