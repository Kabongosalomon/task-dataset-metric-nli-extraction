{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd65eba3",
   "metadata": {},
   "source": [
    "# Package loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5f46ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "import ipdb\n",
    "import spacy\n",
    "# spacy.load(\"en_core_web_sm\")\n",
    "import torch\n",
    "from torchtext import data\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "\n",
    "from transformers import RobertaTokenizer, BertTokenizer, BertModel, TransfoXLTokenizer, TransfoXLModel, AdamW\n",
    "from transformers import BigBirdTokenizer, BigBirdForSequenceClassification\n",
    "from transformers import LongformerTokenizer, LongformerForSequenceClassification\n",
    "from transformers import XLNetTokenizer, TFXLNetForSequenceClassification\n",
    "\n",
    "import optuna\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37d5fbf",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b8bebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../data/paperwithcode/new/60Neg800unk/twofoldwithunk/fold1/train.tsv\", \n",
    "                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])\n",
    "\n",
    "test_df = pd.read_csv(\"../data/paperwithcode/new/60Neg800unk/twofoldwithunk/fold1/test.tsv\", \n",
    "                   sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "204784d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>1810.02575v1.pdf</td>\n",
       "      <td>Semantic Segmentation; Nighttime Driving; mIoU</td>\n",
       "      <td>Dark Model Adaptation: Semantic Image Segmenta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>1810.02575v1.pdf</td>\n",
       "      <td>Extractive Text Summarization; DebateSum; ROUGE-L</td>\n",
       "      <td>Dark Model Adaptation: Semantic Image Segmenta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>1810.02575v1.pdf</td>\n",
       "      <td>Action Recognition; Something-Something V1; To...</td>\n",
       "      <td>Dark Model Adaptation: Semantic Image Segmenta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>1810.02575v1.pdf</td>\n",
       "      <td>Multi-Object Tracking; MOTS20; sMOTSA</td>\n",
       "      <td>Dark Model Adaptation: Semantic Image Segmenta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>1810.02575v1.pdf</td>\n",
       "      <td>Continuous Control; PyBullet Ant; Return</td>\n",
       "      <td>Dark Model Adaptation: Semantic Image Segmenta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label             title                                                TDM  \\\n",
       "0   True  1810.02575v1.pdf     Semantic Segmentation; Nighttime Driving; mIoU   \n",
       "1  False  1810.02575v1.pdf  Extractive Text Summarization; DebateSum; ROUGE-L   \n",
       "2  False  1810.02575v1.pdf  Action Recognition; Something-Something V1; To...   \n",
       "3  False  1810.02575v1.pdf              Multi-Object Tracking; MOTS20; sMOTSA   \n",
       "4  False  1810.02575v1.pdf           Continuous Control; PyBullet Ant; Return   \n",
       "\n",
       "                                             Context  \n",
       "0  Dark Model Adaptation: Semantic Image Segmenta...  \n",
       "1  Dark Model Adaptation: Semantic Image Segmenta...  \n",
       "2  Dark Model Adaptation: Semantic Image Segmenta...  \n",
       "3  Dark Model Adaptation: Semantic Image Segmenta...  \n",
       "4  Dark Model Adaptation: Semantic Image Segmenta...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9fa48e37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>1707.03497v2.pdf</td>\n",
       "      <td>Atari Games; Atari 2600 Seaquest; Score</td>\n",
       "      <td>Value Prediction Network This paper proposes a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>1707.03497v2.pdf</td>\n",
       "      <td>Atari Games; Atari 2600 Amidar; Score</td>\n",
       "      <td>Value Prediction Network This paper proposes a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>1707.03497v2.pdf</td>\n",
       "      <td>Atari Games; Atari 2600 Krull; Score</td>\n",
       "      <td>Value Prediction Network This paper proposes a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>1707.03497v2.pdf</td>\n",
       "      <td>Atari Games; Atari 2600 Alien; Score</td>\n",
       "      <td>Value Prediction Network This paper proposes a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>1707.03497v2.pdf</td>\n",
       "      <td>Atari Games; Atari 2600 Enduro; Score</td>\n",
       "      <td>Value Prediction Network This paper proposes a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label             title                                      TDM  \\\n",
       "0   True  1707.03497v2.pdf  Atari Games; Atari 2600 Seaquest; Score   \n",
       "1   True  1707.03497v2.pdf    Atari Games; Atari 2600 Amidar; Score   \n",
       "2   True  1707.03497v2.pdf     Atari Games; Atari 2600 Krull; Score   \n",
       "3   True  1707.03497v2.pdf     Atari Games; Atari 2600 Alien; Score   \n",
       "4   True  1707.03497v2.pdf    Atari Games; Atari 2600 Enduro; Score   \n",
       "\n",
       "                                             Context  \n",
       "0  Value Prediction Network This paper proposes a...  \n",
       "1  Value Prediction Network This paper proposes a...  \n",
       "2  Value Prediction Network This paper proposes a...  \n",
       "3  Value Prediction Network This paper proposes a...  \n",
       "4  Value Prediction Network This paper proposes a...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565e0a19",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb005179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] [SEP] <pad> <unk>\n"
     ]
    }
   ],
   "source": [
    "model_key = 'google/bigbird-roberta-base'\n",
    "# model_key = 'allenai/longformer-base-4096'\n",
    "# model_key = 'allenai/scibert_scivocab_uncased'\n",
    "# model_key = 'xlnet-base-cased'\n",
    "\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base') # roberta-base, bert-base-uncased\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BigBirdTokenizer.from_pretrained(model_key)\n",
    "# tokenizer = LongformerTokenizer.from_pretrained(model_key)\n",
    "# tokenizer =  XLNetTokenizer.from_pretrained(model_key)\n",
    "\n",
    "init_token = tokenizer.cls_token\n",
    "eos_token = tokenizer.sep_token\n",
    "pad_token = tokenizer.pad_token\n",
    "unk_token = tokenizer.unk_token\n",
    "\n",
    "print(init_token, eos_token, pad_token, unk_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c465a271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 66 0 100\n"
     ]
    }
   ],
   "source": [
    "init_token_idx = tokenizer.convert_tokens_to_ids(init_token)\n",
    "eos_token_idx = tokenizer.convert_tokens_to_ids(eos_token)\n",
    "pad_token_idx = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "unk_token_idx = tokenizer.convert_tokens_to_ids(unk_token)\n",
    "\n",
    "print(init_token_idx, eos_token_idx, pad_token_idx, unk_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "041a1bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    }
   ],
   "source": [
    "max_input_length = tokenizer.max_model_input_sizes[model_key]\n",
    "\n",
    "print(max_input_length)\n",
    "\n",
    "def tokenize_and_cut(sentence):\n",
    "    tokens = tokenizer.tokenize(sentence) \n",
    "    tokens = tokens[:max_input_length-2]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0350f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformersNLI(Dataset):\n",
    "    def __init__(self, train_df, val_df, tokenizer, max_input_length, base_path=\"./\"):\n",
    "        self.label_dict = {'True': 0, 'False': 1} # Default {'entailment': 0, 'contradiction': 1, 'neutral': 2}\n",
    "        self.train_df = train_df\n",
    "        self.val_df = val_df\n",
    "        self.base_path = base_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.train_data = None\n",
    "        self.val_data = None\n",
    "#         self.init_data()\n",
    "        \n",
    "    def init_data(self):\n",
    "        self.train_data = self.load_data(self.train_df)\n",
    "        self.val_data = self.load_data(self.val_df)\n",
    "        \n",
    "    def _truncate_seq_pair(self, tokens_a, tokens_b, max_length):\n",
    "        \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "        # This is a simple heuristic which will always truncate the longer sequence\n",
    "        # one token at a time. This makes more sense than truncating an equal percent\n",
    "        # of tokens from each, since if one sequence is very short then each token\n",
    "        # that's truncated likely contains more information than a longer sequence.\n",
    "        while True:\n",
    "            total_length = len(tokens_a) + len(tokens_b)\n",
    "            if total_length <= max_length:\n",
    "                break\n",
    "            if len(tokens_a) > len(tokens_b):\n",
    "                tokens_a.pop()\n",
    "            else:\n",
    "                tokens_b.pop()\n",
    "        \n",
    "    def load_data(self, df):\n",
    "        MAX_LEN = self.max_input_length\n",
    "        token_ids = []\n",
    "        mask_ids = []\n",
    "        seg_ids = []\n",
    "        y = []\n",
    "\n",
    "        premise_list = df['TDM'].to_list()           # df['sentence1'].to_list()\n",
    "        hypothesis_list = df['Context'].to_list()    # df['sentence2'].to_list()\n",
    "        label_list = df['label'].to_list()           # df['gold_label'].to_list()\n",
    "\n",
    "        for (premise, hypothesis, label) in tqdm(zip(premise_list, hypothesis_list, label_list), total=len(label_list)):\n",
    "            premise_id = self.tokenizer.encode(premise, add_special_tokens = False)\n",
    "            hypothesis_id = self.tokenizer.encode(hypothesis, add_special_tokens = False)\n",
    "#             if len(premise_id)+len(hypothesis_id) >= MAX_LEN:\n",
    "#                 ipdb.set_trace()\n",
    "#                 pass\n",
    "            # ignore the warning as the ong sequence issuw is taken care of here \n",
    "            self._truncate_seq_pair(premise_id, hypothesis_id, MAX_LEN-3) # -3 to account for the special characters \n",
    "            \n",
    "            pair_token_ids = [self.tokenizer.cls_token_id] + premise_id \\\n",
    "                            + [self.tokenizer.sep_token_id] + hypothesis_id \\\n",
    "                            + [self.tokenizer.sep_token_id]\n",
    "            premise_len = len(premise_id)\n",
    "            hypothesis_len = len(hypothesis_id)\n",
    "\n",
    "            segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (hypothesis_len + 1))  # sentence 0 and sentence 1\n",
    "            attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values\n",
    "\n",
    "            token_ids.append(torch.tensor(pair_token_ids))\n",
    "            seg_ids.append(segment_ids)\n",
    "            mask_ids.append(attention_mask_ids)\n",
    "            # we have str(label) to have the key work proprely \n",
    "            y.append(self.label_dict[str(label)]) # y.append(self.label_dict[label]) \n",
    "            \n",
    "        token_ids = pad_sequence(token_ids, batch_first=True)\n",
    "        mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
    "        seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
    "        y = torch.tensor(y)\n",
    "        dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)\n",
    "        print(len(dataset))\n",
    "        return dataset\n",
    "\n",
    "    def get_data_loaders(self, batch_size=32, shuffle=True):\n",
    "        self.init_data()\n",
    "        train_loader = DataLoader(\n",
    "          self.train_data,\n",
    "          shuffle=shuffle,\n",
    "          batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "          self.val_data,\n",
    "          shuffle=shuffle,\n",
    "          batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    def get_inference_data(self, test_df, batch_size=32, shuffle=False):\n",
    "        test_data = self.load_data(test_df)\n",
    "                       \n",
    "        test_loader = DataLoader(\n",
    "          test_data,\n",
    "          shuffle=shuffle,\n",
    "          batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "          self.val_data,\n",
    "          shuffle=shuffle,\n",
    "          batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbe36ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 256008/256008 [15:06<00:00, 282.54it/s]\n",
      "  0%|          | 102/108456 [00:00<04:40, 385.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 108456/108456 [06:38<00:00, 272.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108456\n"
     ]
    }
   ],
   "source": [
    "TDM_dataset = TransformersNLI(train_df, test_df, tokenizer, max_input_length, base_path=\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6312bbb1-5ba4-47ae-a767-0e0d770f504a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, valid_loader = TDM_dataset.get_data_loaders(batch_size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c407a6-8d2f-4f30-95a7-4b33f2264046",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a879b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RobertaModel.from_pretrained('roberta-base') # bert-base-cased, bert-large-cased\n",
    "# bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "# model = TransfoXLModel.from_pretrained('transfo-xl-wt103')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60c568d2-6be2-444c-b10b-c8758f7b0de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.nn as nn\n",
    "\n",
    "# class BERTGRUSentiment(nn.Module):\n",
    "#     def __init__(self,\n",
    "#                  bert,\n",
    "#                  hidden_dim,\n",
    "#                  output_dim,\n",
    "#                  n_layers,\n",
    "#                  bidirectional,\n",
    "#                  dropout):\n",
    "        \n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.bert = bert\n",
    "        \n",
    "#         embedding_dim = bert.config.to_dict()['hidden_size']\n",
    "        \n",
    "#         self.rnn = nn.GRU(embedding_dim,\n",
    "#                           hidden_dim,\n",
    "#                           num_layers = n_layers,\n",
    "#                           bidirectional = bidirectional,\n",
    "#                           batch_first = True,\n",
    "#                           dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "#         self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "#     def forward(self, text):\n",
    "        \n",
    "#         #text = [batch size, sent len]\n",
    "                \n",
    "#         with torch.no_grad():\n",
    "#             embedded = self.bert(text)[0]\n",
    "                \n",
    "#         #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "#         _, hidden = self.rnn(embedded)\n",
    "        \n",
    "#         #hidden = [n layers * n directions, batch size, emb dim]\n",
    "        \n",
    "#         if self.rnn.bidirectional:\n",
    "#             hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "#         else:\n",
    "#             hidden = self.dropout(hidden[-1,:,:])\n",
    "                \n",
    "#         #hidden = [batch size, hid dim]\n",
    "        \n",
    "#         output = self.out(hidden)\n",
    "        \n",
    "#         #output = [batch size, out dim]\n",
    "        \n",
    "#         return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67eeee41-0d68-47bf-956d-783eefeb2b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN_DIM = 256\n",
    "# OUTPUT_DIM = 1\n",
    "# N_LAYERS = 2\n",
    "# BIDIRECTIONAL = True\n",
    "# DROPOUT = 0.25\n",
    "\n",
    "# model = BERTGRUSentiment(bert,\n",
    "#                          HIDDEN_DIM,\n",
    "#                          OUTPUT_DIM,\n",
    "#                          N_LAYERS,\n",
    "#                          BIDIRECTIONAL,\n",
    "#                          DROPOUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9b5008d2-0b45-4411-a353-1898dab4690b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "994ec47c-81b4-4276-9e24-91603e9f6759",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/bigbird-roberta-base were not used when initializing BigBirdForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BigBirdForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BigBirdForSequenceClassification were not initialized from the model checkpoint at google/bigbird-roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BigBirdForSequenceClassification.from_pretrained(model_key, num_labels=2)\n",
    "# LongformerForSequenceClassification.from_pretrained(model_key, num_labels=2)\n",
    "model = model.to(device)\n",
    "# criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b060d62-204d-4ea7-80e3-14007ba1be53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr = 1e-3, momentum=0.9, \n",
    "#                       weight_decay=0, dampening=0, nesterov=True)\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr = 1e-2, betas=(0.9, 0.999), \n",
    "#                       weight_decay=0.0, amsgrad=False)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "# criterion = nn.CrossEntropyLoss(weight=w)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# model = BERTGRUSentiment(model,\n",
    "#                          HIDDEN_DIM,\n",
    "#                          OUTPUT_DIM,\n",
    "#                          N_LAYERS,\n",
    "#                          BIDIRECTIONAL,\n",
    "#                          DROPOUT)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81eb2503-3bdc-4b78-a3a9-610667d7a7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 128,060,930 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80997ae3-5829-4b04-9b34-26873263297f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():                \n",
    "#     if name.startswith('bert'):\n",
    "#         param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d88bbbe-2705-402e-a239-901311f4aa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb4e40ec-47a1-4a9c-9d7b-8eb6114ce843",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75ee35c0-656e-49e5-b792-e2415e101cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    train_loss = AverageMeter()\n",
    "    train_acc = AverageMeter()\n",
    "    train_f1 = AverageMeter()\n",
    "    \n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        pair_token_ids = pair_token_ids.to(device)\n",
    "        mask_ids = mask_ids.to(device)\n",
    "        seg_ids = seg_ids.to(device)\n",
    "        labels = y.to(device)\n",
    "\n",
    "        loss, prediction = model(pair_token_ids, \n",
    "                             token_type_ids=seg_ids, \n",
    "                             attention_mask=mask_ids, \n",
    "                             labels=labels).values()          \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        prediction = torch.log_softmax(prediction, dim=1).argmax(dim=1)\n",
    "   \n",
    "        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/len(labels)) # accuracy_score(labels.cpu(), prediction.cpu())\n",
    "        train_f1.update(f1_score(labels.cpu(), prediction.cpu(), average ='macro'))\n",
    "        train_loss.update(loss.item())  \n",
    "        \n",
    "        if (batch_idx + 1) % 1000 == 0:\n",
    "            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f], [train f1 %.5f]' % (\n",
    "                epoch, batch_idx + 1, len(iterator), train_loss.avg, train_acc.avg, train_f1.avg))\n",
    "            \n",
    "    return train_loss.avg, train_acc.avg, train_f1.avg\n",
    "\n",
    "def evaluate(model, iterator, optimizer):\n",
    "    \n",
    "    model.eval()\n",
    "    val_loss = AverageMeter()\n",
    "    val_acc = AverageMeter()\n",
    "    val_f1 = AverageMeter()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "#             optimizer.zero_grad()\n",
    "            pair_token_ids = pair_token_ids.to(device)\n",
    "            mask_ids = mask_ids.to(device)\n",
    "            seg_ids = seg_ids.to(device)\n",
    "            labels = y.to(device)\n",
    "\n",
    "            loss, prediction = model(pair_token_ids, \n",
    "                                 token_type_ids=seg_ids, \n",
    "                                 attention_mask=mask_ids, \n",
    "                                 labels=labels).values()\n",
    "\n",
    "            prediction = torch.log_softmax(prediction, dim=1).argmax(dim=1)\n",
    "   \n",
    "            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/len(labels)) # accuracy_score(labels.cpu(), prediction.cpu())\n",
    "            val_f1.update(f1_score(labels.cpu(), prediction.cpu(), average ='macro'))\n",
    "            val_loss.update(loss.item())        \n",
    "\n",
    "    \n",
    "    print('------------------------------------------------------------')\n",
    "    print(f\"Accuracy Score : {val_acc.avg}; F1 Score : {val_f1.avg}\")\n",
    "    print('------------------------------------------------------------')\n",
    "    \n",
    "    return val_loss.avg, val_acc.avg, val_f1.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "612082ba-34db-4655-9570-5d54663b903b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5067b33e-a8ea-4535-a322-5fccbd6be18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1000/42668 [37:14<25:51:05,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 1000 / 42668], [train loss 0.41968], [train acc 0.85583], [train f1 0.65853]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 2000/42668 [1:14:28<25:14:48,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 2000 / 42668], [train loss 0.41917], [train acc 0.85467], [train f1 0.65229]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 3000/42668 [1:51:41<24:34:26,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 3000 / 42668], [train loss 0.41910], [train acc 0.85417], [train f1 0.64812]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 4000/42668 [2:28:53<23:59:47,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 4000 / 42668], [train loss 0.42317], [train acc 0.85138], [train f1 0.64355]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 5000/42668 [3:06:06<23:20:45,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 5000 / 42668], [train loss 0.42193], [train acc 0.85190], [train f1 0.64573]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 6000/42668 [3:43:18<22:43:01,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 6000 / 42668], [train loss 0.41990], [train acc 0.85294], [train f1 0.64831]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▋        | 7000/42668 [4:20:31<22:08:51,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 7000 / 42668], [train loss 0.41941], [train acc 0.85312], [train f1 0.64872]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 8000/42668 [4:57:44<21:29:13,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 8000 / 42668], [train loss 0.41874], [train acc 0.85342], [train f1 0.64872]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 9000/42668 [5:34:59<20:52:18,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 9000 / 42668], [train loss 0.41854], [train acc 0.85348], [train f1 0.64950]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 10000/42668 [6:12:11<20:15:09,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 10000 / 42668], [train loss 0.41921], [train acc 0.85305], [train f1 0.64924]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 11000/42668 [6:49:25<19:39:50,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 11000 / 42668], [train loss 0.41857], [train acc 0.85339], [train f1 0.64950]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 12000/42668 [7:26:39<19:01:32,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 12000 / 42668], [train loss 0.41868], [train acc 0.85331], [train f1 0.64938]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 13000/42668 [8:03:52<18:23:55,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 13000 / 42668], [train loss 0.41915], [train acc 0.85300], [train f1 0.64883]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 14000/42668 [8:41:06<17:46:29,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 14000 / 42668], [train loss 0.41878], [train acc 0.85317], [train f1 0.64878]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 15000/42668 [9:18:20<17:08:41,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 15000 / 42668], [train loss 0.41958], [train acc 0.85269], [train f1 0.64809]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 16000/42668 [9:55:34<16:32:24,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 16000 / 42668], [train loss 0.41951], [train acc 0.85271], [train f1 0.64832]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 17000/42668 [10:32:45<15:54:03,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 17000 / 42668], [train loss 0.41975], [train acc 0.85255], [train f1 0.64809]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 18000/42668 [11:09:56<15:17:38,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 18000 / 42668], [train loss 0.41960], [train acc 0.85261], [train f1 0.64821]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▍     | 19000/42668 [11:47:07<14:39:18,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 19000 / 42668], [train loss 0.41981], [train acc 0.85248], [train f1 0.64781]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 20000/42668 [12:24:17<14:01:56,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 20000 / 42668], [train loss 0.41972], [train acc 0.85253], [train f1 0.64829]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 21000/42668 [13:01:26<13:25:17,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 21000 / 42668], [train loss 0.41960], [train acc 0.85259], [train f1 0.64862]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 22000/42668 [13:38:34<12:47:32,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 22000 / 42668], [train loss 0.41997], [train acc 0.85238], [train f1 0.64841]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 23000/42668 [14:15:43<12:10:25,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 23000 / 42668], [train loss 0.41953], [train acc 0.85262], [train f1 0.64899]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 24000/42668 [14:52:51<11:36:02,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 24000 / 42668], [train loss 0.41959], [train acc 0.85257], [train f1 0.64906]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 25000/42668 [15:30:00<10:56:04,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 25000 / 42668], [train loss 0.42006], [train acc 0.85229], [train f1 0.64865]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 26000/42668 [16:07:08<10:22:28,  2.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 26000 / 42668], [train loss 0.42006], [train acc 0.85228], [train f1 0.64864]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 27000/42668 [16:44:16<9:41:50,  2.23s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 27000 / 42668], [train loss 0.42004], [train acc 0.85228], [train f1 0.64869]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 28000/42668 [17:21:25<9:04:51,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 28000 / 42668], [train loss 0.41973], [train acc 0.85245], [train f1 0.64896]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 29000/42668 [17:58:33<8:27:30,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 29000 / 42668], [train loss 0.42014], [train acc 0.85219], [train f1 0.64879]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 30000/42668 [18:35:41<7:50:31,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 30000 / 42668], [train loss 0.42017], [train acc 0.85217], [train f1 0.64903]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 31000/42668 [19:12:49<7:13:19,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 31000 / 42668], [train loss 0.42009], [train acc 0.85220], [train f1 0.64912]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 32000/42668 [19:49:57<6:36:12,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 32000 / 42668], [train loss 0.42023], [train acc 0.85211], [train f1 0.64883]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 33000/42668 [20:27:06<5:59:03,  2.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 0], [iter 33000 / 42668], [train loss 0.42036], [train acc 0.85202], [train f1 0.64873]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 33627/42668 [20:50:23<5:35:42,  2.23s/it]"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 2\n",
    "\n",
    "best_valid_loss = 0.30 #float('inf')\n",
    "best_valid_f1 = 0.5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_f1 = train(model, train_loader, optimizer)\n",
    "    valid_loss, valid_acc, valid_f1 = evaluate(model, valid_loader, optimizer)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_f1*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_f1*100:.2f}%')\n",
    "    \n",
    "    if valid_f1 > best_valid_f1 : #and abs(valid_loss - best_valid_loss) < 1e-1\n",
    "        best_valid_f1 = valid_f1\n",
    "        print('Saving Model ...')\n",
    "        torch.save(model.state_dict(), 'Model_f1_'+str(best_valid_f1)[:4]+'.pt')\n",
    "        print('*****************************************************')\n",
    "        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f], [val f1 %.5f]' % (epoch, valid_loss, valid_acc, valid_f1))\n",
    "        print('*****************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cd9eb3-24f6-4036-9f8b-0a25fd60a873",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "We'll then use the model to test the sentiment of some sequences. We tokenize the input sequence, trim it down to the maximum length, add the special tokens to either side, convert it to a tensor, add a fake batch dimension and then pass it through our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bd499fd-ee3b-4909-bbc5-9f5af8c7c5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reload the best model\n",
    "model.load_state_dict(torch.load('Model_f1_0.93.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e64d764-527a-4d51-836f-04e003c89b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>1203.1005v3.pdf</td>\n",
       "      <td>Single Image Deraining; Rain100H; PSNR</td>\n",
       "      <td>Sparse Subspace Clustering: Algorithm, Theory,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>1203.1005v3.pdf</td>\n",
       "      <td>Question Answering; YahooCQA; P@1</td>\n",
       "      <td>Sparse Subspace Clustering: Algorithm, Theory,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>1203.1005v3.pdf</td>\n",
       "      <td>Atari Games; Atari 2600 Private Eye; Score</td>\n",
       "      <td>Sparse Subspace Clustering: Algorithm, Theory,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>1203.1005v3.pdf</td>\n",
       "      <td>Speech Recognition; MediaSpeech; WER for Turkish</td>\n",
       "      <td>Sparse Subspace Clustering: Algorithm, Theory,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>1203.1005v3.pdf</td>\n",
       "      <td>3D Point Cloud Classification; ModelNet40; Mea...</td>\n",
       "      <td>Sparse Subspace Clustering: Algorithm, Theory,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label            title                                                TDM  \\\n",
       "0   True  1203.1005v3.pdf             Single Image Deraining; Rain100H; PSNR   \n",
       "1   True  1203.1005v3.pdf                  Question Answering; YahooCQA; P@1   \n",
       "2   True  1203.1005v3.pdf         Atari Games; Atari 2600 Private Eye; Score   \n",
       "3   True  1203.1005v3.pdf   Speech Recognition; MediaSpeech; WER for Turkish   \n",
       "4   True  1203.1005v3.pdf  3D Point Cloud Classification; ModelNet40; Mea...   \n",
       "\n",
       "                                             Context  \n",
       "0  Sparse Subspace Clustering: Algorithm, Theory,...  \n",
       "1  Sparse Subspace Clustering: Algorithm, Theory,...  \n",
       "2  Sparse Subspace Clustering: Algorithm, Theory,...  \n",
       "3  Sparse Subspace Clustering: Algorithm, Theory,...  \n",
       "4  Sparse Subspace Clustering: Algorithm, Theory,...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_df = pd.read_csv(\"../data/paperwithcode/new/60Neg800unk/twofoldwithunk/fold1/test_results.tsv\", \n",
    "#                    sep=\"\\t\", names=[\"true\", \"false\"])\n",
    "\n",
    "test_df = pd.read_csv(\"../data/paperwithcode/new/jar/10Neg20unk/testOutput.tsv\", \n",
    "                   sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d81d25ea-3635-4d9b-81e6-bf116431e900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2655/2655 [00:39<00:00, 67.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2655\n"
     ]
    }
   ],
   "source": [
    "test_loader = TDM_dataset.get_inference_data(test_df, batch_size=16, shuffle=False) # this shuffle should be false to preserve the order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7891d91-b5e2-4696-8ad3-fb0de3a3ef32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = iter(test_loader)\n",
    "# sample.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dab4f06-470a-4fa3-92d6-3852f579b6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def predict_TDM_from_pdf(model, tokenizer, sentence):\n",
    "#     model.eval()\n",
    "#     tokens = tokenizer.tokenize(sentence)\n",
    "#     tokens = tokens[:max_input_length-2]\n",
    "#     indexed = [init_token_idx] + tokenizer.convert_tokens_to_ids(tokens) + [eos_token_idx]\n",
    "#     tensor = torch.LongTensor(indexed).to(device)\n",
    "#     tensor = tensor.unsqueeze(0)\n",
    "#     prediction = torch.sigmoid(model(tensor))\n",
    "#     return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "762f6352-d446-481b-bcb0-b4b2971329d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_TDM_from_pdf(model, tokenizer, iterator):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in tqdm(enumerate(iterator), total=len(iterator)):\n",
    "            pair_token_ids = pair_token_ids.to(device)\n",
    "            mask_ids = mask_ids.to(device)\n",
    "            seg_ids = seg_ids.to(device)\n",
    "            labels = y.to(device)\n",
    "\n",
    "            loss, prediction = model(pair_token_ids, \n",
    "                                 token_type_ids=seg_ids, \n",
    "                                 attention_mask=mask_ids, \n",
    "                                 labels=labels).values()\n",
    "\n",
    "            prediction_scalled = torch.sigmoid(prediction)\n",
    "            \n",
    "            with open(\"test_results.tsv\", \"a+\", encoding=\"utf-8\") as text_file:\n",
    "                for true, false in prediction_scalled.cpu():\n",
    "                    text_file.write(str(true.item())+\"\\t\"+str(false.item())+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4c6f79b9-1a81-4f46-9e61-3481124bc3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 166/166 [00:31<00:00,  5.35it/s]\n"
     ]
    }
   ],
   "source": [
    "predict_TDM_from_pdf(model, tokenizer, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b7940ca5-e2ea-49ef-bf52-33381d919850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "def get_top_n_prediction_label(path_to_test_file, path_to_prediction_file, n = 5):\n",
    "    \"\"\"\n",
    "    This function return the label with the highest proba\n",
    "    \"\"\"\n",
    "    top5 = deque()\n",
    "    with open(f\"{path_to_test_file}\") as f:\n",
    "        txt_test_files = f.read().splitlines()\n",
    "    with open(f\"{path_to_prediction_file}\") as f:\n",
    "        txt_prediction_files = f.read().splitlines()\n",
    "    \n",
    "    highest = 0\n",
    "    for example, prediction in zip(txt_test_files, txt_prediction_files):\n",
    "        true_prob, false_prob = prediction.split(\"\\t\")\n",
    "        true_prob, false_prob = float(true_prob), float(false_prob)\n",
    "        if true_prob > false_prob:\n",
    "            label = example.split(\"\\t\")[2]\n",
    "            highest = true_prob\n",
    "            top5.append((label, true_prob))\n",
    "    return deque(sorted(top5, key=lambda x: x[1] if x else x, reverse=False), n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1bca4393-e4a3-48d7-bec5-b798421c4e7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([('Image Clustering; Extended Yale-B; Accuracy', 0.8984434008598328)])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n_prediction_label(\n",
    "    path_to_test_file=\"../data/paperwithcode/new/jar/10Neg20unk/testOutput.tsv\",\n",
    "    path_to_prediction_file=\"test_results.tsv\", \n",
    "    n = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e575f45a-904d-48b0-ba9d-a9df520e8799",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
