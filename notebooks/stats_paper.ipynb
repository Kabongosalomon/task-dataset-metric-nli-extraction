{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats in Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ipdb, os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IBM_train_csv = \"~/Research/task-dataset-metric-nli-extraction/data/ibm/exp/few-shot-setup/NLP-TDMS/paperVersion/train.tsv\"\n",
    "IBM_test_csv = \"~/Research/task-dataset-metric-nli-extraction/data/ibm/exp/few-shot-setup/NLP-TDMS/paperVersion/test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_IBM = pd.read_csv(IBM_train_csv, \n",
    "                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])\n",
    "# train_IBM['label'] = train_IBM.label.apply(lambda x: \"true\" if x else \"false\")\n",
    "\n",
    "test_IBM = pd.read_csv(IBM_test_csv, \n",
    "                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(path_to_df): \n",
    "    \n",
    "    unique_labels = path_to_df[(path_to_df.label == True)].TDM.tolist()\n",
    "    \n",
    "    TDM = set()\n",
    "    Uniq_task = set()\n",
    "    Uniq_dataset = set()\n",
    "    Uniq_metric = set()\n",
    "    unknown_count = 0\n",
    "    avg_tdm_per_paper = defaultdict(lambda : 0)\n",
    "    \n",
    "    TDM_count = defaultdict(lambda : 0)\n",
    "    \n",
    "    for contrib in unique_labels:\n",
    "        split = contrib.split(';')\n",
    "        \n",
    "        if(len(split) == 1):\n",
    "            \n",
    "            unknown_count += 1 \n",
    "        else:\n",
    "            if len(split) !=3:\n",
    "#                 ipdb.set_trace()\n",
    "                task, dataset, metric, _ = split\n",
    "                \n",
    "            else:\n",
    "                task, dataset, metric = split\n",
    "            \n",
    "            t, d, m = task.strip(), dataset.strip(), metric.strip()\n",
    "            TDM.add(f\"{t}#{d}#{m}\")\n",
    "            \n",
    "            TDM_count[f\"{t}#{d}#{m}\"] += 1\n",
    "            \n",
    "            Uniq_task.add(t)\n",
    "            Uniq_dataset.add(d)\n",
    "            Uniq_metric.add(m)\n",
    "    \n",
    "    for paper in path_to_df[(path_to_df.label == True) & (path_to_df.TDM != 'unknown') ].title.tolist():\n",
    "        avg_tdm_per_paper[paper] += 1\n",
    "    \n",
    "    print(f\"Number of papers: {len(set(path_to_df[(path_to_df.label == True)].title.tolist()))}\")\n",
    "    print(f\"Unknown count: {unknown_count}\")\n",
    "    print(f\"Total leaderboards: {len(path_to_df[(path_to_df.label == True) & (path_to_df.TDM != 'unknown')].title.tolist())}\")\n",
    "    print(f\"Avg leaderboard per paper: {round(np.mean(list(avg_tdm_per_paper.values())), 2)}\")\n",
    "    print(f\"Distinc leaderboard: {len(TDM)}\")\n",
    "    print(f\"Distinct taks: {len(Uniq_task)}\")\n",
    "    print(f\"Distinc datasets: {len(Uniq_dataset)}\")\n",
    "    print(f\"Distinc metrics: {len(Uniq_metric)}\")\n",
    "    print(f\"Max leaderboard per paper: {round(np.max(list(avg_tdm_per_paper.values())), 2)}\")\n",
    "    print(f\"Min leaderboard per paper: {round(np.min(list(avg_tdm_per_paper.values())), 2)}\")\n",
    "    \n",
    "    oder_TDM_count = sorted(TDM_count.items(), key=lambda item: item[1])\n",
    "    print(f\"Least frequent leaderboard :{oder_TDM_count[:3]}\")\n",
    "    print(f\"Most frequent leaderboard :{oder_TDM_count[-3:]}\")\n",
    "    \n",
    "    return oder_TDM_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>D16-1036.pdf</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Multi-view Response Selection for Human-Comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>D16-1036.pdf</td>\n",
       "      <td>question answering; SQuAD; F1</td>\n",
       "      <td>Multi-view Response Selection for Human-Comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>D16-1036.pdf</td>\n",
       "      <td>relation prediction; FB15K-237; H@1</td>\n",
       "      <td>Multi-view Response Selection for Human-Comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>D16-1036.pdf</td>\n",
       "      <td>word sense disambiguation; SemEval 2013; F1</td>\n",
       "      <td>Multi-view Response Selection for Human-Comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>D16-1036.pdf</td>\n",
       "      <td>language modeling; 1B Words / Google Billion W...</td>\n",
       "      <td>Multi-view Response Selection for Human-Comput...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label         title                                                TDM  \\\n",
       "0   True  D16-1036.pdf                                            unknown   \n",
       "1  False  D16-1036.pdf                      question answering; SQuAD; F1   \n",
       "2  False  D16-1036.pdf                relation prediction; FB15K-237; H@1   \n",
       "3  False  D16-1036.pdf        word sense disambiguation; SemEval 2013; F1   \n",
       "4  False  D16-1036.pdf  language modeling; 1B Words / Google Billion W...   \n",
       "\n",
       "                                             Context  \n",
       "0  Multi-view Response Selection for Human-Comput...  \n",
       "1  Multi-view Response Selection for Human-Comput...  \n",
       "2  Multi-view Response Selection for Human-Comput...  \n",
       "3  Multi-view Response Selection for Human-Comput...  \n",
       "4  Multi-view Response Selection for Human-Comput...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_IBM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_IBM.title.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 170\n",
      "Unknown count: 46\n",
      "Total leaderboards: 327\n",
      "Avg leaderboard per paper: 2.64\n",
      "Distinc leaderboard: 78\n",
      "Distinct taks: 18\n",
      "Distinc datasets: 44\n",
      "Distinc metrics: 31\n",
      "Max leaderboard per paper: 10\n",
      "Min leaderboard per paper: 1\n"
     ]
    }
   ],
   "source": [
    "avg_tdm_per_paper = get_stats(train_IBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>1803.11175.pdf</td>\n",
       "      <td>sentiment analysis; SUBJ; Accuracy</td>\n",
       "      <td>Universal Sentence Encoder We present models f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>1803.11175.pdf</td>\n",
       "      <td>text classification; TREC; Error</td>\n",
       "      <td>Universal Sentence Encoder We present models f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>1803.11175.pdf</td>\n",
       "      <td>question answering; SQuAD; F1</td>\n",
       "      <td>Universal Sentence Encoder We present models f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>1803.11175.pdf</td>\n",
       "      <td>relation prediction; FB15K-237; H@1</td>\n",
       "      <td>Universal Sentence Encoder We present models f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>1803.11175.pdf</td>\n",
       "      <td>word sense disambiguation; SemEval 2013; F1</td>\n",
       "      <td>Universal Sentence Encoder We present models f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label           title                                          TDM  \\\n",
       "0   True  1803.11175.pdf           sentiment analysis; SUBJ; Accuracy   \n",
       "1   True  1803.11175.pdf             text classification; TREC; Error   \n",
       "2  False  1803.11175.pdf                question answering; SQuAD; F1   \n",
       "3  False  1803.11175.pdf          relation prediction; FB15K-237; H@1   \n",
       "4  False  1803.11175.pdf  word sense disambiguation; SemEval 2013; F1   \n",
       "\n",
       "                                             Context  \n",
       "0  Universal Sentence Encoder We present models f...  \n",
       "1  Universal Sentence Encoder We present models f...  \n",
       "2  Universal Sentence Encoder We present models f...  \n",
       "3  Universal Sentence Encoder We present models f...  \n",
       "4  Universal Sentence Encoder We present models f...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_IBM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 167\n",
      "Unknown count: 45\n",
      "Total leaderboards: 294\n",
      "Avg leaderboard per paper: 2.41\n",
      "Distinc leaderboard: 78\n",
      "Distinct taks: 18\n",
      "Distinc datasets: 44\n",
      "Distinc metrics: 31\n",
      "Max leaderboard per paper: 7\n",
      "Min leaderboard per paper: 1\n"
     ]
    }
   ],
   "source": [
    "metric = get_stats(test_IBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Make sure that all leaderboard in test are present in train \n",
    "count = []\n",
    "for paper in test_IBM.TDM.to_list():\n",
    "    if paper not in train_IBM.TDM.to_list():\n",
    "        count.append(paper)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "count = []\n",
    "for paper in train_IBM.TDM.to_list():\n",
    "    if paper not in test_IBM.TDM.to_list():\n",
    "        print(paper)\n",
    "        count.append(paper)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New_train_csv = \"~/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_800/twofoldwithunk/fold1/train.tsv\"\n",
    "# New_test_csv = \"~/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_800/twofoldwithunk/fold1/dev.tsv\"\n",
    "\n",
    "New_train_csv = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_10000/10Neg10000unk/twofoldwithunk/fold2/train.tsv\"\n",
    "New_test_csv = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_10000/10Neg10000unk/twofoldwithunk/fold2/dev.tsv\"\n",
    "\n",
    "# New_train_csv = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_5000/10Neg5000unk/twofoldwithunk/fold2/train.tsv\"\n",
    "# New_test_csv = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_5000/10Neg5000unk/twofoldwithunk/fold2/dev.tsv\"\n",
    "\n",
    "# New_train_csv = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_1000/10Neg1000unk/twofoldwithunk/fold2/train.tsv\"\n",
    "# New_test_csv = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_1000/10Neg1000unk/twofoldwithunk/fold2/dev.tsv\"\n",
    "\n",
    "# New_train_csv = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_500/10Neg500unk/twofoldwithunk/fold1/train.tsv\"\n",
    "# New_test_csv = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_500/10Neg500unk/twofoldwithunk/fold1/dev.tsv\"\n",
    "\n",
    "# New_train_csv = IBM_train_csv\n",
    "# New_test_csv = IBM_test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_New = pd.read_csv(New_train_csv, \n",
    "                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])\n",
    "# train_IBM['label'] = train_IBM.label.apply(lambda x: \"true\" if x else \"false\")\n",
    "\n",
    "test_New = pd.read_csv(New_test_csv, \n",
    "                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50207"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_New.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50207"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_New)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 3753\n",
      "Unknown count: 920\n",
      "Total leaderboards: 11757\n",
      "Avg leaderboard per paper: 4.15\n",
      "Distinc leaderboard: 1820\n",
      "Distinct taks: 291\n",
      "Distinc datasets: 912\n",
      "Distinc metrics: 553\n",
      "Max leaderboard per paper: 58\n",
      "Min leaderboard per paper: 1\n",
      "Least frequent leaderboard :[('Word Sense Disambiguation#WiC-TSV#Task 1 Accuracy: all', 1), ('Word Sense Disambiguation#WiC-TSV#Task 1 Accuracy: domain specific', 1), ('Word Sense Disambiguation#WiC-TSV#Task 1 Accuracy: general purpose', 1)]\n",
      "Most frequent leaderboard :[('Image Classification#CIFAR-10#Percentage correct', 51), ('Object Detection#COCO test-dev#box AP', 57), ('Image Classification#ImageNet#Top 1 Accuracy', 93)]\n"
     ]
    }
   ],
   "source": [
    "TDM_count = get_stats(train_New)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TDM_count[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "======\n",
      "Number of papers: 3753\n",
      "Avg Unknown count: 922\n",
      "Avg Total leaderboards: 11724\n",
      "Avg leaderboard per paper: 4.1\n",
      "Avg Distinc leaderboard: 1806\n",
      "Avg Distinct taks: 288\n",
      "Avg Distinc datasets: 908\n",
      "Avg Distinc metrics: 550\n"
     ]
    }
   ],
   "source": [
    "print(\"Train\")\n",
    "print(\"======\")\n",
    "print(f\"Number of papers: {round((3753 + 3753)/2)}\")\n",
    "print(f\"Avg Unknown count: {round((923 + 920)/2)}\")\n",
    "print(f\"Avg Total leaderboards: {round((11690 + 11757)/2)}\")\n",
    "print(f\"Avg leaderboard per paper: {round((4.13 + 4.15)/2, 1)}\")\n",
    "print(f\"Avg Distinc leaderboard: {round((1791 + 1820)/2)}\")\n",
    "print(f\"Avg Distinct taks: {round((286 + 291)/2)}\")\n",
    "print(f\"Avg Distinc datasets: {round((905 + 912)/2)}\")\n",
    "print(f\"Avg Distinc metrics: {round((547 + 553)/2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 1608\n",
      "Unknown count: 381\n",
      "Total leaderboards: 5027\n",
      "Avg leaderboard per paper: 4.1\n",
      "Distinc leaderboard: 1541\n",
      "Distinct taks: 250\n",
      "Distinc datasets: 790\n",
      "Distinc metrics: 466\n",
      "Max leaderboard per paper: 58\n",
      "Min leaderboard per paper: 1\n",
      "Least frequent leaderboard :[('Visual Object Tracking#GOT-10k#Average Overlap', 1), ('Visual Object Tracking#GOT-10k#Success Rate 0.5', 1), ('Image Clustering#Extended Yale-B#Accuracy', 1)]\n",
      "Most frequent leaderboard :[('Image Classification#CIFAR-100#Percentage correct', 30), ('Image Classification#CIFAR-10#Percentage correct', 40), ('Image Classification#ImageNet#Top 1 Accuracy', 45)]\n"
     ]
    }
   ],
   "source": [
    "TDM_count_test = get_stats(test_New)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "======\n",
      "Number of papers: 1608\n",
      "Avg Unknown count: 380\n",
      "Avg Total leaderboards: 5060\n",
      "Avg leaderboard per paper: 4.1\n",
      "Avg Distinc leaderboard: 1548\n",
      "Avg Distinct taks: 252\n",
      "Avg Distinc datasets: 798\n",
      "Avg Distinc metrics: 469\n"
     ]
    }
   ],
   "source": [
    "print(\"Test\")\n",
    "print(\"======\")\n",
    "print(f\"Number of papers: {round((1608 + 1608)/2)}\")\n",
    "print(f\"Avg Unknown count: {round((378 + 381)/2)}\")\n",
    "print(f\"Avg Total leaderboards: {round((5094 + 5027)/2)}\")\n",
    "print(f\"Avg leaderboard per paper: {round((4.14 + 4.1)/2, 1)}\")\n",
    "print(f\"Avg Distinc leaderboard: {round((1556 + 1541)/2)}\")\n",
    "print(f\"Avg Distinct taks: {round((254 + 250)/2)}\")\n",
    "print(f\"Avg Distinc datasets: {round((806 + 790)/2)}\")\n",
    "print(f\"Avg Distinc metrics: {round((472 + 466)/2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "count = []\n",
    "for tdm in test_New.TDM.to_list():\n",
    "    if tdm not in train_New.TDM.to_list():\n",
    "        count.append(tdm)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = []\n",
    "for tdm in train_New.TDM.to_list():\n",
    "    if tdm not in test_New.TDM.to_list():\n",
    "        count.append(tdm)\n",
    "print(len(count))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
