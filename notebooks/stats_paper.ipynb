{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats in Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ipdb, os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IBM_train_csv = \"~/Research/task-dataset-metric-nli-extraction/data/ibm/exp/few-shot-setup/NLP-TDMS/paperVersion/train.tsv\"\n",
    "IBM_test_csv = \"~/Research/task-dataset-metric-nli-extraction/data/ibm/exp/few-shot-setup/NLP-TDMS/paperVersion/test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_IBM = pd.read_csv(IBM_train_csv, \n",
    "                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])\n",
    "# train_IBM['label'] = train_IBM.label.apply(lambda x: \"true\" if x else \"false\")\n",
    "\n",
    "test_IBM = pd.read_csv(IBM_test_csv, \n",
    "                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(path_to_train_df, path_to_test_df): \n",
    "    \n",
    "    TDM_over_all = set()\n",
    "    unique_labels = path_to_train_df[(path_to_train_df.label == True)].TDM.tolist()\n",
    "    \n",
    "    TDM = set()\n",
    "    Uniq_task = set()\n",
    "    Uniq_dataset = set()\n",
    "    Uniq_metric = set()\n",
    "    unknown_count = 0\n",
    "    avg_tdm_per_paper = defaultdict(lambda : 0)\n",
    "    \n",
    "    TDM_count = defaultdict(lambda : 0)\n",
    "    \n",
    "    for contrib in unique_labels:\n",
    "        split = contrib.split(';')\n",
    "        \n",
    "        if(len(split) == 1):\n",
    "            \n",
    "            unknown_count += 1 \n",
    "        else:\n",
    "            if len(split) !=3:\n",
    "#                 ipdb.set_trace()\n",
    "                task, dataset, metric, _ = split\n",
    "                \n",
    "            else:\n",
    "                task, dataset, metric = split\n",
    "            \n",
    "            t, d, m = task.strip(), dataset.strip(), metric.strip()\n",
    "            TDM.add(f\"{t}#{d}#{m}\")\n",
    "            TDM_over_all.add(f\"{t}#{d}#{m}\")\n",
    "            \n",
    "            TDM_count[f\"{t}#{d}#{m}\"] += 1\n",
    "            \n",
    "            Uniq_task.add(t)\n",
    "            Uniq_dataset.add(d)\n",
    "            Uniq_metric.add(m)\n",
    "    \n",
    "    for paper in path_to_train_df[(path_to_train_df.label == True) & (path_to_train_df.TDM != 'unknown') ].title.tolist():\n",
    "        avg_tdm_per_paper[paper] += 1\n",
    "    \n",
    "    print(\"Train: \")\n",
    "    print(f\"Number of papers: {len(set(path_to_train_df[(path_to_train_df.label == True)].title.tolist()))}\")\n",
    "    print(f\"Unknown count: {unknown_count}\")\n",
    "    print(f\"Total leaderboards: {len(path_to_train_df[(path_to_train_df.label == True) & (path_to_train_df.TDM != 'unknown')].title.tolist())}\")\n",
    "    print(f\"Avg leaderboard per paper: {round(np.mean(list(avg_tdm_per_paper.values())), 2)}\")\n",
    "    print(f\"Distinc leaderboard: {len(TDM)}\")\n",
    "    print(f\"Distinct taks: {len(Uniq_task)}\")\n",
    "    print(f\"Distinc datasets: {len(Uniq_dataset)}\")\n",
    "    print(f\"Distinc metrics: {len(Uniq_metric)}\")\n",
    "    print(f\"Max leaderboard per paper: {round(np.max(list(avg_tdm_per_paper.values())), 2)}\")\n",
    "    print(f\"Min leaderboard per paper: {round(np.min(list(avg_tdm_per_paper.values())), 2)}\")\n",
    "    \n",
    "    oder_TDM_count1 = sorted(TDM_count.items(), key=lambda item: item[1])\n",
    "    print(f\"Least frequent leaderboard :{oder_TDM_count1[:3]}\")\n",
    "    print(f\"Most frequent leaderboard :{oder_TDM_count1[-3:]}\")\n",
    "    \n",
    "    print(\" \\n==========================================================================\\n \")\n",
    "    unique_labels = path_to_test_df[(path_to_test_df.label == True)].TDM.tolist()\n",
    "    \n",
    "    TDM = set()\n",
    "    Uniq_task = set()\n",
    "    Uniq_dataset = set()\n",
    "    Uniq_metric = set()\n",
    "    unknown_count = 0\n",
    "    avg_tdm_per_paper = defaultdict(lambda : 0)\n",
    "    \n",
    "    TDM_count = defaultdict(lambda : 0)\n",
    "    \n",
    "    for contrib in unique_labels:\n",
    "        split = contrib.split(';')\n",
    "        \n",
    "        if(len(split) == 1):\n",
    "            \n",
    "            unknown_count += 1 \n",
    "        else:\n",
    "            if len(split) !=3:\n",
    "#                 ipdb.set_trace()\n",
    "                task, dataset, metric, _ = split\n",
    "                \n",
    "            else:\n",
    "                task, dataset, metric = split\n",
    "            \n",
    "            t, d, m = task.strip(), dataset.strip(), metric.strip()\n",
    "            TDM.add(f\"{t}#{d}#{m}\")\n",
    "            TDM_over_all.add(f\"{t}#{d}#{m}\")\n",
    "            \n",
    "            TDM_count[f\"{t}#{d}#{m}\"] += 1\n",
    "            \n",
    "            Uniq_task.add(t)\n",
    "            Uniq_dataset.add(d)\n",
    "            Uniq_metric.add(m)\n",
    "    \n",
    "    for paper in path_to_test_df[(path_to_test_df.label == True) & (path_to_test_df.TDM != 'unknown') ].title.tolist():\n",
    "        avg_tdm_per_paper[paper] += 1\n",
    "    print(\"Test: \")\n",
    "    print(f\"Number of papers: {len(set(path_to_test_df[(path_to_test_df.label == True)].title.tolist()))}\")\n",
    "    print(f\"Unknown count: {unknown_count}\")\n",
    "    print(f\"Total leaderboards: {len(path_to_test_df[(path_to_test_df.label == True) & (path_to_test_df.TDM != 'unknown')].title.tolist())}\")\n",
    "    print(f\"Avg leaderboard per paper: {round(np.mean(list(avg_tdm_per_paper.values())), 2)}\")\n",
    "    print(f\"Distinc leaderboard: {len(TDM)}\")\n",
    "    print(f\"Distinct taks: {len(Uniq_task)}\")\n",
    "    print(f\"Distinc datasets: {len(Uniq_dataset)}\")\n",
    "    print(f\"Distinc metrics: {len(Uniq_metric)}\")\n",
    "    print(f\"Max leaderboard per paper: {round(np.max(list(avg_tdm_per_paper.values())), 2)}\")\n",
    "    print(f\"Min leaderboard per paper: {round(np.min(list(avg_tdm_per_paper.values())), 2)}\")\n",
    "    \n",
    "    oder_TDM_count2 = sorted(TDM_count.items(), key=lambda item: item[1])\n",
    "    print(f\"Least frequent leaderboard :{oder_TDM_count2[:3]}\")\n",
    "    print(f\"Most frequent leaderboard :{oder_TDM_count2[-3:]}\")\n",
    "    \n",
    "    print(\" \\n==========================================================================\\n \")\n",
    "    \n",
    "    print(f\"Unique Triples over all: {len(TDM_over_all)}\")\n",
    "    \n",
    "    return oder_TDM_count1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats_old(path_to_df): \n",
    "    \n",
    "    unique_labels = path_to_df[(path_to_df.label == True)].TDM.tolist()\n",
    "    \n",
    "    TDM = set()\n",
    "    Uniq_task = set()\n",
    "    Uniq_dataset = set()\n",
    "    Uniq_metric = set()\n",
    "    unknown_count = 0\n",
    "    avg_tdm_per_paper = defaultdict(lambda : 0)\n",
    "    \n",
    "    for contrib in unique_labels:\n",
    "        split = contrib.split(';')\n",
    "        \n",
    "        if(len(split) == 1):\n",
    "            \n",
    "            unknown_count += 1 \n",
    "        else:\n",
    "            if len(split) !=3:\n",
    "#                 ipdb.set_trace()\n",
    "                task, dataset, metric, _ = split\n",
    "                \n",
    "            else:\n",
    "                task, dataset, metric = split\n",
    "            \n",
    "            t, d, m = task.strip(), dataset.strip(), metric.strip()\n",
    "            TDM.add(f\"{t}#{d}#{m}\")\n",
    "            \n",
    "            Uniq_task.add(t)\n",
    "            Uniq_dataset.add(d)\n",
    "            Uniq_metric.add(m)\n",
    "    \n",
    "    for paper in path_to_df[(path_to_df.label == True) & (path_to_df.TDM != 'unknown') ].title.tolist():\n",
    "        avg_tdm_per_paper[paper] += 1\n",
    "    \n",
    "    print(f\"Number of papers: {len(set(path_to_df[(path_to_df.label == True)].title.tolist()))}\")\n",
    "    print(f\"Unknown count: {unknown_count}\")\n",
    "    print(f\"Total leaderboards: {len(path_to_df[(path_to_df.label == True) & (path_to_df.TDM != 'unknown')].title.tolist())}\")\n",
    "    print(f\"Avg leaderboard per paper: {round(np.mean(list(avg_tdm_per_paper.values())), 2)}\")\n",
    "    print(f\"Distinc leaderboard: {len(TDM)}\")\n",
    "    print(f\"Distinct taks: {len(Uniq_task)}\")\n",
    "    print(f\"Distinc datasets: {len(Uniq_dataset)}\")\n",
    "    print(f\"Distinc metrics: {len(Uniq_metric)}\")\n",
    "    print(f\"Max leaderboard per paper: {round(np.max(list(avg_tdm_per_paper.values())), 2)}\")\n",
    "    print(f\"Min leaderboard per paper: {round(np.min(list(avg_tdm_per_paper.values())), 2)}\")\n",
    "    \n",
    "    \n",
    "    return avg_tdm_per_paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>D16-1036.pdf</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Multi-view Response Selection for Human-Comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>D16-1036.pdf</td>\n",
       "      <td>question answering; SQuAD; F1</td>\n",
       "      <td>Multi-view Response Selection for Human-Comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>D16-1036.pdf</td>\n",
       "      <td>relation prediction; FB15K-237; H@1</td>\n",
       "      <td>Multi-view Response Selection for Human-Comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>D16-1036.pdf</td>\n",
       "      <td>word sense disambiguation; SemEval 2013; F1</td>\n",
       "      <td>Multi-view Response Selection for Human-Comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>D16-1036.pdf</td>\n",
       "      <td>language modeling; 1B Words / Google Billion W...</td>\n",
       "      <td>Multi-view Response Selection for Human-Comput...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label         title                                                TDM  \\\n",
       "0   True  D16-1036.pdf                                            unknown   \n",
       "1  False  D16-1036.pdf                      question answering; SQuAD; F1   \n",
       "2  False  D16-1036.pdf                relation prediction; FB15K-237; H@1   \n",
       "3  False  D16-1036.pdf        word sense disambiguation; SemEval 2013; F1   \n",
       "4  False  D16-1036.pdf  language modeling; 1B Words / Google Billion W...   \n",
       "\n",
       "                                             Context  \n",
       "0  Multi-view Response Selection for Human-Comput...  \n",
       "1  Multi-view Response Selection for Human-Comput...  \n",
       "2  Multi-view Response Selection for Human-Comput...  \n",
       "3  Multi-view Response Selection for Human-Comput...  \n",
       "4  Multi-view Response Selection for Human-Comput...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_IBM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_IBM.title.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_tdm_per_paper = get_stats(train_IBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_tdm_per_paper = get_stats(test_IBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: \n",
      "Number of papers: 170\n",
      "Unknown count: 46\n",
      "Total leaderboards: 327\n",
      "Avg leaderboard per paper: 2.64\n",
      "Distinc leaderboard: 78\n",
      "Distinct taks: 18\n",
      "Distinc datasets: 44\n",
      "Distinc metrics: 31\n",
      "Max leaderboard per paper: 10\n",
      "Min leaderboard per paper: 1\n",
      "Least frequent leaderboard :[('language modeling#Penn Treebank#Bit per Character (BPC)', 1), ('chunking#Penn Treebank#F1', 2), ('word segmentation#MSR#F1', 2)]\n",
      "Most frequent leaderboard :[('dependency parsing#Penn Treebank#UAS', 8), ('question answering#SQuAD#F1', 11), ('question answering#SQuAD#EM', 11)]\n",
      " \n",
      "==========================================================================\n",
      " \n",
      "Test: \n",
      "Number of papers: 167\n",
      "Unknown count: 45\n",
      "Total leaderboards: 294\n",
      "Avg leaderboard per paper: 2.41\n",
      "Distinc leaderboard: 78\n",
      "Distinct taks: 18\n",
      "Distinc datasets: 44\n",
      "Distinc metrics: 31\n",
      "Max leaderboard per paper: 7\n",
      "Min leaderboard per paper: 1\n",
      "Least frequent leaderboard :[('sentiment analysis#SUBJ#Accuracy', 2), ('text classification#DBpedia#Error', 2), ('text classification#AG News#Error', 2)]\n",
      "Most frequent leaderboard :[('dependency parsing#Penn Treebank#UAS', 8), ('question answering#SQuAD#F1', 10), ('question answering#SQuAD#EM', 10)]\n",
      " \n",
      "==========================================================================\n",
      " \n",
      "Unique Triples over all: 78\n"
     ]
    }
   ],
   "source": [
    "avg_tdm_per_paper = get_stats(train_IBM, test_IBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Make sure that all leaderboard in test are present in train \n",
    "count = []\n",
    "for paper in test_IBM.TDM.to_list():\n",
    "    if paper not in train_IBM.TDM.to_list():\n",
    "        count.append(paper)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "count = []\n",
    "for paper in train_IBM.TDM.to_list():\n",
    "    if paper not in test_IBM.TDM.to_list():\n",
    "        print(paper)\n",
    "        count.append(paper)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New_train_csv = \"~/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_800/twofoldwithunk/fold1/train.tsv\"\n",
    "# New_test_csv = \"~/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_800/twofoldwithunk/fold1/dev.tsv\"\n",
    "\n",
    "# New_train_csv = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_10000/10Neg10000unk/twofoldwithunk/fold2/train.tsv\"\n",
    "# New_test_csv = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_10000/10Neg10000unk/twofoldwithunk/fold2/dev.tsv\"\n",
    "\n",
    "# New_train_csv = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_5000/10Neg5000unk/twofoldwithunk/fold2/train.tsv\"\n",
    "# New_test_csv = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_5000/10Neg5000unk/twofoldwithunk/fold2/dev.tsv\"\n",
    "\n",
    "# New_train_csv = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_1000/10Neg1000unk/twofoldwithunk/fold2/train.tsv\"\n",
    "# New_test_csv = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_1000/10Neg1000unk/twofoldwithunk/fold2/dev.tsv\"\n",
    "\n",
    "# New_train_csv = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_500/10Neg500unk/twofoldwithunk/fold1/train.tsv\"\n",
    "# New_test_csv = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_500/10Neg500unk/twofoldwithunk/fold1/dev.tsv\"\n",
    "\n",
    "New_train_csv = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_latex_text/PWC_latex_5_10_10000/twofoldwithunk/fold1/train.tsv\"\n",
    "New_test_csv = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_latex_text/PWC_latex_5_10_10000/twofoldwithunk/fold1/dev.tsv\"\n",
    "# New_train_csv = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_latex_text/PWC_latex_5_10_10000/twofoldwithunk/fold2/train.tsv\"\n",
    "# New_test_csv = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/pwc_latex_text/PWC_latex_5_10_10000/twofoldwithunk/fold2/dev.tsv\"\n",
    "\n",
    "\n",
    "# New_train_csv = IBM_train_csv\n",
    "# New_test_csv = IBM_test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_New = pd.read_csv(New_train_csv, \n",
    "                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])\n",
    "# train_IBM['label'] = train_IBM.label.apply(lambda x: \"true\" if x else \"false\")\n",
    "\n",
    "test_New = pd.read_csv(New_test_csv, \n",
    "                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40538"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_New.drop_duplicates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41733"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_New)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4208"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2946+1262"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: \n",
      "Number of papers: 2951\n",
      "Unknown count: 2370\n",
      "Total leaderboards: 9853\n",
      "Avg leaderboard per paper: 4.33\n",
      "Distinc leaderboard: 1673\n",
      "Distinct taks: 262\n",
      "Distinc datasets: 853\n",
      "Distinc metrics: 530\n",
      "Max leaderboard per paper: 58\n",
      "Min leaderboard per paper: 1\n",
      "Least frequent leaderboard :[('Named Entity Recognition#CoNLL++#F1', 1), ('Graph Classification#HIV-fMRI-77#Accuracy', 1), ('Graph Classification#HIV-fMRI-77#F1', 1)]\n",
      "Most frequent leaderboard :[('Object Detection#COCO test-dev#box AP', 55), ('Image Classification#CIFAR-10#Percentage correct', 55), ('Image Classification#ImageNet#Top 1 Accuracy', 73)]\n",
      " \n",
      "==========================================================================\n",
      " \n",
      "Test: \n",
      "Number of papers: 1258\n",
      "Unknown count: 981\n",
      "Total leaderboards: 3856\n",
      "Avg leaderboard per paper: 4.08\n",
      "Distinc leaderboard: 1363\n",
      "Distinct taks: 228\n",
      "Distinc datasets: 712\n",
      "Distinc metrics: 434\n",
      "Max leaderboard per paper: 57\n",
      "Min leaderboard per paper: 1\n",
      "Least frequent leaderboard :[('Domain Adaptation#SVNH-to-MNIST#Accuracy', 1), ('Question Answering#SQuAD1.1 dev#F1', 1), ('Self-Supervised Action Recognition#HMDB51 (finetuned)#Top-1 Accuracy', 1)]\n",
      "Most frequent leaderboard :[('Image Classification#ImageNet#Top 5 Accuracy', 22), ('Image Classification#CIFAR-10#Percentage correct', 23), ('Image Classification#ImageNet#Top 1 Accuracy', 33)]\n",
      " \n",
      "==========================================================================\n",
      " \n",
      "Unique Triples over all: 1724\n"
     ]
    }
   ],
   "source": [
    "TDM_count = get_stats(train_New, test_New)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TDM_count[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "======\n",
      "Number of papers: 2946\n",
      "Avg Unknown count: 2359\n",
      "Avg Total leaderboards: 9614\n",
      "Avg leaderboard per paper: 4.3\n",
      "Avg Distinc leaderboard: 1668\n",
      "Avg Distinct taks: 262\n",
      "Avg Distinc datasets: 853\n",
      "Avg Distinc metrics: 528\n",
      "Unique Triples over all: 1724\n"
     ]
    }
   ],
   "source": [
    "print(\"Train\")\n",
    "print(\"======\")\n",
    "print(f\"Number of papers: {round((2951 + 2942)/2)}\")\n",
    "print(f\"Avg Unknown count: {round((2370 + 2348)/2)}\")\n",
    "print(f\"Avg Total leaderboards: {round((9853 + 9374)/2)}\")\n",
    "print(f\"Avg leaderboard per paper: {round((4.33 + 4.2)/2, 1)}\")\n",
    "print(f\"Avg Distinc leaderboard: {round((1673 + 1663)/2)}\")\n",
    "print(f\"Avg Distinct taks: {round((262 + 263)/2)}\")\n",
    "print(f\"Avg Distinc datasets: {round((853 + 853)/2)}\")\n",
    "print(f\"Avg Distinc metrics: {round((530 + 525)/2)}\")\n",
    "print(f\"Unique Triples over all: {round((1724 + 1724)/2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "======\n",
      "Number of papers: 1262\n",
      "Avg Unknown count: 992\n",
      "Avg Total leaderboards: 4096\n",
      "Avg leaderboard per paper: 4.2\n",
      "Avg Distinc leaderboard: 1377\n",
      "Avg Distinct taks: 228\n",
      "Avg Distinc datasets: 714\n",
      "Avg Distinc metrics: 434\n"
     ]
    }
   ],
   "source": [
    "print(\"Test\")\n",
    "print(\"======\")\n",
    "print(f\"Number of papers: {round((1258 + 1267)/2)}\")\n",
    "print(f\"Avg Unknown count: {round((981 + 1003)/2)}\")\n",
    "print(f\"Avg Total leaderboards: {round((3856 + 4335)/2)}\")\n",
    "print(f\"Avg leaderboard per paper: {round((4.08 + 4.37)/2, 1)}\")\n",
    "print(f\"Avg Distinc leaderboard: {round((1363 + 1391)/2)}\")\n",
    "print(f\"Avg Distinct taks: {round((228 + 229)/2)}\")\n",
    "print(f\"Avg Distinc datasets: {round((712 + 717)/2)}\")\n",
    "print(f\"Avg Distinc metrics: {round((434 + 435)/2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 3753\n",
      "Unknown count: 920\n",
      "Total leaderboards: 11757\n",
      "Avg leaderboard per paper: 4.15\n",
      "Distinc leaderboard: 1820\n",
      "Distinct taks: 291\n",
      "Distinc datasets: 912\n",
      "Distinc metrics: 553\n",
      "Max leaderboard per paper: 58\n",
      "Min leaderboard per paper: 1\n"
     ]
    }
   ],
   "source": [
    "Unique Triples over all: 1850"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "======\n",
      "Number of papers: 3753\n",
      "Avg Unknown count: 922\n",
      "Avg Total leaderboards: 11724\n",
      "Avg leaderboard per paper: 4.1\n",
      "Avg Distinc leaderboard: 1806\n",
      "Avg Distinct taks: 288\n",
      "Avg Distinc datasets: 908\n",
      "Avg Distinc metrics: 550\n",
      "Unique Triples over all: 1850\n"
     ]
    }
   ],
   "source": [
    "# print(\"Train\")\n",
    "# print(\"======\")\n",
    "# print(f\"Number of papers: {round((3753 + 3753)/2)}\")\n",
    "# print(f\"Avg Unknown count: {round((923 + 920)/2)}\")\n",
    "# print(f\"Avg Total leaderboards: {round((11690 + 11757)/2)}\")\n",
    "# print(f\"Avg leaderboard per paper: {round((4.13 + 4.15)/2, 1)}\")\n",
    "# print(f\"Avg Distinc leaderboard: {round((1791 + 1820)/2)}\")\n",
    "# print(f\"Avg Distinct taks: {round((286 + 291)/2)}\")\n",
    "# print(f\"Avg Distinc datasets: {round((905 + 912)/2)}\")\n",
    "# print(f\"Avg Distinc metrics: {round((547 + 553)/2)}\")\n",
    "# print(f\"Unique Triples over all: {round((1850 + 1850)/2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "======\n",
      "Number of papers: 1608\n",
      "Avg Unknown count: 380\n",
      "Avg Total leaderboards: 5060\n",
      "Avg leaderboard per paper: 4.1\n",
      "Avg Distinc leaderboard: 1548\n",
      "Avg Distinct taks: 252\n",
      "Avg Distinc datasets: 798\n",
      "Avg Distinc metrics: 469\n"
     ]
    }
   ],
   "source": [
    "# print(\"Test\")\n",
    "# print(\"======\")\n",
    "# print(f\"Number of papers: {round((1608 + 1608)/2)}\")\n",
    "# print(f\"Avg Unknown count: {round((378 + 381)/2)}\")\n",
    "# print(f\"Avg Total leaderboards: {round((5094 + 5027)/2)}\")\n",
    "# print(f\"Avg leaderboard per paper: {round((4.14 + 4.1)/2, 1)}\")\n",
    "# print(f\"Avg Distinc leaderboard: {round((1556 + 1541)/2)}\")\n",
    "# print(f\"Avg Distinct taks: {round((254 + 250)/2)}\")\n",
    "# print(f\"Avg Distinc datasets: {round((806 + 790)/2)}\")\n",
    "# print(f\"Avg Distinc metrics: {round((472 + 466)/2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "count = []\n",
    "for tdm in test_New.TDM.to_list():\n",
    "    if tdm not in train_New.TDM.to_list():\n",
    "        count.append(tdm)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "count = []\n",
    "for tdm in train_New.TDM.to_list():\n",
    "    if tdm not in test_New.TDM.to_list():\n",
    "        count.append(tdm)\n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5363"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(\"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/pdf_IBM\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
