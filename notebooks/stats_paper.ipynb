{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Client `s2orc-doc2json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pdfs = os.listdir(\"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/pdf\") \n",
    "list_df_tei = os.listdir(\"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/pdf_tei_xml\") \n",
    "list_pdf_json = os.listdir(\"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/pdf_json\") \n",
    "list_pdf_txt = os.listdir(\"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/pdf_txt\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats in Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ipdb, os, re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "IBM_train_csv = \"~/Research/task-dataset-metric-nli-extraction/data/ibm/exp/few-shot-setup/NLP-TDMS/paperVersion/train.tsv\"\n",
    "IBM_test_csv = \"~/Research/task-dataset-metric-nli-extraction/data/ibm/exp/few-shot-setup/NLP-TDMS/paperVersion/test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_IBM = pd.read_csv(IBM_train_csv, \n",
    "                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])\n",
    "# train_IBM['label'] = train_IBM.label.apply(lambda x: \"true\" if x else \"false\")\n",
    "\n",
    "test_IBM = pd.read_csv(IBM_test_csv, \n",
    "                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(path_to_df): \n",
    "    \n",
    "    unique_labels = path_to_df[(path_to_df.label == True)].TDM.tolist()\n",
    "    \n",
    "    TDM = set()\n",
    "    Uniq_task = set()\n",
    "    Uniq_dataset = set()\n",
    "    Uniq_metric = set()\n",
    "    unknown_count = 0\n",
    "    avg_tdm_per_paper = defaultdict(lambda : 0)\n",
    "    \n",
    "    for contrib in unique_labels:\n",
    "        split = contrib.split(';')\n",
    "        \n",
    "        if(len(split) == 1):\n",
    "            \n",
    "            unknown_count += 1 \n",
    "        else:\n",
    "            if len(split) !=3:\n",
    "#                 ipdb.set_trace()\n",
    "                task, dataset, metric, _ = split\n",
    "                \n",
    "            else:\n",
    "                task, dataset, metric = split\n",
    "            \n",
    "            t, d, m = task.strip(), dataset.strip(), metric.strip()\n",
    "            TDM.add(f\"{t}#{d}#{m}\")\n",
    "            \n",
    "            Uniq_task.add(t)\n",
    "            Uniq_dataset.add(d)\n",
    "            Uniq_metric.add(m)\n",
    "    \n",
    "    for paper in path_to_df[(path_to_df.label == True) & (path_to_df.TDM != 'unknown') ].title.tolist():\n",
    "        avg_tdm_per_paper[paper] += 1\n",
    "    \n",
    "    print(f\"Number of papers: {len(set(path_to_df[(path_to_df.label == True)].title.tolist()))}\")\n",
    "    print(f\"Unknown count: {unknown_count}\")\n",
    "    print(f\"Total leaderboards: {len(path_to_df[(path_to_df.label == True) & (path_to_df.TDM != 'unknown')].title.tolist())}\")\n",
    "    print(f\"Avg leaderboard per paper: {round(np.mean(list(avg_tdm_per_paper.values())), 2)}\")\n",
    "    print(f\"Distinc leaderboard: {len(TDM)}\")\n",
    "    print(f\"Distinct taks: {len(Uniq_task)}\")\n",
    "    print(f\"Distinc datasets: {len(Uniq_dataset)}\")\n",
    "    print(f\"Distinc metrics: {len(Uniq_metric)}\")\n",
    "    \n",
    "    \n",
    "    return avg_tdm_per_paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>D16-1036.pdf</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Multi-view Response Selection for Human-Comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>D16-1036.pdf</td>\n",
       "      <td>question answering; SQuAD; F1</td>\n",
       "      <td>Multi-view Response Selection for Human-Comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>D16-1036.pdf</td>\n",
       "      <td>relation prediction; FB15K-237; H@1</td>\n",
       "      <td>Multi-view Response Selection for Human-Comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>D16-1036.pdf</td>\n",
       "      <td>word sense disambiguation; SemEval 2013; F1</td>\n",
       "      <td>Multi-view Response Selection for Human-Comput...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>D16-1036.pdf</td>\n",
       "      <td>language modeling; 1B Words / Google Billion W...</td>\n",
       "      <td>Multi-view Response Selection for Human-Comput...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label         title                                                TDM  \\\n",
       "0   True  D16-1036.pdf                                            unknown   \n",
       "1  False  D16-1036.pdf                      question answering; SQuAD; F1   \n",
       "2  False  D16-1036.pdf                relation prediction; FB15K-237; H@1   \n",
       "3  False  D16-1036.pdf        word sense disambiguation; SemEval 2013; F1   \n",
       "4  False  D16-1036.pdf  language modeling; 1B Words / Google Billion W...   \n",
       "\n",
       "                                             Context  \n",
       "0  Multi-view Response Selection for Human-Comput...  \n",
       "1  Multi-view Response Selection for Human-Comput...  \n",
       "2  Multi-view Response Selection for Human-Comput...  \n",
       "3  Multi-view Response Selection for Human-Comput...  \n",
       "4  Multi-view Response Selection for Human-Comput...  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_IBM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_IBM.title.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 170\n",
      "Unknown count: 46\n",
      "Total leaderboards: 327\n",
      "Avg leaderboard per paper: 2.64\n",
      "Distinc leaderboard: 78\n",
      "Distinct taks: 18\n",
      "Distinc datasets: 44\n",
      "Distinc metrics: 31\n"
     ]
    }
   ],
   "source": [
    "avg_tdm_per_paper = get_stats(train_IBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>1803.11175.pdf</td>\n",
       "      <td>sentiment analysis; SUBJ; Accuracy</td>\n",
       "      <td>Universal Sentence Encoder We present models f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>1803.11175.pdf</td>\n",
       "      <td>text classification; TREC; Error</td>\n",
       "      <td>Universal Sentence Encoder We present models f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>1803.11175.pdf</td>\n",
       "      <td>question answering; SQuAD; F1</td>\n",
       "      <td>Universal Sentence Encoder We present models f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>1803.11175.pdf</td>\n",
       "      <td>relation prediction; FB15K-237; H@1</td>\n",
       "      <td>Universal Sentence Encoder We present models f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>1803.11175.pdf</td>\n",
       "      <td>word sense disambiguation; SemEval 2013; F1</td>\n",
       "      <td>Universal Sentence Encoder We present models f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label           title                                          TDM  \\\n",
       "0   True  1803.11175.pdf           sentiment analysis; SUBJ; Accuracy   \n",
       "1   True  1803.11175.pdf             text classification; TREC; Error   \n",
       "2  False  1803.11175.pdf                question answering; SQuAD; F1   \n",
       "3  False  1803.11175.pdf          relation prediction; FB15K-237; H@1   \n",
       "4  False  1803.11175.pdf  word sense disambiguation; SemEval 2013; F1   \n",
       "\n",
       "                                             Context  \n",
       "0  Universal Sentence Encoder We present models f...  \n",
       "1  Universal Sentence Encoder We present models f...  \n",
       "2  Universal Sentence Encoder We present models f...  \n",
       "3  Universal Sentence Encoder We present models f...  \n",
       "4  Universal Sentence Encoder We present models f...  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_IBM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 167\n",
      "Unknown count: 45\n",
      "Total leaderboards: 294\n",
      "Avg leaderboard per paper: 2.41\n",
      "Distinc leaderboard: 78\n",
      "Distinct taks: 18\n",
      "Distinc datasets: 44\n",
      "Distinc metrics: 31\n"
     ]
    }
   ],
   "source": [
    "metric = get_stats(test_IBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Make sure that all leaderboard in test are present in train \n",
    "count = []\n",
    "for paper in test_IBM.TDM.to_list():\n",
    "    if paper not in train_IBM.TDM.to_list():\n",
    "        count.append(paper)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "count = []\n",
    "for paper in train_IBM.TDM.to_list():\n",
    "    if paper not in test_IBM.TDM.to_list():\n",
    "        print(paper)\n",
    "        count.append(paper)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "New_train_csv = \"~/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_800/twofoldwithunk/fold1/train.tsv\"\n",
    "New_test_csv = \"~/Research/task-dataset-metric-nli-extraction/data/pwc_ibm_150_5_10_800/twofoldwithunk/fold1/dev.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_New = pd.read_csv(New_train_csv, \n",
    "                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])\n",
    "# train_IBM['label'] = train_IBM.label.apply(lambda x: \"true\" if x else \"false\")\n",
    "\n",
    "test_New = pd.read_csv(New_test_csv, \n",
    "                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 3181\n",
      "Unknown count: 531\n",
      "Total leaderboards: 35752\n",
      "Avg leaderboard per paper: 11.72\n",
      "Distinc leaderboard: 2518\n",
      "Distinct taks: 365\n",
      "Distinc datasets: 1220\n",
      "Distinc metrics: 733\n"
     ]
    }
   ],
   "source": [
    "avg_tdm_per_paper = get_stats(train_New)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35752"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(np.sum(list(avg_tdm_per_paper.values())), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers: 1361\n",
      "Unknown count: 269\n",
      "Total leaderboards: 16700\n",
      "Avg leaderboard per paper: 12.85\n",
      "Distinc leaderboard: 1992\n",
      "Distinct taks: 304\n",
      "Distinc datasets: 974\n",
      "Distinc metrics: 561\n"
     ]
    }
   ],
   "source": [
    "avg_tdm_per_paper = get_stats(test_New)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "count = []\n",
    "for tdm in test_New.TDM.to_list():\n",
    "    if tdm not in train_New.TDM.to_list():\n",
    "        count.append(tdm)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "count = []\n",
    "for tdm in train_New.TDM.to_list():\n",
    "    if tdm not in test_New.TDM.to_list():\n",
    "        count.append(tdm)\n",
    "print(len(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
