{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Client `s2orc-doc2json`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, ipdb, traceback, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_pdfs = os.listdir(\"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/pdf\") \n",
    "list_df_tei = os.listdir(\"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/pdf_tei_xml\") \n",
    "list_pdf_json = os.listdir(\"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/pdf_json\") \n",
    "list_pdf_txt = os.listdir(\"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/pdf_txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght pdf considered: 5203\n",
      "Lenght python grobid tei.xml: 5194        \n",
      "Lenght python grobid json: 5203\n",
      "Lenght python grobid txt: 5203\n"
     ]
    }
   ],
   "source": [
    "print(f\"Lenght pdf considered: {len(list_pdfs)}\\nLenght python grobid tei.xml: {len(list_df_tei)}\\\n",
    "        \\nLenght python grobid json: {len(list_pdfs)}\\nLenght python grobid txt: {len(list_pdfs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_latex_src = os.listdir(\"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/exp/arxiv_src\") \n",
    "list_latex_src_convert = os.listdir(\"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/exp/arxiv_src_txt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lenght pdf with latex source code: 5195        \n",
      "Lenght of files that could be converted from .tex to .txt using pandoc (locally): 3574\n"
     ]
    }
   ],
   "source": [
    "print(f\"Lenght pdf with latex source code: {len(list_latex_src)}\\\n",
    "        \\nLenght of files that could be converted from .tex to .txt using pandoc (locally): {len(list_latex_src_convert)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate TDMS in the parsed texts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Pipiline stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats_from_files(path_to_annotation=\"resultsAnnotation.tsv\", \n",
    "                         file_extention=\".txt\",\n",
    "                         path_to_txt_folder=\"../pdf_txt/\"):\n",
    "    with open(path_to_annotation) as f:\n",
    "        resultsAnnotation = f.read().splitlines()\n",
    "    \n",
    "    missed = 0\n",
    "    \n",
    "    total_papers = 0\n",
    "    \n",
    "    global_t = 0\n",
    "    global_d = 0\n",
    "    global_m = 0\n",
    "    global_s = 0\n",
    "    global_td = 0\n",
    "    global_tdm = 0\n",
    "    global_tdms = 0\n",
    "        \n",
    "    \n",
    "    for paper in resultsAnnotation:\n",
    "        \n",
    "        task_set = set()\n",
    "        dataset_set = set()\n",
    "        metric_set = set()\n",
    "        score_set = set()\n",
    "        \n",
    "        t = 0\n",
    "        d = 0\n",
    "        m = 0\n",
    "        s = 0\n",
    "        td = 0\n",
    "        tdm = 0\n",
    "        tdms = 0\n",
    "        \n",
    "        list_parsed_pdf = os.listdir(path_to_txt_folder)\n",
    "        if '.ipynb_checkpoint' in list_parsed_pdf:\n",
    "            list_parsed_pdf.remove('.ipynb_checkpoint')\n",
    "        \n",
    "        pdf_id = '.'.join(paper.split(\"\\t\")[0].split('/')[-1].split('.')[:-1])\n",
    "        \n",
    "        # Avoid paper that are not parsed\n",
    "        if pdf_id+file_extention not in list_parsed_pdf:\n",
    "            continue \n",
    "            \n",
    "        total_contrib = len(paper.split(\"\\t\")[-1].split(\"$\"))\n",
    "        \n",
    "        with open(f\"{path_to_txt_folder}{pdf_id}{file_extention}\", errors='replace') as f:\n",
    "            txt = f.read().splitlines()\n",
    "            \n",
    "        file_txt = ' '.join(txt)\n",
    "        file_txt = re.sub(r'[^\\w]', ' ', file_txt)\n",
    "        \n",
    "        for contrib in paper.split(\"\\t\")[-1].split(\"$\"):\n",
    "            if len(contrib.split(\"#\")) != 4:\n",
    "                missed += 1\n",
    "                continue\n",
    "#                 ipdb.set_trace()\n",
    "            task, dataset, metric, score = contrib.split(\"#\")\n",
    "            task, dataset, metric, score = re.sub(r'[^\\w]', ' ', task), re.sub(r'[^\\w]', ' ', dataset), re.sub(r'[^\\w]', ' ', metric), re.sub(r'[^\\w]', ' ', score)\n",
    "            found_task = re.search(rf\"\\b{task}\\b\", file_txt)\n",
    "            found_dataset = re.search(rf\"\\b{dataset}\\b\", file_txt)                  \n",
    "            found_metric = re.search(rf\"\\b{metric}\\b\", file_txt)\n",
    "            found_score = re.search(rf\"\\b{score}\\b\", file_txt)\n",
    "            \n",
    "            if found_task and found_dataset and found_metric and found_score:\n",
    "                tdms += 1\n",
    "            if found_task and found_dataset and found_metric:\n",
    "                tdm += 1\n",
    "            if found_task and found_dataset:\n",
    "                td += 1          \n",
    "            if found_task:\n",
    "                t += 1           \n",
    "            if found_dataset:\n",
    "                d += 1             \n",
    "            if found_metric:\n",
    "                m += 1\n",
    "            if found_score:\n",
    "                s += 1\n",
    "        \n",
    "        global_t += t/total_contrib\n",
    "        global_d += d/total_contrib\n",
    "        global_m += m/total_contrib\n",
    "        global_s += s/total_contrib\n",
    "        global_td += td/total_contrib\n",
    "        global_tdm += tdm/total_contrib\n",
    "        global_tdms += tdms/total_contrib\n",
    "        \n",
    "#         # Relax the count of found TDN within a contrib\n",
    "#         global_t += t/(total_contrib/2)\n",
    "#         global_d += d/(total_contrib/2)\n",
    "#         global_m += m/(total_contrib/2)\n",
    "#         global_s += s/(total_contrib/2)\n",
    "#         global_td += td/(total_contrib/2)\n",
    "#         global_tdm += tdm/(total_contrib/2)\n",
    "#         global_tdms += tdms/(total_contrib/2)\n",
    "        \n",
    "        total_papers += 1\n",
    "        \n",
    "    \n",
    "    print(f\"Pourcentage task {round((global_t/total_papers)*100, 2)} %\")\n",
    "    print(f\"Pourcentage dataset {round((global_d/total_papers)*100, 2)} %\")\n",
    "    print(f\"Pourcentage metric {round((global_m/total_papers)*100, 2)} %\")\n",
    "    print(f\"Pourcentage score {round((global_s/total_papers)*100, 2)} %\")\n",
    "    print(f\"Pourcentage task-dataset {round((global_td/total_papers)*100, 2)} %\")\n",
    "    print(f\"Pourcentage task-dataset-metric {round((global_tdm/total_papers)*100, 2)} %\")\n",
    "    print(f\"Pourcentage task-dataset-metric-score {round((global_tdms/total_papers)*100, 2)} %\")\n",
    "    \n",
    "    print(f\"Total missed data {missed} and Total number of paper {total_papers}\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage task 39.0 %\n",
      "Pourcentage dataset 48.79 %\n",
      "Pourcentage metric 33.38 %\n",
      "Pourcentage score 18.81 %\n",
      "Pourcentage task-dataset 19.59 %\n",
      "Pourcentage task-dataset-metric 6.78 %\n",
      "Pourcentage task-dataset-metric-score 1.44 %\n",
      "Total missed data 16 and Total number of paper 5193\n"
     ]
    }
   ],
   "source": [
    "get_stats_from_files(path_to_annotation=\"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/resultsAnnotation.tsv\", \n",
    "                         file_extention=\".txt\",\n",
    "                         path_to_txt_folder=\"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/pdf_txt/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Arxiv source code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage task 32.84 %\n",
      "Pourcentage dataset 39.19 %\n",
      "Pourcentage metric 30.87 %\n",
      "Pourcentage score 48.0 %\n",
      "Pourcentage task-dataset 17.1 %\n",
      "Pourcentage task-dataset-metric 7.2 %\n",
      "Pourcentage task-dataset-metric-score 5.22 %\n",
      "Total missed data 18 and Total number of paper 5195\n"
     ]
    }
   ],
   "source": [
    "# .tex file \n",
    "get_stats_from_files(path_to_annotation=\"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/resultsAnnotation.tsv\",\n",
    "                          file_extention=\".tex\", \n",
    "                          path_to_txt_folder=\"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/exp/arxiv_src/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage task 26.92 %\n",
      "Pourcentage dataset 50.12 %\n",
      "Pourcentage metric 37.51 %\n",
      "Pourcentage score 49.79 %\n",
      "Pourcentage task-dataset 14.01 %\n",
      "Pourcentage task-dataset-metric 5.71 %\n",
      "Pourcentage task-dataset-metric-score 3.28 %\n",
      "Total missed data 10 and Total number of paper 3574\n"
     ]
    }
   ],
   "source": [
    "# .txt file converted using pandoc\n",
    "get_stats_from_files(path_to_annotation=\"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/resultsAnnotation.tsv\",\n",
    "                          file_extention=\".txt\", \n",
    "                          path_to_txt_folder=\"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/exp/arxiv_src_txt/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IBM DocTEA Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_train = \"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/new/60Neg800unk/twofoldwithunk/fold1/train.tsv\"\n",
    "f1_test = \"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/new/60Neg800unk/twofoldwithunk/fold1/test.tsv\"\n",
    "f2_train = \"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/new/60Neg800unk/twofoldwithunk/fold2/train.tsv\"\n",
    "f2_test = \"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/new/60Neg800unk/twofoldwithunk/fold2/test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_paper_contest = {}\n",
    "with open(f\"{f1_train}\") as f:\n",
    "    txt_f1_train = f.read().splitlines()\n",
    "with open(f\"{f1_test}\") as f:\n",
    "    txt_f1_test = f.read().splitlines()\n",
    "with open(f\"{f2_train}\") as f:\n",
    "    txt_f2_train = f.read().splitlines()\n",
    "with open(f\"{f2_test}\") as f:\n",
    "    txt_f2_test = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256008"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(txt_f1_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_dict(cache_dict, list_txt_files):\n",
    "    for item in list_txt_files:\n",
    "        id_file = item.split(\"\\t\")[1].split(\".pdf\")[0]\n",
    "        content = item.split(\"\\t\")[-1]\n",
    "        if id_file not in cache_dict.keys():\n",
    "            cache_dict[id_file] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_dict(dict_paper_contest, txt_f1_train)\n",
    "fill_dict(dict_paper_contest, txt_f1_test)\n",
    "fill_dict(dict_paper_contest, txt_f2_train)\n",
    "fill_dict(dict_paper_contest, txt_f2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5193"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_paper_contest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats_from_files_with_dict(path_to_annotation=\"resultsAnnotation.tsv\",\n",
    "                          dict_annotation=dict_paper_contest,\n",
    "                          path_to_txt_folder=\"../pdf_txt/\"):\n",
    "    with open(path_to_annotation) as f:\n",
    "        resultsAnnotation = f.read().splitlines()\n",
    "    \n",
    "    missed = 0\n",
    "    \n",
    "    total_papers = 0\n",
    "    \n",
    "    global_t = 0\n",
    "    global_d = 0\n",
    "    global_m = 0\n",
    "    global_s = 0\n",
    "    global_td = 0\n",
    "    global_tdm = 0\n",
    "    global_tdms = 0\n",
    "        \n",
    "    \n",
    "    for paper in resultsAnnotation:\n",
    "        \n",
    "        task_set = set()\n",
    "        dataset_set = set()\n",
    "        metric_set = set()\n",
    "        score_set = set()\n",
    "        \n",
    "        t = 0\n",
    "        d = 0\n",
    "        m = 0\n",
    "        s = 0\n",
    "        td = 0\n",
    "        tdm = 0\n",
    "        tdms = 0\n",
    "        \n",
    "        list_parsed_pdf = os.listdir(path_to_txt_folder)\n",
    "        if '.ipynb_checkpoint' in list_parsed_pdf:\n",
    "            list_parsed_pdf.remove('.ipynb_checkpoint')\n",
    "        \n",
    "        pdf_id = '.'.join(paper.split(\"\\t\")[0].split('/')[-1].split('.')[:-1])\n",
    "        \n",
    "        # Avoid paper that are not parsed\n",
    "        if pdf_id+\".txt\" not in list_parsed_pdf or pdf_id not in dict_annotation.keys():\n",
    "            continue \n",
    "        total_contrib = len(paper.split(\"\\t\")[-1].split(\"$\"))      \n",
    "        \n",
    "        file_txt = dict_paper_contest[pdf_id]\n",
    "        file_txt = re.sub(r'[^\\w]', ' ', file_txt)\n",
    "        \n",
    "        for contrib in paper.split(\"\\t\")[-1].split(\"$\"):\n",
    "            if len(contrib.split(\"#\")) != 4:\n",
    "                missed += 1\n",
    "                continue\n",
    "#                 ipdb.set_trace()\n",
    "            task, dataset, metric, score = contrib.split(\"#\")\n",
    "            task, dataset, metric, score = re.sub(r'[^\\w]', ' ', task), re.sub(r'[^\\w]', ' ', dataset), re.sub(r'[^\\w]', ' ', metric), re.sub(r'[^\\w]', ' ', score)\n",
    "            found_task = re.search(rf\"\\b{task}\\b\", file_txt)\n",
    "            found_dataset = re.search(rf\"\\b{dataset}\\b\", file_txt)                  \n",
    "            found_metric = re.search(rf\"\\b{metric}\\b\", file_txt)\n",
    "            found_score = re.search(rf\"\\b{score}\\b\", file_txt)\n",
    "            \n",
    "            if found_task and found_dataset and found_metric and found_score:\n",
    "                tdms += 1\n",
    "            if found_task and found_dataset and found_metric:\n",
    "                tdm += 1\n",
    "            if found_task and found_dataset:\n",
    "                td += 1          \n",
    "            if found_task:\n",
    "                t += 1           \n",
    "            if found_dataset:\n",
    "                d += 1             \n",
    "            if found_metric:\n",
    "                m += 1\n",
    "            if found_score:\n",
    "                s += 1\n",
    "            \n",
    "        global_t += t/total_contrib\n",
    "        global_d += d/total_contrib\n",
    "        global_m += m/total_contrib\n",
    "        global_s += s/total_contrib\n",
    "        global_td += td/total_contrib\n",
    "        global_tdm += tdm/total_contrib\n",
    "        global_tdms += tdms/total_contrib\n",
    "        \n",
    "        total_papers += 1\n",
    "        \n",
    "    print(f\"Pourcentage task {round((global_t/total_papers)*100, 2)}\")\n",
    "    print(f\"Pourcentage dataset {round((global_d/total_papers)*100, 2)}\")\n",
    "    print(f\"Pourcentage metric {round((global_m/total_papers)*100, 2)}\")\n",
    "    print(f\"Pourcentage score {round((global_s/total_papers)*100, 2)}\")\n",
    "    print(f\"Pourcentage task-dataset {round((global_td/total_papers)*100, 2)}\")\n",
    "    print(f\"Pourcentage task-dataset-metric {round((global_tdm/total_papers)*100, 2)}\")\n",
    "    print(f\"Pourcentage task-dataset-metric-score {round((global_tdms/total_papers)*100, 2)}\")\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pourcentage task 29.34\n",
      "Pourcentage dataset 43.41\n",
      "Pourcentage metric 30.09\n",
      "Pourcentage score 6.18\n",
      "Pourcentage task-dataset 13.41\n",
      "Pourcentage task-dataset-metric 4.6\n",
      "Pourcentage task-dataset-metric-score 0.45\n"
     ]
    }
   ],
   "source": [
    "get_stats_from_files_with_dict(path_to_annotation=\"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/data/resultsAnnotation.tsv\",\n",
    "                          dict_annotation=dict_paper_contest, \n",
    "                          path_to_txt_folder=\"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/pdf_txt/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IBM Data eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_IBM_train = \"../../ibm/exp/few-shot-setup/NLP-TDMS/paperVersion/train.tsv\"\n",
    "f1_IBM_test = \"../../ibm/exp/few-shot-setup/NLP-TDMS/paperVersion/test.tsv\"\n",
    "\n",
    "dict_paper_IBM_contest = {}\n",
    "with open(f\"{f1_IBM_train}\") as f:\n",
    "    txt_f1_IBM_train = f.read().splitlines()\n",
    "with open(f\"{f1_IBM_test}\") as f:\n",
    "    txt_f1_IBM_test = f.read().splitlines()\n",
    "    \n",
    "def fill_dict_IBM(cache_dict, list_txt_files):\n",
    "    for item in list_txt_files:\n",
    "        id_file = item.split(\"\\t\")[1].split(\".pdf\")[0]\n",
    "        content = item.split(\"\\t\")[-1]\n",
    "        if id_file not in cache_dict.keys():\n",
    "            cache_dict[id_file] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get paper that are in In IBM eval not in Our Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5193"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_paper_contest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_dict_IBM(dict_paper_IBM_contest, txt_f1_IBM_train)\n",
    "fill_dict_IBM(dict_paper_IBM_contest, txt_f1_IBM_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_paper_IBM_contest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_paper = set()\n",
    "\n",
    "for paper in dict_paper_IBM_contest:\n",
    "    if paper not in dict_paper_contest.keys():\n",
    "        missing_paper.add(paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D16-1036', '1803.09074', '1806.06228', 'P17-1127', '1602.02373']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(dict_paper_IBM_contest.keys())[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'1806.06228' in dict_paper_contest.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creat TDM taxonomy for IBM experiment reproducibility  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task-dataset-metric-extraction/data/ibm/NLP-TDMS/annotations/resultsAnnotation.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_Annotation_IBM = \"../../ibm/NLP-TDMS/annotations/resultsAnnotation.tsv\"\n",
    "\n",
    "with open(f\"{results_Annotation_IBM}\") as f:\n",
    "    list_results_Annotation_IBM = f.read().splitlines()\n",
    "    \n",
    "# with open(f\"{f1_IBM_test}\") as f:\n",
    "#     txt_f1_IBM_test = f.read().splitlines()\n",
    "    \n",
    "# def fill_dict_IBM(cache_dict, list_txt_files):\n",
    "#     for item in list_txt_files:\n",
    "#         id_file = item.split(\"\\t\")[1].split(\".pdf\")[0]\n",
    "#         content = item.split(\"\\t\")[-1]\n",
    "#         if id_file not in cache_dict.keys():\n",
    "#             cache_dict[id_file] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "dict_results_Annotation_IBM = defaultdict(lambda: 0 )\n",
    "\n",
    "for paper_list in list_results_Annotation_IBM:\n",
    "    for paper in paper_list.split(\"$\"):\n",
    "        dict_results_Annotation_IBM[paper.split(\"\\t\")[-1].rsplit('#', 1)[0]] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dict_results_Annotation_IBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'part-of-speech_tagging#UD#Avg Accuracy': 3,\n",
       "             'summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-1': 14,\n",
       "             'summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-2': 14,\n",
       "             'summarization#CNN / Daily Mail (Non-anonymized version)#ROUGE-L': 14,\n",
       "             'summarization#CNN / Daily Mail (Non-anonymized version)#METEOR': 14,\n",
       "             'summarization#Gigaword#ROUGE-1': 13,\n",
       "             'summarization#Gigaword#ROUGE-2': 13,\n",
       "             'summarization#Gigaword#ROUGE-L': 13,\n",
       "             'summarization#DUC 2004 Task 1#ROUGE-1': 7,\n",
       "             'summarization#DUC 2004 Task 1#ROUGE-2': 7,\n",
       "             'summarization#DUC 2004 Task 1#ROUGE-L': 7,\n",
       "             'summarization#CNN / Daily Mail (Anonymized version)#ROUGE-1': 11,\n",
       "             'summarization#CNN / Daily Mail (Anonymized version)#ROUGE-2': 11,\n",
       "             'summarization#CNN / Daily Mail (Anonymized version)#ROUGE-L': 11,\n",
       "             'summarization#CNN / Daily Mail (Anonymized version)#METEOR': 11,\n",
       "             'constituency_parsing#Penn Treebank#F1': 10,\n",
       "             'sentiment_analysis#SUBJ#Accuracy': 5,\n",
       "             'natural_language_inference#MultiNLI#Matched Accuracy': 3,\n",
       "             'natural_language_inference#MultiNLI#Mismatched Accuracy': 3,\n",
       "             'semantic_textual_similarity#SICK-R#Pearson Correlation': 3,\n",
       "             'semantic_textual_similarity#SICK-E#Accuracy': 3,\n",
       "             'semantic_textual_similarity#Quora Question Pairs#Accuracy': 4,\n",
       "             'amr_parsing#LDC2015E86#Smatch': 6,\n",
       "             'amr_parsing#LDC2016E25#Smatch': 3,\n",
       "             'lexical_normalization#LexNorm#Accuracy': 4,\n",
       "             'word_segmentation#Chinese Treebank 7#F1': 2,\n",
       "             'word_segmentation#VLSP 2013 word segmentation shared task#F1': 6,\n",
       "             'relation_prediction#FB15K-237#H@10': 8,\n",
       "             'relation_prediction#FB15K-237#H@1': 8,\n",
       "             'relation_prediction#FB15K-237#MRR': 8,\n",
       "             'question_answering#CNN / Daily Mail#Accuracy on CNN': 5,\n",
       "             'question_answering#CNN / Daily Mail#Accuracy on Daily Mail': 5,\n",
       "             'machine_translation#WMT 2014 EN-DE#BLEU': 7,\n",
       "             'machine_translation#WMT 2014 EN-FR#BLEU': 7,\n",
       "             'relationship_extraction#SemEval-2010 Task 8#F1': 13,\n",
       "             'dialogue_state_tracking#Second dialogue state tracking challenge#Request Accuracy': 4,\n",
       "             'dialogue_state_tracking#Second dialogue state tracking challenge#Joint goal Accuracy': 4,\n",
       "             'language_modeling#WikiText-103#Validation perplexity': 7,\n",
       "             'language_modeling#WikiText-103#Test perplexity': 7,\n",
       "             'language_modeling#WikiText-103#Number of params': 7,\n",
       "             'language_modeling#1B Words / Google Billion Word benchmark#Test perplexity': 5,\n",
       "             'language_modeling#1B Words / Google Billion Word benchmark#Number of params': 5,\n",
       "             'coreference_resolution#CoNLL 2012#Avg F1': 3,\n",
       "             'sql_parsing#ATIS#Question Split': 2,\n",
       "             'sql_parsing#ATIS#Query Split': 2,\n",
       "             'sql_parsing#Advising#Question Split': 2,\n",
       "             'sql_parsing#Advising#Query Split': 2,\n",
       "             'sql_parsing#GeoQuery#Question Split': 2,\n",
       "             'sql_parsing#GeoQuery#Query Split': 2,\n",
       "             'sql_parsing#Scholar#Question Split': 2,\n",
       "             'sql_parsing#Scholar#Query Split': 2,\n",
       "             'sql_parsing#Smaller Datasets#Question Split': 2,\n",
       "             'sql_parsing#Smaller Datasets#Query Split': 2,\n",
       "             'question_answering#Story Cloze Test#Accuracy': 4,\n",
       "             'sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on DVD': 4,\n",
       "             'sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on Books': 4,\n",
       "             'sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on Electronics': 4,\n",
       "             'sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on Kitchen': 4,\n",
       "             'sentiment_analysis#Multi-Domain Sentiment Dataset#Accuracy on Average': 4,\n",
       "             'dependency_parsing#Penn Treebank#POS': 9,\n",
       "             'dependency_parsing#Penn Treebank#UAS': 16,\n",
       "             'dependency_parsing#Penn Treebank#LAS': 11,\n",
       "             'named_entity_recognition#VLSP 2016 NER shared task#F1': 5,\n",
       "             'part-of-speech_tagging#VLSP 2013 POS tagging shared task#Accuracy': 7,\n",
       "             'multimodal_sentiment_analysis#MOSI#Accuracy': 2,\n",
       "             'amr_parsing#LDC2014T12#F1 on Newswire': 7,\n",
       "             'amr_parsing#LDC2014T12#F1 on Full': 7,\n",
       "             'question_answering#SQuAD#EM': 21,\n",
       "             'question_answering#SQuAD#F1': 21,\n",
       "             'question_answering#NarrativeQA#BLEU-1': 3,\n",
       "             'question_answering#NarrativeQA#BLEU-4': 3,\n",
       "             'question_answering#NarrativeQA#METEOR': 3,\n",
       "             'question_answering#NarrativeQA#ROUGE-L': 3,\n",
       "             'question_answering#Quasar#EM (Quasar-T)': 5,\n",
       "             'question_answering#Quasar#F1 (Quasar-T)': 5,\n",
       "             'natural_language_inference#SciTail#Accuracy': 3,\n",
       "             'taxonomy_learning#SemEval 2018#MAP': 9,\n",
       "             'taxonomy_learning#SemEval 2018#MRR': 9,\n",
       "             'taxonomy_learning#SemEval 2018#P@5': 9,\n",
       "             'temporal_information_extraction#TimeBank#F1': 2,\n",
       "             'part-of-speech_tagging#Penn Treebank#Accuracy': 10,\n",
       "             'sentiment_analysis#SemEval-2017 Task 4 subtask A Tweet Sentiment Classification dataset#F1-score': 2,\n",
       "             'multimodal_emotion_recognition#IEMOCAP#Weighted Accuracy (WAA)': 1,\n",
       "             'named_entity_recognition#Long-tail emerging entities#F1': 3,\n",
       "             'named_entity_recognition#Long-tail emerging entities#F1 (surface form)': 3,\n",
       "             'question_answering#NewsQA#F1': 4,\n",
       "             'question_answering#NewsQA#EM': 4,\n",
       "             'question_answering#SearchQA#Unigram Acc': 7,\n",
       "             'question_answering#SearchQA#N-gram F1': 7,\n",
       "             'question_answering#SearchQA#EM': 7,\n",
       "             'question_answering#SearchQA#F1': 7,\n",
       "             'language_modeling#Penn Treebank#Validation perplexity': 8,\n",
       "             'language_modeling#Penn Treebank#Test perplexity': 8,\n",
       "             'language_modeling#Penn Treebank#Number of params': 12,\n",
       "             'language_modeling#WikiText-2#Validation perplexity': 6,\n",
       "             'language_modeling#WikiText-2#Test perplexity': 6,\n",
       "             'language_modeling#WikiText-2#Number of params': 6,\n",
       "             'multimodal_emotion_recognition#IEMOCAP#Accuracy': 2,\n",
       "             'grammatical_error_correction#CoNLL-2014 Shared Task Unrestricted#F0.5': 1,\n",
       "             'grammatical_error_correction#CoNLL-2014 10 Annotations Unrestricted#F0.5': 1,\n",
       "             'grammatical_error_correction#JFLEG Unrestricted#GLEU': 1,\n",
       "             'relationship_extraction#New York Times Corpus#P@10%': 5,\n",
       "             'relationship_extraction#New York Times Corpus#P@30%': 5,\n",
       "             'word_sense_disambiguation#Senseval 2#F1': 10,\n",
       "             'word_sense_disambiguation#Senseval 3#F1': 10,\n",
       "             'word_sense_disambiguation#SemEval 2007#F1': 10,\n",
       "             'word_sense_disambiguation#SemEval 2013#F1': 10,\n",
       "             'word_sense_disambiguation#SemEval 2015#F1': 10,\n",
       "             'text_classification#TREC#Error': 8,\n",
       "             'named_entity_recognition#CoNLL 2003 (English)#F1': 13,\n",
       "             'entity_linking#AIDA CoNLL-YAGO Dataset#Micro-Precision': 2,\n",
       "             'dialogue_act_classification#Switchboard corpus#Accuracy': 3,\n",
       "             'sentiment_analysis#SemEval-2014 Task 4 subtask 2 Aspect Term Polarity#Restaurant (acc)': 7,\n",
       "             'sentiment_analysis#SemEval-2014 Task 4 subtask 2 Aspect Term Polarity#Laptop (acc)': 7,\n",
       "             'dialogue_act_classification#ICSI Meeting Recorder Dialog Act (MRDA) corpus#Accuracy': 2,\n",
       "             'language_modeling#Hutter Prize#Bit per Character (BPC)': 7,\n",
       "             'language_modeling#Hutter Prize#Number of params': 7,\n",
       "             'language_modeling#Text8#Bit per Character (BPC)': 7,\n",
       "             'language_modeling#Text8#Number of params': 7,\n",
       "             'retrieval-based_chatbot#Ubuntu Corpus#R_2@1': 4,\n",
       "             'retrieval-based_chatbot#Ubuntu Corpus#R_10@1': 4,\n",
       "             'relation_prediction#WN18RR#H@10': 6,\n",
       "             'relation_prediction#WN18RR#H@1': 6,\n",
       "             'relation_prediction#WN18RR#MRR': 6,\n",
       "             'semantic_role_labeling#OntoNotes#F1': 4,\n",
       "             'sentiment_analysis#SST-5#Accuracy': 2,\n",
       "             'common_sense#SWAG#Accuracy on Dev': 2,\n",
       "             'common_sense#SWAG#Accuracy on Test': 2,\n",
       "             'sql_parsing#WikiSQL#Acc ex': 3,\n",
       "             'summarization#Google Dataset#F1': 3,\n",
       "             'summarization#Google Dataset#CR': 3,\n",
       "             'dependency_parsing#benchmark Vietnamese dependency treebank VnDT#LAS': 7,\n",
       "             'dependency_parsing#benchmark Vietnamese dependency treebank VnDT#UAS': 7,\n",
       "             'named_entity_recognition#Ontonotes v5 (English)#F1': 7,\n",
       "             'machine_translation#The IWSLT 2015 Evaluation Campaign#BLEU': 5,\n",
       "             'chunking#Penn Treebank#F1': 5,\n",
       "             'text_classification#AG News#Error': 5,\n",
       "             'text_classification#DBpedia#Error': 5,\n",
       "             'sentiment_analysis#IMDb#Accuracy': 5,\n",
       "             'sentiment_analysis#Yelp#Error': 4,\n",
       "             'sentiment_analysis#SST-2#Accuracy': 5,\n",
       "             'language_modeling#Penn Treebank#Bit per Character (BPC)': 5,\n",
       "             'question_answering#RACE#Accuracy on RACE-m': 2,\n",
       "             'question_answering#RACE#Accuracy on RACE-h': 2,\n",
       "             'question_answering#RACE#Accuracy on RACE': 2,\n",
       "             'common_sense#Event2Mind#Average Cross-Entropy on Dev': 1,\n",
       "             'common_sense#Event2Mind#Average Cross-Entropy on Test': 1,\n",
       "             'word_segmentation#Chinese Treebank 6#F1': 6,\n",
       "             'grammatical_error_correction#CoNLL-2014 Shared Task Restricted#F0.5': 4,\n",
       "             'grammatical_error_correction#CoNLL-2014 10 Annotations Restricted#F0.5': 2,\n",
       "             'dialogue_state_tracking#Wizard-of-Oz#Request Accuracy': 3,\n",
       "             'dialogue_state_tracking#Wizard-of-Oz#Joint goal Accuracy': 3,\n",
       "             'sentiment_analysis#Sentihood#Aspect (F1)': 3,\n",
       "             'sentiment_analysis#Sentihood#Sentiment (acc)': 3,\n",
       "             'word_segmentation#PKU#F1': 5,\n",
       "             'word_segmentation#MSR#F1': 5,\n",
       "             'timex_normalisation#PNT#F1': 3,\n",
       "             'stance_detection#RumourEval#Accuracy': 2,\n",
       "             'temporal_information_extraction#TempEval-3#Temporal awareness': 2,\n",
       "             'word_segmentation#AS#F1': 4,\n",
       "             'word_segmentation#CityU#F1': 3,\n",
       "             'lexical_normalization#LexNorm2015#F1': 2,\n",
       "             'lexical_normalization#LexNorm2015#Precision': 2,\n",
       "             'lexical_normalization#LexNorm2015#Recall': 2,\n",
       "             'common_sense#Winograd Schema Challenge#Accuracy': 2,\n",
       "             'ccg_supertagging#CCGBank#Accuracy': 5,\n",
       "             'timex_normalisation#TimeBank#F1': 2,\n",
       "             'relationship_extraction#SemEval-2010 Task 8#F1#84.385.9<a href=\"': 1,\n",
       "             'relationship_extraction#SemEval-2010 Task 8#F1#82.784.3<a href=\"': 1,\n",
       "             'question_answering#CliCR#F1': 1,\n",
       "             'part-of-speech_tagging#Social media#Accuracy': 2})"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_results_Annotation_IBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_TDM_taxonomy(TDM_taxonomy):      \n",
    "    with open(\"../../ibm/NLP-TDMS/annotations/TDM_taxonomy\", \"a+\", encoding=\"utf-8\") as text_file:\n",
    "        for key, value in TDM_taxonomy.items():\n",
    "            text_file.write(key+\"\\t\"+str(value)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_TDM_taxonomy(dict_results_Annotation_IBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The main data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset_train = \"../60Neg800unk/twofoldwithunk/fold1/train.tsv\"\n",
    "path_to_dataset_test = \"../60Neg800unk/twofoldwithunk/fold1/test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_dataset_train) as f:\n",
    "    path_to_dataset_train_list = f.read().splitlines()\n",
    "    \n",
    "with open(path_to_dataset_test) as f:\n",
    "    path_to_dataset_test_list = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142396"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(path_to_dataset_train_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142396"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "142396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62450"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(path_to_dataset_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62450"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "62450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"true\\t1409.0473v7.pdf\\tMachine Translation; IWSLT2015 German-English; BLEU score\\tPublished as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neu-ral machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture , and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition. We evaluate the proposed approach on the task of English-to-French translation We use the bilingual, parallel corpora provided by ACL WMT '14 We use the same training procedures and the same dataset for both models WMT '14 contains the following English-French parallel corpora: Europarl (61M words), news commentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively, totaling 850M words Following the procedure described in, we reduce the size of the combined corpus to have 348M words using the data selection method by We do not use any monolingual data other than the mentioned parallel corpora, although it maybe possible to use a much larger monolingual corpus to pretrain an encoder Table 2: Learning statistics and relevant information. Each update corresponds to updating the parameters once using a single minibatch. One epoch is one pass through the training set. NLL is the average conditional log-probabilities of the sentences in either the training set or the development set. Note that the lengths of the sentences differ. Updates ( ×10 5 Epochs Train NLL Dev . NLL\",\n",
       " \"true\\t1409.0473v7.pdf\\tMachine Translation; WMT2014 English-French; BLEU score\\tPublished as a conference paper at ICLR 2015 NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neu-ral machine translation often belong to a family of encoder-decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture , and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition. We evaluate the proposed approach on the task of English-to-French translation We use the bilingual, parallel corpora provided by ACL WMT '14 We use the same training procedures and the same dataset for both models WMT '14 contains the following English-French parallel corpora: Europarl (61M words), news commentary (5.5M), UN (421M) and two crawled corpora of 90M and 272.5M words respectively, totaling 850M words Following the procedure described in, we reduce the size of the combined corpus to have 348M words using the data selection method by We do not use any monolingual data other than the mentioned parallel corpora, although it maybe possible to use a much larger monolingual corpus to pretrain an encoder Table 2: Learning statistics and relevant information. Each update corresponds to updating the parameters once using a single minibatch. One epoch is one pass through the training set. NLL is the average conditional log-probabilities of the sentences in either the training set or the development set. Note that the lengths of the sentences differ. Updates ( ×10 5 Epochs Train NLL Dev . NLL\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_dataset_train_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "TDM_train = set()\n",
    "Paper_train = set()\n",
    "Uniq_task_train = set()\n",
    "Uniq_dataset_train = set()\n",
    "Uniq_metric_train = set()\n",
    "\n",
    "unknown_count_train = 0\n",
    "\n",
    "for contrib in path_to_dataset_train_list:\n",
    "    label, title, tdm, _ = contrib.split('\\t')\n",
    "    if label =='true':\n",
    "        TDM_train.add(tdm)\n",
    "        Paper_train.add(title)\n",
    "        if len(tdm.split(';')) == 3:\n",
    "            t, d, m = tdm.split(';')\n",
    "            Uniq_task_train.add(t.strip())\n",
    "            Uniq_dataset_train.add(d.strip())\n",
    "            Uniq_metric_train.add(m.strip())\n",
    "        elif(tdm == \"unknow\"):\n",
    "            unknown_count_train += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1862"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Paper_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1226"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TDM_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Uniq_task_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "663"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Uniq_dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "340"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Uniq_metric_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_count_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "TDM_test = set()\n",
    "Paper_test = set()\n",
    "Uniq_task_test = set()\n",
    "Uniq_dataset_test = set()\n",
    "Uniq_metric_test = set()\n",
    "\n",
    "unknown_count_test = 0\n",
    "\n",
    "for contrib in path_to_dataset_test_list:\n",
    "    label, title, tdm, _ = contrib.split('\\t')\n",
    "    if label =='true':\n",
    "        TDM_test.add(tdm)\n",
    "        Paper_test.add(title)\n",
    "        if len(tdm.split(';')) == 3:\n",
    "            t, d, m = tdm.split(';')\n",
    "            Uniq_task_test.add(t.strip())\n",
    "            Uniq_dataset_test.add(d.strip())\n",
    "            Uniq_metric_test.add(m.strip())\n",
    "        elif(tdm == \"unknow\"):\n",
    "            unknown_count_test += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "805"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Paper_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unknown_count_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1032"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TDM_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "155"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Uniq_task_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "559"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Uniq_dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Uniq_metric_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' COCO minival'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Object Detection; COCO minival; APM'.split(\";\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1862"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Paper_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "TDM_test = set()\n",
    "Paper_test = set()\n",
    "\n",
    "for contrib in path_to_dataset_test_list:\n",
    "    label, title, tdm, _ = contrib.split('\\t')\n",
    "    if label =='true':\n",
    "        TDM_test.add(tdm)\n",
    "        Paper_test.add(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1032"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TDM_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "805"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "805"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Paper_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep= []\n",
    "for tdd in TDM_test:\n",
    "    if tdd not in TDM:\n",
    "        keep.append(tdd)\n",
    "len(keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
