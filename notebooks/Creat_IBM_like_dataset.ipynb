{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b8a9a8-a789-4c8d-999e-09dd9f6e4b91",
   "metadata": {},
   "source": [
    "# Dataset Creation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb75687-c055-4cec-be20-e1a28b2482c6",
   "metadata": {},
   "source": [
    "we have files \n",
    "- `resultsAnnotation.tsv`, \n",
    "- `datasetAnnotation.tsv`, \n",
    "- `taskAnnotation.tsv`, \n",
    "- `paper_links.tsv`, \n",
    "- `TDM_taxonomy.tsv`, \n",
    "- `TDMs_taxonomy.tsv` \n",
    "- `paper_name_taxonomy.tsv` \n",
    "\n",
    "Created mostly from the file `evaluation-tables.json` from [paperswithcode](https://paperswithcode.com/about)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5e304a6-9964-4f5f-8075-c967023c096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ipdb, os\n",
    "\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca11a08b-b531-4f51-9d0e-72da98dd7119",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"../data/resultsAnnotation.tsv\", errors='replace') as f:\n",
    "    resultsAnnotation = f.read().splitlines()\n",
    "\n",
    "with open(f\"../data/datasetAnnotation.tsv\", errors='replace') as f:\n",
    "    datasetAnnotation = f.read().splitlines()\n",
    "    \n",
    "with open(f\"../data/taskAnnotation.tsv\", errors='replace') as f:\n",
    "    taskAnnotation = f.read().splitlines()\n",
    "    \n",
    "with open(f\"../data/TDM_taxonomy.tsv\", errors='replace') as f:\n",
    "    TDM_taxonomy = f.read().splitlines()\n",
    "    \n",
    "with open(f\"../data/paper_name_taxonomy.tsv\", errors='replace') as f:\n",
    "    paper_name_taxonomy = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08a3628b-a131-4f52-b542-5605b06786fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1510.05067v4.pdf\\tHandwritten Digit Recognition#MNIST#PERCENTAGE ERROR#0.91$Image Classification#STL-10#Percentage correct#57.32$Image Classification#CIFAR-100#Percentage correct#48.75$Image Classification#SVHN#Percentage error#10.16$Image Classification#CIFAR-10#Percentage correct#80.98'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultsAnnotation[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9796fbc-74e5-43e4-984b-51dfab646e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1510.05067v4.pdf\\tMNIST#STL-10#CIFAR-100#SVHN#CIFAR-10'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasetAnnotation[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30c3f964-1c2b-4c64-8fe7-56634e59ca0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1510.05067v4.pdf\\tHandwritten Digit Recognition'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taskAnnotation[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c369983-a87d-4b43-a26b-c2e444e00e07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Deblurring#HIDE (trained on GOPRO)#PSNR (sRGB)\\t8'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TDM_taxonomy[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3493a03-50f6-4c3f-ac1b-4d87dc444ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1510.05067v4.pdf\\t5'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_name_taxonomy[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fde67e0-c77e-4500-bb21-f6735e559b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "ea5cf3c1-7ec1-401b-b050-087296ae2100",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(path_to_resultsAnnotation, path_to_TDM_taxonomy, path_parsed_files,\n",
    "                         output_dir, test_set_portion=0.2,\n",
    "                         leaderboard_threshold=5, num_negative_instances=5, allowed_unknown=10):\n",
    "    \n",
    "    with open(f\"{path_to_resultsAnnotation}/resultsAnnotation.tsv\", errors='replace') as f:\n",
    "        resultsAnnotation = f.read().splitlines()\n",
    "    \n",
    "    paper_TDM = {}\n",
    "    for paper in resultsAnnotation:\n",
    "        if len(paper.split(\"\\t\")) != 2:\n",
    "            continue\n",
    "            \n",
    "        title, TDMSList = paper.split(\"\\t\")\n",
    "        \n",
    "        title = '.'.join(title.split('/')[-1].split('.')[:-1])\n",
    "        paper_TDM[title] = TDMSList\n",
    "\n",
    "    with open(f\"{path_to_TDM_taxonomy}/TDM_taxonomy.tsv\", errors='replace') as f:\n",
    "        TDM_taxonomy = f.read().splitlines()\n",
    "        \n",
    "    TDM_taxonomy_dict = {}\n",
    "    unknown_count = 0\n",
    "    for TDMCount in TDM_taxonomy:\n",
    "        if len(TDMCount.split(\"\\t\")) != 2:\n",
    "            ipdb.set_trace()\n",
    "            continue\n",
    "        TDM, count = TDMCount.split(\"\\t\")\n",
    "        count = int(count)\n",
    "        if count >= leaderboard_threshold:\n",
    "            TDM_taxonomy_dict[TDM] = count\n",
    "#     ipdb.set_trace()\n",
    "    list_parsed_pdf = os.listdir(path_parsed_files)\n",
    "    if '.ipynb_checkpoints' in list_parsed_pdf:\n",
    "        list_parsed_pdf.remove('.ipynb_checkpoints')\n",
    "\n",
    "    \n",
    "    # ToDo: will it be interresting to use stratified ? using the label ? \n",
    "    train_valid = train_test_split(list_parsed_pdf, test_size=10/100, shuffle=True)\n",
    "    train, valid = train_valid[0], train_valid[1]\n",
    "    \n",
    "    if os.path.exists(f\"{output_dir}/train.tsv\"):\n",
    "        os.remove(f\"{output_dir}/train.tsv\")\n",
    "    \n",
    "    \n",
    "    for paper in train :\n",
    "#         ipdb.set_trace()\n",
    "        with open(f\"{path_parsed_files}{paper}\", errors='replace') as f:\n",
    "            txt = f.read().splitlines()\n",
    "        content = ' '.join(txt)\n",
    "#         ipdb.set_trace()\n",
    "        content = re.sub(r\"[^a-zA-Z0-9?,'’‘´`%]+\", ' ', content).strip()\n",
    "#         content = re.sub(r\"[\\n]+\", '', content).strip()\n",
    "#         content = ' '.join(\"None\")\n",
    "        \n",
    "        paper_id = '.'.join(paper.split('/')[-1].split('.')[:-1])\n",
    "        \n",
    "        not_seen = True\n",
    "        if paper_id in paper_TDM.keys():\n",
    "            cache_tdm = set()\n",
    "            for contrib in paper_TDM[paper_id].split(\"$\"):\n",
    "                # keep uniq TDM \n",
    "                \n",
    "                if len(contrib.split(\"#\")) != 4:\n",
    "    #                 missed += 1\n",
    "#                     ipdb.set_trace()\n",
    "                    continue\n",
    "\n",
    "                task, dataset, metric, score = contrib.split(\"#\")\n",
    "        \n",
    "                if (f\"{task}#{dataset}#{metric}\" in cache_tdm):\n",
    "                    continue\n",
    "                \n",
    "                if f\"{task}#{dataset}#{metric}\" in TDM_taxonomy_dict.keys():\n",
    "                    not_seen = False\n",
    "                    cache_tdm.add(f\"{task}#{dataset}#{metric}\")\n",
    "                    with open(f\"{output_dir}/train.tsv\", \"a+\", encoding=\"utf-8\") as text_file:\n",
    "                        text_file.write(f\"true\\t{paper_id}\\t{task}#{dataset}#{metric}\\t{content}\\n\")\n",
    "                        \n",
    "            if not_seen and (unknown_count <= allowed_unknown):\n",
    "                with open(f\"{output_dir}/train.tsv\", \"a+\", encoding=\"utf-8\") as text_file:\n",
    "                    text_file.write(f\"true\\t{paper_id}\\tunknown\\t{content}\\n\")\n",
    "                    \n",
    "                    \n",
    "            random_tdm =  list(TDM_taxonomy_dict.keys()) \n",
    "            random_tdm.sort()\n",
    "            for RandTDM in random_tdm[:num_negative_instances]:\n",
    "                task, dataset, metric = RandTDM.split(\"#\")\n",
    "                with open(f\"{output_dir}/train.tsv\", \"a+\", encoding=\"utf-8\") as text_file:\n",
    "                    text_file.write(f\"false\\t{paper_id}\\t{task}#{dataset}#{metric}\\t{content}\\n\")\n",
    "        else:\n",
    "            print(f\"Paper {paper_id} not in the resultsAnnotation.tssv file\")\n",
    "                        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "499871dc-9c5e-4bb7-b88f-dd9cf70ea600",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_grobid_full_txt = \"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/pdf_txt/\"\n",
    "path_latex_source_tex = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/exp/arxiv_src/\"\n",
    "path_latex_source_pandoc_txt = \"/nfs/home/kabenamualus/Research/task-dataset-metric-nli-extraction/exp/arxiv_src_txt/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "a18a30e7-b8ee-41c7-9648-b0f862ac7bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-301-3e5d9fbdc62d>\u001b[0m(55)\u001b[0;36mcreate_training_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     54 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 55 \u001b[0;31m        \u001b[0mpaper_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     56 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m     50 \u001b[0m        \u001b[0mipdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     51 \u001b[0m\u001b[0;31m#         content = re.sub(r\"[^a-zA-Z0-9?,'’‘´`%]+\", '', content).strip()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     52 \u001b[0m\u001b[0;31m#         content = re.sub(r\"[\\n]+\", '', content).strip()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     53 \u001b[0m\u001b[0;31m#         content = ' '.join(\"None\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     54 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 55 \u001b[0;31m        \u001b[0mpaper_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[1;32m     56 \u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     57 \u001b[0m        \u001b[0mnot_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     58 \u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mpaper_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaper_TDM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     59 \u001b[0m            \u001b[0mcache_tdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     60 \u001b[0m            \u001b[0;32mfor\u001b[0m \u001b[0mcontrib\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpaper_TDM\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpaper_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"$\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  content\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Title\\tEvaluation of Deep Convolutional Nets for Document Image Classification and Retrieval Abstract:\\tThis paper presents a new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs). In object and scene analysis, deep neural nets are capable of learning a hierarchical chain of abstraction from pixel inputs to concise and descriptive representations. The current work explores this capacity in the realm of document analysis, and confirms that this representation strategy is superior to a variety of popular hand-crafted alternatives. Experiments also show that (i) features extracted from CNNs are robust to compression, (ii) CNNs trained on non-document images transfer well to document analysis tasks, and (iii) enforcing region-specific feature-learning is unnecessary given sufficient training data. This work also makes available a new labelled subset of the IIT-CDIP collection, containing 400,000 document images across 16 categories, useful for training new CNNs for document analysis. I. INTRODUCTION\\tMany document types have a distinct visual style. For example, \"letter\" documents are typically written in a standard format, which is recognizable even at scales where the text is unreadable. Motivated by this observation, this paper addresses the problem of document classification and retrieval, based on the visual structure and layout of document images. Content-based analysis of document images has a number of applications. In digital libraries, documents are often stored as images before they are processed by an optical character recognition (OCR) system, which means basic image analysis is the only available tool for initial indexing and classification [26] . As a pre-processing stage, document image analysis can facilitate and improve OCR by providing information about each document\\'s visual layout [10] . Furthermore, document information that is lost in OCR, such as typeface, graphics, and layout, can only be stored and indexed using images or image descriptors. Therefore, image analysis is complementary to OCR at several stages of document analysis. The challenge of document image analysis arises from the fact that within each document type, there exists a wide range of visual variability. For example, of the correspondence documents shown in Figure 1 , no two documents share the exact same spatial arrangement of header, date, address, body, and signature; some of the documents even omit these components entirely. This level of intra-class variability renders spatial layout analysis difficult, and rigid template matching impossible [8] . Another issue is that documents of different categories often have substantial visual similarities. For instance, there exist advertisements that look like news articles, and questionnaires that look like forms, and so on. From Fig. 1 . Examples of document images that share the visual style of \"letter\". Note that even when the text of these documents is illegible, their style type is clear. The documents have similar spatial configurations of various parts: addresses and dates typically appear near the top, and signatures typically appear near the bottom, but no two documents share the exact same layout. Identifying the style type of these documents is therefore difficult, but can potentially facilitate the extraction of further information. the perspective of \"visual styles\", some erroneous retrievals in such circumstances may be justifiable, but in general the task of document image analysis is to effectively classify and retrieve documents despite intra-class variability, and interclass similarity. Similar challenges appear in other fields, such as object recognition and scene classification. In those domains the current state-of-the-art approach involves training a deep convolutional neural network (CNN) to learn features for the task [24, 19, 29] . Inspired by the success of CNNs in other domains, this paper presents an extensive evaluation of CNNs for document classification and retrieval. In the end, it is determined that features extracted from deep CNNs exceed the performance of all popular alternative features on both classification and retrieval, by a large margin. Experiments are also presented on transfer learning, which demonstrate that CNNs trained on object recognition learn features that are surprisingly effective at describing documents. Furthermore, it is found that the deep net strategy is not significantly improved by additional guidance toward region-focused features, suggesting that a CNN trained on whole images may already be capable of learning some amount of the information that region-based analysis would add.  A. Related Work\\tIn the past twenty years of document image analysis, research has oscillated from region-based analysis to whole image analysis, and simultaneously, from handcrafted features to machine-learned ones. The power of region-based analysis of document images arXiv:1502.07058v1 [cs.CV] 25 Feb 2015 has been clearly demonstrated in the domain of rigidly structured documents, such as forms and business letters [7, 18] . In general, this approach assumes that many document types have a distinct and consistent configuration of visually-identifiable components. For example, formal business letters typically share a particular spatial configuration of letterhead, date and salutation. To some extent, the classification of perfectly rigid documents (e.g., forms) can be reduced to the problem of template matching [7] , and less-rigid document types (e.g., letters) can similarly be classified by fitting the geometric configuration of the document\\'s components to one of several template configurations, via geometric transformations [15] . The drawback of this approach is that it requires the manual definition of a template for each document type to be categorized. Furthermore, the approach is limited to documents for which a template definition is possible. For documents with more flexible structures, as considered herein, template-based approaches are inapplicable. An alternative strategy is to treat document images holistically, or at least in very large regions, and search for discriminative \"landmark\" features that may appear anywhere in the document [32, 31] . This strategy is sometimes called a \"bag of visual words\" approach, since it describes images with a histogram over an orderless vocabulary of features [12] . For example, a landmark feature discriminating letters from most other document classes is the salutation: finding a salutation in a document (potentially through OCR) is a good cue that the document is a letter, regardless of that feature\\'s exact spatial position [32] . The advantage of holistic analysis is that the resulting representation of documents is invariant to the geometric configuration of the features. This approach has therefore been successful in retrieving and classifying a broader range of documents than the templatebased approaches, although the approach is less discriminating in the domain of rigid-template documents. Recently, there have been attempts to bridge the gap between region-based and holistic analyses. By concatenating image features pooled at several stages, beginning with a whole-image pool and proceeding into smaller and smaller regions, it is possible to build a descriptor that contains both global and local layout characteristics [23] . This technique, known as spatial pyramid matching, was initially developed for categorizing scenes, but it has been shown to apply well to documents also, especially if the pooling regions are designed with document categorization in mind [22] . For document retrieval, this type of representation represents the current state-of-the-art. At the same time, many researchers have replaced handcrafted features and representations with machine-learned variants [10, 9] . A popular area of research in this domain concerns the task of learning document structure. This typically involves training a decision tree to navigate the various possible geometric configurations of fixed features (i.e. \"landmarks\") within each document type, toward the goal of structure-based classification [10, 21] . Most recently, it was shown that the entire pipeline of supervised document image classification, from feature-building to decision making, can be learned by a convolutional neural network (CNN) [17] . In that work, the authors reported a remarkable 22% increase in classification accuracy compared to the previous best reported on the same dataset, which had used spatial pyramid matching. However, the CNN approach has not yet been applied to document retrieval. A shift toward machine-learned features has been taking place in other areas of computer vision as well. In the object recognition literature, CNNs currently exceed the performance of every other approach by a very large margin [19, 13] . The CNN approach has even been shown to apply well to domains for which it was traditionally believed ill-suited, such as attribute detection, and fine-grained object recognition [29] . The success of CNNs in fine-grained object recognition is especially relevant to document image analysis, since the two fields share some significant challenges, e.g., (i) the items being distinguished are very similar to each other, and (ii) there do not exist problem-specific datasets large enough to train a powerful CNN without causing it to overfit. It makes sense, therefore, to draw inspiration from fine-grained object recognition research on how to overcome these challenges. Two major points on the training and usage of CNNs can be gleaned from fine-grained classification research. First, before training the CNN on the data of interest, it is recommended to pre-train the network on a much larger related problem, such as the ILSVRC 2012 challenge [30, 13, 6] . This regularization technique addresses the issue of overfitting, and allows large CNNs to be effectively applied to small problems. Second, in problems where spatial information is important, it is potentially better to encode this information in multiple networks trained on specific regions of interest than in a single network trained on the entire image [6, 5, 33] . More generally, this second point suggests that it is unnecessary to rely entirely on machine learning, especially when human knowledge can be easily implemented in the system. This paper seeks to investigate whether these insights are relevant to document image analysis. Finally, CNNs in other domains have recently been extended to the task of image retrieval. After a CNN is trained on classification, the layers of the network can be interpreted as forming a hierarchical chain of abstraction, where the lowest layers contain simple features, and the highest layers contain concise and descriptive representations [24] . Therefore, output extracted near the top of a CNN can serve as a feature vector which can be used for any task, including retrieval [29, 3, 14, 2] . The present work is the first to apply this idea toward document retrieval.  B. Contributions\\tIn the light of previous work, this paper makes the following contributions. First, the paper thoroughly evaluates the power of deep CNN features for representing document images. Toward this end, the paper presents experiments in CNN design, training, feature processing, and compression. Results show that features extracted from CNNs are superior to all handcrafted competitors, and furthermore can be compressed to very short codes with negligible loss in performance. Second, this work demonstrates that CNNs trained on nondocument images transfer well to document-related tasks. Third, this paper explores a strategy of embedding human knowledge of document structure into CNN architectures, by guiding an ensemble of CNNs toward learning region-specific features. Interestingly, results show little to no improvement in classification and retrieval after this augmentation, suggesting that a basic holistic CNN may be learning region-specific features (or perhaps better features) automatically. Finally, this work makes available a new labelled subset of the IIT-CDIP collection of tobacco litigation documents [25] , containing 400,000 document images across 16 categories.  II. TECHNICAL APPROACH\\tIn structured documents, the layout of text and graphics elements often reflects important information about genre. Therefore, documents of a category often share region-specific features. This paper attempts to learn these informative features by training either a single holistic CNN or an ensemble of region-based CNNs. Additionally, the paper explores two different initialization strategies: the first initializes the weights of the CNNs randomly, and relies entirely on the training process to find the features; the second transfers weights from a network trained on another task, and relies on training only to fine-tune the features to the domain of document analysis.  A. Holistic convolutional neural networks\\tIn most modern implementations of neural networks for computer vision, the network takes as input a square matrix of pixels as input, processes this input through a stack of convolutional layers, then classifies the output of those convolutional layers using two or three fully-connected layers [24, 19] . A typical network of this type has approximately 60 million trainable parameters; this vast representational capacity, along with the hierarchical organization of that representation, is assumed to be responsible for the network\\'s power as a featurebuilder and classifier [24] . Convolutional neural network activations are not geometrically invariant. In applications such as object detection, this is sometimes an inconvenient property. Much work has been done to add spatial invariance to CNNs, e.g., by \"jittering\" the training data to add geometric variants of each image in the dataset [24] , or by altering the architecture of the CNN to process the input at multiple scales and positions [14] . For document analysis, however, spatial specificity in CNN activations may be beneficial. For example, it makes sense to treat the header region of a document differently than the footer region. By design, a holistic CNN trained on a dataset of well-aligned document images should be capable of learning region-specific features automatically. Typically, CNNs are trained to perform a classification task, but a CNN trained on classification can be exploited to perform retrieval also. It has been found that the activation patterns near the top of a deep CNN make very descriptive feature vectors [29] . These feature vectors are high-dimensional (e.g., 4096 dimensions), but their dimensionality can be reduced significantly via principal component analysis (e.g., to 128 dimensions) without significantly affecting their discriminative power [3] . Retrieval involves computing the Euclidean distance between a query descriptor and every descriptor of the training set. The sorted distances are then used to rank the images of the training data, and return a sorted list of documents.   B. Region-based guidance\\tAccounting for the possibility that a holistic CNN may not take advantage of region-specific information in document images, guiding CNNs to learn region-based features may aid fine-grained discrimination by isolating subtle region-specific appearance differences between document categories. Consider the example of discriminating letters and memos, as illustrated in Figure 2 . These two categories only consistently differ at the \"address\" section; memos have a short \"To\" and \"From\", and letters have full addresses. It is possible that a holistic CNN will learn this automatically, but training a CNN to classify documents using only this region increases the likelihood that this feature will be learned. The idea of this approach is to devote one CNN to each region of interest, and therefore force multiple CNNs to learn rich region dependent representations, from which features can be extracted and combined. Any number of region-specific CNNs can be used in this approach. In this work, a total of five CNNs are used. Four of these are region-tuned, placed at the header, left body, right body, and footer of the document images. The fifth is a holistic CNN, trained on the entire images. The final region-based representation of document images is built by combining and compressing features extracted from each region-tuned CNN. The final descriptor is represented by the concatenation of region specific features: [φ 0 , φ 1 , . . . , φ n ], where φ 0 represents the PCA-compressed feature vector extracted from the holistic CNN, and φ 1 , . . . , φ n represent the analogous vectors extracted from regions 1 through n. Figure 3 illustrates the full process of this vector\\'s construction. For retrieval, this new vector is used directly. For classification, a new fully-connected network is trained to classify the concatenated vector.  C. Transfer learning\\tThe goal of transfer learning is to take advantage of shared structure in related problems, to facilitate learning on problems with little training data [1] . In the context of CNNs, transfer learning can be implemented at the weight initialization step. The typical initialization strategy for CNNs is to set all weights to small random numbers, and set all biases to either 1 or 0 [24] . An alternative strategy is to pretrain the network network on a complementary task, which potentially has more training data than the target task. This puts the network near a good solution in the target problem, and prevents it from descending into local minima early in the training process [29] . A popular choice for pre-training is the ILSVRC 2012 ImageNet challenge, as it contains over a million training examples of natural images, categorized into 1000 object categories [30] . Features extracted from an ImageNet-trained network have been shown to be effective general-purpose features in a variety of other vision challenges, even without fine-tuning on the target problem [29] . This paper studies three questions about transfer learning for document analysis. First, the paper investigates whether the ImageNet features are general enough to be applied to documents. That is, with no fine-tuning on documents, are generic object-recognition features applicable to document analysis? Second, the paper addresses the question of whether the initialization provided by pre-training on the ILSVRC challenge provides better results than random initialization for documentclassifying CNNs. Third, the paper seeks to investigate the usefulness of transfer learning between document categories; if a CNN is trained with a small number of document categories, are the features learned in that process useful for discriminating between unseen document categories? These questions will be answered in the retrieval tasks to follow.  A. Datasets\\tThe performance of the various proposed approaches was evaluated on two versions of the IIT CDIP Test Collection [25] . This collection contains high resolution images of scanned documents, collected from public records of lawsuits against American tobacco companies. In total, the database has over seven million documents, hand-labelled with tags. Often, the first tag of a document image is indicative of the document\\'s category, but many documents in the dataset have missing or erroneous tags. The first version of the dataset, listed in the results as SmallTobacco, is a sample of 3482 images from the collection, selected and labelled in another work [20] . This version of the dataset was used in a number of related papers [20, 22, 17] . Each image has one of ten labels. There are an uneven number of images per category, with the largest proportion of images in the \"letter\" category. The distribution of categories is representative of the distribution present in the full dataset. The second version of the dataset, listed in the results as BigTobacco, is a new random sample of 25000 images from each of 16 categories in the IIT CDIP collection, for a total of 400000 labelled images. This sample was collected specifically for the present paper. The 16 categories are \"letter\", \"memo\", \"email\", \"filefolder\", \"form\", \"handwritten\", \"invoice\", \"advertisement\", \"budget\", \"news article\", \"presentation\", \"scientific publication\", \"questionnaire\", \"resume\", \"scientific report\", and \"specification\". The selection of categories was guided by earlier work on document categorization [27] , and also by the range of categories present in the already-existing SmallTobacco sample from the same collection. Another factor was the knowledge that CNNs do well with large datasets (e.g., over a million images) [19] , so selection was restricted to categories that were well represented in the dataset. A representative sample of the dataset is shown in Figure 4 . The final categories are not perfectly distinct: many images were originally labelled with multiple tags, which potentially covered several of the categories eventually selected; in this version of the dataset each image is labelled with a single category. Each dataset was split into three subsets for the purposes of experimentation. The SmallTobacco dataset was split as in the related work [20, 22, 17] : 800 images were used for training, 200 for validation, and the remainder for testing. Since this is a small dataset, 10 random splits in those proportions were created; results reflect the median performance from those splits. In the case of retrieval, the median was selected based on mean average precision at the 10th retrieval (mAP@10). The BigTobacco dataset was split in proportions similar to those of ImageNet [30] : 320000 images were used for training, 40000 images for validation, and 40000 images for testing. The validation sets were used to find plateaus in the CNN training process. All results are reported on the test sets.  B. Implementation details\\tThe CNNs were implemented in Caffe [16] . All networks computed an N -way softmax at the top layer, where N is the number of categories being learned. All but two of the CNNs used Caffe\\'s reference ImageNet architecture, which is based on the work of Krizhevsky et al. [19] . This network has five convolutional layers, and three fully-connected layers. The network takes images of size 227 × 227. The full architecture can be written as 227 × 227 − 11 × 11 × 96 − 5 × 5 × 256 − 3 × 3 × 384 − 3 × 3 × 384 − 3 × 3 × 256 − 4096 − 4096 − N . Features were extracted from these CNNs by taking the output of the first fully-connected layer, which has 4096 dimensions. The first network with a different architecture is listed in the results as \"Small holistic CNN\", which uses hyperparameters established in another work on document image analysis [17] . This network has two convolutional layers and three fully-connected layers, with pooling, ReLU, and dropout employed at several stages in between. The network takes as input images of size 150 × 150. The full architecture can be written as 150×150−36×36×20−8×50−1000−1000−N . Fig. 4 . Representative examples from each category of the dataset. For each category, three images are shown in a column. In order, the document classes shown are \"letter\", \"memo\", \"email\", \"filefolder\", \"form\", \"handwritten\", \"invoice\", \"advertisement\", \"budget\", \"news article\", \"presentation\", \"scientific publication\", \"questionnaire\", \"resume\", \"scientific report\", and \"specification\". Notice that although each category has certain distinctive features, there is wide variation within each category, and images from certain pairs of categories could easily be confused (e.g., \"memo\" and \"letter\"). As with the ImageNet networks, features were extracted from this network by taking the output of rhe first fully-connected network, which in this case has 1000 dimensions. The second network with a different architecture is the \"Ensemble of CNNs\" network, which uses vectors extracted from the region-based CNNs to perform classification. Since a vector of length 4096 • 5 is too large to classify, the individual region-based vectors were compressed using principle component analysis (PCA) to 640 dimensions before they were concatenated for classification. The network architecture can be written as 3200 × 4096 × N . For retrieval, features for this approach were created by individually compressing each region\\'s feature vector to 128 dimensions, and then concatenating, resulting in a vector with 640 dimensions. To test the effect of transfer learning between categories of documents, one holistic CNN was trained using only two categories of the BigTobacco dataset: letters and memos. This network was pre-trained on ImageNet. In the results, it is listed as \"LetterMemo CNN\". To extract regions from the images, all images were first resized to 780 × 600. The header region was defined by the first 256 rows of pixels in each image. The footer region was similarly defined by the last 256 rows of pixels in each image. The left body region was delineated by the intersection of the 400 central rows and the 300 left columns; the right body region was symmetrically defined. Every extracted region was resized to 227 × 227 before being used as input. Several state-of-the-art bag of words (BoW) approaches to document representation were also implemented. As in previous work [22] , the words were k-means clustered SURF features [4] . These features were pooled in a spatial pyramid [23] , as well as in various combinations of horizontal and vertical partitions [22] . In the results, we denote these horizontalvertical partitioning schemes with HaVb, where a is the number of times the image was recursively split horizontally, and b is the number of times the image was recursively split vertically. For example, H0V3 has 15 bags: 1 for the original image, 2 for the first vertical split, 4 for the second vertical split, and 8 for the third. For the holistic bag of words, the resulting feature vector has 300 dimensions; H2V0 has 2100 dimensions; H0V3 has 4500 dimensions; H2V3 and L3 both have 6300 dimensions. For classification of the BoW features, a random forest with 500 trees and √ D feature dimensions was trained, where D was the length of the feature vector of the complete (concatenated) bag of words. Three additional features were added as baselines to the featured approaches: the GIST descriptor [28] , average brightness, and ensemble-of-regions average brightness. The GIST descriptor has been shown to perform well on image retrieval tasks [11] , but has not yet been applied to document analysis. Average brightness acts as a baseline for minimum performance; images in this representation are represented with a single value. Ensemble-of-regions average brightness represents document images a vector of five elements, corresponding to the average brightness in each of the regions created for the ensemble of CNNs approach. This is intended to demonstrate on a small scale the basic benefit afforded by region-based analysis. Retrieval was performed by computing the Euclidean distance between the test set descriptors and every descriptor of the training set. The sorted distances were then used to rank the images of the training data, and return a sorted list of documents for each test query. For all approaches with feature vectors larger than 128 dimensions, the vectors were first compressed to 128 dimensions using PCA before they were used for retrieval. This is consistent with the related work [29, 14] ; it not only enables fast retrieval, but also to keeps the task within reasonable memory limits. As in the related work, the feature vectors were L2-normalized before and after PCA compression. Table I shows the classification accuracies of the various BoW approaches, along the various CNNs-based appraoches, on both the SmallTobacco dataset and the BigTobacco dataset.  C. Classification results\\tOn SmallTobacco, the ensemble of region-based CNNs performed better than any other approach, achieving 79.9% classification accuracy. The previous best reported result on this dataset was 65.4% with a randomly initialized \"Small\" CNN, which was approximately replicated here. The holistic network performed only slightly worse than the ensemble of CNNs, suggesting that the holistic CNN may be learning some amount of the information that region-based analysis was expected to add. Interestingly, the \"Small\" CNN compares similarly to the large-sized holistic CNN when both are initialized with random weights. This appears to indicate that the additional parameters in the large network are not necessarily beneficial. Initializing the larger networks with ImageNettrained weights improves performance substantially. Without this initialization, the CNNs perform similarly to (or worse than) the BoW approaches. Between the BoW approaches, the spatial-pyramid-pooled BoW performs best. On BigTobacco, the holistic CNN finetuned from Imagenet performed better than any other approach, including the ensemble of CNNs. This suggests that given sufficient training data, the advantage gained by region-tuned analysis is eliminated by the learning power of the holistic CNN. In these results, the CNN approaches perform far better than the BoW approaches, likely due to the benefit of additional training data. As observed in SmallTobacco, finetuning improves results, although by a smaller margin here than in the small dataset. Comparing the performance of BoW approaches between the two datasets, it is interesting to observe that performance drops by nearly 20%, suggesting that (i) the larger dataset presents a more difficult classification task (likely because it has more categories), and perhaps also (ii) the additional training data does not help these approaches. The confusion matrix for the holistic CNN is shown in Figure 5 . The CNN trained to classify only letters and memos achieved 95% accuracy on that task.  D. Retrieval results\\tRetrieval was measured using mean average precision (mAP). Average precision computes the average value of precision as a function of recall on some interval. Formally, the discrete version of this metric is given by AP = n k=1 (P (k) × rel(k)) number of relevant documents , where k is the rank of the document being retrieved, and rel(k) equals 1 if the document is relevant and 0 otherwise. This metric is sensitive to ranking order, so the score is higher if relevant documents are retrieved before irrelevant documents. Mean average precision is simply the average precision summed over all queries, divided by the number of queries. Retrieved documents were determined to be \"relevant\" if they had the same class label as the query image. Mean average precision for the first 10 retrievals on both datasets are summarized in Figure 6 . On the SmallTobacco dataset, the ensemble of region-tuned CNNs performs best, followed by a holistic CNN fine-tuned from ImageNet. Interestingly, the generic ImageNet descriptor performs well also, exceeding the performance of most other descriptors. Between the BoW approaches, the spatialpyramid-pooled BoW performs best. The GIST descriptor performs approximately as well as the BoW approaches. On the BigTobacco dataset, the holistic CNN performs best, exceeding the ensemble of region-tuned CNNs by a small margin, but exceeding most other approaches by a large margin. The confusion matrix for the finetuned holistic CNN, computed using the first 10 retrievals, is shown in Figure 5 . The BoW approaches are outperformed by every CNN vector, including the generic ImageNet vector. The \"LetterMemo\" CNN slightly improves upon the generic ImageNet descriptor, suggesting that some of the knowledge learned from letters and memos transfers to all 16 categories, but the gain is only marginal. Between the BoW approaches, the spatial-pyramidpooled BoW performs best, as in SmallTobacco. Interestingly, the GIST descriptor exceeds the performance of the BoW descriptors by a large margin on this dataset. Figure 8 shows a representative sample of the retrieval output of the holistic CNN on the BigTobacco dataset. In that figure, it is interesting to notice that in the first row, in which the query image is a memo, the top seven retrievals are all different memos from the same author (with the same signature) as the memo in the query image. The final row is similarly impressive: every document in the top ten retrievals has the same letterhead as the query document, despite variations in the other content, and also despite differing typefaces of the letterhead. There may exist biases in the dataset that lead to such fortunate retrievals (e.g., only a few letterheads, and only a few memo authors), but the results are still remarkable. An additional experiment was performed to measure the effect of PCA compression on mAP@10 performance on the BigTobacco dataset, the results of which are summarized in Figure 7 . Remarkably, the CNN vectors show almost no loss in performance until they are reduced to 16 dimensions. At all levels of compression, the holistic CNN performs exceeds the performance of every other approach.  IV. CONCLUSION\\tThis paper established a new state-of-the-art for document image classification and retrieval, using features learned by deep convolutional neural networks (CNNs). Generic features extracted from a CNN trained on ImageNet exceeded the performance of the state-of-the-art alternatives, and fine-tuning these features on document images pushed results even higher. Interestingly, experiments also showed that given sufficient training data, enforcing region-specific feature-learning is unnecessary; a single CNN trained on entire images performed approximately as well as an ensemble of CNNs trained on specific subregions of document images. In all, this work showed that the CNN approach to document image representation exceeds the power of hand-crafted alternatives. Fig. 6 . Mean average precision at retrievals 1 through 10 for a variety of approaches on the SmallTobacco dataset (left) and the BigTobacco dataset (right). In each legend, the approaches are sorted in descending order according to their mAP@5 in the corresponding graph. Fig. 7 . The effect of PCA reduction on mean average precision at the 10th retrieval (mAP@10). The holistic CNN achieves the highest mAP at all levels of PCA reduction, with remarkably little loss across the first several steps of reduction. to A.W.H.). The authors thank Palomino Systems for helpful discussions. The authors gratefully acknowledge the support of NVIDIA Corporation with the donation of a Tesla K40 GPU used for this research. Fig. 8 . Representative output of the retrieval process. This figure is best viewed on a computer monitor, in a zoomable PDF. Query images are shown in the first column, and the top ten retrievals are shown in the following columns in order. Retrievals from the same class are shown with a green border; retrievals from a different class are shown with a red border. Retrievals from other classes are considered incorrect, but they are often good retrievals nonetheless. '\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  content = re.sub(r\"[^a-zA-Z0-9?,'’‘´`%]+\", ' ', content).strip()\n",
      "ipdb>  content\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Title Evaluation of Deep Convolutional Nets for Document Image Classification and Retrieval Abstract This paper presents a new state of the art for document image classification and retrieval, using features learned by deep convolutional neural networks CNNs In object and scene analysis, deep neural nets are capable of learning a hierarchical chain of abstraction from pixel inputs to concise and descriptive representations The current work explores this capacity in the realm of document analysis, and confirms that this representation strategy is superior to a variety of popular hand crafted alternatives Experiments also show that i features extracted from CNNs are robust to compression, ii CNNs trained on non document images transfer well to document analysis tasks, and iii enforcing region specific feature learning is unnecessary given sufficient training data This work also makes available a new labelled subset of the IIT CDIP collection, containing 400,000 document images across 16 categories, useful for training new CNNs for document analysis I INTRODUCTION Many document types have a distinct visual style For example, letter documents are typically written in a standard format, which is recognizable even at scales where the text is unreadable Motivated by this observation, this paper addresses the problem of document classification and retrieval, based on the visual structure and layout of document images Content based analysis of document images has a number of applications In digital libraries, documents are often stored as images before they are processed by an optical character recognition OCR system, which means basic image analysis is the only available tool for initial indexing and classification 26 As a pre processing stage, document image analysis can facilitate and improve OCR by providing information about each document's visual layout 10 Furthermore, document information that is lost in OCR, such as typeface, graphics, and layout, can only be stored and indexed using images or image descriptors Therefore, image analysis is complementary to OCR at several stages of document analysis The challenge of document image analysis arises from the fact that within each document type, there exists a wide range of visual variability For example, of the correspondence documents shown in Figure 1 , no two documents share the exact same spatial arrangement of header, date, address, body, and signature some of the documents even omit these components entirely This level of intra class variability renders spatial layout analysis difficult, and rigid template matching impossible 8 Another issue is that documents of different categories often have substantial visual similarities For instance, there exist advertisements that look like news articles, and questionnaires that look like forms, and so on From Fig 1 Examples of document images that share the visual style of letter Note that even when the text of these documents is illegible, their style type is clear The documents have similar spatial configurations of various parts addresses and dates typically appear near the top, and signatures typically appear near the bottom, but no two documents share the exact same layout Identifying the style type of these documents is therefore difficult, but can potentially facilitate the extraction of further information the perspective of visual styles , some erroneous retrievals in such circumstances may be justifiable, but in general the task of document image analysis is to effectively classify and retrieve documents despite intra class variability, and interclass similarity Similar challenges appear in other fields, such as object recognition and scene classification In those domains the current state of the art approach involves training a deep convolutional neural network CNN to learn features for the task 24, 19, 29 Inspired by the success of CNNs in other domains, this paper presents an extensive evaluation of CNNs for document classification and retrieval In the end, it is determined that features extracted from deep CNNs exceed the performance of all popular alternative features on both classification and retrieval, by a large margin Experiments are also presented on transfer learning, which demonstrate that CNNs trained on object recognition learn features that are surprisingly effective at describing documents Furthermore, it is found that the deep net strategy is not significantly improved by additional guidance toward region focused features, suggesting that a CNN trained on whole images may already be capable of learning some amount of the information that region based analysis would add A Related Work In the past twenty years of document image analysis, research has oscillated from region based analysis to whole image analysis, and simultaneously, from handcrafted features to machine learned ones The power of region based analysis of document images arXiv 1502 07058v1 cs CV 25 Feb 2015 has been clearly demonstrated in the domain of rigidly structured documents, such as forms and business letters 7, 18 In general, this approach assumes that many document types have a distinct and consistent configuration of visually identifiable components For example, formal business letters typically share a particular spatial configuration of letterhead, date and salutation To some extent, the classification of perfectly rigid documents e g , forms can be reduced to the problem of template matching 7 , and less rigid document types e g , letters can similarly be classified by fitting the geometric configuration of the document's components to one of several template configurations, via geometric transformations 15 The drawback of this approach is that it requires the manual definition of a template for each document type to be categorized Furthermore, the approach is limited to documents for which a template definition is possible For documents with more flexible structures, as considered herein, template based approaches are inapplicable An alternative strategy is to treat document images holistically, or at least in very large regions, and search for discriminative landmark features that may appear anywhere in the document 32, 31 This strategy is sometimes called a bag of visual words approach, since it describes images with a histogram over an orderless vocabulary of features 12 For example, a landmark feature discriminating letters from most other document classes is the salutation finding a salutation in a document potentially through OCR is a good cue that the document is a letter, regardless of that feature's exact spatial position 32 The advantage of holistic analysis is that the resulting representation of documents is invariant to the geometric configuration of the features This approach has therefore been successful in retrieving and classifying a broader range of documents than the templatebased approaches, although the approach is less discriminating in the domain of rigid template documents Recently, there have been attempts to bridge the gap between region based and holistic analyses By concatenating image features pooled at several stages, beginning with a whole image pool and proceeding into smaller and smaller regions, it is possible to build a descriptor that contains both global and local layout characteristics 23 This technique, known as spatial pyramid matching, was initially developed for categorizing scenes, but it has been shown to apply well to documents also, especially if the pooling regions are designed with document categorization in mind 22 For document retrieval, this type of representation represents the current state of the art At the same time, many researchers have replaced handcrafted features and representations with machine learned variants 10, 9 A popular area of research in this domain concerns the task of learning document structure This typically involves training a decision tree to navigate the various possible geometric configurations of fixed features i e landmarks within each document type, toward the goal of structure based classification 10, 21 Most recently, it was shown that the entire pipeline of supervised document image classification, from feature building to decision making, can be learned by a convolutional neural network CNN 17 In that work, the authors reported a remarkable 22% increase in classification accuracy compared to the previous best reported on the same dataset, which had used spatial pyramid matching However, the CNN approach has not yet been applied to document retrieval A shift toward machine learned features has been taking place in other areas of computer vision as well In the object recognition literature, CNNs currently exceed the performance of every other approach by a very large margin 19, 13 The CNN approach has even been shown to apply well to domains for which it was traditionally believed ill suited, such as attribute detection, and fine grained object recognition 29 The success of CNNs in fine grained object recognition is especially relevant to document image analysis, since the two fields share some significant challenges, e g , i the items being distinguished are very similar to each other, and ii there do not exist problem specific datasets large enough to train a powerful CNN without causing it to overfit It makes sense, therefore, to draw inspiration from fine grained object recognition research on how to overcome these challenges Two major points on the training and usage of CNNs can be gleaned from fine grained classification research First, before training the CNN on the data of interest, it is recommended to pre train the network on a much larger related problem, such as the ILSVRC 2012 challenge 30, 13, 6 This regularization technique addresses the issue of overfitting, and allows large CNNs to be effectively applied to small problems Second, in problems where spatial information is important, it is potentially better to encode this information in multiple networks trained on specific regions of interest than in a single network trained on the entire image 6, 5, 33 More generally, this second point suggests that it is unnecessary to rely entirely on machine learning, especially when human knowledge can be easily implemented in the system This paper seeks to investigate whether these insights are relevant to document image analysis Finally, CNNs in other domains have recently been extended to the task of image retrieval After a CNN is trained on classification, the layers of the network can be interpreted as forming a hierarchical chain of abstraction, where the lowest layers contain simple features, and the highest layers contain concise and descriptive representations 24 Therefore, output extracted near the top of a CNN can serve as a feature vector which can be used for any task, including retrieval 29, 3, 14, 2 The present work is the first to apply this idea toward document retrieval B Contributions In the light of previous work, this paper makes the following contributions First, the paper thoroughly evaluates the power of deep CNN features for representing document images Toward this end, the paper presents experiments in CNN design, training, feature processing, and compression Results show that features extracted from CNNs are superior to all handcrafted competitors, and furthermore can be compressed to very short codes with negligible loss in performance Second, this work demonstrates that CNNs trained on nondocument images transfer well to document related tasks Third, this paper explores a strategy of embedding human knowledge of document structure into CNN architectures, by guiding an ensemble of CNNs toward learning region specific features Interestingly, results show little to no improvement in classification and retrieval after this augmentation, suggesting that a basic holistic CNN may be learning region specific features or perhaps better features automatically Finally, this work makes available a new labelled subset of the IIT CDIP collection of tobacco litigation documents 25 , containing 400,000 document images across 16 categories II TECHNICAL APPROACH In structured documents, the layout of text and graphics elements often reflects important information about genre Therefore, documents of a category often share region specific features This paper attempts to learn these informative features by training either a single holistic CNN or an ensemble of region based CNNs Additionally, the paper explores two different initialization strategies the first initializes the weights of the CNNs randomly, and relies entirely on the training process to find the features the second transfers weights from a network trained on another task, and relies on training only to fine tune the features to the domain of document analysis A Holistic convolutional neural networks In most modern implementations of neural networks for computer vision, the network takes as input a square matrix of pixels as input, processes this input through a stack of convolutional layers, then classifies the output of those convolutional layers using two or three fully connected layers 24, 19 A typical network of this type has approximately 60 million trainable parameters this vast representational capacity, along with the hierarchical organization of that representation, is assumed to be responsible for the network's power as a featurebuilder and classifier 24 Convolutional neural network activations are not geometrically invariant In applications such as object detection, this is sometimes an inconvenient property Much work has been done to add spatial invariance to CNNs, e g , by jittering the training data to add geometric variants of each image in the dataset 24 , or by altering the architecture of the CNN to process the input at multiple scales and positions 14 For document analysis, however, spatial specificity in CNN activations may be beneficial For example, it makes sense to treat the header region of a document differently than the footer region By design, a holistic CNN trained on a dataset of well aligned document images should be capable of learning region specific features automatically Typically, CNNs are trained to perform a classification task, but a CNN trained on classification can be exploited to perform retrieval also It has been found that the activation patterns near the top of a deep CNN make very descriptive feature vectors 29 These feature vectors are high dimensional e g , 4096 dimensions , but their dimensionality can be reduced significantly via principal component analysis e g , to 128 dimensions without significantly affecting their discriminative power 3 Retrieval involves computing the Euclidean distance between a query descriptor and every descriptor of the training set The sorted distances are then used to rank the images of the training data, and return a sorted list of documents B Region based guidance Accounting for the possibility that a holistic CNN may not take advantage of region specific information in document images, guiding CNNs to learn region based features may aid fine grained discrimination by isolating subtle region specific appearance differences between document categories Consider the example of discriminating letters and memos, as illustrated in Figure 2 These two categories only consistently differ at the address section memos have a short To and From , and letters have full addresses It is possible that a holistic CNN will learn this automatically, but training a CNN to classify documents using only this region increases the likelihood that this feature will be learned The idea of this approach is to devote one CNN to each region of interest, and therefore force multiple CNNs to learn rich region dependent representations, from which features can be extracted and combined Any number of region specific CNNs can be used in this approach In this work, a total of five CNNs are used Four of these are region tuned, placed at the header, left body, right body, and footer of the document images The fifth is a holistic CNN, trained on the entire images The final region based representation of document images is built by combining and compressing features extracted from each region tuned CNN The final descriptor is represented by the concatenation of region specific features 0 , 1 , , n , where 0 represents the PCA compressed feature vector extracted from the holistic CNN, and 1 , , n represent the analogous vectors extracted from regions 1 through n Figure 3 illustrates the full process of this vector's construction For retrieval, this new vector is used directly For classification, a new fully connected network is trained to classify the concatenated vector C Transfer learning The goal of transfer learning is to take advantage of shared structure in related problems, to facilitate learning on problems with little training data 1 In the context of CNNs, transfer learning can be implemented at the weight initialization step The typical initialization strategy for CNNs is to set all weights to small random numbers, and set all biases to either 1 or 0 24 An alternative strategy is to pretrain the network network on a complementary task, which potentially has more training data than the target task This puts the network near a good solution in the target problem, and prevents it from descending into local minima early in the training process 29 A popular choice for pre training is the ILSVRC 2012 ImageNet challenge, as it contains over a million training examples of natural images, categorized into 1000 object categories 30 Features extracted from an ImageNet trained network have been shown to be effective general purpose features in a variety of other vision challenges, even without fine tuning on the target problem 29 This paper studies three questions about transfer learning for document analysis First, the paper investigates whether the ImageNet features are general enough to be applied to documents That is, with no fine tuning on documents, are generic object recognition features applicable to document analysis? Second, the paper addresses the question of whether the initialization provided by pre training on the ILSVRC challenge provides better results than random initialization for documentclassifying CNNs Third, the paper seeks to investigate the usefulness of transfer learning between document categories if a CNN is trained with a small number of document categories, are the features learned in that process useful for discriminating between unseen document categories? These questions will be answered in the retrieval tasks to follow A Datasets The performance of the various proposed approaches was evaluated on two versions of the IIT CDIP Test Collection 25 This collection contains high resolution images of scanned documents, collected from public records of lawsuits against American tobacco companies In total, the database has over seven million documents, hand labelled with tags Often, the first tag of a document image is indicative of the document's category, but many documents in the dataset have missing or erroneous tags The first version of the dataset, listed in the results as SmallTobacco, is a sample of 3482 images from the collection, selected and labelled in another work 20 This version of the dataset was used in a number of related papers 20, 22, 17 Each image has one of ten labels There are an uneven number of images per category, with the largest proportion of images in the letter category The distribution of categories is representative of the distribution present in the full dataset The second version of the dataset, listed in the results as BigTobacco, is a new random sample of 25000 images from each of 16 categories in the IIT CDIP collection, for a total of 400000 labelled images This sample was collected specifically for the present paper The 16 categories are letter , memo , email , filefolder , form , handwritten , invoice , advertisement , budget , news article , presentation , scientific publication , questionnaire , resume , scientific report , and specification The selection of categories was guided by earlier work on document categorization 27 , and also by the range of categories present in the already existing SmallTobacco sample from the same collection Another factor was the knowledge that CNNs do well with large datasets e g , over a million images 19 , so selection was restricted to categories that were well represented in the dataset A representative sample of the dataset is shown in Figure 4 The final categories are not perfectly distinct many images were originally labelled with multiple tags, which potentially covered several of the categories eventually selected in this version of the dataset each image is labelled with a single category Each dataset was split into three subsets for the purposes of experimentation The SmallTobacco dataset was split as in the related work 20, 22, 17 800 images were used for training, 200 for validation, and the remainder for testing Since this is a small dataset, 10 random splits in those proportions were created results reflect the median performance from those splits In the case of retrieval, the median was selected based on mean average precision at the 10th retrieval mAP 10 The BigTobacco dataset was split in proportions similar to those of ImageNet 30 320000 images were used for training, 40000 images for validation, and 40000 images for testing The validation sets were used to find plateaus in the CNN training process All results are reported on the test sets B Implementation details The CNNs were implemented in Caffe 16 All networks computed an N way softmax at the top layer, where N is the number of categories being learned All but two of the CNNs used Caffe's reference ImageNet architecture, which is based on the work of Krizhevsky et al 19 This network has five convolutional layers, and three fully connected layers The network takes images of size 227 227 The full architecture can be written as 227 227 11 11 96 5 5 256 3 3 384 3 3 384 3 3 256 4096 4096 N Features were extracted from these CNNs by taking the output of the first fully connected layer, which has 4096 dimensions The first network with a different architecture is listed in the results as Small holistic CNN , which uses hyperparameters established in another work on document image analysis 17 This network has two convolutional layers and three fully connected layers, with pooling, ReLU, and dropout employed at several stages in between The network takes as input images of size 150 150 The full architecture can be written as 150 150 36 36 20 8 50 1000 1000 N Fig 4 Representative examples from each category of the dataset For each category, three images are shown in a column In order, the document classes shown are letter , memo , email , filefolder , form , handwritten , invoice , advertisement , budget , news article , presentation , scientific publication , questionnaire , resume , scientific report , and specification Notice that although each category has certain distinctive features, there is wide variation within each category, and images from certain pairs of categories could easily be confused e g , memo and letter As with the ImageNet networks, features were extracted from this network by taking the output of rhe first fully connected network, which in this case has 1000 dimensions The second network with a different architecture is the Ensemble of CNNs network, which uses vectors extracted from the region based CNNs to perform classification Since a vector of length 4096 5 is too large to classify, the individual region based vectors were compressed using principle component analysis PCA to 640 dimensions before they were concatenated for classification The network architecture can be written as 3200 4096 N For retrieval, features for this approach were created by individually compressing each region's feature vector to 128 dimensions, and then concatenating, resulting in a vector with 640 dimensions To test the effect of transfer learning between categories of documents, one holistic CNN was trained using only two categories of the BigTobacco dataset letters and memos This network was pre trained on ImageNet In the results, it is listed as LetterMemo CNN To extract regions from the images, all images were first resized to 780 600 The header region was defined by the first 256 rows of pixels in each image The footer region was similarly defined by the last 256 rows of pixels in each image The left body region was delineated by the intersection of the 400 central rows and the 300 left columns the right body region was symmetrically defined Every extracted region was resized to 227 227 before being used as input Several state of the art bag of words BoW approaches to document representation were also implemented As in previous work 22 , the words were k means clustered SURF features 4 These features were pooled in a spatial pyramid 23 , as well as in various combinations of horizontal and vertical partitions 22 In the results, we denote these horizontalvertical partitioning schemes with HaVb, where a is the number of times the image was recursively split horizontally, and b is the number of times the image was recursively split vertically For example, H0V3 has 15 bags 1 for the original image, 2 for the first vertical split, 4 for the second vertical split, and 8 for the third For the holistic bag of words, the resulting feature vector has 300 dimensions H2V0 has 2100 dimensions H0V3 has 4500 dimensions H2V3 and L3 both have 6300 dimensions For classification of the BoW features, a random forest with 500 trees and D feature dimensions was trained, where D was the length of the feature vector of the complete concatenated bag of words Three additional features were added as baselines to the featured approaches the GIST descriptor 28 , average brightness, and ensemble of regions average brightness The GIST descriptor has been shown to perform well on image retrieval tasks 11 , but has not yet been applied to document analysis Average brightness acts as a baseline for minimum performance images in this representation are represented with a single value Ensemble of regions average brightness represents document images a vector of five elements, corresponding to the average brightness in each of the regions created for the ensemble of CNNs approach This is intended to demonstrate on a small scale the basic benefit afforded by region based analysis Retrieval was performed by computing the Euclidean distance between the test set descriptors and every descriptor of the training set The sorted distances were then used to rank the images of the training data, and return a sorted list of documents for each test query For all approaches with feature vectors larger than 128 dimensions, the vectors were first compressed to 128 dimensions using PCA before they were used for retrieval This is consistent with the related work 29, 14 it not only enables fast retrieval, but also to keeps the task within reasonable memory limits As in the related work, the feature vectors were L2 normalized before and after PCA compression Table I shows the classification accuracies of the various BoW approaches, along the various CNNs based appraoches, on both the SmallTobacco dataset and the BigTobacco dataset C Classification results On SmallTobacco, the ensemble of region based CNNs performed better than any other approach, achieving 79 9% classification accuracy The previous best reported result on this dataset was 65 4% with a randomly initialized Small CNN, which was approximately replicated here The holistic network performed only slightly worse than the ensemble of CNNs, suggesting that the holistic CNN may be learning some amount of the information that region based analysis was expected to add Interestingly, the Small CNN compares similarly to the large sized holistic CNN when both are initialized with random weights This appears to indicate that the additional parameters in the large network are not necessarily beneficial Initializing the larger networks with ImageNettrained weights improves performance substantially Without this initialization, the CNNs perform similarly to or worse than the BoW approaches Between the BoW approaches, the spatial pyramid pooled BoW performs best On BigTobacco, the holistic CNN finetuned from Imagenet performed better than any other approach, including the ensemble of CNNs This suggests that given sufficient training data, the advantage gained by region tuned analysis is eliminated by the learning power of the holistic CNN In these results, the CNN approaches perform far better than the BoW approaches, likely due to the benefit of additional training data As observed in SmallTobacco, finetuning improves results, although by a smaller margin here than in the small dataset Comparing the performance of BoW approaches between the two datasets, it is interesting to observe that performance drops by nearly 20%, suggesting that i the larger dataset presents a more difficult classification task likely because it has more categories , and perhaps also ii the additional training data does not help these approaches The confusion matrix for the holistic CNN is shown in Figure 5 The CNN trained to classify only letters and memos achieved 95% accuracy on that task D Retrieval results Retrieval was measured using mean average precision mAP Average precision computes the average value of precision as a function of recall on some interval Formally, the discrete version of this metric is given by AP n k 1 P k rel k number of relevant documents , where k is the rank of the document being retrieved, and rel k equals 1 if the document is relevant and 0 otherwise This metric is sensitive to ranking order, so the score is higher if relevant documents are retrieved before irrelevant documents Mean average precision is simply the average precision summed over all queries, divided by the number of queries Retrieved documents were determined to be relevant if they had the same class label as the query image Mean average precision for the first 10 retrievals on both datasets are summarized in Figure 6 On the SmallTobacco dataset, the ensemble of region tuned CNNs performs best, followed by a holistic CNN fine tuned from ImageNet Interestingly, the generic ImageNet descriptor performs well also, exceeding the performance of most other descriptors Between the BoW approaches, the spatialpyramid pooled BoW performs best The GIST descriptor performs approximately as well as the BoW approaches On the BigTobacco dataset, the holistic CNN performs best, exceeding the ensemble of region tuned CNNs by a small margin, but exceeding most other approaches by a large margin The confusion matrix for the finetuned holistic CNN, computed using the first 10 retrievals, is shown in Figure 5 The BoW approaches are outperformed by every CNN vector, including the generic ImageNet vector The LetterMemo CNN slightly improves upon the generic ImageNet descriptor, suggesting that some of the knowledge learned from letters and memos transfers to all 16 categories, but the gain is only marginal Between the BoW approaches, the spatial pyramidpooled BoW performs best, as in SmallTobacco Interestingly, the GIST descriptor exceeds the performance of the BoW descriptors by a large margin on this dataset Figure 8 shows a representative sample of the retrieval output of the holistic CNN on the BigTobacco dataset In that figure, it is interesting to notice that in the first row, in which the query image is a memo, the top seven retrievals are all different memos from the same author with the same signature as the memo in the query image The final row is similarly impressive every document in the top ten retrievals has the same letterhead as the query document, despite variations in the other content, and also despite differing typefaces of the letterhead There may exist biases in the dataset that lead to such fortunate retrievals e g , only a few letterheads, and only a few memo authors , but the results are still remarkable An additional experiment was performed to measure the effect of PCA compression on mAP 10 performance on the BigTobacco dataset, the results of which are summarized in Figure 7 Remarkably, the CNN vectors show almost no loss in performance until they are reduced to 16 dimensions At all levels of compression, the holistic CNN performs exceeds the performance of every other approach IV CONCLUSION This paper established a new state of the art for document image classification and retrieval, using features learned by deep convolutional neural networks CNNs Generic features extracted from a CNN trained on ImageNet exceeded the performance of the state of the art alternatives, and fine tuning these features on document images pushed results even higher Interestingly, experiments also showed that given sufficient training data, enforcing region specific feature learning is unnecessary a single CNN trained on entire images performed approximately as well as an ensemble of CNNs trained on specific subregions of document images In all, this work showed that the CNN approach to document image representation exceeds the power of hand crafted alternatives Fig 6 Mean average precision at retrievals 1 through 10 for a variety of approaches on the SmallTobacco dataset left and the BigTobacco dataset right In each legend, the approaches are sorted in descending order according to their mAP 5 in the corresponding graph Fig 7 The effect of PCA reduction on mean average precision at the 10th retrieval mAP 10 The holistic CNN achieves the highest mAP at all levels of PCA reduction, with remarkably little loss across the first several steps of reduction to A W H The authors thank Palomino Systems for helpful discussions The authors gratefully acknowledge the support of NVIDIA Corporation with the donation of a Tesla K40 GPU used for this research Fig 8 Representative output of the retrieval process This figure is best viewed on a computer monitor, in a zoomable PDF Query images are shown in the first column, and the top ten retrievals are shown in the following columns in order Retrievals from the same class are shown with a green border retrievals from a different class are shown with a red border Retrievals from other classes are considered incorrect, but they are often good retrievals nonetheless\"\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-303-da6b65db3768>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m                          \u001b[0mpath_to_TDM_taxonomy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../data/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_parsed_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_grobid_full_txt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                          \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"../data/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m                         leaderboard_threshold=5, num_negative_instances=10, allowed_unknown=10)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-301-3e5d9fbdc62d>\u001b[0m in \u001b[0;36mcreate_training_data\u001b[0;34m(path_to_resultsAnnotation, path_to_TDM_taxonomy, path_parsed_files, output_dir, test_set_portion, leaderboard_threshold, num_negative_instances, allowed_unknown)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m#         content = ' '.join(\"None\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mpaper_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mnot_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-301-3e5d9fbdc62d>\u001b[0m in \u001b[0;36mcreate_training_data\u001b[0;34m(path_to_resultsAnnotation, path_to_TDM_taxonomy, path_parsed_files, output_dir, test_set_portion, leaderboard_threshold, num_negative_instances, allowed_unknown)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m#         content = ' '.join(\"None\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mpaper_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mnot_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/python3/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "create_training_data(path_to_resultsAnnotation=\"../data/\", \\\n",
    "                         path_to_TDM_taxonomy=\"../data/\", path_parsed_files=path_grobid_full_txt,\n",
    "                         output_dir=\"../data/\",\n",
    "                        leaderboard_threshold=5, num_negative_instances=10, allowed_unknown=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d763f115-b3be-4bc3-b152-5898c4f01a8e",
   "metadata": {},
   "source": [
    "## View created data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "0d879524-e2af-43a3-9bb4-4c509c20a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "7756db66-b65c-4cc2-b730-516e42c30560",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f\"../data/train.tsv\", \n",
    "                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "6b869068-b1ba-4bba-8db2-8c1bdf962ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>1903.10176v3</td>\n",
       "      <td>Image Super-Resolution#Set5 - 8x upscaling#PSNR</td>\n",
       "      <td>TitleDeepREDDeepImagePriorPoweredbyREDAbstract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>1903.10176v3</td>\n",
       "      <td>Image Super-Resolution#Set14 - 8x upscaling#PSNR</td>\n",
       "      <td>TitleDeepREDDeepImagePriorPoweredbyREDAbstract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>1903.10176v3</td>\n",
       "      <td>Image Super-Resolution#Set5 - 4x upscaling#PSNR</td>\n",
       "      <td>TitleDeepREDDeepImagePriorPoweredbyREDAbstract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>1903.10176v3</td>\n",
       "      <td>Image Super-Resolution#Set14 - 4x upscaling#PSNR</td>\n",
       "      <td>TitleDeepREDDeepImagePriorPoweredbyREDAbstract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>1903.10176v3</td>\n",
       "      <td>3D Absolute Human Pose Estimation#Human3.6M#MRPE</td>\n",
       "      <td>TitleDeepREDDeepImagePriorPoweredbyREDAbstract...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label         title                                               TDM  \\\n",
       "0   True  1903.10176v3   Image Super-Resolution#Set5 - 8x upscaling#PSNR   \n",
       "1   True  1903.10176v3  Image Super-Resolution#Set14 - 8x upscaling#PSNR   \n",
       "2   True  1903.10176v3   Image Super-Resolution#Set5 - 4x upscaling#PSNR   \n",
       "3   True  1903.10176v3  Image Super-Resolution#Set14 - 4x upscaling#PSNR   \n",
       "4  False  1903.10176v3  3D Absolute Human Pose Estimation#Human3.6M#MRPE   \n",
       "\n",
       "                                             Context  \n",
       "0  TitleDeepREDDeepImagePriorPoweredbyREDAbstract...  \n",
       "1  TitleDeepREDDeepImagePriorPoweredbyREDAbstract...  \n",
       "2  TitleDeepREDDeepImagePriorPoweredbyREDAbstract...  \n",
       "3  TitleDeepREDDeepImagePriorPoweredbyREDAbstract...  \n",
       "4  TitleDeepREDDeepImagePriorPoweredbyREDAbstract...  "
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "b85ef0b6-7c67-4109-aec4-7e570c49bab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">False</th>\n",
       "      <th rowspan=\"5\" valign=\"top\">1903.11816v1</th>\n",
       "      <th>3D Face Reconstruction#AFLW2000-3D#Mean NME</th>\n",
       "      <th>Title</th>\n",
       "      <th>FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation Abstract:</th>\n",
       "      <th>Modern approaches for semantic segmentation usually employ dilated convolutions in the backbone to extract highresolution feature maps, which brings heavy computation complexity and memory footprint. To replace the time and memory consuming dilated convolutions, we propose a novel joint upsampling module named Joint Pyramid Upsampling (JPU) by formulating the task of extracting highresolution feature maps into a joint upsampling problem. With the proposed JPU, our method reduces the computation complexity by more than three times without performance loss. Experiments show that JPU is superior to other upsampling modules, which can be plugged into many existing approaches to reduce computation complexity and improve performance. By replacing dilated convolutions with the proposed JPU module, our method achieves the state-of-the-art performance in Pascal Context dataset (mIoU of 53.13%) and ADE20K dataset (final score of 0.5584) while running 3 times faster. Code is available in https://github.com/wuhuikai/FastFCN . Introduction</th>\n",
       "      <th>Semantic segmentation [23, 40, 4] is one of the fundamental tasks in computer vision, with the goal of assigning a semantic label to each pixel of an image. Modern approaches usually employ a Fully Convolution Network (FCN) [22] to address this task, achieving tremendous success among several segmentation benchmarks. The original FCN is proposed by Long et al. [22] , which is transformed from a Convolutional Neural Network (CNN) [16, 15] designed for image classification. Inheriting from the design for image classification, the original FCN downsamples the input image progressively by stride convolutions and/or spatial pooling layers, resulting in a final feature map in low resolution. Although the final feature map encodes rich semantic information, the fine image structure information is lost, leading to inaccurate predictions around the object boundaries. As shown in Figure 1a , the original FCN typically downsamples the input image 5 times, reducing the spatial resolution of the final feature map by a factor of 32. To obtain a high-resolution final feature map, [3, 28, 18, 30, 27] employ the original FCN as the encoder to capture high-level semantic information, and a decoder is designed to gradually recover the spatial information by combining multi-level feature maps from the encoder. As shown in Figure 1b , we term such methods EncoderDecoder, of which the final prediction generated by the decoder is in high resolution. Alternatively, DeepLab [5] removes the last two downsampling operations from the original FCN and introduces dilated (atrous) convolutions to maintain the receptive field of view unchanged. 1 Following DeepLab, [38, 6, 36] employ a multi-scale context module on top of the final feature map, outperforming most EncoderDecoder methods significantly on several segmentation benchmarks. As shown in Figure 1c , the spatial resolution of the last feature map in DilatedFCN is 4 times larger than that in the original FCN, thus maintaining more structure and location information. The dilated convolutions play an important role in maintaining the spatial resolution of the final feature map, leading to superior performance compared to most methods in EncoderDecoder. However, the introduced dilated convolutions bring heavy computation complexity and memory footprint, which limit the usage in many real-time applications. Taking ResNet-101 [13] as an example, compared to the original FCN, 23 residual blocks (69 convolution layers) in DilatedFCN require to take 4 times more computation resources and memory usages, and 3 residual blocks (9 convolution layers) need to take 16 times more resources. We aim at tackling the aforementioned issue caused by dilated convolutions in this paper. To achieve this, we propose a novel joint upsampling module to replace the time and memory consuming dilated convolutions, namely Joint Pyramid Upsampling (JPU). As a result, our method employs the original FCN as the backbone while applying JPU to upsample the low-resolution final feature map with output stride (OS) 32, resulting in a high-resolution feature map (OS=8). Accordingly, the computation time and memory footprint of the whole segmentation framework is dramatically reduced. Meanwhile, there's no performance loss when replacing the dilated convolutions with the proposed JPU. We attribute this to the ability of JPU to exploit multiscale context across multi-level feature maps. To validate the effectiveness of our method, we first conduct a systematical experiment, showing that the proposed JPU can replace dilated convolutions in several popular approaches without performance loss. We then test the proposed method on several segmentation benchmarks. Results show that our method achieves the state-of-the-art performance while running more than 3 times faster. Concretely, we outperform all the baselines on Pascal Context dataset [23] by a large margin, which achieves the state-ofthe-art performance with mIoU of 53.13%. On ADE20K dataset [40] , we obtain the mIoU of 42.75% with ResNet-50 as the backbone, which sets a new record on the val set. Moreover, our method with ResNet-101 achieves the stateof-the-art performance in the test set of ADE20K dataset. In summary, our contributions are three folds, which are: (1) We propose a computationally efficient joint upsampling module named JPU to replace the time and memory consuming dilated convolutions in the backbone. (2) Based on the proposed JPU, the computation time and memory footprint of the whole segmentation framework can be reduced by a factor of more than 3 and meanwhile achieves better performance. (3) Our method achieves the new state-ofthe-art performance in both Pascal Context dataset (mIoU of 53.13%) and ADE20K dataset (mIoU of 42.75% with ResNet-50 as the backbone on the val set and final score of 0.5584 with ResNet-101 on the test set).  Related Work</th>\n",
       "      <th>In this section, we first give an overview on methods for semantic segmentation, which can be categorized into two directions. We then introduce some related works on upsampling.  Semantic Segmentation</th>\n",
       "      <th>FCNs [22] have achieved huge success in semantic segmentation. Following FCN, there're two prominent directions, namely DilatedFCN and EncoderDecoder. Dilated-FCNs [11, 34, 7, 6, 38, 36, 5] utilize dilated convolutions to keep the receptive field of view and employ a multi-scale context module to process high-level feature maps. Alternatively, EncoderDecoders [24, 28, 18, 1, 26, 12, 33, 37] propose to utilize an encoder to extract multi-level feature maps, which are then combined into the final prediction by a decoder. DilatedFCN In order to capture multi-scale context information on the high-resolution final feature map, PSP-Net [38] performs pooling operations at multiple grid scales while DeepLabV3 [6] employs parallel atrous convolutions with different rates named ASPP. Alternatively, EncNet [36] utilizes the Context Encoding Module to capture global contextual information. Differently, our method proposes a joint upsampling module named JPU to replace the dilated convolutions in the backbone of DilatedFCNs, which can JPU Encoding/PSP/ASPP Head C o n v 5 , 3 2 x C o n v 4 , 1 6 x C o n v 3 , 8 x C o n v 2 , 4 x C o n v 1 , 2 x 8 x 8 x Figure 2 : Framework Overview of Our Method. Our method employs the same backbone as the original FCN. After the backbone, a novel upsampling module named Joint Pyramid Upsampling (JPU) is proposed, which takes the last three feature maps as the inputs and generates a high-resolution feature map. A multi-scale/global context module is then employed to produce the final label map. Best viewed in color. reduce computation complexity dramatically without performance loss. EncoderDecoder To gradually recover the spatial information, [28] introduces skip connections to construct U-Net, which combines the encoder features and the corresponding decoder activations. [18] proposes a multipath refinement network, which explicitly exploits all the information available along the down-sampling process. DeepLabV3+ [8] combines the advantages of DilatedFCN and EncoderDecoder, which employs DeepLabV3 as the encoder. Our method is complementary to DeepLabV3+, which can reduce the computation overload of DeepLabV3 without performance loss.  Upsampling</th>\n",
       "      <th>In our method, we propose a module to upsample a lowresolution feature map given high-resolution feature maps as guidance, which is closely related to joint upsampling as well as data-dependent upsampling. Joint Upsampling In the literature of image processing, joint upsampling aims at leveraging the guidance image as a prior and transferring the structural details from the guidance image to the target image. [17] constructs a joint filter based on CNNs, which learns to recover the structure details in the guidance image. [31] proposes an end-to-end trainable guided filtering module, which upsamples a lowresolution image conditionally. Our method is related to the aforementioned approaches. However, the proposed JPU is designed for processing feature maps with a large number of channels while [17, 31] are specially designed for pro-cessing 3-channel images, which fail to capture the complex relations in high dimensional feature maps. Besides, the motivation and target of our method is completely different. Data-Dependent Upsampling DUpsampling [29] is also related to our method, which takes advantages of the redundancy in the segmentation label space and is able to recover the pixel-wise prediction from low-resolution outputs of CNNs. Compared to our method, DUpsampling has a strong dependency on the label space, which generalizes poorly to a larger or more complex label space.  Method</th>\n",
       "      <th>In this section, we first introduce the most popular methods for semantic segmentation, named DilatedFCNs. We then reform the architecture of DilatedFCNs with a novel joint upsampling module, Joint Pyramid Upsampling (JPU). Finally, we discuss the proposed JPU in details, before which joint upsampling, dilated convolution and stride convolution are briefly introduced.  DilatedFCN</th>\n",
       "      <th>To exploit Deep CNNs in semantic segmentation, Long et al. [22] transform the CNN designed for image classification into FCN. Taking ResNet-101 as an example, the original CNN contains 5 convolution stages, a global average pooling layer and a linear layer. To construct an FCN, the global average pooling layer and the linear layer are replaced by a convolution layer, which is used to generate the final label map, as shown in Figure 1a . Between each two consecutive convolution stages, stride convolutions and/or spatial pooling layers are employed, resulting in 5 feature maps with gradually reduced spatial resolutions. The spatial resolution of the last feature map in FCN is reduced by a factor of 32, leading to inaccurate predictions about the locations and details. To obtain a final feature map with high resolution, DeepLab [5] removes the downsampling operations before the last two feature maps, as shown in Figure 1c . Besides, the convolution layers inside the last two convolution stages are replaced by dilated convolutions to maintain the receptive field of view, thus named Dilated-FCN. As a result, the resolution of the last feature map is reduced by a factor of 8, which reserves more location and detail information. Following DeepLab, [38, 6] propose a multi-scale context module to capture context information from the last feature map, achieving tremendous success in several segmentation benchmarks.  The Framework of Our Method</th>\n",
       "      <th>To obtain a high-resolution final feature map, methods in DilatedFCN remove the last two downsampling operations from the original FCN, which bring in heavy computation complexity and memory footprint due to the enlarged feature maps. In this paper, we aim at seeking an alternative way to approximate the final feature map of DilatedFCN without computation and memory overload. Meanwhile, we expect the performance of our method to be as good as that of the original DilatedFCNs. To achieve this, we first put back all the stride convolutions removed by DilatedFCN, while replacing all the dilated convolutions with regular convolution layers. As shown in Figure 2 , the backbone of our method is the same as that of the original FCN, where the spatial resolutions of the five feature maps (Conv1−Conv5) are gradually reduced by a factor of 2. To obtain a feature map similar to the final feature map of DilatedFCN, we propose a novel module named Joint Pyramid Upsampling (JPU), which takes the last three feature maps (Conv3−Conv5) as inputs. Then a multi-scale context module (PSP [38] /ASPP [6] ) or a global context module (Encoding [36] ) is employed to produce the final predictions. Compared to DilatedFCN, our method takes 4 times fewer computation and memory resources in 23 residual blocks (69 layers) and 16 times fewer in 3 blocks (9 layers) when the backbone is ResNet-101. Thus, our method runs much faster than DilatedFCN while consuming less memory.  Joint Pyramid Upsampling</th>\n",
       "      <th>The proposed JPU is designed for generating a feature map that approximates the activations of the final feature map from the backbone of DilatedFCN. Such a problem can be reformulated into joint upsampling, which is then resolved by a CNN designed for this task.  Background</th>\n",
       "      <th>Joint Upsampling Given a low-resolution target image and a high-resolution guidance image, joint upsampling aims at generating a high-resolution target image by transferring details and structures from the guidance image. Generally, the low-resolution target image y l is generated by employing a transformation f (•) on the low-resolution guidance image x l , i.e. y l = f (x l ). Given x l and y l , we are required to obtain a transformationf (  y h =f (x h ), wheref (•) = argmin h(•)∈H ||y l − h(x l )||, (1) where H is a set of all possible transformation functions, and || • || is a pre-defined distance metric. Dilated Convolution Dilated convolution is introduced in DeepLab [5] for obtaining high-resolution feature maps while maintaining the receptive field of view. Figure 3a gives an illustration of the dilated convolution in 1D (dilation rate = 2), which can be divided into the following three steps: (1) split the input feature f in into two groups f 0 in and f 1 in according to the parity of the index, (2) process each feature with the same convolution layer, resulting in f 0 out and f 1 out , and (3) merge the two generated features interlaced to obtain the output feature f out . Stride Convolution Stride convolution is proposed to transform the input feature into an output feature with reduced spatial resolution, which is equivalent to the following two steps as shown in Figure 3b: (1) process the input feature f in with a regular convolution to obtain the intermediate feature f m , and (2) remove the elements with an odd index, resulting in f out .  Reformulating into Joint Upsampling</th>\n",
       "      <th>The differences between the backbone of our method and DilatedFCN lie on the last two convolution stages. Taking the 4th convolution stage (Conv4) as an example, in Dilat-edFCN, the input feature map is first processed by a regular convolution layer, followed by a series of dilated convolutions (d=2). Differently, our method first processes the input feature map with a stride convolution (s=2), and then employs several regular convolutions to generate the output. Formally, given the input feature map x, the output feature map y d in DilatedFCN is obtained as follows: Fig 3a) , = ! \"# ! $%&amp; ! \"# ! $%&amp; ! \"# ' ! \"# ( ! $%&amp; ' ! $%&amp; ( Split Merge Conv DilatedConv, d=2 y d = x → C r → C d → ...... → C d n = x → C r → SC r M → ...... → SC r M n (Fig 3a) = x → C r → S → C r → ...... → C r n → M = y m → S → C n r → M = {y 0 m , y 1 m } → C n r → M ( while in our method, the output feature map y s is generated as follows: (Fig 3b) . y s = x → C s → C r → ...... → C r n = x → C r → R → C r → ...... → C r n (Fig 3b) = y m → R → C n r = y 0 m → C n r (3) C r , C d , and C s represent a regular/dilated/stride convolution respectively, and C n r is n layers of regular convolutions. S, M and R are split, merge, and reduce operations in Figure 3 , where adjacent S and M operations can be canceled out. Notably, the convolutions in Equations 2 and 3 are in 1D, which is for simplicity. Similar results can be obtained for 2D convolutions. The aforementioned equations show that y s and y d can be obtained with the same function C n r with different inputs: y 0 m and y m , where the former is downsampled from the latter. Thus, given x and y s , the feature map y that ap-proximates y d can be obtained as follows: EQUATION which is the same as the joint upsampling problem defined in Equation 1. Similar conclusions can be easily obtained for the 5th convolution stage (Conv5).  Solving with CNNs</th>\n",
       "      <th>Equation 4 is an optimization problem, which takes lots of time to converge through the iterative gradient descent. Alternatively, we propose to approximate the optimization process with a CNN module. To achieve this, we first require to generate y m given x, as shown in Equation 4. Then, features from y 0 m and y s need to be gathered for learning the mappingĥ. Finally, a convolution block is required to transform the gathered features into the final prediction y. Following the aforementioned analysis, we design the JPU module as in Figure 4 . Concretely, each input feature map is firstly processed by a regular convolution block (Fig. 4a) , which is designed for (1) generating y m given x, and (2) transforming f m into an embedding space with reduced dimensions. As a result, all the input features are mapped into the same space, which enables a better fusion and reduces the computation complexity. Then, the generated feature maps are upsampled and concatenated, resulting in y c (Fig. 4b) . Four separable convolutions [14, 9] the convolution with dilation rate 1 is employed to capture the relation between y 0 m and the rest part of y m , as shown by the blue box in Figure 5 . Alternatively, the convolutions with dilation rate 2, 4 and 8 are designed for learning the mappingĥ to transform y 0 m into y s , as shown by the green boxes in Figure 5 . Thus, JPU can extract multi-scale context information from multi-level feature maps, which leads to a better performance. This is significantly different from ASPP [6] , which only exploit the information in the last feature map. The extracted features encode the mapping between y 0 m and y s as well as the relation between y 0 m and the rest part of y m . Thus, another regular convolution block is employed, which transforms the features into the final predictions ( Fig. 4c) . Notably, the proposed JPU module solves two closely related joint upsampling problems jointly, which are (1) upsampling Conv4 based on Conv3 (the 4th convolution stage), and (2) upscaling Conv5 with the guidance of the enlarged Conv4 (the 5th convolution stage).  Experiment</th>\n",
       "      <td>In this section, we first introduce the datase...</td>\n",
       "      <td>Dataset Pascal Context dataset [23] is based o...</td>\n",
       "      <td>To show the effectiveness of the proposed meth...</td>\n",
       "      <td>Pascal Context In Table 1 , our method employs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3D Face Reconstruction#AFLW2000-3D#NME</th>\n",
       "      <th>Title</th>\n",
       "      <th>FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation Abstract:</th>\n",
       "      <th>Modern approaches for semantic segmentation usually employ dilated convolutions in the backbone to extract highresolution feature maps, which brings heavy computation complexity and memory footprint. To replace the time and memory consuming dilated convolutions, we propose a novel joint upsampling module named Joint Pyramid Upsampling (JPU) by formulating the task of extracting highresolution feature maps into a joint upsampling problem. With the proposed JPU, our method reduces the computation complexity by more than three times without performance loss. Experiments show that JPU is superior to other upsampling modules, which can be plugged into many existing approaches to reduce computation complexity and improve performance. By replacing dilated convolutions with the proposed JPU module, our method achieves the state-of-the-art performance in Pascal Context dataset (mIoU of 53.13%) and ADE20K dataset (final score of 0.5584) while running 3 times faster. Code is available in https://github.com/wuhuikai/FastFCN . Introduction</th>\n",
       "      <th>Semantic segmentation [23, 40, 4] is one of the fundamental tasks in computer vision, with the goal of assigning a semantic label to each pixel of an image. Modern approaches usually employ a Fully Convolution Network (FCN) [22] to address this task, achieving tremendous success among several segmentation benchmarks. The original FCN is proposed by Long et al. [22] , which is transformed from a Convolutional Neural Network (CNN) [16, 15] designed for image classification. Inheriting from the design for image classification, the original FCN downsamples the input image progressively by stride convolutions and/or spatial pooling layers, resulting in a final feature map in low resolution. Although the final feature map encodes rich semantic information, the fine image structure information is lost, leading to inaccurate predictions around the object boundaries. As shown in Figure 1a , the original FCN typically downsamples the input image 5 times, reducing the spatial resolution of the final feature map by a factor of 32. To obtain a high-resolution final feature map, [3, 28, 18, 30, 27] employ the original FCN as the encoder to capture high-level semantic information, and a decoder is designed to gradually recover the spatial information by combining multi-level feature maps from the encoder. As shown in Figure 1b , we term such methods EncoderDecoder, of which the final prediction generated by the decoder is in high resolution. Alternatively, DeepLab [5] removes the last two downsampling operations from the original FCN and introduces dilated (atrous) convolutions to maintain the receptive field of view unchanged. 1 Following DeepLab, [38, 6, 36] employ a multi-scale context module on top of the final feature map, outperforming most EncoderDecoder methods significantly on several segmentation benchmarks. As shown in Figure 1c , the spatial resolution of the last feature map in DilatedFCN is 4 times larger than that in the original FCN, thus maintaining more structure and location information. The dilated convolutions play an important role in maintaining the spatial resolution of the final feature map, leading to superior performance compared to most methods in EncoderDecoder. However, the introduced dilated convolutions bring heavy computation complexity and memory footprint, which limit the usage in many real-time applications. Taking ResNet-101 [13] as an example, compared to the original FCN, 23 residual blocks (69 convolution layers) in DilatedFCN require to take 4 times more computation resources and memory usages, and 3 residual blocks (9 convolution layers) need to take 16 times more resources. We aim at tackling the aforementioned issue caused by dilated convolutions in this paper. To achieve this, we propose a novel joint upsampling module to replace the time and memory consuming dilated convolutions, namely Joint Pyramid Upsampling (JPU). As a result, our method employs the original FCN as the backbone while applying JPU to upsample the low-resolution final feature map with output stride (OS) 32, resulting in a high-resolution feature map (OS=8). Accordingly, the computation time and memory footprint of the whole segmentation framework is dramatically reduced. Meanwhile, there's no performance loss when replacing the dilated convolutions with the proposed JPU. We attribute this to the ability of JPU to exploit multiscale context across multi-level feature maps. To validate the effectiveness of our method, we first conduct a systematical experiment, showing that the proposed JPU can replace dilated convolutions in several popular approaches without performance loss. We then test the proposed method on several segmentation benchmarks. Results show that our method achieves the state-of-the-art performance while running more than 3 times faster. Concretely, we outperform all the baselines on Pascal Context dataset [23] by a large margin, which achieves the state-ofthe-art performance with mIoU of 53.13%. On ADE20K dataset [40] , we obtain the mIoU of 42.75% with ResNet-50 as the backbone, which sets a new record on the val set. Moreover, our method with ResNet-101 achieves the stateof-the-art performance in the test set of ADE20K dataset. In summary, our contributions are three folds, which are: (1) We propose a computationally efficient joint upsampling module named JPU to replace the time and memory consuming dilated convolutions in the backbone. (2) Based on the proposed JPU, the computation time and memory footprint of the whole segmentation framework can be reduced by a factor of more than 3 and meanwhile achieves better performance. (3) Our method achieves the new state-ofthe-art performance in both Pascal Context dataset (mIoU of 53.13%) and ADE20K dataset (mIoU of 42.75% with ResNet-50 as the backbone on the val set and final score of 0.5584 with ResNet-101 on the test set).  Related Work</th>\n",
       "      <th>In this section, we first give an overview on methods for semantic segmentation, which can be categorized into two directions. We then introduce some related works on upsampling.  Semantic Segmentation</th>\n",
       "      <th>FCNs [22] have achieved huge success in semantic segmentation. Following FCN, there're two prominent directions, namely DilatedFCN and EncoderDecoder. Dilated-FCNs [11, 34, 7, 6, 38, 36, 5] utilize dilated convolutions to keep the receptive field of view and employ a multi-scale context module to process high-level feature maps. Alternatively, EncoderDecoders [24, 28, 18, 1, 26, 12, 33, 37] propose to utilize an encoder to extract multi-level feature maps, which are then combined into the final prediction by a decoder. DilatedFCN In order to capture multi-scale context information on the high-resolution final feature map, PSP-Net [38] performs pooling operations at multiple grid scales while DeepLabV3 [6] employs parallel atrous convolutions with different rates named ASPP. Alternatively, EncNet [36] utilizes the Context Encoding Module to capture global contextual information. Differently, our method proposes a joint upsampling module named JPU to replace the dilated convolutions in the backbone of DilatedFCNs, which can JPU Encoding/PSP/ASPP Head C o n v 5 , 3 2 x C o n v 4 , 1 6 x C o n v 3 , 8 x C o n v 2 , 4 x C o n v 1 , 2 x 8 x 8 x Figure 2 : Framework Overview of Our Method. Our method employs the same backbone as the original FCN. After the backbone, a novel upsampling module named Joint Pyramid Upsampling (JPU) is proposed, which takes the last three feature maps as the inputs and generates a high-resolution feature map. A multi-scale/global context module is then employed to produce the final label map. Best viewed in color. reduce computation complexity dramatically without performance loss. EncoderDecoder To gradually recover the spatial information, [28] introduces skip connections to construct U-Net, which combines the encoder features and the corresponding decoder activations. [18] proposes a multipath refinement network, which explicitly exploits all the information available along the down-sampling process. DeepLabV3+ [8] combines the advantages of DilatedFCN and EncoderDecoder, which employs DeepLabV3 as the encoder. Our method is complementary to DeepLabV3+, which can reduce the computation overload of DeepLabV3 without performance loss.  Upsampling</th>\n",
       "      <th>In our method, we propose a module to upsample a lowresolution feature map given high-resolution feature maps as guidance, which is closely related to joint upsampling as well as data-dependent upsampling. Joint Upsampling In the literature of image processing, joint upsampling aims at leveraging the guidance image as a prior and transferring the structural details from the guidance image to the target image. [17] constructs a joint filter based on CNNs, which learns to recover the structure details in the guidance image. [31] proposes an end-to-end trainable guided filtering module, which upsamples a lowresolution image conditionally. Our method is related to the aforementioned approaches. However, the proposed JPU is designed for processing feature maps with a large number of channels while [17, 31] are specially designed for pro-cessing 3-channel images, which fail to capture the complex relations in high dimensional feature maps. Besides, the motivation and target of our method is completely different. Data-Dependent Upsampling DUpsampling [29] is also related to our method, which takes advantages of the redundancy in the segmentation label space and is able to recover the pixel-wise prediction from low-resolution outputs of CNNs. Compared to our method, DUpsampling has a strong dependency on the label space, which generalizes poorly to a larger or more complex label space.  Method</th>\n",
       "      <th>In this section, we first introduce the most popular methods for semantic segmentation, named DilatedFCNs. We then reform the architecture of DilatedFCNs with a novel joint upsampling module, Joint Pyramid Upsampling (JPU). Finally, we discuss the proposed JPU in details, before which joint upsampling, dilated convolution and stride convolution are briefly introduced.  DilatedFCN</th>\n",
       "      <th>To exploit Deep CNNs in semantic segmentation, Long et al. [22] transform the CNN designed for image classification into FCN. Taking ResNet-101 as an example, the original CNN contains 5 convolution stages, a global average pooling layer and a linear layer. To construct an FCN, the global average pooling layer and the linear layer are replaced by a convolution layer, which is used to generate the final label map, as shown in Figure 1a . Between each two consecutive convolution stages, stride convolutions and/or spatial pooling layers are employed, resulting in 5 feature maps with gradually reduced spatial resolutions. The spatial resolution of the last feature map in FCN is reduced by a factor of 32, leading to inaccurate predictions about the locations and details. To obtain a final feature map with high resolution, DeepLab [5] removes the downsampling operations before the last two feature maps, as shown in Figure 1c . Besides, the convolution layers inside the last two convolution stages are replaced by dilated convolutions to maintain the receptive field of view, thus named Dilated-FCN. As a result, the resolution of the last feature map is reduced by a factor of 8, which reserves more location and detail information. Following DeepLab, [38, 6] propose a multi-scale context module to capture context information from the last feature map, achieving tremendous success in several segmentation benchmarks.  The Framework of Our Method</th>\n",
       "      <th>To obtain a high-resolution final feature map, methods in DilatedFCN remove the last two downsampling operations from the original FCN, which bring in heavy computation complexity and memory footprint due to the enlarged feature maps. In this paper, we aim at seeking an alternative way to approximate the final feature map of DilatedFCN without computation and memory overload. Meanwhile, we expect the performance of our method to be as good as that of the original DilatedFCNs. To achieve this, we first put back all the stride convolutions removed by DilatedFCN, while replacing all the dilated convolutions with regular convolution layers. As shown in Figure 2 , the backbone of our method is the same as that of the original FCN, where the spatial resolutions of the five feature maps (Conv1−Conv5) are gradually reduced by a factor of 2. To obtain a feature map similar to the final feature map of DilatedFCN, we propose a novel module named Joint Pyramid Upsampling (JPU), which takes the last three feature maps (Conv3−Conv5) as inputs. Then a multi-scale context module (PSP [38] /ASPP [6] ) or a global context module (Encoding [36] ) is employed to produce the final predictions. Compared to DilatedFCN, our method takes 4 times fewer computation and memory resources in 23 residual blocks (69 layers) and 16 times fewer in 3 blocks (9 layers) when the backbone is ResNet-101. Thus, our method runs much faster than DilatedFCN while consuming less memory.  Joint Pyramid Upsampling</th>\n",
       "      <th>The proposed JPU is designed for generating a feature map that approximates the activations of the final feature map from the backbone of DilatedFCN. Such a problem can be reformulated into joint upsampling, which is then resolved by a CNN designed for this task.  Background</th>\n",
       "      <th>Joint Upsampling Given a low-resolution target image and a high-resolution guidance image, joint upsampling aims at generating a high-resolution target image by transferring details and structures from the guidance image. Generally, the low-resolution target image y l is generated by employing a transformation f (•) on the low-resolution guidance image x l , i.e. y l = f (x l ). Given x l and y l , we are required to obtain a transformationf (  y h =f (x h ), wheref (•) = argmin h(•)∈H ||y l − h(x l )||, (1) where H is a set of all possible transformation functions, and || • || is a pre-defined distance metric. Dilated Convolution Dilated convolution is introduced in DeepLab [5] for obtaining high-resolution feature maps while maintaining the receptive field of view. Figure 3a gives an illustration of the dilated convolution in 1D (dilation rate = 2), which can be divided into the following three steps: (1) split the input feature f in into two groups f 0 in and f 1 in according to the parity of the index, (2) process each feature with the same convolution layer, resulting in f 0 out and f 1 out , and (3) merge the two generated features interlaced to obtain the output feature f out . Stride Convolution Stride convolution is proposed to transform the input feature into an output feature with reduced spatial resolution, which is equivalent to the following two steps as shown in Figure 3b: (1) process the input feature f in with a regular convolution to obtain the intermediate feature f m , and (2) remove the elements with an odd index, resulting in f out .  Reformulating into Joint Upsampling</th>\n",
       "      <th>The differences between the backbone of our method and DilatedFCN lie on the last two convolution stages. Taking the 4th convolution stage (Conv4) as an example, in Dilat-edFCN, the input feature map is first processed by a regular convolution layer, followed by a series of dilated convolutions (d=2). Differently, our method first processes the input feature map with a stride convolution (s=2), and then employs several regular convolutions to generate the output. Formally, given the input feature map x, the output feature map y d in DilatedFCN is obtained as follows: Fig 3a) , = ! \"# ! $%&amp; ! \"# ! $%&amp; ! \"# ' ! \"# ( ! $%&amp; ' ! $%&amp; ( Split Merge Conv DilatedConv, d=2 y d = x → C r → C d → ...... → C d n = x → C r → SC r M → ...... → SC r M n (Fig 3a) = x → C r → S → C r → ...... → C r n → M = y m → S → C n r → M = {y 0 m , y 1 m } → C n r → M ( while in our method, the output feature map y s is generated as follows: (Fig 3b) . y s = x → C s → C r → ...... → C r n = x → C r → R → C r → ...... → C r n (Fig 3b) = y m → R → C n r = y 0 m → C n r (3) C r , C d , and C s represent a regular/dilated/stride convolution respectively, and C n r is n layers of regular convolutions. S, M and R are split, merge, and reduce operations in Figure 3 , where adjacent S and M operations can be canceled out. Notably, the convolutions in Equations 2 and 3 are in 1D, which is for simplicity. Similar results can be obtained for 2D convolutions. The aforementioned equations show that y s and y d can be obtained with the same function C n r with different inputs: y 0 m and y m , where the former is downsampled from the latter. Thus, given x and y s , the feature map y that ap-proximates y d can be obtained as follows: EQUATION which is the same as the joint upsampling problem defined in Equation 1. Similar conclusions can be easily obtained for the 5th convolution stage (Conv5).  Solving with CNNs</th>\n",
       "      <th>Equation 4 is an optimization problem, which takes lots of time to converge through the iterative gradient descent. Alternatively, we propose to approximate the optimization process with a CNN module. To achieve this, we first require to generate y m given x, as shown in Equation 4. Then, features from y 0 m and y s need to be gathered for learning the mappingĥ. Finally, a convolution block is required to transform the gathered features into the final prediction y. Following the aforementioned analysis, we design the JPU module as in Figure 4 . Concretely, each input feature map is firstly processed by a regular convolution block (Fig. 4a) , which is designed for (1) generating y m given x, and (2) transforming f m into an embedding space with reduced dimensions. As a result, all the input features are mapped into the same space, which enables a better fusion and reduces the computation complexity. Then, the generated feature maps are upsampled and concatenated, resulting in y c (Fig. 4b) . Four separable convolutions [14, 9] the convolution with dilation rate 1 is employed to capture the relation between y 0 m and the rest part of y m , as shown by the blue box in Figure 5 . Alternatively, the convolutions with dilation rate 2, 4 and 8 are designed for learning the mappingĥ to transform y 0 m into y s , as shown by the green boxes in Figure 5 . Thus, JPU can extract multi-scale context information from multi-level feature maps, which leads to a better performance. This is significantly different from ASPP [6] , which only exploit the information in the last feature map. The extracted features encode the mapping between y 0 m and y s as well as the relation between y 0 m and the rest part of y m . Thus, another regular convolution block is employed, which transforms the features into the final predictions ( Fig. 4c) . Notably, the proposed JPU module solves two closely related joint upsampling problems jointly, which are (1) upsampling Conv4 based on Conv3 (the 4th convolution stage), and (2) upscaling Conv5 with the guidance of the enlarged Conv4 (the 5th convolution stage).  Experiment</th>\n",
       "      <td>In this section, we first introduce the datase...</td>\n",
       "      <td>Dataset Pascal Context dataset [23] is based o...</td>\n",
       "      <td>To show the effectiveness of the proposed meth...</td>\n",
       "      <td>Pascal Context In Table 1 , our method employs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3D Face Reconstruction#Florence#Average 3D Error</th>\n",
       "      <th>Title</th>\n",
       "      <th>FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation Abstract:</th>\n",
       "      <th>Modern approaches for semantic segmentation usually employ dilated convolutions in the backbone to extract highresolution feature maps, which brings heavy computation complexity and memory footprint. To replace the time and memory consuming dilated convolutions, we propose a novel joint upsampling module named Joint Pyramid Upsampling (JPU) by formulating the task of extracting highresolution feature maps into a joint upsampling problem. With the proposed JPU, our method reduces the computation complexity by more than three times without performance loss. Experiments show that JPU is superior to other upsampling modules, which can be plugged into many existing approaches to reduce computation complexity and improve performance. By replacing dilated convolutions with the proposed JPU module, our method achieves the state-of-the-art performance in Pascal Context dataset (mIoU of 53.13%) and ADE20K dataset (final score of 0.5584) while running 3 times faster. Code is available in https://github.com/wuhuikai/FastFCN . Introduction</th>\n",
       "      <th>Semantic segmentation [23, 40, 4] is one of the fundamental tasks in computer vision, with the goal of assigning a semantic label to each pixel of an image. Modern approaches usually employ a Fully Convolution Network (FCN) [22] to address this task, achieving tremendous success among several segmentation benchmarks. The original FCN is proposed by Long et al. [22] , which is transformed from a Convolutional Neural Network (CNN) [16, 15] designed for image classification. Inheriting from the design for image classification, the original FCN downsamples the input image progressively by stride convolutions and/or spatial pooling layers, resulting in a final feature map in low resolution. Although the final feature map encodes rich semantic information, the fine image structure information is lost, leading to inaccurate predictions around the object boundaries. As shown in Figure 1a , the original FCN typically downsamples the input image 5 times, reducing the spatial resolution of the final feature map by a factor of 32. To obtain a high-resolution final feature map, [3, 28, 18, 30, 27] employ the original FCN as the encoder to capture high-level semantic information, and a decoder is designed to gradually recover the spatial information by combining multi-level feature maps from the encoder. As shown in Figure 1b , we term such methods EncoderDecoder, of which the final prediction generated by the decoder is in high resolution. Alternatively, DeepLab [5] removes the last two downsampling operations from the original FCN and introduces dilated (atrous) convolutions to maintain the receptive field of view unchanged. 1 Following DeepLab, [38, 6, 36] employ a multi-scale context module on top of the final feature map, outperforming most EncoderDecoder methods significantly on several segmentation benchmarks. As shown in Figure 1c , the spatial resolution of the last feature map in DilatedFCN is 4 times larger than that in the original FCN, thus maintaining more structure and location information. The dilated convolutions play an important role in maintaining the spatial resolution of the final feature map, leading to superior performance compared to most methods in EncoderDecoder. However, the introduced dilated convolutions bring heavy computation complexity and memory footprint, which limit the usage in many real-time applications. Taking ResNet-101 [13] as an example, compared to the original FCN, 23 residual blocks (69 convolution layers) in DilatedFCN require to take 4 times more computation resources and memory usages, and 3 residual blocks (9 convolution layers) need to take 16 times more resources. We aim at tackling the aforementioned issue caused by dilated convolutions in this paper. To achieve this, we propose a novel joint upsampling module to replace the time and memory consuming dilated convolutions, namely Joint Pyramid Upsampling (JPU). As a result, our method employs the original FCN as the backbone while applying JPU to upsample the low-resolution final feature map with output stride (OS) 32, resulting in a high-resolution feature map (OS=8). Accordingly, the computation time and memory footprint of the whole segmentation framework is dramatically reduced. Meanwhile, there's no performance loss when replacing the dilated convolutions with the proposed JPU. We attribute this to the ability of JPU to exploit multiscale context across multi-level feature maps. To validate the effectiveness of our method, we first conduct a systematical experiment, showing that the proposed JPU can replace dilated convolutions in several popular approaches without performance loss. We then test the proposed method on several segmentation benchmarks. Results show that our method achieves the state-of-the-art performance while running more than 3 times faster. Concretely, we outperform all the baselines on Pascal Context dataset [23] by a large margin, which achieves the state-ofthe-art performance with mIoU of 53.13%. On ADE20K dataset [40] , we obtain the mIoU of 42.75% with ResNet-50 as the backbone, which sets a new record on the val set. Moreover, our method with ResNet-101 achieves the stateof-the-art performance in the test set of ADE20K dataset. In summary, our contributions are three folds, which are: (1) We propose a computationally efficient joint upsampling module named JPU to replace the time and memory consuming dilated convolutions in the backbone. (2) Based on the proposed JPU, the computation time and memory footprint of the whole segmentation framework can be reduced by a factor of more than 3 and meanwhile achieves better performance. (3) Our method achieves the new state-ofthe-art performance in both Pascal Context dataset (mIoU of 53.13%) and ADE20K dataset (mIoU of 42.75% with ResNet-50 as the backbone on the val set and final score of 0.5584 with ResNet-101 on the test set).  Related Work</th>\n",
       "      <th>In this section, we first give an overview on methods for semantic segmentation, which can be categorized into two directions. We then introduce some related works on upsampling.  Semantic Segmentation</th>\n",
       "      <th>FCNs [22] have achieved huge success in semantic segmentation. Following FCN, there're two prominent directions, namely DilatedFCN and EncoderDecoder. Dilated-FCNs [11, 34, 7, 6, 38, 36, 5] utilize dilated convolutions to keep the receptive field of view and employ a multi-scale context module to process high-level feature maps. Alternatively, EncoderDecoders [24, 28, 18, 1, 26, 12, 33, 37] propose to utilize an encoder to extract multi-level feature maps, which are then combined into the final prediction by a decoder. DilatedFCN In order to capture multi-scale context information on the high-resolution final feature map, PSP-Net [38] performs pooling operations at multiple grid scales while DeepLabV3 [6] employs parallel atrous convolutions with different rates named ASPP. Alternatively, EncNet [36] utilizes the Context Encoding Module to capture global contextual information. Differently, our method proposes a joint upsampling module named JPU to replace the dilated convolutions in the backbone of DilatedFCNs, which can JPU Encoding/PSP/ASPP Head C o n v 5 , 3 2 x C o n v 4 , 1 6 x C o n v 3 , 8 x C o n v 2 , 4 x C o n v 1 , 2 x 8 x 8 x Figure 2 : Framework Overview of Our Method. Our method employs the same backbone as the original FCN. After the backbone, a novel upsampling module named Joint Pyramid Upsampling (JPU) is proposed, which takes the last three feature maps as the inputs and generates a high-resolution feature map. A multi-scale/global context module is then employed to produce the final label map. Best viewed in color. reduce computation complexity dramatically without performance loss. EncoderDecoder To gradually recover the spatial information, [28] introduces skip connections to construct U-Net, which combines the encoder features and the corresponding decoder activations. [18] proposes a multipath refinement network, which explicitly exploits all the information available along the down-sampling process. DeepLabV3+ [8] combines the advantages of DilatedFCN and EncoderDecoder, which employs DeepLabV3 as the encoder. Our method is complementary to DeepLabV3+, which can reduce the computation overload of DeepLabV3 without performance loss.  Upsampling</th>\n",
       "      <th>In our method, we propose a module to upsample a lowresolution feature map given high-resolution feature maps as guidance, which is closely related to joint upsampling as well as data-dependent upsampling. Joint Upsampling In the literature of image processing, joint upsampling aims at leveraging the guidance image as a prior and transferring the structural details from the guidance image to the target image. [17] constructs a joint filter based on CNNs, which learns to recover the structure details in the guidance image. [31] proposes an end-to-end trainable guided filtering module, which upsamples a lowresolution image conditionally. Our method is related to the aforementioned approaches. However, the proposed JPU is designed for processing feature maps with a large number of channels while [17, 31] are specially designed for pro-cessing 3-channel images, which fail to capture the complex relations in high dimensional feature maps. Besides, the motivation and target of our method is completely different. Data-Dependent Upsampling DUpsampling [29] is also related to our method, which takes advantages of the redundancy in the segmentation label space and is able to recover the pixel-wise prediction from low-resolution outputs of CNNs. Compared to our method, DUpsampling has a strong dependency on the label space, which generalizes poorly to a larger or more complex label space.  Method</th>\n",
       "      <th>In this section, we first introduce the most popular methods for semantic segmentation, named DilatedFCNs. We then reform the architecture of DilatedFCNs with a novel joint upsampling module, Joint Pyramid Upsampling (JPU). Finally, we discuss the proposed JPU in details, before which joint upsampling, dilated convolution and stride convolution are briefly introduced.  DilatedFCN</th>\n",
       "      <th>To exploit Deep CNNs in semantic segmentation, Long et al. [22] transform the CNN designed for image classification into FCN. Taking ResNet-101 as an example, the original CNN contains 5 convolution stages, a global average pooling layer and a linear layer. To construct an FCN, the global average pooling layer and the linear layer are replaced by a convolution layer, which is used to generate the final label map, as shown in Figure 1a . Between each two consecutive convolution stages, stride convolutions and/or spatial pooling layers are employed, resulting in 5 feature maps with gradually reduced spatial resolutions. The spatial resolution of the last feature map in FCN is reduced by a factor of 32, leading to inaccurate predictions about the locations and details. To obtain a final feature map with high resolution, DeepLab [5] removes the downsampling operations before the last two feature maps, as shown in Figure 1c . Besides, the convolution layers inside the last two convolution stages are replaced by dilated convolutions to maintain the receptive field of view, thus named Dilated-FCN. As a result, the resolution of the last feature map is reduced by a factor of 8, which reserves more location and detail information. Following DeepLab, [38, 6] propose a multi-scale context module to capture context information from the last feature map, achieving tremendous success in several segmentation benchmarks.  The Framework of Our Method</th>\n",
       "      <th>To obtain a high-resolution final feature map, methods in DilatedFCN remove the last two downsampling operations from the original FCN, which bring in heavy computation complexity and memory footprint due to the enlarged feature maps. In this paper, we aim at seeking an alternative way to approximate the final feature map of DilatedFCN without computation and memory overload. Meanwhile, we expect the performance of our method to be as good as that of the original DilatedFCNs. To achieve this, we first put back all the stride convolutions removed by DilatedFCN, while replacing all the dilated convolutions with regular convolution layers. As shown in Figure 2 , the backbone of our method is the same as that of the original FCN, where the spatial resolutions of the five feature maps (Conv1−Conv5) are gradually reduced by a factor of 2. To obtain a feature map similar to the final feature map of DilatedFCN, we propose a novel module named Joint Pyramid Upsampling (JPU), which takes the last three feature maps (Conv3−Conv5) as inputs. Then a multi-scale context module (PSP [38] /ASPP [6] ) or a global context module (Encoding [36] ) is employed to produce the final predictions. Compared to DilatedFCN, our method takes 4 times fewer computation and memory resources in 23 residual blocks (69 layers) and 16 times fewer in 3 blocks (9 layers) when the backbone is ResNet-101. Thus, our method runs much faster than DilatedFCN while consuming less memory.  Joint Pyramid Upsampling</th>\n",
       "      <th>The proposed JPU is designed for generating a feature map that approximates the activations of the final feature map from the backbone of DilatedFCN. Such a problem can be reformulated into joint upsampling, which is then resolved by a CNN designed for this task.  Background</th>\n",
       "      <th>Joint Upsampling Given a low-resolution target image and a high-resolution guidance image, joint upsampling aims at generating a high-resolution target image by transferring details and structures from the guidance image. Generally, the low-resolution target image y l is generated by employing a transformation f (•) on the low-resolution guidance image x l , i.e. y l = f (x l ). Given x l and y l , we are required to obtain a transformationf (  y h =f (x h ), wheref (•) = argmin h(•)∈H ||y l − h(x l )||, (1) where H is a set of all possible transformation functions, and || • || is a pre-defined distance metric. Dilated Convolution Dilated convolution is introduced in DeepLab [5] for obtaining high-resolution feature maps while maintaining the receptive field of view. Figure 3a gives an illustration of the dilated convolution in 1D (dilation rate = 2), which can be divided into the following three steps: (1) split the input feature f in into two groups f 0 in and f 1 in according to the parity of the index, (2) process each feature with the same convolution layer, resulting in f 0 out and f 1 out , and (3) merge the two generated features interlaced to obtain the output feature f out . Stride Convolution Stride convolution is proposed to transform the input feature into an output feature with reduced spatial resolution, which is equivalent to the following two steps as shown in Figure 3b: (1) process the input feature f in with a regular convolution to obtain the intermediate feature f m , and (2) remove the elements with an odd index, resulting in f out .  Reformulating into Joint Upsampling</th>\n",
       "      <th>The differences between the backbone of our method and DilatedFCN lie on the last two convolution stages. Taking the 4th convolution stage (Conv4) as an example, in Dilat-edFCN, the input feature map is first processed by a regular convolution layer, followed by a series of dilated convolutions (d=2). Differently, our method first processes the input feature map with a stride convolution (s=2), and then employs several regular convolutions to generate the output. Formally, given the input feature map x, the output feature map y d in DilatedFCN is obtained as follows: Fig 3a) , = ! \"# ! $%&amp; ! \"# ! $%&amp; ! \"# ' ! \"# ( ! $%&amp; ' ! $%&amp; ( Split Merge Conv DilatedConv, d=2 y d = x → C r → C d → ...... → C d n = x → C r → SC r M → ...... → SC r M n (Fig 3a) = x → C r → S → C r → ...... → C r n → M = y m → S → C n r → M = {y 0 m , y 1 m } → C n r → M ( while in our method, the output feature map y s is generated as follows: (Fig 3b) . y s = x → C s → C r → ...... → C r n = x → C r → R → C r → ...... → C r n (Fig 3b) = y m → R → C n r = y 0 m → C n r (3) C r , C d , and C s represent a regular/dilated/stride convolution respectively, and C n r is n layers of regular convolutions. S, M and R are split, merge, and reduce operations in Figure 3 , where adjacent S and M operations can be canceled out. Notably, the convolutions in Equations 2 and 3 are in 1D, which is for simplicity. Similar results can be obtained for 2D convolutions. The aforementioned equations show that y s and y d can be obtained with the same function C n r with different inputs: y 0 m and y m , where the former is downsampled from the latter. Thus, given x and y s , the feature map y that ap-proximates y d can be obtained as follows: EQUATION which is the same as the joint upsampling problem defined in Equation 1. Similar conclusions can be easily obtained for the 5th convolution stage (Conv5).  Solving with CNNs</th>\n",
       "      <th>Equation 4 is an optimization problem, which takes lots of time to converge through the iterative gradient descent. Alternatively, we propose to approximate the optimization process with a CNN module. To achieve this, we first require to generate y m given x, as shown in Equation 4. Then, features from y 0 m and y s need to be gathered for learning the mappingĥ. Finally, a convolution block is required to transform the gathered features into the final prediction y. Following the aforementioned analysis, we design the JPU module as in Figure 4 . Concretely, each input feature map is firstly processed by a regular convolution block (Fig. 4a) , which is designed for (1) generating y m given x, and (2) transforming f m into an embedding space with reduced dimensions. As a result, all the input features are mapped into the same space, which enables a better fusion and reduces the computation complexity. Then, the generated feature maps are upsampled and concatenated, resulting in y c (Fig. 4b) . Four separable convolutions [14, 9] the convolution with dilation rate 1 is employed to capture the relation between y 0 m and the rest part of y m , as shown by the blue box in Figure 5 . Alternatively, the convolutions with dilation rate 2, 4 and 8 are designed for learning the mappingĥ to transform y 0 m into y s , as shown by the green boxes in Figure 5 . Thus, JPU can extract multi-scale context information from multi-level feature maps, which leads to a better performance. This is significantly different from ASPP [6] , which only exploit the information in the last feature map. The extracted features encode the mapping between y 0 m and y s as well as the relation between y 0 m and the rest part of y m . Thus, another regular convolution block is employed, which transforms the features into the final predictions ( Fig. 4c) . Notably, the proposed JPU module solves two closely related joint upsampling problems jointly, which are (1) upsampling Conv4 based on Conv3 (the 4th convolution stage), and (2) upscaling Conv5 with the guidance of the enlarged Conv4 (the 5th convolution stage).  Experiment</th>\n",
       "      <td>In this section, we first introduce the datase...</td>\n",
       "      <td>Dataset Pascal Context dataset [23] is based o...</td>\n",
       "      <td>To show the effectiveness of the proposed meth...</td>\n",
       "      <td>Pascal Context In Table 1 , our method employs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3D Face Reconstruction#Florence#Mean NME</th>\n",
       "      <th>Title</th>\n",
       "      <th>FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation Abstract:</th>\n",
       "      <th>Modern approaches for semantic segmentation usually employ dilated convolutions in the backbone to extract highresolution feature maps, which brings heavy computation complexity and memory footprint. To replace the time and memory consuming dilated convolutions, we propose a novel joint upsampling module named Joint Pyramid Upsampling (JPU) by formulating the task of extracting highresolution feature maps into a joint upsampling problem. With the proposed JPU, our method reduces the computation complexity by more than three times without performance loss. Experiments show that JPU is superior to other upsampling modules, which can be plugged into many existing approaches to reduce computation complexity and improve performance. By replacing dilated convolutions with the proposed JPU module, our method achieves the state-of-the-art performance in Pascal Context dataset (mIoU of 53.13%) and ADE20K dataset (final score of 0.5584) while running 3 times faster. Code is available in https://github.com/wuhuikai/FastFCN . Introduction</th>\n",
       "      <th>Semantic segmentation [23, 40, 4] is one of the fundamental tasks in computer vision, with the goal of assigning a semantic label to each pixel of an image. Modern approaches usually employ a Fully Convolution Network (FCN) [22] to address this task, achieving tremendous success among several segmentation benchmarks. The original FCN is proposed by Long et al. [22] , which is transformed from a Convolutional Neural Network (CNN) [16, 15] designed for image classification. Inheriting from the design for image classification, the original FCN downsamples the input image progressively by stride convolutions and/or spatial pooling layers, resulting in a final feature map in low resolution. Although the final feature map encodes rich semantic information, the fine image structure information is lost, leading to inaccurate predictions around the object boundaries. As shown in Figure 1a , the original FCN typically downsamples the input image 5 times, reducing the spatial resolution of the final feature map by a factor of 32. To obtain a high-resolution final feature map, [3, 28, 18, 30, 27] employ the original FCN as the encoder to capture high-level semantic information, and a decoder is designed to gradually recover the spatial information by combining multi-level feature maps from the encoder. As shown in Figure 1b , we term such methods EncoderDecoder, of which the final prediction generated by the decoder is in high resolution. Alternatively, DeepLab [5] removes the last two downsampling operations from the original FCN and introduces dilated (atrous) convolutions to maintain the receptive field of view unchanged. 1 Following DeepLab, [38, 6, 36] employ a multi-scale context module on top of the final feature map, outperforming most EncoderDecoder methods significantly on several segmentation benchmarks. As shown in Figure 1c , the spatial resolution of the last feature map in DilatedFCN is 4 times larger than that in the original FCN, thus maintaining more structure and location information. The dilated convolutions play an important role in maintaining the spatial resolution of the final feature map, leading to superior performance compared to most methods in EncoderDecoder. However, the introduced dilated convolutions bring heavy computation complexity and memory footprint, which limit the usage in many real-time applications. Taking ResNet-101 [13] as an example, compared to the original FCN, 23 residual blocks (69 convolution layers) in DilatedFCN require to take 4 times more computation resources and memory usages, and 3 residual blocks (9 convolution layers) need to take 16 times more resources. We aim at tackling the aforementioned issue caused by dilated convolutions in this paper. To achieve this, we propose a novel joint upsampling module to replace the time and memory consuming dilated convolutions, namely Joint Pyramid Upsampling (JPU). As a result, our method employs the original FCN as the backbone while applying JPU to upsample the low-resolution final feature map with output stride (OS) 32, resulting in a high-resolution feature map (OS=8). Accordingly, the computation time and memory footprint of the whole segmentation framework is dramatically reduced. Meanwhile, there's no performance loss when replacing the dilated convolutions with the proposed JPU. We attribute this to the ability of JPU to exploit multiscale context across multi-level feature maps. To validate the effectiveness of our method, we first conduct a systematical experiment, showing that the proposed JPU can replace dilated convolutions in several popular approaches without performance loss. We then test the proposed method on several segmentation benchmarks. Results show that our method achieves the state-of-the-art performance while running more than 3 times faster. Concretely, we outperform all the baselines on Pascal Context dataset [23] by a large margin, which achieves the state-ofthe-art performance with mIoU of 53.13%. On ADE20K dataset [40] , we obtain the mIoU of 42.75% with ResNet-50 as the backbone, which sets a new record on the val set. Moreover, our method with ResNet-101 achieves the stateof-the-art performance in the test set of ADE20K dataset. In summary, our contributions are three folds, which are: (1) We propose a computationally efficient joint upsampling module named JPU to replace the time and memory consuming dilated convolutions in the backbone. (2) Based on the proposed JPU, the computation time and memory footprint of the whole segmentation framework can be reduced by a factor of more than 3 and meanwhile achieves better performance. (3) Our method achieves the new state-ofthe-art performance in both Pascal Context dataset (mIoU of 53.13%) and ADE20K dataset (mIoU of 42.75% with ResNet-50 as the backbone on the val set and final score of 0.5584 with ResNet-101 on the test set).  Related Work</th>\n",
       "      <th>In this section, we first give an overview on methods for semantic segmentation, which can be categorized into two directions. We then introduce some related works on upsampling.  Semantic Segmentation</th>\n",
       "      <th>FCNs [22] have achieved huge success in semantic segmentation. Following FCN, there're two prominent directions, namely DilatedFCN and EncoderDecoder. Dilated-FCNs [11, 34, 7, 6, 38, 36, 5] utilize dilated convolutions to keep the receptive field of view and employ a multi-scale context module to process high-level feature maps. Alternatively, EncoderDecoders [24, 28, 18, 1, 26, 12, 33, 37] propose to utilize an encoder to extract multi-level feature maps, which are then combined into the final prediction by a decoder. DilatedFCN In order to capture multi-scale context information on the high-resolution final feature map, PSP-Net [38] performs pooling operations at multiple grid scales while DeepLabV3 [6] employs parallel atrous convolutions with different rates named ASPP. Alternatively, EncNet [36] utilizes the Context Encoding Module to capture global contextual information. Differently, our method proposes a joint upsampling module named JPU to replace the dilated convolutions in the backbone of DilatedFCNs, which can JPU Encoding/PSP/ASPP Head C o n v 5 , 3 2 x C o n v 4 , 1 6 x C o n v 3 , 8 x C o n v 2 , 4 x C o n v 1 , 2 x 8 x 8 x Figure 2 : Framework Overview of Our Method. Our method employs the same backbone as the original FCN. After the backbone, a novel upsampling module named Joint Pyramid Upsampling (JPU) is proposed, which takes the last three feature maps as the inputs and generates a high-resolution feature map. A multi-scale/global context module is then employed to produce the final label map. Best viewed in color. reduce computation complexity dramatically without performance loss. EncoderDecoder To gradually recover the spatial information, [28] introduces skip connections to construct U-Net, which combines the encoder features and the corresponding decoder activations. [18] proposes a multipath refinement network, which explicitly exploits all the information available along the down-sampling process. DeepLabV3+ [8] combines the advantages of DilatedFCN and EncoderDecoder, which employs DeepLabV3 as the encoder. Our method is complementary to DeepLabV3+, which can reduce the computation overload of DeepLabV3 without performance loss.  Upsampling</th>\n",
       "      <th>In our method, we propose a module to upsample a lowresolution feature map given high-resolution feature maps as guidance, which is closely related to joint upsampling as well as data-dependent upsampling. Joint Upsampling In the literature of image processing, joint upsampling aims at leveraging the guidance image as a prior and transferring the structural details from the guidance image to the target image. [17] constructs a joint filter based on CNNs, which learns to recover the structure details in the guidance image. [31] proposes an end-to-end trainable guided filtering module, which upsamples a lowresolution image conditionally. Our method is related to the aforementioned approaches. However, the proposed JPU is designed for processing feature maps with a large number of channels while [17, 31] are specially designed for pro-cessing 3-channel images, which fail to capture the complex relations in high dimensional feature maps. Besides, the motivation and target of our method is completely different. Data-Dependent Upsampling DUpsampling [29] is also related to our method, which takes advantages of the redundancy in the segmentation label space and is able to recover the pixel-wise prediction from low-resolution outputs of CNNs. Compared to our method, DUpsampling has a strong dependency on the label space, which generalizes poorly to a larger or more complex label space.  Method</th>\n",
       "      <th>In this section, we first introduce the most popular methods for semantic segmentation, named DilatedFCNs. We then reform the architecture of DilatedFCNs with a novel joint upsampling module, Joint Pyramid Upsampling (JPU). Finally, we discuss the proposed JPU in details, before which joint upsampling, dilated convolution and stride convolution are briefly introduced.  DilatedFCN</th>\n",
       "      <th>To exploit Deep CNNs in semantic segmentation, Long et al. [22] transform the CNN designed for image classification into FCN. Taking ResNet-101 as an example, the original CNN contains 5 convolution stages, a global average pooling layer and a linear layer. To construct an FCN, the global average pooling layer and the linear layer are replaced by a convolution layer, which is used to generate the final label map, as shown in Figure 1a . Between each two consecutive convolution stages, stride convolutions and/or spatial pooling layers are employed, resulting in 5 feature maps with gradually reduced spatial resolutions. The spatial resolution of the last feature map in FCN is reduced by a factor of 32, leading to inaccurate predictions about the locations and details. To obtain a final feature map with high resolution, DeepLab [5] removes the downsampling operations before the last two feature maps, as shown in Figure 1c . Besides, the convolution layers inside the last two convolution stages are replaced by dilated convolutions to maintain the receptive field of view, thus named Dilated-FCN. As a result, the resolution of the last feature map is reduced by a factor of 8, which reserves more location and detail information. Following DeepLab, [38, 6] propose a multi-scale context module to capture context information from the last feature map, achieving tremendous success in several segmentation benchmarks.  The Framework of Our Method</th>\n",
       "      <th>To obtain a high-resolution final feature map, methods in DilatedFCN remove the last two downsampling operations from the original FCN, which bring in heavy computation complexity and memory footprint due to the enlarged feature maps. In this paper, we aim at seeking an alternative way to approximate the final feature map of DilatedFCN without computation and memory overload. Meanwhile, we expect the performance of our method to be as good as that of the original DilatedFCNs. To achieve this, we first put back all the stride convolutions removed by DilatedFCN, while replacing all the dilated convolutions with regular convolution layers. As shown in Figure 2 , the backbone of our method is the same as that of the original FCN, where the spatial resolutions of the five feature maps (Conv1−Conv5) are gradually reduced by a factor of 2. To obtain a feature map similar to the final feature map of DilatedFCN, we propose a novel module named Joint Pyramid Upsampling (JPU), which takes the last three feature maps (Conv3−Conv5) as inputs. Then a multi-scale context module (PSP [38] /ASPP [6] ) or a global context module (Encoding [36] ) is employed to produce the final predictions. Compared to DilatedFCN, our method takes 4 times fewer computation and memory resources in 23 residual blocks (69 layers) and 16 times fewer in 3 blocks (9 layers) when the backbone is ResNet-101. Thus, our method runs much faster than DilatedFCN while consuming less memory.  Joint Pyramid Upsampling</th>\n",
       "      <th>The proposed JPU is designed for generating a feature map that approximates the activations of the final feature map from the backbone of DilatedFCN. Such a problem can be reformulated into joint upsampling, which is then resolved by a CNN designed for this task.  Background</th>\n",
       "      <th>Joint Upsampling Given a low-resolution target image and a high-resolution guidance image, joint upsampling aims at generating a high-resolution target image by transferring details and structures from the guidance image. Generally, the low-resolution target image y l is generated by employing a transformation f (•) on the low-resolution guidance image x l , i.e. y l = f (x l ). Given x l and y l , we are required to obtain a transformationf (  y h =f (x h ), wheref (•) = argmin h(•)∈H ||y l − h(x l )||, (1) where H is a set of all possible transformation functions, and || • || is a pre-defined distance metric. Dilated Convolution Dilated convolution is introduced in DeepLab [5] for obtaining high-resolution feature maps while maintaining the receptive field of view. Figure 3a gives an illustration of the dilated convolution in 1D (dilation rate = 2), which can be divided into the following three steps: (1) split the input feature f in into two groups f 0 in and f 1 in according to the parity of the index, (2) process each feature with the same convolution layer, resulting in f 0 out and f 1 out , and (3) merge the two generated features interlaced to obtain the output feature f out . Stride Convolution Stride convolution is proposed to transform the input feature into an output feature with reduced spatial resolution, which is equivalent to the following two steps as shown in Figure 3b: (1) process the input feature f in with a regular convolution to obtain the intermediate feature f m , and (2) remove the elements with an odd index, resulting in f out .  Reformulating into Joint Upsampling</th>\n",
       "      <th>The differences between the backbone of our method and DilatedFCN lie on the last two convolution stages. Taking the 4th convolution stage (Conv4) as an example, in Dilat-edFCN, the input feature map is first processed by a regular convolution layer, followed by a series of dilated convolutions (d=2). Differently, our method first processes the input feature map with a stride convolution (s=2), and then employs several regular convolutions to generate the output. Formally, given the input feature map x, the output feature map y d in DilatedFCN is obtained as follows: Fig 3a) , = ! \"# ! $%&amp; ! \"# ! $%&amp; ! \"# ' ! \"# ( ! $%&amp; ' ! $%&amp; ( Split Merge Conv DilatedConv, d=2 y d = x → C r → C d → ...... → C d n = x → C r → SC r M → ...... → SC r M n (Fig 3a) = x → C r → S → C r → ...... → C r n → M = y m → S → C n r → M = {y 0 m , y 1 m } → C n r → M ( while in our method, the output feature map y s is generated as follows: (Fig 3b) . y s = x → C s → C r → ...... → C r n = x → C r → R → C r → ...... → C r n (Fig 3b) = y m → R → C n r = y 0 m → C n r (3) C r , C d , and C s represent a regular/dilated/stride convolution respectively, and C n r is n layers of regular convolutions. S, M and R are split, merge, and reduce operations in Figure 3 , where adjacent S and M operations can be canceled out. Notably, the convolutions in Equations 2 and 3 are in 1D, which is for simplicity. Similar results can be obtained for 2D convolutions. The aforementioned equations show that y s and y d can be obtained with the same function C n r with different inputs: y 0 m and y m , where the former is downsampled from the latter. Thus, given x and y s , the feature map y that ap-proximates y d can be obtained as follows: EQUATION which is the same as the joint upsampling problem defined in Equation 1. Similar conclusions can be easily obtained for the 5th convolution stage (Conv5).  Solving with CNNs</th>\n",
       "      <th>Equation 4 is an optimization problem, which takes lots of time to converge through the iterative gradient descent. Alternatively, we propose to approximate the optimization process with a CNN module. To achieve this, we first require to generate y m given x, as shown in Equation 4. Then, features from y 0 m and y s need to be gathered for learning the mappingĥ. Finally, a convolution block is required to transform the gathered features into the final prediction y. Following the aforementioned analysis, we design the JPU module as in Figure 4 . Concretely, each input feature map is firstly processed by a regular convolution block (Fig. 4a) , which is designed for (1) generating y m given x, and (2) transforming f m into an embedding space with reduced dimensions. As a result, all the input features are mapped into the same space, which enables a better fusion and reduces the computation complexity. Then, the generated feature maps are upsampled and concatenated, resulting in y c (Fig. 4b) . Four separable convolutions [14, 9] the convolution with dilation rate 1 is employed to capture the relation between y 0 m and the rest part of y m , as shown by the blue box in Figure 5 . Alternatively, the convolutions with dilation rate 2, 4 and 8 are designed for learning the mappingĥ to transform y 0 m into y s , as shown by the green boxes in Figure 5 . Thus, JPU can extract multi-scale context information from multi-level feature maps, which leads to a better performance. This is significantly different from ASPP [6] , which only exploit the information in the last feature map. The extracted features encode the mapping between y 0 m and y s as well as the relation between y 0 m and the rest part of y m . Thus, another regular convolution block is employed, which transforms the features into the final predictions ( Fig. 4c) . Notably, the proposed JPU module solves two closely related joint upsampling problems jointly, which are (1) upsampling Conv4 based on Conv3 (the 4th convolution stage), and (2) upscaling Conv5 with the guidance of the enlarged Conv4 (the 5th convolution stage).  Experiment</th>\n",
       "      <td>In this section, we first introduce the datase...</td>\n",
       "      <td>Dataset Pascal Context dataset [23] is based o...</td>\n",
       "      <td>To show the effectiveness of the proposed meth...</td>\n",
       "      <td>Pascal Context In Table 1 , our method employs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3D Face Reconstruction#NoW Benchmark#Mean Reconstruction Error (mm)</th>\n",
       "      <th>Title</th>\n",
       "      <th>FastFCN: Rethinking Dilated Convolution in the Backbone for Semantic Segmentation Abstract:</th>\n",
       "      <th>Modern approaches for semantic segmentation usually employ dilated convolutions in the backbone to extract highresolution feature maps, which brings heavy computation complexity and memory footprint. To replace the time and memory consuming dilated convolutions, we propose a novel joint upsampling module named Joint Pyramid Upsampling (JPU) by formulating the task of extracting highresolution feature maps into a joint upsampling problem. With the proposed JPU, our method reduces the computation complexity by more than three times without performance loss. Experiments show that JPU is superior to other upsampling modules, which can be plugged into many existing approaches to reduce computation complexity and improve performance. By replacing dilated convolutions with the proposed JPU module, our method achieves the state-of-the-art performance in Pascal Context dataset (mIoU of 53.13%) and ADE20K dataset (final score of 0.5584) while running 3 times faster. Code is available in https://github.com/wuhuikai/FastFCN . Introduction</th>\n",
       "      <th>Semantic segmentation [23, 40, 4] is one of the fundamental tasks in computer vision, with the goal of assigning a semantic label to each pixel of an image. Modern approaches usually employ a Fully Convolution Network (FCN) [22] to address this task, achieving tremendous success among several segmentation benchmarks. The original FCN is proposed by Long et al. [22] , which is transformed from a Convolutional Neural Network (CNN) [16, 15] designed for image classification. Inheriting from the design for image classification, the original FCN downsamples the input image progressively by stride convolutions and/or spatial pooling layers, resulting in a final feature map in low resolution. Although the final feature map encodes rich semantic information, the fine image structure information is lost, leading to inaccurate predictions around the object boundaries. As shown in Figure 1a , the original FCN typically downsamples the input image 5 times, reducing the spatial resolution of the final feature map by a factor of 32. To obtain a high-resolution final feature map, [3, 28, 18, 30, 27] employ the original FCN as the encoder to capture high-level semantic information, and a decoder is designed to gradually recover the spatial information by combining multi-level feature maps from the encoder. As shown in Figure 1b , we term such methods EncoderDecoder, of which the final prediction generated by the decoder is in high resolution. Alternatively, DeepLab [5] removes the last two downsampling operations from the original FCN and introduces dilated (atrous) convolutions to maintain the receptive field of view unchanged. 1 Following DeepLab, [38, 6, 36] employ a multi-scale context module on top of the final feature map, outperforming most EncoderDecoder methods significantly on several segmentation benchmarks. As shown in Figure 1c , the spatial resolution of the last feature map in DilatedFCN is 4 times larger than that in the original FCN, thus maintaining more structure and location information. The dilated convolutions play an important role in maintaining the spatial resolution of the final feature map, leading to superior performance compared to most methods in EncoderDecoder. However, the introduced dilated convolutions bring heavy computation complexity and memory footprint, which limit the usage in many real-time applications. Taking ResNet-101 [13] as an example, compared to the original FCN, 23 residual blocks (69 convolution layers) in DilatedFCN require to take 4 times more computation resources and memory usages, and 3 residual blocks (9 convolution layers) need to take 16 times more resources. We aim at tackling the aforementioned issue caused by dilated convolutions in this paper. To achieve this, we propose a novel joint upsampling module to replace the time and memory consuming dilated convolutions, namely Joint Pyramid Upsampling (JPU). As a result, our method employs the original FCN as the backbone while applying JPU to upsample the low-resolution final feature map with output stride (OS) 32, resulting in a high-resolution feature map (OS=8). Accordingly, the computation time and memory footprint of the whole segmentation framework is dramatically reduced. Meanwhile, there's no performance loss when replacing the dilated convolutions with the proposed JPU. We attribute this to the ability of JPU to exploit multiscale context across multi-level feature maps. To validate the effectiveness of our method, we first conduct a systematical experiment, showing that the proposed JPU can replace dilated convolutions in several popular approaches without performance loss. We then test the proposed method on several segmentation benchmarks. Results show that our method achieves the state-of-the-art performance while running more than 3 times faster. Concretely, we outperform all the baselines on Pascal Context dataset [23] by a large margin, which achieves the state-ofthe-art performance with mIoU of 53.13%. On ADE20K dataset [40] , we obtain the mIoU of 42.75% with ResNet-50 as the backbone, which sets a new record on the val set. Moreover, our method with ResNet-101 achieves the stateof-the-art performance in the test set of ADE20K dataset. In summary, our contributions are three folds, which are: (1) We propose a computationally efficient joint upsampling module named JPU to replace the time and memory consuming dilated convolutions in the backbone. (2) Based on the proposed JPU, the computation time and memory footprint of the whole segmentation framework can be reduced by a factor of more than 3 and meanwhile achieves better performance. (3) Our method achieves the new state-ofthe-art performance in both Pascal Context dataset (mIoU of 53.13%) and ADE20K dataset (mIoU of 42.75% with ResNet-50 as the backbone on the val set and final score of 0.5584 with ResNet-101 on the test set).  Related Work</th>\n",
       "      <th>In this section, we first give an overview on methods for semantic segmentation, which can be categorized into two directions. We then introduce some related works on upsampling.  Semantic Segmentation</th>\n",
       "      <th>FCNs [22] have achieved huge success in semantic segmentation. Following FCN, there're two prominent directions, namely DilatedFCN and EncoderDecoder. Dilated-FCNs [11, 34, 7, 6, 38, 36, 5] utilize dilated convolutions to keep the receptive field of view and employ a multi-scale context module to process high-level feature maps. Alternatively, EncoderDecoders [24, 28, 18, 1, 26, 12, 33, 37] propose to utilize an encoder to extract multi-level feature maps, which are then combined into the final prediction by a decoder. DilatedFCN In order to capture multi-scale context information on the high-resolution final feature map, PSP-Net [38] performs pooling operations at multiple grid scales while DeepLabV3 [6] employs parallel atrous convolutions with different rates named ASPP. Alternatively, EncNet [36] utilizes the Context Encoding Module to capture global contextual information. Differently, our method proposes a joint upsampling module named JPU to replace the dilated convolutions in the backbone of DilatedFCNs, which can JPU Encoding/PSP/ASPP Head C o n v 5 , 3 2 x C o n v 4 , 1 6 x C o n v 3 , 8 x C o n v 2 , 4 x C o n v 1 , 2 x 8 x 8 x Figure 2 : Framework Overview of Our Method. Our method employs the same backbone as the original FCN. After the backbone, a novel upsampling module named Joint Pyramid Upsampling (JPU) is proposed, which takes the last three feature maps as the inputs and generates a high-resolution feature map. A multi-scale/global context module is then employed to produce the final label map. Best viewed in color. reduce computation complexity dramatically without performance loss. EncoderDecoder To gradually recover the spatial information, [28] introduces skip connections to construct U-Net, which combines the encoder features and the corresponding decoder activations. [18] proposes a multipath refinement network, which explicitly exploits all the information available along the down-sampling process. DeepLabV3+ [8] combines the advantages of DilatedFCN and EncoderDecoder, which employs DeepLabV3 as the encoder. Our method is complementary to DeepLabV3+, which can reduce the computation overload of DeepLabV3 without performance loss.  Upsampling</th>\n",
       "      <th>In our method, we propose a module to upsample a lowresolution feature map given high-resolution feature maps as guidance, which is closely related to joint upsampling as well as data-dependent upsampling. Joint Upsampling In the literature of image processing, joint upsampling aims at leveraging the guidance image as a prior and transferring the structural details from the guidance image to the target image. [17] constructs a joint filter based on CNNs, which learns to recover the structure details in the guidance image. [31] proposes an end-to-end trainable guided filtering module, which upsamples a lowresolution image conditionally. Our method is related to the aforementioned approaches. However, the proposed JPU is designed for processing feature maps with a large number of channels while [17, 31] are specially designed for pro-cessing 3-channel images, which fail to capture the complex relations in high dimensional feature maps. Besides, the motivation and target of our method is completely different. Data-Dependent Upsampling DUpsampling [29] is also related to our method, which takes advantages of the redundancy in the segmentation label space and is able to recover the pixel-wise prediction from low-resolution outputs of CNNs. Compared to our method, DUpsampling has a strong dependency on the label space, which generalizes poorly to a larger or more complex label space.  Method</th>\n",
       "      <th>In this section, we first introduce the most popular methods for semantic segmentation, named DilatedFCNs. We then reform the architecture of DilatedFCNs with a novel joint upsampling module, Joint Pyramid Upsampling (JPU). Finally, we discuss the proposed JPU in details, before which joint upsampling, dilated convolution and stride convolution are briefly introduced.  DilatedFCN</th>\n",
       "      <th>To exploit Deep CNNs in semantic segmentation, Long et al. [22] transform the CNN designed for image classification into FCN. Taking ResNet-101 as an example, the original CNN contains 5 convolution stages, a global average pooling layer and a linear layer. To construct an FCN, the global average pooling layer and the linear layer are replaced by a convolution layer, which is used to generate the final label map, as shown in Figure 1a . Between each two consecutive convolution stages, stride convolutions and/or spatial pooling layers are employed, resulting in 5 feature maps with gradually reduced spatial resolutions. The spatial resolution of the last feature map in FCN is reduced by a factor of 32, leading to inaccurate predictions about the locations and details. To obtain a final feature map with high resolution, DeepLab [5] removes the downsampling operations before the last two feature maps, as shown in Figure 1c . Besides, the convolution layers inside the last two convolution stages are replaced by dilated convolutions to maintain the receptive field of view, thus named Dilated-FCN. As a result, the resolution of the last feature map is reduced by a factor of 8, which reserves more location and detail information. Following DeepLab, [38, 6] propose a multi-scale context module to capture context information from the last feature map, achieving tremendous success in several segmentation benchmarks.  The Framework of Our Method</th>\n",
       "      <th>To obtain a high-resolution final feature map, methods in DilatedFCN remove the last two downsampling operations from the original FCN, which bring in heavy computation complexity and memory footprint due to the enlarged feature maps. In this paper, we aim at seeking an alternative way to approximate the final feature map of DilatedFCN without computation and memory overload. Meanwhile, we expect the performance of our method to be as good as that of the original DilatedFCNs. To achieve this, we first put back all the stride convolutions removed by DilatedFCN, while replacing all the dilated convolutions with regular convolution layers. As shown in Figure 2 , the backbone of our method is the same as that of the original FCN, where the spatial resolutions of the five feature maps (Conv1−Conv5) are gradually reduced by a factor of 2. To obtain a feature map similar to the final feature map of DilatedFCN, we propose a novel module named Joint Pyramid Upsampling (JPU), which takes the last three feature maps (Conv3−Conv5) as inputs. Then a multi-scale context module (PSP [38] /ASPP [6] ) or a global context module (Encoding [36] ) is employed to produce the final predictions. Compared to DilatedFCN, our method takes 4 times fewer computation and memory resources in 23 residual blocks (69 layers) and 16 times fewer in 3 blocks (9 layers) when the backbone is ResNet-101. Thus, our method runs much faster than DilatedFCN while consuming less memory.  Joint Pyramid Upsampling</th>\n",
       "      <th>The proposed JPU is designed for generating a feature map that approximates the activations of the final feature map from the backbone of DilatedFCN. Such a problem can be reformulated into joint upsampling, which is then resolved by a CNN designed for this task.  Background</th>\n",
       "      <th>Joint Upsampling Given a low-resolution target image and a high-resolution guidance image, joint upsampling aims at generating a high-resolution target image by transferring details and structures from the guidance image. Generally, the low-resolution target image y l is generated by employing a transformation f (•) on the low-resolution guidance image x l , i.e. y l = f (x l ). Given x l and y l , we are required to obtain a transformationf (  y h =f (x h ), wheref (•) = argmin h(•)∈H ||y l − h(x l )||, (1) where H is a set of all possible transformation functions, and || • || is a pre-defined distance metric. Dilated Convolution Dilated convolution is introduced in DeepLab [5] for obtaining high-resolution feature maps while maintaining the receptive field of view. Figure 3a gives an illustration of the dilated convolution in 1D (dilation rate = 2), which can be divided into the following three steps: (1) split the input feature f in into two groups f 0 in and f 1 in according to the parity of the index, (2) process each feature with the same convolution layer, resulting in f 0 out and f 1 out , and (3) merge the two generated features interlaced to obtain the output feature f out . Stride Convolution Stride convolution is proposed to transform the input feature into an output feature with reduced spatial resolution, which is equivalent to the following two steps as shown in Figure 3b: (1) process the input feature f in with a regular convolution to obtain the intermediate feature f m , and (2) remove the elements with an odd index, resulting in f out .  Reformulating into Joint Upsampling</th>\n",
       "      <th>The differences between the backbone of our method and DilatedFCN lie on the last two convolution stages. Taking the 4th convolution stage (Conv4) as an example, in Dilat-edFCN, the input feature map is first processed by a regular convolution layer, followed by a series of dilated convolutions (d=2). Differently, our method first processes the input feature map with a stride convolution (s=2), and then employs several regular convolutions to generate the output. Formally, given the input feature map x, the output feature map y d in DilatedFCN is obtained as follows: Fig 3a) , = ! \"# ! $%&amp; ! \"# ! $%&amp; ! \"# ' ! \"# ( ! $%&amp; ' ! $%&amp; ( Split Merge Conv DilatedConv, d=2 y d = x → C r → C d → ...... → C d n = x → C r → SC r M → ...... → SC r M n (Fig 3a) = x → C r → S → C r → ...... → C r n → M = y m → S → C n r → M = {y 0 m , y 1 m } → C n r → M ( while in our method, the output feature map y s is generated as follows: (Fig 3b) . y s = x → C s → C r → ...... → C r n = x → C r → R → C r → ...... → C r n (Fig 3b) = y m → R → C n r = y 0 m → C n r (3) C r , C d , and C s represent a regular/dilated/stride convolution respectively, and C n r is n layers of regular convolutions. S, M and R are split, merge, and reduce operations in Figure 3 , where adjacent S and M operations can be canceled out. Notably, the convolutions in Equations 2 and 3 are in 1D, which is for simplicity. Similar results can be obtained for 2D convolutions. The aforementioned equations show that y s and y d can be obtained with the same function C n r with different inputs: y 0 m and y m , where the former is downsampled from the latter. Thus, given x and y s , the feature map y that ap-proximates y d can be obtained as follows: EQUATION which is the same as the joint upsampling problem defined in Equation 1. Similar conclusions can be easily obtained for the 5th convolution stage (Conv5).  Solving with CNNs</th>\n",
       "      <th>Equation 4 is an optimization problem, which takes lots of time to converge through the iterative gradient descent. Alternatively, we propose to approximate the optimization process with a CNN module. To achieve this, we first require to generate y m given x, as shown in Equation 4. Then, features from y 0 m and y s need to be gathered for learning the mappingĥ. Finally, a convolution block is required to transform the gathered features into the final prediction y. Following the aforementioned analysis, we design the JPU module as in Figure 4 . Concretely, each input feature map is firstly processed by a regular convolution block (Fig. 4a) , which is designed for (1) generating y m given x, and (2) transforming f m into an embedding space with reduced dimensions. As a result, all the input features are mapped into the same space, which enables a better fusion and reduces the computation complexity. Then, the generated feature maps are upsampled and concatenated, resulting in y c (Fig. 4b) . Four separable convolutions [14, 9] the convolution with dilation rate 1 is employed to capture the relation between y 0 m and the rest part of y m , as shown by the blue box in Figure 5 . Alternatively, the convolutions with dilation rate 2, 4 and 8 are designed for learning the mappingĥ to transform y 0 m into y s , as shown by the green boxes in Figure 5 . Thus, JPU can extract multi-scale context information from multi-level feature maps, which leads to a better performance. This is significantly different from ASPP [6] , which only exploit the information in the last feature map. The extracted features encode the mapping between y 0 m and y s as well as the relation between y 0 m and the rest part of y m . Thus, another regular convolution block is employed, which transforms the features into the final predictions ( Fig. 4c) . Notably, the proposed JPU module solves two closely related joint upsampling problems jointly, which are (1) upsampling Conv4 based on Conv3 (the 4th convolution stage), and (2) upscaling Conv5 with the guidance of the enlarged Conv4 (the 5th convolution stage).  Experiment</th>\n",
       "      <td>In this section, we first introduce the datase...</td>\n",
       "      <td>Dataset Pascal Context dataset [23] is based o...</td>\n",
       "      <td>To show the effectiveness of the proposed meth...</td>\n",
       "      <td>Pascal Context In Table 1 , our method employs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                label  \\\n",
       "False 1903.11816v1 3D Face Reconstruction#AFLW2000-3D#Mean NME        Title FastFCN: Rethinking Dilated Convolution in the ... Modern approaches for semantic segmentation usu... Semantic segmentation [23, 40, 4] is one of the... In this section, we first give an overview on m... FCNs [22] have achieved huge success in semanti... In our method, we propose a module to upsample ... In this section, we first introduce the most po... To exploit Deep CNNs in semantic segmentation, ... To obtain a high-resolution final feature map, ... The proposed JPU is designed for generating a f... Joint Upsampling Given a low-resolution target ... The differences between the backbone of our met... Equation 4 is an optimization problem, which ta...  In this section, we first introduce the datase...   \n",
       "                   3D Face Reconstruction#AFLW2000-3D#NME             Title FastFCN: Rethinking Dilated Convolution in the ... Modern approaches for semantic segmentation usu... Semantic segmentation [23, 40, 4] is one of the... In this section, we first give an overview on m... FCNs [22] have achieved huge success in semanti... In our method, we propose a module to upsample ... In this section, we first introduce the most po... To exploit Deep CNNs in semantic segmentation, ... To obtain a high-resolution final feature map, ... The proposed JPU is designed for generating a f... Joint Upsampling Given a low-resolution target ... The differences between the backbone of our met... Equation 4 is an optimization problem, which ta...  In this section, we first introduce the datase...   \n",
       "                   3D Face Reconstruction#Florence#Average 3D Error   Title FastFCN: Rethinking Dilated Convolution in the ... Modern approaches for semantic segmentation usu... Semantic segmentation [23, 40, 4] is one of the... In this section, we first give an overview on m... FCNs [22] have achieved huge success in semanti... In our method, we propose a module to upsample ... In this section, we first introduce the most po... To exploit Deep CNNs in semantic segmentation, ... To obtain a high-resolution final feature map, ... The proposed JPU is designed for generating a f... Joint Upsampling Given a low-resolution target ... The differences between the backbone of our met... Equation 4 is an optimization problem, which ta...  In this section, we first introduce the datase...   \n",
       "                   3D Face Reconstruction#Florence#Mean NME           Title FastFCN: Rethinking Dilated Convolution in the ... Modern approaches for semantic segmentation usu... Semantic segmentation [23, 40, 4] is one of the... In this section, we first give an overview on m... FCNs [22] have achieved huge success in semanti... In our method, we propose a module to upsample ... In this section, we first introduce the most po... To exploit Deep CNNs in semantic segmentation, ... To obtain a high-resolution final feature map, ... The proposed JPU is designed for generating a f... Joint Upsampling Given a low-resolution target ... The differences between the backbone of our met... Equation 4 is an optimization problem, which ta...  In this section, we first introduce the datase...   \n",
       "                   3D Face Reconstruction#NoW Benchmark#Mean Recon... Title FastFCN: Rethinking Dilated Convolution in the ... Modern approaches for semantic segmentation usu... Semantic segmentation [23, 40, 4] is one of the... In this section, we first give an overview on m... FCNs [22] have achieved huge success in semanti... In our method, we propose a module to upsample ... In this section, we first introduce the most po... To exploit Deep CNNs in semantic segmentation, ... To obtain a high-resolution final feature map, ... The proposed JPU is designed for generating a f... Joint Upsampling Given a low-resolution target ... The differences between the backbone of our met... Equation 4 is an optimization problem, which ta...  In this section, we first introduce the datase...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                title  \\\n",
       "False 1903.11816v1 3D Face Reconstruction#AFLW2000-3D#Mean NME        Title FastFCN: Rethinking Dilated Convolution in the ... Modern approaches for semantic segmentation usu... Semantic segmentation [23, 40, 4] is one of the... In this section, we first give an overview on m... FCNs [22] have achieved huge success in semanti... In our method, we propose a module to upsample ... In this section, we first introduce the most po... To exploit Deep CNNs in semantic segmentation, ... To obtain a high-resolution final feature map, ... The proposed JPU is designed for generating a f... Joint Upsampling Given a low-resolution target ... The differences between the backbone of our met... Equation 4 is an optimization problem, which ta...  Dataset Pascal Context dataset [23] is based o...   \n",
       "                   3D Face Reconstruction#AFLW2000-3D#NME             Title FastFCN: Rethinking Dilated Convolution in the ... Modern approaches for semantic segmentation usu... Semantic segmentation [23, 40, 4] is one of the... In this section, we first give an overview on m... FCNs [22] have achieved huge success in semanti... In our method, we propose a module to upsample ... In this section, we first introduce the most po... To exploit Deep CNNs in semantic segmentation, ... To obtain a high-resolution final feature map, ... The proposed JPU is designed for generating a f... Joint Upsampling Given a low-resolution target ... The differences between the backbone of our met... Equation 4 is an optimization problem, which ta...  Dataset Pascal Context dataset [23] is based o...   \n",
       "                   3D Face Reconstruction#Florence#Average 3D Error   Title FastFCN: Rethinking Dilated Convolution in the ... Modern approaches for semantic segmentation usu... Semantic segmentation [23, 40, 4] is one of the... In this section, we first give an overview on m... FCNs [22] have achieved huge success in semanti... In our method, we propose a module to upsample ... In this section, we first introduce the most po... To exploit Deep CNNs in semantic segmentation, ... To obtain a high-resolution final feature map, ... The proposed JPU is designed for generating a f... Joint Upsampling Given a low-resolution target ... The differences between the backbone of our met... Equation 4 is an optimization problem, which ta...  Dataset Pascal Context dataset [23] is based o...   \n",
       "                   3D Face Reconstruction#Florence#Mean NME           Title FastFCN: Rethinking Dilated Convolution in the ... Modern approaches for semantic segmentation usu... Semantic segmentation [23, 40, 4] is one of the... In this section, we first give an overview on m... FCNs [22] have achieved huge success in semanti... In our method, we propose a module to upsample ... In this section, we first introduce the most po... To exploit Deep CNNs in semantic segmentation, ... To obtain a high-resolution final feature map, ... The proposed JPU is designed for generating a f... Joint Upsampling Given a low-resolution target ... The differences between the backbone of our met... Equation 4 is an optimization problem, which ta...  Dataset Pascal Context dataset [23] is based o...   \n",
       "                   3D Face Reconstruction#NoW Benchmark#Mean Recon... Title FastFCN: Rethinking Dilated Convolution in the ... Modern approaches for semantic segmentation usu... Semantic segmentation [23, 40, 4] is one of the... In this section, we first give an overview on m... FCNs [22] have achieved huge success in semanti... In our method, we propose a module to upsample ... In this section, we first introduce the most po... To exploit Deep CNNs in semantic segmentation, ... To obtain a high-resolution final feature map, ... The proposed JPU is designed for generating a f... Joint Upsampling Given a low-resolution target ... The differences between the backbone of our met... Equation 4 is an optimization problem, which ta...  Dataset Pascal Context dataset [23] is based o...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  TDM  \\\n",
       "False 1903.11816v1 3D Face Reconstruction#AFLW2000-3D#Mean NME        Title FastFCN: Rethinking Dilated Convolution in the ... Modern approaches for semantic segmentation usu... Semantic segmentation [23, 40, 4] is one of the... In this section, we first give an overview on m... FCNs [22] have achieved huge success in semanti... In our method, we propose a module to upsample ... In this section, we first introduce the most po... To exploit Deep CNNs in semantic segmentation, ... To obtain a high-resolution final feature map, ... The proposed JPU is designed for generating a f... Joint Upsampling Given a low-resolution target ... The differences between the backbone of our met... Equation 4 is an optimization problem, which ta...  To show the effectiveness of the proposed meth...   \n",
       "                   3D Face Reconstruction#AFLW2000-3D#NME             Title FastFCN: Rethinking Dilated Convolution in the ... Modern approaches for semantic segmentation usu... Semantic segmentation [23, 40, 4] is one of the... In this section, we first give an overview on m... FCNs [22] have achieved huge success in semanti... In our method, we propose a module to upsample ... In this section, we first introduce the most po... To exploit Deep CNNs in semantic segmentation, ... To obtain a high-resolution final feature map, ... The proposed JPU is designed for generating a f... Joint Upsampling Given a low-resolution target ... The differences between the backbone of our met... Equation 4 is an optimization problem, which ta...  To show the effectiveness of the proposed meth...   \n",
       "                   3D Face Reconstruction#Florence#Average 3D Error   Title FastFCN: Rethinking Dilated Convolution in the ... Modern approaches for semantic segmentation usu... Semantic segmentation [23, 40, 4] is one of the... In this section, we first give an overview on m... FCNs [22] have achieved huge success in semanti... In our method, we propose a module to upsample ... In this section, we first introduce the most po... To exploit Deep CNNs in semantic segmentation, ... To obtain a high-resolution final feature map, ... The proposed JPU is designed for generating a f... Joint Upsampling Given a low-resolution target ... The differences between the backbone of our met... Equation 4 is an optimization problem, which ta...  To show the effectiveness of the proposed meth...   \n",
       "                   3D Face Reconstruction#Florence#Mean NME           Title FastFCN: Rethinking Dilated Convolution in the ... Modern approaches for semantic segmentation usu... Semantic segmentation [23, 40, 4] is one of the... In this section, we first give an overview on m... FCNs [22] have achieved huge success in semanti... In our method, we propose a module to upsample ... In this section, we first introduce the most po... To exploit Deep CNNs in semantic segmentation, ... To obtain a high-resolution final feature map, ... The proposed JPU is designed for generating a f... Joint Upsampling Given a low-resolution target ... The differences between the backbone of our met... Equation 4 is an optimization problem, which ta...  To show the effectiveness of the proposed meth...   \n",
       "                   3D Face Reconstruction#NoW Benchmark#Mean Recon... Title FastFCN: Rethinking Dilated Convolution in the ... Modern approaches for semantic segmentation usu... Semantic segmentation [23, 40, 4] is one of the... In this section, we first give an overview on m... FCNs [22] have achieved huge success in semanti... In our method, we propose a module to upsample ... In this section, we first introduce the most po... To exploit Deep CNNs in semantic segmentation, ... To obtain a high-resolution final feature map, ... The proposed JPU is designed for generating a f... Joint Upsampling Given a low-resolution target ... The differences between the backbone of our met... Equation 4 is an optimization problem, which ta...  To show the effectiveness of the proposed meth...   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              Context  \n",
       "False 1903.11816v1 3D Face Reconstruction#AFLW2000-3D#Mean NME        Title FastFCN: Rethinking Dilated Convolution in the ... Modern approaches for semantic segmentation usu... Semantic segmentation [23, 40, 4] is one of the... In this section, we first give an overview on m... FCNs [22] have achieved huge success in semanti... In our method, we propose a module to upsample ... In this section, we first introduce the most po... To exploit Deep CNNs in semantic segmentation, ... To obtain a high-resolution final feature map, ... The proposed JPU is designed for generating a f... Joint Upsampling Given a low-resolution target ... The differences between the backbone of our met... Equation 4 is an optimization problem, which ta...  Pascal Context In Table 1 , our method employs...  \n",
       "                   3D Face Reconstruction#AFLW2000-3D#NME             Title FastFCN: Rethinking Dilated Convolution in the ... Modern approaches for semantic segmentation usu... Semantic segmentation [23, 40, 4] is one of the... In this section, we first give an overview on m... FCNs [22] have achieved huge success in semanti... In our method, we propose a module to upsample ... In this section, we first introduce the most po... To exploit Deep CNNs in semantic segmentation, ... To obtain a high-resolution final feature map, ... The proposed JPU is designed for generating a f... Joint Upsampling Given a low-resolution target ... The differences between the backbone of our met... Equation 4 is an optimization problem, which ta...  Pascal Context In Table 1 , our method employs...  \n",
       "                   3D Face Reconstruction#Florence#Average 3D Error   Title FastFCN: Rethinking Dilated Convolution in the ... Modern approaches for semantic segmentation usu... Semantic segmentation [23, 40, 4] is one of the... In this section, we first give an overview on m... FCNs [22] have achieved huge success in semanti... In our method, we propose a module to upsample ... In this section, we first introduce the most po... To exploit Deep CNNs in semantic segmentation, ... To obtain a high-resolution final feature map, ... The proposed JPU is designed for generating a f... Joint Upsampling Given a low-resolution target ... The differences between the backbone of our met... Equation 4 is an optimization problem, which ta...  Pascal Context In Table 1 , our method employs...  \n",
       "                   3D Face Reconstruction#Florence#Mean NME           Title FastFCN: Rethinking Dilated Convolution in the ... Modern approaches for semantic segmentation usu... Semantic segmentation [23, 40, 4] is one of the... In this section, we first give an overview on m... FCNs [22] have achieved huge success in semanti... In our method, we propose a module to upsample ... In this section, we first introduce the most po... To exploit Deep CNNs in semantic segmentation, ... To obtain a high-resolution final feature map, ... The proposed JPU is designed for generating a f... Joint Upsampling Given a low-resolution target ... The differences between the backbone of our met... Equation 4 is an optimization problem, which ta...  Pascal Context In Table 1 , our method employs...  \n",
       "                   3D Face Reconstruction#NoW Benchmark#Mean Recon... Title FastFCN: Rethinking Dilated Convolution in the ... Modern approaches for semantic segmentation usu... Semantic segmentation [23, 40, 4] is one of the... In this section, we first give an overview on m... FCNs [22] have achieved huge success in semanti... In our method, we propose a module to upsample ... In this section, we first introduce the most po... To exploit Deep CNNs in semantic segmentation, ... To obtain a high-resolution final feature map, ... The proposed JPU is designed for generating a f... Joint Upsampling Given a low-resolution target ... The differences between the backbone of our met... Equation 4 is an optimization problem, which ta...  Pascal Context In Table 1 , our method employs...  "
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "342e7fcd-5c4e-45c8-bc58-0d78884842d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>1607.07155v1</td>\n",
       "      <td>Face Detection#WIDER Face (Hard)#AP</td>\n",
       "      <td>N o n e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>1607.07155v1</td>\n",
       "      <td>Pedestrian Detection#Caltech#Reasonable Miss Rate</td>\n",
       "      <td>N o n e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>True</td>\n",
       "      <td>1911.01616v4</td>\n",
       "      <td>Aspect Sentiment Triplet Extraction#SemEval#F1</td>\n",
       "      <td>N o n e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>True</td>\n",
       "      <td>1412.7259v3</td>\n",
       "      <td>Image Classification#MNIST#Percentage error</td>\n",
       "      <td>N o n e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>True</td>\n",
       "      <td>1412.7259v3</td>\n",
       "      <td>Image Classification#STL-10#Percentage correct</td>\n",
       "      <td>N o n e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>True</td>\n",
       "      <td>1704.07156v1</td>\n",
       "      <td>Part-Of-Speech Tagging#Penn Treebank#Accuracy</td>\n",
       "      <td>N o n e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>True</td>\n",
       "      <td>1704.07156v1</td>\n",
       "      <td>Grammatical Error Detection#FCE#F0.5</td>\n",
       "      <td>N o n e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>True</td>\n",
       "      <td>1704.07156v1</td>\n",
       "      <td>Grammatical Error Detection#CoNLL-2014 A2#F0.5</td>\n",
       "      <td>N o n e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>True</td>\n",
       "      <td>1704.07156v1</td>\n",
       "      <td>Grammatical Error Detection#CoNLL-2014 A1#F0.5</td>\n",
       "      <td>N o n e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>True</td>\n",
       "      <td>2003.12060v1</td>\n",
       "      <td>Few-Shot Image Classification#CUB 200 5-way 1-...</td>\n",
       "      <td>N o n e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    label         title                                                TDM  \\\n",
       "0    True  1607.07155v1                Face Detection#WIDER Face (Hard)#AP   \n",
       "1    True  1607.07155v1  Pedestrian Detection#Caltech#Reasonable Miss Rate   \n",
       "12   True  1911.01616v4     Aspect Sentiment Triplet Extraction#SemEval#F1   \n",
       "23   True   1412.7259v3        Image Classification#MNIST#Percentage error   \n",
       "24   True   1412.7259v3     Image Classification#STL-10#Percentage correct   \n",
       "35   True  1704.07156v1      Part-Of-Speech Tagging#Penn Treebank#Accuracy   \n",
       "36   True  1704.07156v1               Grammatical Error Detection#FCE#F0.5   \n",
       "37   True  1704.07156v1     Grammatical Error Detection#CoNLL-2014 A2#F0.5   \n",
       "38   True  1704.07156v1     Grammatical Error Detection#CoNLL-2014 A1#F0.5   \n",
       "49   True  2003.12060v1  Few-Shot Image Classification#CUB 200 5-way 1-...   \n",
       "\n",
       "    Context  \n",
       "0   N o n e  \n",
       "1   N o n e  \n",
       "12  N o n e  \n",
       "23  N o n e  \n",
       "24  N o n e  \n",
       "35  N o n e  \n",
       "36  N o n e  \n",
       "37  N o n e  \n",
       "38  N o n e  \n",
       "49  N o n e  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.label==True].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "4252297f-ce4e-48bf-be4d-169ed67dcae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49978</th>\n",
       "      <td>True</td>\n",
       "      <td>1911.08251v2</td>\n",
       "      <td>Image Classification#STL-10#Percentage correct</td>\n",
       "      <td>N o n e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49979</th>\n",
       "      <td>False</td>\n",
       "      <td>1911.08251v2</td>\n",
       "      <td>3D Absolute Human Pose Estimation#Human3.6M#MRPE</td>\n",
       "      <td>N o n e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49980</th>\n",
       "      <td>False</td>\n",
       "      <td>1911.08251v2</td>\n",
       "      <td>3D Action Recognition#100 sleep nights of 8 ca...</td>\n",
       "      <td>N o n e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49981</th>\n",
       "      <td>False</td>\n",
       "      <td>1911.08251v2</td>\n",
       "      <td>3D Canonical Hand Pose Estimation#Ego3DHands#AUC</td>\n",
       "      <td>N o n e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49982</th>\n",
       "      <td>False</td>\n",
       "      <td>1911.08251v2</td>\n",
       "      <td>3D Canonical Hand Pose Estimation#RHP#AUC</td>\n",
       "      <td>N o n e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label         title                                                TDM  \\\n",
       "49978   True  1911.08251v2     Image Classification#STL-10#Percentage correct   \n",
       "49979  False  1911.08251v2   3D Absolute Human Pose Estimation#Human3.6M#MRPE   \n",
       "49980  False  1911.08251v2  3D Action Recognition#100 sleep nights of 8 ca...   \n",
       "49981  False  1911.08251v2   3D Canonical Hand Pose Estimation#Ego3DHands#AUC   \n",
       "49982  False  1911.08251v2          3D Canonical Hand Pose Estimation#RHP#AUC   \n",
       "\n",
       "       Context  \n",
       "49978  N o n e  \n",
       "49979  N o n e  \n",
       "49980  N o n e  \n",
       "49981  N o n e  \n",
       "49982  N o n e  "
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.title==\"1911.08251v2\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "9673ca02-7a0c-49e9-8b6c-4f5c3543c90f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>True</td>\n",
       "      <td>2005.03059v3</td>\n",
       "      <td>unknown</td>\n",
       "      <td>N o n e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>True</td>\n",
       "      <td>2102.02717v3</td>\n",
       "      <td>unknown</td>\n",
       "      <td>N o n e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>True</td>\n",
       "      <td>2102.05126v1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>N o n e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>True</td>\n",
       "      <td>2008.06223v2</td>\n",
       "      <td>unknown</td>\n",
       "      <td>N o n e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>True</td>\n",
       "      <td>2003.06520v1</td>\n",
       "      <td>unknown</td>\n",
       "      <td>N o n e</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label         title      TDM  Context\n",
       "103   True  2005.03059v3  unknown  N o n e\n",
       "235   True  2102.02717v3  unknown  N o n e\n",
       "276   True  2102.05126v1  unknown  N o n e\n",
       "299   True  2008.06223v2  unknown  N o n e\n",
       "346   True  2003.06520v1  unknown  N o n e"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.TDM==\"unknown\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78535cb-d4d4-4977-b368-be82f0b9db17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"len\"]train.Context.apply(lambda content: len(content),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a9a7d1-c57f-4d33-8e29-37085c4eeec8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2152aaf-b1b0-4f35-ae95-d421ef810e83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a9411134-f456-4215-84ad-5b4680188ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "tdm = pd.read_csv(f\"/nfs/home/kabenamualus/Research/task-dataset-metric-extraction/data/paperwithcode/new/60Neg800unk/twofoldwithunk/fold1/train.tsv\", \n",
    "                    sep=\"\\t\", names=[\"label\", \"title\", \"TDM\", \"Context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "81df55d9-b393-4a5d-90a4-96e3426e2f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>1810.02575v1.pdf</td>\n",
       "      <td>Semantic Segmentation; Nighttime Driving; mIoU</td>\n",
       "      <td>Dark Model Adaptation: Semantic Image Segmenta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>1810.02575v1.pdf</td>\n",
       "      <td>Extractive Text Summarization; DebateSum; ROUGE-L</td>\n",
       "      <td>Dark Model Adaptation: Semantic Image Segmenta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>False</td>\n",
       "      <td>1810.02575v1.pdf</td>\n",
       "      <td>Action Recognition; Something-Something V1; To...</td>\n",
       "      <td>Dark Model Adaptation: Semantic Image Segmenta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>False</td>\n",
       "      <td>1810.02575v1.pdf</td>\n",
       "      <td>Multi-Object Tracking; MOTS20; sMOTSA</td>\n",
       "      <td>Dark Model Adaptation: Semantic Image Segmenta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>False</td>\n",
       "      <td>1810.02575v1.pdf</td>\n",
       "      <td>Continuous Control; PyBullet Ant; Return</td>\n",
       "      <td>Dark Model Adaptation: Semantic Image Segmenta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label             title                                                TDM  \\\n",
       "0   True  1810.02575v1.pdf     Semantic Segmentation; Nighttime Driving; mIoU   \n",
       "1  False  1810.02575v1.pdf  Extractive Text Summarization; DebateSum; ROUGE-L   \n",
       "2  False  1810.02575v1.pdf  Action Recognition; Something-Something V1; To...   \n",
       "3  False  1810.02575v1.pdf              Multi-Object Tracking; MOTS20; sMOTSA   \n",
       "4  False  1810.02575v1.pdf           Continuous Control; PyBullet Ant; Return   \n",
       "\n",
       "                                             Context  \n",
       "0  Dark Model Adaptation: Semantic Image Segmenta...  \n",
       "1  Dark Model Adaptation: Semantic Image Segmenta...  \n",
       "2  Dark Model Adaptation: Semantic Image Segmenta...  \n",
       "3  Dark Model Adaptation: Semantic Image Segmenta...  \n",
       "4  Dark Model Adaptation: Semantic Image Segmenta...  "
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "ff623608-f14c-4c3b-897d-81b94c8ca09d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>256003</th>\n",
       "      <td>False</td>\n",
       "      <td>1307.0414v1.pdf</td>\n",
       "      <td>Skeleton Based Action Recognition; SHREC 2017 ...</td>\n",
       "      <td>Challenges in Representation Learning: A repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256004</th>\n",
       "      <td>False</td>\n",
       "      <td>1307.0414v1.pdf</td>\n",
       "      <td>Fake News Detection; FNC-1; Weighted Accuracy</td>\n",
       "      <td>Challenges in Representation Learning: A repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256005</th>\n",
       "      <td>False</td>\n",
       "      <td>1307.0414v1.pdf</td>\n",
       "      <td>Multimodal Unsupervised Image-To-Image Transla...</td>\n",
       "      <td>Challenges in Representation Learning: A repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256006</th>\n",
       "      <td>False</td>\n",
       "      <td>1307.0414v1.pdf</td>\n",
       "      <td>Graph Classification; PTC; Accuracy</td>\n",
       "      <td>Challenges in Representation Learning: A repor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256007</th>\n",
       "      <td>False</td>\n",
       "      <td>1307.0414v1.pdf</td>\n",
       "      <td>Pose Estimation; UPenn Action; Mean PCK@0.2</td>\n",
       "      <td>Challenges in Representation Learning: A repor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        label            title  \\\n",
       "256003  False  1307.0414v1.pdf   \n",
       "256004  False  1307.0414v1.pdf   \n",
       "256005  False  1307.0414v1.pdf   \n",
       "256006  False  1307.0414v1.pdf   \n",
       "256007  False  1307.0414v1.pdf   \n",
       "\n",
       "                                                      TDM  \\\n",
       "256003  Skeleton Based Action Recognition; SHREC 2017 ...   \n",
       "256004      Fake News Detection; FNC-1; Weighted Accuracy   \n",
       "256005  Multimodal Unsupervised Image-To-Image Transla...   \n",
       "256006                Graph Classification; PTC; Accuracy   \n",
       "256007        Pose Estimation; UPenn Action; Mean PCK@0.2   \n",
       "\n",
       "                                                  Context  \n",
       "256003  Challenges in Representation Learning: A repor...  \n",
       "256004  Challenges in Representation Learning: A repor...  \n",
       "256005  Challenges in Representation Learning: A repor...  \n",
       "256006  Challenges in Representation Learning: A repor...  \n",
       "256007  Challenges in Representation Learning: A repor...  "
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "5c53b092-3ae2-41d8-ad54-e0f61fbd2d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>TDM</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>True</td>\n",
       "      <td>1908.05786v1.pdf</td>\n",
       "      <td>unknow</td>\n",
       "      <td>TASED-Net: Temporally-Aggregating Spatial Enco...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label             title     TDM  \\\n",
       "610   True  1908.05786v1.pdf  unknow   \n",
       "\n",
       "                                               Context  \n",
       "610  TASED-Net: Temporally-Aggregating Spatial Enco...  "
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm[tdm.title ==\"1908.05786v1.pdf\" ].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300a4b4f-e23c-431b-80ce-ca5ba3271aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "3112754d-bf19-43f9-867c-0e0c7e23f27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tdm</th>\n",
       "      <th>count_tdm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>Skeleton Based Action Recognition#NTU RGB+D#Ac...</td>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>704</th>\n",
       "      <td>Skeleton Based Action Recognition#NTU RGB+D#Ac...</td>\n",
       "      <td>912</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tdm  count_tdm\n",
       "703  Skeleton Based Action Recognition#NTU RGB+D#Ac...        896\n",
       "704  Skeleton Based Action Recognition#NTU RGB+D#Ac...        912"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tdm[tdm.count_tdm>800].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb46fbe1-566e-4419-b1d3-ce532434110e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7ed13d-f103-4577-8074-7de09653baf0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d3f910ed-ecc7-4fd4-9237-6690316fd286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# re.sub(r\"[^a-zA-Z0-9?,'’‘´`%]+\", '', \"Hell\\no*%& \\n\").strip()\n",
    "f = re.sub(r\"[\\n]+\", '', \"Hell\\no*%& \\n\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ff03bdb9-544a-4c53-9e58-6fed3940f808",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello*%&'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c092a-eeb4-458e-a4c8-c204a01b9508",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
