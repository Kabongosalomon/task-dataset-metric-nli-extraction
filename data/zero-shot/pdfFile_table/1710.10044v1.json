[{"caption":"Table 1: Mean and median of best scores across 57 Atari \n2600 games, measured as percentages of human baseline \n","rows":["DQN","QR - DQN - 0","DUEL .","PRIOR .","QR - DQN - 1","DDQN","PR . DUEL .","C51"],"columns":["33","Mean","79%","37","38","39","Median","40"],"mergedAllColumns":[],"numberCells":[{"number":"592%","isBolded":false,"associatedRows":["PR . DUEL ."],"associatedColumns":["Mean","79%","33","37","39"],"associatedMergedColumns":[]},{"number":"151%","isBolded":false,"associatedRows":["DUEL ."],"associatedColumns":["Median","79%","33"],"associatedMergedColumns":[]},{"number":"701%","isBolded":false,"associatedRows":["C51"],"associatedColumns":["Mean","79%","33","37","39","39"],"associatedMergedColumns":[]},{"number":"881%","isBolded":false,"associatedRows":["QR - DQN - 0"],"associatedColumns":["Mean","79%","33","37","39","39","40"],"associatedMergedColumns":[]},{"number":"373%","isBolded":false,"associatedRows":["DUEL ."],"associatedColumns":["Mean","79%","33"],"associatedMergedColumns":[]},{"number":"124%","isBolded":false,"associatedRows":["PRIOR ."],"associatedColumns":["Median","79%","33","37"],"associatedMergedColumns":[]},{"number":"118%","isBolded":false,"associatedRows":["DDQN"],"associatedColumns":["Median","79%"],"associatedMergedColumns":[]},{"number":"211%","isBolded":true,"associatedRows":["QR - DQN - 1"],"associatedColumns":["Median","79%","33","37","39","39","40","38"],"associatedMergedColumns":[]},{"number":"178%","isBolded":false,"associatedRows":["C51"],"associatedColumns":["Median","79%","33","37","39","39"],"associatedMergedColumns":[]},{"number":"199%","isBolded":false,"associatedRows":["QR - DQN - 0"],"associatedColumns":["Median","79%","33","37","39","39","40"],"associatedMergedColumns":[]},{"number":"228%","isBolded":false,"associatedRows":["DQN"],"associatedColumns":["Mean"],"associatedMergedColumns":[]},{"number":"307%","isBolded":false,"associatedRows":["DDQN"],"associatedColumns":["Mean","79%"],"associatedMergedColumns":[]},{"number":"172%","isBolded":false,"associatedRows":["PR . DUEL ."],"associatedColumns":["Median","79%","33","37","39"],"associatedMergedColumns":[]},{"number":"915%","isBolded":true,"associatedRows":["QR - DQN - 1"],"associatedColumns":["Mean","79%","33","37","39","39","40","38"],"associatedMergedColumns":[]},{"number":"434%","isBolded":false,"associatedRows":["PRIOR ."],"associatedColumns":["Mean","79%","33","37"],"associatedMergedColumns":[]}]},{"caption":"Table 2: Notation used in the paper \n\nSymbol \nDescription of usage \n\nReinforcement Learning \nM \nMDP (X , A, R, P , ?) \nX \nState space of MDP \nA \nAction space of MDP \nR, R t \nReward function, random variable reward \nP \nTransition probabilities, P (x |x, a) \n? \nDiscount factor, ? ? [0, 1) \nx, x t ? X \nStates \na, a  *  , b ? A \nActions \nr, r t ? R \nRewards \n? \nPolicy \nT ? \n(dist.) Bellman operator \nT \n(dist.) Bellman optimality operator \nV ? , V \nValue function, state-value function \nQ ? , Q \nAction-value function \n? \nStep-size parameter, learning rate \nExploration rate, -greedy \n\nADAM \n\nAdam parameter \n? \nHuber-loss parameter \nL ? \nHuber-loss with parameter ? \n\nDistributional Reinforcement Learning \nZ ? , Z \nRandom return, value distribution \nZ ? \n\nM C \n\nMonte-Carlo value distribution under policy ? \nZ \nSpace of value distribution? \nZ ? \nFixed point of convergence for ? W1 T ? \nz ? Z \nInstantiated return sample \np \nMetric order \nW p \np-Wasserstein metric \nL p \nMetric order p \nd p \nmaximal form of Wasserstein \n? \nProjection used by C51 \n? W1 \n1-Wasserstein projection \n? ? \nQuantile regression loss \n? ? \n\n? \n\nHuber quantile loss \nq 1 , . . . , q N \nProbabilities, parameterized probabilities \n? 0 , ? 1 , . . . , ? N Cumulative probabilities with ? 0 :\u003d 0 \n? 1 , . . . ,? N \nMidpoint quantile targets \n? \nSample from unit interval \n? z \nDirac function at z ? R \n? \nParameterized function \nB \nBernoulli distribution \nB ? \nParameterized Bernoulli distribution \nZ Q \nSpace of quantile (value) distributions \nZ ? \nParameterized quantile (value) distribution \nY \nRandom variable over R \nY 1 , . . . , Y m \nRandom variable sample? \nY m \nEmpirical distribution from m-Diracs \n\n","rows":["Cumulative probabilities with ?","?"],"columns":["Policy","r , r","Z Q","Action - value function","States","Action space of MDP","ADAM","Transition probabilities , P ( x |x , a )","Step - size parameter , learning rate","z ? Z","Exploration rate ,","? , Q","? W1","Dirac function at z ? R","Random return , value distribution","Adam parameter","Actions","?","maximal form of Wasserstein","R , R t","?","Huber - loss with parameter ?","Z ? , Z","?","?","?","Table 2 : Notation used in the paper","Projection used by C51","?","?","A","B","?","Metric order","Symbol","Reward function , random variable reward","Metric order p","?","State space of MDP","Quantile regression loss","L","Monte - Carlo value distribution under policy ?","N","Q","Huber - loss parameter","T","( dist . ) Bellman optimality operator","Description of usage","Rewards","X","Y","Z","Value function , state - value function","V ? , V","Huber quantile loss","a , a * , b ? A","t ? X","d","W p","- greedy","p - Wasserstein metric","Discount factor , ? ? [ 0 , 1 )","Probabilities , parameterized probabilities","p","Space of value distribution?","q","Instantiated return sample","T ?","x , x","Z ?","1 - Wasserstein projection","Fixed point of convergence for ? W1 T ?","t ? R"],"mergedAllColumns":["M C"],"numberCells":[{"number":"0:\u003d","isBolded":false,"associatedRows":["?"],"associatedColumns":["Table 2 : Notation used in the paper","Description of usage","State space of MDP","Action space of MDP","Reward function , random variable reward","Transition probabilities , P ( x |x , a )","Discount factor , ? ? [ 0 , 1 )","States","Actions","Rewards","Policy","( dist . ) Bellman optimality operator","Value function , state - value function","Action - value function","Step - size parameter , learning rate","- greedy","Adam parameter","Huber - loss parameter","Huber - loss with parameter ?","Random return , value distribution","Monte - Carlo value distribution under policy ?","Space of value distribution?","Fixed point of convergence for ? W1 T ?","Instantiated return sample","Metric order","p - Wasserstein metric","Metric order p","maximal form of Wasserstein","Projection used by C51","1 - Wasserstein projection","Quantile regression loss","Huber quantile loss","Probabilities , parameterized probabilities"],"associatedMergedColumns":["M C"]},{"number":"0","isBolded":true,"associatedRows":["?","Cumulative probabilities with ?"],"associatedColumns":["Table 2 : Notation used in the paper","Description of usage","State space of MDP","Action space of MDP","Reward function , random variable reward","Transition probabilities , P ( x |x , a )","Discount factor , ? ? [ 0 , 1 )","States","Actions","Rewards","Policy","( dist . ) Bellman optimality operator","Value function , state - value function","Action - value function","Step - size parameter , learning rate","- greedy","Adam parameter","Huber - loss parameter","Huber - loss with parameter ?","Random return , value distribution","Monte - Carlo value distribution under policy ?","Space of value distribution?","Fixed point of convergence for ? W1 T ?","Instantiated return sample","Metric order","p - Wasserstein metric","Metric order p","maximal form of Wasserstein","Projection used by C51","1 - Wasserstein projection","Quantile regression loss","Huber quantile loss","Probabilities , parameterized probabilities"],"associatedMergedColumns":["M C"]},{"number":"0,?","isBolded":false,"associatedRows":[],"associatedColumns":["Table 2 : Notation used in the paper","Symbol","X","State space of MDP","A","R , R t","Transition probabilities , P ( x |x , a )","Discount factor , ? ? [ 0 , 1 )","t ? X","x , x","a , a * , b ? A","t ? R","r , r","?","T ?","T","V ? , V","? , Q","Q","?","Exploration rate ,","ADAM","?","L","?","Z ? , Z","Z ?","Z","Z ?","z ? Z","p","W p","p","L","d","p","?","? W1","?","?","?","?","?","q"],"associatedMergedColumns":["M C"]},{"number":"1,...,?N","isBolded":false,"associatedRows":["?"],"associatedColumns":["Table 2 : Notation used in the paper","Symbol","State space of MDP","Action space of MDP","R , R t","Reward function , random variable reward","Transition probabilities , P ( x |x , a )","Discount factor , ? ? [ 0 , 1 )","x , x","a , a * , b ? A","Actions","r , r","?","Policy","( dist . ) Bellman optimality operator","V ? , V","Value function , state - value function","? , Q","Q","?","Exploration rate ,","ADAM","?","L","?","Z ? , Z","Z ?","Z","Fixed point of convergence for ? W1 T ?","p","Metric order","W p","Metric order p","d","p","?","? W1","?","?","?","Huber quantile loss","?","q"],"associatedMergedColumns":["M C"]},{"number":"1,...,Ym","isBolded":false,"associatedRows":[],"associatedColumns":["Table 2 : Notation used in the paper","Symbol","State space of MDP","Action space of MDP","R , R t","Reward function , random variable reward","Transition probabilities , P ( x |x , a )","Discount factor , ? ? [ 0 , 1 )","t ? X","x , x","a , a * , b ? A","t ? R","r , r","?","( dist . ) Bellman optimality operator","V ? , V","Value function , state - value function","? , Q","Q","?","Exploration rate ,","ADAM","?","L","?","Z ? , Z","Z ?","Z","Fixed point of convergence for ? W1 T ?","p","Metric order","W p","p","L","d","p","?","? W1","?","?","?","Huber quantile loss","?","q","N","?","Dirac function at z ? R","?","B","B","?","Z Q","Z","?","Y"],"associatedMergedColumns":["M C"]},{"number":"1,...,?","isBolded":false,"associatedRows":[],"associatedColumns":["Table 2 : Notation used in the paper","Symbol","State space of MDP","Action space of MDP","R , R t","Reward function , random variable reward","Transition probabilities , P ( x |x , a )","Discount factor , ? ? [ 0 , 1 )","x , x","a , a * , b ? A","Actions","t ? R","r , r","?","( dist . ) Bellman optimality operator","V ? , V","Value function , state - value function","? , Q","Q","?","Exploration rate ,","ADAM","?","L","?","Z ? , Z","Z ?","Z","Fixed point of convergence for ? W1 T ?","z ? Z","p","W p","p","L","d","p","?","? W1","?","?","?","?","?","q"],"associatedMergedColumns":["M C"]},{"number":"1,...,qN","isBolded":false,"associatedRows":[],"associatedColumns":["Table 2 : Notation used in the paper","Symbol","State space of MDP","Action space of MDP","R , R t","Reward function , random variable reward","Transition probabilities , P ( x |x , a )","Discount factor , ? ? [ 0 , 1 )","t ? X","x , x","a , a * , b ? A","t ? R","r , r","?","( dist . ) Bellman optimality operator","V ? , V","Value function , state - value function","? , Q","Q","?","Exploration rate ,","ADAM","?","L","?","Z ? , Z","Z ?","Z","Fixed point of convergence for ? W1 T ?","z ? Z","p","W p","p","L","d","p","?","? W1","?","?","?","?","Huber quantile loss"],"associatedMergedColumns":["M C"]}]}]