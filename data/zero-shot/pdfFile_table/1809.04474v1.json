[{"caption":"Table 1: Summary of results: aggregate scores for IMPALA and PopArt-IMPALA. We report median human normalised score \nfor Atari-57, and mean capped human normalised score for DmLab-30. In Atari, Random and Human refer to whether the \ntrained agent is evaluated with random or human starts. In DmLab-30 the test score includes evaluation on the held-out levels. \n\n","rows":["PopArt - IMPALA","IMPALA"],"columns":["Human","DmLab - 30","Random","Atari - 57 ( unclipped )","Atari - 57","Test","Train"],"mergedAllColumns":[],"numberCells":[{"number":"107.0%","isBolded":false,"associatedRows":["PopArt - IMPALA"],"associatedColumns":["Atari - 57 ( unclipped )","Random"],"associatedMergedColumns":[]},{"number":"59.7%","isBolded":false,"associatedRows":["IMPALA"],"associatedColumns":["Atari - 57","Random"],"associatedMergedColumns":[]},{"number":"58.4%","isBolded":false,"associatedRows":["IMPALA"],"associatedColumns":["DmLab - 30","Test"],"associatedMergedColumns":[]},{"number":"73.5%","isBolded":false,"associatedRows":["PopArt - IMPALA"],"associatedColumns":["DmLab - 30","Train"],"associatedMergedColumns":[]},{"number":"28.5%","isBolded":false,"associatedRows":["IMPALA"],"associatedColumns":["Atari - 57","Human"],"associatedMergedColumns":[]},{"number":"72.8%","isBolded":false,"associatedRows":["PopArt - IMPALA"],"associatedColumns":["DmLab - 30","Test"],"associatedMergedColumns":[]},{"number":"60.6%","isBolded":false,"associatedRows":["IMPALA"],"associatedColumns":["DmLab - 30","Train"],"associatedMergedColumns":[]},{"number":"1.0%","isBolded":false,"associatedRows":["IMPALA"],"associatedColumns":["Atari - 57 ( unclipped )","Human"],"associatedMergedColumns":[]},{"number":"110.7%","isBolded":false,"associatedRows":["PopArt - IMPALA"],"associatedColumns":["Atari - 57","Random"],"associatedMergedColumns":[]},{"number":"0.3%","isBolded":false,"associatedRows":["IMPALA"],"associatedColumns":["Atari - 57 ( unclipped )","Random"],"associatedMergedColumns":[]},{"number":"93.7%","isBolded":false,"associatedRows":["PopArt - IMPALA"],"associatedColumns":["Atari - 57 ( unclipped )","Human"],"associatedMergedColumns":[]},{"number":"101.5%","isBolded":false,"associatedRows":["PopArt - IMPALA"],"associatedColumns":["Atari - 57","Human"],"associatedMergedColumns":[]}]},{"caption":"Table 4: DeepMind Lab preprocessing. As in previous work \non DeepMind Lab, we render the observation with a resolu-\ntion of [72, 96], as well as use 4 action repeats. We also em-\nploy the optimistic asymmetric rescaling (OAR) of rewards, \nthat was introduced in Espeholt et al. for exploration. \n\n","rows":["Image Height","Image Width"],"columns":["value"],"mergedAllColumns":[],"numberCells":[{"number":"72","isBolded":false,"associatedRows":["Image Height"],"associatedColumns":["value"],"associatedMergedColumns":[]},{"number":"96","isBolded":false,"associatedRows":["Image Width"],"associatedColumns":["value"],"associatedMergedColumns":[]}]},{"caption":"Table 5: Atari preprocessing. The standard Atari-\npreprocessing is used in the Atari experiments. Since \nthe introduction of DQN these setting have become a \nstandard practice when training deep RL agent on Atari. \nNote however, that we report experiments training agents \nboth with and without reward clipping. \n\n","rows":["Frame Stacking","Number of action repeats","Max - pooling"],"columns":["Hyperparameter","True","Image Height","Grey scaling","84","value","[ - 1 ,","Image Width"],"mergedAllColumns":["True"],"numberCells":[{"number":"4","isBolded":false,"associatedRows":["Frame Stacking"],"associatedColumns":["value","84","84","True"],"associatedMergedColumns":["True"]},{"number":"2consecutiveframes","isBolded":true,"associatedRows":["Max - pooling"],"associatedColumns":["Hyperparameter","Image Height","Image Width","Grey scaling"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["Number of action repeats"],"associatedColumns":["value","84","84","True","True","[ - 1 ,"],"associatedMergedColumns":["True"]}]},{"caption":"Table 6: Other agent hyperparameters: These hyperparame-\nters are the same used by Espeholt et al. \n\n","rows":["Baseline loss weight ?","Unroll length","Image Height","Image Width"],"columns":["4","Reward Clipping ( if used )","Frame Stacking","Hyperparameter","True","Number of action repeats","Grey scaling","Max - pooling 2 consecutive frames","( Atari ) ,","value","End of episode on life loss","[ - 1 ,"],"mergedAllColumns":[],"numberCells":[{"number":"32","isBolded":false,"associatedRows":["Baseline loss weight ?"],"associatedColumns":["value","True","Max - pooling 2 consecutive frames","4","True","[ - 1 ,","4","value","( Atari ) ,"],"associatedMergedColumns":[]},{"number":"84","isBolded":false,"associatedRows":["Baseline loss weight ?","Image Height"],"associatedColumns":["value"],"associatedMergedColumns":[]},{"number":"84","isBolded":false,"associatedRows":["Baseline loss weight ?","Image Width"],"associatedColumns":["value"],"associatedMergedColumns":[]},{"number":"0.5","isBolded":false,"associatedRows":["Baseline loss weight ?"],"associatedColumns":["value","True","Max - pooling 2 consecutive frames","4","True","[ - 1 ,","4","value","( Atari ) ,"],"associatedMergedColumns":[]},{"number":"20","isBolded":false,"associatedRows":["Unroll length"],"associatedColumns":["Hyperparameter","Grey scaling","Max - pooling 2 consecutive frames","Frame Stacking","End of episode on life loss","Reward Clipping ( if used )","Number of action repeats","Hyperparameter"],"associatedMergedColumns":[]},{"number":"0.99","isBolded":false,"associatedRows":["Baseline loss weight ?"],"associatedColumns":["value","True","Max - pooling 2 consecutive frames","4","True","[ - 1 ,","4","value","( Atari ) ,"],"associatedMergedColumns":[]}]},{"caption":"Table 7: Network hyperparameters. The network architec-\nture is described in details in Espeholt et al., For complete-\nness, we also report in the Table below the complete spec-\nification of the network. Convolutional layers are specified \naccording to the pattern (num layers, kernel size, stride). \n\n","rows":["- Max - Pool","- Number of sections","- Conv","3x3","/"],"columns":["[ 16 ,","ReLU","1 )","32 ]","Identity","value","/"],"mergedAllColumns":["Convolutional Stack","ResNet section"],"numberCells":[{"number":"1","isBolded":false,"associatedRows":["- Conv","/","3x3","/"],"associatedColumns":["value","32 ]","ReLU","1 )","Identity"],"associatedMergedColumns":["ResNet section"]},{"number":"2","isBolded":false,"associatedRows":["- Conv"],"associatedColumns":["value","[ 16 ,","ReLU","/"],"associatedMergedColumns":["ResNet section"]},{"number":"3","isBolded":false,"associatedRows":["- Number of sections","/"],"associatedColumns":["value"],"associatedMergedColumns":["Convolutional Stack"]},{"number":"2","isBolded":false,"associatedRows":["- Max - Pool","/","3x3","/"],"associatedColumns":["value","32 ]","ReLU","1 )"],"associatedMergedColumns":["ResNet section"]},{"number":"1","isBolded":false,"associatedRows":["- Max - Pool"],"associatedColumns":["value","[ 16 ,","ReLU","/"],"associatedMergedColumns":["ResNet section"]},{"number":"1","isBolded":false,"associatedRows":["- Conv","/","3x3","/"],"associatedColumns":["value","32 ]","ReLU","1 )"],"associatedMergedColumns":["ResNet section"]},{"number":"2","isBolded":false,"associatedRows":["- Conv"],"associatedColumns":["value","[ 16 ,","ReLU","/","Identity"],"associatedMergedColumns":["ResNet section"]},{"number":"1","isBolded":false,"associatedRows":["- Conv"],"associatedColumns":["value","[ 16 ,","ReLU"],"associatedMergedColumns":["ResNet section"]}]},{"caption":"Table 9: hyperparameters tuned with population based train-\ning are listed below: note that these are the same used by all \nbaseline agents we compare to, to ensure fair comparisons. \n\n","rows":["Population Size ( Atari )"],"columns":["value"],"mergedAllColumns":[],"numberCells":[{"number":"24","isBolded":false,"associatedRows":["Population Size ( Atari )"],"associatedColumns":["value"],"associatedMergedColumns":[]}]}]