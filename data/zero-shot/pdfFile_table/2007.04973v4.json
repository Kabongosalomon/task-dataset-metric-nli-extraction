[{"caption":"Table 2: Zero-shot code clone detection with cosine similarity probe. Contrastive and hybrid representations \nimprove clone detection AUROC on unmodified (natural) HackerRank programs by +8% and +10% AUROC \nover a heuristic textual similarity probe, respectively, suggesting they are predictive of functionality. Contrastive \nrepresentations are also the most robust to adversarial code transformations. \n\n","rows":["75 . 73?0 . 75","72 . 31?0 . 79","31 . 63?0 . 82","22 . 72?0 . 20","12 . 11?0 . 54","+ ContraCode + RoBERTa MLM","3 . 09?0 . 28","64 . 97?0 . 24","69 . 55?0 . 81","79 . 39?0 . 70","4 . 51?0 . 33","10 . 09?0 . 50","25 . 83?0 . 21","74 . 04?0 . 77","Edit distance heuristic","+ ContraCode pre - train","37 . 81?0 . 24","Randomly initialized Transformer","+ RoBERTa MLM pre - train","58 . 32?0 . 88"],"columns":["Natural code","Adversarial ( N \u003d4 )","Adversarial ( N \u003d16 )","AP"],"mergedAllColumns":[],"numberCells":[{"number":"39.46","isBolded":false,"associatedRows":["+ RoBERTa MLM pre - train","74 . 04?0 . 77","25 . 83?0 . 21"],"associatedColumns":["Adversarial ( N \u003d4 )","AP"],"associatedMergedColumns":[]},{"number":"30.95","isBolded":false,"associatedRows":["Randomly initialized Transformer","72 . 31?0 . 79","22 . 72?0 . 20","3 . 09?0 . 28"],"associatedColumns":["Adversarial ( N \u003d16 )","AP"],"associatedMergedColumns":[]},{"number":"66.23","isBolded":true,"associatedRows":["+ ContraCode pre - train","75 . 73?0 . 75","64 . 97?0 . 24"],"associatedColumns":["Adversarial ( N \u003d4 )","AP"],"associatedMergedColumns":[]},{"number":"31.17","isBolded":false,"associatedRows":["+ RoBERTa MLM pre - train","74 . 04?0 . 77","25 . 83?0 . 21","4 . 51?0 . 33"],"associatedColumns":["Adversarial ( N \u003d16 )","AP"],"associatedMergedColumns":[]},{"number":"51.42","isBolded":false,"associatedRows":["Randomly initialized Transformer","+ ContraCode + RoBERTa MLM","79 . 39?0 . 70","37 . 81?0 . 24"],"associatedColumns":["Adversarial ( N \u003d4 )","AP"],"associatedMergedColumns":[]},{"number":"42.85","isBolded":false,"associatedRows":["Edit distance heuristic","69 . 55?0 . 81","31 . 63?0 . 82"],"associatedColumns":["Adversarial ( N \u003d4 )","AP"],"associatedMergedColumns":[]},{"number":"37.73","isBolded":false,"associatedRows":["Randomly initialized Transformer","72 . 31?0 . 79","22 . 72?0 . 20"],"associatedColumns":["Adversarial ( N \u003d4 )","AP"],"associatedMergedColumns":[]},{"number":"81.47","isBolded":true,"associatedRows":["Randomly initialized Transformer","+ ContraCode + RoBERTa MLM","79 . 39?0 . 70"],"associatedColumns":["Natural code","AP"],"associatedMergedColumns":[]},{"number":"32.52","isBolded":false,"associatedRows":["Randomly initialized Transformer","+ ContraCode + RoBERTa MLM","79 . 39?0 . 70","37 . 81?0 . 24","10 . 09?0 . 50"],"associatedColumns":["Adversarial ( N \u003d16 )","AP"],"associatedMergedColumns":[]},{"number":"73.75","isBolded":false,"associatedRows":["Edit distance heuristic","69 . 55?0 . 81"],"associatedColumns":["Natural code","AP"],"associatedMergedColumns":[]},{"number":"78.02","isBolded":false,"associatedRows":["+ ContraCode pre - train","75 . 73?0 . 75"],"associatedColumns":["Natural code","AP"],"associatedMergedColumns":[]},{"number":"75.82","isBolded":false,"associatedRows":["Randomly initialized Transformer","72 . 31?0 . 79"],"associatedColumns":["Natural code","AP"],"associatedMergedColumns":[]},{"number":"77.65","isBolded":false,"associatedRows":["+ RoBERTa MLM pre - train","74 . 04?0 . 77"],"associatedColumns":["Natural code","AP"],"associatedMergedColumns":[]},{"number":"32.46","isBolded":false,"associatedRows":["Edit distance heuristic","69 . 55?0 . 81","31 . 63?0 . 82","12 . 11?0 . 54"],"associatedColumns":["Adversarial ( N \u003d16 )","AP"],"associatedMergedColumns":[]},{"number":"59.66","isBolded":true,"associatedRows":["+ ContraCode pre - train","75 . 73?0 . 75","64 . 97?0 . 24","58 . 32?0 . 88"],"associatedColumns":["Adversarial ( N \u003d16 )","AP"],"associatedMergedColumns":[]}]},{"caption":"Table 3: Type inference accuracy on TypeScript \nprograms. As ContraCode does not modify model \narchitecture, contrastive pre-training improves both \nBiLSTM and Transformer accuracy (1.5% to 2.28%). \nCompared with TypeScript\u0027s built-in type inference, \nwe improve accuracy by 8.9%. \n\n","rows":["TypeScript CheckJS","GPT - 3 Codex ( few - shot , 175B )","Transformer","DeepTyper BiLSTM","GPT - 3 Codex ( zero - shot , 175B )","+ ContraCode + MLM ( hybrid )","+ ContraCode pre - train","+ RoBERTa MLM pre - train","DeepTyper , variable name only"],"columns":["Acc@1 Acc@5"],"mergedAllColumns":["-"],"numberCells":[{"number":"40.85%","isBolded":false,"associatedRows":["GPT - 3 Codex ( few - shot , 175B )","+ RoBERTa MLM pre - train"],"associatedColumns":["Acc@1 Acc@5"],"associatedMergedColumns":["-"]},{"number":"30.55%","isBolded":false,"associatedRows":["GPT - 3 Codex ( few - shot , 175B )","+ RoBERTa MLM pre - train"],"associatedColumns":["Acc@1 Acc@5"],"associatedMergedColumns":["-"]},{"number":"47.16%","isBolded":true,"associatedRows":["+ ContraCode + MLM ( hybrid )"],"associatedColumns":["Acc@1 Acc@5"],"associatedMergedColumns":["-"]},{"number":"82.85%","isBolded":false,"associatedRows":["+ RoBERTa MLM pre - train"],"associatedColumns":["Acc@1 Acc@5"],"associatedMergedColumns":["-"]},{"number":"50.24%","isBolded":false,"associatedRows":["+ RoBERTa MLM pre - train"],"associatedColumns":["Acc@1 Acc@5"],"associatedMergedColumns":["-"]},{"number":"54.01%","isBolded":true,"associatedRows":["+ ContraCode pre - train"],"associatedColumns":["Acc@1 Acc@5"],"associatedMergedColumns":["-"]},{"number":"28.94%","isBolded":false,"associatedRows":["DeepTyper , variable name only","+ RoBERTa MLM pre - train"],"associatedColumns":["Acc@1 Acc@5"],"associatedMergedColumns":["-"]},{"number":"45.66%","isBolded":false,"associatedRows":["Transformer","+ RoBERTa MLM pre - train"],"associatedColumns":["Acc@1 Acc@5"],"associatedMergedColumns":["-"]},{"number":"45.11%","isBolded":false,"associatedRows":["TypeScript CheckJS","+ RoBERTa MLM pre - train"],"associatedColumns":["Acc@1 Acc@5"],"associatedMergedColumns":[]},{"number":"46.86%","isBolded":false,"associatedRows":["+ ContraCode pre - train"],"associatedColumns":["Acc@1 Acc@5"],"associatedMergedColumns":["-"]},{"number":"81.44%","isBolded":false,"associatedRows":["GPT - 3 Codex ( few - shot , 175B )","+ ContraCode + MLM ( hybrid )"],"associatedColumns":["Acc@1 Acc@5"],"associatedMergedColumns":["-"]},{"number":"51.73%","isBolded":false,"associatedRows":["DeepTyper BiLSTM"],"associatedColumns":["Acc@1 Acc@5"],"associatedMergedColumns":["-"]},{"number":"80.08%","isBolded":false,"associatedRows":["Transformer","+ ContraCode + MLM ( hybrid )"],"associatedColumns":["Acc@1 Acc@5"],"associatedMergedColumns":["-"]},{"number":"75.76%","isBolded":false,"associatedRows":["GPT - 3 Codex ( few - shot , 175B )","+ RoBERTa MLM pre - train"],"associatedColumns":["Acc@1 Acc@5"],"associatedMergedColumns":["-"]},{"number":"70.07%","isBolded":false,"associatedRows":["DeepTyper , variable name only","+ RoBERTa MLM pre - train"],"associatedColumns":["Acc@1 Acc@5"],"associatedMergedColumns":["-"]},{"number":"82.71%","isBolded":false,"associatedRows":["DeepTyper BiLSTM"],"associatedColumns":["Acc@1 Acc@5"],"associatedMergedColumns":["-"]},{"number":"81.85%","isBolded":true,"associatedRows":["+ ContraCode pre - train"],"associatedColumns":["Acc@1 Acc@5"],"associatedMergedColumns":["-"]},{"number":"26.62%","isBolded":false,"associatedRows":["GPT - 3 Codex ( zero - shot , 175B )","+ RoBERTa MLM pre - train"],"associatedColumns":["Acc@1 Acc@5"],"associatedMergedColumns":["-"]},{"number":"85.55%","isBolded":true,"associatedRows":["+ ContraCode pre - train"],"associatedColumns":["Acc@1 Acc@5"],"associatedMergedColumns":["-"]}]},{"caption":"Table 4: Results for different settings of code sum-\nmarization: supervised training with 81K functions, \nmasked language model pre-training, training from \nscratch and contrastive pre-training with fine-tuning. \n\n","rows":["Transformer","RoBERTa MLM","+ ContraCode","code2vec","code2seq"],"columns":["Table 8 ( not our best performing model ) .","Precision","Recall","F1"],"mergedAllColumns":[],"numberCells":[{"number":"9.34%","isBolded":false,"associatedRows":["code2vec"],"associatedColumns":["Table 8 ( not our best performing model ) .","F1"],"associatedMergedColumns":[]},{"number":"10.78%","isBolded":false,"associatedRows":["code2vec"],"associatedColumns":["Table 8 ( not our best performing model ) .","Precision"],"associatedMergedColumns":[]},{"number":"7.65%","isBolded":false,"associatedRows":["code2seq"],"associatedColumns":["Table 8 ( not our best performing model ) .","Recall"],"associatedMergedColumns":[]},{"number":"8.24%","isBolded":false,"associatedRows":["code2vec"],"associatedColumns":["Table 8 ( not our best performing model ) .","Recall"],"associatedMergedColumns":[]},{"number":"18.11%","isBolded":false,"associatedRows":["Transformer"],"associatedColumns":["Table 8 ( not our best performing model ) .","Precision"],"associatedMergedColumns":[]},{"number":"12.17%","isBolded":false,"associatedRows":["code2seq"],"associatedColumns":["Table 8 ( not our best performing model ) .","Precision"],"associatedMergedColumns":[]},{"number":"12.45%","isBolded":false,"associatedRows":["RoBERTa MLM"],"associatedColumns":["Table 8 ( not our best performing model ) .","F1"],"associatedMergedColumns":[]},{"number":"15.13%","isBolded":false,"associatedRows":["RoBERTa MLM"],"associatedColumns":["Table 8 ( not our best performing model ) .","Precision"],"associatedMergedColumns":[]},{"number":"15.78%","isBolded":false,"associatedRows":["Transformer"],"associatedColumns":["Table 8 ( not our best performing model ) .","Recall"],"associatedMergedColumns":[]},{"number":"11.47%","isBolded":false,"associatedRows":["RoBERTa MLM"],"associatedColumns":["Table 8 ( not our best performing model ) .","Recall"],"associatedMergedColumns":[]},{"number":"20.34%","isBolded":false,"associatedRows":["RoBERTa MLM","+ ContraCode"],"associatedColumns":["Table 8 ( not our best performing model ) .","Precision"],"associatedMergedColumns":[]},{"number":"14.96%","isBolded":false,"associatedRows":["RoBERTa MLM","+ ContraCode"],"associatedColumns":["Table 8 ( not our best performing model ) .","Recall"],"associatedMergedColumns":[]},{"number":"16.86%","isBolded":false,"associatedRows":["Transformer"],"associatedColumns":["Table 8 ( not our best performing model ) .","F1"],"associatedMergedColumns":[]},{"number":"17.24%","isBolded":true,"associatedRows":["RoBERTa MLM","+ ContraCode"],"associatedColumns":["Table 8 ( not our best performing model ) .","F1"],"associatedMergedColumns":[]},{"number":"9.39%","isBolded":false,"associatedRows":["code2seq"],"associatedColumns":["Table 8 ( not our best performing model ) .","F1"],"associatedMergedColumns":[]}]},{"caption":"Table 8: Contrasting global, sequence-level representations outperforms contrasting local representations. We \ncompare using the terminal (global) hidden states of the DeepTyper BiLSTM and the mean pooled token-level \n(local) hidden states. \n\n","rows":["InfoNCE with mean token rep . , 10K steps","InfoNCE with terminal hidden state , 20K steps","Local","InfoNCE with terminal hidden state , 10K steps"],"columns":["Acc@1","Acc@5"],"mergedAllColumns":["Global"],"numberCells":[{"number":"80.03%","isBolded":false,"associatedRows":["Local","InfoNCE with mean token rep . , 10K steps"],"associatedColumns":["Acc@5"],"associatedMergedColumns":["Global"]},{"number":"83.03%","isBolded":false,"associatedRows":["InfoNCE with terminal hidden state , 10K steps"],"associatedColumns":["Acc@5"],"associatedMergedColumns":[]},{"number":"84.60%","isBolded":true,"associatedRows":["InfoNCE with terminal hidden state , 20K steps"],"associatedColumns":["Acc@5"],"associatedMergedColumns":[]},{"number":"49.32%","isBolded":false,"associatedRows":["Local","InfoNCE with mean token rep . , 10K steps"],"associatedColumns":["Acc@1"],"associatedMergedColumns":["Global"]},{"number":"51.70%","isBolded":false,"associatedRows":["InfoNCE with terminal hidden state , 10K steps"],"associatedColumns":["Acc@1"],"associatedMergedColumns":[]},{"number":"52.65%","isBolded":true,"associatedRows":["InfoNCE with terminal hidden state , 20K steps"],"associatedColumns":["Acc@1"],"associatedMergedColumns":[]}]},{"caption":"Table 9: Training time and decoder depth ablation on the method name prediction task. Longer pre-training \nsignificantly improves downstream performance when a shallow, 1 layer decoder is used. \n\n","rows":["Original set","Transformer , 4 layers","Transformer , 1 layer","MoCo , 45k steps","MoCo , 10k steps"],"columns":["Precision","( 81k programs )","Recall","F1","Supervision"],"mergedAllColumns":[],"numberCells":[{"number":"11.91%","isBolded":false,"associatedRows":["Transformer , 1 layer","MoCo , 10k steps","Original set"],"associatedColumns":["Supervision","Precision","( 81k programs )"],"associatedMergedColumns":[]},{"number":"17.71%","isBolded":true,"associatedRows":["Transformer , 1 layer","MoCo , 45k steps","Original set"],"associatedColumns":["Supervision","Precision","( 81k programs )"],"associatedMergedColumns":[]},{"number":"18.21%","isBolded":true,"associatedRows":["Transformer , 4 layers","MoCo , 45k steps","Original set"],"associatedColumns":["Supervision","Precision","( 81k programs )"],"associatedMergedColumns":[]},{"number":"14.56%","isBolded":true,"associatedRows":["Transformer , 4 layers","MoCo , 45k steps","Original set"],"associatedColumns":["Supervision","F1","( 81k programs )"],"associatedMergedColumns":[]},{"number":"12.57%","isBolded":true,"associatedRows":["Transformer , 1 layer","MoCo , 45k steps","Original set"],"associatedColumns":["Supervision","Recall","( 81k programs )"],"associatedMergedColumns":[]},{"number":"13.21%","isBolded":true,"associatedRows":["Transformer , 4 layers","MoCo , 45k steps","Original set"],"associatedColumns":["Supervision","Recall","( 81k programs )"],"associatedMergedColumns":[]},{"number":"7.49%","isBolded":false,"associatedRows":["Transformer , 1 layer","MoCo , 10k steps","Original set"],"associatedColumns":["Supervision","F1","( 81k programs )"],"associatedMergedColumns":[]},{"number":"5.96%","isBolded":false,"associatedRows":["Transformer , 1 layer","MoCo , 10k steps","Original set"],"associatedColumns":["Supervision","Recall","( 81k programs )"],"associatedMergedColumns":[]},{"number":"13.79%","isBolded":true,"associatedRows":["Transformer , 1 layer","MoCo , 45k steps","Original set"],"associatedColumns":["Supervision","F1","( 81k programs )"],"associatedMergedColumns":[]}]}]