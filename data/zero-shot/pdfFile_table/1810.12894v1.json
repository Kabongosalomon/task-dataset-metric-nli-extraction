[{"caption":"Table 1: Comparison to baselines results. Final mean performance for various methods. State of \nthe art results taken from: [1] ","rows":["3 , 700","3 , 426","3 , 387","PPO","SOTA","400","2 , 209","15 , 806","Dynamics","105","3 , 371","2 , 497","12 , 380"],"columns":["- 3","33","8 , 152","Solaris","Venture","3 , 246","8 , 666","3 , 906","Gravitar","the final training performance for each algorithm is listed , alongside the state of the art","PrivateEye","1 , 712","Montezuma \u0027 s Revenge","1 , 859","Pitfall !","3 , 282"],"mergedAllColumns":["from previous work and average human performance ."],"numberCells":[{"number":"0","isBolded":true,"associatedRows":["SOTA","2 , 209","3 , 700"],"associatedColumns":["the final training performance for each algorithm is listed , alongside the state of the art","Pitfall !","- 3","33"],"associatedMergedColumns":["from previous work and average human performance ."]},{"number":"2","isBolded":false,"associatedRows":["SOTA","2 , 209","3 , 700","105"],"associatedColumns":["the final training performance for each algorithm is listed , alongside the state of the art","PrivateEye","8 , 666","33"],"associatedMergedColumns":["from previous work and average human performance ."]},{"number":"0","isBolded":false,"associatedRows":["PPO","3 , 426","2 , 497"],"associatedColumns":["the final training performance for each algorithm is listed , alongside the state of the art","Pitfall !","- 3"],"associatedMergedColumns":["from previous work and average human performance ."]},{"number":"0","isBolded":false,"associatedRows":["PPO","3 , 426","2 , 497","105","3 , 387"],"associatedColumns":["the final training performance for each algorithm is listed , alongside the state of the art","Venture","1 , 859"],"associatedMergedColumns":["from previous work and average human performance ."]},{"number":"1","isBolded":false,"associatedRows":["SOTA","2 , 209","3 , 700","15 , 806","3 , 387"],"associatedColumns":["the final training performance for each algorithm is listed , alongside the state of the art","Solaris","3 , 282","3 , 246"],"associatedMergedColumns":["from previous work and average human performance ."]},{"number":"3","isBolded":false,"associatedRows":["SOTA","2 , 209","3 , 700","15 , 806","12 , 380"],"associatedColumns":["the final training performance for each algorithm is listed , alongside the state of the art","Venture","1 , 859","1 , 712"],"associatedMergedColumns":["from previous work and average human performance ."]},{"number":"1","isBolded":false,"associatedRows":["SOTA","2 , 209"],"associatedColumns":["the final training performance for each algorithm is listed , alongside the state of the art","Gravitar","3 , 906","33"],"associatedMergedColumns":["from previous work and average human performance ."]},{"number":"2","isBolded":false,"associatedRows":["SOTA","2 , 209","3 , 700"],"associatedColumns":["the final training performance for each algorithm is listed , alongside the state of the art","Montezuma \u0027 s Revenge","8 , 152","33"],"associatedMergedColumns":["from previous work and average human performance ."]},{"number":"0","isBolded":false,"associatedRows":["Dynamics","3 , 371","400"],"associatedColumns":["the final training performance for each algorithm is listed , alongside the state of the art","Pitfall !","- 3"],"associatedMergedColumns":["from previous work and average human performance ."]}]},{"caption":"Table 3: Preprocessing details for policy and \nvalue network for all experiments. \n\n","rows":["Entropy coefficient","E","Learning rate","Number of optimization epochs","[ 0 . 9 ,","Proportion of experience used for training predictor","? I","Clip range","Number of minibatches","?","Coefficient of extrinsic reward","In Table","?","Coefficient of intrinsic reward"],"columns":["PPO AND RND HYPERPARAMETERS","128","Adam ( Kingma \u0026 Ba ( 2015 ) )","Value","30K"],"mergedAllColumns":["the code accompanying this paper ."],"numberCells":[{"number":"4","isBolded":false,"associatedRows":["Number of optimization epochs"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"A.4","isBolded":false,"associatedRows":[],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.001","isBolded":false,"associatedRows":["Entropy coefficient"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128","Adam ( Kingma \u0026 Ba ( 2015 ) )"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"5thehyperparametersforthePPORLalgorithmalongwithanyadditionalhyperparameters","isBolded":false,"associatedRows":["In Table"],"associatedColumns":["PPO AND RND HYPERPARAMETERS"],"associatedMergedColumns":[]},{"number":"0.25","isBolded":false,"associatedRows":["Proportion of experience used for training predictor"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128","Adam ( Kingma \u0026 Ba ( 2015 ) )"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"0.95","isBolded":false,"associatedRows":["?"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128","Adam ( Kingma \u0026 Ba ( 2015 ) )"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"2","isBolded":false,"associatedRows":["Coefficient of extrinsic reward"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"1","isBolded":false,"associatedRows":["Coefficient of intrinsic reward"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"1.1]","isBolded":true,"associatedRows":["Clip range","[ 0 . 9 ,"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128","Adam ( Kingma \u0026 Ba ( 2015 ) )"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"4","isBolded":false,"associatedRows":["Number of minibatches"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"0.0001","isBolded":true,"associatedRows":["Learning rate"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"0.99","isBolded":false,"associatedRows":["? I"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128","Adam ( Kingma \u0026 Ba ( 2015 ) )"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"0.999","isBolded":false,"associatedRows":["?","E"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128","Adam ( Kingma \u0026 Ba ( 2015 ) )"],"associatedMergedColumns":["the code accompanying this paper ."]}]},{"caption":"Table 4: Preprocessing details for target and pre-\ndictor networks for all experiments. \n\n","rows":["Entropy coefficient","E","Learning rate","Number of optimization epochs","[ 0 . 9 ,","Proportion of experience used for training predictor","? I","Clip range","Number of minibatches","?","Coefficient of extrinsic reward","In Table","?","Coefficient of intrinsic reward"],"columns":["PPO AND RND HYPERPARAMETERS","128","Adam ( Kingma \u0026 Ba ( 2015 ) )","Value","30K"],"mergedAllColumns":["the code accompanying this paper ."],"numberCells":[{"number":"0.99","isBolded":false,"associatedRows":["? I"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128","Adam ( Kingma \u0026 Ba ( 2015 ) )"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"0.999","isBolded":false,"associatedRows":["?","E"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128","Adam ( Kingma \u0026 Ba ( 2015 ) )"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"A.4","isBolded":false,"associatedRows":[],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0001","isBolded":true,"associatedRows":["Learning rate"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"0.95","isBolded":false,"associatedRows":["?"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128","Adam ( Kingma \u0026 Ba ( 2015 ) )"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"1.1]","isBolded":true,"associatedRows":["Clip range","[ 0 . 9 ,"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128","Adam ( Kingma \u0026 Ba ( 2015 ) )"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"1","isBolded":false,"associatedRows":["Coefficient of intrinsic reward"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"0.001","isBolded":false,"associatedRows":["Entropy coefficient"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128","Adam ( Kingma \u0026 Ba ( 2015 ) )"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"4","isBolded":false,"associatedRows":["Number of minibatches"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"2","isBolded":false,"associatedRows":["Coefficient of extrinsic reward"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"0.25","isBolded":false,"associatedRows":["Proportion of experience used for training predictor"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128","Adam ( Kingma \u0026 Ba ( 2015 ) )"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"5thehyperparametersforthePPORLalgorithmalongwithanyadditionalhyperparameters","isBolded":false,"associatedRows":["In Table"],"associatedColumns":["PPO AND RND HYPERPARAMETERS"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["Number of optimization epochs"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K"],"associatedMergedColumns":["the code accompanying this paper ."]}]},{"caption":"Table 5: Default hyperparameters for PPO and RND algorithms for experiments where applicable. \nAny differences to these defaults are detailed in the main text. \n\n","rows":["Entropy coefficient","E","Learning rate","Number of optimization epochs","[ 0 . 9 ,","Proportion of experience used for training predictor","? I","Clip range","Number of minibatches","?","Coefficient of extrinsic reward","In Table","?","Coefficient of intrinsic reward"],"columns":["PPO AND RND HYPERPARAMETERS","128","Adam ( Kingma \u0026 Ba ( 2015 ) )","Value","30K"],"mergedAllColumns":["the code accompanying this paper ."],"numberCells":[{"number":"4","isBolded":false,"associatedRows":["Number of minibatches"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"2","isBolded":false,"associatedRows":["Coefficient of extrinsic reward"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"0.0001","isBolded":true,"associatedRows":["Learning rate"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"0.001","isBolded":false,"associatedRows":["Entropy coefficient"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128","Adam ( Kingma \u0026 Ba ( 2015 ) )"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"0.999","isBolded":false,"associatedRows":["?","E"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128","Adam ( Kingma \u0026 Ba ( 2015 ) )"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"0.99","isBolded":false,"associatedRows":["? I"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128","Adam ( Kingma \u0026 Ba ( 2015 ) )"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"5thehyperparametersforthePPORLalgorithmalongwithanyadditionalhyperparameters","isBolded":false,"associatedRows":["In Table"],"associatedColumns":["PPO AND RND HYPERPARAMETERS"],"associatedMergedColumns":[]},{"number":"A.4","isBolded":false,"associatedRows":[],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["Number of optimization epochs"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"0.95","isBolded":false,"associatedRows":["?"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128","Adam ( Kingma \u0026 Ba ( 2015 ) )"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"1.1]","isBolded":true,"associatedRows":["Clip range","[ 0 . 9 ,"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128","Adam ( Kingma \u0026 Ba ( 2015 ) )"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"0.25","isBolded":false,"associatedRows":["Proportion of experience used for training predictor"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K","128","Adam ( Kingma \u0026 Ba ( 2015 ) )"],"associatedMergedColumns":["the code accompanying this paper ."]},{"number":"1","isBolded":false,"associatedRows":["Coefficient of intrinsic reward"],"associatedColumns":["PPO AND RND HYPERPARAMETERS","Value","128","30K"],"associatedMergedColumns":["the code accompanying this paper ."]}]},{"caption":"Table 6: The numbers of seeds run for each experiment is shown in the table. The results of each \nseed are then averaged to provide a mean curve in each figure, and the standard error is used make \nthe shaded region surrounding each curve. \n","rows":[],"columns":["Figure number","Number of seeds"],"mergedAllColumns":["NA","10"],"numberCells":[{"number":"5","isBolded":false,"associatedRows":[],"associatedColumns":["Number of seeds"],"associatedMergedColumns":["10"]},{"number":"9","isBolded":false,"associatedRows":[],"associatedColumns":["Figure number"],"associatedMergedColumns":["10"]},{"number":"3","isBolded":false,"associatedRows":[],"associatedColumns":["Figure number"],"associatedMergedColumns":["10"]},{"number":"3","isBolded":false,"associatedRows":[],"associatedColumns":["Number of seeds"],"associatedMergedColumns":["10"]},{"number":"7","isBolded":false,"associatedRows":[],"associatedColumns":["Figure number"],"associatedMergedColumns":["10"]},{"number":"8","isBolded":false,"associatedRows":[],"associatedColumns":["Figure number"],"associatedMergedColumns":["10"]},{"number":"5","isBolded":false,"associatedRows":[],"associatedColumns":["Figure number"],"associatedMergedColumns":["10"]},{"number":"2","isBolded":false,"associatedRows":[],"associatedColumns":["Figure number"],"associatedMergedColumns":["NA"]},{"number":"6","isBolded":false,"associatedRows":[],"associatedColumns":["Figure number"],"associatedMergedColumns":["10"]},{"number":"5","isBolded":false,"associatedRows":[],"associatedColumns":["Number of seeds"],"associatedMergedColumns":["10"]},{"number":"5","isBolded":false,"associatedRows":[],"associatedColumns":["Number of seeds"],"associatedMergedColumns":["10"]},{"number":"5","isBolded":false,"associatedRows":[],"associatedColumns":["Number of seeds"],"associatedMergedColumns":["10"]},{"number":"4","isBolded":false,"associatedRows":[],"associatedColumns":["Figure number"],"associatedMergedColumns":["10"]},{"number":"5","isBolded":false,"associatedRows":[],"associatedColumns":["Number of seeds"],"associatedMergedColumns":["10"]},{"number":"1","isBolded":false,"associatedRows":[],"associatedColumns":["Figure number"],"associatedMergedColumns":[]}]}]