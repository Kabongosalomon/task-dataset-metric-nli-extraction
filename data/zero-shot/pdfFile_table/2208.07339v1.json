[{"caption":"Table 1: C4 validation perplexities of quantization methods for different transformer sizes ranging \nfrom 125M to 13B parameters. We see that absmax, row-wise, zeropoint, and vector-wise quantization \nleads to significant performance degradation as we scale, particularly at the 13B mark where 8-bit \n13B perplexity is worse than 8-bit 6.7B perplexity. If we use LLM.int8(), we recover full perplexity \nas we scale. Zeropoint quantization shows an advantage due to asymmetric quantization but is no \nlonger advantageous when used with mixed-precision decomposition. \nParameters \n125M 1.3B \n2.7B \n6.7B \n13B \n32-bit Float \n25.65 15.91 14.43 13.30 12.45 \n\nInt8 absmax \n87.76 16.55 15.11 14.59 19.08 \nInt8 zeropoint \n56.66 16.24 14.76 13.49 13.94 \n\nInt8 absmax row-wise \n30.93 17.08 15.24 14.13 16.49 \nInt8 absmax vector-wise \n35.84 16.82 14.98 14.13 16.48 \nInt8 zeropoint vector-wise \n25.72 15.94 14.36 13.38 13.47 \n\nInt8 absmax row-wise + decomposition \n30.76 16.19 14.65 13.25 12.46 \nAbsmax LLM.int8() (vector-wise + decomp) \n25.83 15.93 14.44 13.24 12.45 \nZeropoint LLM.int8() (vector-wise + decomp) 25.69 15.92 14.43 13.24 12.45   ","rows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )","13B perplexity is worse than 8 - bit","Zeropoint LLM . int8 ( ) ( vector - wise + decomp )","125M"],"columns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"mergedAllColumns":["13B","longer advantageous when used with mixed - precision decomposition .","leads to significant performance degradation as we scale , particularly at the 13B mark where 8 - bit"],"numberCells":[{"number":"12.45","isBolded":true,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"16.82","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"14.65","isBolded":false,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"12.45","isBolded":true,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"13.47","isBolded":false,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"15.11","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"14.13","isBolded":false,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"13.24","isBolded":true,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"16.49","isBolded":false,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"13.25","isBolded":false,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"25.72","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"13.30","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"30.76","isBolded":false,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"14.98","isBolded":false,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"12.46","isBolded":false,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"13.94","isBolded":false,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"15.24","isBolded":false,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"14.76","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"15.93","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"14.59","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"87.76","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"25.65","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"14.43","isBolded":true,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"13.49","isBolded":false,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"16.24","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"35.84","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"25.69","isBolded":true,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"25.83","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"16.55","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"1.3B","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )","125M"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["longer advantageous when used with mixed - precision decomposition ."]},{"number":"16.48","isBolded":false,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"15.91","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"14.13","isBolded":false,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"6.7B","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )","125M"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["longer advantageous when used with mixed - precision decomposition ."]},{"number":"19.08","isBolded":false,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"14.44","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"17.08","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"13.24","isBolded":true,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"30.93","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"12.45","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"15.94","isBolded":false,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"56.66","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"15.92","isBolded":true,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"6.7Bperplexity.IfweuseLLM.int8(),werecoverfullperplexity","isBolded":false,"associatedRows":["13B perplexity is worse than 8 - bit"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["leads to significant performance degradation as we scale , particularly at the 13B mark where 8 - bit"]},{"number":"14.43","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"14.36","isBolded":false,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"2.7B","isBolded":false,"associatedRows":["Absmax LLM . int8 ( ) ( vector - wise + decomp )","125M"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["longer advantageous when used with mixed - precision decomposition ."]},{"number":"16.19","isBolded":false,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]},{"number":"13.38","isBolded":false,"associatedRows":["Zeropoint LLM . int8 ( ) ( vector - wise + decomp )"],"associatedColumns":["Table 1 : C4 validation perplexities of quantization methods for different transformer sizes ranging"],"associatedMergedColumns":["13B"]}]},{"caption":"Table 2: Different hardware setups and which methods can be run in 16-bit vs. 8-bit precision. We \ncan see that our 8-bit method makes many models accessible that were not accessible before, in \nparticular, OPT-175B/BLOOM. \n\n","rows":["8x A100","Colab","GPT - 2","Enterprise","Paid Cloud","Colab Pro","Free Cloud","4x RTX 3090","T0 / T5 - 11B","Academic desktop"],"columns":["16 - bit","GPU Memory","GPT - J - 6B","24s GB","OPT - 30B","Largest Model that can be run","OPT - 13B","OPT - 175B / BLOOM","OPT - 66B"],"mergedAllColumns":[],"numberCells":[{"number":"40GB","isBolded":false,"associatedRows":["Enterprise","8x A100"],"associatedColumns":["Largest Model that can be run","GPU Memory","OPT - 175B / BLOOM"],"associatedMergedColumns":[]},{"number":"80GB","isBolded":false,"associatedRows":["Enterprise","8x A100"],"associatedColumns":["Largest Model that can be run","GPU Memory"],"associatedMergedColumns":[]},{"number":"15GB","isBolded":false,"associatedRows":["Paid Cloud","Colab Pro"],"associatedColumns":["Largest Model that can be run","GPU Memory","OPT - 175B / BLOOM","OPT - 175B / BLOOM","24s GB","OPT - 66B"],"associatedMergedColumns":[]},{"number":"24GB","isBolded":false,"associatedRows":["Academic desktop","4x RTX 3090"],"associatedColumns":["Largest Model that can be run","GPU Memory","OPT - 175B / BLOOM","OPT - 175B / BLOOM","24s GB"],"associatedMergedColumns":[]},{"number":"1.3B","isBolded":false,"associatedRows":["Free Cloud","Colab","T0 / T5 - 11B","GPT - 2"],"associatedColumns":["Largest Model that can be run","16 - bit","OPT - 175B / BLOOM","OPT - 66B","OPT - 66B","OPT - 30B","GPT - J - 6B"],"associatedMergedColumns":[]},{"number":"12GB","isBolded":false,"associatedRows":["Free Cloud","Colab"],"associatedColumns":["Largest Model that can be run","GPU Memory","OPT - 175B / BLOOM","OPT - 175B / BLOOM","24s GB","OPT - 66B","OPT - 13B"],"associatedMergedColumns":[]}]},{"caption":"Table 4: Inference speedups compared to 16-bit matrix multiplication for the first hidden layer in \nthe feed-forward of differently sized GPT-3 transformers. The hidden dimension is 4x the model \ndimension. The 8-bit without overhead speedups assumes that no quantization or dequantization is \nperformed. Numbers small than 1.0x represent slowdowns. Int8 matrix multiplication speeds up \ninference only for models with large model and hidden dimensions. \n\n","rows":["Small","XL","Vector - wise","LLM . int8 ( ) ( vector - wise+decomp )","Medium","GPT - 3 Size","Large","Absmax PyTorch+NVIDIA","Vector - wise PyTorch+NVIDIA","FP16 - bit baseline","Int8 without overhead"],"columns":["13B","1536","5140","4096","175B","2048","1024","12288","768","2560"],"mergedAllColumns":[],"numberCells":[{"number":"0.65x","isBolded":false,"associatedRows":["Vector - wise PyTorch+NVIDIA"],"associatedColumns":["13B","4096"],"associatedMergedColumns":[]},{"number":"1.08x","isBolded":false,"associatedRows":["Int8 without overhead"],"associatedColumns":["13B","1024"],"associatedMergedColumns":[]},{"number":"0.36x","isBolded":false,"associatedRows":["LLM . int8 ( ) ( vector - wise+decomp )"],"associatedColumns":["13B","1536"],"associatedMergedColumns":[]},{"number":"0.96x","isBolded":false,"associatedRows":["Absmax PyTorch+NVIDIA"],"associatedColumns":["13B","5140"],"associatedMergedColumns":[]},{"number":"0.99x","isBolded":false,"associatedRows":["Int8 without overhead"],"associatedColumns":["13B","768"],"associatedMergedColumns":[]},{"number":"0.70x","isBolded":false,"associatedRows":["Absmax PyTorch+NVIDIA"],"associatedColumns":["13B","4096"],"associatedMergedColumns":[]},{"number":"0.43x","isBolded":true,"associatedRows":["Vector - wise"],"associatedColumns":["13B","768"],"associatedMergedColumns":[]},{"number":"0.51x","isBolded":false,"associatedRows":["LLM . int8 ( ) ( vector - wise+decomp )"],"associatedColumns":["13B","2048"],"associatedMergedColumns":[]},{"number":"1.81x","isBolded":false,"associatedRows":["LLM . int8 ( ) ( vector - wise+decomp )"],"associatedColumns":["175B","12288"],"associatedMergedColumns":[]},{"number":"1.63x","isBolded":false,"associatedRows":["Int8 without overhead"],"associatedColumns":["13B","2560"],"associatedMergedColumns":[]},{"number":"0.22x","isBolded":false,"associatedRows":["Vector - wise PyTorch+NVIDIA"],"associatedColumns":["13B","1024"],"associatedMergedColumns":[]},{"number":"0.21x","isBolded":false,"associatedRows":["Vector - wise PyTorch+NVIDIA"],"associatedColumns":["13B","768"],"associatedMergedColumns":[]},{"number":"0.53x","isBolded":false,"associatedRows":["Absmax PyTorch+NVIDIA"],"associatedColumns":["13B","2560"],"associatedMergedColumns":[]},{"number":"0.25x","isBolded":false,"associatedRows":["Absmax PyTorch+NVIDIA"],"associatedColumns":["13B","768"],"associatedMergedColumns":[]},{"number":"0.45x","isBolded":false,"associatedRows":["Absmax PyTorch+NVIDIA"],"associatedColumns":["13B","2048"],"associatedMergedColumns":[]},{"number":"1.00x","isBolded":false,"associatedRows":["FP16 - bit baseline"],"associatedColumns":["13B","768"],"associatedMergedColumns":[]},{"number":"0.24x","isBolded":false,"associatedRows":["Absmax PyTorch+NVIDIA"],"associatedColumns":["13B","1024"],"associatedMergedColumns":[]},{"number":"1.18x","isBolded":true,"associatedRows":["Vector - wise"],"associatedColumns":["13B","4096"],"associatedMergedColumns":[]},{"number":"1.50x","isBolded":false,"associatedRows":["Vector - wise PyTorch+NVIDIA"],"associatedColumns":["175B","12288"],"associatedMergedColumns":[]},{"number":"1.43x","isBolded":false,"associatedRows":["Int8 without overhead"],"associatedColumns":["13B","1536"],"associatedMergedColumns":[]},{"number":"1.50x","isBolded":false,"associatedRows":["Absmax PyTorch+NVIDIA"],"associatedColumns":["175B","12288"],"associatedMergedColumns":[]},{"number":"6.7B","isBolded":false,"associatedRows":["GPT - 3 Size","Small","Medium","Large","XL"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"1.00x","isBolded":false,"associatedRows":["FP16 - bit baseline"],"associatedColumns":["13B","2560"],"associatedMergedColumns":[]},{"number":"0.64x","isBolded":false,"associatedRows":["LLM . int8 ( ) ( vector - wise+decomp )"],"associatedColumns":["13B","2560"],"associatedMergedColumns":[]},{"number":"1.67x","isBolded":false,"associatedRows":["Int8 without overhead"],"associatedColumns":["13B","4096"],"associatedMergedColumns":[]},{"number":"0.14x","isBolded":false,"associatedRows":["LLM . int8 ( ) ( vector - wise+decomp )"],"associatedColumns":["13B","768"],"associatedMergedColumns":[]},{"number":"2.29x","isBolded":false,"associatedRows":["Int8 without overhead"],"associatedColumns":["175B","12288"],"associatedMergedColumns":[]},{"number":"1.00x","isBolded":false,"associatedRows":["FP16 - bit baseline"],"associatedColumns":["13B","1024"],"associatedMergedColumns":[]},{"number":"1.00x","isBolded":false,"associatedRows":["FP16 - bit baseline"],"associatedColumns":["13B","5140"],"associatedMergedColumns":[]},{"number":"1.00x","isBolded":false,"associatedRows":["FP16 - bit baseline"],"associatedColumns":["175B","12288"],"associatedMergedColumns":[]},{"number":"1.00x","isBolded":false,"associatedRows":["FP16 - bit baseline"],"associatedColumns":["13B","2048"],"associatedMergedColumns":[]},{"number":"1.00x","isBolded":false,"associatedRows":["FP16 - bit baseline"],"associatedColumns":["13B","4096"],"associatedMergedColumns":[]},{"number":"0.50x","isBolded":false,"associatedRows":["Vector - wise PyTorch+NVIDIA"],"associatedColumns":["13B","2560"],"associatedMergedColumns":[]},{"number":"2.13x","isBolded":false,"associatedRows":["Int8 without overhead"],"associatedColumns":["13B","5140"],"associatedMergedColumns":[]},{"number":"0.49x","isBolded":true,"associatedRows":["Vector - wise"],"associatedColumns":["13B","1024"],"associatedMergedColumns":[]},{"number":"0.33x","isBolded":false,"associatedRows":["Vector - wise PyTorch+NVIDIA"],"associatedColumns":["13B","1536"],"associatedMergedColumns":[]},{"number":"1.61x","isBolded":false,"associatedRows":["Int8 without overhead"],"associatedColumns":["13B","2048"],"associatedMergedColumns":[]},{"number":"1.59x","isBolded":true,"associatedRows":["Vector - wise"],"associatedColumns":["13B","5140"],"associatedMergedColumns":[]},{"number":"0.36x","isBolded":false,"associatedRows":["Absmax PyTorch+NVIDIA"],"associatedColumns":["13B","1536"],"associatedMergedColumns":[]},{"number":"0.94x","isBolded":true,"associatedRows":["Vector - wise"],"associatedColumns":["13B","2560"],"associatedMergedColumns":[]},{"number":"0.91x","isBolded":false,"associatedRows":["Vector - wise PyTorch+NVIDIA"],"associatedColumns":["13B","5140"],"associatedMergedColumns":[]},{"number":"2.00x","isBolded":true,"associatedRows":["Vector - wise"],"associatedColumns":["175B","12288"],"associatedMergedColumns":[]},{"number":"2.7B","isBolded":false,"associatedRows":["GPT - 3 Size","Small","Medium","Large","XL"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.41x","isBolded":false,"associatedRows":["Vector - wise PyTorch+NVIDIA"],"associatedColumns":["13B","2048"],"associatedMergedColumns":[]},{"number":"1.22x","isBolded":false,"associatedRows":["LLM . int8 ( ) ( vector - wise+decomp )"],"associatedColumns":["13B","5140"],"associatedMergedColumns":[]},{"number":"0.91x","isBolded":true,"associatedRows":["Vector - wise"],"associatedColumns":["13B","2048"],"associatedMergedColumns":[]},{"number":"0.86x","isBolded":false,"associatedRows":["LLM . int8 ( ) ( vector - wise+decomp )"],"associatedColumns":["13B","4096"],"associatedMergedColumns":[]},{"number":"0.74x","isBolded":true,"associatedRows":["Vector - wise"],"associatedColumns":["13B","1536"],"associatedMergedColumns":[]},{"number":"1.00x","isBolded":false,"associatedRows":["FP16 - bit baseline"],"associatedColumns":["13B","1536"],"associatedMergedColumns":[]},{"number":"0.20x","isBolded":false,"associatedRows":["LLM . int8 ( ) ( vector - wise+decomp )"],"associatedColumns":["13B","1024"],"associatedMergedColumns":[]}]},{"caption":"Table 5: Ablation study on the number of GPUs used to run several types of inferences of BLOOM-\n176B model. We compare the number of GPUs used by our quantized BLOOM-176B model together \nwith the native BLOOM-176B model. We also report the per-token generation speed in milliseconds \nfor different batch sizes. We use our method integrated into transformers(Wolf et al., 2019) powered \nby accelerate library from HuggingFace to deal with multi-GPU inference. Our method reaches a \nsimilar performance to the native model by fitting into fewer GPUs than the native model. \nBatch Size \nHardware \n1 \n8 \n32 \nbfloat16 baseline 8xA100 80GB 239 32 9.94 \n\nLLM.int8() \n8xA100 80GB 253 34 10.44 \nLLM.int8() \n4xA100 80GB 246 33 9.40 \nLLM.int8() \n3xA100 80GB 247 33 9.11 \n\n","rows":["bfloat16 baseline","253","1","4xA100 80GB","246","247","Batch Size","8","239","3xA100 80GB","Hardware","8xA100 80GB","LLM . int8 ( )"],"columns":["Table 5 : Ablation study on the number of GPUs used to run several types of inferences of BLOOM -"],"mergedAllColumns":["similar performance to the native model by fitting into fewer GPUs than the native model ."],"numberCells":[{"number":"32","isBolded":false,"associatedRows":["Batch Size","Hardware","1","8"],"associatedColumns":["Table 5 : Ablation study on the number of GPUs used to run several types of inferences of BLOOM -"],"associatedMergedColumns":["similar performance to the native model by fitting into fewer GPUs than the native model ."]},{"number":"9.40","isBolded":false,"associatedRows":["LLM . int8 ( )","4xA100 80GB","246"],"associatedColumns":["Table 5 : Ablation study on the number of GPUs used to run several types of inferences of BLOOM -"],"associatedMergedColumns":["similar performance to the native model by fitting into fewer GPUs than the native model ."]},{"number":"34","isBolded":false,"associatedRows":["LLM . int8 ( )","8xA100 80GB","253"],"associatedColumns":["Table 5 : Ablation study on the number of GPUs used to run several types of inferences of BLOOM -"],"associatedMergedColumns":["similar performance to the native model by fitting into fewer GPUs than the native model ."]},{"number":"32","isBolded":true,"associatedRows":["bfloat16 baseline","8xA100 80GB","239"],"associatedColumns":["Table 5 : Ablation study on the number of GPUs used to run several types of inferences of BLOOM -"],"associatedMergedColumns":["similar performance to the native model by fitting into fewer GPUs than the native model ."]},{"number":"9.11","isBolded":true,"associatedRows":["LLM . int8 ( )","3xA100 80GB","247"],"associatedColumns":["Table 5 : Ablation study on the number of GPUs used to run several types of inferences of BLOOM -"],"associatedMergedColumns":["similar performance to the native model by fitting into fewer GPUs than the native model ."]},{"number":"33","isBolded":false,"associatedRows":["LLM . int8 ( )","4xA100 80GB","246"],"associatedColumns":["Table 5 : Ablation study on the number of GPUs used to run several types of inferences of BLOOM -"],"associatedMergedColumns":["similar performance to the native model by fitting into fewer GPUs than the native model ."]},{"number":"9.94","isBolded":false,"associatedRows":["bfloat16 baseline","8xA100 80GB","239"],"associatedColumns":["Table 5 : Ablation study on the number of GPUs used to run several types of inferences of BLOOM -"],"associatedMergedColumns":["similar performance to the native model by fitting into fewer GPUs than the native model ."]},{"number":"10.44","isBolded":false,"associatedRows":["LLM . int8 ( )","8xA100 80GB","253"],"associatedColumns":["Table 5 : Ablation study on the number of GPUs used to run several types of inferences of BLOOM -"],"associatedMergedColumns":["similar performance to the native model by fitting into fewer GPUs than the native model ."]},{"number":"33","isBolded":false,"associatedRows":["LLM . int8 ( )","3xA100 80GB","247"],"associatedColumns":["Table 5 : Ablation study on the number of GPUs used to run several types of inferences of BLOOM -"],"associatedMergedColumns":["similar performance to the native model by fitting into fewer GPUs than the native model ."]}]},{"caption":"Table 6: Initial results on small and large-scale language modeling. Doing attention in 8-bit severely \ndegrades performance and performance cannot fully recovered with mixed-precision decomposition. \nWhile small-scale language models is close to baseline performance for both 8-bit FFN and 8-bit \nlinear projects in the attention layers performance degrades at the large scale. \nIs 8-bit \n\nParams FFN Linear Attention Decomp PPL \n\n209M \n0% \n16.74 \n209M \n0% \n16.77 \n209M \n. \n0% \n16.83 \n\n209M \n2% \n16.78 \n209M \n5% \n16.77 \n209M \n10% \n16.80 \n\n209M \n2% \n24.33 \n209M \n5% \n20.00 \n209M \n10% \n19.00 \n\n1.1B \n0% \n9.99 \n1.1B \n0% \n9.93 \n1.1B \n0% \n10.52 \n1.1B \n1% \n10.41 \n\n","rows":["5%","2%","1%","209M","0%","10%","."],"columns":["Params","PPL","Table 6 : Initial results on small and large - scale language modeling . Doing attention in 8 - bit severely"],"mergedAllColumns":["Is 8 - bit"],"numberCells":[{"number":"24.33","isBolded":false,"associatedRows":["209M","2%"],"associatedColumns":["Table 6 : Initial results on small and large - scale language modeling . Doing attention in 8 - bit severely","PPL"],"associatedMergedColumns":["Is 8 - bit"]},{"number":"9.93","isBolded":false,"associatedRows":["209M","0%"],"associatedColumns":["Table 6 : Initial results on small and large - scale language modeling . Doing attention in 8 - bit severely","PPL"],"associatedMergedColumns":["Is 8 - bit"]},{"number":"16.83","isBolded":false,"associatedRows":["209M",".","0%"],"associatedColumns":["Table 6 : Initial results on small and large - scale language modeling . Doing attention in 8 - bit severely","PPL"],"associatedMergedColumns":["Is 8 - bit"]},{"number":"1.1B","isBolded":false,"associatedRows":[],"associatedColumns":["Table 6 : Initial results on small and large - scale language modeling . Doing attention in 8 - bit severely","Params"],"associatedMergedColumns":["Is 8 - bit"]},{"number":"20.00","isBolded":false,"associatedRows":["209M","5%"],"associatedColumns":["Table 6 : Initial results on small and large - scale language modeling . Doing attention in 8 - bit severely","PPL"],"associatedMergedColumns":["Is 8 - bit"]},{"number":"19.00","isBolded":false,"associatedRows":["209M","10%"],"associatedColumns":["Table 6 : Initial results on small and large - scale language modeling . Doing attention in 8 - bit severely","PPL"],"associatedMergedColumns":["Is 8 - bit"]},{"number":"10.52","isBolded":false,"associatedRows":["209M","0%"],"associatedColumns":["Table 6 : Initial results on small and large - scale language modeling . Doing attention in 8 - bit severely","PPL"],"associatedMergedColumns":["Is 8 - bit"]},{"number":"16.78","isBolded":false,"associatedRows":["209M","2%"],"associatedColumns":["Table 6 : Initial results on small and large - scale language modeling . Doing attention in 8 - bit severely","PPL"],"associatedMergedColumns":["Is 8 - bit"]},{"number":"1.1B","isBolded":false,"associatedRows":[],"associatedColumns":["Table 6 : Initial results on small and large - scale language modeling . Doing attention in 8 - bit severely","Params"],"associatedMergedColumns":["Is 8 - bit"]},{"number":"16.74","isBolded":false,"associatedRows":["209M","0%"],"associatedColumns":["Table 6 : Initial results on small and large - scale language modeling . Doing attention in 8 - bit severely","PPL"],"associatedMergedColumns":["Is 8 - bit"]},{"number":"16.77","isBolded":false,"associatedRows":["209M","0%"],"associatedColumns":["Table 6 : Initial results on small and large - scale language modeling . Doing attention in 8 - bit severely","PPL"],"associatedMergedColumns":["Is 8 - bit"]},{"number":"1.1B","isBolded":false,"associatedRows":[],"associatedColumns":["Table 6 : Initial results on small and large - scale language modeling . Doing attention in 8 - bit severely","Params"],"associatedMergedColumns":["Is 8 - bit"]},{"number":"16.80","isBolded":false,"associatedRows":["209M","10%"],"associatedColumns":["Table 6 : Initial results on small and large - scale language modeling . Doing attention in 8 - bit severely","PPL"],"associatedMergedColumns":["Is 8 - bit"]},{"number":"9.99","isBolded":false,"associatedRows":["209M","0%"],"associatedColumns":["Table 6 : Initial results on small and large - scale language modeling . Doing attention in 8 - bit severely","PPL"],"associatedMergedColumns":["Is 8 - bit"]},{"number":"10.41","isBolded":false,"associatedRows":["209M","1%"],"associatedColumns":["Table 6 : Initial results on small and large - scale language modeling . Doing attention in 8 - bit severely","PPL"],"associatedMergedColumns":["Is 8 - bit"]},{"number":"16.77","isBolded":false,"associatedRows":["209M","5%"],"associatedColumns":["Table 6 : Initial results on small and large - scale language modeling . Doing attention in 8 - bit severely","PPL"],"associatedMergedColumns":["Is 8 - bit"]},{"number":"1.1B","isBolded":false,"associatedRows":[],"associatedColumns":["Table 6 : Initial results on small and large - scale language modeling . Doing attention in 8 - bit severely","Params"],"associatedMergedColumns":["Is 8 - bit"]}]},{"caption":"Table 7: Neural machine translation results for 8-bit FFN and linear attention layers for WMT14+16. \nDecomp indicates the percentage that is computed in 16-bit instead of 8-bit. The BLEU score is the \nmedian of three random seeds. \nIs 8-bit \n\nFFN Linear Decomp BLEU \n\n0% \n28.9 \n0% \n28.8 \n0% \nunstable \n2% \n28.0 \n5% \n27.6 \n10% \n27.5 \n\n","rows":["5%","2%","0%","10%"],"columns":["unstable","Table 7 : Neural machine translation results for 8 - bit FFN and linear attention layers for WMT14+16 .","BLEU"],"mergedAllColumns":["Is 8 - bit"],"numberCells":[{"number":"28.9","isBolded":false,"associatedRows":["0%"],"associatedColumns":["Table 7 : Neural machine translation results for 8 - bit FFN and linear attention layers for WMT14+16 .","BLEU"],"associatedMergedColumns":["Is 8 - bit"]},{"number":"27.5","isBolded":false,"associatedRows":["10%"],"associatedColumns":["Table 7 : Neural machine translation results for 8 - bit FFN and linear attention layers for WMT14+16 .","BLEU","unstable"],"associatedMergedColumns":["Is 8 - bit"]},{"number":"27.6","isBolded":false,"associatedRows":["5%"],"associatedColumns":["Table 7 : Neural machine translation results for 8 - bit FFN and linear attention layers for WMT14+16 .","BLEU","unstable"],"associatedMergedColumns":["Is 8 - bit"]},{"number":"28.0","isBolded":false,"associatedRows":["2%"],"associatedColumns":["Table 7 : Neural machine translation results for 8 - bit FFN and linear attention layers for WMT14+16 .","BLEU","unstable"],"associatedMergedColumns":["Is 8 - bit"]},{"number":"28.8","isBolded":false,"associatedRows":["0%"],"associatedColumns":["Table 7 : Neural machine translation results for 8 - bit FFN and linear attention layers for WMT14+16 .","BLEU"],"associatedMergedColumns":["Is 8 - bit"]}]},{"caption":"Table 8: GLUE finetuning results for quantization methods for the feedforward layer in 8-bit while the \nrest is in 16-bit. No mixed-precision decomposition is used. We can see that vector-wise quantization \nimprove upon the baselines. \n\n","rows":["32 - bit Baseline","Q - BERT ( Shen et al . , 2020 )","Vector - wise","PSQ ( Chen et al . , 2020 )","32 - bit Replication","Q8BERT ( Zafrir et al . , 2019 )"],"columns":["QQP","CoLA","Mean","RTE","SST - 2","MNLI","QNLI","MRPC","STS - B"],"mergedAllColumns":[],"numberCells":[{"number":"88.83","isBolded":false,"associatedRows":["32 - bit Replication"],"associatedColumns":["Mean"],"associatedMergedColumns":[]},{"number":"96.6","isBolded":false,"associatedRows":["32 - bit Replication"],"associatedColumns":["SST - 2"],"associatedMergedColumns":[]},{"number":"91.1","isBolded":false,"associatedRows":["Q - BERT ( Shen et al . , 2020 )"],"associatedColumns":["STS - B"],"associatedMergedColumns":[]},{"number":"68.8","isBolded":false,"associatedRows":["32 - bit Replication"],"associatedColumns":["CoLA"],"associatedMergedColumns":[]},{"number":"88.2","isBolded":false,"associatedRows":["Q - BERT ( Shen et al . , 2020 )"],"associatedColumns":["MRPC"],"associatedMergedColumns":[]},{"number":"91.1","isBolded":false,"associatedRows":["Q8BERT ( Zafrir et al . , 2019 )"],"associatedColumns":["STS - B"],"associatedMergedColumns":[]},{"number":"86.8","isBolded":true,"associatedRows":["PSQ ( Chen et al . , 2020 )"],"associatedColumns":["RTE"],"associatedMergedColumns":[]},{"number":"91.9","isBolded":true,"associatedRows":["Vector - wise"],"associatedColumns":["STS - B"],"associatedMergedColumns":[]},{"number":"88.61","isBolded":false,"associatedRows":["32 - bit Baseline"],"associatedColumns":["Mean"],"associatedMergedColumns":[]},{"number":"87.8","isBolded":false,"associatedRows":["Q - BERT ( Shen et al . , 2020 )"],"associatedColumns":["MNLI"],"associatedMergedColumns":[]},{"number":"90.6","isBolded":false,"associatedRows":["Q - BERT ( Shen et al . , 2020 )"],"associatedColumns":["QQP"],"associatedMergedColumns":[]},{"number":"65.0","isBolded":false,"associatedRows":["Q8BERT ( Zafrir et al . , 2019 )"],"associatedColumns":["CoLA"],"associatedMergedColumns":[]},{"number":"94.7","isBolded":true,"associatedRows":["Vector - wise"],"associatedColumns":["QNLI"],"associatedMergedColumns":[]},{"number":"88.65","isBolded":false,"associatedRows":["PSQ ( Chen et al . , 2020 )"],"associatedColumns":["Mean"],"associatedMergedColumns":[]},{"number":"92.0","isBolded":false,"associatedRows":["PSQ ( Chen et al . , 2020 )"],"associatedColumns":["QQP"],"associatedMergedColumns":[]},{"number":"94.5","isBolded":false,"associatedRows":["PSQ ( Chen et al . , 2020 )"],"associatedColumns":["QNLI"],"associatedMergedColumns":[]},{"number":"91.0","isBolded":true,"associatedRows":["Vector - wise"],"associatedColumns":["MRPC"],"associatedMergedColumns":[]},{"number":"67.5","isBolded":false,"associatedRows":["PSQ ( Chen et al . , 2020 )"],"associatedColumns":["CoLA"],"associatedMergedColumns":[]},{"number":"92.3","isBolded":false,"associatedRows":["32 - bit Replication"],"associatedColumns":["QQP"],"associatedMergedColumns":[]},{"number":"90.1","isBolded":false,"associatedRows":["32 - bit Baseline"],"associatedColumns":["MRPC"],"associatedMergedColumns":[]},{"number":"67.4","isBolded":false,"associatedRows":["32 - bit Baseline"],"associatedColumns":["CoLA"],"associatedMergedColumns":[]},{"number":"94.9","isBolded":false,"associatedRows":["32 - bit Baseline"],"associatedColumns":["QNLI"],"associatedMergedColumns":[]},{"number":"90.3","isBolded":false,"associatedRows":["32 - bit Replication"],"associatedColumns":["MNLI"],"associatedMergedColumns":[]},{"number":"68.6","isBolded":true,"associatedRows":["Vector - wise"],"associatedColumns":["CoLA"],"associatedMergedColumns":[]},{"number":"96.2","isBolded":false,"associatedRows":["PSQ ( Chen et al . , 2020 )"],"associatedColumns":["SST - 2"],"associatedMergedColumns":[]},{"number":"90.4","isBolded":false,"associatedRows":["32 - bit Baseline"],"associatedColumns":["MNLI"],"associatedMergedColumns":[]},{"number":"84.7","isBolded":false,"associatedRows":["Q - BERT ( Shen et al . , 2020 )"],"associatedColumns":["RTE"],"associatedMergedColumns":[]},{"number":"93.0","isBolded":false,"associatedRows":["Q8BERT ( Zafrir et al . , 2019 )"],"associatedColumns":["QNLI"],"associatedMergedColumns":[]},{"number":"93.0","isBolded":false,"associatedRows":["Q - BERT ( Shen et al . , 2020 )"],"associatedColumns":["QNLI"],"associatedMergedColumns":[]},{"number":"86.91","isBolded":false,"associatedRows":["Q - BERT ( Shen et al . , 2020 )"],"associatedColumns":["Mean"],"associatedMergedColumns":[]},{"number":"92.3","isBolded":true,"associatedRows":["Vector - wise"],"associatedColumns":["QQP"],"associatedMergedColumns":[]},{"number":"65.1","isBolded":false,"associatedRows":["Q - BERT ( Shen et al . , 2020 )"],"associatedColumns":["CoLA"],"associatedMergedColumns":[]},{"number":"92.0","isBolded":false,"associatedRows":["32 - bit Replication"],"associatedColumns":["STS - B"],"associatedMergedColumns":[]},{"number":"89.7","isBolded":false,"associatedRows":["Q8BERT ( Zafrir et al . , 2019 )"],"associatedColumns":["MRPC"],"associatedMergedColumns":[]},{"number":"94.7","isBolded":false,"associatedRows":["Q8BERT ( Zafrir et al . , 2019 )"],"associatedColumns":["SST - 2"],"associatedMergedColumns":[]},{"number":"90.1","isBolded":false,"associatedRows":["Q8BERT ( Zafrir et al . , 2019 )"],"associatedColumns":["QQP"],"associatedMergedColumns":[]},{"number":"88.81","isBolded":true,"associatedRows":["Vector - wise"],"associatedColumns":["Mean"],"associatedMergedColumns":[]},{"number":"94.8","isBolded":false,"associatedRows":["32 - bit Replication"],"associatedColumns":["QNLI"],"associatedMergedColumns":[]},{"number":"85.6","isBolded":false,"associatedRows":["Q8BERT ( Zafrir et al . , 2019 )"],"associatedColumns":["MNLI"],"associatedMergedColumns":[]},{"number":"94.8","isBolded":false,"associatedRows":["Q - BERT ( Shen et al . , 2020 )"],"associatedColumns":["SST - 2"],"associatedMergedColumns":[]},{"number":"92.2","isBolded":false,"associatedRows":["32 - bit Baseline"],"associatedColumns":["QQP"],"associatedMergedColumns":[]},{"number":"85.4","isBolded":false,"associatedRows":["Vector - wise"],"associatedColumns":["RTE"],"associatedMergedColumns":[]},{"number":"90.4","isBolded":false,"associatedRows":["PSQ ( Chen et al . , 2020 )"],"associatedColumns":["MRPC"],"associatedMergedColumns":[]},{"number":"85.4","isBolded":false,"associatedRows":["32 - bit Replication"],"associatedColumns":["RTE"],"associatedMergedColumns":[]},{"number":"86.75","isBolded":false,"associatedRows":["Q8BERT ( Zafrir et al . , 2019 )"],"associatedColumns":["Mean"],"associatedMergedColumns":[]},{"number":"91.9","isBolded":false,"associatedRows":["PSQ ( Chen et al . , 2020 )"],"associatedColumns":["STS - B"],"associatedMergedColumns":[]},{"number":"96.4","isBolded":true,"associatedRows":["Vector - wise"],"associatedColumns":["SST - 2"],"associatedMergedColumns":[]},{"number":"90.4","isBolded":false,"associatedRows":["32 - bit Replication"],"associatedColumns":["MRPC"],"associatedMergedColumns":[]},{"number":"90.2","isBolded":true,"associatedRows":["Vector - wise"],"associatedColumns":["MNLI"],"associatedMergedColumns":[]},{"number":"84.5","isBolded":false,"associatedRows":["32 - bit Baseline"],"associatedColumns":["RTE"],"associatedMergedColumns":[]},{"number":"96.4","isBolded":false,"associatedRows":["32 - bit Baseline"],"associatedColumns":["SST - 2"],"associatedMergedColumns":[]},{"number":"89.9","isBolded":false,"associatedRows":["PSQ ( Chen et al . , 2020 )"],"associatedColumns":["MNLI"],"associatedMergedColumns":[]},{"number":"93.0","isBolded":false,"associatedRows":["32 - bit Baseline"],"associatedColumns":["STS - B"],"associatedMergedColumns":[]},{"number":"84.8","isBolded":false,"associatedRows":["Q8BERT ( Zafrir et al . , 2019 )"],"associatedColumns":["RTE"],"associatedMergedColumns":[]}]}]