[{"caption":"Table 1: Comparisons of the linear evaluation for shape \nclassification on ModelNet40. A linear classifier is trained \non the representations learned by different self-supervised \napproaches on the ShapeNet dataset. \n\n","rows":["3D - PointCapsNet [ 75 ]","3D - GAN [ 69 ]","Latent - GAN [ 1 ]","Sauder et al . + PointNet [ 53 ]","Poursaeed et al . + PointNet [ 46 ]","Poursaeed et al . + DGCNN [ 46 ]","STRL + PointNet ( ours )","FoldingNet [ 75 ]","SO - Net [ 38 ]","MAP - VAE [ 75 ]","STRL + DGCNN ( ours )","MRTNet [ 21 ]","Sauder et al . + DGCNN [ 53 ]"],"columns":["ModelNet40"],"mergedAllColumns":[],"numberCells":[{"number":"87.3%","isBolded":false,"associatedRows":["Sauder et al . + PointNet [ 53 ]"],"associatedColumns":["ModelNet40"],"associatedMergedColumns":[]},{"number":"88.4%","isBolded":false,"associatedRows":["FoldingNet [ 75 ]"],"associatedColumns":["ModelNet40"],"associatedMergedColumns":[]},{"number":"88.9%","isBolded":false,"associatedRows":["3D - PointCapsNet [ 75 ]"],"associatedColumns":["ModelNet40"],"associatedMergedColumns":[]},{"number":"90.7%","isBolded":false,"associatedRows":["Poursaeed et al . + DGCNN [ 46 ]"],"associatedColumns":["ModelNet40"],"associatedMergedColumns":[]},{"number":"88.3%","isBolded":false,"associatedRows":["STRL + PointNet ( ours )"],"associatedColumns":["ModelNet40"],"associatedMergedColumns":[]},{"number":"90.9%","isBolded":true,"associatedRows":["STRL + DGCNN ( ours )"],"associatedColumns":["ModelNet40"],"associatedMergedColumns":[]},{"number":"90.6%","isBolded":false,"associatedRows":["Sauder et al . + DGCNN [ 53 ]"],"associatedColumns":["ModelNet40"],"associatedMergedColumns":[]},{"number":"88.4%","isBolded":false,"associatedRows":["MAP - VAE [ 75 ]"],"associatedColumns":["ModelNet40"],"associatedMergedColumns":[]},{"number":"88.6%","isBolded":false,"associatedRows":["Poursaeed et al . + PointNet [ 46 ]"],"associatedColumns":["ModelNet40"],"associatedMergedColumns":[]},{"number":"83.3%","isBolded":false,"associatedRows":["3D - GAN [ 69 ]"],"associatedColumns":["ModelNet40"],"associatedMergedColumns":[]},{"number":"86.4%","isBolded":false,"associatedRows":["MRTNet [ 21 ]"],"associatedColumns":["ModelNet40"],"associatedMergedColumns":[]},{"number":"85.7%","isBolded":false,"associatedRows":["Latent - GAN [ 1 ]"],"associatedColumns":["ModelNet40"],"associatedMergedColumns":[]},{"number":"87.3%","isBolded":false,"associatedRows":["SO - Net [ 38 ]"],"associatedColumns":["ModelNet40"],"associatedMergedColumns":[]}]},{"caption":"Table 2: Shape classification fine-tuned on ModelNet40. \nThe self-supervised pre-trained model serves as the initial \nweight for supervised learning methods. \n\n","rows":["PointNet++ [ 49 ]","ShellNet [ 78 ]","PointNet [ 48 ]","DGCNN [ 67 ]","Supervised","PointCNN [ 39 ]","STRL + DGCNN ( ours )","Sauder et al . + DGCNN [ 53 ]","DGCNN"],"columns":["5%","Accuracy","1%","10%","( a ) Fine - tuned on Full Training Set","20%","Method"],"mergedAllColumns":["( b ) Fine - tuned on Few Training Samples"],"numberCells":[{"number":"89.7%","isBolded":true,"associatedRows":["DGCNN","STRL + DGCNN ( ours )"],"associatedColumns":["( a ) Fine - tuned on Full Training Set","Accuracy","20%"],"associatedMergedColumns":["( b ) Fine - tuned on Few Training Samples"]},{"number":"89.2%","isBolded":false,"associatedRows":["Supervised","PointNet [ 48 ]"],"associatedColumns":["( a ) Fine - tuned on Full Training Set","Accuracy"],"associatedMergedColumns":[]},{"number":"58.4%","isBolded":false,"associatedRows":["DGCNN"],"associatedColumns":["( a ) Fine - tuned on Full Training Set","Method","1%"],"associatedMergedColumns":["( b ) Fine - tuned on Few Training Samples"]},{"number":"88.1%","isBolded":false,"associatedRows":["DGCNN","STRL + DGCNN ( ours )"],"associatedColumns":["( a ) Fine - tuned on Full Training Set","Accuracy","20%"],"associatedMergedColumns":["( b ) Fine - tuned on Few Training Samples"]},{"number":"93.1%","isBolded":true,"associatedRows":["DGCNN","STRL + DGCNN ( ours )"],"associatedColumns":["( a ) Fine - tuned on Full Training Set","Accuracy"],"associatedMergedColumns":[]},{"number":"92.4%","isBolded":false,"associatedRows":["Supervised","Sauder et al . + DGCNN [ 53 ]"],"associatedColumns":["( a ) Fine - tuned on Full Training Set","Accuracy"],"associatedMergedColumns":[]},{"number":"85.2%","isBolded":false,"associatedRows":["DGCNN","ShellNet [ 78 ]"],"associatedColumns":["( a ) Fine - tuned on Full Training Set","Accuracy","10%"],"associatedMergedColumns":["( b ) Fine - tuned on Few Training Samples"]},{"number":"92.2%","isBolded":false,"associatedRows":["Supervised","PointCNN [ 39 ]"],"associatedColumns":["( a ) Fine - tuned on Full Training Set","Accuracy"],"associatedMergedColumns":[]},{"number":"93.1%","isBolded":false,"associatedRows":["Supervised","ShellNet [ 78 ]"],"associatedColumns":["( a ) Fine - tuned on Full Training Set","Accuracy"],"associatedMergedColumns":[]},{"number":"60.5%","isBolded":true,"associatedRows":["DGCNN"],"associatedColumns":["( a ) Fine - tuned on Full Training Set","Method","1%"],"associatedMergedColumns":["( b ) Fine - tuned on Few Training Samples"]},{"number":"86.5%","isBolded":true,"associatedRows":["DGCNN","ShellNet [ 78 ]"],"associatedColumns":["( a ) Fine - tuned on Full Training Set","Accuracy","10%"],"associatedMergedColumns":["( b ) Fine - tuned on Few Training Samples"]},{"number":"80.7%","isBolded":false,"associatedRows":["DGCNN","ShellNet [ 78 ]"],"associatedColumns":["( a ) Fine - tuned on Full Training Set","Method","5%"],"associatedMergedColumns":["( b ) Fine - tuned on Few Training Samples"]},{"number":"82.7%","isBolded":true,"associatedRows":["DGCNN","ShellNet [ 78 ]"],"associatedColumns":["( a ) Fine - tuned on Full Training Set","Method","5%"],"associatedMergedColumns":["( b ) Fine - tuned on Few Training Samples"]},{"number":"90.7%","isBolded":false,"associatedRows":["Supervised","PointNet++ [ 49 ]"],"associatedColumns":["( a ) Fine - tuned on Full Training Set","Accuracy"],"associatedMergedColumns":[]},{"number":"92.2%","isBolded":false,"associatedRows":["Supervised","DGCNN [ 67 ]"],"associatedColumns":["( a ) Fine - tuned on Full Training Set","Accuracy"],"associatedMergedColumns":[]}]},{"caption":"Table 3: 3D object detection fine-tuned on SUN RGB-D \n\nModel \nMethod \nInput \nmAP@0.25 IoU \n\nVoteNet \nfrom scratch \nGeo+Height \n57.7 \nGeo \n57.0 \n\nSR-UNet [9] PointContrast [73] \nGeo \n57.5 \nVoteNet \nSTRL (ours) \nGeo \n58.2 \n\n","rows":["Geo","VoteNet","STRL ( ours )","Geo+Height","PointContrast [ 73 ]","SR - UNet [ 9 ]"],"columns":["Table 3 : 3D object detection fine - tuned on SUN RGB - D","from scratch","mAP@0 . 25 IoU"],"mergedAllColumns":[],"numberCells":[{"number":"57.5","isBolded":false,"associatedRows":["SR - UNet [ 9 ]","PointContrast [ 73 ]","Geo"],"associatedColumns":["Table 3 : 3D object detection fine - tuned on SUN RGB - D","mAP@0 . 25 IoU","from scratch"],"associatedMergedColumns":[]},{"number":"57.7","isBolded":false,"associatedRows":["SR - UNet [ 9 ]","PointContrast [ 73 ]","Geo+Height"],"associatedColumns":["Table 3 : 3D object detection fine - tuned on SUN RGB - D","mAP@0 . 25 IoU"],"associatedMergedColumns":[]},{"number":"58.2","isBolded":true,"associatedRows":["VoteNet","STRL ( ours )","Geo"],"associatedColumns":["Table 3 : 3D object detection fine - tuned on SUN RGB - D","mAP@0 . 25 IoU","from scratch"],"associatedMergedColumns":[]},{"number":"57.0","isBolded":false,"associatedRows":["SR - UNet [ 9 ]","PointContrast [ 73 ]","Geo"],"associatedColumns":["Table 3 : 3D object detection fine - tuned on SUN RGB - D","mAP@0 . 25 IoU"],"associatedMergedColumns":[]}]},{"caption":"Table 4: 3D semantic segmentation fine-tuned on S3DIS. \nWe train the pre-trained or initialized models in a semi-\nsupervised manner on one of the Areas 1-5. Performances \nbelow are evaluated on Area 6 of the S3DIS dataset. \n\n","rows":["Area 2 ( 4440 samples )","from scratch","Area 4 ( 3662 samples )","Area 5 ( 6852 samples )","Area 1 ( 3687 samples )","STRL","Area 3 ( 1650 samples )"],"columns":["mIoU","Acc ."],"mergedAllColumns":[],"numberCells":[{"number":"84.57%","isBolded":false,"associatedRows":["Area 1 ( 3687 samples )","from scratch"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"77.68%","isBolded":false,"associatedRows":["Area 3 ( 1650 samples )","from scratch"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"48.63","isBolded":false,"associatedRows":["Area 5 ( 6852 samples )","from scratch"],"associatedColumns":["mIoU"],"associatedMergedColumns":[]},{"number":"57.85","isBolded":false,"associatedRows":["Area 1 ( 3687 samples )","from scratch"],"associatedColumns":["mIoU"],"associatedMergedColumns":[]},{"number":"79.12%","isBolded":true,"associatedRows":["Area 3 ( 1650 samples )","STRL"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"38.50","isBolded":false,"associatedRows":["Area 4 ( 3662 samples )","from scratch"],"associatedColumns":["mIoU"],"associatedMergedColumns":[]},{"number":"59.15","isBolded":true,"associatedRows":["Area 1 ( 3687 samples )","STRL"],"associatedColumns":["mIoU"],"associatedMergedColumns":[]},{"number":"85.28%","isBolded":true,"associatedRows":["Area 1 ( 3687 samples )","STRL"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"77.28%","isBolded":true,"associatedRows":["Area 5 ( 6852 samples )","STRL"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"51.88","isBolded":true,"associatedRows":["Area 3 ( 1650 samples )","STRL"],"associatedColumns":["mIoU"],"associatedMergedColumns":[]},{"number":"76.85%","isBolded":false,"associatedRows":["Area 5 ( 6852 samples )","from scratch"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"38.86","isBolded":false,"associatedRows":["Area 2 ( 4440 samples )","from scratch"],"associatedColumns":["mIoU"],"associatedMergedColumns":[]},{"number":"72.37%","isBolded":true,"associatedRows":["Area 2 ( 4440 samples )","STRL"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"39.21","isBolded":true,"associatedRows":["Area 2 ( 4440 samples )","STRL"],"associatedColumns":["mIoU"],"associatedMergedColumns":[]},{"number":"70.56%","isBolded":false,"associatedRows":["Area 2 ( 4440 samples )","from scratch"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"49.49","isBolded":false,"associatedRows":["Area 3 ( 1650 samples )","from scratch"],"associatedColumns":["mIoU"],"associatedMergedColumns":[]},{"number":"73.81%","isBolded":true,"associatedRows":["Area 4 ( 3662 samples )","STRL"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"49.53","isBolded":true,"associatedRows":["Area 5 ( 6852 samples )","STRL"],"associatedColumns":["mIoU"],"associatedMergedColumns":[]},{"number":"73.55%","isBolded":false,"associatedRows":["Area 4 ( 3662 samples )","from scratch"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"39.28","isBolded":true,"associatedRows":["Area 4 ( 3662 samples )","STRL"],"associatedColumns":["mIoU"],"associatedMergedColumns":[]}]},{"caption":"Table 5: 3D object detection fine-tuned on KITTI. We \nreport 3D detection performance with moderate difficulty \non the val set of KITTI dataset. Performances below are \nevaluated by mAP with 40 recall positions. \n\n","rows":["STRL + PV - RCNN"],"columns":["3D","BEV","Pedestrian","Cyclist","Car ( IoU\u003d0 . 7 )"],"mergedAllColumns":["( from scratch )","( frozen backbone )","Method"],"numberCells":[{"number":"70.14","isBolded":false,"associatedRows":[],"associatedColumns":["Cyclist","3D"],"associatedMergedColumns":["Method"]},{"number":"59.84","isBolded":false,"associatedRows":[],"associatedColumns":["Pedestrian","BEV"],"associatedMergedColumns":["Method"]},{"number":"90.53","isBolded":false,"associatedRows":[],"associatedColumns":["Car ( IoU\u003d0 . 7 )","BEV"],"associatedMergedColumns":["Method"]},{"number":"57.06","isBolded":false,"associatedRows":[],"associatedColumns":["Pedestrian","3D"],"associatedMergedColumns":["Method"]},{"number":"84.50","isBolded":false,"associatedRows":[],"associatedColumns":["Car ( IoU\u003d0 . 7 )","3D"],"associatedMergedColumns":["Method"]},{"number":"42.41","isBolded":false,"associatedRows":[],"associatedColumns":["Pedestrian","BEV"],"associatedMergedColumns":["( from scratch )"]},{"number":"75.04","isBolded":false,"associatedRows":[],"associatedColumns":["Cyclist","BEV"],"associatedMergedColumns":["Method"]},{"number":"87.84","isBolded":false,"associatedRows":[],"associatedColumns":["Car ( IoU\u003d0 . 7 )","BEV"],"associatedMergedColumns":["( from scratch )"]},{"number":"60.83","isBolded":true,"associatedRows":["STRL + PV - RCNN"],"associatedColumns":["Pedestrian","BEV"],"associatedMergedColumns":["( frozen backbone )"]},{"number":"69.65","isBolded":false,"associatedRows":[],"associatedColumns":["Cyclist","3D"],"associatedMergedColumns":["( from scratch )"]},{"number":"5.3.AnalyticExperimentsandDiscussions","isBolded":true,"associatedRows":[],"associatedColumns":["Pedestrian","BEV"],"associatedMergedColumns":["( frozen backbone )"]},{"number":"39.62","isBolded":false,"associatedRows":[],"associatedColumns":["Pedestrian","3D"],"associatedMergedColumns":["( from scratch )"]},{"number":"57.80","isBolded":true,"associatedRows":["STRL + PV - RCNN"],"associatedColumns":["Pedestrian","3D"],"associatedMergedColumns":["( frozen backbone )"]},{"number":"84.70","isBolded":true,"associatedRows":["STRL + PV - RCNN"],"associatedColumns":["Car ( IoU\u003d0 . 7 )","3D"],"associatedMergedColumns":["( frozen backbone )"]},{"number":"76.65","isBolded":true,"associatedRows":["STRL + PV - RCNN"],"associatedColumns":["Cyclist","BEV"],"associatedMergedColumns":["( frozen backbone )"]},{"number":"81.63","isBolded":false,"associatedRows":[],"associatedColumns":["Car ( IoU\u003d0 . 7 )","3D"],"associatedMergedColumns":["( from scratch )"]},{"number":"71.88","isBolded":true,"associatedRows":["STRL + PV - RCNN"],"associatedColumns":["Cyclist","3D"],"associatedMergedColumns":["( frozen backbone )"]},{"number":"74.20","isBolded":false,"associatedRows":[],"associatedColumns":["Cyclist","BEV"],"associatedMergedColumns":["( from scratch )"]},{"number":"90.75","isBolded":true,"associatedRows":["STRL + PV - RCNN"],"associatedColumns":["Car ( IoU\u003d0 . 7 )","BEV"],"associatedMergedColumns":["( frozen backbone )"]}]},{"caption":"Table 6: Ablation study: cross-domain generalizability \n\n(a) Linear evaluation for shape classification on ModelNet40. \n\n","rows":["STRL + VoteNet","ShapeNet","ScanNet"],"columns":["Accuracy","mAP@0 . 25 IoU","Pre - train Dataset"],"mergedAllColumns":["( b ) Fine - tuned 3D object detection on SUN RGB - D .","STRL + DGCNN ( linear )"],"numberCells":[{"number":"59.2","isBolded":true,"associatedRows":["STRL + VoteNet","ShapeNet","ShapeNet"],"associatedColumns":["Pre - train Dataset","mAP@0 . 25 IoU"],"associatedMergedColumns":["( b ) Fine - tuned 3D object detection on SUN RGB - D ."]},{"number":"92.9%","isBolded":false,"associatedRows":["STRL + VoteNet","ScanNet","ScanNet"],"associatedColumns":["Accuracy"],"associatedMergedColumns":["STRL + DGCNN ( linear )"]},{"number":"90.4%","isBolded":false,"associatedRows":["STRL + VoteNet","ScanNet","ScanNet"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"58.2","isBolded":false,"associatedRows":["STRL + VoteNet","ScanNet","ShapeNet"],"associatedColumns":["Pre - train Dataset","mAP@0 . 25 IoU"],"associatedMergedColumns":["( b ) Fine - tuned 3D object detection on SUN RGB - D ."]},{"number":"90.9%","isBolded":true,"associatedRows":["STRL + VoteNet","ScanNet","ShapeNet"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"93.1%","isBolded":true,"associatedRows":["STRL + VoteNet","ScanNet","ShapeNet"],"associatedColumns":["Accuracy"],"associatedMergedColumns":["STRL + DGCNN ( linear )"]}]},{"caption":"Table 7: Ablation study: temporal transformation \n\n(a) Synthetic Shapes. We evaluate the pre-trained PointNet model \non ModelNet40 by linear evaluation under different temporal \ntransformations. \n\n","rows":["?","Remove scaling","Remove rot . + sca . + trans .","?","Remove translation","Full","Remove rotation"],"columns":["Accuracy","Easy","Moderate","Hard","Synthetic View Transformations"],"mergedAllColumns":["Natural Sequence"],"numberCells":[{"number":"85.5%","isBolded":false,"associatedRows":["Remove rot . + sca . + trans .","?"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"79.39","isBolded":true,"associatedRows":["Remove rot . + sca . + trans .","?"],"associatedColumns":["Accuracy","Hard"],"associatedMergedColumns":["Natural Sequence"]},{"number":"87.9%","isBolded":false,"associatedRows":["?","Remove scaling"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"91.08","isBolded":true,"associatedRows":["?","Full"],"associatedColumns":["Synthetic View Transformations","Easy"],"associatedMergedColumns":["Natural Sequence"]},{"number":"87.2%","isBolded":false,"associatedRows":["Remove translation","?"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"81.21","isBolded":false,"associatedRows":["Remove translation","?"],"associatedColumns":["Synthetic View Transformations","Moderate"],"associatedMergedColumns":["Natural Sequence"]},{"number":"90.17","isBolded":false,"associatedRows":["?","Full"],"associatedColumns":["Synthetic View Transformations","Easy"],"associatedMergedColumns":["Natural Sequence"]},{"number":"79.05","isBolded":false,"associatedRows":["Remove rot . + sca . + trans .","?"],"associatedColumns":["Accuracy","Hard"],"associatedMergedColumns":["Natural Sequence"]},{"number":"87.8%","isBolded":false,"associatedRows":["?","Remove rotation"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"88.3%","isBolded":true,"associatedRows":["?","Full"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"81.63","isBolded":true,"associatedRows":["Remove translation","?"],"associatedColumns":["Synthetic View Transformations","Moderate"],"associatedMergedColumns":["Natural Sequence"]}]},{"caption":"Table 8: Ablation study: spatial data augmentation. We \npre-train the PointNet model on ShapeNet with different \nspatial transformations. Performances below reflect the lin-\near evaluation results on ModelNet40. \n\n","rows":["Remove Crop","Remove Crop and Cutout","Remove Cutout","Down - sample only","Full"],"columns":["Accuracy"],"mergedAllColumns":[],"numberCells":[{"number":"88.3%","isBolded":true,"associatedRows":["Full"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"86.1%","isBolded":false,"associatedRows":["Down - sample only"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"87.4%","isBolded":false,"associatedRows":["Remove Crop and Cutout"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"88.1%","isBolded":false,"associatedRows":["Remove Cutout"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"87.5%","isBolded":false,"associatedRows":["Remove Crop"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]}]}]