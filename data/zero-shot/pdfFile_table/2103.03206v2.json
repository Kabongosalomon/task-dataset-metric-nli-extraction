[{"caption":"77.9 \nResNet-50 (FF) \n73.5 \nViT-B-16 (FF) \n76.7 \nTransformer (64x64, FF) \n57.0 \nPerceiver (FF) \n78.0 \n\nTable 1. Top-1 validation accuracy (in %) on ImageNet. Models \nthat use 2D convolutions exploit domain-specific grid structure \narchitecturally, while models that only use global attention do not. \nThe first block reports standard performance from pixels -these \nnumbers are taken from the literature. The second block shows \nperformance when the inputs are RGB values concatenated with \n2D Fourier features (FF) -the same that the Perceiver receives. \nThis block uses our implementation of the baselines. The Perceiver \nis competitive with standard baselines on ImageNet without relying \non domain-specific architectural assumptions. \n\n","rows":["ViT - B - 16 ( FF )","ResNet - 50 ( He et al . , 2016 )","ResNet - 50 ( FF )","Transformer ( 64x64 , FF )","Perceiver ( FF )"],"columns":[],"mergedAllColumns":[],"numberCells":[{"number":"77.9","isBolded":false,"associatedRows":["ResNet - 50 ( He et al . , 2016 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"73.5","isBolded":false,"associatedRows":["ResNet - 50 ( FF )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"57.0","isBolded":false,"associatedRows":["Transformer ( 64x64 , FF )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"77.6","isBolded":false,"associatedRows":["ResNet - 50 ( He et al . , 2016 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"76.7","isBolded":false,"associatedRows":["ViT - B - 16 ( FF )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"78.0","isBolded":false,"associatedRows":["Perceiver ( FF )"],"associatedColumns":[],"associatedMergedColumns":[]}]},{"caption":"Table 2. Top-1 validation accuracy (in %) on standard (raw) and \npermuted ImageNet (higher is better). Position encodings (in \nparentheses) are constructed before permutation, see text for de-\ntails. While models that only use global attention are stable under \npermutation, models that use 2D convolutions to process local \nneighborhoods are not. The size of the local neighborhood at input \nis given by the input receptive field (RF) size, in pixels. \n\n","rows":["ViT - B - 16 ( FF )","( FF )","ResNet - 50 ( FF )","Transformer ( 64x64 ) ( FF )"],"columns":["Perm .","Raw"],"mergedAllColumns":["256","49","Perceiver :"],"numberCells":[{"number":"39.4","isBolded":false,"associatedRows":["ResNet - 50 ( FF )"],"associatedColumns":["Perm ."],"associatedMergedColumns":[]},{"number":"57.0","isBolded":false,"associatedRows":["Transformer ( 64x64 ) ( FF )"],"associatedColumns":["Raw"],"associatedMergedColumns":["256"]},{"number":"57.0","isBolded":false,"associatedRows":["Transformer ( 64x64 ) ( FF )"],"associatedColumns":["Perm ."],"associatedMergedColumns":["256"]},{"number":"78.0","isBolded":false,"associatedRows":["( FF )"],"associatedColumns":["Perm ."],"associatedMergedColumns":["Perceiver :"]},{"number":"78.0","isBolded":false,"associatedRows":["( FF )"],"associatedColumns":["Raw"],"associatedMergedColumns":["Perceiver :"]},{"number":"61.7","isBolded":false,"associatedRows":["ViT - B - 16 ( FF )"],"associatedColumns":["Perm ."],"associatedMergedColumns":["49"]},{"number":"76.7","isBolded":false,"associatedRows":["ViT - B - 16 ( FF )"],"associatedColumns":["Raw"],"associatedMergedColumns":["49"]},{"number":"73.5","isBolded":false,"associatedRows":["ResNet - 50 ( FF )"],"associatedColumns":["Raw"],"associatedMergedColumns":[]}]},{"caption":"Table 3. Perceiver performance on AudioSet, compared to state-of-the-art models (mAP, higher is better). \n\n","rows":["Perceiver ( mel spectrogram )","CNN - 14 ( no balancing \u0026 no mixup ) ( Kong et al . , 2020 )","Attention AV - fusion ( Fayek \u0026 Kumar , 2020 )","Attention ( Kong et al . , 2018 )","Multi - level Attention ( Yu et al . , 2018 )","Perceiver ( raw audio )","G - blend ( Wang et al . , 2020c )","Perceiver ( mel spectrogram - tuned )","CNN - 14 ( Kong et al . , 2020 )","-","ResNet - 50 ( Ford et al . , 2019 )"],"columns":["-"],"mergedAllColumns":[],"numberCells":[{"number":"32.7","isBolded":false,"associatedRows":["Attention ( Kong et al . , 2018 )"],"associatedColumns":["-"],"associatedMergedColumns":[]},{"number":"25.7","isBolded":false,"associatedRows":["Attention AV - fusion ( Fayek \u0026 Kumar , 2020 )"],"associatedColumns":["-","-","-","-","-","-"],"associatedMergedColumns":[]},{"number":"18.8","isBolded":false,"associatedRows":["G - blend ( Wang et al . , 2020c )"],"associatedColumns":["-","-","-","-","-","-"],"associatedMergedColumns":[]},{"number":"43.1","isBolded":false,"associatedRows":["CNN - 14 ( Kong et al . , 2020 )"],"associatedColumns":["-","-","-","-"],"associatedMergedColumns":[]},{"number":"38.0","isBolded":false,"associatedRows":["ResNet - 50 ( Ford et al . , 2019 )"],"associatedColumns":["-","-","-"],"associatedMergedColumns":[]},{"number":"36.0","isBolded":false,"associatedRows":["Multi - level Attention ( Yu et al . , 2018 )"],"associatedColumns":["-","-"],"associatedMergedColumns":[]},{"number":"37.5","isBolded":false,"associatedRows":["CNN - 14 ( no balancing \u0026 no mixup ) ( Kong et al . , 2020 )"],"associatedColumns":["-","-","-","-","-"],"associatedMergedColumns":[]},{"number":"25.8","isBolded":false,"associatedRows":["Perceiver ( raw audio )"],"associatedColumns":["-","-","-","-","-","-"],"associatedMergedColumns":[]},{"number":"44.2","isBolded":false,"associatedRows":["Perceiver ( mel spectrogram - tuned )","-","-"],"associatedColumns":["-","-","-","-","-","-"],"associatedMergedColumns":[]},{"number":"43.5","isBolded":false,"associatedRows":["Perceiver ( raw audio )"],"associatedColumns":["-","-","-","-","-","-"],"associatedMergedColumns":[]},{"number":"25.8","isBolded":false,"associatedRows":["Perceiver ( mel spectrogram )"],"associatedColumns":["-","-","-","-","-","-"],"associatedMergedColumns":[]},{"number":"46.2","isBolded":false,"associatedRows":["Attention AV - fusion ( Fayek \u0026 Kumar , 2020 )"],"associatedColumns":["-","-","-","-","-","-"],"associatedMergedColumns":[]},{"number":"32.4","isBolded":false,"associatedRows":["G - blend ( Wang et al . , 2020c )"],"associatedColumns":["-","-","-","-","-","-"],"associatedMergedColumns":[]},{"number":"38.4","isBolded":false,"associatedRows":["Attention AV - fusion ( Fayek \u0026 Kumar , 2020 )"],"associatedColumns":["-","-","-","-","-","-"],"associatedMergedColumns":[]},{"number":"38.4","isBolded":false,"associatedRows":["Perceiver ( mel spectrogram )"],"associatedColumns":["-","-","-","-","-","-"],"associatedMergedColumns":[]},{"number":"31.4","isBolded":false,"associatedRows":["CNN - 14 ( no balancing \u0026 no mixup ) ( Kong et al . , 2020 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"38.3","isBolded":false,"associatedRows":["Perceiver ( raw audio )"],"associatedColumns":["-","-","-","-","-","-"],"associatedMergedColumns":[]},{"number":"41.8","isBolded":false,"associatedRows":["G - blend ( Wang et al . , 2020c )"],"associatedColumns":["-","-","-","-","-","-"],"associatedMergedColumns":[]},{"number":"43.2","isBolded":false,"associatedRows":["Perceiver ( mel spectrogram )"],"associatedColumns":["-","-","-","-","-","-"],"associatedMergedColumns":[]}]},{"caption":"91.9 \nResNet-50 (FF) \n66.3 \nViT-B-2 (FF) \n78.9 \nViT-B-4 (FF) \n73.4 \nViT-B-8 (FF) \n65.3 \nViT-B-16 (FF) \n59.6 \nTransformer (44x44) \n82.1 \nPerceiver \n85.7 \n\nTable 4. Top-1 test-set classification accuracy (in %) on Model-\nNet40. Higher is better. We report best result per model class, \nselected by test-set score. There are no RGB features nor a natural \ngrid structure on this dataset. We compare to the generic baselines \nconsidered in previous sections with Fourier feature encodings of \npositions, as well as to a specialized model: PointNet++ (Qi et al., \n2017). PointNet++ uses extra geometric features and performs \nmore advanced augmentations that we did not consider here and \nare not used for the models in blue. \n\n","rows":["ViT - B - 16 ( FF )","ViT - B - 8 ( FF )","ViT - B - 2 ( FF )","ResNet - 50 ( FF )","Transformer ( 44x44 )","ViT - B - 4 ( FF )","Perceiver"],"columns":["Accuracy"],"mergedAllColumns":[],"numberCells":[{"number":"66.3","isBolded":false,"associatedRows":["ResNet - 50 ( FF )"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"82.1","isBolded":false,"associatedRows":["Transformer ( 44x44 )"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"85.7","isBolded":true,"associatedRows":["Perceiver"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"73.4","isBolded":false,"associatedRows":["ViT - B - 4 ( FF )"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"59.6","isBolded":false,"associatedRows":["ViT - B - 16 ( FF )"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"91.9","isBolded":true,"associatedRows":["ResNet - 50 ( FF )"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"78.9","isBolded":false,"associatedRows":["ViT - B - 2 ( FF )"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"65.3","isBolded":false,"associatedRows":["ViT - B - 8 ( FF )"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]}]},{"caption":"Table 5. Performance of models built from a stack of cross-\nattention layers with no latent transformers. We do not share \nweights between cross-attention modules in this experiment. Mod-\nels with 12 cross-attends run out of memory on the largest device \nconfiguration we use (64 TPUs). Results are top-1 validation \naccuracy (in %) on ImageNet (higher is better). \n\n","rows":["4 ( interleaved )","2 ( at start )","1 ( at start )","2 ( interleaved )","8 ( interleaved )","8 ( at start )","1 ( interleaved )","4 ( at start )"],"columns":["FLOPs","Params","Acc ."],"mergedAllColumns":[],"numberCells":[{"number":"447.6B","isBolded":false,"associatedRows":["2 ( interleaved )"],"associatedColumns":["FLOPs"],"associatedMergedColumns":[]},{"number":"707.2B","isBolded":false,"associatedRows":["8 ( at start )"],"associatedColumns":["FLOPs"],"associatedMergedColumns":[]},{"number":"76.7","isBolded":false,"associatedRows":["2 ( at start )"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"76.5","isBolded":false,"associatedRows":["2 ( interleaved )"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"73.7","isBolded":false,"associatedRows":["8 ( at start )"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"75.9","isBolded":false,"associatedRows":["4 ( at start )"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"42.1M","isBolded":false,"associatedRows":["1 ( interleaved )"],"associatedColumns":["Params"],"associatedMergedColumns":[]},{"number":"404.3B","isBolded":false,"associatedRows":["1 ( interleaved )"],"associatedColumns":["FLOPs"],"associatedMergedColumns":[]},{"number":"447.6B","isBolded":false,"associatedRows":["2 ( at start )"],"associatedColumns":["FLOPs"],"associatedMergedColumns":[]},{"number":"76.7","isBolded":false,"associatedRows":["1 ( interleaved )"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"76.5","isBolded":false,"associatedRows":["4 ( interleaved )"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"78.0","isBolded":true,"associatedRows":["8 ( interleaved )"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"534.1B","isBolded":false,"associatedRows":["4 ( interleaved )"],"associatedColumns":["FLOPs"],"associatedMergedColumns":[]},{"number":"44.9M","isBolded":false,"associatedRows":["2 ( interleaved )"],"associatedColumns":["Params"],"associatedMergedColumns":[]},{"number":"44.9M","isBolded":false,"associatedRows":["4 ( interleaved )"],"associatedColumns":["Params"],"associatedMergedColumns":[]},{"number":"76.7","isBolded":false,"associatedRows":["1 ( at start )"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"707.2B","isBolded":false,"associatedRows":["8 ( interleaved )"],"associatedColumns":["FLOPs"],"associatedMergedColumns":[]},{"number":"44.9M","isBolded":false,"associatedRows":["2 ( at start )"],"associatedColumns":["Params"],"associatedMergedColumns":[]},{"number":"44.9M","isBolded":false,"associatedRows":["4 ( at start )"],"associatedColumns":["Params"],"associatedMergedColumns":[]},{"number":"44.9M","isBolded":false,"associatedRows":["8 ( interleaved )"],"associatedColumns":["Params"],"associatedMergedColumns":[]},{"number":"534.1B","isBolded":false,"associatedRows":["4 ( at start )"],"associatedColumns":["FLOPs"],"associatedMergedColumns":[]},{"number":"44.9M","isBolded":false,"associatedRows":["8 ( at start )"],"associatedColumns":["Params"],"associatedMergedColumns":[]},{"number":"404.3B","isBolded":false,"associatedRows":["1 ( at start )"],"associatedColumns":["FLOPs"],"associatedMergedColumns":[]},{"number":"41.1M","isBolded":false,"associatedRows":["1 ( at start )"],"associatedColumns":["Params"],"associatedMergedColumns":[]}]},{"caption":"Table 6. Performance as a function of # of cross-attends and their \narrangement. In \"interleaved,\" cross-attention layers are spaced \nthroughout the network (for re-entrant processing), while in \"at \nstart\" all cross-attends are placed at the start of the network fol-\nlowed by all latent self-attend layers. All cross-attention layers \nexcept the initial one are shared, and self-attends are shared as \nusual (using 8 blocks of 6 self-attention modules). Results are \ntop-1 validation accuracy (in %) on ImageNet (higher is better). \n\n","rows":["4 ( interleaved )","2 ( at start )","1 ( at start )","2 ( interleaved )","8 ( interleaved )","8 ( at start )","1 ( interleaved )","4 ( at start )"],"columns":["FLOPs","Params","Acc ."],"mergedAllColumns":[],"numberCells":[{"number":"41.1M","isBolded":false,"associatedRows":["1 ( at start )"],"associatedColumns":["Params"],"associatedMergedColumns":[]},{"number":"404.3B","isBolded":false,"associatedRows":["1 ( interleaved )"],"associatedColumns":["FLOPs"],"associatedMergedColumns":[]},{"number":"76.5","isBolded":false,"associatedRows":["4 ( interleaved )"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"707.2B","isBolded":false,"associatedRows":["8 ( at start )"],"associatedColumns":["FLOPs"],"associatedMergedColumns":[]},{"number":"76.5","isBolded":false,"associatedRows":["2 ( interleaved )"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"76.7","isBolded":false,"associatedRows":["1 ( at start )"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"447.6B","isBolded":false,"associatedRows":["2 ( interleaved )"],"associatedColumns":["FLOPs"],"associatedMergedColumns":[]},{"number":"404.3B","isBolded":false,"associatedRows":["1 ( at start )"],"associatedColumns":["FLOPs"],"associatedMergedColumns":[]},{"number":"76.7","isBolded":false,"associatedRows":["1 ( interleaved )"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"75.9","isBolded":false,"associatedRows":["4 ( at start )"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"534.1B","isBolded":false,"associatedRows":["4 ( interleaved )"],"associatedColumns":["FLOPs"],"associatedMergedColumns":[]},{"number":"534.1B","isBolded":false,"associatedRows":["4 ( at start )"],"associatedColumns":["FLOPs"],"associatedMergedColumns":[]},{"number":"44.9M","isBolded":false,"associatedRows":["8 ( interleaved )"],"associatedColumns":["Params"],"associatedMergedColumns":[]},{"number":"707.2B","isBolded":false,"associatedRows":["8 ( interleaved )"],"associatedColumns":["FLOPs"],"associatedMergedColumns":[]},{"number":"447.6B","isBolded":false,"associatedRows":["2 ( at start )"],"associatedColumns":["FLOPs"],"associatedMergedColumns":[]},{"number":"76.7","isBolded":false,"associatedRows":["2 ( at start )"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"44.9M","isBolded":false,"associatedRows":["4 ( interleaved )"],"associatedColumns":["Params"],"associatedMergedColumns":[]},{"number":"44.9M","isBolded":false,"associatedRows":["2 ( at start )"],"associatedColumns":["Params"],"associatedMergedColumns":[]},{"number":"44.9M","isBolded":false,"associatedRows":["8 ( at start )"],"associatedColumns":["Params"],"associatedMergedColumns":[]},{"number":"73.7","isBolded":false,"associatedRows":["8 ( at start )"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"44.9M","isBolded":false,"associatedRows":["4 ( at start )"],"associatedColumns":["Params"],"associatedMergedColumns":[]},{"number":"78.0","isBolded":true,"associatedRows":["8 ( interleaved )"],"associatedColumns":["Acc ."],"associatedMergedColumns":[]},{"number":"42.1M","isBolded":false,"associatedRows":["1 ( interleaved )"],"associatedColumns":["Params"],"associatedMergedColumns":[]},{"number":"44.9M","isBolded":false,"associatedRows":["2 ( interleaved )"],"associatedColumns":["Params"],"associatedMergedColumns":[]}]}]