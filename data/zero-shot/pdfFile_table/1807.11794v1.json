[{"caption":"Configuration \nAccuracy (%) \nLRCN \n34.48 \nConvLSTM \n51.72 \nConvLSTM-attention \n63.79 \ntemporal-optical flow \n44.83 \ntemporal-warp flow \n48.28 \ntwo-stream (average) \n67.24 \ntwo-stream (joint train) \n77.59 \n\nTable 1: Comparison of different configurations on the fixed split of GTEA 61 dataset. \n\n","rows":["LRCN","temporal - warp flow","two - stream ( average )","temporal - optical flow","ConvLSTM - attention","two - stream ( joint train )","ConvLSTM"],"columns":["Accuracy ( % )"],"mergedAllColumns":[],"numberCells":[{"number":"77.59","isBolded":false,"associatedRows":["two - stream ( joint train )"],"associatedColumns":["Accuracy ( % )"],"associatedMergedColumns":[]},{"number":"44.83","isBolded":false,"associatedRows":["temporal - optical flow"],"associatedColumns":["Accuracy ( % )"],"associatedMergedColumns":[]},{"number":"48.28","isBolded":false,"associatedRows":["temporal - warp flow"],"associatedColumns":["Accuracy ( % )"],"associatedMergedColumns":[]},{"number":"63.79","isBolded":false,"associatedRows":["ConvLSTM - attention"],"associatedColumns":["Accuracy ( % )"],"associatedMergedColumns":[]},{"number":"67.24","isBolded":false,"associatedRows":["two - stream ( average )"],"associatedColumns":["Accuracy ( % )"],"associatedMergedColumns":[]},{"number":"34.48","isBolded":false,"associatedRows":["LRCN"],"associatedColumns":["Accuracy ( % )"],"associatedMergedColumns":[]},{"number":"51.72","isBolded":false,"associatedRows":["ConvLSTM"],"associatedColumns":["Accuracy ( % )"],"associatedMergedColumns":[]}]},{"caption":"Table 2: Comparison with state-of-the-art methods on popular egocentric datasets, we report \nrecognition accuracy in %. (  *  : fixed split;  *  *  : trained with strong supervision) \n\ndescriptors thus generated from the frames of a video are then encoded using an LSTM with \n512 hidden units. With the convLSTM module, as explained in section 3.1, we are able to \nkeep the spatial dimensions and a memory tensor is propagated across time which enables \nin capturing the evolution of spatio-temporal changes with time. The advantages of using \nconvLSTM can also be validated by comparing the recognition performance obtained with \nthe respective models. We also plot the t-SNE embedding ","rows":["77","79","Ours","TSN [ 23 ]","Ma et al . [ 11 ] * *","Two stream [ 18 ]","64"],"columns":["* *"],"mergedAllColumns":[],"numberCells":[{"number":"66.8","isBolded":false,"associatedRows":["Ma et al . [ 11 ] * *"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"69.33","isBolded":false,"associatedRows":["TSN [ 23 ]"],"associatedColumns":["* *"],"associatedMergedColumns":[]},{"number":"77.59","isBolded":true,"associatedRows":["Ours"],"associatedColumns":["* *"],"associatedMergedColumns":[]},{"number":"49.65","isBolded":false,"associatedRows":["Two stream [ 18 ]","79"],"associatedColumns":["* *"],"associatedMergedColumns":[]},{"number":"73.02","isBolded":false,"associatedRows":["Ma et al . [ 11 ] * *"],"associatedColumns":["* *"],"associatedMergedColumns":[]},{"number":"67.76","isBolded":false,"associatedRows":["TSN [ 23 ]"],"associatedColumns":["* *"],"associatedMergedColumns":[]},{"number":"73.24","isBolded":false,"associatedRows":["Ma et al . [ 11 ] * *","64"],"associatedColumns":["* *"],"associatedMergedColumns":[]},{"number":"51.58","isBolded":false,"associatedRows":["Two stream [ 18 ]"],"associatedColumns":["* *"],"associatedMergedColumns":[]},{"number":"57.64","isBolded":false,"associatedRows":["Two stream [ 18 ]"],"associatedColumns":["* *"],"associatedMergedColumns":[]},{"number":"58.77","isBolded":false,"associatedRows":["Two stream [ 18 ]","79"],"associatedColumns":["* *"],"associatedMergedColumns":[]},{"number":"75.08","isBolded":false,"associatedRows":["Ma et al . [ 11 ] * *"],"associatedColumns":["* *"],"associatedMergedColumns":[]},{"number":"60.5","isBolded":false,"associatedRows":["Two stream [ 18 ]","64"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"66.4","isBolded":true,"associatedRows":["Ma et al . [ 11 ] * *","64"],"associatedColumns":["* *"],"associatedMergedColumns":[]},{"number":"67.23","isBolded":false,"associatedRows":["TSN [ 23 ]","79"],"associatedColumns":["* *"],"associatedMergedColumns":[]},{"number":"62.1","isBolded":false,"associatedRows":["Two stream [ 18 ]","64"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"55.25","isBolded":false,"associatedRows":["TSN [ 23 ]","79"],"associatedColumns":["* *"],"associatedMergedColumns":[]},{"number":"60.13","isBolded":false,"associatedRows":["Ours","79","77"],"associatedColumns":["* *"],"associatedMergedColumns":[]}]},{"caption":"Table 3: Recognition accuracy (in %) obtained for different train-test splits of EGTEA Gaze+ \ndataset compared against state-of-the-art techniques. (*: Result provided by the dataset \ndevelopers) \n\n","rows":["Ours","I3D * [ 1 ]","Two Stream * [ 18 ]","TSN [ 23 ]"],"columns":["Average","SUDHAKARAN , LANZ : OBJECT - CENTRIC ATTENTION FOR ACTIVITY RECOGNITION","Split 3","Split 2","Split 1"],"mergedAllColumns":[],"numberCells":[{"number":"41.47","isBolded":false,"associatedRows":["Two Stream * [ 18 ]"],"associatedColumns":["SUDHAKARAN , LANZ : OBJECT - CENTRIC ATTENTION FOR ACTIVITY RECOGNITION","Split 2"],"associatedMergedColumns":[]},{"number":"43.78","isBolded":false,"associatedRows":["Two Stream * [ 18 ]"],"associatedColumns":["SUDHAKARAN , LANZ : OBJECT - CENTRIC ATTENTION FOR ACTIVITY RECOGNITION","Split 1"],"associatedMergedColumns":[]},{"number":"58.01","isBolded":false,"associatedRows":["TSN [ 23 ]"],"associatedColumns":["SUDHAKARAN , LANZ : OBJECT - CENTRIC ATTENTION FOR ACTIVITY RECOGNITION","Split 1"],"associatedMergedColumns":[]},{"number":"55.93","isBolded":false,"associatedRows":["TSN [ 23 ]"],"associatedColumns":["SUDHAKARAN , LANZ : OBJECT - CENTRIC ATTENTION FOR ACTIVITY RECOGNITION","Average"],"associatedMergedColumns":[]},{"number":"41.84","isBolded":false,"associatedRows":["Two Stream * [ 18 ]"],"associatedColumns":["SUDHAKARAN , LANZ : OBJECT - CENTRIC ATTENTION FOR ACTIVITY RECOGNITION","Average"],"associatedMergedColumns":[]},{"number":"40.28","isBolded":false,"associatedRows":["Two Stream * [ 18 ]"],"associatedColumns":["SUDHAKARAN , LANZ : OBJECT - CENTRIC ATTENTION FOR ACTIVITY RECOGNITION","Split 3"],"associatedMergedColumns":[]},{"number":"51.68","isBolded":false,"associatedRows":["I3D * [ 1 ]"],"associatedColumns":["SUDHAKARAN , LANZ : OBJECT - CENTRIC ATTENTION FOR ACTIVITY RECOGNITION","Average"],"associatedMergedColumns":[]},{"number":"55.01","isBolded":false,"associatedRows":["TSN [ 23 ]"],"associatedColumns":["SUDHAKARAN , LANZ : OBJECT - CENTRIC ATTENTION FOR ACTIVITY RECOGNITION","Split 2"],"associatedMergedColumns":[]},{"number":"61.47","isBolded":true,"associatedRows":["Ours"],"associatedColumns":["SUDHAKARAN , LANZ : OBJECT - CENTRIC ATTENTION FOR ACTIVITY RECOGNITION","Split 2"],"associatedMergedColumns":[]},{"number":"60.76","isBolded":true,"associatedRows":["Ours"],"associatedColumns":["SUDHAKARAN , LANZ : OBJECT - CENTRIC ATTENTION FOR ACTIVITY RECOGNITION","Average"],"associatedMergedColumns":[]},{"number":"54.19","isBolded":false,"associatedRows":["I3D * [ 1 ]"],"associatedColumns":["SUDHAKARAN , LANZ : OBJECT - CENTRIC ATTENTION FOR ACTIVITY RECOGNITION","Split 1"],"associatedMergedColumns":[]},{"number":"54.78","isBolded":false,"associatedRows":["TSN [ 23 ]"],"associatedColumns":["SUDHAKARAN , LANZ : OBJECT - CENTRIC ATTENTION FOR ACTIVITY RECOGNITION","Split 3"],"associatedMergedColumns":[]},{"number":"62.17","isBolded":true,"associatedRows":["Ours"],"associatedColumns":["SUDHAKARAN , LANZ : OBJECT - CENTRIC ATTENTION FOR ACTIVITY RECOGNITION","Split 1"],"associatedMergedColumns":[]},{"number":"51.45","isBolded":false,"associatedRows":["I3D * [ 1 ]"],"associatedColumns":["SUDHAKARAN , LANZ : OBJECT - CENTRIC ATTENTION FOR ACTIVITY RECOGNITION","Split 2"],"associatedMergedColumns":[]},{"number":"49.41","isBolded":false,"associatedRows":["I3D * [ 1 ]"],"associatedColumns":["SUDHAKARAN , LANZ : OBJECT - CENTRIC ATTENTION FOR ACTIVITY RECOGNITION","Split 3"],"associatedMergedColumns":[]},{"number":"58.63","isBolded":true,"associatedRows":["Ours"],"associatedColumns":["SUDHAKARAN , LANZ : OBJECT - CENTRIC ATTENTION FOR ACTIVITY RECOGNITION","Split 3"],"associatedMergedColumns":[]}]}]