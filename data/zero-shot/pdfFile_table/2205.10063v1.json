[{"caption":"Sampling Strategy (25%) \n\nPyramid SM Pre-train ImageNet-1K \nADE20K \nCOCO \nSupport Ratio \nLoss \nTop-1 Acc \nmIoU aAcc AP AP50 AP75 \n(a) RS (MAE [19] Baseline) \n? \n-\n0.4256 \n82.88 \n42.54 80.85 46.0 64.7 49.8 \n(b) GS \n-\n0.3682 \n82.48 \n38.79 79.16 44.4 63.2 48.6 \n(c) US (Ours) \n-\n0.3858 \n82.74 \n41.55 80.48 45.5 64.2 49.6 \n\n(d) UM (Ours) \n\n15% 0.4171 \n82.75 \n41.68 80.54 45.8 64.6 49.8 \n25% 0.4395 \n82.88 \n42.59 80.80 45.9 64.5 50.2 \n35% 0.4645 \n82.68 \n42.02 80.72 45.9 64.6 50.1 \n\nTable 1: Comparisons among different sampling strategies using Vanilla ViT-Base (ViT-B/16) backbone under 200-epoch pre-training. \n","rows":["( d ) UM ( Ours )","15%","25%","35%","( a ) RS ( MAE [ 19 ] Baseline )","?","( c ) US ( Ours )","-","( b ) GS"],"columns":["mIoU","ADE20K","AP75","ImageNet - 1K","AP50","Pre - train","Loss","COCO","Top - 1 Acc","aAcc","AP"],"mergedAllColumns":["Sampling Strategy ( 25% )"],"numberCells":[{"number":"45.9","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","35%"],"associatedColumns":["COCO","AP"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"0.4645","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","35%"],"associatedColumns":["Pre - train","Loss"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"0.3858","isBolded":false,"associatedRows":["( c ) US ( Ours )","-"],"associatedColumns":["Pre - train","Loss"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"48.6","isBolded":false,"associatedRows":["( b ) GS","-"],"associatedColumns":["COCO","AP75"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"82.75","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","15%"],"associatedColumns":["ImageNet - 1K","Top - 1 Acc"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"38.79","isBolded":false,"associatedRows":["( b ) GS","-"],"associatedColumns":["ADE20K","mIoU"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"63.2","isBolded":false,"associatedRows":["( b ) GS","-"],"associatedColumns":["COCO","AP50"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"82.74","isBolded":false,"associatedRows":["( c ) US ( Ours )","-"],"associatedColumns":["ImageNet - 1K","Top - 1 Acc"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"46.0","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","?","-"],"associatedColumns":["COCO","AP"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"49.8","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","15%"],"associatedColumns":["COCO","AP75"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"64.6","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","35%"],"associatedColumns":["COCO","AP50"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"0.3682","isBolded":false,"associatedRows":["( b ) GS","-"],"associatedColumns":["Pre - train","Loss"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"45.5","isBolded":false,"associatedRows":["( c ) US ( Ours )","-"],"associatedColumns":["COCO","AP"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"82.88","isBolded":false,"associatedRows":["( d ) UM ( Ours )","25%"],"associatedColumns":["ImageNet - 1K","Top - 1 Acc"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"0.4256","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","?","-"],"associatedColumns":["Pre - train","Loss"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"82.88","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","?","-"],"associatedColumns":["ImageNet - 1K","Top - 1 Acc"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"50.2","isBolded":false,"associatedRows":["( d ) UM ( Ours )","25%"],"associatedColumns":["COCO","AP75"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"42.54","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","?","-"],"associatedColumns":["ADE20K","mIoU"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"45.8","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","15%"],"associatedColumns":["COCO","AP"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"0.4395","isBolded":false,"associatedRows":["( d ) UM ( Ours )","25%"],"associatedColumns":["Pre - train","Loss"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"42.59","isBolded":false,"associatedRows":["( d ) UM ( Ours )","25%"],"associatedColumns":["ADE20K","mIoU"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"80.72","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","35%"],"associatedColumns":["ADE20K","aAcc"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"42.02","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","35%"],"associatedColumns":["ADE20K","mIoU"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"64.5","isBolded":false,"associatedRows":["( d ) UM ( Ours )","25%"],"associatedColumns":["COCO","AP50"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"82.48","isBolded":false,"associatedRows":["( b ) GS","-"],"associatedColumns":["ImageNet - 1K","Top - 1 Acc"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"79.16","isBolded":false,"associatedRows":["( b ) GS","-"],"associatedColumns":["ADE20K","aAcc"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"49.6","isBolded":false,"associatedRows":["( c ) US ( Ours )","-"],"associatedColumns":["COCO","AP75"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"80.80","isBolded":false,"associatedRows":["( d ) UM ( Ours )","25%"],"associatedColumns":["ADE20K","aAcc"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"50.1","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","35%"],"associatedColumns":["COCO","AP75"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"80.85","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","?","-"],"associatedColumns":["ADE20K","aAcc"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"64.7","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","?","-"],"associatedColumns":["COCO","AP50"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"49.8","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","?","-"],"associatedColumns":["COCO","AP75"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"64.6","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","15%"],"associatedColumns":["COCO","AP50"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"80.48","isBolded":false,"associatedRows":["( c ) US ( Ours )","-"],"associatedColumns":["ADE20K","aAcc"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"45.9","isBolded":false,"associatedRows":["( d ) UM ( Ours )","25%"],"associatedColumns":["COCO","AP"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"41.55","isBolded":false,"associatedRows":["( c ) US ( Ours )","-"],"associatedColumns":["ADE20K","mIoU"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"0.4171","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","15%"],"associatedColumns":["Pre - train","Loss"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"82.68","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","35%"],"associatedColumns":["ImageNet - 1K","Top - 1 Acc"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"44.4","isBolded":false,"associatedRows":["( b ) GS","-"],"associatedColumns":["COCO","AP"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"41.68","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","15%"],"associatedColumns":["ADE20K","mIoU"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"80.54","isBolded":false,"associatedRows":["( a ) RS ( MAE [ 19 ] Baseline )","15%"],"associatedColumns":["ADE20K","aAcc"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]},{"number":"64.2","isBolded":false,"associatedRows":["( c ) US ( Ours )","-"],"associatedColumns":["COCO","AP50"],"associatedMergedColumns":["Sampling Strategy ( 25% )"]}]},{"caption":"Table 2: Secondary Masking Ratio. Based on Swin-\nT, we pre-train models using different SM ratios for \n200 epochs. The Top-1 Accuracy for IN1K, mIoU for \nADE20K and AP for COCO are reported. 25% per-\nforms good overall considering multiple tasks. \n\n","rows":["25%","20%","30%"],"columns":["ADE20K","COCO","IN1K"],"mergedAllColumns":[],"numberCells":[{"number":"47.5","isBolded":false,"associatedRows":["20%"],"associatedColumns":["COCO"],"associatedMergedColumns":[]},{"number":"82.10","isBolded":true,"associatedRows":["30%"],"associatedColumns":["IN1K"],"associatedMergedColumns":[]},{"number":"45.96","isBolded":true,"associatedRows":["25%"],"associatedColumns":["ADE20K"],"associatedMergedColumns":[]},{"number":"47.3","isBolded":false,"associatedRows":["30%"],"associatedColumns":["COCO"],"associatedMergedColumns":[]},{"number":"82.04","isBolded":false,"associatedRows":["20%"],"associatedColumns":["IN1K"],"associatedMergedColumns":[]},{"number":"45.81","isBolded":false,"associatedRows":["30%"],"associatedColumns":["ADE20K"],"associatedMergedColumns":[]},{"number":"45.75","isBolded":false,"associatedRows":["20%"],"associatedColumns":["ADE20K"],"associatedMergedColumns":[]},{"number":"47.7","isBolded":true,"associatedRows":["25%"],"associatedColumns":["COCO"],"associatedMergedColumns":[]},{"number":"82.04","isBolded":false,"associatedRows":["25%"],"associatedColumns":["IN1K"],"associatedMergedColumns":[]}]},{"caption":"Table 3: Efficiency comparisons with representative fine-tuning performance. The real time (hours) and memory consumption (GB) are \nevaluated on 8 GeForce RTX 3090 GPUs with 128 images per GPU ( *  denotes the estimation due to memory limitation 24 GB). To train \nSwin-T under SimMIM which exceeds the GPU memory, we halve the batch size per GPU and double the accumulation step for maintaining \nthe same effective global batch size. The models fine-tuned on ImageNet are also used for fine-tuning on COCO object detection and ADE20K \nsemantic segmentation (i.e., the intermediate fine-tuning scheme [2, 27]). Compared to SimMIM, UM-MAE speeds up by ? 2? and reduces \nthe memory by at least ? 2?, whilst performing competitively. \n\n","rows":["Supervised from Scratch ( Baseline )","UM - MAE ( ours )","Swin - T [ 28 ]","SimMIM [ 42 ]","PVT - S [ 37 ]"],"columns":["Pre - train ( 200 epoch )","ADE20K","ImageNet - 1K","Memory","COCO","Time","Fine - tune ( / Scratch ) Performance","Method"],"mergedAllColumns":[],"numberCells":[{"number":"38.0h","isBolded":false,"associatedRows":["PVT - S [ 37 ]","SimMIM [ 42 ]"],"associatedColumns":["Pre - train ( 200 epoch )","Method","Time"],"associatedMergedColumns":[]},{"number":"25.0h","isBolded":true,"associatedRows":["Swin - T [ 28 ]","UM - MAE ( ours )"],"associatedColumns":["Pre - train ( 200 epoch )","Method","Time"],"associatedMergedColumns":[]},{"number":"49.3h","isBolded":false,"associatedRows":["Swin - T [ 28 ]","SimMIM [ 42 ]"],"associatedColumns":["Pre - train ( 200 epoch )","Method","Time"],"associatedMergedColumns":[]},{"number":"47.7(+0.5)","isBolded":true,"associatedRows":["Swin - T [ 28 ]","UM - MAE ( ours )"],"associatedColumns":["Fine - tune ( / Scratch ) Performance","Method","COCO"],"associatedMergedColumns":[]},{"number":"45.1(+2.8)","isBolded":true,"associatedRows":["Swin - T [ 28 ]","UM - MAE ( ours )"],"associatedColumns":["Fine - tune ( / Scratch ) Performance","Method","COCO"],"associatedMergedColumns":[]},{"number":"47.2","isBolded":false,"associatedRows":["Swin - T [ 28 ]","Supervised from Scratch ( Baseline )"],"associatedColumns":["Fine - tune ( / Scratch ) Performance","Method","COCO"],"associatedMergedColumns":[]},{"number":"82.20(+0.38)","isBolded":true,"associatedRows":["Swin - T [ 28 ]","SimMIM [ 42 ]"],"associatedColumns":["Fine - tune ( / Scratch ) Performance","Method","ImageNet - 1K"],"associatedMergedColumns":[]},{"number":"37.4GB*","isBolded":false,"associatedRows":["Swin - T [ 28 ]","SimMIM [ 42 ]"],"associatedColumns":["Pre - train ( 200 epoch )","Method","Memory"],"associatedMergedColumns":[]},{"number":"43.01(+2.63)","isBolded":false,"associatedRows":["Swin - T [ 28 ]","UM - MAE ( ours )"],"associatedColumns":["Fine - tune ( / Scratch ) Performance","Method","ADE20K"],"associatedMergedColumns":[]},{"number":"40.38","isBolded":false,"associatedRows":["PVT - S [ 37 ]","Supervised from Scratch ( Baseline )"],"associatedColumns":["Fine - tune ( / Scratch ) Performance","Method","ADE20K"],"associatedMergedColumns":[]},{"number":"13.4GB","isBolded":true,"associatedRows":["Swin - T [ 28 ]","UM - MAE ( ours )"],"associatedColumns":["Pre - train ( 200 epoch )","Method","Memory"],"associatedMergedColumns":[]},{"number":"45.96(+1.45)","isBolded":true,"associatedRows":["Swin - T [ 28 ]","UM - MAE ( ours )"],"associatedColumns":["Fine - tune ( / Scratch ) Performance","Method","ADE20K"],"associatedMergedColumns":[]},{"number":"82.04(+0.22)","isBolded":false,"associatedRows":["Swin - T [ 28 ]","UM - MAE ( ours )"],"associatedColumns":["Fine - tune ( / Scratch ) Performance","Method","ImageNet - 1K"],"associatedMergedColumns":[]},{"number":"43.04(+2.66)","isBolded":true,"associatedRows":["PVT - S [ 37 ]","SimMIM [ 42 ]"],"associatedColumns":["Fine - tune ( / Scratch ) Performance","Method","ADE20K"],"associatedMergedColumns":[]},{"number":"11.6GB","isBolded":true,"associatedRows":["Swin - T [ 28 ]","UM - MAE ( ours )"],"associatedColumns":["Pre - train ( 200 epoch )","Method","Memory"],"associatedMergedColumns":[]},{"number":"81.82","isBolded":false,"associatedRows":["Swin - T [ 28 ]","Supervised from Scratch ( Baseline )"],"associatedColumns":["Fine - tune ( / Scratch ) Performance","Method","ImageNet - 1K"],"associatedMergedColumns":[]},{"number":"44.8(+2.5)","isBolded":false,"associatedRows":["PVT - S [ 37 ]","SimMIM [ 42 ]"],"associatedColumns":["Fine - tune ( / Scratch ) Performance","Method","COCO"],"associatedMergedColumns":[]},{"number":"21.3h","isBolded":true,"associatedRows":["Swin - T [ 28 ]","UM - MAE ( ours )"],"associatedColumns":["Pre - train ( 200 epoch )","Method","Time"],"associatedMergedColumns":[]},{"number":"47.6(+0.4)","isBolded":false,"associatedRows":["Swin - T [ 28 ]","SimMIM [ 42 ]"],"associatedColumns":["Fine - tune ( / Scratch ) Performance","Method","COCO"],"associatedMergedColumns":[]},{"number":"42.3","isBolded":false,"associatedRows":["Swin - T [ 28 ]","Supervised from Scratch ( Baseline )"],"associatedColumns":["Fine - tune ( / Scratch ) Performance","Method","COCO"],"associatedMergedColumns":[]},{"number":"77.84","isBolded":false,"associatedRows":["PVT - S [ 37 ]","Supervised from Scratch ( Baseline )"],"associatedColumns":["Fine - tune ( / Scratch ) Performance","Method","ImageNet - 1K"],"associatedMergedColumns":[]},{"number":"20.6GB","isBolded":false,"associatedRows":["PVT - S [ 37 ]","SimMIM [ 42 ]"],"associatedColumns":["Pre - train ( 200 epoch )","Method","Memory"],"associatedMergedColumns":[]},{"number":"79.28(+1.44)","isBolded":false,"associatedRows":["PVT - S [ 37 ]","SimMIM [ 42 ]"],"associatedColumns":["Fine - tune ( / Scratch ) Performance","Method","ImageNet - 1K"],"associatedMergedColumns":[]},{"number":"44.51","isBolded":false,"associatedRows":["Swin - T [ 28 ]","Supervised from Scratch ( Baseline )"],"associatedColumns":["Fine - tune ( / Scratch ) Performance","Method","ADE20K"],"associatedMergedColumns":[]},{"number":"45.35(+0.84)","isBolded":false,"associatedRows":["Swin - T [ 28 ]","SimMIM [ 42 ]"],"associatedColumns":["Fine - tune ( / Scratch ) Performance","Method","ADE20K"],"associatedMergedColumns":[]},{"number":"79.31(+1.47)","isBolded":true,"associatedRows":["Swin - T [ 28 ]","UM - MAE ( ours )"],"associatedColumns":["Fine - tune ( / Scratch ) Performance","Method","ImageNet - 1K"],"associatedMergedColumns":[]}]},{"caption":"Table 4: IN1K Performance on large models. The re-\nsult of SimMIM is borrowed from the paper [42]. \"EP-\nSize\" denotes the Effective Pre-training Size. The reor-\nganized compact 2D input in UM-MAE quarters the Pre-\ntraining Size from 256 2 to 128 2 by dropping 75% to-\nkens. \n\n","rows":["training Size from 256","the proposed UM - MAE significantly saves almost half of the pre - training hours and around","UM - MAE ( ours )","256","pre - training time and memory usage are evaluated on","128","192","to","are comparable ( and sometimes better , e . g . ,","SimMIM [ 42 ]"],"columns":["EP - Size",", we show","P - Size","IN1K"],"mergedAllColumns":["GPU memory budgets against SimMIM , where their performances under multiple downstream tasks","fine - tuning and supervised - from - scratch performances are as well demonstrated . The statistics of","ganized compact 2D input in UM - MAE quarters the Pre -"],"numberCells":[{"number":"2","isBolded":true,"associatedRows":["pre - training time and memory usage are evaluated on","SimMIM [ 42 ]","192"],"associatedColumns":[", we show","P - Size"],"associatedMergedColumns":["GPU memory budgets against SimMIM , where their performances under multiple downstream tasks"]},{"number":"45.96vs.","isBolded":false,"associatedRows":["are comparable ( and sometimes better , e . g . ,"],"associatedColumns":[", we show"],"associatedMergedColumns":["GPU memory budgets against SimMIM , where their performances under multiple downstream tasks"]},{"number":"45.35mIoUonADE20K).","isBolded":false,"associatedRows":["are comparable ( and sometimes better , e . g . ,"],"associatedColumns":[", we show"],"associatedMergedColumns":["GPU memory budgets against SimMIM , where their performances under multiple downstream tasks"]},{"number":"2","isBolded":true,"associatedRows":["pre - training time and memory usage are evaluated on","UM - MAE ( ours )","256"],"associatedColumns":[", we show","EP - Size"],"associatedMergedColumns":["GPU memory budgets against SimMIM , where their performances under multiple downstream tasks"]},{"number":"1","isBolded":true,"associatedRows":["the proposed UM - MAE significantly saves almost half of the pre - training hours and around","UM - MAE ( ours )","192","192"],"associatedColumns":[", we show"],"associatedMergedColumns":["fine - tuning and supervised - from - scratch performances are as well demonstrated . The statistics of"]},{"number":"85.4","isBolded":false,"associatedRows":["the proposed UM - MAE significantly saves almost half of the pre - training hours and around","SimMIM [ 42 ]","192","192"],"associatedColumns":[", we show","IN1K"],"associatedMergedColumns":["GPU memory budgets against SimMIM , where their performances under multiple downstream tasks"]},{"number":"85.3","isBolded":false,"associatedRows":["the proposed UM - MAE significantly saves almost half of the pre - training hours and around","UM - MAE ( ours )","256","128"],"associatedColumns":[", we show","IN1K"],"associatedMergedColumns":["GPU memory budgets against SimMIM , where their performances under multiple downstream tasks"]},{"number":"8GeForceRTX3090GPUs.Itisobservedthat","isBolded":false,"associatedRows":["pre - training time and memory usage are evaluated on"],"associatedColumns":[", we show"],"associatedMergedColumns":["fine - tuning and supervised - from - scratch performances are as well demonstrated . The statistics of"]},{"number":"2","isBolded":true,"associatedRows":["the proposed UM - MAE significantly saves almost half of the pre - training hours and around","UM - MAE ( ours )","192","192"],"associatedColumns":[", we show"],"associatedMergedColumns":["fine - tuning and supervised - from - scratch performances are as well demonstrated . The statistics of"]},{"number":"2","isBolded":true,"associatedRows":["pre - training time and memory usage are evaluated on","SimMIM [ 42 ]","192"],"associatedColumns":[", we show","EP - Size"],"associatedMergedColumns":["GPU memory budgets against SimMIM , where their performances under multiple downstream tasks"]},{"number":"2","isBolded":true,"associatedRows":["pre - training time and memory usage are evaluated on","UM - MAE ( ours )","256"],"associatedColumns":[", we show","P - Size"],"associatedMergedColumns":["GPU memory budgets against SimMIM , where their performances under multiple downstream tasks"]},{"number":"2","isBolded":true,"associatedRows":["pre - training time and memory usage are evaluated on","training Size from 256","256"],"associatedColumns":[", we show","P - Size"],"associatedMergedColumns":["ganized compact 2D input in UM - MAE quarters the Pre -"]},{"number":"3","isBolded":true,"associatedRows":["the proposed UM - MAE significantly saves almost half of the pre - training hours and around","UM - MAE ( ours )","192","192","to"],"associatedColumns":[", we show"],"associatedMergedColumns":["fine - tuning and supervised - from - scratch performances are as well demonstrated . The statistics of"]},{"number":"2","isBolded":true,"associatedRows":["pre - training time and memory usage are evaluated on","UM - MAE ( ours )"],"associatedColumns":[", we show","P - Size"],"associatedMergedColumns":["ganized compact 2D input in UM - MAE quarters the Pre -"]},{"number":"2","isBolded":true,"associatedRows":["the proposed UM - MAE significantly saves almost half of the pre - training hours and around","UM - MAE ( ours )","192","192","to"],"associatedColumns":[", we show"],"associatedMergedColumns":["fine - tuning and supervised - from - scratch performances are as well demonstrated . The statistics of"]}]},{"caption":"Table 5: COCO Performance on large models. The result of the \nBaseline refers to the paper [28]. \"Epoch\": training schedule on \nCOCO. \n\n","rows":["Baseline [ 28 ]","UM - MAE ( ours )","36","IN1K , unsup","IN22K , sup","72"],"columns":["Table 4 reports the Top - 1 accuracy , showing that our","AP box","The model is further fine - tuned on COCO under the same HTC++ [ 4 , 28 ]","AP"],"mergedAllColumns":["using only half the training epochs of Baseline ."],"numberCells":[{"number":"57.4","isBolded":true,"associatedRows":["UM - MAE ( ours )","IN1K , unsup","36"],"associatedColumns":["Table 4 reports the Top - 1 accuracy , showing that our","The model is further fine - tuned on COCO under the same HTC++ [ 4 , 28 ]","AP box"],"associatedMergedColumns":["using only half the training epochs of Baseline ."]},{"number":"49.5","isBolded":false,"associatedRows":["Baseline [ 28 ]","IN22K , sup","72"],"associatedColumns":["Table 4 reports the Top - 1 accuracy , showing that our","The model is further fine - tuned on COCO under the same HTC++ [ 4 , 28 ]","AP"],"associatedMergedColumns":["using only half the training epochs of Baseline ."]},{"number":"57.1","isBolded":false,"associatedRows":["Baseline [ 28 ]","IN22K , sup","72"],"associatedColumns":["Table 4 reports the Top - 1 accuracy , showing that our","The model is further fine - tuned on COCO under the same HTC++ [ 4 , 28 ]","AP box"],"associatedMergedColumns":["using only half the training epochs of Baseline ."]},{"number":"49.8","isBolded":true,"associatedRows":["UM - MAE ( ours )","IN1K , unsup","36"],"associatedColumns":["Table 4 reports the Top - 1 accuracy , showing that our","The model is further fine - tuned on COCO under the same HTC++ [ 4 , 28 ]","AP"],"associatedMergedColumns":["using only half the training epochs of Baseline ."]}]},{"caption":"Table 6: The effectiveness of intermediate fine-tuning based on \nSwin-T when transferring it to downstream semantic segmen-\ntation tasks. The intermediate fine-tuning is crucial for Pyramid-\nbased ViTs self-supervised by various MIM frameworks. \"Inter-\nmediate\" denotes the usage of intermediate fine-tuning [2, 27] on \nIN1K for 100 epochs, after the self-supervising process. \n\n","rows":["SimMIM","?","Supervised","-","UM - MAE"],"columns":["mIoU","mAcc","aAcc"],"mergedAllColumns":[],"numberCells":[{"number":"45.35","isBolded":false,"associatedRows":["SimMIM","?"],"associatedColumns":["mIoU"],"associatedMergedColumns":[]},{"number":"55.65","isBolded":false,"associatedRows":["SimMIM","?"],"associatedColumns":["mAcc"],"associatedMergedColumns":[]},{"number":"80.10","isBolded":false,"associatedRows":["SimMIM","?"],"associatedColumns":["aAcc"],"associatedMergedColumns":[]},{"number":"56.55","isBolded":false,"associatedRows":["UM - MAE","?"],"associatedColumns":["mAcc"],"associatedMergedColumns":[]},{"number":"40.25","isBolded":false,"associatedRows":["SimMIM","?"],"associatedColumns":["mIoU"],"associatedMergedColumns":[]},{"number":"45.96","isBolded":false,"associatedRows":["UM - MAE","?"],"associatedColumns":["mIoU"],"associatedMergedColumns":[]},{"number":"50.89","isBolded":false,"associatedRows":["SimMIM","?"],"associatedColumns":["mAcc"],"associatedMergedColumns":[]},{"number":"54.95","isBolded":false,"associatedRows":["Supervised","-"],"associatedColumns":["mAcc"],"associatedMergedColumns":[]},{"number":"81.81","isBolded":false,"associatedRows":["SimMIM","?"],"associatedColumns":["aAcc"],"associatedMergedColumns":[]},{"number":"44.51","isBolded":false,"associatedRows":["Supervised","-"],"associatedColumns":["mIoU"],"associatedMergedColumns":[]},{"number":"81.58","isBolded":false,"associatedRows":["UM - MAE","?"],"associatedColumns":["aAcc"],"associatedMergedColumns":[]},{"number":"80.01","isBolded":false,"associatedRows":["UM - MAE","?"],"associatedColumns":["aAcc"],"associatedMergedColumns":[]},{"number":"52.13","isBolded":false,"associatedRows":["UM - MAE","?"],"associatedColumns":["mAcc"],"associatedMergedColumns":[]},{"number":"41.14","isBolded":false,"associatedRows":["UM - MAE","?"],"associatedColumns":["mIoU"],"associatedMergedColumns":[]},{"number":"81.52","isBolded":false,"associatedRows":["Supervised","-"],"associatedColumns":["aAcc"],"associatedMergedColumns":[]}]},{"caption":"Table 7: The effectiveness of layer-wise learning rate decay (\"lw-\nlr decay\" for short). Under MIM, lw-lr decay is crucial for Vanilla \nViT, but is harmful for Pyramid-based ViTs. \"lw-lr decay\" being \n1.00 means no decay applied. The pre-trained ViT-B model is down-\nloaded from the official MAE github. \n\n","rows":["ViT - B","Swin - T"],"columns":["lw - lr decay","mIoU","mAcc","aAcc"],"mergedAllColumns":["MAE","200 ,","1600 ,"],"numberCells":[{"number":"56.19","isBolded":false,"associatedRows":["ViT - B"],"associatedColumns":["mAcc"],"associatedMergedColumns":[]},{"number":"0.85","isBolded":false,"associatedRows":["ViT - B"],"associatedColumns":["lw - lr decay"],"associatedMergedColumns":[]},{"number":"58.99","isBolded":true,"associatedRows":["Swin - T"],"associatedColumns":["mAcc"],"associatedMergedColumns":["1600 ,"]},{"number":"81.58","isBolded":true,"associatedRows":["Swin - T"],"associatedColumns":["aAcc"],"associatedMergedColumns":["MAE"]},{"number":"45.80","isBolded":false,"associatedRows":["Swin - T"],"associatedColumns":["mIoU"],"associatedMergedColumns":["MAE"]},{"number":"56.55","isBolded":false,"associatedRows":["Swin - T"],"associatedColumns":["mAcc"],"associatedMergedColumns":["MAE"]},{"number":"45.96","isBolded":true,"associatedRows":["Swin - T"],"associatedColumns":["mIoU"],"associatedMergedColumns":["MAE"]},{"number":"81.45","isBolded":false,"associatedRows":["Swin - T"],"associatedColumns":["aAcc"],"associatedMergedColumns":["MAE"]},{"number":"83.05","isBolded":false,"associatedRows":["Swin - T"],"associatedColumns":["aAcc"],"associatedMergedColumns":["1600 ,"]},{"number":"45.87","isBolded":false,"associatedRows":["ViT - B"],"associatedColumns":["mIoU"],"associatedMergedColumns":[]},{"number":"48.15","isBolded":true,"associatedRows":["Swin - T"],"associatedColumns":["mIoU"],"associatedMergedColumns":["1600 ,"]},{"number":"1.00","isBolded":false,"associatedRows":["ViT - B"],"associatedColumns":["lw - lr decay"],"associatedMergedColumns":[]},{"number":"0.65","isBolded":false,"associatedRows":["Swin - T"],"associatedColumns":["lw - lr decay"],"associatedMergedColumns":["200 ,"]},{"number":"1.00","isBolded":false,"associatedRows":["Swin - T"],"associatedColumns":["lw - lr decay"],"associatedMergedColumns":["MAE"]},{"number":"56.68","isBolded":true,"associatedRows":["Swin - T"],"associatedColumns":["mAcc"],"associatedMergedColumns":["MAE"]},{"number":"56.43","isBolded":false,"associatedRows":["Swin - T"],"associatedColumns":["mAcc"],"associatedMergedColumns":["200 ,"]},{"number":"47.80","isBolded":false,"associatedRows":["ViT - B"],"associatedColumns":["mIoU"],"associatedMergedColumns":[]},{"number":"0.65","isBolded":false,"associatedRows":["Swin - T"],"associatedColumns":["lw - lr decay"],"associatedMergedColumns":["1600 ,"]},{"number":"82.46","isBolded":false,"associatedRows":["Swin - T"],"associatedColumns":["aAcc"],"associatedMergedColumns":[]},{"number":"80.74","isBolded":false,"associatedRows":["Swin - T"],"associatedColumns":["aAcc"],"associatedMergedColumns":["200 ,"]},{"number":"45.42","isBolded":false,"associatedRows":["Swin - T"],"associatedColumns":["mIoU"],"associatedMergedColumns":["200 ,"]},{"number":"83.27","isBolded":true,"associatedRows":["ViT - B"],"associatedColumns":["aAcc"],"associatedMergedColumns":[]},{"number":"58.26","isBolded":false,"associatedRows":["ViT - B"],"associatedColumns":["mAcc"],"associatedMergedColumns":[]},{"number":"0.85","isBolded":false,"associatedRows":["Swin - T"],"associatedColumns":["lw - lr decay"],"associatedMergedColumns":["MAE"]}]},{"caption":"Table 8: Pre-training and fine-tuning settings on ImageNet-1K dataset. \n","rows":["optimizer momentum","?1 , ?2\u003d0 . 9 ,","mixup","label smoothing","augmentation","cutmix","weight decay","RandAug ( 9 ,","-","drop path","RandomResizedCrop","layer - wise lr decay"],"columns":["2","5","1024","AdamW","Pre - train Value","1 . 5e - 4","Fine - tune Value","cosine decay","5e - 4","64"],"mergedAllColumns":[],"numberCells":[{"number":"1.0","isBolded":false,"associatedRows":["cutmix","-"],"associatedColumns":["Fine - tune Value","AdamW","5e - 4","1024","64","2","cosine decay","5"],"associatedMergedColumns":[]},{"number":"0.1(ViT-B,Swin-T,PVT-S),","isBolded":false,"associatedRows":["drop path","-"],"associatedColumns":["Fine - tune Value","AdamW","5e - 4","1024","64","2","cosine decay","5"],"associatedMergedColumns":[]},{"number":"0.7(Swin-L),","isBolded":false,"associatedRows":["layer - wise lr decay","?1 , ?2\u003d0 . 9 ,"],"associatedColumns":["Fine - tune Value","AdamW","5e - 4"],"associatedMergedColumns":[]},{"number":"0.05","isBolded":false,"associatedRows":["weight decay"],"associatedColumns":["Pre - train Value","AdamW","1 . 5e - 4"],"associatedMergedColumns":[]},{"number":"0.95","isBolded":true,"associatedRows":["optimizer momentum","?1 , ?2\u003d0 . 9 ,"],"associatedColumns":["Pre - train Value","AdamW","1 . 5e - 4"],"associatedMergedColumns":[]},{"number":"0.85(Swin-T,PVT-S)","isBolded":false,"associatedRows":["layer - wise lr decay","RandomResizedCrop","?1 , ?2\u003d0 . 9 ,"],"associatedColumns":["Fine - tune Value","AdamW","5e - 4"],"associatedMergedColumns":[]},{"number":"1.0","isBolded":false,"associatedRows":["layer - wise lr decay"],"associatedColumns":["Pre - train Value","AdamW","1 . 5e - 4"],"associatedMergedColumns":[]},{"number":"0.8(ViT-B),","isBolded":false,"associatedRows":["layer - wise lr decay","RandomResizedCrop","?1 , ?2\u003d0 . 9 ,"],"associatedColumns":["Fine - tune Value","AdamW","5e - 4"],"associatedMergedColumns":[]},{"number":"0.2(Swin-L)","isBolded":false,"associatedRows":["drop path","-","RandAug ( 9 ,"],"associatedColumns":["Fine - tune Value","AdamW","5e - 4","1024","64","2","cosine decay","5"],"associatedMergedColumns":[]},{"number":"0.999","isBolded":true,"associatedRows":["optimizer momentum","?1 , ?2\u003d0 . 9 ,","?1 , ?2\u003d0 . 9 ,"],"associatedColumns":["Fine - tune Value","AdamW","5e - 4"],"associatedMergedColumns":[]},{"number":"0.5)","isBolded":false,"associatedRows":["augmentation","RandomResizedCrop","RandAug ( 9 ,"],"associatedColumns":["Fine - tune Value","AdamW","5e - 4","1024","64","2","cosine decay","5"],"associatedMergedColumns":[]},{"number":"0.1","isBolded":false,"associatedRows":["label smoothing","-"],"associatedColumns":["Fine - tune Value","AdamW","5e - 4","1024","64","2","cosine decay","5"],"associatedMergedColumns":[]},{"number":"0.05","isBolded":false,"associatedRows":["weight decay","?1 , ?2\u003d0 . 9 ,"],"associatedColumns":["Fine - tune Value","AdamW","5e - 4"],"associatedMergedColumns":[]},{"number":"0.8","isBolded":false,"associatedRows":["mixup","-"],"associatedColumns":["Fine - tune Value","AdamW","5e - 4","1024","64","2","cosine decay","5"],"associatedMergedColumns":[]}]}]