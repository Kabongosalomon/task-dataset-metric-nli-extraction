[{"caption":"Table 1. For each method, we report supervised metrics (i.e., ones requiring human references): B@1 \u003d BLEU-1, M \u003d METEOR, C \u003d \nCIDEr, S \u003d SPICE. We also report diversity metrics, which measures the vocabulary size (Vocab), and the number of novel sentences w.r.t \nthe training set (%Novel). Finally, we report semantic relatedness to the image (CLIP-S), and to the human references (CLIP-S Ref ) based \non CLIP\u0027s embeddings. \n\n","rows":["Ours","100%","1125","1650","2464","ClipCap [ 51 ]","CLIP - VL [ 64 ]","8681","VinVL [ 78 ]"],"columns":["CLIP - S","CLIP - S Ref","C","S","Unsupervised Metric","%Novel","B@4","Supervised Metrics","Diversity Metrics","M"],"mergedAllColumns":[],"numberCells":[{"number":"0.77","isBolded":false,"associatedRows":["CLIP - VL [ 64 ]","2464"],"associatedColumns":["Unsupervised Metric","CLIP - S"],"associatedMergedColumns":[]},{"number":"0.79","isBolded":false,"associatedRows":["Ours"],"associatedColumns":["Supervised Metrics","CLIP - S Ref"],"associatedMergedColumns":[]},{"number":"0.87","isBolded":true,"associatedRows":["Ours","8681","100%"],"associatedColumns":["Unsupervised Metric","CLIP - S"],"associatedMergedColumns":[]},{"number":"5.5","isBolded":false,"associatedRows":["Ours"],"associatedColumns":["Supervised Metrics","S"],"associatedMergedColumns":[]},{"number":"0.83","isBolded":true,"associatedRows":["VinVL [ 78 ]"],"associatedColumns":["Supervised Metrics","CLIP - S Ref"],"associatedMergedColumns":[]},{"number":"11.5","isBolded":false,"associatedRows":["Ours"],"associatedColumns":["Supervised Metrics","M"],"associatedMergedColumns":[]},{"number":"77.9%","isBolded":false,"associatedRows":["VinVL [ 78 ]","1125"],"associatedColumns":["Diversity Metrics","%Novel"],"associatedMergedColumns":[]},{"number":"66.4%","isBolded":false,"associatedRows":["ClipCap [ 51 ]","1650"],"associatedColumns":["Diversity Metrics","%Novel"],"associatedMergedColumns":[]},{"number":"32.15","isBolded":false,"associatedRows":["ClipCap [ 51 ]"],"associatedColumns":["Supervised Metrics","B@4"],"associatedMergedColumns":[]},{"number":"27.1","isBolded":false,"associatedRows":["ClipCap [ 51 ]"],"associatedColumns":["Supervised Metrics","M"],"associatedMergedColumns":[]},{"number":"0.77","isBolded":false,"associatedRows":["ClipCap [ 51 ]","1650"],"associatedColumns":["Unsupervised Metric","CLIP - S"],"associatedMergedColumns":[]},{"number":"108.35","isBolded":false,"associatedRows":["ClipCap [ 51 ]"],"associatedColumns":["Supervised Metrics","C"],"associatedMergedColumns":[]},{"number":"40.2","isBolded":false,"associatedRows":["CLIP - VL [ 64 ]"],"associatedColumns":["Supervised Metrics","B@4"],"associatedMergedColumns":[]},{"number":"31.1","isBolded":true,"associatedRows":["VinVL [ 78 ]"],"associatedColumns":["Supervised Metrics","M"],"associatedMergedColumns":[]},{"number":"85.1%","isBolded":false,"associatedRows":["CLIP - VL [ 64 ]","2464"],"associatedColumns":["Diversity Metrics","%Novel"],"associatedMergedColumns":[]},{"number":"29.7","isBolded":false,"associatedRows":["CLIP - VL [ 64 ]"],"associatedColumns":["Supervised Metrics","M"],"associatedMergedColumns":[]},{"number":"23.8","isBolded":false,"associatedRows":["CLIP - VL [ 64 ]"],"associatedColumns":["Supervised Metrics","S"],"associatedMergedColumns":[]},{"number":"0.78","isBolded":false,"associatedRows":["VinVL [ 78 ]","1125"],"associatedColumns":["Unsupervised Metric","CLIP - S"],"associatedMergedColumns":[]},{"number":"14.6","isBolded":false,"associatedRows":["Ours"],"associatedColumns":["Supervised Metrics","C"],"associatedMergedColumns":[]},{"number":"25.2","isBolded":true,"associatedRows":["VinVL [ 78 ]"],"associatedColumns":["Supervised Metrics","S"],"associatedMergedColumns":[]},{"number":"20.12","isBolded":false,"associatedRows":["ClipCap [ 51 ]"],"associatedColumns":["Supervised Metrics","S"],"associatedMergedColumns":[]},{"number":"2.6","isBolded":false,"associatedRows":["Ours"],"associatedColumns":["Supervised Metrics","B@4"],"associatedMergedColumns":[]},{"number":"0.81","isBolded":false,"associatedRows":["ClipCap [ 51 ]"],"associatedColumns":["Supervised Metrics","CLIP - S Ref"],"associatedMergedColumns":[]},{"number":"41.0","isBolded":true,"associatedRows":["VinVL [ 78 ]"],"associatedColumns":["Supervised Metrics","B@4"],"associatedMergedColumns":[]},{"number":"134.2","isBolded":false,"associatedRows":["CLIP - VL [ 64 ]"],"associatedColumns":["Supervised Metrics","C"],"associatedMergedColumns":[]},{"number":"0.82","isBolded":false,"associatedRows":["CLIP - VL [ 64 ]"],"associatedColumns":["Supervised Metrics","CLIP - S Ref"],"associatedMergedColumns":[]},{"number":"140.9","isBolded":true,"associatedRows":["VinVL [ 78 ]"],"associatedColumns":["Supervised Metrics","C"],"associatedMergedColumns":[]}]},{"caption":"Table 3. Comparison of our method with and without optimiza-\ntion. We show two variants: (A1) selecting tokens one by one to \nmaximize the CLIP score, and (A2) doing so on a score that com-\nbines CLIP score with an LM-score. \n\n","rows":["A1","A2","Ours"],"columns":["CLIP - S","Perplexity"],"mergedAllColumns":[],"numberCells":[{"number":"0.98","isBolded":true,"associatedRows":["A1"],"associatedColumns":["CLIP - S"],"associatedMergedColumns":[]},{"number":"6.04","isBolded":false,"associatedRows":["A2"],"associatedColumns":["Perplexity"],"associatedMergedColumns":[]},{"number":"8.61","isBolded":false,"associatedRows":["A1"],"associatedColumns":["Perplexity"],"associatedMergedColumns":[]},{"number":"0.87","isBolded":false,"associatedRows":["Ours"],"associatedColumns":["CLIP - S"],"associatedMergedColumns":[]},{"number":"0.91","isBolded":false,"associatedRows":["A2"],"associatedColumns":["CLIP - S"],"associatedMergedColumns":[]},{"number":"5.50","isBolded":true,"associatedRows":["Ours"],"associatedColumns":["Perplexity"],"associatedMergedColumns":[]}]}]