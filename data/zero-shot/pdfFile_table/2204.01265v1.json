[{"caption":"Method \nLRW \nLRW-1000 \n\nYang et al. [60] \n83.0 \n38.19 \nMulti-Grained [51] \n83.3 \n36.91 \nPCPG [30] \n83.5 \n38.70 \nDeformation Flow [56] \n84.1 \n41.93 \nMI Maximization [62] \n84.4 \n38.79 \nFace Cutout [61] \n85.0 \n45.24 \nMS-TCN [31] \n85.3 \n41.40 \nProposed Method \n85.4 \n50.82 \n\nTable 1. Lip reading word accuracy comparison with visual modal \ninputs on LRW and LRW-1000 dataset. \n\n","rows":["Proposed Method","Yang et al . [ 60 ]","PCPG [ 30 ]","MI Maximization [ 62 ]","Deformation Flow [ 56 ]","Multi - Grained [ 51 ]","Face Cutout [ 61 ]","MS - TCN [ 31 ]"],"columns":["LRW","LRW - 1000"],"mergedAllColumns":[],"numberCells":[{"number":"41.40","isBolded":false,"associatedRows":["MS - TCN [ 31 ]"],"associatedColumns":["LRW - 1000"],"associatedMergedColumns":[]},{"number":"85.4","isBolded":true,"associatedRows":["Proposed Method"],"associatedColumns":["LRW"],"associatedMergedColumns":[]},{"number":"85.3","isBolded":false,"associatedRows":["MS - TCN [ 31 ]"],"associatedColumns":["LRW"],"associatedMergedColumns":[]},{"number":"85.0","isBolded":false,"associatedRows":["Face Cutout [ 61 ]"],"associatedColumns":["LRW"],"associatedMergedColumns":[]},{"number":"50.82","isBolded":true,"associatedRows":["Proposed Method"],"associatedColumns":["LRW - 1000"],"associatedMergedColumns":[]},{"number":"38.19","isBolded":false,"associatedRows":["Yang et al . [ 60 ]"],"associatedColumns":["LRW - 1000"],"associatedMergedColumns":[]},{"number":"83.3","isBolded":false,"associatedRows":["Multi - Grained [ 51 ]"],"associatedColumns":["LRW"],"associatedMergedColumns":[]},{"number":"83.5","isBolded":false,"associatedRows":["PCPG [ 30 ]"],"associatedColumns":["LRW"],"associatedMergedColumns":[]},{"number":"83.0","isBolded":false,"associatedRows":["Yang et al . [ 60 ]"],"associatedColumns":["LRW"],"associatedMergedColumns":[]},{"number":"45.24","isBolded":false,"associatedRows":["Face Cutout [ 61 ]"],"associatedColumns":["LRW - 1000"],"associatedMergedColumns":[]},{"number":"84.4","isBolded":false,"associatedRows":["MI Maximization [ 62 ]"],"associatedColumns":["LRW"],"associatedMergedColumns":[]},{"number":"84.1","isBolded":false,"associatedRows":["Deformation Flow [ 56 ]"],"associatedColumns":["LRW"],"associatedMergedColumns":[]},{"number":"41.93","isBolded":false,"associatedRows":["Deformation Flow [ 56 ]"],"associatedColumns":["LRW - 1000"],"associatedMergedColumns":[]},{"number":"38.79","isBolded":false,"associatedRows":["MI Maximization [ 62 ]"],"associatedColumns":["LRW - 1000"],"associatedMergedColumns":[]},{"number":"36.91","isBolded":false,"associatedRows":["Multi - Grained [ 51 ]"],"associatedColumns":["LRW - 1000"],"associatedMergedColumns":[]},{"number":"38.70","isBolded":false,"associatedRows":["PCPG [ 30 ]"],"associatedColumns":["LRW - 1000"],"associatedMergedColumns":[]}]},{"caption":"started [sta] \n\nstart [sta] \n\nInput video \n\nAddress score \n\nset [set] \nset [set] \n\nInput video \n\nAddress score \n\n(a) \n\n(b) \n\nFigure 3. Face video clips (source modality) and corresponding addressing vectors for recalling audio modality (target modality) from \nlearned representations inside memory: (a) results from lip reading and (b) results from speech reconstruction from silent video. \n\nMethod \nSTOI \nESTOI \nPESQ \nVid2Speech [13]  0.491 \n0.335 \n1.734 \nLip2AudSpec [3]  0.513 \n0.352 \n1.673 \nVougioukas et al. [50]  0.564 \n0.361 \n1.684 \nEphrat et al. [12]  0.659 \n0.376 \n1.825 \nLip2Wav [39]  0.731 \n0.535 \n1.772 \nYadav et al. [58]  0.724 \n0.540 \n1.932 \nProposed Method \n0.738 \n0.579 \n1.984 \n\nTable 2. Performance of speech reconstruction comparison with \nvisual modal inputs in a speaker-dependent setting on GRID. \n\n","rows":["Proposed Method","Vougioukas et al . [ 50 ]","Yadav et al . [ 58 ]","Lip2Wav [ 39 ]","-","Lip2AudSpec [ 3 ]","Ephrat et al . [ 12 ]","Ground Truth","Vid2Speech [ 13 ]"],"columns":["started [ sta ]","Naturalness","start [ sta ]","ESTOI","PESQ","Intelligibility","set [ set ]","video","STOI"],"mergedAllColumns":["Proposed Method","learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video .","( +WaveNet vocoder [ 59 ] )","visual modal inputs in a speaker - dependent setting on GRID ."],"numberCells":[{"number":"0.513","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.724","isBolded":false,"associatedRows":["Yadav et al . [ 58 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.445","isBolded":false,"associatedRows":["Proposed Method","Vougioukas et al . [ 50 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness","STOI"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.535","isBolded":false,"associatedRows":["Lip2Wav [ 39 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["Proposed Method"]},{"number":"1.42?","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]","Vid2Speech [ 13 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.772","isBolded":false,"associatedRows":["Lip2Wav [ 39 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["Proposed Method"]},{"number":"0.731","isBolded":false,"associatedRows":["Lip2Wav [ 39 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["Proposed Method"]},{"number":"0.335","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.315","isBolded":true,"associatedRows":["Proposed Method","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility","ESTOI"],"associatedMergedColumns":["visual modal inputs in a speaker - dependent setting on GRID ."]},{"number":"2.94?","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.21","isBolded":true,"associatedRows":["Vougioukas et al . [ 50 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.565","isBolded":false,"associatedRows":["Proposed Method","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness","STOI"],"associatedMergedColumns":["visual modal inputs in a speaker - dependent setting on GRID ."]},{"number":"2.83?","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.31?","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]","Vid2Speech [ 13 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.600","isBolded":true,"associatedRows":["Proposed Method","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness","STOI"],"associatedMergedColumns":["visual modal inputs in a speaker - dependent setting on GRID ."]},{"number":"2.93?","isBolded":true,"associatedRows":["Vougioukas et al . [ 50 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.279","isBolded":false,"associatedRows":["Proposed Method","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility","ESTOI"],"associatedMergedColumns":["visual modal inputs in a speaker - dependent setting on GRID ."]},{"number":"1.734","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.14","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]","Ground Truth"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.352","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.825","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.579","isBolded":true,"associatedRows":["Proposed Method"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.24","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]","Vid2Speech [ 13 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.14","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.564","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.659","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.13","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]","Ground Truth"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.376","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.684","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.738","isBolded":true,"associatedRows":["Proposed Method"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"1.279","isBolded":false,"associatedRows":["Proposed Method","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility","PESQ"],"associatedMergedColumns":["visual modal inputs in a speaker - dependent setting on GRID ."]},{"number":"0.491","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.673","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.540","isBolded":false,"associatedRows":["Yadav et al . [ 58 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"1.984","isBolded":true,"associatedRows":["Proposed Method"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"4.37?","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.16","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"4.57?","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]","Ground Truth"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"4.27?","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.19","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.332","isBolded":true,"associatedRows":["Proposed Method","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility","PESQ"],"associatedMergedColumns":["visual modal inputs in a speaker - dependent setting on GRID ."]},{"number":"1.932","isBolded":false,"associatedRows":["Yadav et al . [ 58 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"1.240","isBolded":false,"associatedRows":["Proposed Method","Vougioukas et al . [ 50 ]","-"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility","PESQ"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.21","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"3.56?","isBolded":true,"associatedRows":["Vougioukas et al . [ 50 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"4.62?","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]","Ground Truth"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.361","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.23","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]","Vid2Speech [ 13 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.19","isBolded":true,"associatedRows":["Vougioukas et al . [ 50 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]}]},{"caption":"Table 3. Mean opinion scores for human evaluation on GRID. \n\n","rows":["Proposed Method","Vougioukas et al . [ 50 ]","Yadav et al . [ 58 ]","Lip2Wav [ 39 ]","-","Lip2AudSpec [ 3 ]","Ephrat et al . [ 12 ]","Ground Truth","Vid2Speech [ 13 ]"],"columns":["started [ sta ]","Naturalness","start [ sta ]","ESTOI","PESQ","Intelligibility","set [ set ]","video","STOI"],"mergedAllColumns":["Proposed Method","learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video .","( +WaveNet vocoder [ 59 ] )","visual modal inputs in a speaker - dependent setting on GRID ."],"numberCells":[{"number":"4.62?","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]","Ground Truth"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.14","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.772","isBolded":false,"associatedRows":["Lip2Wav [ 39 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["Proposed Method"]},{"number":"0.535","isBolded":false,"associatedRows":["Lip2Wav [ 39 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["Proposed Method"]},{"number":"0.565","isBolded":false,"associatedRows":["Proposed Method","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness","STOI"],"associatedMergedColumns":["visual modal inputs in a speaker - dependent setting on GRID ."]},{"number":"1.932","isBolded":false,"associatedRows":["Yadav et al . [ 58 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.14","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]","Ground Truth"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.315","isBolded":true,"associatedRows":["Proposed Method","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility","ESTOI"],"associatedMergedColumns":["visual modal inputs in a speaker - dependent setting on GRID ."]},{"number":"0.23","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]","Vid2Speech [ 13 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.600","isBolded":true,"associatedRows":["Proposed Method","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness","STOI"],"associatedMergedColumns":["visual modal inputs in a speaker - dependent setting on GRID ."]},{"number":"0.21","isBolded":true,"associatedRows":["Vougioukas et al . [ 50 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"2.94?","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.564","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.13","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]","Ground Truth"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.513","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"4.27?","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.332","isBolded":true,"associatedRows":["Proposed Method","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility","PESQ"],"associatedMergedColumns":["visual modal inputs in a speaker - dependent setting on GRID ."]},{"number":"0.19","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.734","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.540","isBolded":false,"associatedRows":["Yadav et al . [ 58 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.738","isBolded":true,"associatedRows":["Proposed Method"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.335","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.491","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"4.37?","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.31?","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]","Vid2Speech [ 13 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.21","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.659","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.361","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.673","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"2.93?","isBolded":true,"associatedRows":["Vougioukas et al . [ 50 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.42?","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]","Vid2Speech [ 13 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"2.83?","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.825","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.279","isBolded":false,"associatedRows":["Proposed Method","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility","ESTOI"],"associatedMergedColumns":["visual modal inputs in a speaker - dependent setting on GRID ."]},{"number":"1.279","isBolded":false,"associatedRows":["Proposed Method","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility","PESQ"],"associatedMergedColumns":["visual modal inputs in a speaker - dependent setting on GRID ."]},{"number":"3.56?","isBolded":true,"associatedRows":["Vougioukas et al . [ 50 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"4.57?","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]","Ground Truth"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.724","isBolded":false,"associatedRows":["Yadav et al . [ 58 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.16","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.684","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.731","isBolded":false,"associatedRows":["Lip2Wav [ 39 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["Proposed Method"]},{"number":"0.24","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]","Vid2Speech [ 13 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.19","isBolded":true,"associatedRows":["Vougioukas et al . [ 50 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.240","isBolded":false,"associatedRows":["Proposed Method","Vougioukas et al . [ 50 ]","-"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility","PESQ"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.376","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.352","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.445","isBolded":false,"associatedRows":["Proposed Method","Vougioukas et al . [ 50 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness","STOI"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.579","isBolded":true,"associatedRows":["Proposed Method"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"1.984","isBolded":true,"associatedRows":["Proposed Method"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]}]},{"caption":"Method \nSTOI \nESTOI \nPESQ \nVougioukas et al. [50] \n0.445 \n-\n1.240 \nLip2Wav [39] \n0.565 \n0.279 \n1.279 \nProposed Method \n0.600 \n0.315 \n1.332 \n\nTable 4. Performance of speech reconstruction comparison with \nvisual modal inputs on the speaker-independent setting on GRID. \n\n","rows":["Proposed Method","Vougioukas et al . [ 50 ]","Yadav et al . [ 58 ]","Lip2Wav [ 39 ]","-","Lip2AudSpec [ 3 ]","Ephrat et al . [ 12 ]","Ground Truth","Vid2Speech [ 13 ]"],"columns":["started [ sta ]","Naturalness","start [ sta ]","ESTOI","PESQ","Intelligibility","set [ set ]","video","STOI"],"mergedAllColumns":["Proposed Method","learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video .","( +WaveNet vocoder [ 59 ] )","visual modal inputs in a speaker - dependent setting on GRID ."],"numberCells":[{"number":"0.445","isBolded":false,"associatedRows":["Proposed Method","Vougioukas et al . [ 50 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness","STOI"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.21","isBolded":true,"associatedRows":["Vougioukas et al . [ 50 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.600","isBolded":true,"associatedRows":["Proposed Method","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness","STOI"],"associatedMergedColumns":["visual modal inputs in a speaker - dependent setting on GRID ."]},{"number":"3.56?","isBolded":true,"associatedRows":["Vougioukas et al . [ 50 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.772","isBolded":false,"associatedRows":["Lip2Wav [ 39 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["Proposed Method"]},{"number":"1.332","isBolded":true,"associatedRows":["Proposed Method","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility","PESQ"],"associatedMergedColumns":["visual modal inputs in a speaker - dependent setting on GRID ."]},{"number":"0.376","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.565","isBolded":false,"associatedRows":["Proposed Method","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness","STOI"],"associatedMergedColumns":["visual modal inputs in a speaker - dependent setting on GRID ."]},{"number":"0.19","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.14","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"2.94?","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.279","isBolded":false,"associatedRows":["Proposed Method","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility","PESQ"],"associatedMergedColumns":["visual modal inputs in a speaker - dependent setting on GRID ."]},{"number":"0.16","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.21","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.731","isBolded":false,"associatedRows":["Lip2Wav [ 39 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["Proposed Method"]},{"number":"0.315","isBolded":true,"associatedRows":["Proposed Method","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility","ESTOI"],"associatedMergedColumns":["visual modal inputs in a speaker - dependent setting on GRID ."]},{"number":"0.24","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]","Vid2Speech [ 13 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"4.57?","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]","Ground Truth"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.352","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.738","isBolded":true,"associatedRows":["Proposed Method"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.361","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.14","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]","Ground Truth"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.279","isBolded":false,"associatedRows":["Proposed Method","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility","ESTOI"],"associatedMergedColumns":["visual modal inputs in a speaker - dependent setting on GRID ."]},{"number":"4.27?","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"4.62?","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]","Ground Truth"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"1.31?","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]","Vid2Speech [ 13 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.984","isBolded":true,"associatedRows":["Proposed Method"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.579","isBolded":true,"associatedRows":["Proposed Method"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.13","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]","Ground Truth"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"1.673","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.535","isBolded":false,"associatedRows":["Lip2Wav [ 39 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["Proposed Method"]},{"number":"2.93?","isBolded":true,"associatedRows":["Vougioukas et al . [ 50 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.724","isBolded":false,"associatedRows":["Yadav et al . [ 58 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"1.734","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.19","isBolded":true,"associatedRows":["Vougioukas et al . [ 50 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.659","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.42?","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]","Vid2Speech [ 13 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.240","isBolded":false,"associatedRows":["Proposed Method","Vougioukas et al . [ 50 ]","-"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility","PESQ"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"0.491","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.564","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.513","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","STOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.23","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]","Vid2Speech [ 13 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Intelligibility"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"1.684","isBolded":false,"associatedRows":["Vougioukas et al . [ 50 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"4.37?","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]","Proposed Method"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.335","isBolded":false,"associatedRows":["Vid2Speech [ 13 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"0.540","isBolded":false,"associatedRows":["Yadav et al . [ 58 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","ESTOI"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"1.932","isBolded":false,"associatedRows":["Yadav et al . [ 58 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["( +WaveNet vocoder [ 59 ] )"]},{"number":"1.825","isBolded":false,"associatedRows":["Ephrat et al . [ 12 ]"],"associatedColumns":["video","started [ sta ]","set [ set ]","PESQ"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]},{"number":"2.83?","isBolded":false,"associatedRows":["Lip2AudSpec [ 3 ]","Lip2Wav [ 39 ]"],"associatedColumns":["video","start [ sta ]","set [ set ]","Naturalness"],"associatedMergedColumns":["learned representations inside memory : ( a ) results from lip reading and ( b ) results from speech reconstruction from silent video ."]}]}]