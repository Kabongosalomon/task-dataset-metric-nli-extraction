[{"caption":"Training \n\nEvaluation \nMethod \nDatasets used \nTotal # hours LRS2 LRS3 \n\nLIBS [73] \nLRS2, LRS3 \n698 \n65.3 \n-\nHyb. CTC/Att. [51] \nLRS2, LRW \n389 \n63.5 \n-\nTDNN [69] \nLRS2 \n224 \n48.9 \n-\nConv-seq2seq [71] \nLRS2, LRS3 \n698 \n51.7 \n60.1 \nCTC + KD [3] \nLRS2, LRS3, VoxCeleb2  ? \n1,032 \n51.3 \n59.8 \nHyb. + Conformer [42] LRS2, LRW \n389 \n37.9 \n-\nHyb. + Conformer [42] LRS3, LRW \n639 \n-\n43.3 \nOurs \nLRS2, LRS3 \n698 \n28.9 \n40.6 \n\nTM-seq2seq [1] \nLRS2, LRS3, LRW, MV-LRS  ? \n1,637 \n48.3 \n58.9 \nCTC-V2P [58] \nLSVSR  ? \n3,886 \n-\n55.1 \nRNN-T [44] \nYT31k  ? \n31,000 \n-\n33.6 \nOurs \nLRS2, LRS3, MV-LRS  ? , TEDx ext \n2,676 \n22.6 \n30.7 \n\nTable 1. Comparison of different lip reading models on the test sets of the LRS2 and LRS3 datasets in terms of Word Error Rate % (WER, \nlower is better), along with the datasets and the aggregate number of hours used for training each model. Our model achieves state-of-\nthe-art results, outperforming all previous baselines when trained on publicly available data (i.e. LRS2 and LRS3). If we additionally \nuse MV-LRS and TEDxext for training, then our best model obtains results comparable with that of [44], even though we are only using \nan order of magnitude less data. This is indicative of the data efficiency of our proposed pipeline.  ? Large non-public labelled datasets: \nMV-LRS [1] contains 730 hours, LSVSR [58] 3.9k hours, and YT31k [44] 31k hours of transcribed video.  ? unlabelled dataset. Results \nshown in blue have been obtained by training (partly or entirely) on non-public data. \n\n","rows":["Conv - seq2seq [ 71 ]","LRS2 , LRW","1 , 032","CTC - V2P [ 58 ]","1 , 637","LSVSR ?","LRS2 , LRS3 , VoxCeleb2 ?","698","YT31k ?","639","TM - seq2seq [ 1 ]","3 , 886","Ours","LRS2","CTC + KD [ 3 ]","MV - LRS [ 1 ] contains 730 hours , LSVSR [ 58 ]","Hyb . CTC / Att . [ 51 ]","LRS2 , LRS3","LRS3 , LRW","-","LRS2 , LRS3 , LRW , MV - LRS ?","389","224","Hyb . + Conformer [ 42 ]","LIBS [ 73 ]","31 , 000","TDNN [ 69 ]","RNN - T [ 44 ]","2 , 676","LRS2 , LRS3 , MV - LRS ? , TEDx ext"],"columns":["Training","?","Evaluation","an order of magnitude less data . This is indicative of the data efficiency of our proposed pipeline .","LRS3","LRS2","Total # hours","the - art results , outperforming all previous baselines when trained on publicly available data ( i . e ."],"mergedAllColumns":["-","use MV - LRS and TEDxext for training , then our best model obtains results comparable with that of [ 44 ] , even though we are only using"],"numberCells":[{"number":"58.9","isBolded":false,"associatedRows":["TM - seq2seq [ 1 ]","LRS2 , LRS3 , LRW , MV - LRS ?","1 , 637","-"],"associatedColumns":["Evaluation","LRS3"],"associatedMergedColumns":["-"]},{"number":"60.1","isBolded":false,"associatedRows":["Conv - seq2seq [ 71 ]","LRS2 , LRS3","698","-"],"associatedColumns":["Evaluation","LRS3"],"associatedMergedColumns":["-"]},{"number":"55.1","isBolded":false,"associatedRows":["CTC - V2P [ 58 ]","LSVSR ?","3 , 886","-"],"associatedColumns":["Evaluation","LRS3"],"associatedMergedColumns":["-"]},{"number":"48.9","isBolded":false,"associatedRows":["TDNN [ 69 ]","LRS2","224"],"associatedColumns":["Evaluation","LRS2"],"associatedMergedColumns":["-"]},{"number":"3.9khours,andYT31k[44]31khoursoftranscribedvideo.","isBolded":false,"associatedRows":["MV - LRS [ 1 ] contains 730 hours , LSVSR [ 58 ]"],"associatedColumns":["Training","Total # hours","the - art results , outperforming all previous baselines when trained on publicly available data ( i . e .","?","an order of magnitude less data . This is indicative of the data efficiency of our proposed pipeline ."],"associatedMergedColumns":["use MV - LRS and TEDxext for training , then our best model obtains results comparable with that of [ 44 ] , even though we are only using"]},{"number":"48.3","isBolded":false,"associatedRows":["TM - seq2seq [ 1 ]","LRS2 , LRS3 , LRW , MV - LRS ?","1 , 637"],"associatedColumns":["Evaluation","LRS2"],"associatedMergedColumns":["-"]},{"number":"59.8","isBolded":false,"associatedRows":["CTC + KD [ 3 ]","LRS2 , LRS3 , VoxCeleb2 ?","1 , 032","-"],"associatedColumns":["Evaluation","LRS3"],"associatedMergedColumns":["-"]},{"number":"30.7","isBolded":true,"associatedRows":["Ours","LRS2 , LRS3 , MV - LRS ? , TEDx ext","2 , 676","-"],"associatedColumns":["Evaluation","LRS3"],"associatedMergedColumns":["-"]},{"number":"33.6","isBolded":false,"associatedRows":["RNN - T [ 44 ]","YT31k ?","31 , 000","-"],"associatedColumns":["Evaluation","LRS3"],"associatedMergedColumns":["-"]},{"number":"22.6","isBolded":true,"associatedRows":["Ours","LRS2 , LRS3 , MV - LRS ? , TEDx ext","2 , 676"],"associatedColumns":["Evaluation","LRS2"],"associatedMergedColumns":["-"]},{"number":"51.3","isBolded":false,"associatedRows":["CTC + KD [ 3 ]","LRS2 , LRS3 , VoxCeleb2 ?","1 , 032"],"associatedColumns":["Evaluation","LRS2"],"associatedMergedColumns":["-"]},{"number":"37.9","isBolded":false,"associatedRows":["Hyb . + Conformer [ 42 ]","LRS2 , LRW","389"],"associatedColumns":["Evaluation","LRS2"],"associatedMergedColumns":["-"]},{"number":"43.3","isBolded":false,"associatedRows":["Hyb . + Conformer [ 42 ]","LRS3 , LRW","639","-"],"associatedColumns":["Evaluation","LRS3"],"associatedMergedColumns":["-"]},{"number":"28.9","isBolded":true,"associatedRows":["Ours","LRS2 , LRS3","698"],"associatedColumns":["Evaluation","LRS2"],"associatedMergedColumns":["-"]},{"number":"65.3","isBolded":false,"associatedRows":["LIBS [ 73 ]","LRS2 , LRS3","698"],"associatedColumns":["Evaluation","LRS2"],"associatedMergedColumns":[]},{"number":"51.7","isBolded":false,"associatedRows":["Conv - seq2seq [ 71 ]","LRS2 , LRS3","698"],"associatedColumns":["Evaluation","LRS2"],"associatedMergedColumns":["-"]},{"number":"63.5","isBolded":false,"associatedRows":["Hyb . CTC / Att . [ 51 ]","LRS2 , LRW","389"],"associatedColumns":["Evaluation","LRS2"],"associatedMergedColumns":["-"]},{"number":"40.6","isBolded":true,"associatedRows":["Ours","LRS2 , LRS3","698","-"],"associatedColumns":["Evaluation","LRS3"],"associatedMergedColumns":["-"]}]},{"caption":"Table 2. Ablation on the design improvements proposed in this \nwork. The results reported are for the test set of the LRS2 dataset. \nIt is clear that all the proposed components contribute indepen-\ndently to the performance boost.  ? The baseline is an improved \nversion of TM-seq2seq ","rows":["0","VTP @ ( H / 8 , W / 8 )","2","3","6","Without VTP","VTP @ ( H / 4 , W / 4 )","VTP @ ( H / 16 , W / 16 )"],"columns":["WER"],"mergedAllColumns":[],"numberCells":[{"number":"35.7","isBolded":false,"associatedRows":["VTP @ ( H / 16 , W / 16 )","2"],"associatedColumns":["WER"],"associatedMergedColumns":[]},{"number":"33.8","isBolded":false,"associatedRows":["VTP @ ( H / 8 , W / 8 )","3"],"associatedColumns":["WER"],"associatedMergedColumns":[]},{"number":"37.2","isBolded":false,"associatedRows":["Without VTP","0"],"associatedColumns":["WER"],"associatedMergedColumns":[]},{"number":"30.9","isBolded":false,"associatedRows":["VTP @ ( H / 4 , W / 4 )","6"],"associatedColumns":["WER"],"associatedMergedColumns":[]}]},{"caption":"Table 3. Ablation on the input spatial resolution for the VTP mod-\nule. The number of Transformer layers for each stage is chosen \nsuch that the total number of parameters in the visual front-end is \napproximately the same. We see that pooling from higher resolu-\ntion feature maps clearly leads to better results. \n\n","rows":["0","VTP @ ( H / 8 , W / 8 )","2","3","6","Without VTP","VTP @ ( H / 4 , W / 4 )","VTP @ ( H / 16 , W / 16 )"],"columns":["WER"],"mergedAllColumns":[],"numberCells":[{"number":"35.7","isBolded":false,"associatedRows":["VTP @ ( H / 16 , W / 16 )","2"],"associatedColumns":["WER"],"associatedMergedColumns":[]},{"number":"33.8","isBolded":false,"associatedRows":["VTP @ ( H / 8 , W / 8 )","3"],"associatedColumns":["WER"],"associatedMergedColumns":[]},{"number":"37.2","isBolded":false,"associatedRows":["Without VTP","0"],"associatedColumns":["WER"],"associatedMergedColumns":[]},{"number":"30.9","isBolded":false,"associatedRows":["VTP @ ( H / 4 , W / 4 )","6"],"associatedColumns":["WER"],"associatedMergedColumns":[]}]},{"caption":"Table 4. Visual Speech Detection performance on the validation \n(val) and test sets of the AVA ActiveSpeaker benchmark dataset. \nThe A and V columns denote which modalities the corresponding \nmethod uses as input. Our VTP model outperforms the baseline \nvideo-only model of ","rows":["MAAS - TAN [ 35 ]","Ours ( CNN + GAP )","Chung et al . [ 13 ]","Alcazar et al . [ 6 ]","TalkNet [ 63 ]","Roth et al . [ 56 ]","Ours ( VTP )"],"columns":["mAP ( val )","We refer to","Moreover , to once again show -","mAP ( test )","model on this task as well , by 8 mAP points ."],"mergedAllColumns":["pipeline , including VSD and lip reading ."],"numberCells":[{"number":"82.1","isBolded":false,"associatedRows":["Roth et al . [ 56 ]"],"associatedColumns":["Moreover , to once again show -","We refer to","mAP ( test )"],"associatedMergedColumns":["pipeline , including VSD and lip reading ."]},{"number":"86.7","isBolded":false,"associatedRows":["Alcazar et al . [ 6 ]"],"associatedColumns":["Moreover , to once again show -","We refer to","mAP ( test )"],"associatedMergedColumns":["pipeline , including VSD and lip reading ."]},{"number":"88.3","isBolded":false,"associatedRows":["MAAS - TAN [ 35 ]"],"associatedColumns":["Moreover , to once again show -","We refer to","mAP ( test )"],"associatedMergedColumns":["pipeline , including VSD and lip reading ."]},{"number":"87.8","isBolded":false,"associatedRows":["Chung et al . [ 13 ]"],"associatedColumns":["Moreover , to once again show -","We refer to","mAP ( test )"],"associatedMergedColumns":["pipeline , including VSD and lip reading ."]},{"number":"89.2","isBolded":true,"associatedRows":["Ours ( VTP )"],"associatedColumns":["Moreover , to once again show -","model on this task as well , by 8 mAP points .","mAP ( val )"],"associatedMergedColumns":["pipeline , including VSD and lip reading ."]},{"number":"88.2","isBolded":true,"associatedRows":["Ours ( VTP )"],"associatedColumns":["Moreover , to once again show -","We refer to","mAP ( test )"],"associatedMergedColumns":["pipeline , including VSD and lip reading ."]},{"number":"92.3","isBolded":true,"associatedRows":["TalkNet [ 63 ]"],"associatedColumns":["Moreover , to once again show -","model on this task as well , by 8 mAP points .","mAP ( val )"],"associatedMergedColumns":["pipeline , including VSD and lip reading ."]},{"number":"81.4","isBolded":false,"associatedRows":["Ours ( CNN + GAP )"],"associatedColumns":["Moreover , to once again show -","model on this task as well , by 8 mAP points .","mAP ( val )"],"associatedMergedColumns":["pipeline , including VSD and lip reading ."]},{"number":"80.2","isBolded":false,"associatedRows":["Ours ( CNN + GAP )"],"associatedColumns":["Moreover , to once again show -","We refer to","mAP ( test )"],"associatedMergedColumns":["pipeline , including VSD and lip reading ."]},{"number":"87.8","isBolded":false,"associatedRows":["Chung et al . [ 13 ]"],"associatedColumns":["Moreover , to once again show -","model on this task as well , by 8 mAP points .","mAP ( val )"],"associatedMergedColumns":["pipeline , including VSD and lip reading ."]},{"number":"90.8","isBolded":true,"associatedRows":["TalkNet [ 63 ]"],"associatedColumns":["Moreover , to once again show -","We refer to","mAP ( test )"],"associatedMergedColumns":["pipeline , including VSD and lip reading ."]},{"number":"79.2","isBolded":false,"associatedRows":["Roth et al . [ 56 ]"],"associatedColumns":["Moreover , to once again show -","model on this task as well , by 8 mAP points .","mAP ( val )"],"associatedMergedColumns":["pipeline , including VSD and lip reading ."]},{"number":"71.1","isBolded":false,"associatedRows":["Roth et al . [ 56 ]"],"associatedColumns":["Moreover , to once again show -","We refer to","mAP ( test )"],"associatedMergedColumns":["pipeline , including VSD and lip reading ."]},{"number":"88.8","isBolded":false,"associatedRows":["MAAS - TAN [ 35 ]"],"associatedColumns":["Moreover , to once again show -","model on this task as well , by 8 mAP points .","mAP ( val )"],"associatedMergedColumns":["pipeline , including VSD and lip reading ."]},{"number":"87.1","isBolded":false,"associatedRows":["Alcazar et al . [ 6 ]"],"associatedColumns":["Moreover , to once again show -","model on this task as well , by 8 mAP points .","mAP ( val )"],"associatedMergedColumns":["pipeline , including VSD and lip reading ."]},{"number":"73.5","isBolded":false,"associatedRows":["Roth et al . [ 56 ]"],"associatedColumns":["Moreover , to once again show -","model on this task as well , by 8 mAP points .","mAP ( val )"],"associatedMergedColumns":["pipeline , including VSD and lip reading ."]}]},{"caption":"Table 5. The grayed out lines denote that \nthose were used in the CNN baseline only, and were re-\nmoved when adding VTP layers after a desired feature map \nresolution. \n\n","rows":["( 1 , 1 )","( 2 , 2 , 2 )","( 2 , 2 )","( 3 , 3 )","conv 3 , 1","conv 3 , 2","T ?","conv 3 , 3","-","input","conv 1 , 1","3","256","128","conv 2 , 2","conv 2 , 3","conv 2 , 1"],"columns":["Stride","Output dims"],"mergedAllColumns":[],"numberCells":[{"number":"24","isBolded":false,"associatedRows":["conv 2 , 1","128","( 3 , 3 )","( 2 , 2 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"12?","isBolded":false,"associatedRows":["conv 3 , 2","256","( 3 , 3 )","( 1 , 1 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"48?","isBolded":false,"associatedRows":["conv 1 , 1","128","( 3 , 3 )","( 2 , 2 )","( 2 , 2 , 2 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"24?","isBolded":false,"associatedRows":["conv 2 , 2","128","( 3 , 3 )","( 1 , 1 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"96","isBolded":false,"associatedRows":["input","128","3","-","-","-","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"12?","isBolded":false,"associatedRows":["conv 3 , 3","256","( 3 , 3 )","( 1 , 1 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"24?","isBolded":false,"associatedRows":["conv 2 , 3","128","( 3 , 3 )","( 1 , 1 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"24?","isBolded":false,"associatedRows":["conv 2 , 1","128","( 3 , 3 )","( 2 , 2 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"96?","isBolded":false,"associatedRows":["input","128","3","-","-","-","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"12?","isBolded":false,"associatedRows":["conv 3 , 1","256","( 3 , 3 )","( 2 , 2 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"24","isBolded":false,"associatedRows":["conv 2 , 2","128","( 3 , 3 )","( 1 , 1 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"12","isBolded":false,"associatedRows":["conv 3 , 3","256","( 3 , 3 )","( 1 , 1 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"48","isBolded":false,"associatedRows":["conv 1 , 1","128","( 3 , 3 )","( 2 , 2 )","( 2 , 2 , 2 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"12","isBolded":false,"associatedRows":["conv 3 , 1","256","( 3 , 3 )","( 2 , 2 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"64(5,5,5)(1,2,2)","isBolded":true,"associatedRows":["conv 1 , 1"],"associatedColumns":["Stride"],"associatedMergedColumns":[]},{"number":"24","isBolded":false,"associatedRows":["conv 2 , 3","128","( 3 , 3 )","( 1 , 1 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"12","isBolded":false,"associatedRows":["conv 3 , 2","256","( 3 , 3 )","( 1 , 1 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]}]},{"caption":"Table 5. Architecture details for the visual CNN backbone. Batch \nNormalization and ReLU activation are added after every convo-\nlutional layer. Shortcut connections are also added at each layer, \nexcept for the first layer of every residual block -i.e. the ones \nwith stride \u003e 1. The layers shown in gray, are only used by the \nTM-seq2seq baseline and not in our best model. \n\n","rows":["( 1 , 1 )","( 2 , 2 , 2 )","( 2 , 2 )","( 3 , 3 )","conv 3 , 1","conv 3 , 2","T ?","conv 3 , 3","-","input","conv 1 , 1","3","256","128","conv 2 , 2","conv 2 , 3","conv 2 , 1"],"columns":["Stride","Output dims"],"mergedAllColumns":[],"numberCells":[{"number":"48?","isBolded":false,"associatedRows":["conv 1 , 1","128","( 3 , 3 )","( 2 , 2 )","( 2 , 2 , 2 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"12","isBolded":false,"associatedRows":["conv 3 , 1","256","( 3 , 3 )","( 2 , 2 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"12?","isBolded":false,"associatedRows":["conv 3 , 3","256","( 3 , 3 )","( 1 , 1 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"12?","isBolded":false,"associatedRows":["conv 3 , 2","256","( 3 , 3 )","( 1 , 1 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"24","isBolded":false,"associatedRows":["conv 2 , 2","128","( 3 , 3 )","( 1 , 1 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"12","isBolded":false,"associatedRows":["conv 3 , 3","256","( 3 , 3 )","( 1 , 1 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"24?","isBolded":false,"associatedRows":["conv 2 , 2","128","( 3 , 3 )","( 1 , 1 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"24?","isBolded":false,"associatedRows":["conv 2 , 1","128","( 3 , 3 )","( 2 , 2 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"24","isBolded":false,"associatedRows":["conv 2 , 1","128","( 3 , 3 )","( 2 , 2 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"64(5,5,5)(1,2,2)","isBolded":true,"associatedRows":["conv 1 , 1"],"associatedColumns":["Stride"],"associatedMergedColumns":[]},{"number":"48","isBolded":false,"associatedRows":["conv 1 , 1","128","( 3 , 3 )","( 2 , 2 )","( 2 , 2 , 2 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"24?","isBolded":false,"associatedRows":["conv 2 , 3","128","( 3 , 3 )","( 1 , 1 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"12?","isBolded":false,"associatedRows":["conv 3 , 1","256","( 3 , 3 )","( 2 , 2 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"24","isBolded":false,"associatedRows":["conv 2 , 3","128","( 3 , 3 )","( 1 , 1 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"96?","isBolded":false,"associatedRows":["input","128","3","-","-","-","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"96","isBolded":false,"associatedRows":["input","128","3","-","-","-","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]},{"number":"12","isBolded":false,"associatedRows":["conv 3 , 2","256","( 3 , 3 )","( 1 , 1 )","( 1 , 1 )","T ?"],"associatedColumns":["Output dims"],"associatedMergedColumns":[]}]}]