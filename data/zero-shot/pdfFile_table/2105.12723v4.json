[{"caption":"Table 1: Test accuracy on CIFAR with input size 32?32. The \ncompared convolutional architectures are optimized models \nfor CIFAR. All transformer-based architectures are trained \nfrom random initialization with the same data augmentation. \nDeiT uses S \u003d 2. Swin and our NesT uses S \u003d 1. means \nmodel tends to diverge. \n\n","rows":["full - attention","WRN28 - 10","Transformer","NesT - T","NesT - S","Swin - B","DeiT - T","PVT - S","DeiT - S","PVT - T","CCT - 7 / 3?1","Swin - S","DeiT - B","NesT - B","Swin - T","Pyramid - 164 - 48","PVT - B"],"columns":["C10 ( % )","C100 ( % )"],"mergedAllColumns":["Convolutional","Transformer","local - attention"],"numberCells":[{"number":"78.69","isBolded":false,"associatedRows":["Transformer","NesT - T"],"associatedColumns":["C100 ( % )"],"associatedMergedColumns":["Transformer"]},{"number":"43.78","isBolded":true,"associatedRows":["full - attention","PVT - B"],"associatedColumns":["C100 ( % )"],"associatedMergedColumns":["Transformer"]},{"number":"81.70","isBolded":false,"associatedRows":["Transformer","NesT - S"],"associatedColumns":["C100 ( % )"],"associatedMergedColumns":["local - attention"]},{"number":"69.62","isBolded":false,"associatedRows":["full - attention","PVT - T"],"associatedColumns":["C100 ( % )"],"associatedMergedColumns":["Transformer"]},{"number":"92.34","isBolded":false,"associatedRows":["full - attention","PVT - S"],"associatedColumns":["C10 ( % )"],"associatedMergedColumns":["Transformer"]},{"number":"82.56","isBolded":true,"associatedRows":["Transformer","NesT - B"],"associatedColumns":["C100 ( % )"],"associatedMergedColumns":["local - attention"]},{"number":"94.46","isBolded":false,"associatedRows":["Transformer","Swin - T"],"associatedColumns":["C10 ( % )"],"associatedMergedColumns":["Transformer"]},{"number":"78.45","isBolded":false,"associatedRows":["Transformer","Swin - B"],"associatedColumns":["C100 ( % )"],"associatedMergedColumns":["Transformer"]},{"number":"69.79","isBolded":false,"associatedRows":["full - attention","PVT - S"],"associatedColumns":["C100 ( % )"],"associatedMergedColumns":["Transformer"]},{"number":"92.44","isBolded":false,"associatedRows":["full - attention","DeiT - S"],"associatedColumns":["C10 ( % )"],"associatedMergedColumns":["Convolutional"]},{"number":"80.70","isBolded":false,"associatedRows":["full - attention","Pyramid - 164 - 48"],"associatedColumns":["C100 ( % )"],"associatedMergedColumns":[]},{"number":"95.83","isBolded":false,"associatedRows":["full - attention","WRN28 - 10"],"associatedColumns":["C10 ( % )"],"associatedMergedColumns":[]},{"number":"70.49","isBolded":false,"associatedRows":["full - attention","DeiT - B"],"associatedColumns":["C100 ( % )"],"associatedMergedColumns":["Convolutional"]},{"number":"96.97","isBolded":false,"associatedRows":["Transformer","NesT - S"],"associatedColumns":["C10 ( % )"],"associatedMergedColumns":["local - attention"]},{"number":"94.17","isBolded":false,"associatedRows":["Transformer","Swin - S"],"associatedColumns":["C10 ( % )"],"associatedMergedColumns":["Transformer"]},{"number":"90.51","isBolded":false,"associatedRows":["full - attention","PVT - T"],"associatedColumns":["C10 ( % )"],"associatedMergedColumns":["Transformer"]},{"number":"97.20","isBolded":true,"associatedRows":["Transformer","NesT - B"],"associatedColumns":["C10 ( % )"],"associatedMergedColumns":["local - attention"]},{"number":"78.07","isBolded":false,"associatedRows":["Transformer","Swin - T"],"associatedColumns":["C100 ( % )"],"associatedMergedColumns":["Transformer"]},{"number":"76.67","isBolded":false,"associatedRows":["full - attention","CCT - 7 / 3?1"],"associatedColumns":["C100 ( % )"],"associatedMergedColumns":["Transformer"]},{"number":"88.39","isBolded":false,"associatedRows":["full - attention","DeiT - T"],"associatedColumns":["C10 ( % )"],"associatedMergedColumns":["Convolutional"]},{"number":"92.41","isBolded":false,"associatedRows":["full - attention","DeiT - B"],"associatedColumns":["C10 ( % )"],"associatedMergedColumns":["Convolutional"]},{"number":"67.52","isBolded":false,"associatedRows":["full - attention","DeiT - T"],"associatedColumns":["C100 ( % )"],"associatedMergedColumns":["Convolutional"]},{"number":"96.04","isBolded":false,"associatedRows":["Transformer","NesT - T"],"associatedColumns":["C10 ( % )"],"associatedMergedColumns":["Transformer"]},{"number":"80.75","isBolded":false,"associatedRows":["full - attention","WRN28 - 10"],"associatedColumns":["C100 ( % )"],"associatedMergedColumns":[]},{"number":"94.72","isBolded":false,"associatedRows":["full - attention","CCT - 7 / 3?1"],"associatedColumns":["C10 ( % )"],"associatedMergedColumns":["Transformer"]},{"number":"69.78","isBolded":false,"associatedRows":["full - attention","DeiT - S"],"associatedColumns":["C100 ( % )"],"associatedMergedColumns":["Convolutional"]},{"number":"85.05","isBolded":true,"associatedRows":["full - attention","PVT - B"],"associatedColumns":["C10 ( % )"],"associatedMergedColumns":["Transformer"]},{"number":"77.01","isBolded":false,"associatedRows":["Transformer","Swin - S"],"associatedColumns":["C100 ( % )"],"associatedMergedColumns":["Transformer"]},{"number":"94.55","isBolded":false,"associatedRows":["Transformer","Swin - B"],"associatedColumns":["C10 ( % )"],"associatedMergedColumns":["Transformer"]},{"number":"95.97","isBolded":false,"associatedRows":["full - attention","Pyramid - 164 - 48"],"associatedColumns":["C10 ( % )"],"associatedMergedColumns":[]}]},{"caption":"Table 2: Comparison on the ImageNet dataset. All models are \ntrained from random initialization. ViT-B/16 uses an image \nsize 384 and others use 224. \n\n","rows":["ResNet - 50","Transformer","NesT - T","NesT - S","38M","Swin - B","68M","22M","88M","86M","84M","50M","10? faster than the best compared result","DeiT - B","NesT - B","ImageNet Acc . ( % )","Convolutional","29M","17M","25M","DeiT - S","21M","ViT - B / 16","RegNetY - 16G","Swin - S","Swin - T","RegNetY - 4G"],"columns":["#Params","ViT - B / 16","the performance of local self - attention .","3 - T ( S \u003d 2 ) leads to","Nest - B","Top - 1 acc . ( % )","Swin - B"],"mergedAllColumns":["full - attention","Dosovitskiy et al . 2021 ) . The pre - training is 90 epoch on","Transformer","NesT again achieves competitive results , with a significantly","on 384?384 ImageNet images . Table 3 compares the results .","local - attention","NesT in Fig . A1 of Appendix outperform compared methods","more straightforward design ."],"numberCells":[{"number":"83.3","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","Swin - B","88M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["full - attention"]},{"number":"94.5%CIFAR10accuracywith5384images/sthroughout,","isBolded":true,"associatedRows":[],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )","Nest - B","3 - T ( S \u003d 2 ) leads to"],"associatedMergedColumns":["NesT in Fig . A1 of Appendix outperform compared methods"]},{"number":"94.6%accuracy.","isBolded":true,"associatedRows":["10? faster than the best compared result"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )","Nest - B","3 - T ( S \u003d 2 ) leads to"],"associatedMergedColumns":["NesT in Fig . A1 of Appendix outperform compared methods"]},{"number":"83.8","isBolded":true,"associatedRows":["Transformer","ImageNet Acc . ( % )","NesT - B","68M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["local - attention"]},{"number":"79.8","isBolded":false,"associatedRows":["Convolutional","ImageNet Acc . ( % )","DeiT - S","22M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["more straightforward design ."]},{"number":"84.0","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","NesT - B"],"associatedColumns":["the performance of local self - attention .","#Params","ViT - B / 16"],"associatedMergedColumns":["local - attention"]},{"number":"86.0","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","NesT - B"],"associatedColumns":["the performance of local self - attention .","#Params","Swin - B"],"associatedMergedColumns":["local - attention"]},{"number":"77.9","isBolded":false,"associatedRows":["Convolutional","ImageNet Acc . ( % )","ViT - B / 16","86M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["more straightforward design ."]},{"number":"86.2","isBolded":true,"associatedRows":["Transformer","ImageNet Acc . ( % )","NesT - B","68M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )","Nest - B"],"associatedMergedColumns":["local - attention"]},{"number":"81.8","isBolded":false,"associatedRows":["Convolutional","ImageNet Acc . ( % )","DeiT - B","86M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["Transformer"]},{"number":"82.9","isBolded":false,"associatedRows":["Convolutional","ImageNet Acc . ( % )","RegNetY - 16G","84M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["NesT again achieves competitive results , with a significantly"]},{"number":"80.0","isBolded":false,"associatedRows":["Convolutional","ImageNet Acc . ( % )","RegNetY - 4G","21M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["on 384?384 ImageNet images . Table 3 compares the results ."]},{"number":"76.2","isBolded":false,"associatedRows":["Convolutional","ImageNet Acc . ( % )","ResNet - 50","25M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["Dosovitskiy et al . 2021 ) . The pre - training is 90 epoch on"]},{"number":"83.0","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","Swin - S","50M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["full - attention"]},{"number":"81.5","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","NesT - T","17M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["full - attention"]},{"number":"81.3","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","Swin - T","29M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["full - attention"]},{"number":"83.3","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","NesT - S","38M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["local - attention"]}]},{"caption":"Table 3: Comparison on ImageNet benchmark with \nImageNet-22K pre-training. \n\n","rows":["ResNet - 50","Transformer","NesT - T","NesT - S","38M","Swin - B","68M","22M","88M","86M","84M","50M","10? faster than the best compared result","DeiT - B","NesT - B","ImageNet Acc . ( % )","Convolutional","29M","17M","25M","DeiT - S","21M","ViT - B / 16","RegNetY - 16G","Swin - S","Swin - T","RegNetY - 4G"],"columns":["#Params","ViT - B / 16","the performance of local self - attention .","3 - T ( S \u003d 2 ) leads to","Nest - B","Top - 1 acc . ( % )","Swin - B"],"mergedAllColumns":["full - attention","Dosovitskiy et al . 2021 ) . The pre - training is 90 epoch on","Transformer","NesT again achieves competitive results , with a significantly","on 384?384 ImageNet images . Table 3 compares the results .","local - attention","NesT in Fig . A1 of Appendix outperform compared methods","more straightforward design ."],"numberCells":[{"number":"79.8","isBolded":false,"associatedRows":["Convolutional","ImageNet Acc . ( % )","DeiT - S","22M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["more straightforward design ."]},{"number":"81.3","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","Swin - T","29M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["full - attention"]},{"number":"83.3","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","Swin - B","88M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["full - attention"]},{"number":"83.3","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","NesT - S","38M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["local - attention"]},{"number":"86.0","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","NesT - B"],"associatedColumns":["the performance of local self - attention .","#Params","Swin - B"],"associatedMergedColumns":["local - attention"]},{"number":"83.8","isBolded":true,"associatedRows":["Transformer","ImageNet Acc . ( % )","NesT - B","68M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["local - attention"]},{"number":"81.5","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","NesT - T","17M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["full - attention"]},{"number":"77.9","isBolded":false,"associatedRows":["Convolutional","ImageNet Acc . ( % )","ViT - B / 16","86M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["more straightforward design ."]},{"number":"86.2","isBolded":true,"associatedRows":["Transformer","ImageNet Acc . ( % )","NesT - B","68M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )","Nest - B"],"associatedMergedColumns":["local - attention"]},{"number":"94.5%CIFAR10accuracywith5384images/sthroughout,","isBolded":true,"associatedRows":[],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )","Nest - B","3 - T ( S \u003d 2 ) leads to"],"associatedMergedColumns":["NesT in Fig . A1 of Appendix outperform compared methods"]},{"number":"83.0","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","Swin - S","50M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["full - attention"]},{"number":"84.0","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","NesT - B"],"associatedColumns":["the performance of local self - attention .","#Params","ViT - B / 16"],"associatedMergedColumns":["local - attention"]},{"number":"76.2","isBolded":false,"associatedRows":["Convolutional","ImageNet Acc . ( % )","ResNet - 50","25M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["Dosovitskiy et al . 2021 ) . The pre - training is 90 epoch on"]},{"number":"80.0","isBolded":false,"associatedRows":["Convolutional","ImageNet Acc . ( % )","RegNetY - 4G","21M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["on 384?384 ImageNet images . Table 3 compares the results ."]},{"number":"81.8","isBolded":false,"associatedRows":["Convolutional","ImageNet Acc . ( % )","DeiT - B","86M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["Transformer"]},{"number":"94.6%accuracy.","isBolded":true,"associatedRows":["10? faster than the best compared result"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )","Nest - B","3 - T ( S \u003d 2 ) leads to"],"associatedMergedColumns":["NesT in Fig . A1 of Appendix outperform compared methods"]},{"number":"82.9","isBolded":false,"associatedRows":["Convolutional","ImageNet Acc . ( % )","RegNetY - 16G","84M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["NesT again achieves competitive results , with a significantly"]}]},{"caption":"ViT-B/16 Swin-B Nest-B \n\nImageNet Acc. (%) \n84.0 \n86.0 \n86.2 \n\nbigger model size. PVT (Wang et al. 2021b) has also a full \nself-attention based design, though with a pyramid structure. \nPVT-T seems to perform better than DeiT-T when model \nsize is small, however, the performance largely drops and \nbecomes unstable when scaling up, further suggesting that \nfull self-attention at bottom layers is not desirable for data \nefficiency. Other transformer-based methods improve slowly \nwith increasing model size, suggesting that bigger models \nare more challenging to train with less data. We attribute this \nto to their complex design (i.e. shifted windows with masked \nMSA) requiring larger training datasets, while NesT ben-\nefiting from a judiciously-designed block aggregation. We \nalso include comparisons with convolutional architectures \nthat are specifically optimized for small CIFAR images and \nshow that NesT can give better accuracy without any small \ndataset specific architecture optimizations (while still being \nlarger and slower, as they do not incorporate convolutional \ninductive biases). The learning capacity and performance of \nNesT get better with increased model size. Most variants of \nNesT in Fig. A1 of Appendix outperform compared methods \nwith far better throughput. E.g., NesT 3 -T (S \u003d 2) leads to \n94.5% CIFAR10 accuracy with 5384 images/s throughout, \n10? faster than the best compared result 94.6% accuracy. \nMore details can be found in Appendix. \nImageNet. We test NesT on standard ImageNet 2012 bench-\nmarks (Deng et al. 2009) with commonly used 300 epoch \ntraining on TPUs in Table 2. The input size is 224 ? 224 \nand no extra pre-training data is used. DeiT does not use \nteacher distillation, so it can be viewed as ViT (Dosovitskiy  et al. 2021) with better data augmentation and regularization. \nNesT matches the performance of prior work with a signif-\nicantly more straightforward design (e.g. NesT-S matches \nthe accuracy of Swin-B, 83.3%). The results of NesT suggest \nthat correctly aggregating the local transformer can improve \n\nthe performance of local self-attention. \nImageNet-22K. We scale up NesT to ImageNet-22K fol-\nlowing the exact training schedules in (Liu et al. 2021;  Dosovitskiy et al. 2021). The pre-training is 90 epoch on \n224?224 ImageNet21K images and finetuning is 30 epoch \non 384?384 ImageNet images. Table 3 compares the results. \nNesT again achieves competitive results, with a significantly \nmore straightforward design. \n\n","rows":["ResNet - 50","Transformer","NesT - T","NesT - S","38M","Swin - B","68M","22M","88M","86M","84M","50M","10? faster than the best compared result","DeiT - B","NesT - B","ImageNet Acc . ( % )","Convolutional","29M","17M","25M","DeiT - S","21M","ViT - B / 16","RegNetY - 16G","Swin - S","Swin - T","RegNetY - 4G"],"columns":["#Params","ViT - B / 16","the performance of local self - attention .","3 - T ( S \u003d 2 ) leads to","Nest - B","Top - 1 acc . ( % )","Swin - B"],"mergedAllColumns":["full - attention","Dosovitskiy et al . 2021 ) . The pre - training is 90 epoch on","Transformer","NesT again achieves competitive results , with a significantly","on 384?384 ImageNet images . Table 3 compares the results .","local - attention","NesT in Fig . A1 of Appendix outperform compared methods","more straightforward design ."],"numberCells":[{"number":"83.0","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","Swin - S","50M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["full - attention"]},{"number":"76.2","isBolded":false,"associatedRows":["Convolutional","ImageNet Acc . ( % )","ResNet - 50","25M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["Dosovitskiy et al . 2021 ) . The pre - training is 90 epoch on"]},{"number":"94.5%CIFAR10accuracywith5384images/sthroughout,","isBolded":true,"associatedRows":[],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )","Nest - B","3 - T ( S \u003d 2 ) leads to"],"associatedMergedColumns":["NesT in Fig . A1 of Appendix outperform compared methods"]},{"number":"80.0","isBolded":false,"associatedRows":["Convolutional","ImageNet Acc . ( % )","RegNetY - 4G","21M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["on 384?384 ImageNet images . Table 3 compares the results ."]},{"number":"86.2","isBolded":true,"associatedRows":["Transformer","ImageNet Acc . ( % )","NesT - B","68M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )","Nest - B"],"associatedMergedColumns":["local - attention"]},{"number":"81.5","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","NesT - T","17M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["full - attention"]},{"number":"82.9","isBolded":false,"associatedRows":["Convolutional","ImageNet Acc . ( % )","RegNetY - 16G","84M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["NesT again achieves competitive results , with a significantly"]},{"number":"79.8","isBolded":false,"associatedRows":["Convolutional","ImageNet Acc . ( % )","DeiT - S","22M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["more straightforward design ."]},{"number":"83.3","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","NesT - S","38M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["local - attention"]},{"number":"77.9","isBolded":false,"associatedRows":["Convolutional","ImageNet Acc . ( % )","ViT - B / 16","86M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["more straightforward design ."]},{"number":"83.3","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","Swin - B","88M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["full - attention"]},{"number":"83.8","isBolded":true,"associatedRows":["Transformer","ImageNet Acc . ( % )","NesT - B","68M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["local - attention"]},{"number":"94.6%accuracy.","isBolded":true,"associatedRows":["10? faster than the best compared result"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )","Nest - B","3 - T ( S \u003d 2 ) leads to"],"associatedMergedColumns":["NesT in Fig . A1 of Appendix outperform compared methods"]},{"number":"86.0","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","NesT - B"],"associatedColumns":["the performance of local self - attention .","#Params","Swin - B"],"associatedMergedColumns":["local - attention"]},{"number":"84.0","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","NesT - B"],"associatedColumns":["the performance of local self - attention .","#Params","ViT - B / 16"],"associatedMergedColumns":["local - attention"]},{"number":"81.8","isBolded":false,"associatedRows":["Convolutional","ImageNet Acc . ( % )","DeiT - B","86M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["Transformer"]},{"number":"81.3","isBolded":false,"associatedRows":["Transformer","ImageNet Acc . ( % )","Swin - T","29M"],"associatedColumns":["the performance of local self - attention .","Top - 1 acc . ( % )"],"associatedMergedColumns":["full - attention"]}]}]