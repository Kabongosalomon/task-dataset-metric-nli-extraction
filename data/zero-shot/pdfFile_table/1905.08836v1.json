[{"caption":"Model \nR1 \nR2 \nRL \n\nOther Abs. Sum. models * \n\nCelikyilmaz et al. (2018) \n41.69 19.47 37.92 \nCopyTransformer (4-layer) \n39.25 17.54 36.45 \nGehrmann et al. (2018) \nGPT-2 (48-layer, zero-shot) \n29.34 8.27 26.58 \nRadford et al. (2019) \n\nNo Pre-training \n\nBidirEncoder-Decoder (4-layer) 37.74 16.27 34.76 \nEncoder-Decoder (12-layer) \n36.72 15.22 33.84 \nTransformer LM (12-layer) \n37.72 16.14 34.62 \n\nWith Pre-training (all 12-layer) \n\nPre-train Encoder only \n36.05 15.48 33.48 \nPre-train Decoder only \n27.48 6.87 25.40 \nEncoder-Decoder \n39.18 17.00 36.33 \nTransformer LM \n39.65 17.74 36.85 \n\nTable 1: Summarization results when using the full \ntraining set. Our scores are averaged over three models \ntrained with different random seeds. * Other abstractive \nsummarization model scores are provided to contextu-\nalize performance on this task but are not directly com-\nparable to our models. \n\n","rows":["Celikyilmaz et al . ( 2018 )","BidirEncoder - Decoder ( 4 - layer )","Encoder - Decoder ( 12 - layer )","Encoder - Decoder","Transformer LM","GPT - 2 ( 48 - layer , zero - shot )","Pre - train Decoder only","CopyTransformer ( 4 - layer )","Transformer LM ( 12 - layer )","Pre - train Encoder only"],"columns":["R2","RL","R1"],"mergedAllColumns":["No Pre - training","With Pre - training ( all 12 - layer )","Other Abs . Sum . models","Gehrmann et al . ( 2018 )"],"numberCells":[{"number":"41.69","isBolded":false,"associatedRows":["Celikyilmaz et al . ( 2018 )"],"associatedColumns":["R1"],"associatedMergedColumns":["Other Abs . Sum . models"]},{"number":"34.62","isBolded":false,"associatedRows":["Transformer LM ( 12 - layer )"],"associatedColumns":["RL"],"associatedMergedColumns":["No Pre - training"]},{"number":"36.33","isBolded":false,"associatedRows":["Encoder - Decoder"],"associatedColumns":["RL"],"associatedMergedColumns":["With Pre - training ( all 12 - layer )"]},{"number":"17.54","isBolded":false,"associatedRows":["CopyTransformer ( 4 - layer )"],"associatedColumns":["R2"],"associatedMergedColumns":["Other Abs . Sum . models"]},{"number":"6.87","isBolded":false,"associatedRows":["Pre - train Decoder only"],"associatedColumns":["R2"],"associatedMergedColumns":["With Pre - training ( all 12 - layer )"]},{"number":"37.74","isBolded":false,"associatedRows":["BidirEncoder - Decoder ( 4 - layer )"],"associatedColumns":["R1"],"associatedMergedColumns":["No Pre - training"]},{"number":"36.05","isBolded":false,"associatedRows":["Pre - train Encoder only"],"associatedColumns":["R1"],"associatedMergedColumns":["With Pre - training ( all 12 - layer )"]},{"number":"17.00","isBolded":false,"associatedRows":["Encoder - Decoder"],"associatedColumns":["R2"],"associatedMergedColumns":["With Pre - training ( all 12 - layer )"]},{"number":"8.27","isBolded":false,"associatedRows":["GPT - 2 ( 48 - layer , zero - shot )"],"associatedColumns":["R2"],"associatedMergedColumns":["Gehrmann et al . ( 2018 )"]},{"number":"36.72","isBolded":false,"associatedRows":["Encoder - Decoder ( 12 - layer )"],"associatedColumns":["R1"],"associatedMergedColumns":["No Pre - training"]},{"number":"34.76","isBolded":false,"associatedRows":["BidirEncoder - Decoder ( 4 - layer )"],"associatedColumns":["RL"],"associatedMergedColumns":["No Pre - training"]},{"number":"37.72","isBolded":false,"associatedRows":["Transformer LM ( 12 - layer )"],"associatedColumns":["R1"],"associatedMergedColumns":["No Pre - training"]},{"number":"15.48","isBolded":false,"associatedRows":["Pre - train Encoder only"],"associatedColumns":["R2"],"associatedMergedColumns":["With Pre - training ( all 12 - layer )"]},{"number":"16.27","isBolded":false,"associatedRows":["BidirEncoder - Decoder ( 4 - layer )"],"associatedColumns":["R2"],"associatedMergedColumns":["No Pre - training"]},{"number":"25.40","isBolded":false,"associatedRows":["Pre - train Decoder only"],"associatedColumns":["RL"],"associatedMergedColumns":["With Pre - training ( all 12 - layer )"]},{"number":"39.25","isBolded":false,"associatedRows":["CopyTransformer ( 4 - layer )"],"associatedColumns":["R1"],"associatedMergedColumns":["Other Abs . Sum . models"]},{"number":"33.84","isBolded":false,"associatedRows":["Encoder - Decoder ( 12 - layer )"],"associatedColumns":["RL"],"associatedMergedColumns":["No Pre - training"]},{"number":"39.18","isBolded":false,"associatedRows":["Encoder - Decoder"],"associatedColumns":["R1"],"associatedMergedColumns":["With Pre - training ( all 12 - layer )"]},{"number":"19.47","isBolded":false,"associatedRows":["Celikyilmaz et al . ( 2018 )"],"associatedColumns":["R2"],"associatedMergedColumns":["Other Abs . Sum . models"]},{"number":"37.92","isBolded":false,"associatedRows":["Celikyilmaz et al . ( 2018 )"],"associatedColumns":["RL"],"associatedMergedColumns":["Other Abs . Sum . models"]},{"number":"26.58","isBolded":false,"associatedRows":["GPT - 2 ( 48 - layer , zero - shot )"],"associatedColumns":["RL"],"associatedMergedColumns":["Gehrmann et al . ( 2018 )"]},{"number":"29.34","isBolded":false,"associatedRows":["GPT - 2 ( 48 - layer , zero - shot )"],"associatedColumns":["R1"],"associatedMergedColumns":["Gehrmann et al . ( 2018 )"]},{"number":"15.22","isBolded":false,"associatedRows":["Encoder - Decoder ( 12 - layer )"],"associatedColumns":["R2"],"associatedMergedColumns":["No Pre - training"]},{"number":"16.14","isBolded":false,"associatedRows":["Transformer LM ( 12 - layer )"],"associatedColumns":["R2"],"associatedMergedColumns":["No Pre - training"]},{"number":"39.65","isBolded":false,"associatedRows":["Transformer LM"],"associatedColumns":["R1"],"associatedMergedColumns":["With Pre - training ( all 12 - layer )"]},{"number":"17.74","isBolded":false,"associatedRows":["Transformer LM"],"associatedColumns":["R2"],"associatedMergedColumns":["With Pre - training ( all 12 - layer )"]},{"number":"36.45","isBolded":false,"associatedRows":["CopyTransformer ( 4 - layer )"],"associatedColumns":["RL"],"associatedMergedColumns":["Other Abs . Sum . models"]},{"number":"36.85","isBolded":false,"associatedRows":["Transformer LM"],"associatedColumns":["RL"],"associatedMergedColumns":["With Pre - training ( all 12 - layer )"]},{"number":"33.48","isBolded":false,"associatedRows":["Pre - train Encoder only"],"associatedColumns":["RL"],"associatedMergedColumns":["With Pre - training ( all 12 - layer )"]},{"number":"27.48","isBolded":false,"associatedRows":["Pre - train Decoder only"],"associatedColumns":["R1"],"associatedMergedColumns":["With Pre - training ( all 12 - layer )"]}]}]