[{"caption":"Table 1: Accuracies of the models on SNLI. Our final model achieves the accuracy of 88.6%, the best \nresult observed on SNLI, while our enhanced sequential encoding model attains an accuracy of 88.0%, \nwhich also outperform the previous models. \n\n","rows":["580K","250K",")","( 9 ) 300D mLSTM ( Wang and Jiang , 2016 )","( 12 ) Intra - sentence attention + ( 11 ) ( Parikh et al . , 2016 )","( 6 ) 600D BiLSTM intra - attention encoders ( Liu et al . , 2016 )","( 13 ) 300D NTI - SLSTM - LSTM ( Munkhdalai and Yu , 2016b )","( 17 ) HIM ( 600D ESIM + 300D Syntactic tree - LSTM )","( 8 ) 100D LSTM with attention ( Rockt?schel et al . , 2015 )","380K","( 11 ) 200D decomposable attention model ( Parikh et al . , 2016 )","( 16 ) 600D ESIM","( 10 ) 450D LSTMN with deep attention fusion ( Cheng et al . , 2016 )","( 14 ) 300D re - read LSTM ( Sha et al . , 2016 )","( 15 ) 300D btree - LSTM encoders ( Paria et al . , 2016 )"],"columns":[],"mergedAllColumns":[],"numberCells":[{"number":"2.8M","isBolded":false,"associatedRows":["( 6 ) 600D BiLSTM intra - attention encoders ( Liu et al . , 2016 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"90.5","isBolded":false,"associatedRows":["( 12 ) Intra - sentence attention + ( 11 ) ( Parikh et al . , 2016 )","580K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"2.0M","isBolded":false,"associatedRows":["( 15 ) 300D btree - LSTM encoders ( Paria et al . , 2016 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"92.6","isBolded":false,"associatedRows":["( 16 ) 600D ESIM","580K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"88.5","isBolded":false,"associatedRows":["( 13 ) 300D NTI - SLSTM - LSTM ( Munkhdalai and Yu , 2016b )","580K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"7.7M","isBolded":false,"associatedRows":["( 17 ) HIM ( 600D ESIM + 300D Syntactic tree - LSTM )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"86.3","isBolded":false,"associatedRows":["( 11 ) 200D decomposable attention model ( Parikh et al . , 2016 )","380K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"87.3","isBolded":false,"associatedRows":["( 13 ) 300D NTI - SLSTM - LSTM ( Munkhdalai and Yu , 2016b )","580K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"84.2","isBolded":false,"associatedRows":["( 6 ) 600D BiLSTM intra - attention encoders ( Liu et al . , 2016 )","250K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"89.5","isBolded":false,"associatedRows":["( 11 ) 200D decomposable attention model ( Parikh et al . , 2016 )","380K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"84.5","isBolded":false,"associatedRows":["( 6 ) 600D BiLSTM intra - attention encoders ( Liu et al . , 2016 )","250K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"86.3","isBolded":false,"associatedRows":["( 10 ) 450D LSTMN with deep attention fusion ( Cheng et al . , 2016 )","380K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"86.2","isBolded":false,"associatedRows":[")","250K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"84.6","isBolded":false,"associatedRows":[")","250K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"88.6","isBolded":false,"associatedRows":["( 15 ) 300D btree - LSTM encoders ( Paria et al . , 2016 )","580K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"85.3","isBolded":false,"associatedRows":["( 8 ) 100D LSTM with attention ( Rockt?schel et al . , 2015 )","250K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"3.4M","isBolded":false,"associatedRows":["( 10 ) 450D LSTMN with deep attention fusion ( Cheng et al . , 2016 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"87.5","isBolded":false,"associatedRows":["( 14 ) 300D re - read LSTM ( Sha et al . , 2016 )","580K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"87.6","isBolded":false,"associatedRows":["( 15 ) 300D btree - LSTM encoders ( Paria et al . , 2016 )","580K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"1.9M","isBolded":false,"associatedRows":["( 9 ) 300D mLSTM ( Wang and Jiang , 2016 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"86.1","isBolded":false,"associatedRows":["( 9 ) 300D mLSTM ( Wang and Jiang , 2016 )","250K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"88.6","isBolded":true,"associatedRows":["( 17 ) HIM ( 600D ESIM + 300D Syntactic tree - LSTM )","580K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"88.5","isBolded":false,"associatedRows":["( 10 ) 450D LSTMN with deep attention fusion ( Cheng et al . , 2016 )","380K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"3.0M","isBolded":false,"associatedRows":[")"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"93.5","isBolded":false,"associatedRows":["( 17 ) HIM ( 600D ESIM + 300D Syntactic tree - LSTM )","580K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"92.0","isBolded":false,"associatedRows":["( 9 ) 300D mLSTM ( Wang and Jiang , 2016 )","250K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"90.7","isBolded":false,"associatedRows":["( 14 ) 300D re - read LSTM ( Sha et al . , 2016 )","580K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"86.8","isBolded":false,"associatedRows":["( 12 ) Intra - sentence attention + ( 11 ) ( Parikh et al . , 2016 )","580K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"83.5","isBolded":false,"associatedRows":["( 8 ) 100D LSTM with attention ( Rockt?schel et al . , 2015 )","250K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"4.3M","isBolded":false,"associatedRows":["( 16 ) 600D ESIM"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"3.2M","isBolded":false,"associatedRows":["( 13 ) 300D NTI - SLSTM - LSTM ( Munkhdalai and Yu , 2016b )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"88.0","isBolded":false,"associatedRows":["( 16 ) 600D ESIM","580K"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"2.0M","isBolded":false,"associatedRows":["( 14 ) 300D re - read LSTM ( Sha et al . , 2016 )"],"associatedColumns":[],"associatedMergedColumns":[]}]},{"caption":"Table 2: Ablation performance of the models. \n\n","rows":["( 20 ) ESIM - diff . / prod .","( 24 ) ESIM - H - based attention","HIM ( ESIM + syn . tree )","( 23 ) ESIM - P - based attention","( 21 ) ESIM - inference BiLSTM","( 19 ) ESIM - ave . / max","( 18 ) ESIM + tree","( 22 ) ESIM - encoding BiLSTM","( 16 ) ESIM"],"columns":["Test","Train"],"mergedAllColumns":[],"numberCells":[{"number":"91.4","isBolded":false,"associatedRows":["( 24 ) ESIM - H - based attention"],"associatedColumns":["Train"],"associatedMergedColumns":[]},{"number":"91.5","isBolded":false,"associatedRows":["( 20 ) ESIM - diff . / prod ."],"associatedColumns":["Train"],"associatedMergedColumns":[]},{"number":"91.6","isBolded":false,"associatedRows":["( 23 ) ESIM - P - based attention"],"associatedColumns":["Train"],"associatedMergedColumns":[]},{"number":"87.3","isBolded":false,"associatedRows":["( 21 ) ESIM - inference BiLSTM"],"associatedColumns":["Test"],"associatedMergedColumns":[]},{"number":"87.8","isBolded":false,"associatedRows":["( 24 ) ESIM - H - based attention"],"associatedColumns":["Test"],"associatedMergedColumns":[]},{"number":"88.6","isBolded":false,"associatedRows":["( 21 ) ESIM - inference BiLSTM","HIM ( ESIM + syn . tree )"],"associatedColumns":["Test"],"associatedMergedColumns":[]},{"number":"88.2","isBolded":false,"associatedRows":["( 18 ) ESIM + tree"],"associatedColumns":["Test"],"associatedMergedColumns":[]},{"number":"92.9","isBolded":false,"associatedRows":["( 19 ) ESIM - ave . / max"],"associatedColumns":["Train"],"associatedMergedColumns":[]},{"number":"86.5","isBolded":false,"associatedRows":["( 24 ) ESIM - H - based attention"],"associatedColumns":["Test"],"associatedMergedColumns":[]},{"number":"93.5","isBolded":false,"associatedRows":["( 21 ) ESIM - inference BiLSTM","HIM ( ESIM + syn . tree )"],"associatedColumns":["Train"],"associatedMergedColumns":[]},{"number":"92.6","isBolded":false,"associatedRows":["( 16 ) ESIM","HIM ( ESIM + syn . tree )"],"associatedColumns":["Train"],"associatedMergedColumns":[]},{"number":"91.9","isBolded":false,"associatedRows":["( 18 ) ESIM + tree"],"associatedColumns":["Train"],"associatedMergedColumns":[]},{"number":"88.7","isBolded":false,"associatedRows":["( 22 ) ESIM - encoding BiLSTM"],"associatedColumns":["Train"],"associatedMergedColumns":[]},{"number":"87.2","isBolded":false,"associatedRows":["( 23 ) ESIM - P - based attention"],"associatedColumns":["Test"],"associatedMergedColumns":[]},{"number":"88.0","isBolded":false,"associatedRows":["( 21 ) ESIM - inference BiLSTM"],"associatedColumns":["Test"],"associatedMergedColumns":[]},{"number":"86.3","isBolded":false,"associatedRows":["( 22 ) ESIM - encoding BiLSTM"],"associatedColumns":["Test"],"associatedMergedColumns":[]},{"number":"87.0","isBolded":false,"associatedRows":["( 20 ) ESIM - diff . / prod ."],"associatedColumns":["Test"],"associatedMergedColumns":[]},{"number":"92.9","isBolded":false,"associatedRows":["( 24 ) ESIM - H - based attention"],"associatedColumns":["Train"],"associatedMergedColumns":[]},{"number":"91.3","isBolded":false,"associatedRows":["( 21 ) ESIM - inference BiLSTM"],"associatedColumns":["Train"],"associatedMergedColumns":[]},{"number":"87.1","isBolded":false,"associatedRows":["( 19 ) ESIM - ave . / max"],"associatedColumns":["Test"],"associatedMergedColumns":[]}]}]