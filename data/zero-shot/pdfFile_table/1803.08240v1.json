[{"caption":"Penn Treebank (Character) \nenwik8 \nWikiText-103 \nTrain Valid \nTest \nTrain Valid Test \nTrain \nValid Test \n\nTokens \n5.01M 393k \n442k \n90M \n5M \n5M 103.2M 217k 245k \n\nVocab size \n51 \n205 \n267,735 \nOoV rate \n-\n-\n0.4% \n\nTable 1. Statistics of the character-level Penn Treebank, character-level enwik8 dataset, and WikiText-103. The out of vocabulary (OoV) \nrate notes what percentage of tokens have been replaced by an unk token, not applicable to character-level datasets. \n\n","rows":["442k","OoV rate","393k","90M","5M","-","Tokens"],"columns":["267 , 735","217k","Valid","WikiText - 103","Penn Treebank ( Character )","Train"],"mergedAllColumns":[],"numberCells":[{"number":"0.4%","isBolded":false,"associatedRows":["OoV rate","-","-"],"associatedColumns":["WikiText - 103","Valid","217k","267 , 735"],"associatedMergedColumns":[]},{"number":"5.01M","isBolded":false,"associatedRows":["Tokens"],"associatedColumns":["Penn Treebank ( Character )","Train"],"associatedMergedColumns":[]},{"number":"103.2M","isBolded":false,"associatedRows":["Tokens","393k","442k","90M","5M","5M"],"associatedColumns":["WikiText - 103","Train"],"associatedMergedColumns":[]}]},{"caption":"Model \nBPC Params \n\nZoneout LSTM (Krueger et al., 2016) \n1.27 \n-\n2-Layers LSTM (Mujika et al., 2017) 1.243 \n6.6M \nHM-LSTM (Chung et al., 2016) \n1.24 \n-\nHyperLSTM -small (Ha et al., 2016) 1.233 \n5.1M \nHyperLSTM (Ha et al., 2016) \n1.219 14.4M \nNASCell (small) (Zoph \u0026 Le, 2016) \n1.228 \n6.6M \nNASCell (Zoph \u0026 Le, 2016) \n1.214 16.3M \nFS-LSTM-2 (Mujika et al., 2017) \n1.190 \n7.2M \nFS-LSTM-4 (Mujika et al., 2017) \n1.193 \n6.5M \n\n6 layer QRNN (h \u003d 1024) (ours) \n1.187 13.8M \n3 layer LSTM (h \u003d 1000) (ours) \n1.175 13.8M \n\nTable 2. Bits Per Character (BPC) on character-level Penn Tree-\nbank. \n\n","rows":["FS - LSTM - 2 ( Mujika et al . , 2017 )","FS - LSTM - 4 ( Mujika et al . , 2017 )","6 layer QRNN ( h \u003d 1024 ) ( ours )","Zoneout LSTM ( Krueger et al . , 2016 )","2 - Layers LSTM ( Mujika et al . , 2017 )","HyperLSTM ( Ha et al . , 2016 )","NASCell ( Zoph \u0026 Le , 2016 )","HM - LSTM ( Chung et al . , 2016 )","HyperLSTM - small ( Ha et al . , 2016 )","NASCell ( small ) ( Zoph \u0026 Le , 2016 )","3 layer LSTM ( h \u003d 1000 ) ( ours )"],"columns":["BPC","Params"],"mergedAllColumns":["-"],"numberCells":[{"number":"1.228","isBolded":false,"associatedRows":["NASCell ( small ) ( Zoph \u0026 Le , 2016 )"],"associatedColumns":["BPC"],"associatedMergedColumns":["-"]},{"number":"1.190","isBolded":false,"associatedRows":["FS - LSTM - 2 ( Mujika et al . , 2017 )"],"associatedColumns":["BPC"],"associatedMergedColumns":["-"]},{"number":"1.187","isBolded":false,"associatedRows":["6 layer QRNN ( h \u003d 1024 ) ( ours )"],"associatedColumns":["BPC"],"associatedMergedColumns":["-"]},{"number":"1.233","isBolded":false,"associatedRows":["HyperLSTM - small ( Ha et al . , 2016 )"],"associatedColumns":["BPC"],"associatedMergedColumns":["-"]},{"number":"6.5M","isBolded":false,"associatedRows":["FS - LSTM - 4 ( Mujika et al . , 2017 )"],"associatedColumns":["Params"],"associatedMergedColumns":["-"]},{"number":"16.3M","isBolded":false,"associatedRows":["NASCell ( Zoph \u0026 Le , 2016 )"],"associatedColumns":["Params"],"associatedMergedColumns":["-"]},{"number":"6.6M","isBolded":false,"associatedRows":["NASCell ( small ) ( Zoph \u0026 Le , 2016 )"],"associatedColumns":["Params"],"associatedMergedColumns":["-"]},{"number":"14.4M","isBolded":false,"associatedRows":["HyperLSTM ( Ha et al . , 2016 )"],"associatedColumns":["Params"],"associatedMergedColumns":["-"]},{"number":"1.214","isBolded":false,"associatedRows":["NASCell ( Zoph \u0026 Le , 2016 )"],"associatedColumns":["BPC"],"associatedMergedColumns":["-"]},{"number":"1.24","isBolded":false,"associatedRows":["HM - LSTM ( Chung et al . , 2016 )"],"associatedColumns":["BPC"],"associatedMergedColumns":["-"]},{"number":"1.219","isBolded":false,"associatedRows":["HyperLSTM ( Ha et al . , 2016 )"],"associatedColumns":["BPC"],"associatedMergedColumns":["-"]},{"number":"1.175","isBolded":false,"associatedRows":["3 layer LSTM ( h \u003d 1000 ) ( ours )"],"associatedColumns":["BPC"],"associatedMergedColumns":["-"]},{"number":"1.193","isBolded":false,"associatedRows":["FS - LSTM - 4 ( Mujika et al . , 2017 )"],"associatedColumns":["BPC"],"associatedMergedColumns":["-"]},{"number":"7.2M","isBolded":false,"associatedRows":["FS - LSTM - 2 ( Mujika et al . , 2017 )"],"associatedColumns":["Params"],"associatedMergedColumns":["-"]},{"number":"13.8M","isBolded":false,"associatedRows":["3 layer LSTM ( h \u003d 1000 ) ( ours )"],"associatedColumns":["Params"],"associatedMergedColumns":["-"]},{"number":"6.6M","isBolded":false,"associatedRows":["2 - Layers LSTM ( Mujika et al . , 2017 )"],"associatedColumns":["Params"],"associatedMergedColumns":["-"]},{"number":"13.8M","isBolded":false,"associatedRows":["6 layer QRNN ( h \u003d 1024 ) ( ours )"],"associatedColumns":["Params"],"associatedMergedColumns":["-"]},{"number":"1.27","isBolded":false,"associatedRows":["Zoneout LSTM ( Krueger et al . , 2016 )"],"associatedColumns":["BPC"],"associatedMergedColumns":[]},{"number":"1.243","isBolded":false,"associatedRows":["2 - Layers LSTM ( Mujika et al . , 2017 )"],"associatedColumns":["BPC"],"associatedMergedColumns":["-"]},{"number":"5.1M","isBolded":false,"associatedRows":["HyperLSTM - small ( Ha et al . , 2016 )"],"associatedColumns":["Params"],"associatedMergedColumns":["-"]}]},{"caption":"Model \nBPC \nParams \n\nLSTM, 2000 units (Mujika et al., 2017) 1.461 \n18M \nLayer Norm LSTM, 1800 units \n1.402 \n14M \nHyperLSTM (Ha et al., 2016)  1.340 \n27M \nHM-LSTM (Chung et al., 2016)  1.32 \n35M \nSD Zoneout (Rocki et al., 2016)  1.31 \n64M \nRHN -depth 5 (Zilly et al., 2016)  1.31 \n23M \nRHN -depth 10 (Zilly et al., 2016)  1.30 \n21M \nLarge RHN (Zilly et al., 2016)  1.27 \n46M \nFS-LSTM-2 (Mujika et al., 2017)  1.290 \n27M \nFS-LSTM-4 (Mujika et al., 2017) \n1.277 \n27M \nLarge FS-LSTM-4 (Mujika et al., 2017) 1.245 \n47M \n\n4 layer QRNN (h \u003d 1800) (ours) \n1.336 \n26M \n3 layer LSTM (h \u003d 1840) (ours) \n1.232 \n47M \n\ncmix v13 (Knoll, 2018) \n1.225 \n-\n\nTable 3. Bits Per Character (BPC) on enwik8. \n\n","rows":["RHN - depth 5 ( Zilly et al . , 2016 )","HM - LSTM ( Chung et al . , 2016 )","Layer Norm LSTM , 1800 units","LSTM , 2000 units ( Mujika et al . , 2017 )","3 layer LSTM ( h \u003d 1840 ) ( ours )","SD Zoneout ( Rocki et al . , 2016 )","cmix v13 ( Knoll , 2018 )","Large RHN ( Zilly et al . , 2016 )","FS - LSTM - 2 ( Mujika et al . , 2017 )","FS - LSTM - 4 ( Mujika et al . , 2017 )","RHN - depth 10 ( Zilly et al . , 2016 )","HyperLSTM ( Ha et al . , 2016 )","Large FS - LSTM - 4 ( Mujika et al . , 2017 )","4 layer QRNN ( h \u003d 1800 ) ( ours )"],"columns":["BPC"],"mergedAllColumns":["18M","27M","26M","14M","47M","35M","46M","23M","21M","64M"],"numberCells":[{"number":"1.340","isBolded":false,"associatedRows":["HyperLSTM ( Ha et al . , 2016 )"],"associatedColumns":["BPC"],"associatedMergedColumns":["14M"]},{"number":"1.277","isBolded":false,"associatedRows":["FS - LSTM - 4 ( Mujika et al . , 2017 )"],"associatedColumns":["BPC"],"associatedMergedColumns":["27M"]},{"number":"1.31","isBolded":false,"associatedRows":["RHN - depth 5 ( Zilly et al . , 2016 )"],"associatedColumns":["BPC"],"associatedMergedColumns":["64M"]},{"number":"1.30","isBolded":false,"associatedRows":["RHN - depth 10 ( Zilly et al . , 2016 )"],"associatedColumns":["BPC"],"associatedMergedColumns":["23M"]},{"number":"1.461","isBolded":false,"associatedRows":["LSTM , 2000 units ( Mujika et al . , 2017 )"],"associatedColumns":["BPC"],"associatedMergedColumns":[]},{"number":"1.402","isBolded":false,"associatedRows":["Layer Norm LSTM , 1800 units"],"associatedColumns":["BPC"],"associatedMergedColumns":["18M"]},{"number":"1.225","isBolded":false,"associatedRows":["cmix v13 ( Knoll , 2018 )"],"associatedColumns":["BPC"],"associatedMergedColumns":["47M"]},{"number":"1.32","isBolded":false,"associatedRows":["HM - LSTM ( Chung et al . , 2016 )"],"associatedColumns":["BPC"],"associatedMergedColumns":["27M"]},{"number":"1.336","isBolded":false,"associatedRows":["4 layer QRNN ( h \u003d 1800 ) ( ours )"],"associatedColumns":["BPC"],"associatedMergedColumns":["47M"]},{"number":"1.27","isBolded":false,"associatedRows":["Large RHN ( Zilly et al . , 2016 )"],"associatedColumns":["BPC"],"associatedMergedColumns":["21M"]},{"number":"1.290","isBolded":false,"associatedRows":["FS - LSTM - 2 ( Mujika et al . , 2017 )"],"associatedColumns":["BPC"],"associatedMergedColumns":["46M"]},{"number":"1.245","isBolded":false,"associatedRows":["Large FS - LSTM - 4 ( Mujika et al . , 2017 )"],"associatedColumns":["BPC"],"associatedMergedColumns":["27M"]},{"number":"1.31","isBolded":false,"associatedRows":["SD Zoneout ( Rocki et al . , 2016 )"],"associatedColumns":["BPC"],"associatedMergedColumns":["35M"]},{"number":"1.232","isBolded":false,"associatedRows":["3 layer LSTM ( h \u003d 1840 ) ( ours )"],"associatedColumns":["BPC"],"associatedMergedColumns":["26M"]}]},{"caption":"Model \nVal Test \n\nGrave et al. (2016a) \n-\n48.7 \nDauphin et al. (2016), 1 GPU \n-\n44.9 \nDauphin et al. (2016), 4 GPUs \n-\n37.2 \n\n4 layer QRNN (h \u003d 2500), 1 GPU 32.0 33.0 \n\nTable 4. Perplexity on the word-level WikiText-103 dataset. The \nmodel was trained for 12 hours (14 epochs) on an NVIDIA Volta. \n\n","rows":["4 layer QRNN ( h \u003d 2500 ) , 1 GPU","Grave et al . ( 2016a )","Dauphin et al . ( 2016 ) , 4 GPUs","-","Dauphin et al . ( 2016 ) , 1 GPU"],"columns":["Val","Test"],"mergedAllColumns":[],"numberCells":[{"number":"33.0","isBolded":true,"associatedRows":["4 layer QRNN ( h \u003d 2500 ) , 1 GPU","-"],"associatedColumns":["Test"],"associatedMergedColumns":[]},{"number":"37.2","isBolded":true,"associatedRows":["Dauphin et al . ( 2016 ) , 4 GPUs","-"],"associatedColumns":["Test"],"associatedMergedColumns":[]},{"number":"48.7","isBolded":false,"associatedRows":["Grave et al . ( 2016a )","-"],"associatedColumns":["Test"],"associatedMergedColumns":[]},{"number":"44.9","isBolded":true,"associatedRows":["Dauphin et al . ( 2016 ) , 1 GPU","-"],"associatedColumns":["Test"],"associatedMergedColumns":[]},{"number":"32.0","isBolded":true,"associatedRows":["4 layer QRNN ( h \u003d 2500 ) , 1 GPU"],"associatedColumns":["Val"],"associatedMergedColumns":[]}]},{"caption":"Table 6. Hyper-parameters for word-and character-level language modeling experiments. Training time is for all noted epochs on an \nNVIDIA Volta. Dropout refers to embedding, (RNN) hidden, input, and output. \n\n","rows":["Total parameters","Learning rate","Epochs","Batch size","Input embedding size","Weight drop","BPTT length"],"columns":["LSTM","QRNN","1000","0","Character PTB","3","1840","4","2500","1 . 2e ? 6","enwik8","0 / 0 . 01 / 0 . 01 / 0 . 4","WikiText - 103","50","0 / 0","[ 300 , 400 ]","0 / 0 . 1 / 0 . 1 / 0 . 1","0 / 0 . 25 / 0 . 1 / 0 . 1"],"mergedAllColumns":["0","60","10"],"numberCells":[{"number":"0.5","isBolded":false,"associatedRows":["Weight drop"],"associatedColumns":["Character PTB","LSTM","3","1000","0 / 0","0 / 0 . 25 / 0 . 1 / 0 . 1"],"associatedMergedColumns":[]},{"number":"400","isBolded":false,"associatedRows":["Input embedding size"],"associatedColumns":["enwik8","LSTM","3","1840","0 / 0","0 / 0 . 01 / 0 . 01 / 0 . 4","1 . 2e ? 6"],"associatedMergedColumns":["60"]},{"number":"0.2","isBolded":false,"associatedRows":["Weight drop"],"associatedColumns":["enwik8","LSTM","3","1840","0 / 0","0 / 0 . 01 / 0 . 01 / 0 . 4"],"associatedMergedColumns":[]},{"number":"13.8M","isBolded":false,"associatedRows":["Total parameters"],"associatedColumns":["Character PTB","LSTM","3","1000","0 / 0","0 / 0 . 25 / 0 . 1 / 0 . 1","1 . 2e ? 6","50","[ 300 , 400 ]"],"associatedMergedColumns":["10"]},{"number":"128","isBolded":false,"associatedRows":["Input embedding size"],"associatedColumns":["Character PTB","LSTM","3","1000","0 / 0","0 / 0 . 25 / 0 . 1 / 0 . 1","1 . 2e ? 6"],"associatedMergedColumns":["60"]},{"number":"128","isBolded":false,"associatedRows":["Batch size"],"associatedColumns":["enwik8","LSTM","3","1840","0 / 0","0 / 0 . 01 / 0 . 01 / 0 . 4","1 . 2e ? 6"],"associatedMergedColumns":["0"]},{"number":"150","isBolded":false,"associatedRows":["BPTT length"],"associatedColumns":["Character PTB","LSTM","3","1000","0 / 0","0 / 0 . 25 / 0 . 1 / 0 . 1","1 . 2e ? 6"],"associatedMergedColumns":["0"]},{"number":"0.001","isBolded":false,"associatedRows":["Learning rate"],"associatedColumns":["enwik8","LSTM","3","1840","0 / 0","0 / 0 . 01 / 0 . 01 / 0 . 4","1 . 2e ? 6"],"associatedMergedColumns":["60"]},{"number":"500","isBolded":false,"associatedRows":["Epochs"],"associatedColumns":["Character PTB","LSTM","3","1000","0 / 0","0 / 0 . 25 / 0 . 1 / 0 . 1","1 . 2e ? 6"],"associatedMergedColumns":["60"]},{"number":"140","isBolded":false,"associatedRows":["BPTT length"],"associatedColumns":["WikiText - 103","QRNN","4","2500","0 / 0","0 / 0 . 1 / 0 . 1 / 0 . 1","0"],"associatedMergedColumns":["0"]},{"number":"400","isBolded":false,"associatedRows":["Input embedding size"],"associatedColumns":["WikiText - 103","QRNN","4","2500","0 / 0","0 / 0 . 1 / 0 . 1 / 0 . 1","0"],"associatedMergedColumns":["60"]},{"number":"200","isBolded":false,"associatedRows":["BPTT length"],"associatedColumns":["enwik8","LSTM","3","1840","0 / 0","0 / 0 . 01 / 0 . 01 / 0 . 4","1 . 2e ? 6"],"associatedMergedColumns":["0"]},{"number":"128","isBolded":false,"associatedRows":["Batch size"],"associatedColumns":["Character PTB","LSTM","3","1000","0 / 0","0 / 0 . 25 / 0 . 1 / 0 . 1","1 . 2e ? 6"],"associatedMergedColumns":["0"]},{"number":"0.002","isBolded":false,"associatedRows":["Learning rate"],"associatedColumns":["Character PTB","LSTM","3","1000","0 / 0","0 / 0 . 25 / 0 . 1 / 0 . 1","1 . 2e ? 6"],"associatedMergedColumns":["60"]},{"number":"0.001","isBolded":false,"associatedRows":["Learning rate"],"associatedColumns":["WikiText - 103","QRNN","4","2500","0 / 0","0 / 0 . 1 / 0 . 1 / 0 . 1","0"],"associatedMergedColumns":["60"]}]}]