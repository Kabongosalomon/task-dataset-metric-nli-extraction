[{"caption":"Table 1: Results on ImageNet-1K pretraining. \n\n","rows":["128M","305M","WideNet - B","ViT - B","WideNet - H","ViT - MoE - L","29M","ViT - L","406M","ViT - MoE - B","87M","geNet we used in this work has 1k classes and","63M","40M","WideNet - L"],"columns":["ImageNet - 1K"],"mergedAllColumns":["et al . 2009 ) as platforms to evaluate our framework . Ima -"],"numberCells":[{"number":"77.4","isBolded":false,"associatedRows":["ViT - MoE - L","406M"],"associatedColumns":["ImageNet - 1K"],"associatedMergedColumns":[]},{"number":"80.1","isBolded":true,"associatedRows":["WideNet - H","63M"],"associatedColumns":["ImageNet - 1K"],"associatedMergedColumns":[]},{"number":"77.5","isBolded":false,"associatedRows":["ViT - L","305M"],"associatedColumns":["ImageNet - 1K"],"associatedMergedColumns":[]},{"number":"77.9","isBolded":false,"associatedRows":["ViT - MoE - B","128M"],"associatedColumns":["ImageNet - 1K"],"associatedMergedColumns":[]},{"number":"1.3Mim-","isBolded":false,"associatedRows":["geNet we used in this work has 1k classes and"],"associatedColumns":["ImageNet - 1K"],"associatedMergedColumns":["et al . 2009 ) as platforms to evaluate our framework . Ima -"]},{"number":"78.6","isBolded":false,"associatedRows":["ViT - B","87M"],"associatedColumns":["ImageNet - 1K"],"associatedMergedColumns":[]},{"number":"77.5","isBolded":false,"associatedRows":["WideNet - B","29M"],"associatedColumns":["ImageNet - 1K"],"associatedMergedColumns":[]},{"number":"79.5","isBolded":false,"associatedRows":["WideNet - L","40M"],"associatedColumns":["ImageNet - 1K"],"associatedMergedColumns":[]}]},{"caption":"Table 2: Results of funetuning on GLUE benchmarks \n\nModel \n#para SQuAD1.1 SQuAD2.0 MNLI SST-2 Avg \n\nALBERT \n12M \n89.3/82.3 \n80.0/77.1 \n81.5 \n90.3 \n84.0 \nBERT \n89M \n89.9/82.8 \n80.3/77.3 \n83.2 \n91.5 \n85.0 \n\nWideNet 4 experts \n26M \n89.6/82.7 \n80.6/77.4 \n82.6 \n91.1 \n84.7 \nWideNet 8 experts \n45M \n90.0/82.7 \n80.6/77.7 \n83.3 \n91.9 \n85.2 \nWideNet 16 experts 83M \n90.9/83.8 \n81.0/77.9 \n84.1 \n92.2 \n85.8 \n\nComparison with baselines We follow the hyper-\nparameter setting of baselines in pretraining and finetuning \nfor a fair comparison. Please see Appendix for details. Such \nimplementation also shows that our model is robust to hyper-\nparameters. \nWe report the Top-1 accuracy on ImageNet-1K in Ta-\nble 1 and Cifar10 in Appendix. Observe that WideNet-H \nachieves the best performance and significantly outperforms \nViT and ViT-MoE models on ImageNet-1K. Compared with \nthe strongest baseline, our WideNet-H outperforms ViT-B \nby 1.5% with less trainable parameters. Even if we use the \nsmallest model, WideNet-B, it still achieves comparable per-\nformance with ViT-L and ViT-MoE-B with over 4? less \ntrainable parameters. When we scale up to WideNet-L, it \nhas surpassed all baselines with half trainable parameters of \nViT-B and 0.13? parameters of ViT-L. \n","rows":["80 . 0 / 77 . 1","89 . 9 / 82 . 8","WideNet 8 experts","80 . 6 / 77 . 4","89 . 6 / 82 . 7","ALBERT","26M","80 . 6 / 77 . 7","12M","89M","45M","80 . 3 / 77 . 3","WideNet 4 experts","83M","90 . 9 / 83 . 8","ViT - B and","by","81 . 0 / 77 . 9","BERT","WideNet 16 experts","89 . 3 / 82 . 3","90 . 0 / 82 . 7"],"columns":["the","#para","Avg","baselines","SST - 2","Table 2 : Results of funetuning on GLUE benchmarks","1 and Cifar10 in Appendix . Observe that WideNet - H","Model","MNLI","We"],"mergedAllColumns":["has surpassed all baselines with half trainable parameters of","the strongest baseline , our WideNet - H outperforms ViT - B"],"numberCells":[{"number":"90.3","isBolded":false,"associatedRows":["by","ALBERT","12M","89 . 3 / 82 . 3","80 . 0 / 77 . 1"],"associatedColumns":["Table 2 : Results of funetuning on GLUE benchmarks","SST - 2"],"associatedMergedColumns":[]},{"number":"1.5%withlesstrainableparameters.Evenifweusethe","isBolded":true,"associatedRows":["by"],"associatedColumns":["Table 2 : Results of funetuning on GLUE benchmarks","#para","baselines","the","1 and Cifar10 in Appendix . Observe that WideNet - H"],"associatedMergedColumns":["the strongest baseline , our WideNet - H outperforms ViT - B"]},{"number":"84.7","isBolded":false,"associatedRows":["ViT - B and","WideNet 4 experts","26M","89 . 6 / 82 . 7","80 . 6 / 77 . 4"],"associatedColumns":["Table 2 : Results of funetuning on GLUE benchmarks","Avg"],"associatedMergedColumns":[]},{"number":"85.2","isBolded":false,"associatedRows":["ViT - B and","WideNet 8 experts","45M","90 . 0 / 82 . 7","80 . 6 / 77 . 7"],"associatedColumns":["Table 2 : Results of funetuning on GLUE benchmarks","Avg"],"associatedMergedColumns":[]},{"number":"82.6","isBolded":false,"associatedRows":["by","WideNet 4 experts","26M","89 . 6 / 82 . 7","80 . 6 / 77 . 4"],"associatedColumns":["Table 2 : Results of funetuning on GLUE benchmarks","MNLI"],"associatedMergedColumns":[]},{"number":"92.2","isBolded":true,"associatedRows":["ViT - B and","WideNet 16 experts","83M","90 . 9 / 83 . 8","81 . 0 / 77 . 9"],"associatedColumns":["Table 2 : Results of funetuning on GLUE benchmarks","SST - 2"],"associatedMergedColumns":[]},{"number":"83.3","isBolded":false,"associatedRows":["by","WideNet 8 experts","45M","90 . 0 / 82 . 7","80 . 6 / 77 . 7"],"associatedColumns":["Table 2 : Results of funetuning on GLUE benchmarks","MNLI"],"associatedMergedColumns":[]},{"number":"81.5","isBolded":false,"associatedRows":["by","ALBERT","12M","89 . 3 / 82 . 3","80 . 0 / 77 . 1"],"associatedColumns":["Table 2 : Results of funetuning on GLUE benchmarks","MNLI"],"associatedMergedColumns":[]},{"number":"84.0","isBolded":false,"associatedRows":["ViT - B and","ALBERT","12M","89 . 3 / 82 . 3","80 . 0 / 77 . 1"],"associatedColumns":["Table 2 : Results of funetuning on GLUE benchmarks","Avg"],"associatedMergedColumns":[]},{"number":"91.5","isBolded":false,"associatedRows":["by","BERT","89M","89 . 9 / 82 . 8","80 . 3 / 77 . 3"],"associatedColumns":["Table 2 : Results of funetuning on GLUE benchmarks","SST - 2"],"associatedMergedColumns":[]},{"number":"91.1","isBolded":false,"associatedRows":["ViT - B and","WideNet 4 experts","26M","89 . 6 / 82 . 7","80 . 6 / 77 . 4"],"associatedColumns":["Table 2 : Results of funetuning on GLUE benchmarks","SST - 2"],"associatedMergedColumns":[]},{"number":"83.2","isBolded":false,"associatedRows":["by","BERT","89M","89 . 9 / 82 . 8","80 . 3 / 77 . 3"],"associatedColumns":["Table 2 : Results of funetuning on GLUE benchmarks","MNLI"],"associatedMergedColumns":[]},{"number":"91.9","isBolded":false,"associatedRows":["ViT - B and","WideNet 8 experts","45M","90 . 0 / 82 . 7","80 . 6 / 77 . 7"],"associatedColumns":["Table 2 : Results of funetuning on GLUE benchmarks","SST - 2"],"associatedMergedColumns":[]},{"number":"0.13?parametersofViT-L.","isBolded":true,"associatedRows":["ViT - B and"],"associatedColumns":["Table 2 : Results of funetuning on GLUE benchmarks","Model","baselines","We","1 and Cifar10 in Appendix . Observe that WideNet - H"],"associatedMergedColumns":["has surpassed all baselines with half trainable parameters of"]},{"number":"85.0","isBolded":false,"associatedRows":["ViT - B and","BERT","89M","89 . 9 / 82 . 8","80 . 3 / 77 . 3"],"associatedColumns":["Table 2 : Results of funetuning on GLUE benchmarks","Avg"],"associatedMergedColumns":[]},{"number":"84.1","isBolded":true,"associatedRows":["ViT - B and","WideNet 16 experts","83M","90 . 9 / 83 . 8","81 . 0 / 77 . 9"],"associatedColumns":["Table 2 : Results of funetuning on GLUE benchmarks","MNLI"],"associatedMergedColumns":[]},{"number":"85.8","isBolded":true,"associatedRows":["ViT - B and","WideNet 16 experts","83M","90 . 9 / 83 . 8","81 . 0 / 77 . 9"],"associatedColumns":["Table 2 : Results of funetuning on GLUE benchmarks","Avg"],"associatedMergedColumns":[]}]},{"caption":"Table 3: Results of ablation study on ImageNet-1K to to in-\nvestigate the contributions of our three key modifications \n(i.e., Independent Layer Normalization, scaling width with \nMoE layer and compressing depth with parameter sharing). \n\n","rows":["w / shared Layer Norm","WideNet - B","WideNet - H","WideNet - L","w / o parameter sharing","w / o MoE layer"],"columns":["Nan","Top - 1"],"mergedAllColumns":["63M","128M","40M","29M","15M","406M"],"numberCells":[{"number":"76.9","isBolded":false,"associatedRows":["w / o MoE layer"],"associatedColumns":["Top - 1","Nan"],"associatedMergedColumns":["40M"]},{"number":"77.9","isBolded":true,"associatedRows":["w / o parameter sharing"],"associatedColumns":["Top - 1","Nan"],"associatedMergedColumns":["29M"]},{"number":"78.3","isBolded":false,"associatedRows":["w / shared Layer Norm"],"associatedColumns":["Top - 1","Nan"],"associatedMergedColumns":["40M"]},{"number":"80.1","isBolded":true,"associatedRows":["WideNet - H"],"associatedColumns":["Top - 1","Nan"],"associatedMergedColumns":["406M"]},{"number":"77.5","isBolded":false,"associatedRows":["WideNet - B","w / shared Layer Norm"],"associatedColumns":["Top - 1"],"associatedMergedColumns":[]},{"number":"79.5","isBolded":true,"associatedRows":["WideNet - L"],"associatedColumns":["Top - 1","Nan"],"associatedMergedColumns":["128M"]},{"number":"79.0","isBolded":false,"associatedRows":["w / o MoE layer"],"associatedColumns":["Top - 1","Nan"],"associatedMergedColumns":["63M"]},{"number":"76.3","isBolded":false,"associatedRows":["WideNet - B","w / shared Layer Norm"],"associatedColumns":["Top - 1"],"associatedMergedColumns":["29M"]},{"number":"76.6","isBolded":false,"associatedRows":["w / shared Layer Norm"],"associatedColumns":["Top - 1","Nan"],"associatedMergedColumns":["63M"]},{"number":"77.4","isBolded":false,"associatedRows":["w / o parameter sharing"],"associatedColumns":["Top - 1","Nan"],"associatedMergedColumns":["15M"]}]},{"caption":"Table 4: Results of ablation study on ImageNet-1K to evaluate our WideNet with comparable speed or computation cost. \n#Blocks is the number of transformer blocks. FNN dim means the dimension of FFN layer. Para Sharing is whether we shared \nparameters across transformer blocks. Time denotes to TPUv3 core days. \n\n","rows":["4096","12","24","305M","ViT - B","8192","ViT - L","15M","24M","40M","of","WideNet - L","y","?","gamma","beta"],"columns":["0","WideNet - B","ViT - B","i - th LayerNorm before MoE layer","4","of","y","Figure 5 : Divergence of ? with LayerNorm layers .","Time","Top - 1"],"mergedAllColumns":["?"],"numberCells":[{"number":"76.9","isBolded":false,"associatedRows":["y","ViT - L","24","4096","?","of"],"associatedColumns":["ViT - B","WideNet - B","y","0","i - th LayerNorm before MoE layer","Figure 5 : Divergence of ? with LayerNorm layers .","Top - 1"],"associatedMergedColumns":[]},{"number":"0.00015","isBolded":false,"associatedRows":["y","ViT - L","24","4096","?","of"],"associatedColumns":["ViT - B","WideNet - B","y"],"associatedMergedColumns":[]},{"number":"0.00030","isBolded":false,"associatedRows":["gamma","ViT - L","24","4096","?","beta"],"associatedColumns":["ViT - B","WideNet - B"],"associatedMergedColumns":[]},{"number":"0.00045","isBolded":false,"associatedRows":["gamma","ViT - L","24","ViT - B","4096","?","beta"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.07K","isBolded":false,"associatedRows":["y","WideNet - L","12","4096","?","of","40M"],"associatedColumns":["ViT - B","WideNet - B","y","4","i - th LayerNorm before MoE layer","Figure 5 : Divergence of ? with LayerNorm layers .","Time"],"associatedMergedColumns":[]},{"number":"0.00035","isBolded":false,"associatedRows":["gamma"],"associatedColumns":["ViT - B","WideNet - B"],"associatedMergedColumns":[]},{"number":"77.5","isBolded":false,"associatedRows":["y","ViT - L","24","4096","?","of"],"associatedColumns":["ViT - B","WideNet - B","y","0","i - th LayerNorm before MoE layer","Figure 5 : Divergence of ? with LayerNorm layers .","Top - 1"],"associatedMergedColumns":[]},{"number":"0.00050","isBolded":false,"associatedRows":["gamma"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.14K","isBolded":false,"associatedRows":["y","WideNet - L","24","4096","?","of","40M"],"associatedColumns":["ViT - B","WideNet - B","y","4","i - th LayerNorm before MoE layer","Figure 5 : Divergence of ? with LayerNorm layers .","Time"],"associatedMergedColumns":["?"]},{"number":"0.00045","isBolded":false,"associatedRows":["gamma"],"associatedColumns":["ViT - B","WideNet - B"],"associatedMergedColumns":[]},{"number":"0.00030","isBolded":false,"associatedRows":["y"],"associatedColumns":["ViT - B","WideNet - B","of"],"associatedMergedColumns":[]},{"number":"78.2","isBolded":true,"associatedRows":["y","WideNet - L","12","4096","?","of"],"associatedColumns":["ViT - B","WideNet - B","y","0","i - th LayerNorm before MoE layer","Figure 5 : Divergence of ? with LayerNorm layers .","Top - 1"],"associatedMergedColumns":[]},{"number":"0.00020","isBolded":false,"associatedRows":["y"],"associatedColumns":["ViT - B","WideNet - B","of"],"associatedMergedColumns":[]},{"number":"0.00040","isBolded":false,"associatedRows":["gamma","ViT - L","24","4096","?","beta"],"associatedColumns":["ViT - B"],"associatedMergedColumns":[]},{"number":"0.00010","isBolded":false,"associatedRows":["y","WideNet - L","24","4096","?","of"],"associatedColumns":["ViT - B","WideNet - B","y"],"associatedMergedColumns":[]},{"number":"0.00025","isBolded":false,"associatedRows":["y"],"associatedColumns":["ViT - B","WideNet - B","of"],"associatedMergedColumns":[]},{"number":"79.5","isBolded":true,"associatedRows":["y","WideNet - L","24","4096","?","of"],"associatedColumns":["ViT - B","WideNet - B","y","0","i - th LayerNorm before MoE layer","Figure 5 : Divergence of ? with LayerNorm layers .","Top - 1"],"associatedMergedColumns":["?"]},{"number":"0.07K","isBolded":false,"associatedRows":["y","ViT - L","24","4096","?","of","15M"],"associatedColumns":["ViT - B","WideNet - B","y","4","i - th LayerNorm before MoE layer","Figure 5 : Divergence of ? with LayerNorm layers .","Time"],"associatedMergedColumns":[]},{"number":"0.00020","isBolded":false,"associatedRows":["y","ViT - L","24","4096","?","of"],"associatedColumns":["ViT - B","WideNet - B","y"],"associatedMergedColumns":[]},{"number":"0.00035","isBolded":false,"associatedRows":["gamma","ViT - L","24","4096","?","beta"],"associatedColumns":["ViT - B","WideNet - B"],"associatedMergedColumns":[]},{"number":"0.08K","isBolded":false,"associatedRows":["y","ViT - L","24","4096","?","of","305M"],"associatedColumns":["ViT - B","WideNet - B","y","4","i - th LayerNorm before MoE layer","Figure 5 : Divergence of ? with LayerNorm layers .","Time"],"associatedMergedColumns":[]},{"number":"0.00025","isBolded":false,"associatedRows":["gamma","ViT - L","24","4096","?","of"],"associatedColumns":["ViT - B","WideNet - B"],"associatedMergedColumns":[]},{"number":"0.09K","isBolded":false,"associatedRows":["y","ViT - L","24","8192","?","of","24M"],"associatedColumns":["ViT - B","WideNet - B","y","4","i - th LayerNorm before MoE layer","Figure 5 : Divergence of ? with LayerNorm layers .","Time"],"associatedMergedColumns":[]},{"number":"0.00040","isBolded":false,"associatedRows":["gamma"],"associatedColumns":["ViT - B","WideNet - B"],"associatedMergedColumns":[]},{"number":"75.8","isBolded":false,"associatedRows":["y","ViT - L","24","8192","?","of"],"associatedColumns":["ViT - B","WideNet - B","y","0","i - th LayerNorm before MoE layer","Figure 5 : Divergence of ? with LayerNorm layers .","Top - 1"],"associatedMergedColumns":[]}]},{"caption":"Table 5: Results on Cifar10 finetuning. \n\n","rows":["85M","305M","WideNet - B","ViT - B","ViT - MoE - L","27M","WideNet - L","38M","ViT - L","ViT - MoE - B","404M","126M"],"columns":["Cifar10"],"mergedAllColumns":[],"numberCells":[{"number":"98.3","isBolded":false,"associatedRows":["ViT - B","85M"],"associatedColumns":["Cifar10"],"associatedMergedColumns":[]},{"number":"98.2","isBolded":false,"associatedRows":["ViT - L","305M"],"associatedColumns":["Cifar10"],"associatedMergedColumns":[]},{"number":"98.8","isBolded":true,"associatedRows":["WideNet - L","38M"],"associatedColumns":["Cifar10"],"associatedMergedColumns":[]},{"number":"98.5","isBolded":false,"associatedRows":["ViT - MoE - L","404M"],"associatedColumns":["Cifar10"],"associatedMergedColumns":[]},{"number":"98.5","isBolded":false,"associatedRows":["ViT - MoE - B","126M"],"associatedColumns":["Cifar10"],"associatedMergedColumns":[]},{"number":"98.4","isBolded":false,"associatedRows":["WideNet - B","27M"],"associatedColumns":["Cifar10"],"associatedMergedColumns":[]}]},{"caption":"Table 6: Hyper-parameters on ImageNet-1K pretraining and \nCifar10 finetuning. \n\n","rows":["Weight Decay","Learning rate","Mixup prob .","Label smoothing","Dropout"],"columns":["0","4096","100","300","ImageNet - 1K","512","Cifar10","30"],"mergedAllColumns":["0"],"numberCells":[{"number":"0.03","isBolded":false,"associatedRows":["Learning rate"],"associatedColumns":["Cifar10","100","0","512"],"associatedMergedColumns":[]},{"number":"0.1","isBolded":false,"associatedRows":["Label smoothing"],"associatedColumns":["ImageNet - 1K","300","30","4096"],"associatedMergedColumns":["0"]},{"number":"0.5","isBolded":false,"associatedRows":["Mixup prob ."],"associatedColumns":["ImageNet - 1K","300","30","4096"],"associatedMergedColumns":["0"]},{"number":"0.1","isBolded":false,"associatedRows":["Dropout"],"associatedColumns":["Cifar10","100","0","512"],"associatedMergedColumns":["0"]},{"number":"0.5","isBolded":false,"associatedRows":["Mixup prob ."],"associatedColumns":["Cifar10","100","0","512"],"associatedMergedColumns":["0"]},{"number":"0.01","isBolded":false,"associatedRows":["Learning rate"],"associatedColumns":["ImageNet - 1K","300","30","4096"],"associatedMergedColumns":[]},{"number":"0.1","isBolded":false,"associatedRows":["Weight Decay"],"associatedColumns":["ImageNet - 1K","300","30","4096"],"associatedMergedColumns":[]},{"number":"0.1","isBolded":false,"associatedRows":["Dropout"],"associatedColumns":["ImageNet - 1K","300","30","4096"],"associatedMergedColumns":["0"]}]},{"caption":"Table 6. On ImageNet-1K cosine learn-\ning rate decay is used after 30 warmup epochs. Please note \nall models are using the same hyper-parameters of ","rows":["Weight Decay","Learning rate","Mixup prob .","Label smoothing","Dropout"],"columns":["0","4096","100","300","ImageNet - 1K","512","Cifar10","30"],"mergedAllColumns":["0"],"numberCells":[{"number":"0.1","isBolded":false,"associatedRows":["Weight Decay"],"associatedColumns":["ImageNet - 1K","300","30","4096"],"associatedMergedColumns":[]},{"number":"0.1","isBolded":false,"associatedRows":["Dropout"],"associatedColumns":["ImageNet - 1K","300","30","4096"],"associatedMergedColumns":["0"]},{"number":"0.1","isBolded":false,"associatedRows":["Dropout"],"associatedColumns":["Cifar10","100","0","512"],"associatedMergedColumns":["0"]},{"number":"0.1","isBolded":false,"associatedRows":["Label smoothing"],"associatedColumns":["ImageNet - 1K","300","30","4096"],"associatedMergedColumns":["0"]},{"number":"0.01","isBolded":false,"associatedRows":["Learning rate"],"associatedColumns":["ImageNet - 1K","300","30","4096"],"associatedMergedColumns":[]},{"number":"0.03","isBolded":false,"associatedRows":["Learning rate"],"associatedColumns":["Cifar10","100","0","512"],"associatedMergedColumns":[]},{"number":"0.5","isBolded":false,"associatedRows":["Mixup prob ."],"associatedColumns":["Cifar10","100","0","512"],"associatedMergedColumns":["0"]},{"number":"0.5","isBolded":false,"associatedRows":["Mixup prob ."],"associatedColumns":["ImageNet - 1K","300","30","4096"],"associatedMergedColumns":["0"]}]}]