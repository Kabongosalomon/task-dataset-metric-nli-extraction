[{"caption":"Table 2: Transfer learning across different datasets. Note that our approach leads to improvements \nin both supervised and self-supervised learning set-up. \n\n","rows":["Stylized ImageNet","200","MoCo V2 Anistropic ( Ours )","Supervised ( Reproduced )","Supervised Anistropic ( Ours )","MoCo V2 ( Chen et al . , 2020b )","Supervised"],"columns":["Cars","DTD","AP *","Dogs","Birds","50","mIoU ( SS )","0 . 50 : 0 . 05 : 0 . 95","-","Aircraft","75"],"mergedAllColumns":[],"numberCells":[{"number":"72.66","isBolded":false,"associatedRows":["MoCo V2 ( Chen et al . , 2020b )","MoCo V2 ( Chen et al . , 2020b )"],"associatedColumns":["50","-","DTD"],"associatedMergedColumns":[]},{"number":"90.3","isBolded":false,"associatedRows":["MoCo V2 ( Chen et al . , 2020b )","MoCo V2 ( Chen et al . , 2020b )"],"associatedColumns":["0 . 50 : 0 . 05 : 0 . 95","-","Birds"],"associatedMergedColumns":[]},{"number":"82.4","isBolded":false,"associatedRows":["MoCo V2 ( Chen et al . , 2020b )","Supervised ( Reproduced )","200"],"associatedColumns":["50","-"],"associatedMergedColumns":[]},{"number":"64.2","isBolded":false,"associatedRows":["MoCo V2 Anistropic ( Ours )","Supervised Anistropic ( Ours )","200"],"associatedColumns":["75","-"],"associatedMergedColumns":[]},{"number":"93.5","isBolded":false,"associatedRows":["MoCo V2 ( Chen et al . , 2020b )","MoCo V2 Anistropic ( Ours )"],"associatedColumns":["75","-","Cars"],"associatedMergedColumns":[]},{"number":"73.03","isBolded":false,"associatedRows":["MoCo V2 ( Chen et al . , 2020b )","Supervised Anistropic ( Ours )"],"associatedColumns":["50","-","DTD"],"associatedMergedColumns":[]},{"number":"53.5","isBolded":false,"associatedRows":["Supervised","Supervised Anistropic ( Ours )","200"],"associatedColumns":["0 . 50 : 0 . 05 : 0 . 95","-"],"associatedMergedColumns":[]},{"number":"70.1","isBolded":false,"associatedRows":["Supervised","Supervised Anistropic ( Ours )","200"],"associatedColumns":["AP *","-"],"associatedMergedColumns":[]},{"number":"92.1","isBolded":false,"associatedRows":["MoCo V2 ( Chen et al . , 2020b )","MoCo V2 ( Chen et al . , 2020b )"],"associatedColumns":["75","-","Cars"],"associatedMergedColumns":[]},{"number":"67.3","isBolded":false,"associatedRows":["MoCo V2 Anistropic ( Ours )","Supervised Anistropic ( Ours )","200"],"associatedColumns":["AP *","-"],"associatedMergedColumns":[]},{"number":"74.73","isBolded":false,"associatedRows":["MoCo V2 ( Chen et al . , 2020b )","MoCo V2 ( Chen et al . , 2020b )"],"associatedColumns":["50","-","DTD"],"associatedMergedColumns":[]},{"number":"86.40","isBolded":false,"associatedRows":["MoCo V2 ( Chen et al . , 2020b )","Supervised Anistropic ( Ours )"],"associatedColumns":["0 . 50 : 0 . 05 : 0 . 95","-","Dogs"],"associatedMergedColumns":[]},{"number":"92.05","isBolded":false,"associatedRows":["MoCo V2 Anistropic ( Ours )","MoCo V2 Anistropic ( Ours )"],"associatedColumns":["50","-","Aircraft"],"associatedMergedColumns":[]},{"number":"66.5","isBolded":false,"associatedRows":["MoCo V2 ( Chen et al . , 2020b )","Supervised Anistropic ( Ours )","200"],"associatedColumns":["AP *","-"],"associatedMergedColumns":[]},{"number":"28.80","isBolded":false,"associatedRows":["Stylized ImageNet","Supervised ( Reproduced )","200"],"associatedColumns":["0 . 50 : 0 . 05 : 0 . 95"],"associatedMergedColumns":[]},{"number":"87.92","isBolded":false,"associatedRows":["MoCo V2 Anistropic ( Ours )","MoCo V2 Anistropic ( Ours )"],"associatedColumns":["0 . 50 : 0 . 05 : 0 . 95","-","Dogs"],"associatedMergedColumns":[]},{"number":"53.5","isBolded":false,"associatedRows":["Supervised","Supervised Anistropic ( Ours )","200"],"associatedColumns":["mIoU ( SS )","-"],"associatedMergedColumns":[]},{"number":"82.8","isBolded":false,"associatedRows":["MoCo V2 Anistropic ( Ours )","Supervised ( Reproduced )","200"],"associatedColumns":["50","-"],"associatedMergedColumns":[]},{"number":"91.67","isBolded":false,"associatedRows":["MoCo V2 Anistropic ( Ours )","Supervised Anistropic ( Ours )"],"associatedColumns":["50","-","Aircraft"],"associatedMergedColumns":[]},{"number":"93.1","isBolded":false,"associatedRows":["MoCo V2 ( Chen et al . , 2020b )","Supervised Anistropic ( Ours )"],"associatedColumns":["75","-","Cars"],"associatedMergedColumns":[]},{"number":"91.57","isBolded":false,"associatedRows":["MoCo V2 Anistropic ( Ours )","MoCo V2 ( Chen et al . , 2020b )"],"associatedColumns":["50","-","Aircraft"],"associatedMergedColumns":[]},{"number":"58.8","isBolded":false,"associatedRows":["Supervised","Supervised Anistropic ( Ours )","200"],"associatedColumns":["75","-"],"associatedMergedColumns":[]},{"number":"90.88","isBolded":false,"associatedRows":["MoCo V2 Anistropic ( Ours )","Supervised Anistropic ( Ours )"],"associatedColumns":["50","-","Aircraft"],"associatedMergedColumns":[]},{"number":"92.13","isBolded":false,"associatedRows":["MoCo V2 Anistropic ( Ours )","MoCo V2 ( Chen et al . , 2020b )"],"associatedColumns":["0 . 50 : 0 . 05 : 0 . 95","-","Birds"],"associatedMergedColumns":[]},{"number":"43.5","isBolded":false,"associatedRows":["Stylized ImageNet","Supervised ( Reproduced )","200"],"associatedColumns":["50"],"associatedMergedColumns":[]},{"number":"55.5","isBolded":false,"associatedRows":["MoCo V2 ( Chen et al . , 2020b )","Supervised Anistropic ( Ours )","200"],"associatedColumns":["mIoU ( SS )","-"],"associatedMergedColumns":[]},{"number":"81.3","isBolded":false,"associatedRows":["Supervised","Supervised ( Reproduced )","200"],"associatedColumns":["50","-"],"associatedMergedColumns":[]},{"number":"92.76","isBolded":false,"associatedRows":["MoCo V2 Anistropic ( Ours )","MoCo V2 Anistropic ( Ours )"],"associatedColumns":["0 . 50 : 0 . 05 : 0 . 95","-","Birds"],"associatedMergedColumns":[]},{"number":"59.7","isBolded":false,"associatedRows":["Stylized ImageNet","Supervised Anistropic ( Ours )","200"],"associatedColumns":["75"],"associatedMergedColumns":[]},{"number":"57.0","isBolded":false,"associatedRows":["MoCo V2 ( Chen et al . , 2020b )","Supervised Anistropic ( Ours )","200"],"associatedColumns":["0 . 50 : 0 . 05 : 0 . 95","-"],"associatedMergedColumns":[]},{"number":"92.8","isBolded":false,"associatedRows":["MoCo V2 ( Chen et al . , 2020b )","MoCo V2 ( Chen et al . , 2020b )"],"associatedColumns":["75","-","Cars"],"associatedMergedColumns":[]},{"number":"63.6","isBolded":false,"associatedRows":["MoCo V2 ( Chen et al . , 2020b )","Supervised Anistropic ( Ours )","200"],"associatedColumns":["75","-"],"associatedMergedColumns":[]},{"number":"85.35","isBolded":false,"associatedRows":["MoCo V2 ( Chen et al . , 2020b )","MoCo V2 ( Chen et al . , 2020b )"],"associatedColumns":["0 . 50 : 0 . 05 : 0 . 95","-","Dogs"],"associatedMergedColumns":[]},{"number":"57.4","isBolded":false,"associatedRows":["MoCo V2 Anistropic ( Ours )","Supervised Anistropic ( Ours )","200"],"associatedColumns":["0 . 50 : 0 . 05 : 0 . 95","-"],"associatedMergedColumns":[]},{"number":"56.1","isBolded":false,"associatedRows":["MoCo V2 Anistropic ( Ours )","Supervised Anistropic ( Ours )","200"],"associatedColumns":["mIoU ( SS )","-"],"associatedMergedColumns":[]},{"number":"91.42","isBolded":false,"associatedRows":["MoCo V2 Anistropic ( Ours )","Supervised Anistropic ( Ours )"],"associatedColumns":["0 . 50 : 0 . 05 : 0 . 95","-","Birds"],"associatedMergedColumns":[]},{"number":"87.13","isBolded":false,"associatedRows":["MoCo V2 ( Chen et al . , 2020b )","MoCo V2 ( Chen et al . , 2020b )"],"associatedColumns":["0 . 50 : 0 . 05 : 0 . 95","-","Dogs"],"associatedMergedColumns":[]},{"number":"75.12","isBolded":false,"associatedRows":["MoCo V2 ( Chen et al . , 2020b )","MoCo V2 Anistropic ( Ours )"],"associatedColumns":["50","-","DTD"],"associatedMergedColumns":[]}]},{"caption":"Table 3: Experiments with Sketch-ImageNet. Use of Anisotropic ImageNet shows that our method \nis better at capturing representation that are less dependent on texture. \n\n","rows":["Anisotropic ( Ours )","ImageNet Baseline","Stylized Baseline"],"columns":["Top - 5 Accuracy","Top - 1 Accuracy"],"mergedAllColumns":[],"numberCells":[{"number":"31.56","isBolded":false,"associatedRows":["Stylized Baseline"],"associatedColumns":["Top - 5 Accuracy"],"associatedMergedColumns":[]},{"number":"26.24","isBolded":false,"associatedRows":["ImageNet Baseline"],"associatedColumns":["Top - 5 Accuracy"],"associatedMergedColumns":[]},{"number":"16.36","isBolded":false,"associatedRows":["Stylized Baseline"],"associatedColumns":["Top - 1 Accuracy"],"associatedMergedColumns":[]},{"number":"13.00","isBolded":false,"associatedRows":["ImageNet Baseline"],"associatedColumns":["Top - 1 Accuracy"],"associatedMergedColumns":[]},{"number":"24.49","isBolded":true,"associatedRows":["Anisotropic ( Ours )"],"associatedColumns":["Top - 1 Accuracy"],"associatedMergedColumns":[]},{"number":"41.81","isBolded":true,"associatedRows":["Anisotropic ( Ours )"],"associatedColumns":["Top - 5 Accuracy"],"associatedMergedColumns":[]}]},{"caption":"Table 4: Comparison using different texture removing methods, with different hyper-parameters for \nAnisotropic diffusion methods. We observe that the most simple (Perona \u0026 Malik, 1990) performs \nthe best and removing more texture from images does not improve performance. \n\n","rows":["Bilateral ImageNet","Perona Malik ( Perona \u0026 Malik , 1990 )","Baseline Supervised","Robust AD ( Black et al . , 1998 )","Cartoon ImageNet","50","Gaussian Blur","-","and the baseline model is","20"],"columns":["Obj . Det .","Top - 1 Acc","Top - 5 Acc"],"mergedAllColumns":[],"numberCells":[{"number":"76.32","isBolded":false,"associatedRows":["Perona Malik ( Perona \u0026 Malik , 1990 )","50"],"associatedColumns":["Top - 1 Acc"],"associatedMergedColumns":[]},{"number":"76.64","isBolded":false,"associatedRows":["Robust AD ( Black et al . , 1998 )","50"],"associatedColumns":["Top - 1 Acc"],"associatedMergedColumns":[]},{"number":"74.37","isBolded":true,"associatedRows":["Perona Malik ( Perona \u0026 Malik , 1990 )","20"],"associatedColumns":["Obj . Det ."],"associatedMergedColumns":[]},{"number":"73.57","isBolded":false,"associatedRows":["Robust AD ( Black et al . , 1998 )","50"],"associatedColumns":["Obj . Det ."],"associatedMergedColumns":[]},{"number":"76.71","isBolded":true,"associatedRows":["Perona Malik ( Perona \u0026 Malik , 1990 )","20"],"associatedColumns":["Top - 1 Acc"],"associatedMergedColumns":[]},{"number":"73.33","isBolded":false,"associatedRows":["Robust AD ( Black et al . , 1998 )","20"],"associatedColumns":["Obj . Det ."],"associatedMergedColumns":[]},{"number":"93.26","isBolded":true,"associatedRows":["Perona Malik ( Perona \u0026 Malik , 1990 )","20"],"associatedColumns":["Top - 5 Acc"],"associatedMergedColumns":[]},{"number":"92.96","isBolded":false,"associatedRows":["Perona Malik ( Perona \u0026 Malik , 1990 )","50"],"associatedColumns":["Top - 5 Acc"],"associatedMergedColumns":[]},{"number":"76.22","isBolded":false,"associatedRows":["Cartoon ImageNet","-"],"associatedColumns":["Top - 1 Acc"],"associatedMergedColumns":[]},{"number":"92.96","isBolded":false,"associatedRows":["Robust AD ( Black et al . , 1998 )","20"],"associatedColumns":["Top - 5 Acc"],"associatedMergedColumns":[]},{"number":"92.64","isBolded":false,"associatedRows":["Gaussian Blur","-"],"associatedColumns":["Top - 5 Acc"],"associatedMergedColumns":[]},{"number":"73.26","isBolded":false,"associatedRows":["Gaussian Blur","-"],"associatedColumns":["Obj . Det ."],"associatedMergedColumns":[]},{"number":"75.99","isBolded":false,"associatedRows":["Bilateral ImageNet","-"],"associatedColumns":["Top - 1 Acc"],"associatedMergedColumns":[]},{"number":"11.49%forTop-1accuracy,Thisresultimpliesthatourmodelcaptures","isBolded":false,"associatedRows":["and the baseline model is"],"associatedColumns":["Obj . Det ."],"associatedMergedColumns":[]},{"number":"92.90","isBolded":false,"associatedRows":["Bilateral ImageNet","-"],"associatedColumns":["Top - 5 Acc"],"associatedMergedColumns":[]},{"number":"76.58","isBolded":false,"associatedRows":["Robust AD ( Black et al . , 1998 )","20"],"associatedColumns":["Top - 1 Acc"],"associatedMergedColumns":[]},{"number":"73.80","isBolded":false,"associatedRows":["Perona Malik ( Perona \u0026 Malik , 1990 )","50"],"associatedColumns":["Obj . Det ."],"associatedMergedColumns":[]},{"number":"76.21","isBolded":false,"associatedRows":["Gaussian Blur","-"],"associatedColumns":["Top - 1 Acc"],"associatedMergedColumns":[]},{"number":"93.09","isBolded":false,"associatedRows":["Robust AD ( Black et al . , 1998 )","50"],"associatedColumns":["Top - 5 Acc"],"associatedMergedColumns":[]},{"number":"93.12","isBolded":false,"associatedRows":["Cartoon ImageNet","-"],"associatedColumns":["Top - 5 Acc"],"associatedMergedColumns":[]},{"number":"72.31","isBolded":false,"associatedRows":["Cartoon ImageNet","-"],"associatedColumns":["Obj . Det ."],"associatedMergedColumns":[]},{"number":"76.13","isBolded":false,"associatedRows":["Baseline Supervised","-"],"associatedColumns":["Top - 1 Acc"],"associatedMergedColumns":[]},{"number":"92.98","isBolded":false,"associatedRows":["Baseline Supervised","-"],"associatedColumns":["Top - 5 Acc"],"associatedMergedColumns":[]},{"number":"70.7","isBolded":false,"associatedRows":["Baseline Supervised","-"],"associatedColumns":["Obj . Det ."],"associatedMergedColumns":[]},{"number":"71.34","isBolded":false,"associatedRows":["Bilateral ImageNet","-"],"associatedColumns":["Obj . Det ."],"associatedMergedColumns":[]}]},{"caption":"Table 5: Comparison of our approach with Jigsaw baseline methods. Using our best model, we \nimprove 2.52 mAP in VOC classification , 0.78 mAP on VOC detection and 8.1 mAP on VOC \nsemantic segmentation(SS) over the baseline models. Note that Stylized ImageNet performs poorly \non VOC classification due to the visual shortcuts. \n\n","rows":["Gaussian ImageNet","Bilateral ImageNet","Only Anisotropic","Stylized ( Geirhos et al . , 2018 )","Anisotropic ImageNet","Cartoon ImageNet","Baseline","2?1 . 2M"],"columns":["SS","VOC Cls .","Dataset Size","VOC Det ."],"mergedAllColumns":[],"numberCells":[{"number":"27.9","isBolded":false,"associatedRows":["Gaussian ImageNet","2?1 . 2M"],"associatedColumns":["SS"],"associatedMergedColumns":[]},{"number":"74.55","isBolded":false,"associatedRows":["Bilateral ImageNet","2?1 . 2M"],"associatedColumns":["VOC Cls ."],"associatedMergedColumns":[]},{"number":"62.74","isBolded":true,"associatedRows":["Bilateral ImageNet","2?1 . 2M"],"associatedColumns":["VOC Det ."],"associatedMergedColumns":[]},{"number":"59.31","isBolded":false,"associatedRows":["Cartoon ImageNet","2?1 . 2M"],"associatedColumns":["VOC Det ."],"associatedMergedColumns":[]},{"number":"28.13","isBolded":false,"associatedRows":["Stylized ( Geirhos et al . , 2018 )","2?1 . 2M"],"associatedColumns":["VOC Det ."],"associatedMergedColumns":[]},{"number":"1.2M","isBolded":false,"associatedRows":["Only Anisotropic"],"associatedColumns":["Dataset Size"],"associatedMergedColumns":[]},{"number":"35.2","isBolded":true,"associatedRows":["Anisotropic ImageNet","2?1 . 2M"],"associatedColumns":["SS"],"associatedMergedColumns":[]},{"number":"61.85","isBolded":false,"associatedRows":["Only Anisotropic","2?1 . 2M"],"associatedColumns":["VOC Det ."],"associatedMergedColumns":[]},{"number":"27.1","isBolded":false,"associatedRows":["Baseline","2?1 . 2M"],"associatedColumns":["SS"],"associatedMergedColumns":[]},{"number":"10.12","isBolded":false,"associatedRows":["Stylized ( Geirhos et al . , 2018 )","2?1 . 2M"],"associatedColumns":["SS"],"associatedMergedColumns":[]},{"number":"61.98","isBolded":false,"associatedRows":["Baseline","2?1 . 2M"],"associatedColumns":["VOC Det ."],"associatedMergedColumns":[]},{"number":"13.81","isBolded":false,"associatedRows":["Stylized ( Geirhos et al . , 2018 )","2?1 . 2M"],"associatedColumns":["VOC Cls ."],"associatedMergedColumns":[]},{"number":"61.59","isBolded":false,"associatedRows":["Anisotropic ImageNet","2?1 . 2M"],"associatedColumns":["VOC Det ."],"associatedMergedColumns":[]},{"number":"76.77","isBolded":false,"associatedRows":["Anisotropic ImageNet","2?1 . 2M"],"associatedColumns":["VOC Cls ."],"associatedMergedColumns":[]},{"number":"75.49","isBolded":false,"associatedRows":["Gaussian ImageNet","2?1 . 2M"],"associatedColumns":["VOC Cls ."],"associatedMergedColumns":[]},{"number":"1.2M","isBolded":false,"associatedRows":["Stylized ( Geirhos et al . , 2018 )"],"associatedColumns":["Dataset Size"],"associatedMergedColumns":[]},{"number":"28.9","isBolded":false,"associatedRows":["Bilateral ImageNet","2?1 . 2M"],"associatedColumns":["SS"],"associatedMergedColumns":[]},{"number":"32.7","isBolded":false,"associatedRows":["Only Anisotropic","2?1 . 2M"],"associatedColumns":["SS"],"associatedMergedColumns":[]},{"number":"74.82","isBolded":false,"associatedRows":["Baseline","2?1 . 2M"],"associatedColumns":["VOC Cls ."],"associatedMergedColumns":[]},{"number":"77.34","isBolded":true,"associatedRows":["Cartoon ImageNet","2?1 . 2M"],"associatedColumns":["VOC Cls ."],"associatedMergedColumns":[]},{"number":"34.1","isBolded":false,"associatedRows":["Cartoon ImageNet","2?1 . 2M"],"associatedColumns":["SS"],"associatedMergedColumns":[]},{"number":"62.39","isBolded":false,"associatedRows":["Gaussian ImageNet","2?1 . 2M"],"associatedColumns":["VOC Det ."],"associatedMergedColumns":[]},{"number":"74.52","isBolded":false,"associatedRows":["Only Anisotropic","2?1 . 2M"],"associatedColumns":["VOC Cls ."],"associatedMergedColumns":[]},{"number":"1.2M","isBolded":false,"associatedRows":["Baseline"],"associatedColumns":["Dataset Size"],"associatedMergedColumns":[]}]},{"caption":"Table 6: ImageNet classification by finetuning the last FC layer. Features from the conv layers are \nkept unchanged. This experiment helps evaluate the quality of features learnt by the convolutional \nlayers. \n\n","rows":["2?1 . 2M","Jigsaw Baseline","Jigsaw anisotropic"],"columns":["VOC Cls","ImageNet Cls . Acc","Dataset Size","VOC Det ."],"mergedAllColumns":[],"numberCells":[{"number":"74.82","isBolded":false,"associatedRows":["Jigsaw Baseline"],"associatedColumns":["VOC Cls"],"associatedMergedColumns":[]},{"number":"26.17","isBolded":false,"associatedRows":["Jigsaw Baseline"],"associatedColumns":["ImageNet Cls . Acc"],"associatedMergedColumns":[]},{"number":"1.2M","isBolded":false,"associatedRows":["Jigsaw Baseline"],"associatedColumns":["Dataset Size"],"associatedMergedColumns":[]},{"number":"26.67","isBolded":false,"associatedRows":["Jigsaw anisotropic","2?1 . 2M"],"associatedColumns":["ImageNet Cls . Acc"],"associatedMergedColumns":[]},{"number":"61.59","isBolded":false,"associatedRows":["Jigsaw anisotropic","2?1 . 2M"],"associatedColumns":["VOC Det ."],"associatedMergedColumns":[]},{"number":"76.77","isBolded":false,"associatedRows":["Jigsaw anisotropic","2?1 . 2M"],"associatedColumns":["VOC Cls"],"associatedMergedColumns":[]},{"number":"61.98","isBolded":false,"associatedRows":["Jigsaw Baseline"],"associatedColumns":["VOC Det ."],"associatedMergedColumns":[]}]},{"caption":"Table 7: Experiments with Alexnet as the backbone. Ideas of anisotropic diffusion filter can extend \nto other architectures like Alexnet. The Anistropic ImageNet model improves over the baseline by \n0.67 mAP \nMethod \nVOC 2007 Classification \n\nJigsaw Baseline(Our Implementation) \n65.21 \nJigsaw anisotropic \n65.88 \n","rows":["Jigsaw Baseline ( Our Implementation )","Jigsaw anisotropic"],"columns":["Table 7 : Experiments with Alexnet as the backbone . Ideas of anisotropic diffusion filter can extend","VOC 2007 Classification"],"mergedAllColumns":["to other architectures like Alexnet . The Anistropic ImageNet model improves over the baseline by"],"numberCells":[{"number":"65.88","isBolded":false,"associatedRows":["Jigsaw anisotropic"],"associatedColumns":["Table 7 : Experiments with Alexnet as the backbone . Ideas of anisotropic diffusion filter can extend","VOC 2007 Classification"],"associatedMergedColumns":["to other architectures like Alexnet . The Anistropic ImageNet model improves over the baseline by"]},{"number":"65.21","isBolded":false,"associatedRows":["Jigsaw Baseline ( Our Implementation )"],"associatedColumns":["Table 7 : Experiments with Alexnet as the backbone . Ideas of anisotropic diffusion filter can extend","VOC 2007 Classification"],"associatedMergedColumns":["to other architectures like Alexnet . The Anistropic ImageNet model improves over the baseline by"]},{"number":"0.67mAP","isBolded":false,"associatedRows":[],"associatedColumns":["Table 7 : Experiments with Alexnet as the backbone . Ideas of anisotropic diffusion filter can extend"],"associatedMergedColumns":["to other architectures like Alexnet . The Anistropic ImageNet model improves over the baseline by"]}]},{"caption":"Table 8: Results on Label corruption task. We can see that our model consistently outperforms the \nbaseline with larger improvement upon increasing the corruption probability. \nCorruption probability Gold Fraction Baseline(Error) Anistropic Model(Error) \n\n0.2 \n0.05 \n29.61 \n28.75 \n0.2 \n0.1 \n28.31 \n27.84 \n0.4 \n0.05 \n31.92 \n30.5 \n0.4 \n0.1 \n32.59 \n31.48 \n0.6 \n0.05 \n39.04 \n38.52 \n0.6 \n0.1 \n36.75 \n34.98 \n0.8 \n0.05 \n54.23 \n52.6 \n0.8 \n0.1 \n44.31 \n43 \n1.0 \n0.05 \n75.05 \n71.21 \n1.0 \n0.1 \n51.19 \n45.51 \n\n","rows":[],"columns":["Corruption probability","Baseline ( Error )","Anistropic Model ( Error )","Gold Fraction","Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the"],"mergedAllColumns":["baseline with larger improvement upon increasing the corruption probability .","43"],"numberCells":[{"number":"0.05","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Gold Fraction"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"28.31","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Baseline ( Error )"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"31.92","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Baseline ( Error )"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"0.8","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Corruption probability"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"52.6","isBolded":true,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Anistropic Model ( Error )"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"44.31","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Baseline ( Error )"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"34.98","isBolded":true,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Anistropic Model ( Error )"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"32.59","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Baseline ( Error )"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"0.05","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Gold Fraction"],"associatedMergedColumns":["43"]},{"number":"71.21","isBolded":true,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Anistropic Model ( Error )"],"associatedMergedColumns":["43"]},{"number":"30.5","isBolded":true,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Anistropic Model ( Error )"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"29.61","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Baseline ( Error )"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"36.75","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Baseline ( Error )"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"0.8","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Corruption probability"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"0.05","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Gold Fraction"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"45.51","isBolded":true,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Anistropic Model ( Error )"],"associatedMergedColumns":["43"]},{"number":"0.1","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Gold Fraction"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"1.0","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Corruption probability"],"associatedMergedColumns":["43"]},{"number":"0.2","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Corruption probability"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"39.04","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Baseline ( Error )"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"54.23","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Baseline ( Error )"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"0.1","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Gold Fraction"],"associatedMergedColumns":["43"]},{"number":"28.75","isBolded":true,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Anistropic Model ( Error )"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"0.1","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Gold Fraction"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"51.19","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Baseline ( Error )"],"associatedMergedColumns":["43"]},{"number":"0.4","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Corruption probability"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"0.05","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Gold Fraction"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"0.1","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Gold Fraction"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"0.1","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Gold Fraction"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"27.84","isBolded":true,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Anistropic Model ( Error )"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"31.48","isBolded":true,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Anistropic Model ( Error )"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"38.52","isBolded":true,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Anistropic Model ( Error )"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"1.0","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Corruption probability"],"associatedMergedColumns":["43"]},{"number":"0.6","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Corruption probability"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"0.6","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Corruption probability"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"75.05","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Baseline ( Error )"],"associatedMergedColumns":["43"]},{"number":"0.4","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Corruption probability"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"0.05","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Gold Fraction"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]},{"number":"0.2","isBolded":false,"associatedRows":[],"associatedColumns":["Table 8 : Results on Label corruption task . We can see that our model consistently outperforms the","Corruption probability"],"associatedMergedColumns":["baseline with larger improvement upon increasing the corruption probability ."]}]},{"caption":"Table 9: Comparison between Stylized ImageNet and our Anisotropic ImageNet. Following \n(Geirhos et al., 2018), we use ResNet50 as our backbone. We finetune our models on only the \nImageNet dataset. We can see that on ImageNet classification and object detection, Anisotropic \nImageNet and Stylized ImageNet have very similar performance. \nMethod \nFinetune Top-1 Accuracy Top-5 Accuracy OBJ Detection \n\nStylized Imagenet \n-\n74.59 \n92.14 \n70.6 \nStylized Imagenet \nIN \n76.72 \n93.28 \n75.1 \nAnisotropic Imagenet \n-\n68.38 \n87.19 \n-\nAnisotropic Imagenet \nIN \n76.71 \n93.26 \n74.27 \nCartoon Imagenet \nIN \n76.22 \n93.12 \n72.31 \n\n","rows":["Stylized Imagenet","IN","Anisotropic Imagenet","Cartoon Imagenet","Standard ImageNet","-","Anistropic ImageNet"],"columns":["Mean Highest probability","OBJ Detection","Top - 5 Accuracy","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","Top - 1 Accuracy","Entropy","( Geirhos et al . , 2018 ) , we use ResNet50 as our backbone .","Comparison between Stylized ImageNet and our Anisotropic ImageNet ."],"mergedAllColumns":["ImageNet and Stylized ImageNet have very similar performance .","-","ImageNet"],"numberCells":[{"number":"70.6","isBolded":false,"associatedRows":["Stylized Imagenet","-"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","OBJ Detection"],"associatedMergedColumns":["ImageNet and Stylized ImageNet have very similar performance ."]},{"number":"76.72","isBolded":false,"associatedRows":["Stylized Imagenet","IN"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","( Geirhos et al . , 2018 ) , we use ResNet50 as our backbone .","We can see that on ImageNet classification and object detection , Anisotropic","Top - 1 Accuracy"],"associatedMergedColumns":["ImageNet and Stylized ImageNet have very similar performance ."]},{"number":"0.81","isBolded":false,"associatedRows":["Cartoon Imagenet","Anistropic ImageNet"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","( Geirhos et al . , 2018 ) , we use ResNet50 as our backbone .","We can see that on ImageNet classification and object detection , Anisotropic","Top - 1 Accuracy","Entropy"],"associatedMergedColumns":["ImageNet"]},{"number":"74.27","isBolded":false,"associatedRows":["Anisotropic Imagenet","IN"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","OBJ Detection"],"associatedMergedColumns":["-"]},{"number":"93.12","isBolded":false,"associatedRows":["Cartoon Imagenet","IN"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","Top - 5 Accuracy"],"associatedMergedColumns":["-"]},{"number":"92.14","isBolded":false,"associatedRows":["Stylized Imagenet","-"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","Top - 5 Accuracy"],"associatedMergedColumns":["ImageNet and Stylized ImageNet have very similar performance ."]},{"number":"72.31","isBolded":false,"associatedRows":["Cartoon Imagenet","IN"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","OBJ Detection"],"associatedMergedColumns":["-"]},{"number":"93.26","isBolded":false,"associatedRows":["Anisotropic Imagenet","IN"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","Top - 5 Accuracy"],"associatedMergedColumns":["-"]},{"number":"76.22","isBolded":false,"associatedRows":["Cartoon Imagenet","IN"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","( Geirhos et al . , 2018 ) , we use ResNet50 as our backbone .","We can see that on ImageNet classification and object detection , Anisotropic","Top - 1 Accuracy"],"associatedMergedColumns":["-"]},{"number":"74.59","isBolded":false,"associatedRows":["Stylized Imagenet","-"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","( Geirhos et al . , 2018 ) , we use ResNet50 as our backbone .","We can see that on ImageNet classification and object detection , Anisotropic","Top - 1 Accuracy"],"associatedMergedColumns":["ImageNet and Stylized ImageNet have very similar performance ."]},{"number":"75.1","isBolded":false,"associatedRows":["Stylized Imagenet","IN"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","OBJ Detection"],"associatedMergedColumns":["ImageNet and Stylized ImageNet have very similar performance ."]},{"number":"1.88","isBolded":false,"associatedRows":["Cartoon Imagenet","Standard ImageNet"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","( Geirhos et al . , 2018 ) , we use ResNet50 as our backbone .","We can see that on ImageNet classification and object detection , Anisotropic","Top - 1 Accuracy","Entropy"],"associatedMergedColumns":["ImageNet"]},{"number":"0.59","isBolded":false,"associatedRows":["Anisotropic Imagenet","Standard ImageNet"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","Top - 5 Accuracy","Mean Highest probability"],"associatedMergedColumns":["ImageNet"]},{"number":"93.28","isBolded":false,"associatedRows":["Stylized Imagenet","IN"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","Top - 5 Accuracy"],"associatedMergedColumns":["ImageNet and Stylized ImageNet have very similar performance ."]},{"number":"0.93","isBolded":false,"associatedRows":["Anisotropic Imagenet","Anistropic ImageNet"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","Top - 5 Accuracy","Mean Highest probability"],"associatedMergedColumns":["ImageNet"]},{"number":"76.71","isBolded":false,"associatedRows":["Anisotropic Imagenet","IN"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","( Geirhos et al . , 2018 ) , we use ResNet50 as our backbone .","We can see that on ImageNet classification and object detection , Anisotropic","Top - 1 Accuracy"],"associatedMergedColumns":["-"]},{"number":"68.38","isBolded":false,"associatedRows":["Anisotropic Imagenet","-"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","( Geirhos et al . , 2018 ) , we use ResNet50 as our backbone .","We can see that on ImageNet classification and object detection , Anisotropic","Top - 1 Accuracy"],"associatedMergedColumns":["ImageNet and Stylized ImageNet have very similar performance ."]},{"number":"87.19","isBolded":false,"associatedRows":["Anisotropic Imagenet","-"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","Top - 5 Accuracy"],"associatedMergedColumns":["ImageNet and Stylized ImageNet have very similar performance ."]}]},{"caption":"Table 10: Experiments discussing the confidence and entropy of Anistropic ImageNet and Standard \nImageNet \nMethod \nEntropy Mean Highest probability \n\nAnistropic ImageNet \n0.81 \n0.93 \nStandard ImageNet \n1.88 \n0.59 \n\n","rows":["Stylized Imagenet","IN","Anisotropic Imagenet","Cartoon Imagenet","Standard ImageNet","-","2?1 . 2M","Anistropic ImageNet","Jigsaw Baseline","Jigsaw anisotropic"],"columns":["Mean Highest probability","OBJ Detection","Top - 5 Accuracy","We finetune our models on only the","Top - 1 Accuracy","ImageNet Cls . Acc","Finetune","Entropy","Method","Dataset Size","Comparison between Stylized ImageNet and our Anisotropic ImageNet .","VOC Det .","We can see that on ImageNet classification and object detection , Anisotropic","VOC Cls .","( Geirhos et al . , 2018 ) , we use ResNet50 as our backbone ."],"mergedAllColumns":["ImageNet and Stylized ImageNet have very similar performance .","layers .","-","ImageNet"],"numberCells":[{"number":"76.72","isBolded":false,"associatedRows":["Stylized Imagenet","IN"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","( Geirhos et al . , 2018 ) , we use ResNet50 as our backbone .","We can see that on ImageNet classification and object detection , Anisotropic","Top - 1 Accuracy"],"associatedMergedColumns":["ImageNet and Stylized ImageNet have very similar performance ."]},{"number":"87.19","isBolded":false,"associatedRows":["Anisotropic Imagenet","-"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","Top - 5 Accuracy"],"associatedMergedColumns":["ImageNet and Stylized ImageNet have very similar performance ."]},{"number":"74.82","isBolded":false,"associatedRows":["Jigsaw Baseline","2?1 . 2M"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","( Geirhos et al . , 2018 ) , we use ResNet50 as our backbone .","We can see that on ImageNet classification and object detection , Anisotropic","Top - 1 Accuracy","Entropy","VOC Cls ."],"associatedMergedColumns":["layers ."]},{"number":"0.59","isBolded":false,"associatedRows":["Anisotropic Imagenet","Standard ImageNet"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","Top - 5 Accuracy","Mean Highest probability"],"associatedMergedColumns":["ImageNet"]},{"number":"26.67","isBolded":false,"associatedRows":["Jigsaw anisotropic","2?1 . 2M"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","OBJ Detection","Mean Highest probability","ImageNet Cls . Acc"],"associatedMergedColumns":["layers ."]},{"number":"75.1","isBolded":false,"associatedRows":["Stylized Imagenet","IN"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","OBJ Detection"],"associatedMergedColumns":["ImageNet and Stylized ImageNet have very similar performance ."]},{"number":"76.77","isBolded":false,"associatedRows":["Jigsaw anisotropic","2?1 . 2M"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","( Geirhos et al . , 2018 ) , we use ResNet50 as our backbone .","We can see that on ImageNet classification and object detection , Anisotropic","Top - 1 Accuracy","Entropy","VOC Cls ."],"associatedMergedColumns":["layers ."]},{"number":"70.6","isBolded":false,"associatedRows":["Stylized Imagenet","-"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","OBJ Detection"],"associatedMergedColumns":["ImageNet and Stylized ImageNet have very similar performance ."]},{"number":"0.81","isBolded":false,"associatedRows":["Cartoon Imagenet","Anistropic ImageNet"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","( Geirhos et al . , 2018 ) , we use ResNet50 as our backbone .","We can see that on ImageNet classification and object detection , Anisotropic","Top - 1 Accuracy","Entropy"],"associatedMergedColumns":["ImageNet"]},{"number":"61.98","isBolded":false,"associatedRows":["Jigsaw Baseline","2?1 . 2M"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","Top - 5 Accuracy","Mean Highest probability","VOC Det ."],"associatedMergedColumns":["layers ."]},{"number":"93.28","isBolded":false,"associatedRows":["Stylized Imagenet","IN"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","Top - 5 Accuracy"],"associatedMergedColumns":["ImageNet and Stylized ImageNet have very similar performance ."]},{"number":"93.26","isBolded":false,"associatedRows":["Anisotropic Imagenet","IN"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","Top - 5 Accuracy"],"associatedMergedColumns":["-"]},{"number":"72.31","isBolded":false,"associatedRows":["Cartoon Imagenet","IN"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","OBJ Detection"],"associatedMergedColumns":["-"]},{"number":"76.22","isBolded":false,"associatedRows":["Cartoon Imagenet","IN"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","( Geirhos et al . , 2018 ) , we use ResNet50 as our backbone .","We can see that on ImageNet classification and object detection , Anisotropic","Top - 1 Accuracy"],"associatedMergedColumns":["-"]},{"number":"61.59","isBolded":false,"associatedRows":["Jigsaw anisotropic","2?1 . 2M"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","Top - 5 Accuracy","Mean Highest probability","VOC Det ."],"associatedMergedColumns":["layers ."]},{"number":"92.14","isBolded":false,"associatedRows":["Stylized Imagenet","-"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","Top - 5 Accuracy"],"associatedMergedColumns":["ImageNet and Stylized ImageNet have very similar performance ."]},{"number":"1.88","isBolded":false,"associatedRows":["Cartoon Imagenet","Standard ImageNet"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","( Geirhos et al . , 2018 ) , we use ResNet50 as our backbone .","We can see that on ImageNet classification and object detection , Anisotropic","Top - 1 Accuracy","Entropy"],"associatedMergedColumns":["ImageNet"]},{"number":"74.59","isBolded":false,"associatedRows":["Stylized Imagenet","-"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","( Geirhos et al . , 2018 ) , we use ResNet50 as our backbone .","We can see that on ImageNet classification and object detection , Anisotropic","Top - 1 Accuracy"],"associatedMergedColumns":["ImageNet and Stylized ImageNet have very similar performance ."]},{"number":"68.38","isBolded":false,"associatedRows":["Anisotropic Imagenet","-"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","( Geirhos et al . , 2018 ) , we use ResNet50 as our backbone .","We can see that on ImageNet classification and object detection , Anisotropic","Top - 1 Accuracy"],"associatedMergedColumns":["ImageNet and Stylized ImageNet have very similar performance ."]},{"number":"93.12","isBolded":false,"associatedRows":["Cartoon Imagenet","IN"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","Top - 5 Accuracy"],"associatedMergedColumns":["-"]},{"number":"26.17","isBolded":false,"associatedRows":["Jigsaw Baseline","2?1 . 2M"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","OBJ Detection","Mean Highest probability","ImageNet Cls . Acc"],"associatedMergedColumns":["layers ."]},{"number":"0.93","isBolded":false,"associatedRows":["Anisotropic Imagenet","Anistropic ImageNet"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","Top - 5 Accuracy","Mean Highest probability"],"associatedMergedColumns":["ImageNet"]},{"number":"1.2M","isBolded":false,"associatedRows":["Jigsaw Baseline"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","( Geirhos et al . , 2018 ) , we use ResNet50 as our backbone .","We can see that on ImageNet classification and object detection , Anisotropic","Finetune","Method","Dataset Size"],"associatedMergedColumns":["layers ."]},{"number":"74.27","isBolded":false,"associatedRows":["Anisotropic Imagenet","IN"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","We finetune our models on only the","We can see that on ImageNet classification and object detection , Anisotropic","OBJ Detection"],"associatedMergedColumns":["-"]},{"number":"76.71","isBolded":false,"associatedRows":["Anisotropic Imagenet","IN"],"associatedColumns":["Comparison between Stylized ImageNet and our Anisotropic ImageNet .","( Geirhos et al . , 2018 ) , we use ResNet50 as our backbone .","We can see that on ImageNet classification and object detection , Anisotropic","Top - 1 Accuracy"],"associatedMergedColumns":["-"]}]},{"caption":"Table 11: ImageNet classification by finetuning the last FC layer. Features from the conv layers are \nkept unchanged. This experiment helps evaluate the quality of features learnt by the convolutional \nlayers. \nMethod \nDataset Size VOC Cls. VOC Det. ImageNet Cls. Acc \n\nJigsaw Baseline \n1.2M \n74.82 \n61.98 \n26.17 \nJigsaw anisotropic \n2?1.2M \n76.77 \n61.59 \n26.67 \n\n","rows":["Standard ImageNet","2?1 . 2M","Anistropic ImageNet","Jigsaw Baseline","Jigsaw anisotropic"],"columns":["Mean Highest probability","ImageNet Cls . Acc","Entropy","Method","VOC Cls .","Dataset Size","Table 10 : Experiments discussing the confidence and entropy of Anistropic ImageNet and Standard","VOC Det ."],"mergedAllColumns":["kept unchanged . This experiment helps evaluate the quality of features learnt by the convolutional","ImageNet"],"numberCells":[{"number":"61.98","isBolded":false,"associatedRows":["Jigsaw Baseline","2?1 . 2M"],"associatedColumns":["Table 10 : Experiments discussing the confidence and entropy of Anistropic ImageNet and Standard","Mean Highest probability","VOC Det ."],"associatedMergedColumns":["kept unchanged . This experiment helps evaluate the quality of features learnt by the convolutional"]},{"number":"0.81","isBolded":false,"associatedRows":["Jigsaw Baseline","Anistropic ImageNet"],"associatedColumns":["Table 10 : Experiments discussing the confidence and entropy of Anistropic ImageNet and Standard","Entropy"],"associatedMergedColumns":["ImageNet"]},{"number":"76.77","isBolded":false,"associatedRows":["Jigsaw anisotropic","2?1 . 2M"],"associatedColumns":["Table 10 : Experiments discussing the confidence and entropy of Anistropic ImageNet and Standard","Entropy","VOC Cls ."],"associatedMergedColumns":["kept unchanged . This experiment helps evaluate the quality of features learnt by the convolutional"]},{"number":"1.88","isBolded":false,"associatedRows":["Jigsaw Baseline","Standard ImageNet"],"associatedColumns":["Table 10 : Experiments discussing the confidence and entropy of Anistropic ImageNet and Standard","Entropy"],"associatedMergedColumns":["ImageNet"]},{"number":"74.82","isBolded":false,"associatedRows":["Jigsaw Baseline","2?1 . 2M"],"associatedColumns":["Table 10 : Experiments discussing the confidence and entropy of Anistropic ImageNet and Standard","Entropy","VOC Cls ."],"associatedMergedColumns":["kept unchanged . This experiment helps evaluate the quality of features learnt by the convolutional"]},{"number":"0.59","isBolded":false,"associatedRows":["Jigsaw Baseline","Standard ImageNet"],"associatedColumns":["Table 10 : Experiments discussing the confidence and entropy of Anistropic ImageNet and Standard","Mean Highest probability"],"associatedMergedColumns":["ImageNet"]},{"number":"26.67","isBolded":false,"associatedRows":["Jigsaw anisotropic","2?1 . 2M"],"associatedColumns":["Table 10 : Experiments discussing the confidence and entropy of Anistropic ImageNet and Standard","Mean Highest probability","ImageNet Cls . Acc"],"associatedMergedColumns":["kept unchanged . This experiment helps evaluate the quality of features learnt by the convolutional"]},{"number":"0.93","isBolded":false,"associatedRows":["Jigsaw Baseline","Anistropic ImageNet"],"associatedColumns":["Table 10 : Experiments discussing the confidence and entropy of Anistropic ImageNet and Standard","Mean Highest probability"],"associatedMergedColumns":["ImageNet"]},{"number":"1.2M","isBolded":false,"associatedRows":["Jigsaw Baseline"],"associatedColumns":["Table 10 : Experiments discussing the confidence and entropy of Anistropic ImageNet and Standard","Method","Dataset Size"],"associatedMergedColumns":["kept unchanged . This experiment helps evaluate the quality of features learnt by the convolutional"]},{"number":"26.17","isBolded":false,"associatedRows":["Jigsaw Baseline","2?1 . 2M"],"associatedColumns":["Table 10 : Experiments discussing the confidence and entropy of Anistropic ImageNet and Standard","Mean Highest probability","ImageNet Cls . Acc"],"associatedMergedColumns":["kept unchanged . This experiment helps evaluate the quality of features learnt by the convolutional"]},{"number":"61.59","isBolded":false,"associatedRows":["Jigsaw anisotropic","2?1 . 2M"],"associatedColumns":["Table 10 : Experiments discussing the confidence and entropy of Anistropic ImageNet and Standard","Mean Highest probability","VOC Det ."],"associatedMergedColumns":["kept unchanged . This experiment helps evaluate the quality of features learnt by the convolutional"]}]},{"caption":"Table 12: Comparison of our approach with Jigsaw baseline methods. Using our best model, we \nimprove 2.52 mAP in VOC classification , 0.78 mAP on VOC detection and 8.1 mAP on VOC \nsemantic segmentation(SS) over the baseline models. Note that Stylized ImageNet performs poorly \non VOC classification due to the visual shortcuts. \nMethod \nDataset Size VOC Cls. VOC Det. \nSS \n\nBaseline \n1.2M \n74.82 \n61.98 \n27.1 \nStylized (Geirhos et al., 2018) \n1.2M \n13.81 \n28.13 \n10.12 \nGaussian ImageNet \n2?1.2M \n75.49 \n62.39 \n27.9 \nBilateral ImageNet \n2?1.2M \n74.55 \n62.74 \n28.9 \nOnly Anisotropic \n1.2M \n74.52 \n61.85 \n32.7 \nAnisotropic ImageNet \n2?1.2M \n76.77 \n61.59 \n35.2 \nCartoon ImageNet \n2?1.2M \n77.34 \n59.31 \n34.1 \n\n","rows":["Gaussian ImageNet","Bilateral ImageNet","Only Anisotropic","improve","Stylized ( Geirhos et al . , 2018 )","Anisotropic ImageNet","Cartoon ImageNet","Baseline","2?1 . 2M"],"columns":["SS","Using our best model , we","Table 12 : Comparison of our approach with Jigsaw baseline methods .","VOC Cls .","Dataset Size","VOC Det ."],"mergedAllColumns":["on VOC classification due to the visual shortcuts ."],"numberCells":[{"number":"13.81","isBolded":false,"associatedRows":["Stylized ( Geirhos et al . , 2018 )","2?1 . 2M"],"associatedColumns":["Table 12 : Comparison of our approach with Jigsaw baseline methods .","VOC Cls ."],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"75.49","isBolded":false,"associatedRows":["Gaussian ImageNet","2?1 . 2M"],"associatedColumns":["Table 12 : Comparison of our approach with Jigsaw baseline methods .","VOC Cls ."],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"32.7","isBolded":false,"associatedRows":["Only Anisotropic","2?1 . 2M"],"associatedColumns":["Using our best model , we","SS"],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"74.82","isBolded":false,"associatedRows":["Baseline","2?1 . 2M"],"associatedColumns":["Table 12 : Comparison of our approach with Jigsaw baseline methods .","VOC Cls ."],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"74.55","isBolded":false,"associatedRows":["Bilateral ImageNet","2?1 . 2M"],"associatedColumns":["Table 12 : Comparison of our approach with Jigsaw baseline methods .","VOC Cls ."],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"61.59","isBolded":false,"associatedRows":["Anisotropic ImageNet","2?1 . 2M"],"associatedColumns":["Using our best model , we","VOC Det ."],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"27.9","isBolded":false,"associatedRows":["Gaussian ImageNet","2?1 . 2M"],"associatedColumns":["Using our best model , we","SS"],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"0.78mAPonVOCdetectionand","isBolded":false,"associatedRows":["improve","Stylized ( Geirhos et al . , 2018 )"],"associatedColumns":["Table 12 : Comparison of our approach with Jigsaw baseline methods ."],"associatedMergedColumns":[]},{"number":"59.31","isBolded":false,"associatedRows":["Cartoon ImageNet","2?1 . 2M"],"associatedColumns":["Using our best model , we","VOC Det ."],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"77.34","isBolded":true,"associatedRows":["Cartoon ImageNet","2?1 . 2M"],"associatedColumns":["Table 12 : Comparison of our approach with Jigsaw baseline methods .","VOC Cls ."],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"8.1mAPonVOC","isBolded":false,"associatedRows":["improve","Stylized ( Geirhos et al . , 2018 )","2?1 . 2M"],"associatedColumns":["Using our best model , we"],"associatedMergedColumns":[]},{"number":"74.52","isBolded":false,"associatedRows":["Only Anisotropic","2?1 . 2M"],"associatedColumns":["Table 12 : Comparison of our approach with Jigsaw baseline methods .","VOC Cls ."],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"1.2M","isBolded":false,"associatedRows":["Baseline"],"associatedColumns":["Table 12 : Comparison of our approach with Jigsaw baseline methods .","Dataset Size"],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"27.1","isBolded":false,"associatedRows":["Baseline","2?1 . 2M"],"associatedColumns":["Using our best model , we","SS"],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"1.2M","isBolded":false,"associatedRows":["Stylized ( Geirhos et al . , 2018 )"],"associatedColumns":["Table 12 : Comparison of our approach with Jigsaw baseline methods .","Dataset Size"],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"62.74","isBolded":true,"associatedRows":["Bilateral ImageNet","2?1 . 2M"],"associatedColumns":["Using our best model , we","VOC Det ."],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"34.1","isBolded":false,"associatedRows":["Cartoon ImageNet","2?1 . 2M"],"associatedColumns":["Using our best model , we","SS"],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"10.12","isBolded":false,"associatedRows":["Stylized ( Geirhos et al . , 2018 )","2?1 . 2M"],"associatedColumns":["Using our best model , we","SS"],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"35.2","isBolded":true,"associatedRows":["Anisotropic ImageNet","2?1 . 2M"],"associatedColumns":["Using our best model , we","SS"],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"62.39","isBolded":false,"associatedRows":["Gaussian ImageNet","2?1 . 2M"],"associatedColumns":["Using our best model , we","VOC Det ."],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"1.2M","isBolded":false,"associatedRows":["Only Anisotropic"],"associatedColumns":["Table 12 : Comparison of our approach with Jigsaw baseline methods .","Dataset Size"],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"76.77","isBolded":false,"associatedRows":["Anisotropic ImageNet","2?1 . 2M"],"associatedColumns":["Table 12 : Comparison of our approach with Jigsaw baseline methods .","VOC Cls ."],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"61.85","isBolded":false,"associatedRows":["Only Anisotropic","2?1 . 2M"],"associatedColumns":["Using our best model , we","VOC Det ."],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"61.98","isBolded":false,"associatedRows":["Baseline","2?1 . 2M"],"associatedColumns":["Using our best model , we","VOC Det ."],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"28.13","isBolded":false,"associatedRows":["Stylized ( Geirhos et al . , 2018 )","2?1 . 2M"],"associatedColumns":["Using our best model , we","VOC Det ."],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]},{"number":"2.52mAPinVOCclassification,","isBolded":false,"associatedRows":["improve"],"associatedColumns":["Table 12 : Comparison of our approach with Jigsaw baseline methods ."],"associatedMergedColumns":[]},{"number":"28.9","isBolded":false,"associatedRows":["Bilateral ImageNet","2?1 . 2M"],"associatedColumns":["Using our best model , we","SS"],"associatedMergedColumns":["on VOC classification due to the visual shortcuts ."]}]}]