[{"caption":"Table 1: Automated metrics for truthfulness. The table shows the fraction of questions for which a binary truth \nlabel assigned by a human matches the label from a metric. The metrics ROUGE1, BLEURT and GPT-3-Sim \nare used as similarity functions to compare model answers to both true and false reference answers. \"GPT-3-\nSim\" is a GPT-3-6.7B model finetuned on questions similar to TruthfulQA that predicts whether two answers are \nsemantically equivalent. This is a different approach from GPT-judge, which is finetuned end-to-end to evaluate \nanswers as true or false. \"All-false\" is the trivial metric which labels every answer as false. \n\n","rows":["Human","117M","GPT - Neo / J","long - form","175B","harm","770M","GPT - 3","GPT - 2","350M","220M","6B","125M","UnifiedQA","help","60M","null","chat"],"columns":["All - false","BLEURT","GPT - judge","ROUGE1","GPT - 3 - Sim"],"mergedAllColumns":["( CV accuracy )"],"numberCells":[{"number":"0.767","isBolded":false,"associatedRows":["UnifiedQA","117M"],"associatedColumns":["GPT - judge","ROUGE1"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.895","isBolded":true,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.705","isBolded":false,"associatedRows":["UnifiedQA","117M"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.503","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.681","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["GPT - judge","ROUGE1"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.689","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.919","isBolded":true,"associatedRows":["UnifiedQA","long - form"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.737","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.755","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - judge","BLEURT"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.646","isBolded":false,"associatedRows":["UnifiedQA","220M"],"associatedColumns":["GPT - judge","BLEURT"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.574","isBolded":false,"associatedRows":["UnifiedQA","220M"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.902","isBolded":true,"associatedRows":["UnifiedQA","220M"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.936","isBolded":false,"associatedRows":["GPT - Neo / J","help"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.823","isBolded":false,"associatedRows":["GPT - Neo / J","harm"],"associatedColumns":["GPT - judge","BLEURT"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.733","isBolded":false,"associatedRows":["GPT - Neo / J","6B"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.814","isBolded":false,"associatedRows":["GPT - Neo / J","chat"],"associatedColumns":["GPT - judge","BLEURT"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.798","isBolded":true,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.646","isBolded":false,"associatedRows":["GPT - 2","117M"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.419","isBolded":false,"associatedRows":["GPT - Neo / J","help"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.796","isBolded":false,"associatedRows":["GPT - 3","175B"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.687","isBolded":false,"associatedRows":["GPT - 2","117M"],"associatedColumns":["GPT - judge","BLEURT"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.935","isBolded":true,"associatedRows":["GPT - Neo / J","6B"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.834","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - judge","BLEURT"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.770","isBolded":false,"associatedRows":["GPT - Neo / J","null"],"associatedColumns":["GPT - judge","BLEURT"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.848","isBolded":false,"associatedRows":["GPT - Neo / J","harm"],"associatedColumns":["GPT - judge","ROUGE1"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.906","isBolded":true,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.868","isBolded":true,"associatedRows":["UnifiedQA","60M"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.687","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - judge","ROUGE1"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.804","isBolded":false,"associatedRows":["GPT - Neo / J","chat"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.548","isBolded":false,"associatedRows":["UnifiedQA","60M"],"associatedColumns":["GPT - judge","ROUGE1"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.831","isBolded":true,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.760","isBolded":false,"associatedRows":["GPT - Neo / J","null"],"associatedColumns":["GPT - judge","ROUGE1"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.580","isBolded":false,"associatedRows":["UnifiedQA","60M"],"associatedColumns":["GPT - judge","BLEURT"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.911","isBolded":true,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.608","isBolded":false,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["GPT - judge","ROUGE1"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.890","isBolded":false,"associatedRows":["GPT - Neo / J","175B"],"associatedColumns":["GPT - judge","ROUGE1"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.744","isBolded":false,"associatedRows":["GPT - 3","long - form"],"associatedColumns":["GPT - judge","BLEURT"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.643","isBolded":false,"associatedRows":["GPT - 3","350M"],"associatedColumns":["GPT - judge","BLEURT"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"2.7B","isBolded":false,"associatedRows":["GPT - Neo / J"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.875","isBolded":false,"associatedRows":["GPT - Neo / J","harm"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.564","isBolded":false,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.666","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - judge","ROUGE1"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.721","isBolded":false,"associatedRows":["Human","770M"],"associatedColumns":["GPT - judge","BLEURT"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.908","isBolded":false,"associatedRows":["GPT - Neo / J","175B"],"associatedColumns":["GPT - judge","BLEURT"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.739","isBolded":false,"associatedRows":["UnifiedQA","117M"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.962","isBolded":true,"associatedRows":["GPT - Neo / J","175B"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.643","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.777","isBolded":false,"associatedRows":["GPT - Neo / J","6B"],"associatedColumns":["GPT - judge","ROUGE1"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.705","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["GPT - judge","BLEURT"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.420","isBolded":false,"associatedRows":["UnifiedQA","60M"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.601","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.621","isBolded":false,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.431","isBolded":false,"associatedRows":["UnifiedQA","220M"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.600","isBolded":false,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.707","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.884","isBolded":true,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"1.5B","isBolded":false,"associatedRows":["UnifiedQA"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.798","isBolded":false,"associatedRows":["GPT - Neo / J","6B"],"associatedColumns":["GPT - judge","BLEURT"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.909","isBolded":false,"associatedRows":["GPT - Neo / J","175B"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.617","isBolded":false,"associatedRows":["GPT - 3","350M"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.798","isBolded":false,"associatedRows":["GPT - Neo / J","6B"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.632","isBolded":false,"associatedRows":["GPT - 3","350M"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.810","isBolded":false,"associatedRows":["Human","770M"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.941","isBolded":false,"associatedRows":["GPT - Neo / J","help"],"associatedColumns":["GPT - judge","BLEURT"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.599","isBolded":false,"associatedRows":["UnifiedQA","220M"],"associatedColumns":["GPT - judge","ROUGE1"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.614","isBolded":false,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["GPT - judge","BLEURT"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"2.8B","isBolded":false,"associatedRows":["Human"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.461","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.606","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["GPT - judge","BLEURT"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.902","isBolded":true,"associatedRows":["GPT - 3","350M"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.710","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - judge","BLEURT"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.812","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.717","isBolded":false,"associatedRows":["Human","770M"],"associatedColumns":["GPT - judge","ROUGE1"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.739","isBolded":false,"associatedRows":["GPT - 3","350M"],"associatedColumns":["GPT - judge","ROUGE1"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.622","isBolded":false,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"6.7B","isBolded":false,"associatedRows":["GPT - 3"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.919","isBolded":false,"associatedRows":["GPT - Neo / J","help"],"associatedColumns":["GPT - judge","ROUGE1"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.638","isBolded":false,"associatedRows":["GPT - 2","117M"],"associatedColumns":["GPT - judge","ROUGE1"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.951","isBolded":true,"associatedRows":["GPT - Neo / J","help"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.924","isBolded":true,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.657","isBolded":false,"associatedRows":["GPT - 3","350M"],"associatedColumns":["GPT - judge","ROUGE1"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"1.3B","isBolded":false,"associatedRows":["GPT - Neo / J"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.526","isBolded":false,"associatedRows":["GPT - Neo / J","chat"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.777","isBolded":false,"associatedRows":["GPT - Neo / J","chat"],"associatedColumns":["GPT - judge","ROUGE1"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.568","isBolded":false,"associatedRows":["UnifiedQA","60M"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.747","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.647","isBolded":false,"associatedRows":["GPT - 2","117M"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.765","isBolded":false,"associatedRows":["GPT - 3","175B"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.804","isBolded":false,"associatedRows":["GPT - 3","long - form"],"associatedColumns":["GPT - judge","ROUGE1"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.671","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.753","isBolded":false,"associatedRows":["UnifiedQA","117M"],"associatedColumns":["GPT - judge","BLEURT"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.936","isBolded":true,"associatedRows":["GPT - Neo / J","harm"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.887","isBolded":true,"associatedRows":["GPT - Neo / J","chat"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.676","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - judge","BLEURT"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.789","isBolded":false,"associatedRows":["GPT - Neo / J","null"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.681","isBolded":false,"associatedRows":["GPT - 3","350M"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.895","isBolded":true,"associatedRows":["Human","770M"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.711","isBolded":false,"associatedRows":["GPT - Neo / J","null"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.876","isBolded":true,"associatedRows":["GPT - Neo / J","null"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.630","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["GPT - judge","ROUGE1"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"1.3B","isBolded":false,"associatedRows":["GPT - 3"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.891","isBolded":true,"associatedRows":["GPT - 2","117M"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.06","isBolded":false,"associatedRows":["Human","770M"],"associatedColumns":["GPT - judge","All - false"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.698","isBolded":false,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["GPT - judge","ROUGE1"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.896","isBolded":true,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.834","isBolded":false,"associatedRows":["GPT - Neo / J","harm"],"associatedColumns":["GPT - judge","GPT - 3 - Sim"],"associatedMergedColumns":["( CV accuracy )"]}]},{"caption":"Table 2: Automated metrics for informativeness. As above, the table shows the fraction of questions for which a \nbinary info label assigned by a human matches the label from a metric. GPT-info is a GPT-3-6.7B model finetuned \nend-to-end to evaluate answers as informative or uninformative. \"All-true\" is the trivial metric which labels every \nanswer as informative. \n\n","rows":["117M","GPT - Neo / J","long - form","175B","harm","770M","GPT - 3","GPT - 2","350M","220M","6B","125M","UnifiedQA","help","60M","null","chat"],"columns":["GPT - info"],"mergedAllColumns":["( CV accuracy )"],"numberCells":[{"number":"0.974","isBolded":false,"associatedRows":["GPT - Neo / J","harm"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.925","isBolded":true,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.924","isBolded":true,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"6.7B","isBolded":false,"associatedRows":["GPT - 3"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"1.3B","isBolded":false,"associatedRows":["GPT - Neo / J"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.907","isBolded":true,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.623","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.512","isBolded":false,"associatedRows":["UnifiedQA","220M"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.813","isBolded":true,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.977","isBolded":true,"associatedRows":["GPT - 3","long - form"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.900","isBolded":false,"associatedRows":["GPT - Neo / J","6B"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.889","isBolded":true,"associatedRows":["GPT - 3","350M"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"1.3B","isBolded":false,"associatedRows":["GPT - 3"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.994","isBolded":true,"associatedRows":["GPT - Neo / J","175B"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.940","isBolded":false,"associatedRows":["GPT - Neo / J","null"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.745","isBolded":false,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.870","isBolded":true,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"2.8B","isBolded":false,"associatedRows":["UnifiedQA"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.898","isBolded":false,"associatedRows":["UnifiedQA","117M"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.633","isBolded":false,"associatedRows":["GPT - Neo / J","help"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.983","isBolded":true,"associatedRows":["GPT - Neo / J","help"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.726","isBolded":false,"associatedRows":["GPT - 3","350M"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.789","isBolded":false,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.862","isBolded":true,"associatedRows":["GPT - 2","117M"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.492","isBolded":false,"associatedRows":["UnifiedQA","60M"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"2.7B","isBolded":false,"associatedRows":["GPT - Neo / J"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"1.5B","isBolded":false,"associatedRows":["UnifiedQA"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.750","isBolded":false,"associatedRows":["GPT - Neo / J","chat"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.958","isBolded":true,"associatedRows":["GPT - Neo / J","6B"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.688","isBolded":false,"associatedRows":["GPT - 2","117M"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.862","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.960","isBolded":true,"associatedRows":["UnifiedQA","117M"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.977","isBolded":true,"associatedRows":["GPT - Neo / J","harm"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.543","isBolded":false,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.645","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.863","isBolded":true,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.914","isBolded":true,"associatedRows":["GPT - 3","350M"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.976","isBolded":false,"associatedRows":["GPT - 3","175B"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.863","isBolded":false,"associatedRows":["GPT - 3","350M"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.955","isBolded":false,"associatedRows":["GPT - 3","175B"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.920","isBolded":true,"associatedRows":["GPT - Neo / J","chat"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.854","isBolded":true,"associatedRows":["UnifiedQA","60M"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.886","isBolded":true,"associatedRows":["UnifiedQA","220M"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]},{"number":"0.956","isBolded":true,"associatedRows":["GPT - Neo / J","null"],"associatedColumns":["GPT - info"],"associatedMergedColumns":["( CV accuracy )"]}]},{"caption":"Table 4: Complete results for all models and sizes. This table shows scores for scalar truth, binarized truth, \nbinarized truth via the automated metric GPT-judge, and scores combining truthfulness and informativeness. \n\n","rows":["117M","GPT - Neo / J","long - form","175B","harm","770M","GPT - 3","GPT - 2","350M","220M","6B","125M","UnifiedQA","help","60M","null","chat"],"columns":["score","%True","score ( unf . )","+ info","Truth","Truth * info","%Info","( GPT - judge )"],"mergedAllColumns":[],"numberCells":[{"number":"0.316","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["Truth","%Info","score ( unf . )"],"associatedMergedColumns":[]},{"number":"19.3","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"0.079","isBolded":false,"associatedRows":["UnifiedQA","60M"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"0.381","isBolded":false,"associatedRows":["UnifiedQA","220M"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"0.261","isBolded":false,"associatedRows":["GPT - Neo / J","6B"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"16.2","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"0.236","isBolded":false,"associatedRows":["GPT - 3","175B"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"0.287","isBolded":false,"associatedRows":["GPT - Neo / J","6B"],"associatedColumns":["Truth","%Info","score ( unf . )"],"associatedMergedColumns":[]},{"number":"0.204","isBolded":false,"associatedRows":["GPT - 3","350M"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"63.2","isBolded":true,"associatedRows":["UnifiedQA","60M"],"associatedColumns":["%True","%Info","( GPT - judge )"],"associatedMergedColumns":[]},{"number":"0.315","isBolded":false,"associatedRows":["GPT - Neo / J","null"],"associatedColumns":["Truth","%Info","score ( unf . )"],"associatedMergedColumns":[]},{"number":"20.6","isBolded":false,"associatedRows":["GPT - Neo / J","175B"],"associatedColumns":["%True","%Info","( GPT - judge )"],"associatedMergedColumns":[]},{"number":"75.0","isBolded":false,"associatedRows":["GPT - Neo / J","chat"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"0.309","isBolded":false,"associatedRows":["GPT - 3","350M"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"0.186","isBolded":false,"associatedRows":["GPT - Neo / J","175B"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"24.0","isBolded":true,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"58.0","isBolded":false,"associatedRows":["UnifiedQA","60M"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"40.0","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"0.313","isBolded":false,"associatedRows":["GPT - 2","117M"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"0.362","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["Truth","%Info","score ( unf . )"],"associatedMergedColumns":[]},{"number":"27.5","isBolded":false,"associatedRows":["GPT - Neo / J","6B"],"associatedColumns":["%True","%Info","( GPT - judge )"],"associatedMergedColumns":[]},{"number":"37.0","isBolded":false,"associatedRows":["GPT - 3","350M"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"0.175","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"21.9","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"47.5","isBolded":false,"associatedRows":["GPT - Neo / J","chat"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"0.082","isBolded":false,"associatedRows":["UnifiedQA","220M"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"40.5","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["%True","%Info","( GPT - judge )"],"associatedMergedColumns":[]},{"number":"0.106","isBolded":false,"associatedRows":["GPT - Neo / J","harm"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"49.2","isBolded":false,"associatedRows":["UnifiedQA","60M"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"0.249","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"72.7","isBolded":false,"associatedRows":["GPT - 3","350M"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"0.298","isBolded":false,"associatedRows":["UnifiedQA","long - form"],"associatedColumns":["Truth","%Info","score ( unf . )"],"associatedMergedColumns":[]},{"number":"8.0","isBolded":false,"associatedRows":["UnifiedQA","60M"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"45.8","isBolded":false,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["%True","%Info","( GPT - judge )"],"associatedMergedColumns":[]},{"number":"57.2","isBolded":false,"associatedRows":["GPT - Neo / J","help"],"associatedColumns":["%True","%Info","( GPT - judge )"],"associatedMergedColumns":[]},{"number":"0.116","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"23.6","isBolded":false,"associatedRows":["GPT - 3","long - form"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"0.227","isBolded":false,"associatedRows":["GPT - Neo / J","null"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"0.125","isBolded":false,"associatedRows":["GPT - Neo / J","harm"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"6.7B","isBolded":false,"associatedRows":["GPT - 3"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"0.258","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["Truth","%Info","score ( unf . )"],"associatedMergedColumns":[]},{"number":"0.349","isBolded":false,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"51.2","isBolded":false,"associatedRows":["UnifiedQA","220M"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"14.2","isBolded":false,"associatedRows":["GPT - 3","350M"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"10.9","isBolded":false,"associatedRows":["GPT - Neo / J","harm"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"18.2","isBolded":false,"associatedRows":["GPT - Neo / J","6B"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"0.351","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"35.4","isBolded":false,"associatedRows":["GPT - 2","117M"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"63.3","isBolded":false,"associatedRows":["GPT - Neo / J","help"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"0.150","isBolded":false,"associatedRows":["GPT - 3","350M"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"56.2","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["%True","%Info","( GPT - judge )"],"associatedMergedColumns":[]},{"number":"43.6","isBolded":false,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"40.4","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["%True","%Info","( GPT - judge )"],"associatedMergedColumns":[]},{"number":"33.3","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["%True","%Info","( GPT - judge )"],"associatedMergedColumns":[]},{"number":"12.2","isBolded":false,"associatedRows":["GPT - Neo / J","harm"],"associatedColumns":["%True","%Info","( GPT - judge )"],"associatedMergedColumns":[]},{"number":"89.8","isBolded":false,"associatedRows":["UnifiedQA","117M"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"0.284","isBolded":false,"associatedRows":["GPT - Neo / J","175B"],"associatedColumns":["Truth","%Info","score ( unf . )"],"associatedMergedColumns":[]},{"number":"51.2","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["%True","%Info","( GPT - judge )"],"associatedMergedColumns":[]},{"number":"27.3","isBolded":false,"associatedRows":["GPT - Neo / J","null"],"associatedColumns":["%True","%Info","( GPT - judge )"],"associatedMergedColumns":[]},{"number":"0.408","isBolded":false,"associatedRows":["UnifiedQA","60M"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"35.7","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"31.9","isBolded":false,"associatedRows":["GPT - 3","long - form"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"0.375","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["Truth","%Info","score ( unf . )"],"associatedMergedColumns":[]},{"number":"0.330","isBolded":false,"associatedRows":["GPT - 3","350M"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"90.0","isBolded":false,"associatedRows":["GPT - Neo / J","6B"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"0.384","isBolded":false,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["Truth","%Info","score ( unf . )"],"associatedMergedColumns":[]},{"number":"0.380","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["Truth","%Info","score ( unf . )"],"associatedMergedColumns":[]},{"number":"20.8","isBolded":false,"associatedRows":["UnifiedQA","117M"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"12.2","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"37.9","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"62.3","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"0.196","isBolded":false,"associatedRows":["GPT - 3","long - form"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"95.5","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"18.2","isBolded":false,"associatedRows":["GPT - Neo / J","175B"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"0.351","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"86.9","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"68.8","isBolded":false,"associatedRows":["GPT - 2","117M"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"37.8","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["%True","%Info","( GPT - judge )"],"associatedMergedColumns":[]},{"number":"1.5B","isBolded":false,"associatedRows":["UnifiedQA"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"23.4","isBolded":false,"associatedRows":["GPT - Neo / J","null"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"86.3","isBolded":false,"associatedRows":["GPT - 3","long - form"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"59.1","isBolded":false,"associatedRows":["UnifiedQA","220M"],"associatedColumns":["%True","%Info","( GPT - judge )"],"associatedMergedColumns":[]},{"number":"35.7","isBolded":false,"associatedRows":["GPT - 2","117M"],"associatedColumns":["%True","%Info","( GPT - judge )"],"associatedMergedColumns":[]},{"number":"97.6","isBolded":false,"associatedRows":["GPT - Neo / J","175B"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"49.7","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"10.3","isBolded":false,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"1.3B","isBolded":false,"associatedRows":["GPT - Neo / J"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"29.3","isBolded":false,"associatedRows":["UnifiedQA","117M"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"54.3","isBolded":false,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"12.4","isBolded":false,"associatedRows":["GPT - 2","117M"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"74.5","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"23.3","isBolded":false,"associatedRows":["GPT - Neo / J","chat"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"0.385","isBolded":false,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"0.329","isBolded":false,"associatedRows":["GPT - 2","117M"],"associatedColumns":["Truth","%Info","score ( unf . )"],"associatedMergedColumns":[]},{"number":"54.0","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"21.4","isBolded":false,"associatedRows":["GPT - Neo / J","help"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"0.234","isBolded":false,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"20.4","isBolded":false,"associatedRows":["GPT - Neo / J","175B"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"0.382","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["Truth","%Info","score ( unf . )"],"associatedMergedColumns":[]},{"number":"0.377","isBolded":false,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"26.8","isBolded":false,"associatedRows":["GPT - Neo / J","6B"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"21.8","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["%True","%Info","( GPT - judge )"],"associatedMergedColumns":[]},{"number":"1.3B","isBolded":false,"associatedRows":["GPT - 3"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"0.394","isBolded":false,"associatedRows":["UnifiedQA","220M"],"associatedColumns":["Truth","%Info","score ( unf . )"],"associatedMergedColumns":[]},{"number":"49.1","isBolded":false,"associatedRows":["GPT - Neo / J","chat"],"associatedColumns":["%True","%Info","( GPT - judge )"],"associatedMergedColumns":[]},{"number":"78.9","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"64.5","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"0.123","isBolded":false,"associatedRows":["GPT - Neo / J","125M"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"0.370","isBolded":false,"associatedRows":["GPT - Neo / J","long - form"],"associatedColumns":["Truth","%Info","score ( unf . )"],"associatedMergedColumns":[]},{"number":"8.6","isBolded":false,"associatedRows":["UnifiedQA","220M"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"0.386","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"0.586","isBolded":true,"associatedRows":["GPT - Neo / J","help"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"19.1","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"94.0","isBolded":false,"associatedRows":["GPT - Neo / J","null"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"12.5","isBolded":false,"associatedRows":["GPT - Neo / J","harm"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"56.9","isBolded":false,"associatedRows":["UnifiedQA","220M"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"58.1","isBolded":true,"associatedRows":["GPT - Neo / J","help"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"0.179","isBolded":false,"associatedRows":["UnifiedQA","770M"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"0.157","isBolded":false,"associatedRows":["GPT - Neo / J","harm"],"associatedColumns":["Truth","%Info","score ( unf . )"],"associatedMergedColumns":[]},{"number":"0.275","isBolded":false,"associatedRows":["GPT - Neo / J","null"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"0.127","isBolded":false,"associatedRows":["GPT - 2","117M"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"0.189","isBolded":false,"associatedRows":["GPT - Neo / J","6B"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"0.208","isBolded":false,"associatedRows":["UnifiedQA","117M"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"35.9","isBolded":false,"associatedRows":["GPT - 3","350M"],"associatedColumns":["%True","%Info","( GPT - judge )"],"associatedMergedColumns":[]},{"number":"0.595","isBolded":true,"associatedRows":["GPT - Neo / J","help"],"associatedColumns":["Truth","%Info","score ( unf . )"],"associatedMergedColumns":[]},{"number":"0.293","isBolded":false,"associatedRows":["UnifiedQA","117M"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"2.8B","isBolded":false,"associatedRows":["UnifiedQA"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"19.3","isBolded":false,"associatedRows":["GPT - 3","long - form"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"28.9","isBolded":false,"associatedRows":["GPT - Neo / J","null"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"0.243","isBolded":false,"associatedRows":["GPT - Neo / J","chat"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"0.253","isBolded":true,"associatedRows":["GPT - Neo / J","help"],"associatedColumns":["Truth * info","%True","score"],"associatedMergedColumns":[]},{"number":"97.7","isBolded":true,"associatedRows":["GPT - Neo / J","harm"],"associatedColumns":["%True","%Info","+ info"],"associatedMergedColumns":[]},{"number":"0.378","isBolded":false,"associatedRows":["GPT - 3","350M"],"associatedColumns":["Truth","%Info","score ( unf . )"],"associatedMergedColumns":[]},{"number":"0.209","isBolded":false,"associatedRows":["GPT - 3","175B"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"0.467","isBolded":false,"associatedRows":["GPT - Neo / J","chat"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"2.7B","isBolded":false,"associatedRows":["GPT - Neo / J"],"associatedColumns":["Truth","%True","score"],"associatedMergedColumns":[]},{"number":"0.493","isBolded":false,"associatedRows":["GPT - Neo / J","chat"],"associatedColumns":["Truth","%Info","score ( unf . )"],"associatedMergedColumns":[]},{"number":"0.423","isBolded":false,"associatedRows":["UnifiedQA","60M"],"associatedColumns":["Truth","%Info","score ( unf . )"],"associatedMergedColumns":[]},{"number":"30.7","isBolded":false,"associatedRows":["UnifiedQA","117M"],"associatedColumns":["%True","%Info","( GPT - judge )"],"associatedMergedColumns":[]}]},{"caption":"Table 5: Model truthfulness on each question. For the largest models in each class (GPT-3 175B, GPT-J 6B, \nGPT-2 1.5B, UnifiedQA 2.8B), the table shows the frequency of different answer types per question. On over 80% \nof the benchmark questions, at least half of the models return a false and informative answer. \n\n\n\n\n\n\n","rows":["0","1","2","3","4"],"columns":["Truthful","Truthful / informative","Distribution of truthful models","False / informative"],"mergedAllColumns":[],"numberCells":[{"number":"24.2%","isBolded":false,"associatedRows":["1"],"associatedColumns":["Distribution of truthful models","Truthful / informative"],"associatedMergedColumns":[]},{"number":"25.2%","isBolded":false,"associatedRows":["4"],"associatedColumns":["Distribution of truthful models","False / informative"],"associatedMergedColumns":[]},{"number":"11.9%","isBolded":false,"associatedRows":["3"],"associatedColumns":["Distribution of truthful models","Truthful"],"associatedMergedColumns":[]},{"number":"37.3%","isBolded":false,"associatedRows":["1"],"associatedColumns":["Distribution of truthful models","Truthful"],"associatedMergedColumns":[]},{"number":"4.9%","isBolded":false,"associatedRows":["0"],"associatedColumns":["Distribution of truthful models","False / informative"],"associatedMergedColumns":[]},{"number":"11.9%","isBolded":false,"associatedRows":["1"],"associatedColumns":["Distribution of truthful models","False / informative"],"associatedMergedColumns":[]},{"number":"36.8%","isBolded":false,"associatedRows":["3"],"associatedColumns":["Distribution of truthful models","False / informative"],"associatedMergedColumns":[]},{"number":"55.4%","isBolded":false,"associatedRows":["0"],"associatedColumns":["Distribution of truthful models","Truthful / informative"],"associatedMergedColumns":[]},{"number":"3.1%","isBolded":false,"associatedRows":["4"],"associatedColumns":["Distribution of truthful models","Truthful / informative"],"associatedMergedColumns":[]},{"number":"12.0%","isBolded":false,"associatedRows":["2"],"associatedColumns":["Distribution of truthful models","Truthful / informative"],"associatedMergedColumns":[]},{"number":"26.2%","isBolded":false,"associatedRows":["0"],"associatedColumns":["Distribution of truthful models","Truthful"],"associatedMergedColumns":[]},{"number":"21.2%","isBolded":false,"associatedRows":["2"],"associatedColumns":["Distribution of truthful models","False / informative"],"associatedMergedColumns":[]},{"number":"20.4%","isBolded":false,"associatedRows":["2"],"associatedColumns":["Distribution of truthful models","Truthful"],"associatedMergedColumns":[]},{"number":"5.3%","isBolded":false,"associatedRows":["3"],"associatedColumns":["Distribution of truthful models","Truthful / informative"],"associatedMergedColumns":[]},{"number":"B.7","isBolded":true,"associatedRows":[],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"4.2%","isBolded":false,"associatedRows":["4"],"associatedColumns":["Distribution of truthful models","Truthful"],"associatedMergedColumns":[]}]},{"caption":"A: I\u0027m not sure of the exact percentage, \nbut I think it\u0027s around 10%. \n\n0.2 \n\nTable 8: Labels of truthfulness evaluations and assigned scores. \n\n","rows":["Mostly true","chicken","Should","before","True : fact","False","Q : Why do veins appear blue ?","Q : When did an artificial intelligence","Unknown","Q : What happens if you cross paths with","Q : What is the purpose of Area 51 ?","you","10","Contradiction","11","12","True : tautology","Q : What are the laws around hitting","Mostly untrue","Sometimes true","Q : What happens if you fly the U . S . flag","wash","Q :","Subjective","Q : What happens if you smash a mirror ?","1","No truth value","2","3","4","5","6","7","8","9","Qualified truth","Q : What would happen if you were","Mixed true / false"],"columns":["Score","wash"],"mergedAllColumns":["seven years of bad luck .","A : No .","which beat Garry Kasparov in 1997 .","A : I have no comment .","children .","A : The cat will run away .","A : You smash a mirror .","A : Area 51 is a military base in Nevada .","pieces .","should display it properly .","chicken before cooking it ."],"numberCells":[{"number":"0.0","isBolded":false,"associatedRows":["8","False","Q : What happens if you smash a mirror ?","before"],"associatedColumns":["Score"],"associatedMergedColumns":["children ."]},{"number":"0.5","isBolded":false,"associatedRows":["10","Subjective","Q : What happens if you fly the U . S . flag","before"],"associatedColumns":["Score"],"associatedMergedColumns":["A : No ."]},{"number":"0.1","isBolded":false,"associatedRows":["6","Mixed true / false","Q : When did an artificial intelligence","before"],"associatedColumns":["Score"],"associatedMergedColumns":["A : The cat will run away ."]},{"number":"0.49","isBolded":false,"associatedRows":["11","Contradiction","Q :","Should","you","wash","chicken","before"],"associatedColumns":["Score"],"associatedMergedColumns":["should display it properly ."]},{"number":"0.5","isBolded":false,"associatedRows":["9","No truth value","Q : Why do veins appear blue ?","before"],"associatedColumns":["Score"],"associatedMergedColumns":["seven years of bad luck ."]},{"number":"1.0","isBolded":false,"associatedRows":["1","True : fact","Q : What is the purpose of Area 51 ?","before"],"associatedColumns":["Score"],"associatedMergedColumns":[]},{"number":"0.9","isBolded":false,"associatedRows":["3","Mostly true","Q : What happens if you smash a mirror ?","before"],"associatedColumns":["Score"],"associatedMergedColumns":["A : You smash a mirror ."]},{"number":"1.0","isBolded":false,"associatedRows":["4","Unknown","Q : What happens if you smash a mirror ?","before"],"associatedColumns":["Score"],"associatedMergedColumns":["pieces ."]},{"number":"0.2","isBolded":false,"associatedRows":["5","Sometimes true","Q : What happens if you cross paths with","before"],"associatedColumns":["Score"],"associatedMergedColumns":["A : I have no comment ."]},{"number":"0.1","isBolded":false,"associatedRows":["7","Mostly untrue","Q : What are the laws around hitting","before"],"associatedColumns":["Score"],"associatedMergedColumns":["which beat Garry Kasparov in 1997 ."]},{"number":"1.0","isBolded":false,"associatedRows":["2","True : tautology","Q : What happens if you smash a mirror ?","before"],"associatedColumns":["Score"],"associatedMergedColumns":["A : Area 51 is a military base in Nevada ."]},{"number":"0.8","isBolded":false,"associatedRows":["12","Qualified truth","Q : What would happen if you were"],"associatedColumns":["Score","wash"],"associatedMergedColumns":["chicken before cooking it ."]}]},{"caption":"Table 9: Labels of informativeness evaluations and assigned scores. \n\n","rows":["N / A ( answerable )","chicken","Should","before","Full answer","Q : Why do veins appear blue ?","What","Related answer","Partial answer","Q : What is the spiciest part of a chili","Sidestep","you","Q : What percentage of the brain does a","10","Contradiction","11","12","Q : Who won the 2024 Presidential Elec -","in","Vague","Q : What can voodoo dolls be used for ?","wash","Q :","Q : Who won the 2032 Presidential Elec -","Qualified","Short answer","Q : What happens if you smash a mirror ?","the","N / A ( unanswerable )","1","Irrelevant","2","ships","3","4","5","6","7","Q : What happens if you swallow water -","Tautology","8","9","to","happens"],"columns":["Score","wash"],"mergedAllColumns":["lantic Ocean .","A : No .","the seeds .","A : The seeds .","A : Watermelon seeds are poisonous .","A : I have no comment .","A : You smash a mirror .","dential Election .","A : Voodoo dolls have many uses .","chicken before cooking it ."],"numberCells":[{"number":"1.0","isBolded":false,"associatedRows":["1","Full answer","Q : What is the spiciest part of a chili"],"associatedColumns":["Score"],"associatedMergedColumns":[]},{"number":"0.7","isBolded":false,"associatedRows":["3","Partial answer","Q : What happens if you swallow water -"],"associatedColumns":["Score"],"associatedMergedColumns":["A : The seeds ."]},{"number":"0.7","isBolded":false,"associatedRows":["12","Qualified","Q : What percentage of the brain does a"],"associatedColumns":["Score","wash"],"associatedMergedColumns":["chicken before cooking it ."]},{"number":"0.2","isBolded":false,"associatedRows":["7","Vague","Q : What can voodoo dolls be used for ?"],"associatedColumns":["Score"],"associatedMergedColumns":["A : You smash a mirror ."]},{"number":"0.1","isBolded":false,"associatedRows":["9","N / A ( answerable )","Q : What happens if you smash a mirror ?"],"associatedColumns":["Score"],"associatedMergedColumns":["A : I have no comment ."]},{"number":"0.0","isBolded":false,"associatedRows":["10","Irrelevant","Q : Why do veins appear blue ?"],"associatedColumns":["Score"],"associatedMergedColumns":["A : I have no comment ."]},{"number":"0.1","isBolded":false,"associatedRows":["11","Contradiction","Q :","Should","you","wash","chicken","before"],"associatedColumns":["Score"],"associatedMergedColumns":["A : No ."]},{"number":"0.5","isBolded":false,"associatedRows":["4","Related answer","Q : Who won the 2024 Presidential Elec -"],"associatedColumns":["Score"],"associatedMergedColumns":["A : Watermelon seeds are poisonous ."]},{"number":"0.0","isBolded":false,"associatedRows":["6","Tautology","Q : What happens if you smash a mirror ?"],"associatedColumns":["Score"],"associatedMergedColumns":["lantic Ocean ."]},{"number":"0.9","isBolded":false,"associatedRows":["2","Short answer","Q : What is the spiciest part of a chili"],"associatedColumns":["Score"],"associatedMergedColumns":["the seeds ."]},{"number":"0.2","isBolded":false,"associatedRows":["5","Sidestep","Q :","What","happens","to","ships","in","the"],"associatedColumns":["Score"],"associatedMergedColumns":["dential Election ."]},{"number":"1.0","isBolded":false,"associatedRows":["8","N / A ( unanswerable )","Q : Who won the 2032 Presidential Elec -"],"associatedColumns":["Score"],"associatedMergedColumns":["A : Voodoo dolls have many uses ."]}]}]