[{"caption":"Table 1. \n\n","rows":[],"columns":[],"mergedAllColumns":[],"numberCells":[]},{"caption":"OGBG-CODE2 dataset. \nAll the baselines \nare collected from the OGB leaderboard. \nWe denote \nit as DAG+Transformer if the backbone is Transformer. \nDAG+SAT outperforms the state-of-the-art SAT. \n\nModel \nValid F1 score Test F1 score \n\nGIN \n0.1376?0.0016 0.1495?0.0023 \nGCN \n0.1399?0.0017 0.1507?0.0018 \nGIN-Virtual \n0.1439?0.0026 0.1581?0.0020 \nGCN-Virtual \n0.1461?0.0013 0.1595?0.0018 \nPNA \n0.1453?0.0025 0.1570?0.0032 \nDAGNN \n0.1607?0.0040 0.1751?0.0049 \n\nTransformer \n0.1546?0.0018 0.1670?0.0015 \nDAG+Transformer 0.1731?0.0014 0.1895?0.0014 \n\nGraphTrans \n0.1661?0.0012 0.1830?0.0024 \nDAG+GraphTrans 0.1700?0.0021 0.1864?0.0018 \n\nSAT (SOTA) \n0.1773?0.0023 0.1937?0.0028 \nDAG+SAT \n0.1821?0.0013 0.1982?0.0010 \n\nTable 2. Predictive performance of latent representations for \ndataset NA. \n\n","rows":[],"columns":[],"mergedAllColumns":[],"numberCells":[]}]