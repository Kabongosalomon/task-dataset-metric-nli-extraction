[{"caption":"IE Models \n\nSpace Complexity Time Complexity NER RE \nPointerNet (Katiyar and Cardie, 2017) O(n) \nO(n 2 ) \n82.6 55.9 \nSpanRE (Dixit and Al-Onaizan, 2019) O(n) \nO(n 2 ) \n86.0 62.8 \nDygie++ (Wadden et al., 2019)  O(n) \nO(n 2 ) \n88.6 63.4 \nOneIE (Lin et al., 2020)  O(n) \nO(n 2 ) \n88.8 67.5 \nTabSeq (Wang and Lu, 2020)  O(n 2 ) \nO(n) \n89.5 67.6 \nHySPA (ours) w/ RoBERTa \nO(n) \nO(n) \n88.9 68.2 \nw/ ALBERT \n89.9 68.0 \n\nTable 1: Joint NER and RE F1 scores of the IE models on the ACE05 test set. Complexities are calculated for the \nentity and relation decoding part of the models (n is the length of the input text). The performance of the TabSeq \nmodel reported here is based on the same ALBERT-xxlarge (Lan et al., 2020) pretrained language model as ours. \n\n","rows":["w / ALBERT","O ( n )","O ( n 2 )","Dygie++ ( Wadden et al . , 2019 )","TabSeq ( Wang and Lu , 2020 )","OneIE ( Lin et al . , 2020 )","w / RoBERTa","PointerNet ( Katiyar and Cardie , 2017 )","HySPA ( ours )","SpanRE ( Dixit and Al - Onaizan , 2019 )"],"columns":["RE","NER"],"mergedAllColumns":[],"numberCells":[{"number":"88.6","isBolded":false,"associatedRows":["Dygie++ ( Wadden et al . , 2019 )","O ( n )","O ( n 2 )"],"associatedColumns":["NER"],"associatedMergedColumns":[]},{"number":"82.6","isBolded":false,"associatedRows":["PointerNet ( Katiyar and Cardie , 2017 )","O ( n )","O ( n 2 )"],"associatedColumns":["NER"],"associatedMergedColumns":[]},{"number":"62.8","isBolded":false,"associatedRows":["SpanRE ( Dixit and Al - Onaizan , 2019 )","O ( n )","O ( n 2 )"],"associatedColumns":["RE"],"associatedMergedColumns":[]},{"number":"88.8","isBolded":false,"associatedRows":["OneIE ( Lin et al . , 2020 )","O ( n )","O ( n 2 )"],"associatedColumns":["NER"],"associatedMergedColumns":[]},{"number":"67.6","isBolded":false,"associatedRows":["TabSeq ( Wang and Lu , 2020 )","O ( n 2 )","O ( n )"],"associatedColumns":["RE"],"associatedMergedColumns":[]},{"number":"67.5","isBolded":false,"associatedRows":["OneIE ( Lin et al . , 2020 )","O ( n )","O ( n 2 )"],"associatedColumns":["RE"],"associatedMergedColumns":[]},{"number":"63.4","isBolded":false,"associatedRows":["Dygie++ ( Wadden et al . , 2019 )","O ( n )","O ( n 2 )"],"associatedColumns":["RE"],"associatedMergedColumns":[]},{"number":"68.2","isBolded":true,"associatedRows":["HySPA ( ours )","w / RoBERTa","O ( n 2 )","O ( n 2 )"],"associatedColumns":["RE"],"associatedMergedColumns":[]},{"number":"86.0","isBolded":false,"associatedRows":["SpanRE ( Dixit and Al - Onaizan , 2019 )","O ( n )","O ( n 2 )"],"associatedColumns":["NER"],"associatedMergedColumns":[]},{"number":"88.9","isBolded":false,"associatedRows":["HySPA ( ours )","w / RoBERTa","O ( n 2 )","O ( n 2 )"],"associatedColumns":["NER"],"associatedMergedColumns":[]},{"number":"55.9","isBolded":false,"associatedRows":["PointerNet ( Katiyar and Cardie , 2017 )","O ( n )","O ( n 2 )"],"associatedColumns":["RE"],"associatedMergedColumns":[]},{"number":"89.5","isBolded":false,"associatedRows":["TabSeq ( Wang and Lu , 2020 )","O ( n 2 )","O ( n )"],"associatedColumns":["NER"],"associatedMergedColumns":[]},{"number":"89.9","isBolded":true,"associatedRows":["w / ALBERT","O ( n 2 )","O ( n )"],"associatedColumns":["NER"],"associatedMergedColumns":[]},{"number":"68.0","isBolded":true,"associatedRows":["w / ALBERT","O ( n 2 )","O ( n 2 )"],"associatedColumns":["RE"],"associatedMergedColumns":[]}]},{"caption":"Table 2: Ablation study on the ACE05 test set. \"-\nTraversal-embedding\": we remove the traversal embed-\nding and instead use sinusoidal position embedding, \nand the following ablations are based on the model af-\nter this ablation. \"-Masking\": we remove the alter-\nnating mask from the hybrid span decoder. \"-BFS\": \nwe use DFS instead of BFS as traversal. \"-Mixed-\nattention\": we remove the mixed-attention layer and \nuse a standard transformer encoder decoder structure. \n\"-Span-attention\": we remove the span attention in the \nspan encoding module and instead average the words \nin the span. \n\n","rows":["- Mixed - attention","- Traversal - embedding","- BFS","- Span - attention","HySPA w / RoBERTa","- Masking"],"columns":["RE F1","NER F1"],"mergedAllColumns":[],"numberCells":[{"number":"64.8","isBolded":false,"associatedRows":["- Masking"],"associatedColumns":["RE F1"],"associatedMergedColumns":[]},{"number":"88.9","isBolded":false,"associatedRows":["HySPA w / RoBERTa"],"associatedColumns":["NER F1"],"associatedMergedColumns":[]},{"number":"66.1","isBolded":false,"associatedRows":["- Span - attention"],"associatedColumns":["RE F1"],"associatedMergedColumns":[]},{"number":"66.7","isBolded":false,"associatedRows":["- Traversal - embedding"],"associatedColumns":["RE F1"],"associatedMergedColumns":[]},{"number":"68.2","isBolded":false,"associatedRows":["HySPA w / RoBERTa"],"associatedColumns":["RE F1"],"associatedMergedColumns":[]},{"number":"88.1","isBolded":false,"associatedRows":["- Masking"],"associatedColumns":["NER F1"],"associatedMergedColumns":[]},{"number":"64.7","isBolded":false,"associatedRows":["- Mixed - attention"],"associatedColumns":["RE F1"],"associatedMergedColumns":[]},{"number":"88.5","isBolded":false,"associatedRows":["- Span - attention"],"associatedColumns":["NER F1"],"associatedMergedColumns":[]},{"number":"88.9","isBolded":false,"associatedRows":["- Traversal - embedding"],"associatedColumns":["NER F1"],"associatedMergedColumns":[]},{"number":"88.7","isBolded":false,"associatedRows":["- BFS"],"associatedColumns":["NER F1"],"associatedMergedColumns":[]},{"number":"88.6","isBolded":false,"associatedRows":["- Mixed - attention"],"associatedColumns":["NER F1"],"associatedMergedColumns":[]},{"number":"66.2","isBolded":false,"associatedRows":["- BFS"],"associatedColumns":["RE F1"],"associatedMergedColumns":[]}]}]