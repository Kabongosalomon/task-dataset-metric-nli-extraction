[{"caption":"Table 1: We use the F1 score on SQuAD-v1 as the accuracy metric. The baseline F1 score is the \nscore obtained by the pre-trained model (BERT-Large) provided on BERT\u0027s public repository (as of \nFebruary 1st, 2019). We use TPUv3s in our experiments. We use the same setting as the baseline: the \nfirst 9/10 of the total epochs used a sequence length of 128 and the last 1/10 of the total epochs used \na sequence length of 512. All the experiments run the same number of epochs. Dev set means the test \ndata. It is worth noting that we can achieve better results by manually tuning the hyperparameters. \nThe data in this ","rows":["125k","64k / 32k","1000k","16","16k","8599","LAMB","250k","62500","31250","Baseline","32k","500k","8k","256","512","1024","4k","128","2k","1k","64","15625","32"],"columns":["F1 score on dev set","table is collected from the untuned version .","Time"],"mergedAllColumns":[],"numberCells":[{"number":"693.6m","isBolded":false,"associatedRows":["LAMB","4k","125k","128"],"associatedColumns":["table is collected from the untuned version .","Time"],"associatedMergedColumns":[]},{"number":"90.395","isBolded":false,"associatedRows":["Baseline","512","1000k"],"associatedColumns":["table is collected from the untuned version .","F1 score on dev set"],"associatedMergedColumns":[]},{"number":"91.946","isBolded":false,"associatedRows":["LAMB","2k","250k"],"associatedColumns":["table is collected from the untuned version .","F1 score on dev set"],"associatedMergedColumns":[]},{"number":"90.584","isBolded":false,"associatedRows":["LAMB","64k / 32k","8599"],"associatedColumns":["table is collected from the untuned version .","F1 score on dev set"],"associatedMergedColumns":[]},{"number":"81.4h","isBolded":false,"associatedRows":["Baseline","512","1000k","16"],"associatedColumns":["table is collected from the untuned version .","Time"],"associatedMergedColumns":[]},{"number":"200.0m","isBolded":false,"associatedRows":["LAMB","16k","31250","512"],"associatedColumns":["table is collected from the untuned version .","Time"],"associatedMergedColumns":[]},{"number":"101.2m","isBolded":false,"associatedRows":["LAMB","32k","15625","1024"],"associatedColumns":["table is collected from the untuned version .","Time"],"associatedMergedColumns":[]},{"number":"91.752","isBolded":false,"associatedRows":["LAMB","512","1000k"],"associatedColumns":["table is collected from the untuned version .","F1 score on dev set"],"associatedMergedColumns":[]},{"number":"76.19m","isBolded":false,"associatedRows":["LAMB","64k / 32k","8599","1024"],"associatedColumns":["table is collected from the untuned version .","Time"],"associatedMergedColumns":[]},{"number":"91.345","isBolded":false,"associatedRows":["LAMB","16k","31250"],"associatedColumns":["table is collected from the untuned version .","F1 score on dev set"],"associatedMergedColumns":[]},{"number":"91.761","isBolded":false,"associatedRows":["LAMB","1k","500k"],"associatedColumns":["table is collected from the untuned version .","F1 score on dev set"],"associatedMergedColumns":[]},{"number":"390.5m","isBolded":false,"associatedRows":["LAMB","8k","62500","256"],"associatedColumns":["table is collected from the untuned version .","Time"],"associatedMergedColumns":[]},{"number":"82.8h","isBolded":false,"associatedRows":["LAMB","512","1000k","16"],"associatedColumns":["table is collected from the untuned version .","Time"],"associatedMergedColumns":[]},{"number":"91.137","isBolded":false,"associatedRows":["LAMB","4k","125k"],"associatedColumns":["table is collected from the untuned version .","F1 score on dev set"],"associatedMergedColumns":[]},{"number":"21.4h","isBolded":false,"associatedRows":["LAMB","2k","250k","64"],"associatedColumns":["table is collected from the untuned version .","Time"],"associatedMergedColumns":[]},{"number":"43.2h","isBolded":false,"associatedRows":["LAMB","1k","500k","32"],"associatedColumns":["table is collected from the untuned version .","Time"],"associatedMergedColumns":[]},{"number":"91.263","isBolded":false,"associatedRows":["LAMB","8k","62500"],"associatedColumns":["table is collected from the untuned version .","F1 score on dev set"],"associatedMergedColumns":[]},{"number":"91.475","isBolded":false,"associatedRows":["LAMB","32k","15625"],"associatedColumns":["table is collected from the untuned version .","F1 score on dev set"],"associatedMergedColumns":[]}]},{"caption":"Table 2: LAMB achieves a higher performance (F1 score) than LARS for all the batch sizes. The \nbaseline achieves a F1 score of 90.390. Thus, LARS stops scaling at the batch size of 16K. \nBatch Size \n512 \n1K \n2K \n4K \n8K \n16K \n32K \n\nLARS \n90.717 90.369 90.748 90.537 90.548 89.589 diverge \nLAMB \n91.752 91.761 91.946 91.137 91.263 91.345 91.475 \n\n4.2 IMAGENET TRAINING WITH RESNET-50. \n\n","rows":["LARS","baseline achieves a F1 score of","LAMB"],"columns":["8K","512","16K","4K","Batch Size","2K","Table 2 :","1K","LAMB achieves a higher performance ( F1 score ) than LARS for all the batch sizes . The","32K"],"mergedAllColumns":["diverge"],"numberCells":[{"number":"91.137","isBolded":false,"associatedRows":["LAMB"],"associatedColumns":["LAMB achieves a higher performance ( F1 score ) than LARS for all the batch sizes . The","4K"],"associatedMergedColumns":["diverge"]},{"number":"91.263","isBolded":false,"associatedRows":["LAMB"],"associatedColumns":["LAMB achieves a higher performance ( F1 score ) than LARS for all the batch sizes . The","8K"],"associatedMergedColumns":["diverge"]},{"number":"91.345","isBolded":false,"associatedRows":["LAMB"],"associatedColumns":["LAMB achieves a higher performance ( F1 score ) than LARS for all the batch sizes . The","16K"],"associatedMergedColumns":["diverge"]},{"number":"90.537","isBolded":false,"associatedRows":["LARS"],"associatedColumns":["LAMB achieves a higher performance ( F1 score ) than LARS for all the batch sizes . The","4K"],"associatedMergedColumns":[]},{"number":"90.369","isBolded":false,"associatedRows":["LARS"],"associatedColumns":["LAMB achieves a higher performance ( F1 score ) than LARS for all the batch sizes . The","1K"],"associatedMergedColumns":[]},{"number":"91.752","isBolded":false,"associatedRows":["LAMB"],"associatedColumns":["LAMB achieves a higher performance ( F1 score ) than LARS for all the batch sizes . The","512"],"associatedMergedColumns":["diverge"]},{"number":"91.761","isBolded":false,"associatedRows":["LAMB"],"associatedColumns":["LAMB achieves a higher performance ( F1 score ) than LARS for all the batch sizes . The","1K"],"associatedMergedColumns":["diverge"]},{"number":"91.475","isBolded":false,"associatedRows":["LAMB"],"associatedColumns":["LAMB achieves a higher performance ( F1 score ) than LARS for all the batch sizes . The","32K"],"associatedMergedColumns":["diverge"]},{"number":"90.717","isBolded":false,"associatedRows":["LARS"],"associatedColumns":["LAMB achieves a higher performance ( F1 score ) than LARS for all the batch sizes . The","512"],"associatedMergedColumns":[]},{"number":"91.946","isBolded":false,"associatedRows":["LAMB"],"associatedColumns":["LAMB achieves a higher performance ( F1 score ) than LARS for all the batch sizes . The","2K"],"associatedMergedColumns":["diverge"]},{"number":"90.390.Thus,LARSstopsscalingatthebatchsizeof16K.","isBolded":false,"associatedRows":["baseline achieves a F1 score of"],"associatedColumns":["LAMB achieves a higher performance ( F1 score ) than LARS for all the batch sizes . The"],"associatedMergedColumns":[]},{"number":"4.2","isBolded":false,"associatedRows":[],"associatedColumns":["Table 2 :","Batch Size"],"associatedMergedColumns":["diverge"]},{"number":"90.748","isBolded":false,"associatedRows":["LARS"],"associatedColumns":["LAMB achieves a higher performance ( F1 score ) than LARS for all the batch sizes . The","2K"],"associatedMergedColumns":[]},{"number":"90.548","isBolded":false,"associatedRows":["LARS"],"associatedColumns":["LAMB achieves a higher performance ( F1 score ) than LARS for all the batch sizes . The","8K"],"associatedMergedColumns":[]},{"number":"89.589","isBolded":false,"associatedRows":["LARS"],"associatedColumns":["LAMB achieves a higher performance ( F1 score ) than LARS for all the batch sizes . The","16K"],"associatedMergedColumns":[]}]},{"caption":"Table 3: Top-1 validation accuracy of ImageNet/RESNET-50 training at the batch size of 16K (90 \nepochs). The performance of momentum was reported by ","rows":["0 . 5538 / 0 . 7201","Accuracy","0 . 6727 / 0 . 7307","0 . 6604 / 0 . 7348"],"columns":["optimizer","lamb","The tuning","momentum"],"mergedAllColumns":["information was in the Appendix ."],"numberCells":[{"number":"0.7520","isBolded":false,"associatedRows":["Accuracy","0 . 5538 / 0 . 7201","0 . 6604 / 0 . 7348","0 . 6727 / 0 . 7307"],"associatedColumns":["The tuning","momentum"],"associatedMergedColumns":["information was in the Appendix ."]},{"number":"4.3","isBolded":false,"associatedRows":[],"associatedColumns":["The tuning","optimizer"],"associatedMergedColumns":["information was in the Appendix ."]},{"number":"0.7666","isBolded":false,"associatedRows":["Accuracy","0 . 5538 / 0 . 7201","0 . 6604 / 0 . 7348","0 . 6727 / 0 . 7307"],"associatedColumns":["The tuning","lamb"],"associatedMergedColumns":["information was in the Appendix ."]}]},{"caption":"Table 4: Untuned LAMB for BERT training across different batch sizes (fixed #epochs). We use \nsquare root LR scaling and linear-epoch warmup. For example, batch size 32K needs to finish 15625 \niterations. It uses 0.2?15625 \u003d 3125 iterations for learning rate warmup. BERT\u0027s baseline achieved a \nF1 score of 90.395. We can achieve an even higher F1 score if we manually tune the hyperparameters. \nBatch Size \n512 \n1K \n2K \n4K \n8K \n16K \n32K \n\nLearning Rate \n\n","rows":["Exact Match","F1 score","F1 score of"],"columns":["Table 4 : Untuned LAMB for BERT training across different batch sizes ( fixed #epochs ) . We use","16K","160","32K","1","320","8K","5","512","4K","80","2K","1K","40","20","10"],"mergedAllColumns":["iterations . It uses 0 . 2?15625 \u003d 3125 iterations for learning rate warmup . BERT \u0027 s baseline achieved a","Learning Rate"],"numberCells":[{"number":"84.939","isBolded":false,"associatedRows":["Exact Match","F1 score"],"associatedColumns":["Table 4 : Untuned LAMB for BERT training across different batch sizes ( fixed #epochs ) . We use","32K","1","5"],"associatedMergedColumns":["Learning Rate"]},{"number":"91.345","isBolded":false,"associatedRows":["F1 score"],"associatedColumns":["Table 4 : Untuned LAMB for BERT training across different batch sizes ( fixed #epochs ) . We use","16K","1","10"],"associatedMergedColumns":["Learning Rate"]},{"number":"84.901","isBolded":false,"associatedRows":["Exact Match","F1 score"],"associatedColumns":["Table 4 : Untuned LAMB for BERT training across different batch sizes ( fixed #epochs ) . We use","8K","1","20"],"associatedMergedColumns":["Learning Rate"]},{"number":"85.260","isBolded":false,"associatedRows":["Exact Match","F1 score"],"associatedColumns":["Table 4 : Untuned LAMB for BERT training across different batch sizes ( fixed #epochs ) . We use","1K","1","160"],"associatedMergedColumns":["Learning Rate"]},{"number":"84.172","isBolded":false,"associatedRows":["Exact Match","F1 score"],"associatedColumns":["Table 4 : Untuned LAMB for BERT training across different batch sizes ( fixed #epochs ) . We use","4K","1","40"],"associatedMergedColumns":["Learning Rate"]},{"number":"91.475","isBolded":false,"associatedRows":["F1 score"],"associatedColumns":["Table 4 : Untuned LAMB for BERT training across different batch sizes ( fixed #epochs ) . We use","32K","1","5"],"associatedMergedColumns":["Learning Rate"]},{"number":"85.090","isBolded":false,"associatedRows":["Exact Match","F1 score"],"associatedColumns":["Table 4 : Untuned LAMB for BERT training across different batch sizes ( fixed #epochs ) . We use","512","1","320"],"associatedMergedColumns":["Learning Rate"]},{"number":"84.816","isBolded":false,"associatedRows":["Exact Match","F1 score"],"associatedColumns":["Table 4 : Untuned LAMB for BERT training across different batch sizes ( fixed #epochs ) . We use","16K","1","10"],"associatedMergedColumns":["Learning Rate"]},{"number":"91.263","isBolded":false,"associatedRows":["F1 score"],"associatedColumns":["Table 4 : Untuned LAMB for BERT training across different batch sizes ( fixed #epochs ) . We use","8K","1","20"],"associatedMergedColumns":["Learning Rate"]},{"number":"90.395.WecanachieveanevenhigherF1scoreifwemanuallytunethehyperparameters.","isBolded":false,"associatedRows":["F1 score of"],"associatedColumns":["Table 4 : Untuned LAMB for BERT training across different batch sizes ( fixed #epochs ) . We use"],"associatedMergedColumns":["iterations . It uses 0 . 2?15625 \u003d 3125 iterations for learning rate warmup . BERT \u0027 s baseline achieved a"]},{"number":"85.355","isBolded":false,"associatedRows":["Exact Match","F1 score"],"associatedColumns":["Table 4 : Untuned LAMB for BERT training across different batch sizes ( fixed #epochs ) . We use","2K","1","80"],"associatedMergedColumns":["Learning Rate"]},{"number":"91.761","isBolded":false,"associatedRows":["F1 score"],"associatedColumns":["Table 4 : Untuned LAMB for BERT training across different batch sizes ( fixed #epochs ) . We use","1K","1","160"],"associatedMergedColumns":["Learning Rate"]},{"number":"91.946","isBolded":false,"associatedRows":["F1 score"],"associatedColumns":["Table 4 : Untuned LAMB for BERT training across different batch sizes ( fixed #epochs ) . We use","2K","1","80"],"associatedMergedColumns":["Learning Rate"]},{"number":"91.137","isBolded":false,"associatedRows":["F1 score"],"associatedColumns":["Table 4 : Untuned LAMB for BERT training across different batch sizes ( fixed #epochs ) . We use","4K","1","40"],"associatedMergedColumns":["Learning Rate"]},{"number":"91.752","isBolded":false,"associatedRows":["F1 score"],"associatedColumns":["Table 4 : Untuned LAMB for BERT training across different batch sizes ( fixed #epochs ) . We use","512","1","320"],"associatedMergedColumns":["Learning Rate"]}]},{"caption":"Table 5: Untuned LAMB for ImageNet training with RESNET-50 for different batch sizes (90 epochs). \nWe use square root LR scaling and linear-epoch warmup. The baseline Goyal et al. (2017) gets 76.3% \ntop-1 accuracy in 90 epochs. Stanford DAWN Bench (Coleman et al., 2017) baseline achieves 93% \ntop-5 accuracy. LAMB achieves both of them. LAMB can achieve an even higher accuracy if we \nmanually tune the hyperparameters. \nBatch Size \n512 \n1K \n2K \n4K \n8K \n16K \n32K \n\nLearning Rate \n\n4 \n2 3.0 ?100 \n\n4 \n2 2.5 ?100 \n\n4 \n2 2.0 ?100 \n\n4 \n2 1.5 ?100 \n\n4 \n2 1.0 ?100 \n\n4 \n2 0.5 ?100 \n\n4 \n2 0.0 ?100 \n\nWarmup Epochs \n0.3125 \n0.625 \n1.25 \n2.5 \n5 \n10 \n20 \nTop-5 Accuracy \n0.9335 \n0.9349 \n0.9353 \n0.9332 \n0.9331 \n0.9322 \n0.9308 \nTop-1 Accuracy \n0.7696 \n0.7706 \n0.7711 \n0.7692 \n0.7689 \n0.7666 \n0.7642 \n\n","rows":["Top - 5 Accuracy","Top - 1 Accuracy","Exact Match","Warmup Epochs","160","320","We use square root LR scaling and linear - epoch warmup . The baseline Goyal et al . ( 2017 ) gets","Learning Rate","80","40","F1 score","20","10"],"columns":["8K","512","16K","4K","2K","1K","Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","32K","20","10"],"mergedAllColumns":["manually tune the hyperparameters ."],"numberCells":[{"number":"1.5?100","isBolded":true,"associatedRows":["Learning Rate","320","160","80"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","4K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"0.9349","isBolded":false,"associatedRows":["Top - 5 Accuracy","320"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","1K","10"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"2","isBolded":false,"associatedRows":["Learning Rate","320"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","1K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"1","isBolded":false,"associatedRows":["Exact Match","320","160"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["Learning Rate"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","512"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"2.5","isBolded":false,"associatedRows":["Warmup Epochs","320","160","80"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","4K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"91.752","isBolded":false,"associatedRows":["F1 score"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.7642","isBolded":false,"associatedRows":["Top - 1 Accuracy","320","160","80","40","20","10"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","32K","20"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"0.9308","isBolded":false,"associatedRows":["Top - 5 Accuracy","320","160","80","40","20","10"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","32K","20"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"0.7689","isBolded":false,"associatedRows":["Top - 1 Accuracy","320","160","80","40"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","8K","10"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"0.9353","isBolded":false,"associatedRows":["Top - 5 Accuracy","320","160"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","2K","10"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"0.9331","isBolded":false,"associatedRows":["Top - 5 Accuracy","320","160","80","40"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","8K","10"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"1","isBolded":false,"associatedRows":["Exact Match"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"76.3%","isBolded":false,"associatedRows":["We use square root LR scaling and linear - epoch warmup . The baseline Goyal et al . ( 2017 ) gets"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) ."],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["Learning Rate","320","160"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","2K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"0.625","isBolded":false,"associatedRows":["Warmup Epochs","320"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","1K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"5","isBolded":false,"associatedRows":["Exact Match","320","160","80","40","20","10"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"84.816","isBolded":false,"associatedRows":["Exact Match","320","160","80","40","20"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["We use square root LR scaling and linear - epoch warmup . The baseline Goyal et al . ( 2017 ) gets","Learning Rate"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","32K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"1","isBolded":false,"associatedRows":["Exact Match","320","160","80","40"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"91.345","isBolded":false,"associatedRows":["F1 score","320","160","80","40","20"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"84.939","isBolded":false,"associatedRows":["Exact Match","320","160","80","40","20","10"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["Learning Rate","320","160","80","40","20","10"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","32K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"91.263","isBolded":false,"associatedRows":["F1 score","320","160","80","40"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"3.0?100","isBolded":true,"associatedRows":["Learning Rate"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","512"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"91.137","isBolded":false,"associatedRows":["F1 score","320","160","80"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.7666","isBolded":false,"associatedRows":["Top - 1 Accuracy","320","160","80","40","20"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","16K","10"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"84.172","isBolded":false,"associatedRows":["Exact Match","320","160","80"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.7706","isBolded":false,"associatedRows":["Top - 1 Accuracy","320"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","1K","10"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"0.0?100","isBolded":true,"associatedRows":["Learning Rate","320","160","80","40","20","10"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","32K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"91.946","isBolded":false,"associatedRows":["F1 score","320","160"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.7711","isBolded":false,"associatedRows":["Top - 1 Accuracy","320","160"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","2K","10"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"0.7692","isBolded":false,"associatedRows":["Top - 1 Accuracy","320","160","80"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","4K","10"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"2.5?100","isBolded":true,"associatedRows":["Learning Rate","320"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","1K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"2","isBolded":false,"associatedRows":["Learning Rate","320","160","80"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","4K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"0.9335","isBolded":false,"associatedRows":["Top - 5 Accuracy"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","512","10"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"0.9332","isBolded":false,"associatedRows":["Top - 5 Accuracy","320","160","80"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","4K","10"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"2","isBolded":false,"associatedRows":["Learning Rate","320","160","80","40"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","8K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"1","isBolded":false,"associatedRows":["Exact Match","320","160","80"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["Learning Rate","320","160","80","40","20"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","8K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"2.0?100","isBolded":true,"associatedRows":["Learning Rate","320","160"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","2K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"1.25","isBolded":false,"associatedRows":["Warmup Epochs","320","160"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","2K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"1","isBolded":false,"associatedRows":["Exact Match","320","160","80","40","20","10"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":["Exact Match","320"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["Learning Rate","320","160","80","40"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","4K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"0.3125","isBolded":false,"associatedRows":["Warmup Epochs"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","512"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"0.5?100","isBolded":true,"associatedRows":["Learning Rate","320","160","80","40","20"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","16K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"85.260","isBolded":false,"associatedRows":["Exact Match","320"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["Learning Rate","320","160","80"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","2K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"1","isBolded":false,"associatedRows":["Exact Match","320","160","80","40","20"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"5","isBolded":false,"associatedRows":["Warmup Epochs","320","160","80","40","20"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","8K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"4","isBolded":false,"associatedRows":["Learning Rate","320"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","1K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"0.9322","isBolded":false,"associatedRows":["Top - 5 Accuracy","320","160","80","40","20"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","16K","10"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"85.090","isBolded":false,"associatedRows":["Exact Match"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["Learning Rate","320","160","80","40","20"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","16K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"0.7696","isBolded":false,"associatedRows":["Top - 1 Accuracy"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","512","10"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"91.761","isBolded":false,"associatedRows":["F1 score","320"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["Learning Rate"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","512"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"84.901","isBolded":false,"associatedRows":["Exact Match","320","160","80","40"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"91.475","isBolded":false,"associatedRows":["F1 score","320","160","80","40","20","10"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"1.0?100","isBolded":true,"associatedRows":["Learning Rate","320","160","80","40"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","8K"],"associatedMergedColumns":["manually tune the hyperparameters ."]},{"number":"85.355","isBolded":false,"associatedRows":["Exact Match","320","160"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["Learning Rate","320","160","80","40","20","10"],"associatedColumns":["Table 5 : Untuned LAMB for ImageNet training with RESNET - 50 for different batch sizes ( 90 epochs ) .","16K"],"associatedMergedColumns":["manually tune the hyperparameters ."]}]},{"caption":"Table 6: CIFAR-10 training with DavidNet (batch size \u003d 512). All of them run 24 epochs and finish \nthe training under one minute on one cloud TPU. We make sure all the solvers are carefully tuned. \nThe learning rate tuning space of Adam, AdamW, Adagrad and LAMB is {0.0001, 0.0002, 0.0004, \n0.0006, 0.0008, 0.001, 0.002, 0.004, 0.006, 0.008, 0.01, 0.02, 0.04, 0.06, 0.08, ","rows":["implementer . The weight decay term of AdamW was tuned by {0 . 0001 ,","Test Accuracy"],"columns":["ADAM","ADAGRAD","LAMB","ADAMW",", 50} . The momentum optimizer was tuned by the baseline","momentum"],"mergedAllColumns":[],"numberCells":[{"number":"0.001,","isBolded":false,"associatedRows":["implementer . The weight decay term of AdamW was tuned by {0 . 0001 ,"],"associatedColumns":[", 50} . The momentum optimizer was tuned by the baseline"],"associatedMergedColumns":[]},{"number":"0.01,","isBolded":false,"associatedRows":["implementer . The weight decay term of AdamW was tuned by {0 . 0001 ,"],"associatedColumns":[", 50} . The momentum optimizer was tuned by the baseline"],"associatedMergedColumns":[]},{"number":"0.9225","isBolded":false,"associatedRows":["Test Accuracy"],"associatedColumns":[", 50} . The momentum optimizer was tuned by the baseline","ADAM"],"associatedMergedColumns":[]},{"number":"0.9372","isBolded":false,"associatedRows":["Test Accuracy"],"associatedColumns":[", 50} . The momentum optimizer was tuned by the baseline","momentum"],"associatedMergedColumns":[]},{"number":"0.9408","isBolded":false,"associatedRows":["Test Accuracy"],"associatedColumns":[", 50} . The momentum optimizer was tuned by the baseline","LAMB"],"associatedMergedColumns":[]},{"number":"0.9271","isBolded":false,"associatedRows":["Test Accuracy"],"associatedColumns":[", 50} . The momentum optimizer was tuned by the baseline","ADAMW"],"associatedMergedColumns":[]},{"number":"0.9074","isBolded":false,"associatedRows":["Test Accuracy"],"associatedColumns":[", 50} . The momentum optimizer was tuned by the baseline","ADAGRAD"],"associatedMergedColumns":[]},{"number":"0.1,1.0}.","isBolded":false,"associatedRows":["implementer . The weight decay term of AdamW was tuned by {0 . 0001 ,"],"associatedColumns":[", 50} . The momentum optimizer was tuned by the baseline"],"associatedMergedColumns":[]}]},{"caption":"Table 7: Test Accuracy by MNIST training with LeNet (30 epochs for Batch Size \u003d 1024). The \ntuning space of learning rate for all the optimizers is {0.0001, 0.001, 0.01, 0.1}. We use the same \nlearning rate warmup and decay schedule for all of them. \nOptimizer \nMomentum Addgrad ADAM ADAMW LAMB \n\nAverage accuracy over 5 runs \n0.9933 \n0.9928 \n0.9936 \n0.9941 \n0.9945 \n\nFigure 5: Our experiments show that even the validation loss is not reliable in the large-scale training. \nA lower validation loss may lead to a worse accuracy. Thus, we use the test/val accuracy or F1 score \non dev set to evaluate the optimizers. \n\n","rows":["Average accuracy over 5 runs","tuning space of learning rate for all the optimizers is {0 . 0001 ,"],"columns":["ADAM","Addgrad","Momentum","LAMB","ADAMW","Test Accuracy by MNIST training with LeNet ( 30 epochs for Batch Size \u003d 1024 ) . The"],"mergedAllColumns":["learning rate warmup and decay schedule for all of them ."],"numberCells":[{"number":"0.001,","isBolded":false,"associatedRows":["tuning space of learning rate for all the optimizers is {0 . 0001 ,"],"associatedColumns":["Test Accuracy by MNIST training with LeNet ( 30 epochs for Batch Size \u003d 1024 ) . The"],"associatedMergedColumns":[]},{"number":"0.9936","isBolded":false,"associatedRows":["Average accuracy over 5 runs"],"associatedColumns":["Test Accuracy by MNIST training with LeNet ( 30 epochs for Batch Size \u003d 1024 ) . The","ADAM"],"associatedMergedColumns":["learning rate warmup and decay schedule for all of them ."]},{"number":"0.9941","isBolded":false,"associatedRows":["Average accuracy over 5 runs"],"associatedColumns":["Test Accuracy by MNIST training with LeNet ( 30 epochs for Batch Size \u003d 1024 ) . The","ADAMW"],"associatedMergedColumns":["learning rate warmup and decay schedule for all of them ."]},{"number":"0.9945","isBolded":false,"associatedRows":["Average accuracy over 5 runs"],"associatedColumns":["Test Accuracy by MNIST training with LeNet ( 30 epochs for Batch Size \u003d 1024 ) . The","LAMB"],"associatedMergedColumns":["learning rate warmup and decay schedule for all of them ."]},{"number":"0.9933","isBolded":false,"associatedRows":["Average accuracy over 5 runs"],"associatedColumns":["Test Accuracy by MNIST training with LeNet ( 30 epochs for Batch Size \u003d 1024 ) . The","Momentum"],"associatedMergedColumns":["learning rate warmup and decay schedule for all of them ."]},{"number":"0.9928","isBolded":false,"associatedRows":["Average accuracy over 5 runs"],"associatedColumns":["Test Accuracy by MNIST training with LeNet ( 30 epochs for Batch Size \u003d 1024 ) . The","Addgrad"],"associatedMergedColumns":["learning rate warmup and decay schedule for all of them ."]},{"number":"0.01,0.1}.Weusethesame","isBolded":false,"associatedRows":["tuning space of learning rate for all the optimizers is {0 . 0001 ,"],"associatedColumns":["Test Accuracy by MNIST training with LeNet ( 30 epochs for Batch Size \u003d 1024 ) . The"],"associatedMergedColumns":[]}]},{"caption":"Table 8: ADAMW stops scaling at the batch size of 16K. The target F1 score is 90.5. LAMB achieves \na F1 score of 91.345. The table shows the tuning information of ADAMW. In this table, we report the \nbest F1 score we observed from our experiments. \nSolver \nbatch size warmup steps \nLR \nlast step infomation \nF1 score on dev set \n\nADAMW \n16K \n0.05?31250 0.0001 loss\u003d8.04471, step\u003d28126 \ndiverged \nADAMW \n16K \n0.05?31250 0.0002 loss\u003d7.89673, step\u003d28126 \ndiverged \nADAMW \n16K \n0.05?31250 0.0003 loss\u003d8.35102, step\u003d28126 \ndiverged \nADAMW \n16K \n0.10?31250 0.0001 loss\u003d2.01419, step\u003d31250 \n86.034 \nADAMW \n16K \n0.10?31250 0.0002 loss\u003d1.04689, step\u003d31250 \n88.540 \nADAMW \n16K \n0.10?31250 0.0003 loss\u003d8.05845, step\u003d20000 \ndiverged \nADAMW \n16K \n0.20?31250 0.0001 loss\u003d1.53706, step\u003d31250 \n85.231 \nADAMW \n16K \n0.20?31250 0.0002 loss\u003d1.15500, step\u003d31250 \n88.110 \nADAMW \n16K \n0.20?31250 0.0003 loss\u003d1.48798, step\u003d31250 \n85.653 \n\n","rows":["loss\u003d2 . 01419 , step\u003d31250","16K","0 . 10?31250","0 . 20?31250","0 . 05?31250","loss\u003d1 . 48798 , step\u003d31250","Table 8 :","loss\u003d1 . 15500 , step\u003d31250","a F1 score of","loss\u003d1 . 04689 , step\u003d31250","loss\u003d1 . 53706 , step\u003d31250","ADAMW","ADAMW stops scaling at the batch size of 16K . The target F1 score is"],"columns":["loss\u003d8 . 04471 , step\u003d28126","loss\u003d7 . 89673 , step\u003d28126","loss\u003d8 . 35102 , step\u003d28126","best F1 score we observed from our experiments .","F1 score on dev set","LR","loss\u003d8 . 05845 , step\u003d20000","diverged"],"mergedAllColumns":[],"numberCells":[{"number":"0.0003","isBolded":false,"associatedRows":["ADAMW","16K","0 . 10?31250"],"associatedColumns":["best F1 score we observed from our experiments .","LR","loss\u003d8 . 04471 , step\u003d28126","loss\u003d7 . 89673 , step\u003d28126","loss\u003d8 . 35102 , step\u003d28126"],"associatedMergedColumns":[]},{"number":"85.653","isBolded":false,"associatedRows":["ADAMW","16K","0 . 20?31250","loss\u003d1 . 48798 , step\u003d31250"],"associatedColumns":["best F1 score we observed from our experiments .","F1 score on dev set","diverged","diverged","diverged","diverged"],"associatedMergedColumns":[]},{"number":"90.5.LAMBachieves","isBolded":false,"associatedRows":["Table 8 :","a F1 score of","ADAMW stops scaling at the batch size of 16K . The target F1 score is"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.0002","isBolded":false,"associatedRows":["ADAMW","16K","0 . 05?31250"],"associatedColumns":["best F1 score we observed from our experiments .","LR","loss\u003d8 . 04471 , step\u003d28126"],"associatedMergedColumns":[]},{"number":"0.0001","isBolded":false,"associatedRows":["ADAMW","16K","0 . 20?31250"],"associatedColumns":["best F1 score we observed from our experiments .","LR","loss\u003d8 . 04471 , step\u003d28126","loss\u003d7 . 89673 , step\u003d28126","loss\u003d8 . 35102 , step\u003d28126","loss\u003d8 . 05845 , step\u003d20000"],"associatedMergedColumns":[]},{"number":"88.540","isBolded":false,"associatedRows":["ADAMW","16K","0 . 10?31250","loss\u003d1 . 04689 , step\u003d31250"],"associatedColumns":["best F1 score we observed from our experiments .","F1 score on dev set","diverged","diverged","diverged"],"associatedMergedColumns":[]},{"number":"88.110","isBolded":false,"associatedRows":["ADAMW","16K","0 . 20?31250","loss\u003d1 . 15500 , step\u003d31250"],"associatedColumns":["best F1 score we observed from our experiments .","F1 score on dev set","diverged","diverged","diverged","diverged"],"associatedMergedColumns":[]},{"number":"0.0001","isBolded":false,"associatedRows":["ADAMW","16K","0 . 05?31250"],"associatedColumns":["best F1 score we observed from our experiments .","LR"],"associatedMergedColumns":[]},{"number":"0.0002","isBolded":false,"associatedRows":["ADAMW","16K","0 . 20?31250"],"associatedColumns":["best F1 score we observed from our experiments .","LR","loss\u003d8 . 04471 , step\u003d28126","loss\u003d7 . 89673 , step\u003d28126","loss\u003d8 . 35102 , step\u003d28126","loss\u003d8 . 05845 , step\u003d20000"],"associatedMergedColumns":[]},{"number":"86.034","isBolded":false,"associatedRows":["ADAMW","16K","0 . 10?31250","loss\u003d2 . 01419 , step\u003d31250"],"associatedColumns":["best F1 score we observed from our experiments .","F1 score on dev set","diverged","diverged","diverged"],"associatedMergedColumns":[]},{"number":"0.0003","isBolded":false,"associatedRows":["ADAMW","16K","0 . 05?31250"],"associatedColumns":["best F1 score we observed from our experiments .","LR","loss\u003d8 . 04471 , step\u003d28126","loss\u003d7 . 89673 , step\u003d28126"],"associatedMergedColumns":[]},{"number":"0.0002","isBolded":false,"associatedRows":["ADAMW","16K","0 . 10?31250"],"associatedColumns":["best F1 score we observed from our experiments .","LR","loss\u003d8 . 04471 , step\u003d28126","loss\u003d7 . 89673 , step\u003d28126","loss\u003d8 . 35102 , step\u003d28126"],"associatedMergedColumns":[]},{"number":"0.0003","isBolded":false,"associatedRows":["ADAMW","16K","0 . 20?31250"],"associatedColumns":["best F1 score we observed from our experiments .","LR","loss\u003d8 . 04471 , step\u003d28126","loss\u003d7 . 89673 , step\u003d28126","loss\u003d8 . 35102 , step\u003d28126","loss\u003d8 . 05845 , step\u003d20000"],"associatedMergedColumns":[]},{"number":"0.0001","isBolded":false,"associatedRows":["ADAMW","16K","0 . 10?31250"],"associatedColumns":["best F1 score we observed from our experiments .","LR","loss\u003d8 . 04471 , step\u003d28126","loss\u003d7 . 89673 , step\u003d28126","loss\u003d8 . 35102 , step\u003d28126"],"associatedMergedColumns":[]},{"number":"91.345.ThetableshowsthetuninginformationofADAMW.Inthistable,wereportthe","isBolded":false,"associatedRows":["a F1 score of"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"85.231","isBolded":false,"associatedRows":["ADAMW","16K","0 . 20?31250","loss\u003d1 . 53706 , step\u003d31250"],"associatedColumns":["best F1 score we observed from our experiments .","F1 score on dev set","diverged","diverged","diverged","diverged"],"associatedMergedColumns":[]}]},{"caption":"Table 9: The accuracy information of tuning default AdaGrad optimizer for ImageNet training with \nResNet-50 (batch size \u003d 16384, 90 epochs, 7038 iterations). \nLearning Rate Top-1 Validation Accuracy \n\n0.0001 \n0.0026855469 \n0.001 \n0.015563965 \n0.002 \n0.022684732 \n0.004 \n0.030924479 \n0.008 \n0.04486084 \n0.010 \n0.054158527 \n0.020 \n0.0758667 \n0.040 \n0.1262614 \n0.080 \n0.24037679 \n0.100 \n0.27357993 \n0.200 \n0.458313 \n0.400 \n0.553833 \n0.800 \n0.54103595 \n1.000 \n0.5489095 \n2.000 \n0.47680664 \n4.000 \n0.5295207 \n6.000 \n0.36950684 \n8.000 \n0.31081137 \n10.00 \n0.30670166 \n12.00 \n0.3091024 \n14.00 \n0.3227946 \n16.00 \n0.0063680015 \n18.00 \n0.11287435 \n20.00 \n0.21602376 \n30.00 \n0.08315023 \n40.00 \n0.0132039385 \n50.00 \n0.0009969076 \n","rows":[],"columns":["Top - 1 Validation Accuracy","Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"mergedAllColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."],"numberCells":[{"number":"0.5489095","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.040","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"50.00","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.5295207","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"12.00","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.21602376","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.0009969076","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.3227946","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.200","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.31081137","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.030924479","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.0063680015","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.100","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"20.00","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.080","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"30.00","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"14.00","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"2.000","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.3091024","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.0132039385","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"6.000","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.47680664","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.022684732","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.27357993","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"18.00","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.36950684","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.400","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.08315023","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"1.000","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.04486084","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.1262614","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.020","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.015563965","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.553833","isBolded":true,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"10.00","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.30670166","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"40.00","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.0758667","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.0026855469","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"4.000","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"8.000","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.11287435","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.800","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.54103595","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.054158527","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.458313","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.010","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"16.00","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]},{"number":"0.24037679","isBolded":false,"associatedRows":[],"associatedColumns":["Table 9 : The accuracy information of tuning default AdaGrad optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) ."]}]},{"caption":"Table 10: The accuracy information of tuning AdaGrad optimizer for ImageNet training with ResNet-\n50 (batch size \u003d 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of ","rows":[],"columns":["Top - 1 Validation Accuracy",") .","Learning Rate"],"mergedAllColumns":[],"numberCells":[{"number":"0.400","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"2.000","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"50.00","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.13429768","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.58614093","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.004","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"8.000","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"16.00","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.26550293","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.010","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.69085693","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"30.00","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"6.000","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.012573242","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.7187703","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.10970052","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.00793457","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.100","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.002","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.7194214","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"14.00","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.029012045","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"1.000","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.0421346","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.41918945","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.7137858","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.0011189779","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.709493","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.71797687","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.019022623","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.70306396","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.67252606","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.020","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.800","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.027079264","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"10.00","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"20.00","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.67976886","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.70458984","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"18.00","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"12.00","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.5519816","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.080","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.72007245","isBolded":true,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"4.000","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.06618246","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"40.00","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.040","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.7149251","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.71293133","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.200","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.008","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]}]},{"caption":"Table 11: The accuracy information of tuning default Adam optimizer for ImageNet training with \nResNet-50 (batch size \u003d 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763 \n(Goyal et al., 2017). \nLearning Rate Top-1 Validation Accuracy \n\n0.0001 \n0.5521 \n0.0002 \n0.6089 \n0.0004 \n0.6432 \n0.0006 \n0.6465 \n0.0008 \n0.6479 \n0.001 \n0.6604 \n0.002 \n0.6408 \n0.004 \n0.5687 \n0.006 \n0.5165 \n0.008 \n0.4812 \n0.010 \n0.3673 \n\n","rows":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . The target accuracy is around"],"columns":["Top - 1 Validation Accuracy","Learning Rate","Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with"],"mergedAllColumns":["( Goyal et al . , 2017 ) ."],"numberCells":[{"number":"0.763","isBolded":false,"associatedRows":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . The target accuracy is around"],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with"],"associatedMergedColumns":[]},{"number":"0.6089","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.010","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.3673","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.6465","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.5521","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.5687","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.5165","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.6479","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.4812","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.6432","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.6408","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.6604","isBolded":true,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 11 : The accuracy information of tuning default Adam optimizer for ImageNet training with","Learning Rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]}]},{"caption":"Table 12: The accuracy information of tuning Adam optimizer for ImageNet training with ResNet-50 \n(batch size \u003d 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of ","rows":[],"columns":["Top - 1 Validation Accuracy",") .","Learning Rate"],"mergedAllColumns":[],"numberCells":[{"number":"0.012","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.0008","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.010","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.410319","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.7081502","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.100","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.55263263","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.040","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.73476154","isBolded":true,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.67997235","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.6996867","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.73286945","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.004","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.69108075","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.014","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.72214764","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.080","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.0004","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.006","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.008","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.060","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.6455485","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.0006","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.002","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.45174155","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.58658856","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.6993001","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.020","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.71010333","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.51090497","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.6774495","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.40297446","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.72648114","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]},{"number":"0.0002","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.016","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Learning Rate"],"associatedMergedColumns":[]},{"number":"0.71466064","isBolded":false,"associatedRows":[],"associatedColumns":[") .","Top - 1 Validation Accuracy"],"associatedMergedColumns":[]}]},{"caption":"Table 13: The accuracy information of tuning default AdamW optimizer for ImageNet training with \nResNet-50 (batch size \u003d 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763 \n(Goyal et al., 2017). \nlearning rate weight decay L2 regularization Top-1 Validation Accuracy \n\n0.0001 \n0.00001 \ndefault (0.01) \n0.53312176 \n0.0002 \n0.00001 \ndefault (0.01) \n0.5542806 \n0.0004 \n0.00001 \ndefault (0.01) \n0.48769125 \n0.0006 \n0.00001 \ndefault (0.01) \n0.46317545 \n0.0008 \n0.00001 \ndefault (0.01) \n0.40903726 \n0.001 \n0.00001 \ndefault (0.01) \n0.42401123 \n0.002 \n0.00001 \ndefault (0.01) \n0.33870444 \n0.004 \n0.00001 \ndefault (0.01) \n0.12339274 \n0.006 \n0.00001 \ndefault (0.01) \n0.122924805 \n0.008 \n0.00001 \ndefault (0.01) \n0.08099365 \n0.010 \n0.00001 \ndefault (0.01) \n0.016764322 \n0.012 \n0.00001 \ndefault (0.01) \n0.032714844 \n0.014 \n0.00001 \ndefault (0.01) \n0.018147787 \n0.016 \n0.00001 \ndefault (0.01) \n0.0066731772 \n0.018 \n0.00001 \ndefault (0.01) \n0.010294597 \n0.020 \n0.00001 \ndefault (0.01) \n0.008260091 \n0.025 \n0.00001 \ndefault (0.01) \n0.008870442 \n0.030 \n0.00001 \ndefault (0.01) \n0.0064493814 \n0.040 \n0.00001 \ndefault (0.01) \n0.0018107096 \n0.050 \n0.00001 \ndefault (0.01) \n0.003540039 \n","rows":["default ( 0 . 01 )","ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . The target accuracy is around"],"columns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy","learning rate","weight decay"],"mergedAllColumns":["( Goyal et al . , 2017 ) ."],"numberCells":[{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.008870442","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.122924805","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.33870444","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.008260091","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.018147787","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.030","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0064493814","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.003540039","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.050","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.08099365","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.12339274","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.025","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0018107096","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.46317545","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.032714844","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.010","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.016764322","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.020","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.48769125","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.5542806","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.42401123","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.010294597","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.012","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.014","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.018","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.40903726","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0066731772","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.763","isBolded":false,"associatedRows":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . The target accuracy is around"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with"],"associatedMergedColumns":[]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.53312176","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.016","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.040","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 13 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]}]},{"caption":"Table 14: The accuracy information of tuning default AdamW optimizer for ImageNet training with \nResNet-50 (batch size \u003d 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763 \n(Goyal et al., 2017). \nlearning rate weight decay L2 regularization Top-1 Validation Accuracy \n\n0.0001 \n0.0001 \ndefault (0.01) \n0.55489093 \n0.0002 \n0.0001 \ndefault (0.01) \n0.56514484 \n0.0004 \n0.0001 \ndefault (0.01) \n0.4986979 \n0.0006 \n0.0001 \ndefault (0.01) \n0.47595215 \n0.0008 \n0.0001 \ndefault (0.01) \n0.44685873 \n0.001 \n0.0001 \ndefault (0.01) \n0.41029868 \n0.002 \n0.0001 \ndefault (0.01) \n0.2808024 \n0.004 \n0.0001 \ndefault (0.01) \n0.08111572 \n0.006 \n0.0001 \ndefault (0.01) \n0.068115234 \n0.008 \n0.0001 \ndefault (0.01) \n0.057922363 \n0.010 \n0.0001 \ndefault (0.01) \n0.05222575 \n0.012 \n0.0001 \ndefault (0.01) \n0.017313639 \n0.014 \n0.0001 \ndefault (0.01) \n0.029785156 \n0.016 \n0.0001 \ndefault (0.01) \n0.016540527 \n0.018 \n0.0001 \ndefault (0.01) \n0.00575765 \n0.020 \n0.0001 \ndefault (0.01) \n0.0102335615 \n0.025 \n0.0001 \ndefault (0.01) \n0.0060831704 \n0.030 \n0.0001 \ndefault (0.01) \n0.0036417644 \n0.040 \n0.0001 \ndefault (0.01) \n0.0010782877 \n0.050 \n0.0001 \ndefault (0.01) \n0.0037638347 \n","rows":["default ( 0 . 01 )","ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . The target accuracy is around"],"columns":["Top - 1 Validation Accuracy","learning rate","Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"mergedAllColumns":["( Goyal et al . , 2017 ) ."],"numberCells":[{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.014","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.040","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0037638347","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.029785156","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.016","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.41029868","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0102335615","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0060831704","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.55489093","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.030","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.025","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.020","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.010","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.763","isBolded":false,"associatedRows":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . The target accuracy is around"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with"],"associatedMergedColumns":[]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.018","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.4986979","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.2808024","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.068115234","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.016540527","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.44685873","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0010782877","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.05222575","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.56514484","isBolded":true,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0036417644","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.050","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.47595215","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.012","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00575765","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.057922363","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.017313639","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.08111572","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 14 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]}]},{"caption":"Table 15: The accuracy information of tuning default AdamW optimizer for ImageNet training with \nResNet-50 (batch size \u003d 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763 \n(Goyal et al., 2017). \nlearning rate weight decay L2 regularization Top-1 Validation Accuracy \n\n0.0001 \n0.001 \ndefault (0.01) \n0.21142578 \n0.0002 \n0.001 \ndefault (0.01) \n0.4289144 \n0.0004 \n0.001 \ndefault (0.01) \n0.13537598 \n0.0006 \n0.001 \ndefault (0.01) \n0.33803305 \n0.0008 \n0.001 \ndefault (0.01) \n0.32611084 \n0.001 \n0.001 \ndefault (0.01) \n0.22194417 \n0.002 \n0.001 \ndefault (0.01) \n0.1833903 \n0.004 \n0.001 \ndefault (0.01) \n0.08256022 \n0.006 \n0.001 \ndefault (0.01) \n0.020507812 \n0.008 \n0.001 \ndefault (0.01) \n0.018269857 \n0.010 \n0.001 \ndefault (0.01) \n0.007507324 \n0.012 \n0.001 \ndefault (0.01) \n0.020080566 \n0.014 \n0.001 \ndefault (0.01) \n0.010762532 \n0.016 \n0.001 \ndefault (0.01) \n0.0021362305 \n0.018 \n0.001 \ndefault (0.01) \n0.007954915 \n0.020 \n0.001 \ndefault (0.01) \n0.005859375 \n0.025 \n0.001 \ndefault (0.01) \n0.009724935 \n0.030 \n0.001 \ndefault (0.01) \n0.0019124349 \n0.040 \n0.001 \ndefault (0.01) \n0.00390625 \n0.050 \n0.001 \ndefault (0.01) \n0.0009969076 \n","rows":["default ( 0 . 01 )","ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . The target accuracy is around"],"columns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy","learning rate","weight decay"],"mergedAllColumns":["( Goyal et al . , 2017 ) ."],"numberCells":[{"number":"0.1833903","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.4289144","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.763","isBolded":false,"associatedRows":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . The target accuracy is around"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with"],"associatedMergedColumns":[]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.21142578","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.33803305","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.010762532","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.005859375","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0009969076","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.13537598","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.32611084","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0019124349","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.050","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.025","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.007954915","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.010","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.040","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.22194417","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.020507812","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.012","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.020080566","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.018","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.016","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.08256022","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.014","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.020","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.030","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.009724935","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.018269857","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00390625","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.007507324","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0021362305","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 15 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]}]},{"caption":"Table 16: The accuracy information of tuning default AdamW optimizer for ImageNet training with \nResNet-50 (batch size \u003d 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763 \n(Goyal et al., 2017). \nlearning rate weight decay L2 regularization Top-1 Validation Accuracy \n\n0.0001 \n0.01 \ndefault (0.01) \n0.0009765625 \n0.0002 \n0.01 \ndefault (0.01) \n0.0009969076 \n0.0004 \n0.01 \ndefault (0.01) \n0.0010172526 \n0.0006 \n0.01 \ndefault (0.01) \n0.0009358724 \n0.0008 \n0.01 \ndefault (0.01) \n0.0022379558 \n0.001 \n0.01 \ndefault (0.01) \n0.001566569 \n0.002 \n0.01 \ndefault (0.01) \n0.009480794 \n0.004 \n0.01 \ndefault (0.01) \n0.0033569336 \n0.006 \n0.01 \ndefault (0.01) \n0.0029907227 \n0.008 \n0.01 \ndefault (0.01) \n0.0018513998 \n0.010 \n0.01 \ndefault (0.01) \n0.009134929 \n0.012 \n0.01 \ndefault (0.01) \n0.0022176106 \n0.014 \n0.01 \ndefault (0.01) \n0.0040690103 \n0.016 \n0.01 \ndefault (0.01) \n0.0017293295 \n0.018 \n0.01 \ndefault (0.01) \n0.00061035156 \n0.020 \n0.01 \ndefault (0.01) \n0.0022379558 \n0.025 \n0.01 \ndefault (0.01) \n0.0017089844 \n0.030 \n0.01 \ndefault (0.01) \n0.0014241537 \n0.040 \n0.01 \ndefault (0.01) \n0.0020345051 \n0.050 \n0.01 \ndefault (0.01) \n0.0012817383 \n","rows":["default ( 0 . 01 )","ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . The target accuracy is around"],"columns":["Top - 1 Validation Accuracy","learning rate","Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"mergedAllColumns":["( Goyal et al . , 2017 ) ."],"numberCells":[{"number":"0.0022176106","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.050","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0022379558","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.010","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.040","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.014","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.016","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001566569","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.020","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0017089844","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0014241537","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.009480794","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0029907227","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.012","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.018","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0018513998","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.030","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0009765625","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0009969076","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0022379558","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0033569336","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.009134929","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.763","isBolded":false,"associatedRows":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . The target accuracy is around"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with"],"associatedMergedColumns":[]},{"number":"0.0017293295","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00061035156","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.025","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0009358724","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0012817383","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0010172526","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0040690103","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0020345051","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 16 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]}]},{"caption":"Table 17: The accuracy information of tuning default AdamW optimizer for ImageNet training with \nResNet-50 (batch size \u003d 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763 \n(Goyal et al., 2017). \nlearning rate weight decay L2 regularization Top-1 Validation Accuracy \n\n0.0001 \n0.00001 \ndisable \n0.48917642 \n0.0002 \n0.00001 \ndisable \n0.58152264 \n0.0004 \n0.00001 \ndisable \n0.63460284 \n0.0006 \n0.00001 \ndisable \n0.64849854 \n0.0008 \n0.00001 \ndisable \n0.6598918 \n0.001 \n0.00001 \ndisable \n0.6662801 \n0.002 \n0.00001 \ndisable \n0.67266846 \n0.004 \n0.00001 \ndisable \n0.6692708 \n0.006 \n0.00001 \ndisable \n0.6573079 \n0.008 \n0.00001 \ndisable \n0.6639404 \n0.010 \n0.00001 \ndisable \n0.65230304 \n0.012 \n0.00001 \ndisable \n0.6505534 \n0.014 \n0.00001 \ndisable \n0.64990234 \n0.016 \n0.00001 \ndisable \n0.65323895 \n0.018 \n0.00001 \ndisable \n0.67026776 \n0.020 \n0.00001 \ndisable \n0.66086835 \n0.025 \n0.00001 \ndisable \n0.65425617 \n0.030 \n0.00001 \ndisable \n0.6476237 \n0.040 \n0.00001 \ndisable \n0.55478925 \n0.050 \n0.00001 \ndisable \n0.61869305 \n","rows":["disable","ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . The target accuracy is around"],"columns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy","learning rate","weight decay"],"mergedAllColumns":["( Goyal et al . , 2017 ) ."],"numberCells":[{"number":"0.6692708","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.6476237","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.6662801","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.58152264","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.65323895","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.050","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.65425617","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.040","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.6639404","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.67266846","isBolded":true,"associatedRows":["disable"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.63460284","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.6598918","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.64990234","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.61869305","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.763","isBolded":false,"associatedRows":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . The target accuracy is around"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with"],"associatedMergedColumns":[]},{"number":"0.006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.55478925","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.025","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.65230304","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.016","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.6573079","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.64849854","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.014","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.67026776","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.030","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.012","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.020","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.66086835","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.6505534","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.48917642","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.018","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.010","isBolded":false,"associatedRows":[],"associatedColumns":["Table 17 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]}]},{"caption":"Table 18: The accuracy information of tuning default AdamW optimizer for ImageNet training with \nResNet-50 (batch size \u003d 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763 \n(Goyal et al., 2017). \nlearning rate weight decay L2 regularization Top-1 Validation Accuracy \n\n0.0001 \n0.0001 \ndisable \n0.5033366 \n0.0002 \n0.0001 \ndisable \n0.5949707 \n0.0004 \n0.0001 \ndisable \n0.62561035 \n0.0006 \n0.0001 \ndisable \n0.6545207 \n0.0008 \n0.0001 \ndisable \n0.66326904 \n0.001 \n0.0001 \ndisable \n0.6677043 \n0.002 \n0.0001 \ndisable \n0.67244464 \n0.004 \n0.0001 \ndisable \n0.6702881 \n0.006 \n0.0001 \ndisable \n0.66033936 \n0.008 \n0.0001 \ndisable \n0.66426593 \n0.010 \n0.0001 \ndisable \n0.66151935 \n0.012 \n0.0001 \ndisable \n0.6545817 \n0.014 \n0.0001 \ndisable \n0.65509033 \n0.016 \n0.0001 \ndisable \n0.6529338 \n0.018 \n0.0001 \ndisable \n0.65651447 \n0.020 \n0.0001 \ndisable \n0.65334064 \n0.025 \n0.0001 \ndisable \n0.655009 \n0.030 \n0.0001 \ndisable \n0.64552814 \n0.040 \n0.0001 \ndisable \n0.6425374 \n0.050 \n0.0001 \ndisable \n0.5988159 \n","rows":["disable","ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . The target accuracy is around"],"columns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy","learning rate","weight decay"],"mergedAllColumns":["( Goyal et al . , 2017 ) ."],"numberCells":[{"number":"0.6425374","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.025","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.6702881","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.64552814","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.014","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.62561035","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.010","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.65334064","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.6545207","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.018","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.6529338","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.020","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.5949707","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.5033366","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.6677043","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.66151935","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.050","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.67244464","isBolded":true,"associatedRows":["disable"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.016","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.66326904","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.030","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.66426593","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.040","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.6545817","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.5988159","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.763","isBolded":false,"associatedRows":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . The target accuracy is around"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with"],"associatedMergedColumns":[]},{"number":"0.008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.65651447","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.65509033","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.655009","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.012","isBolded":false,"associatedRows":[],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.66033936","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 18 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]}]},{"caption":"Table 19: The accuracy information of tuning default AdamW optimizer for ImageNet training with \nResNet-50 (batch size \u003d 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763 \n(Goyal et al., 2017). \nlearning rate weight decay L2 regularization Top-1 Validation Accuracy \n\n0.0001 \n0.001 \ndisable \n0.4611206 \n0.0002 \n0.001 \ndisable \n0.0076293945 \n0.0004 \n0.001 \ndisable \n0.29233804 \n0.0006 \n0.001 \ndisable \n0.57295734 \n0.0008 \n0.001 \ndisable \n0.5574748 \n0.001 \n0.001 \ndisable \n0.5988566 \n0.002 \n0.001 \ndisable \n0.586263 \n0.004 \n0.001 \ndisable \n0.62076825 \n0.006 \n0.001 \ndisable \n0.61503094 \n0.008 \n0.001 \ndisable \n0.4697876 \n0.010 \n0.001 \ndisable \n0.619751 \n0.012 \n0.001 \ndisable \n0.54243976 \n0.014 \n0.001 \ndisable \n0.5429077 \n0.016 \n0.001 \ndisable \n0.55281574 \n0.018 \n0.001 \ndisable \n0.5819295 \n0.020 \n0.001 \ndisable \n0.5938924 \n0.025 \n0.001 \ndisable \n0.541097 \n0.030 \n0.001 \ndisable \n0.45890298 \n0.040 \n0.001 \ndisable \n0.56193036 \n0.050 \n0.001 \ndisable \n0.5279134 \n","rows":["disable","ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . The target accuracy is around"],"columns":["Top - 1 Validation Accuracy","learning rate","Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"mergedAllColumns":["( Goyal et al . , 2017 ) ."],"numberCells":[{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.4697876","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.586263","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.010","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.012","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.61503094","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.54243976","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0076293945","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.018","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.025","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.5279134","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.619751","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.5574748","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.5429077","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.29233804","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.020","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.050","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.763","isBolded":false,"associatedRows":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . The target accuracy is around"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with"],"associatedMergedColumns":[]},{"number":"0.5988566","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.040","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.57295734","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.014","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.5819295","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.4611206","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.55281574","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.62076825","isBolded":true,"associatedRows":["disable"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.016","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.45890298","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.5938924","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.541097","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.030","isBolded":false,"associatedRows":[],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.56193036","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 19 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]}]},{"caption":"Table 20: The accuracy information of tuning default AdamW optimizer for ImageNet training with \nResNet-50 (batch size \u003d 16384, 90 epochs, 7038 iterations). The target accuracy is around 0.763 \n(Goyal et al., 2017). \nlearning rate weight decay L2 regularization Top-1 Validation Accuracy \n\n0.0001 \n0.01 \ndisable \n0.0009969076 \n0.0002 \n0.01 \ndisable \n0.0008951823 \n0.0004 \n0.01 \ndisable \n0.00095621747 \n0.0006 \n0.01 \ndisable \n0.0012817383 \n0.0008 \n0.01 \ndisable \n0.016886393 \n0.001 \n0.01 \ndisable \n0.038146973 \n0.002 \n0.01 \ndisable \n0.0015258789 \n0.004 \n0.01 \ndisable \n0.0014241537 \n0.006 \n0.01 \ndisable \n0.081441246 \n0.008 \n0.01 \ndisable \n0.028116861 \n0.010 \n0.01 \ndisable \n0.011820476 \n0.012 \n0.01 \ndisable \n0.08138021 \n0.014 \n0.01 \ndisable \n0.010111491 \n0.016 \n0.01 \ndisable \n0.0041910806 \n0.018 \n0.01 \ndisable \n0.0038248699 \n0.020 \n0.01 \ndisable \n0.002746582 \n0.025 \n0.01 \ndisable \n0.011555989 \n0.030 \n0.01 \ndisable \n0.0065104165 \n0.040 \n0.01 \ndisable \n0.016438803 \n0.050 \n0.01 \ndisable \n0.007710775 \n","rows":["disable","ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . The target accuracy is around"],"columns":["Top - 1 Validation Accuracy","learning rate","weight decay","Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with"],"mergedAllColumns":["( Goyal et al . , 2017 ) ."],"numberCells":[{"number":"0.025","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.018","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0041910806","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.014","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.763","isBolded":false,"associatedRows":["ResNet - 50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . The target accuracy is around"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with"],"associatedMergedColumns":[]},{"number":"0.0038248699","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.040","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0015258789","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.002746582","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.020","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.050","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.010","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.011555989","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.010111491","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0012817383","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0065104165","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.028116861","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.08138021","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.011820476","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.016","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0014241537","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.016438803","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.030","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.00095621747","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.016886393","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0008951823","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.012","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","learning rate"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.038146973","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.007710775","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.081441246","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.0009969076","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","Top - 1 Validation Accuracy"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 20 : The accuracy information of tuning default AdamW optimizer for ImageNet training with","weight decay"],"associatedMergedColumns":["( Goyal et al . , 2017 ) ."]}]},{"caption":"Table 21: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-\n50 (batch size \u003d 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al., \n2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at \n30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017). \nlearning rate weight decay L2 regularization Top-1 Validation Accuracy \n\n0.0001 \n0.01 \ndefault (0.01) \n0.0009969076 \n0.0002 \n0.01 \ndefault (0.01) \n0.0009969076 \n0.0004 \n0.01 \ndefault (0.01) \n0.0009969076 \n0.0006 \n0.01 \ndefault (0.01) \n0.0009358724 \n0.0008 \n0.01 \ndefault (0.01) \n0.0009969076 \n0.001 \n0.01 \ndefault (0.01) \n0.0009765625 \n0.002 \n0.01 \ndefault (0.01) \n0.0010172526 \n0.004 \n0.01 \ndefault (0.01) \n0.0010172526 \n0.006 \n0.01 \ndefault (0.01) \n0.0010172526 \n0.008 \n0.01 \ndefault (0.01) \n0.0010172526 \n0.0001 \n0.001 \ndefault (0.01) \n0.0010172526 \n0.0002 \n0.001 \ndefault (0.01) \n0.0010172526 \n0.0004 \n0.001 \ndefault (0.01) \n0.0010172526 \n0.0006 \n0.001 \ndefault (0.01) \n0.0009969076 \n0.0008 \n0.001 \ndefault (0.01) \n0.0010172526 \n0.001 \n0.001 \ndefault (0.01) \n0.0010172526 \n0.002 \n0.001 \ndefault (0.01) \n0.0010172526 \n0.004 \n0.001 \ndefault (0.01) \n0.0038452148 \n0.006 \n0.001 \ndefault (0.01) \n0.011881511 \n0.008 \n0.001 \ndefault (0.01) \n0.0061442056 \n","rows":["2017 ) : ( 1 ) 5 - epoch warmup to stablize the initial stage ; and ( 2 ) multiply the learning rate by","30th , 60th , and 80th epoch . The target accuracy is around","default ( 0 . 01 )"],"columns":["Top - 1 Validation Accuracy","learning rate","Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"mergedAllColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"],"numberCells":[{"number":"0.1at","isBolded":false,"associatedRows":["2017 ) : ( 1 ) 5 - epoch warmup to stablize the initial stage ; and ( 2 ) multiply the learning rate by"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0009358724","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.011881511","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0009969076","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0009969076","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0010172526","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0061442056","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0038452148","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0010172526","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0009969076","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0010172526","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.763(Goyaletal.,2017).","isBolded":false,"associatedRows":["30th , 60th , and 80th epoch . The target accuracy is around"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0009969076","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0010172526","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0010172526","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0010172526","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0010172526","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0010172526","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0009765625","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0009969076","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0010172526","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0010172526","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 21 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]}]},{"caption":"Table 22: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-\n50 (batch size \u003d 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al.,  2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at \n30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017). \nlearning rate weight decay L2 regularization Top-1 Validation Accuracy \n\n0.0001 \n0.0001 \ndefault (0.01) \n0.3665975 \n0.0002 \n0.0001 \ndefault (0.01) \n0.5315755 \n0.0004 \n0.0001 \ndefault (0.01) \n0.6369222 \n0.0006 \n0.0001 \ndefault (0.01) \n0.6760457 \n0.0008 \n0.0001 \ndefault (0.01) \n0.69557697 \n0.001 \n0.0001 \ndefault (0.01) \n0.7076009 \n0.002 \n0.0001 \ndefault (0.01) \n0.73065186 \n0.004 \n0.0001 \ndefault (0.01) \n0.72806805 \n0.006 \n0.0001 \ndefault (0.01) \n0.72161865 \n0.008 \n0.0001 \ndefault (0.01) \n0.71816 \n0.0001 \n0.00001 \ndefault (0.01) \n0.49804688 \n0.0002 \n0.00001 \ndefault (0.01) \n0.6287028 \n0.0004 \n0.00001 \ndefault (0.01) \n0.6773885 \n0.0006 \n0.00001 \ndefault (0.01) \n0.67348224 \n0.0008 \n0.00001 \ndefault (0.01) \n0.6622111 \n0.001 \n0.00001 \ndefault (0.01) \n0.6468709 \n0.002 \n0.00001 \ndefault (0.01) \n0.5846761 \n0.004 \n0.00001 \ndefault (0.01) \n0.4868978 \n0.006 \n0.00001 \ndefault (0.01) \n0.34969077 \n0.008 \n0.00001 \ndefault (0.01) \n0.31193033 \n","rows":["2017 ) : ( 1 ) 5 - epoch warmup to stablize the initial stage ; and ( 2 ) multiply the learning rate by","30th , 60th , and 80th epoch . The target accuracy is around","default ( 0 . 01 )"],"columns":["Top - 1 Validation Accuracy","learning rate","Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"mergedAllColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"],"numberCells":[{"number":"0.5315755","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.71816","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.6622111","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.67348224","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.5846761","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.6760457","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.6773885","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.7076009","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.6468709","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.72806805","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.6287028","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.69557697","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.4868978","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.6369222","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.49804688","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.3665975","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.1at","isBolded":false,"associatedRows":["2017 ) : ( 1 ) 5 - epoch warmup to stablize the initial stage ; and ( 2 ) multiply the learning rate by"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.73065186","isBolded":true,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.72161865","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.31193033","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.763(Goyaletal.,2017).","isBolded":false,"associatedRows":["30th , 60th , and 80th epoch . The target accuracy is around"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.00001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.34969077","isBolded":false,"associatedRows":["default ( 0 . 01 )"],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 22 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]}]},{"caption":"Table 23: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-\n50 (batch size \u003d 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al.,  2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at \n30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017). \nlearning rate weight decay L2 regularization Top-1 Validation Accuracy \n\n0.0001 \n0.01 \ndisable \n0.0010172526 \n0.0002 \n0.01 \ndisable \n0.0009765625 \n0.0004 \n0.01 \ndisable \n0.0010172526 \n0.0006 \n0.01 \ndisable \n0.0009969076 \n0.0008 \n0.01 \ndisable \n0.0010172526 \n0.001 \n0.01 \ndisable \n0.0009765625 \n0.002 \n0.01 \ndisable \n0.0009969076 \n0.004 \n0.01 \ndisable \n0.0009969076 \n0.006 \n0.01 \ndisable \n0.0009765625 \n0.008 \n0.01 \ndisable \n0.0010172526 \n0.0001 \n0.001 \ndisable \n0.0009765625 \n0.0002 \n0.001 \ndisable \n0.0010172526 \n0.0004 \n0.001 \ndisable \n0.0010172526 \n0.0006 \n0.001 \ndisable \n0.0010172526 \n0.0008 \n0.001 \ndisable \n0.0010172526 \n0.001 \n0.001 \ndisable \n0.0009969076 \n0.002 \n0.001 \ndisable \n0.0010579427 \n0.004 \n0.001 \ndisable \n0.0016886393 \n0.006 \n0.001 \ndisable \n0.019714355 \n0.008 \n0.001 \ndisable \n0.1329956 \n","rows":["2017 ) : ( 1 ) 5 - epoch warmup to stablize the initial stage ; and ( 2 ) multiply the learning rate by","disable","30th , 60th , and 80th epoch . The target accuracy is around"],"columns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy","learning rate","weight decay"],"mergedAllColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"],"numberCells":[{"number":"0.002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0009969076","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0016886393","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0009765625","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0010172526","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0009969076","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0010172526","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0009765625","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0010579427","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0010172526","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0010172526","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.763(Goyaletal.,2017).","isBolded":false,"associatedRows":["30th , 60th , and 80th epoch . The target accuracy is around"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.019714355","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0010172526","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0009765625","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0010172526","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.1329956","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0009765625","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0010172526","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0009969076","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.1at","isBolded":false,"associatedRows":["2017 ) : ( 1 ) 5 - epoch warmup to stablize the initial stage ; and ( 2 ) multiply the learning rate by"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0010172526","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.01","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0009969076","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 23 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]}]},{"caption":"Table 24: The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet-\n50 (batch size \u003d 16384, 90 epochs, 7038 iterations). We use the learning rate recipe of (Goyal et al.,  2017): (1) 5-epoch warmup to stablize the initial stage; and (2) multiply the learning rate by 0.1 at \n30th, 60th, and 80th epoch. The target accuracy is around 0.763 (Goyal et al., 2017). \nlearning rate weight decay L2 regularization Top-1 Validation Accuracy \n\n0.0001 \n0.0001 \ndisable \n0.28515625 \n0.0002 \n0.0001 \ndisable \n0.44055176 \n0.0004 \n0.0001 \ndisable \n0.56815594 \n0.0006 \n0.0001 \ndisable \n0.6234741 \n0.0008 \n0.0001 \ndisable \n0.6530762 \n0.001 \n0.0001 \ndisable \n0.6695964 \n0.002 \n0.0001 \ndisable \n0.70048016 \n0.004 \n0.0001 \ndisable \n0.71698 \n0.006 \n0.0001 \ndisable \n0.72021484 \n0.008 \n0.0001 \ndisable \n0.7223918 \n0.010 \n0.0001 \ndisable \n0.72017413 \n0.012 \n0.0001 \ndisable \n0.72058105 \n0.014 \n0.0001 \ndisable \n0.7188924 \n0.016 \n0.0001 \ndisable \n0.71695966 \n0.018 \n0.0001 \ndisable \n0.7154134 \n0.020 \n0.0001 \ndisable \n0.71358234 \n0.025 \n0.0001 \ndisable \n0.7145386 \n0.030 \n0.0001 \ndisable \n0.7114258 \n0.040 \n0.0001 \ndisable \n0.7066447 \n0.050 \n0.0001 \ndisable \n0.70284015 \n","rows":["2017 ) : ( 1 ) 5 - epoch warmup to stablize the initial stage ; and ( 2 ) multiply the learning rate by","disable","30th , 60th , and 80th epoch . The target accuracy is around"],"columns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy","learning rate","weight decay"],"mergedAllColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"],"numberCells":[{"number":"0.0008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.016","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.040","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.28515625","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.72021484","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.7145386","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.018","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.050","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.71698","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.6695964","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.030","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.56815594","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.008","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0004","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.7223918","isBolded":true,"associatedRows":["disable"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.70048016","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.012","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.763(Goyaletal.,2017).","isBolded":false,"associatedRows":["30th , 60th , and 80th epoch . The target accuracy is around"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.002","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.010","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.014","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.44055176","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.71695966","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.71358234","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.7114258","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.72017413","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.7154134","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.6234741","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.006","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.025","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.72058105","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.7188924","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.7066447","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.1at","isBolded":false,"associatedRows":["2017 ) : ( 1 ) 5 - epoch warmup to stablize the initial stage ; and ( 2 ) multiply the learning rate by"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.020","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","learning rate"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.70284015","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.0001","isBolded":false,"associatedRows":[],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","weight decay"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]},{"number":"0.6530762","isBolded":false,"associatedRows":["disable"],"associatedColumns":["Table 24 : The accuracy information of tuning AdamW optimizer for ImageNet training with ResNet -","Top - 1 Validation Accuracy"],"associatedMergedColumns":["50 ( batch size \u003d 16384 , 90 epochs , 7038 iterations ) . We use the learning rate recipe of ( Goyal et al . ,"]}]}]