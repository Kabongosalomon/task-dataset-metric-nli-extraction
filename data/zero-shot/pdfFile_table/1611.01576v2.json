[{"caption":"Table 1: Accuracy comparison on the IMDb binary sentiment classification task. All of our models \nuse 256 units per layer; all layers other than the first layer, whose filter width may vary, use filter \nwidth k \u003d 2. Train times are reported on a single NVIDIA K40 GPU. We exclude semi-supervised \nmodels that conduct additional training on the unlabeled portion of the dataset. \n\n","rows":["2 layer sequential BoW CNN ( Johnson \u0026 Zhang , 2014 )","?","2 - layer LSTM ( Longpre et al . , 2016 )","Densely - connected 4 - layer QRNN with k \u003d 4","Residual 2 - layer bi - LSTM ( Longpre et al . , 2016 )","NBSVM - bi ( Wang \u0026 Manning , 2012 )","Ensemble of RNNs and NB - SVM ( Mesnil et al . , 2014 )","Densely - connected 4 - layer QRNN","160","480","150","Densely - connected 4 - layer LSTM ( cuDNN optimized )"],"columns":["Test Acc ( % )"],"mergedAllColumns":["Our models"],"numberCells":[{"number":"91.2","isBolded":false,"associatedRows":["NBSVM - bi ( Wang \u0026 Manning , 2012 )","?"],"associatedColumns":["Test Acc ( % )"],"associatedMergedColumns":[]},{"number":"92.3","isBolded":false,"associatedRows":["2 layer sequential BoW CNN ( Johnson \u0026 Zhang , 2014 )","?"],"associatedColumns":["Test Acc ( % )"],"associatedMergedColumns":[]},{"number":"90.1","isBolded":false,"associatedRows":["Residual 2 - layer bi - LSTM ( Longpre et al . , 2016 )","?"],"associatedColumns":["Test Acc ( % )"],"associatedMergedColumns":[]},{"number":"90.9","isBolded":false,"associatedRows":["Densely - connected 4 - layer LSTM ( cuDNN optimized )","480"],"associatedColumns":["Test Acc ( % )"],"associatedMergedColumns":["Our models"]},{"number":"91.1","isBolded":false,"associatedRows":["Densely - connected 4 - layer QRNN with k \u003d 4","160"],"associatedColumns":["Test Acc ( % )"],"associatedMergedColumns":["Our models"]},{"number":"92.6","isBolded":false,"associatedRows":["Ensemble of RNNs and NB - SVM ( Mesnil et al . , 2014 )","?"],"associatedColumns":["Test Acc ( % )"],"associatedMergedColumns":[]},{"number":"87.6","isBolded":false,"associatedRows":["2 - layer LSTM ( Longpre et al . , 2016 )","?"],"associatedColumns":["Test Acc ( % )"],"associatedMergedColumns":[]},{"number":"91.4","isBolded":false,"associatedRows":["Densely - connected 4 - layer QRNN","150"],"associatedColumns":["Test Acc ( % )"],"associatedMergedColumns":["Our models"]}]},{"caption":"Table 2: Single model perplexity on validation and test sets for the Penn Treebank language model-\ning task. Lower is better. \"Medium\" refers to a two-layer network with 640 or 650 hidden units per \nlayer. All QRNN models include dropout of 0.5 on embeddings and between layers. MC refers to \nMonte Carlo dropout averaging at test time. \n","rows":["Zoneout + Variational LSTM ( medium ) ( Merity et al . , 2016 )","Variational LSTM ( medium , MC ) ( Gal \u0026 Ghahramani , 2016 )","LSTM ( medium ) ( Zaremba et al . , 2014 )","?","19M","18M","LSTM with CharCNN embeddings ( Kim et al . , 2016 )","LSTM ( medium )","QRNN + zoneout ( p \u003d","20M","QRNN ( medium )"],"columns":["Validation","Test","Model"],"mergedAllColumns":["Our models"],"numberCells":[{"number":"78.9","isBolded":false,"associatedRows":["LSTM with CharCNN embeddings ( Kim et al . , 2016 )","19M","?"],"associatedColumns":["Test"],"associatedMergedColumns":[]},{"number":"82.1","isBolded":false,"associatedRows":["QRNN + zoneout ( p \u003d","18M"],"associatedColumns":["Validation"],"associatedMergedColumns":["Our models"]},{"number":"82.7","isBolded":false,"associatedRows":["LSTM ( medium ) ( Zaremba et al . , 2014 )","20M"],"associatedColumns":["Test"],"associatedMergedColumns":[]},{"number":"78.3","isBolded":false,"associatedRows":["QRNN + zoneout ( p \u003d","18M"],"associatedColumns":["Test"],"associatedMergedColumns":["Our models"]},{"number":"79.7","isBolded":false,"associatedRows":["Variational LSTM ( medium , MC ) ( Gal \u0026 Ghahramani , 2016 )","20M"],"associatedColumns":["Test"],"associatedMergedColumns":[]},{"number":"82.9","isBolded":false,"associatedRows":["QRNN ( medium )","18M"],"associatedColumns":["Validation"],"associatedMergedColumns":["Our models"]},{"number":"85.7","isBolded":false,"associatedRows":["LSTM ( medium )","20M"],"associatedColumns":["Validation"],"associatedMergedColumns":["Our models"]},{"number":"82.0","isBolded":false,"associatedRows":["LSTM ( medium )","20M"],"associatedColumns":["Test"],"associatedMergedColumns":["Our models"]},{"number":"79.9","isBolded":false,"associatedRows":["QRNN ( medium )","18M"],"associatedColumns":["Test"],"associatedMergedColumns":["Our models"]},{"number":"80.6","isBolded":false,"associatedRows":["Zoneout + Variational LSTM ( medium ) ( Merity et al . , 2016 )","20M"],"associatedColumns":["Test"],"associatedMergedColumns":[]},{"number":"86.2","isBolded":false,"associatedRows":["LSTM ( medium ) ( Zaremba et al . , 2014 )","20M"],"associatedColumns":["Validation"],"associatedMergedColumns":[]},{"number":"81.9","isBolded":false,"associatedRows":["Variational LSTM ( medium , MC ) ( Gal \u0026 Ghahramani , 2016 )","20M"],"associatedColumns":["Validation"],"associatedMergedColumns":[]},{"number":"84.4","isBolded":false,"associatedRows":["Zoneout + Variational LSTM ( medium ) ( Merity et al . , 2016 )","20M"],"associatedColumns":["Validation"],"associatedMergedColumns":[]},{"number":"0.1)(medium)","isBolded":false,"associatedRows":["QRNN + zoneout ( p \u003d"],"associatedColumns":["Model"],"associatedMergedColumns":["Our models"]}]},{"caption":"Table 3: Translation performance, measured by BLEU, and train speed in hours per epoch, for the \nIWSLT German-English spoken language translation task. All models were trained on in-domain \ndata only, and use negative log-likelihood as the training criterion. Our models were trained for 10 \nepochs. The QRNN model uses k \u003d 2 for all layers other than the first encoder layer. \n\n","rows":["Char - level 4 - layer LSTM","?","Char - level 4 - layer QRNN with k \u003d 6","Word - level LSTM w / attn ( Ranzato et al . , 2016 )","Word - level CNN w / attn , input feeding ( Wiseman \u0026 Rush , 2016 )"],"columns":["Train Time","BLEU ( TED . tst2014 )"],"mergedAllColumns":["Our models"],"numberCells":[{"number":"20.2","isBolded":false,"associatedRows":["Word - level LSTM w / attn ( Ranzato et al . , 2016 )","?"],"associatedColumns":["BLEU ( TED . tst2014 )"],"associatedMergedColumns":[]},{"number":"1.0hrs/epoch","isBolded":false,"associatedRows":["Char - level 4 - layer QRNN with k \u003d 6"],"associatedColumns":["Train Time"],"associatedMergedColumns":["Our models"]},{"number":"4.2hrs/epoch","isBolded":false,"associatedRows":["Char - level 4 - layer LSTM"],"associatedColumns":["Train Time"],"associatedMergedColumns":["Our models"]},{"number":"16.53","isBolded":false,"associatedRows":["Char - level 4 - layer LSTM","?"],"associatedColumns":["BLEU ( TED . tst2014 )"],"associatedMergedColumns":["Our models"]},{"number":"24.0","isBolded":false,"associatedRows":["Word - level CNN w / attn , input feeding ( Wiseman \u0026 Rush , 2016 )","?"],"associatedColumns":["BLEU ( TED . tst2014 )"],"associatedMergedColumns":[]},{"number":"19.41","isBolded":false,"associatedRows":["Char - level 4 - layer QRNN with k \u003d 6","?"],"associatedColumns":["BLEU ( TED . tst2014 )"],"associatedMergedColumns":["Our models"]}]}]