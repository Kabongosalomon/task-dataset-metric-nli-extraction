[{"caption":"Architecture \nImage Resolution \n224 448 512 \n600 \nConViT \n196 784 1024 1369 \nConviformer \n-\n196 256 \n324 \n\nTable 1: Number of patches (Sequence length) for different input image resolutions. Sequence length is directly \nproportional to the memory demand by the self-attention module. \n\n","rows":["Conviformer","ConViT","-"],"columns":["1369","Image Resolution","1024"],"mergedAllColumns":["Architecture"],"numberCells":[{"number":"448","isBolded":false,"associatedRows":["Conviformer"],"associatedColumns":["Image Resolution"],"associatedMergedColumns":[]},{"number":"196","isBolded":false,"associatedRows":["Conviformer","-"],"associatedColumns":["Image Resolution","1024"],"associatedMergedColumns":["Architecture"]},{"number":"512","isBolded":false,"associatedRows":["Conviformer"],"associatedColumns":["Image Resolution"],"associatedMergedColumns":[]},{"number":"324","isBolded":false,"associatedRows":["Conviformer","-"],"associatedColumns":["Image Resolution","1369"],"associatedMergedColumns":["Architecture"]},{"number":"784","isBolded":false,"associatedRows":["ConViT"],"associatedColumns":["Image Resolution"],"associatedMergedColumns":["Architecture"]},{"number":"600","isBolded":false,"associatedRows":["Conviformer"],"associatedColumns":["Image Resolution"],"associatedMergedColumns":[]},{"number":"196","isBolded":false,"associatedRows":["ConViT"],"associatedColumns":["Image Resolution"],"associatedMergedColumns":["Architecture"]},{"number":"224","isBolded":false,"associatedRows":["Conviformer"],"associatedColumns":["Image Resolution"],"associatedMergedColumns":[]},{"number":"256","isBolded":false,"associatedRows":["Conviformer","-"],"associatedColumns":["Image Resolution","1024"],"associatedMergedColumns":["Architecture"]}]},{"caption":"Table 2: iNaturalist 2019: Accuracy obtained for different input image resolutions while training the model with \nrandom initializing. \n\n","rows":["Accuracy"],"columns":["224","Conviformer","512","600","448","ConViT"],"mergedAllColumns":[],"numberCells":[{"number":"69.92","isBolded":false,"associatedRows":["Accuracy"],"associatedColumns":["224","ConViT"],"associatedMergedColumns":[]},{"number":"80.21","isBolded":false,"associatedRows":["Accuracy"],"associatedColumns":["512","Conviformer"],"associatedMergedColumns":[]},{"number":"75.99","isBolded":false,"associatedRows":["Accuracy"],"associatedColumns":["448","Conviformer"],"associatedMergedColumns":[]},{"number":"80.74","isBolded":false,"associatedRows":["Accuracy"],"associatedColumns":["600","Conviformer"],"associatedMergedColumns":[]}]},{"caption":"Table 3: Herbarium 2021: F1 score on the test set of Herbarium 2021 obtained with different architectures. We also \nmention the input image resolution used in training the network. (  *  means scores are taken from the public leaderboard \nof the Kaggle competition). \n\n","rows":["DeIT ( B ) [ 48 ]","Conviformer ( Ours )","SE ResNeXt - 101 * [ 54 ]","SE ResNeXt - 50 * [ 54 ]"],"columns":["F1 score","Resolution"],"mergedAllColumns":[],"numberCells":[{"number":"224","isBolded":false,"associatedRows":["DeIT ( B ) [ 48 ]"],"associatedColumns":["Resolution"],"associatedMergedColumns":[]},{"number":".729","isBolded":true,"associatedRows":["Conviformer ( Ours )"],"associatedColumns":["F1 score"],"associatedMergedColumns":[]},{"number":"448","isBolded":false,"associatedRows":["SE ResNeXt - 101 * [ 54 ]"],"associatedColumns":["Resolution"],"associatedMergedColumns":[]},{"number":"448","isBolded":false,"associatedRows":["SE ResNeXt - 50 * [ 54 ]"],"associatedColumns":["Resolution"],"associatedMergedColumns":[]},{"number":".708","isBolded":false,"associatedRows":["SE ResNeXt - 50 * [ 54 ]"],"associatedColumns":["F1 score"],"associatedMergedColumns":[]},{"number":".706","isBolded":false,"associatedRows":["DeIT ( B ) [ 48 ]"],"associatedColumns":["F1 score"],"associatedMergedColumns":[]},{"number":"448","isBolded":false,"associatedRows":["Conviformer ( Ours )"],"associatedColumns":["Resolution"],"associatedMergedColumns":[]},{"number":".726","isBolded":false,"associatedRows":["SE ResNeXt - 101 * [ 54 ]"],"associatedColumns":["F1 score"],"associatedMergedColumns":[]}]},{"caption":"Table 7: iNaturalist 2019: Accuracy obtained with different baselines. We also mention the input image resolution \nused in training the network. \n\n","rows":["224","Conviformer ( Ours )","ConViT [ 16 ]","LeViT [ 34 ]","448","L and MAE ( ViT - H ) , which have ?300M and ?600M parameters and perform at","CeiT - S [ 33 ]","MaxMIM ( B ) [ 55 ]","MAE ( ViT - B ) [ 53 ]","384"],"columns":["Accuracy","Table 7 we have summarized the results obtained by the Conviformer and other baselines . Our model could surpass"],"mergedAllColumns":["reported so far with similar parameter models . The other two better performing transformer architectures are MixMIM -"],"numberCells":[{"number":"82.70","isBolded":false,"associatedRows":["CeiT - S [ 33 ]","384"],"associatedColumns":["Table 7 we have summarized the results obtained by the Conviformer and other baselines . Our model could surpass","Accuracy"],"associatedMergedColumns":["reported so far with similar parameter models . The other two better performing transformer architectures are MixMIM -"]},{"number":"74.30","isBolded":false,"associatedRows":["LeViT [ 34 ]","384"],"associatedColumns":["Table 7 we have summarized the results obtained by the Conviformer and other baselines . Our model could surpass","Accuracy"],"associatedMergedColumns":["reported so far with similar parameter models . The other two better performing transformer architectures are MixMIM -"]},{"number":"83.9%and","isBolded":false,"associatedRows":["L and MAE ( ViT - H ) , which have ?300M and ?600M parameters and perform at"],"associatedColumns":["Table 7 we have summarized the results obtained by the Conviformer and other baselines . Our model could surpass"],"associatedMergedColumns":["reported so far with similar parameter models . The other two better performing transformer architectures are MixMIM -"]},{"number":"77.84","isBolded":false,"associatedRows":["ConViT [ 16 ]","224"],"associatedColumns":["Table 7 we have summarized the results obtained by the Conviformer and other baselines . Our model could surpass","Accuracy"],"associatedMergedColumns":["reported so far with similar parameter models . The other two better performing transformer architectures are MixMIM -"]},{"number":"88.3%respectively.","isBolded":false,"associatedRows":["L and MAE ( ViT - H ) , which have ?300M and ?600M parameters and perform at"],"associatedColumns":["Table 7 we have summarized the results obtained by the Conviformer and other baselines . Our model could surpass"],"associatedMergedColumns":["reported so far with similar parameter models . The other two better performing transformer architectures are MixMIM -"]},{"number":"82.60","isBolded":false,"associatedRows":["MaxMIM ( B ) [ 55 ]","224"],"associatedColumns":["Table 7 we have summarized the results obtained by the Conviformer and other baselines . Our model could surpass","Accuracy"],"associatedMergedColumns":["reported so far with similar parameter models . The other two better performing transformer architectures are MixMIM -"]},{"number":"82.85","isBolded":true,"associatedRows":["Conviformer ( Ours )","448"],"associatedColumns":["Table 7 we have summarized the results obtained by the Conviformer and other baselines . Our model could surpass","Accuracy"],"associatedMergedColumns":["reported so far with similar parameter models . The other two better performing transformer architectures are MixMIM -"]},{"number":"80.50","isBolded":false,"associatedRows":["MAE ( ViT - B ) [ 53 ]","224"],"associatedColumns":["Table 7 we have summarized the results obtained by the Conviformer and other baselines . Our model could surpass","Accuracy"],"associatedMergedColumns":["reported so far with similar parameter models . The other two better performing transformer architectures are MixMIM -"]}]}]