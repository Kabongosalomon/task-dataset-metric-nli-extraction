[{"caption":"Table 1. Open-vocabulary and zero-shot performance on LVIS v1.0 val. For our mod-\nels, we remove annotations matching LVIS rare category names from all detection \ntraining datasets, such that AP LVIS \nrare measures zero-shot performance. Gray numbers \nindicate models trained on the LVIS frequent and common (\"base\") annotations. For \nreference, ViT-B/32 is comparable to ResNet50 in inference compute (139.6 vs 141.5 \nGFLOPs). For our models, we report the mean performance over three fine-tuning \nruns. Results for COCO and O365 are provided in Appendix A1.8. \n\n","rows":["O365 , GoldG , . . .","CC12M , SBU","CC3M","LVIS base","ViT - H / 14","CLIP","ALIGN","R50x4 - C4","ViLD - ens [ 12 ]","EffNet - b7","OWL - ViT ( ours )","LiT","Cap4M","10","11","12","13","14","R26+B / 32","15","16","O365 , VG","ViT - B / 16","1","2","3","Swin - L","4","840","ResNet50","5","1024","6","ViT - B / 32","7","8","Reg . CLIP [ 45 ]","9","768","Swin - T","ViT - L / 16","R50 - C4","ViT - L / 14","GLIP [ 26 ]","?","OI , O365 , VG , . . ."],"columns":["AP LVIS"],"mergedAllColumns":["LVIS base training :","Unrestricted open - vocabulary training :"],"numberCells":[{"number":"35.3","isBolded":false,"associatedRows":["5","OWL - ViT ( ours )","ViT - H / 14","LiT","LVIS base","840"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["LVIS base training :"]},{"number":"10.1","isBolded":false,"associatedRows":["7","GLIP [ 26 ]","Swin - T","Cap4M","O365 , GoldG , . . .","?"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["Unrestricted open - vocabulary training :"]},{"number":"23.3","isBolded":false,"associatedRows":["9","OWL - ViT ( ours )","ViT - B / 32","LiT","O365 , VG","768"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["Unrestricted open - vocabulary training :"]},{"number":"30.9","isBolded":false,"associatedRows":["12","OWL - ViT ( ours )","ViT - L / 16","LiT","O365 , VG","768"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["Unrestricted open - vocabulary training :"]},{"number":"25.6","isBolded":false,"associatedRows":["6","OWL - ViT ( ours )","ViT - L / 14","CLIP","LVIS base","840"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["LVIS base training :"]},{"number":"30.6","isBolded":false,"associatedRows":["13","OWL - ViT ( ours )","ViT - H / 14","LiT","O365 , VG","840"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["Unrestricted open - vocabulary training :"]},{"number":"20.6","isBolded":false,"associatedRows":["15","OWL - ViT ( ours )","ViT - B / 16","CLIP","O365 , VG","768"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["Unrestricted open - vocabulary training :"]},{"number":"17.2","isBolded":false,"associatedRows":["7","GLIP [ 26 ]","Swin - T","Cap4M","O365 , GoldG , . . .","?"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["Unrestricted open - vocabulary training :"]},{"number":"32.3","isBolded":false,"associatedRows":["4","Reg . CLIP [ 45 ]","R50x4 - C4","CC3M","LVIS base","?"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["LVIS base training :"]},{"number":"21.6","isBolded":false,"associatedRows":["11","OWL - ViT ( ours )","R26+B / 32","LiT","O365 , VG","768"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["Unrestricted open - vocabulary training :"]},{"number":"29.3","isBolded":false,"associatedRows":["2","ViLD - ens [ 12 ]","EffNet - b7","ALIGN","LVIS base","1024"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["LVIS base training :"]},{"number":"16.6","isBolded":false,"associatedRows":["1","ViLD - ens [ 12 ]","ResNet50","CLIP","LVIS base","1024"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["LVIS base training :"]},{"number":"26.3","isBolded":false,"associatedRows":["2","ViLD - ens [ 12 ]","EffNet - b7","ALIGN","LVIS base","1024"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["LVIS base training :"]},{"number":"17.1","isBolded":false,"associatedRows":["3","Reg . CLIP [ 45 ]","R50 - C4","CC3M","LVIS base","?"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["LVIS base training :"]},{"number":"22.1","isBolded":false,"associatedRows":["14","OWL - ViT ( ours )","ViT - B / 32","CLIP","O365 , VG","768"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["Unrestricted open - vocabulary training :"]},{"number":"27.2","isBolded":false,"associatedRows":["15","OWL - ViT ( ours )","ViT - B / 16","CLIP","O365 , VG","768"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["Unrestricted open - vocabulary training :"]},{"number":"28.8","isBolded":false,"associatedRows":["12","OWL - ViT ( ours )","ViT - L / 16","LiT","O365 , VG","768"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["Unrestricted open - vocabulary training :"]},{"number":"31.2","isBolded":false,"associatedRows":["16","OWL - ViT ( ours )","ViT - L / 14","CLIP","O365 , VG","840"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["Unrestricted open - vocabulary training :"]},{"number":"33.6","isBolded":false,"associatedRows":["13","OWL - ViT ( ours )","ViT - H / 14","LiT","O365 , VG","840"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["Unrestricted open - vocabulary training :"]},{"number":"19.7","isBolded":false,"associatedRows":["9","OWL - ViT ( ours )","ViT - B / 32","LiT","O365 , VG","768"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["Unrestricted open - vocabulary training :"]},{"number":"23.6","isBolded":false,"associatedRows":["10","OWL - ViT ( ours )","ViT - B / 16","LiT","O365 , VG","768"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["Unrestricted open - vocabulary training :"]},{"number":"17.1","isBolded":false,"associatedRows":["8","GLIP [ 26 ]","Swin - L","CC12M , SBU","OI , O365 , VG , . . .","?"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["Unrestricted open - vocabulary training :"]},{"number":"25.7","isBolded":false,"associatedRows":["11","OWL - ViT ( ours )","R26+B / 32","LiT","O365 , VG","768"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["Unrestricted open - vocabulary training :"]},{"number":"23.3","isBolded":false,"associatedRows":["5","OWL - ViT ( ours )","ViT - H / 14","LiT","LVIS base","840"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["LVIS base training :"]},{"number":"34.6","isBolded":false,"associatedRows":["16","OWL - ViT ( ours )","ViT - L / 14","CLIP","O365 , VG","840"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["Unrestricted open - vocabulary training :"]},{"number":"34.7","isBolded":false,"associatedRows":["6","OWL - ViT ( ours )","ViT - L / 14","CLIP","LVIS base","840"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["LVIS base training :"]},{"number":"26.7","isBolded":false,"associatedRows":["10","OWL - ViT ( ours )","ViT - B / 16","LiT","O365 , VG","768"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["Unrestricted open - vocabulary training :"]},{"number":"28.2","isBolded":false,"associatedRows":["3","Reg . CLIP [ 45 ]","R50 - C4","CC3M","LVIS base","?"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["LVIS base training :"]},{"number":"26.9","isBolded":false,"associatedRows":["8","GLIP [ 26 ]","Swin - L","CC12M , SBU","OI , O365 , VG , . . .","?"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["Unrestricted open - vocabulary training :"]},{"number":"18.9","isBolded":false,"associatedRows":["14","OWL - ViT ( ours )","ViT - B / 32","CLIP","O365 , VG","768"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["Unrestricted open - vocabulary training :"]},{"number":"25.5","isBolded":false,"associatedRows":["1","ViLD - ens [ 12 ]","ResNet50","CLIP","LVIS base","1024"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["LVIS base training :"]},{"number":"22.0","isBolded":false,"associatedRows":["4","Reg . CLIP [ 45 ]","R50x4 - C4","CC3M","LVIS base","?"],"associatedColumns":["AP LVIS"],"associatedMergedColumns":["LVIS base training :"]}]},{"caption":"Table 2. One-and few-shot image-conditioned detection performance on COCO AP50. \nOur method (R50+H/32 architecture) strongly outperforms prior work and also shows \nmarked improvements as the number of conditioning queries is increased to k \u003d 10. \nCOCO category splits as in ","rows":["AIT [ 7 ]","OWL - ViT ( ours )","SiamMask [ 30 ]","CoAE [ 16 ]","Unseen","Seen","OWL - ViT ( k \u003d 10 ; ours )"],"columns":["Mean","Split 4","Split 3","Split 2","Split 1"],"mergedAllColumns":[],"numberCells":[{"number":"17.6","isBolded":false,"associatedRows":["Unseen","SiamMask [ 30 ]"],"associatedColumns":["Split 2"],"associatedMergedColumns":[]},{"number":"44.5","isBolded":false,"associatedRows":["Unseen","OWL - ViT ( k \u003d 10 ; ours )"],"associatedColumns":["Split 4"],"associatedMergedColumns":[]},{"number":"36.6","isBolded":false,"associatedRows":["Seen","SiamMask [ 30 ]"],"associatedColumns":["Split 4"],"associatedMergedColumns":[]},{"number":"50.1","isBolded":true,"associatedRows":["Seen","AIT [ 7 ]"],"associatedColumns":["Split 1"],"associatedMergedColumns":[]},{"number":"22.3","isBolded":false,"associatedRows":["Unseen","AIT [ 7 ]"],"associatedColumns":["Split 3"],"associatedMergedColumns":[]},{"number":"54.1","isBolded":false,"associatedRows":["Seen","OWL - ViT ( k \u003d 10 ; ours )"],"associatedColumns":["Split 1"],"associatedMergedColumns":[]},{"number":"42.4","isBolded":false,"associatedRows":["Unseen","OWL - ViT ( k \u003d 10 ; ours )"],"associatedColumns":["Split 3"],"associatedMergedColumns":[]},{"number":"41.3","isBolded":false,"associatedRows":["Seen","CoAE [ 16 ]"],"associatedColumns":["Split 4"],"associatedMergedColumns":[]},{"number":"20.4","isBolded":false,"associatedRows":["Unseen","CoAE [ 16 ]"],"associatedColumns":["Split 4"],"associatedMergedColumns":[]},{"number":"22.0","isBolded":false,"associatedRows":["Unseen","CoAE [ 16 ]"],"associatedColumns":["Mean"],"associatedMergedColumns":[]},{"number":"45.8","isBolded":false,"associatedRows":["Seen","AIT [ 7 ]"],"associatedColumns":["Split 3"],"associatedMergedColumns":[]},{"number":"17.0","isBolded":false,"associatedRows":["Unseen","SiamMask [ 30 ]"],"associatedColumns":["Split 4"],"associatedMergedColumns":[]},{"number":"22.6","isBolded":false,"associatedRows":["Unseen","AIT [ 7 ]"],"associatedColumns":["Split 4"],"associatedMergedColumns":[]},{"number":"23.4","isBolded":false,"associatedRows":["Unseen","CoAE [ 16 ]"],"associatedColumns":["Split 1"],"associatedMergedColumns":[]},{"number":"41.3","isBolded":true,"associatedRows":["Unseen","OWL - ViT ( ours )"],"associatedColumns":["Split 2"],"associatedMergedColumns":[]},{"number":"49.1","isBolded":true,"associatedRows":["Seen","OWL - ViT ( ours )"],"associatedColumns":["Mean"],"associatedMergedColumns":[]},{"number":"40.2","isBolded":false,"associatedRows":["Seen","CoAE [ 16 ]"],"associatedColumns":["Split 2"],"associatedMergedColumns":[]},{"number":"49.9","isBolded":false,"associatedRows":["Seen","OWL - ViT ( ours )"],"associatedColumns":["Split 1"],"associatedMergedColumns":[]},{"number":"16.8","isBolded":false,"associatedRows":["Unseen","SiamMask [ 30 ]"],"associatedColumns":["Mean"],"associatedMergedColumns":[]},{"number":"42.2","isBolded":false,"associatedRows":["Seen","CoAE [ 16 ]"],"associatedColumns":["Split 1"],"associatedMergedColumns":[]},{"number":"40.9","isBolded":false,"associatedRows":["Seen","CoAE [ 16 ]"],"associatedColumns":["Mean"],"associatedMergedColumns":[]},{"number":"48.2","isBolded":true,"associatedRows":["Seen","OWL - ViT ( ours )"],"associatedColumns":["Split 4"],"associatedMergedColumns":[]},{"number":"23.6","isBolded":false,"associatedRows":["Unseen","CoAE [ 16 ]"],"associatedColumns":["Split 2"],"associatedMergedColumns":[]},{"number":"49.1","isBolded":true,"associatedRows":["Seen","OWL - ViT ( ours )"],"associatedColumns":["Split 2"],"associatedMergedColumns":[]},{"number":"43.6","isBolded":true,"associatedRows":["Unseen","OWL - ViT ( ours )"],"associatedColumns":["Split 1"],"associatedMergedColumns":[]},{"number":"47.2","isBolded":false,"associatedRows":["Seen","AIT [ 7 ]"],"associatedColumns":["Split 2"],"associatedMergedColumns":[]},{"number":"37.6","isBolded":false,"associatedRows":["Seen","SiamMask [ 30 ]"],"associatedColumns":["Mean"],"associatedMergedColumns":[]},{"number":"15.3","isBolded":false,"associatedRows":["Unseen","SiamMask [ 30 ]"],"associatedColumns":["Split 1"],"associatedMergedColumns":[]},{"number":"40.2","isBolded":true,"associatedRows":["Unseen","OWL - ViT ( ours )"],"associatedColumns":["Split 3"],"associatedMergedColumns":[]},{"number":"46.8","isBolded":false,"associatedRows":["Unseen","OWL - ViT ( k \u003d 10 ; ours )"],"associatedColumns":["Mean"],"associatedMergedColumns":[]},{"number":"41.8","isBolded":true,"associatedRows":["Unseen","OWL - ViT ( ours )"],"associatedColumns":["Mean"],"associatedMergedColumns":[]},{"number":"37.8","isBolded":false,"associatedRows":["Seen","SiamMask [ 30 ]"],"associatedColumns":["Split 3"],"associatedMergedColumns":[]},{"number":"49.3","isBolded":false,"associatedRows":["Unseen","OWL - ViT ( k \u003d 10 ; ours )"],"associatedColumns":["Split 1"],"associatedMergedColumns":[]},{"number":"20.5","isBolded":false,"associatedRows":["Unseen","CoAE [ 16 ]"],"associatedColumns":["Split 3"],"associatedMergedColumns":[]},{"number":"24.3","isBolded":false,"associatedRows":["Unseen","AIT [ 7 ]"],"associatedColumns":["Mean"],"associatedMergedColumns":[]},{"number":"26.0","isBolded":false,"associatedRows":["Unseen","AIT [ 7 ]"],"associatedColumns":["Split 1"],"associatedMergedColumns":[]},{"number":"37.1","isBolded":false,"associatedRows":["Seen","SiamMask [ 30 ]"],"associatedColumns":["Split 2"],"associatedMergedColumns":[]},{"number":"54.9","isBolded":false,"associatedRows":["Seen","OWL - ViT ( k \u003d 10 ; ours )"],"associatedColumns":["Split 4"],"associatedMergedColumns":[]},{"number":"26.4","isBolded":false,"associatedRows":["Unseen","AIT [ 7 ]"],"associatedColumns":["Split 2"],"associatedMergedColumns":[]},{"number":"17.4","isBolded":false,"associatedRows":["Unseen","SiamMask [ 30 ]"],"associatedColumns":["Split 3"],"associatedMergedColumns":[]},{"number":"38.9","isBolded":false,"associatedRows":["Seen","SiamMask [ 30 ]"],"associatedColumns":["Split 1"],"associatedMergedColumns":[]},{"number":"55.3","isBolded":false,"associatedRows":["Seen","OWL - ViT ( k \u003d 10 ; ours )"],"associatedColumns":["Split 2"],"associatedMergedColumns":[]},{"number":"39.9","isBolded":false,"associatedRows":["Seen","CoAE [ 16 ]"],"associatedColumns":["Split 3"],"associatedMergedColumns":[]},{"number":"47.5","isBolded":false,"associatedRows":["Seen","AIT [ 7 ]"],"associatedColumns":["Mean"],"associatedMergedColumns":[]},{"number":"49.2","isBolded":true,"associatedRows":["Seen","OWL - ViT ( ours )"],"associatedColumns":["Split 3"],"associatedMergedColumns":[]},{"number":"41.9","isBolded":true,"associatedRows":["Unseen","OWL - ViT ( ours )"],"associatedColumns":["Split 4"],"associatedMergedColumns":[]},{"number":"46.9","isBolded":false,"associatedRows":["Seen","AIT [ 7 ]"],"associatedColumns":["Split 4"],"associatedMergedColumns":[]},{"number":"51.1","isBolded":false,"associatedRows":["Unseen","OWL - ViT ( k \u003d 10 ; ours )"],"associatedColumns":["Split 2"],"associatedMergedColumns":[]},{"number":"55.1","isBolded":false,"associatedRows":["Seen","OWL - ViT ( k \u003d 10 ; ours )"],"associatedColumns":["Mean"],"associatedMergedColumns":[]},{"number":"56.2","isBolded":false,"associatedRows":["Seen","OWL - ViT ( k \u003d 10 ; ours )"],"associatedColumns":["Split 3"],"associatedMergedColumns":[]}]},{"caption":"Table 3. Ablation study of the main methodological improvements necessary for suc-\ncessful transfer of image-text models to detection. For simplicity, difference in AP to \nthe baseline is shown. Except for the experiment retraining LVIS rare labels (last row), \nall differences are expected to be negative. To reduce variance, all results are averaged \nacross two replicates. All ablations were carried out for the ViT-R26+B/32 model, and \nunless otherwise specified used a 70K step training schedule. \n\n","rows":["( 11 ) No location bias in box predictor","( 14 ) Do not remove OI crowd instances","( 9 ) No mosaics , train 3x longer","( 1 ) Only use VG for training","( 2 ) Only use OI for training","( 15 ) Do not remove LVIS rare labels","( 5 ) No prompts ( train or inference )","( 6 ) No random negatives","( 12 ) Do not filter out any cropped boxes","Baseline","( 7 ) No mosaics","( 10 ) Do not merge overlapping instances","( 3 ) Same LR for image and text encoders","( 13 ) Filter out all cropped boxes","( 8 ) No mosaics , train 2x longer","( 4 ) No prompt ensembling at inference"],"columns":["LVIS","rare","COCO","OI"],"mergedAllColumns":[],"numberCells":[{"number":"0.2","isBolded":true,"associatedRows":["( 15 ) Do not remove LVIS rare labels"],"associatedColumns":["LVIS"],"associatedMergedColumns":[]},{"number":"0.1","isBolded":true,"associatedRows":["( 15 ) Do not remove LVIS rare labels"],"associatedColumns":["LVIS"],"associatedMergedColumns":[]},{"number":"?3.4","isBolded":false,"associatedRows":["( 9 ) No mosaics , train 3x longer"],"associatedColumns":["LVIS"],"associatedMergedColumns":[]},{"number":"?0.7","isBolded":false,"associatedRows":["( 7 ) No mosaics"],"associatedColumns":["OI"],"associatedMergedColumns":[]},{"number":"?0.4","isBolded":false,"associatedRows":["( 6 ) No random negatives"],"associatedColumns":["COCO"],"associatedMergedColumns":[]},{"number":"?0.8","isBolded":false,"associatedRows":["( 9 ) No mosaics , train 3x longer"],"associatedColumns":["OI"],"associatedMergedColumns":[]},{"number":"?0.6","isBolded":false,"associatedRows":["( 13 ) Filter out all cropped boxes"],"associatedColumns":["LVIS","rare"],"associatedMergedColumns":[]},{"number":"?1.8","isBolded":false,"associatedRows":["( 8 ) No mosaics , train 2x longer"],"associatedColumns":["COCO"],"associatedMergedColumns":[]},{"number":"?8.5","isBolded":false,"associatedRows":["( 3 ) Same LR for image and text encoders"],"associatedColumns":["LVIS","rare"],"associatedMergedColumns":[]},{"number":"?0.6","isBolded":false,"associatedRows":["( 5 ) No prompts ( train or inference )"],"associatedColumns":["COCO"],"associatedMergedColumns":[]},{"number":"?1.5","isBolded":false,"associatedRows":["( 7 ) No mosaics"],"associatedColumns":["LVIS","rare"],"associatedMergedColumns":[]},{"number":"?0.1","isBolded":false,"associatedRows":["( 12 ) Do not filter out any cropped boxes"],"associatedColumns":["OI"],"associatedMergedColumns":[]},{"number":"21.0","isBolded":true,"associatedRows":["Baseline","( 3 ) Same LR for image and text encoders"],"associatedColumns":["LVIS"],"associatedMergedColumns":[]},{"number":"?1.0","isBolded":false,"associatedRows":["( 6 ) No random negatives"],"associatedColumns":["LVIS"],"associatedMergedColumns":[]},{"number":"?3.0","isBolded":false,"associatedRows":["( 3 ) Same LR for image and text encoders"],"associatedColumns":["LVIS"],"associatedMergedColumns":[]},{"number":"?1.2","isBolded":false,"associatedRows":["( 5 ) No prompts ( train or inference )"],"associatedColumns":["LVIS"],"associatedMergedColumns":[]},{"number":"?2.3","isBolded":false,"associatedRows":["( 7 ) No mosaics"],"associatedColumns":["LVIS"],"associatedMergedColumns":[]},{"number":"?1.1","isBolded":false,"associatedRows":["( 11 ) No location bias in box predictor"],"associatedColumns":["LVIS","rare"],"associatedMergedColumns":[]},{"number":"0.1","isBolded":true,"associatedRows":["( 12 ) Do not filter out any cropped boxes"],"associatedColumns":["COCO"],"associatedMergedColumns":[]},{"number":"?5.5","isBolded":false,"associatedRows":["( 4 ) No prompt ensembling at inference"],"associatedColumns":["LVIS","rare"],"associatedMergedColumns":[]},{"number":"?1.3","isBolded":false,"associatedRows":["( 10 ) Do not merge overlapping instances"],"associatedColumns":["LVIS","rare"],"associatedMergedColumns":[]},{"number":"?1.0","isBolded":false,"associatedRows":["( 11 ) No location bias in box predictor"],"associatedColumns":["OI"],"associatedMergedColumns":[]},{"number":"?0.7","isBolded":false,"associatedRows":["( 8 ) No mosaics , train 2x longer"],"associatedColumns":["OI"],"associatedMergedColumns":[]},{"number":"?0.6","isBolded":false,"associatedRows":["( 10 ) Do not merge overlapping instances"],"associatedColumns":["COCO"],"associatedMergedColumns":[]},{"number":"?0.4","isBolded":false,"associatedRows":["( 14 ) Do not remove OI crowd instances"],"associatedColumns":["COCO"],"associatedMergedColumns":[]},{"number":"0.7","isBolded":true,"associatedRows":["( 14 ) Do not remove OI crowd instances"],"associatedColumns":["LVIS"],"associatedMergedColumns":[]},{"number":"0.2","isBolded":true,"associatedRows":["( 13 ) Filter out all cropped boxes"],"associatedColumns":["OI"],"associatedMergedColumns":[]},{"number":"?1.8","isBolded":false,"associatedRows":["( 9 ) No mosaics , train 3x longer"],"associatedColumns":["COCO"],"associatedMergedColumns":[]},{"number":"0.0","isBolded":true,"associatedRows":["( 12 ) Do not filter out any cropped boxes"],"associatedColumns":["LVIS"],"associatedMergedColumns":[]},{"number":"?5.9","isBolded":false,"associatedRows":["( 4 ) No prompt ensembling at inference"],"associatedColumns":["COCO"],"associatedMergedColumns":[]},{"number":"18.9","isBolded":true,"associatedRows":["Baseline","( 3 ) Same LR for image and text encoders"],"associatedColumns":["LVIS","rare"],"associatedMergedColumns":[]},{"number":"?1.3","isBolded":false,"associatedRows":["( 11 ) No location bias in box predictor"],"associatedColumns":["COCO"],"associatedMergedColumns":[]},{"number":"0.0","isBolded":true,"associatedRows":["( 14 ) Do not remove OI crowd instances"],"associatedColumns":["LVIS"],"associatedMergedColumns":[]},{"number":"?14.5","isBolded":false,"associatedRows":["( 1 ) Only use VG for training"],"associatedColumns":["LVIS"],"associatedMergedColumns":[]},{"number":"?2.8","isBolded":false,"associatedRows":["( 8 ) No mosaics , train 2x longer"],"associatedColumns":["LVIS","rare"],"associatedMergedColumns":[]},{"number":"?1.3","isBolded":false,"associatedRows":["( 5 ) No prompts ( train or inference )"],"associatedColumns":["LVIS","rare"],"associatedMergedColumns":[]},{"number":"?38.3","isBolded":false,"associatedRows":["( 1 ) Only use VG for training"],"associatedColumns":["OI"],"associatedMergedColumns":[]},{"number":"?0.5","isBolded":false,"associatedRows":["( 3 ) Same LR for image and text encoders"],"associatedColumns":["COCO"],"associatedMergedColumns":[]},{"number":"?0.7","isBolded":false,"associatedRows":["( 10 ) Do not merge overlapping instances"],"associatedColumns":["OI"],"associatedMergedColumns":[]},{"number":"3.0","isBolded":true,"associatedRows":["( 14 ) Do not remove OI crowd instances"],"associatedColumns":["OI"],"associatedMergedColumns":[]},{"number":"?0.1","isBolded":false,"associatedRows":["( 15 ) Do not remove LVIS rare labels"],"associatedColumns":["COCO"],"associatedMergedColumns":[]},{"number":"?6.3","isBolded":false,"associatedRows":["( 5 ) No prompts ( train or inference )"],"associatedColumns":["OI"],"associatedMergedColumns":[]},{"number":"30.9","isBolded":true,"associatedRows":["Baseline","( 3 ) Same LR for image and text encoders"],"associatedColumns":["COCO"],"associatedMergedColumns":[]},{"number":"?2.8","isBolded":false,"associatedRows":["( 6 ) No random negatives"],"associatedColumns":["LVIS","rare"],"associatedMergedColumns":[]},{"number":"0.1","isBolded":true,"associatedRows":["( 13 ) Filter out all cropped boxes"],"associatedColumns":["COCO"],"associatedMergedColumns":[]},{"number":"?0.1","isBolded":false,"associatedRows":["( 13 ) Filter out all cropped boxes"],"associatedColumns":["LVIS"],"associatedMergedColumns":[]},{"number":"?6.9","isBolded":false,"associatedRows":["( 2 ) Only use OI for training"],"associatedColumns":["LVIS"],"associatedMergedColumns":[]},{"number":"?0.1","isBolded":false,"associatedRows":["( 4 ) No prompt ensembling at inference"],"associatedColumns":["OI"],"associatedMergedColumns":[]},{"number":"?1.2","isBolded":false,"associatedRows":["( 11 ) No location bias in box predictor"],"associatedColumns":["LVIS"],"associatedMergedColumns":[]},{"number":"?14.0","isBolded":false,"associatedRows":["( 1 ) Only use VG for training"],"associatedColumns":["LVIS","rare"],"associatedMergedColumns":[]},{"number":"?0.8","isBolded":false,"associatedRows":["( 10 ) Do not merge overlapping instances"],"associatedColumns":["LVIS"],"associatedMergedColumns":[]},{"number":"?0.1","isBolded":false,"associatedRows":["( 12 ) Do not filter out any cropped boxes"],"associatedColumns":["LVIS"],"associatedMergedColumns":[]},{"number":"0.3","isBolded":true,"associatedRows":["( 2 ) Only use OI for training"],"associatedColumns":["OI"],"associatedMergedColumns":[]},{"number":"0.4","isBolded":true,"associatedRows":["( 3 ) Same LR for image and text encoders"],"associatedColumns":["OI"],"associatedMergedColumns":[]},{"number":"?23.6","isBolded":false,"associatedRows":["( 1 ) Only use VG for training"],"associatedColumns":["COCO"],"associatedMergedColumns":[]},{"number":"?5.7","isBolded":false,"associatedRows":["( 2 ) Only use OI for training"],"associatedColumns":["LVIS","rare"],"associatedMergedColumns":[]},{"number":"54.1","isBolded":true,"associatedRows":["Baseline","( 3 ) Same LR for image and text encoders"],"associatedColumns":["OI"],"associatedMergedColumns":[]},{"number":"1.0","isBolded":true,"associatedRows":["( 6 ) No random negatives"],"associatedColumns":["OI"],"associatedMergedColumns":[]},{"number":"?2.8","isBolded":false,"associatedRows":["( 4 ) No prompt ensembling at inference"],"associatedColumns":["LVIS"],"associatedMergedColumns":[]},{"number":"?3.6","isBolded":false,"associatedRows":["( 9 ) No mosaics , train 3x longer"],"associatedColumns":["LVIS","rare"],"associatedMergedColumns":[]},{"number":"1.1","isBolded":true,"associatedRows":["( 15 ) Do not remove LVIS rare labels"],"associatedColumns":["OI"],"associatedMergedColumns":[]},{"number":"?2.9","isBolded":false,"associatedRows":["( 8 ) No mosaics , train 2x longer"],"associatedColumns":["LVIS"],"associatedMergedColumns":[]},{"number":"?1.7","isBolded":false,"associatedRows":["( 7 ) No mosaics"],"associatedColumns":["COCO"],"associatedMergedColumns":[]},{"number":"?4.2","isBolded":false,"associatedRows":["( 2 ) Only use OI for training"],"associatedColumns":["COCO"],"associatedMergedColumns":[]}]},{"caption":"Training duration \nBatch size \nLearning rate \nWeight decay \nImage size \nPool type \nTraining steps \nBatch size \nLearning rate \nWeight decay \nDroplayer rate \n\nImage size \nTraining datasets \nDataset proportions \nMosaic proportions \nRandom negatives \n\nModel \nImage-level pre-training \nDetection fine-tuning \n\nCLIP-based OWL-ViT models from Table 1: \nB/32 \n140k 256 5 ? 10 ?5 0 .2/.1 768 \nO365, VG \n.8/.2 \n.4/.3/.3 yes \nB/16 \n140k 256 5 ? 10 ?5 0 .2/.1 768 \nO365, VG \n.8/.2 \n.4/.3/.3 yes \nL/14 \n70k 256 2 ? 10 ?5 0 .2/.1 840 \nO365, VG \n.8/.2 \n.4/.3/.3 yes \n\nLiT-based OWL-ViT models from Table 1: \nB/32 \n16B 16k 3 ? 10 ?4 1 ? 10 ?5 224 MAP 140k 256 2 ? 10 ?4 0 0.0 768 \nO365, VG \n.8/.2 \n.4/.3/.3 yes \nB/16 \n8B 16k 3 ? 10 ?4 1 ? 10 ?5 224 MAP 140k 256 2 ? 10 ?4 0 0.0 768 \nO365, VG \n.8/.2 \n.4/.3/.3 yes \nR26+B/32 16B 16k 3 ? 10 ?4 1 ? 10 ?5 288 MAP 140k 256 2 ? 10 ?4 0 0.0 768 \nO365, VG \n.8/.2 \n.4/.3/.3 yes \nL/16 \n16B 16k 3 ? 10 ?4 1 ? 10 ?5 224 MAP 70k 256 5 ? 10 ?5 0 0.0 768 \nO365, VG \n.8/.2 \n.4/.3/.3 yes \nH/14 \n12B 16k 3 ? 10 ?4 1 ? 10 ?5 224 MAP 70k 256 5 ? 10 ?5 0 .1/.0 840 \nO365, VG \n.8/.2 \n.4/.3/.3 yes \n\nModel used for one-shot detection (Table 2): \nR50+H/32 24B 12k 7 ? 10 ?4 1 ? 10 ?5 224 GAP 28k 256 2 ? 10 ?4 0 0.1 960 OI, O365, VG .4/.4/.2 .5/.33/.17 yes \n\n","rows":["L / 16","R50+H / 32 24B 12k","8B","MAP 140k 256","2B","12B 16k","256","224 MAP 140k 256","R26+B / 32","28k","288 MAP","16k","224 MAP","140k 256","*","12k","16B 16k","B / 16","R50+H / 32","Models used in the scaling study ( Figures","70k","224","B / 32","GAP","H / 14","288 MAP 140k 256"],"columns":["Detection fine - tuning","Droplayer",". 2 / . 1 840","decay","type","Image",". 2 / . 1 768","Weight","Learning","size","rate",". 1 / . 0 840","? 5","? 4","768","Image - level pre - training"],"mergedAllColumns":["Model used for one - shot detection ( Table 2 ) :","? 5","LiT - based OWL - ViT models from Table 1 :","CLIP - based OWL - ViT models from Table 1 :",":"],"numberCells":[{"number":"5?10","isBolded":false,"associatedRows":["L / 16","16B 16k","224 MAP","70k","256"],"associatedColumns":["rate","size","Learning","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"0.0","isBolded":false,"associatedRows":["L / 16","16B 16k","224 MAP","70k","256"],"associatedColumns":["rate","size","Droplayer","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"0","isBolded":false,"associatedRows":["B / 16","8B","16k","224 MAP 140k 256","256"],"associatedColumns":["decay","size","Weight","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"3?10","isBolded":false,"associatedRows":["B / 32","2B","16k"],"associatedColumns":["rate","size","Learning","Image - level pre - training",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768","? 5",". 1 / . 0 840","? 4"],"associatedMergedColumns":[":"]},{"number":"1?10","isBolded":false,"associatedRows":["B / 16","8B","16k"],"associatedColumns":["decay","type","Weight","Image - level pre - training",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 5","768"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"1?10","isBolded":false,"associatedRows":["B / 32","2B","16k"],"associatedColumns":["decay","type","Weight","Image - level pre - training",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 5","768","? 5","768","? 5","768","? 5","768","? 5",". 1 / . 0 840","? 5"],"associatedMergedColumns":[":"]},{"number":"1?10","isBolded":false,"associatedRows":["L / 16","16B 16k"],"associatedColumns":["decay","type","Weight","Image - level pre - training",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 5","768","? 5","768","? 5","768"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"1?10","isBolded":false,"associatedRows":["B / 32","16B 16k"],"associatedColumns":["decay","type","Weight","Image - level pre - training",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"0.0","isBolded":false,"associatedRows":["R26+B / 32","8B","16k","288 MAP","70k","256"],"associatedColumns":["rate","size","Droplayer","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768","? 5",". 1 / . 0 840","? 4","? 4","768"],"associatedMergedColumns":[":"]},{"number":"0","isBolded":false,"associatedRows":["Models used in the scaling study ( Figures","224","GAP","28k","256"],"associatedColumns":["decay","size","Weight","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768","? 5",". 1 / . 0 840"],"associatedMergedColumns":["Model used for one - shot detection ( Table 2 ) :"]},{"number":"1?10","isBolded":false,"associatedRows":["R50+H / 32","*","12k"],"associatedColumns":["decay","type","Weight","Image - level pre - training",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 5","768","? 5","768","? 5","768","? 5","768","? 5",". 1 / . 0 840","? 5","? 5","768","? 5","768","768"],"associatedMergedColumns":[":"]},{"number":"3?10","isBolded":false,"associatedRows":["B / 16","8B","16k"],"associatedColumns":["rate","size","Learning","Image - level pre - training",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"2?10","isBolded":false,"associatedRows":["B / 16","8B","16k","224 MAP 140k 256","256"],"associatedColumns":["rate","size","Learning","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"3?10?4","isBolded":false,"associatedRows":["L / 16","16B 16k"],"associatedColumns":["rate","size","Learning","Image - level pre - training",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"3?10","isBolded":false,"associatedRows":["R26+B / 32","16B 16k"],"associatedColumns":["rate","size","Learning","Image - level pre - training",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"0","isBolded":false,"associatedRows":["Models used in the scaling study ( Figures","224 MAP 140k 256","140k 256"],"associatedColumns":["decay","size","Weight","Detection fine - tuning",". 2 / . 1 768"],"associatedMergedColumns":["? 5"]},{"number":"0","isBolded":false,"associatedRows":["B / 32","2B","16k","224 MAP","70k","256"],"associatedColumns":["decay","size","Weight","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768","? 5",". 1 / . 0 840","? 4"],"associatedMergedColumns":[":"]},{"number":"3?10","isBolded":false,"associatedRows":["B / 32","16B 16k"],"associatedColumns":["rate","size","Learning","Image - level pre - training",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"0","isBolded":false,"associatedRows":["R26+B / 32","16B 16k","288 MAP 140k 256","256"],"associatedColumns":["decay","size","Weight","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"0.0","isBolded":false,"associatedRows":["B / 16","8B","16k","224 MAP 140k 256","256"],"associatedColumns":["rate","size","Droplayer","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"0","isBolded":false,"associatedRows":["H / 14","12B 16k","224 MAP","70k","256"],"associatedColumns":["decay","size","Weight","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"0","isBolded":false,"associatedRows":["B / 32","16B 16k","224 MAP 140k 256","256"],"associatedColumns":["decay","size","Weight","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"0.0","isBolded":false,"associatedRows":["B / 32","2B","16k","224 MAP","70k","256"],"associatedColumns":["rate","size","Droplayer","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768","? 5",". 1 / . 0 840","? 4"],"associatedMergedColumns":[":"]},{"number":"2?10","isBolded":false,"associatedRows":["B / 32","16B 16k","224 MAP 140k 256","256"],"associatedColumns":["rate","size","Learning","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"0.1","isBolded":false,"associatedRows":["Models used in the scaling study ( Figures","224","GAP","28k","256"],"associatedColumns":["rate","size","Droplayer","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768","? 5",". 1 / . 0 840"],"associatedMergedColumns":["Model used for one - shot detection ( Table 2 ) :"]},{"number":"3?10","isBolded":false,"associatedRows":["R26+B / 32","8B","16k"],"associatedColumns":["rate","size","Learning","Image - level pre - training",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768","? 5",". 1 / . 0 840","? 4","? 4","768"],"associatedMergedColumns":[":"]},{"number":"0.0","isBolded":false,"associatedRows":["B / 32","16B 16k","224 MAP 140k 256","256"],"associatedColumns":["rate","size","Droplayer","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"0","isBolded":false,"associatedRows":["R26+B / 32","8B","16k","288 MAP","70k","256"],"associatedColumns":["decay","size","Weight","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768","? 5",". 1 / . 0 840","? 4","? 4","768"],"associatedMergedColumns":[":"]},{"number":"2?10","isBolded":false,"associatedRows":["R26+B / 32","8B","16k","288 MAP","70k","256"],"associatedColumns":["rate","size","Learning","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768","? 5",". 1 / . 0 840","? 4","? 4","768"],"associatedMergedColumns":[":"]},{"number":"1?10","isBolded":false,"associatedRows":["H / 14","12B 16k"],"associatedColumns":["decay","type","Weight","Image - level pre - training",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 5","768","? 5","768","? 5","768","? 5","768"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"0","isBolded":false,"associatedRows":["*","*","16k","*","*","*","MAP 140k 256","256","*"],"associatedColumns":["decay","size","Weight","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768","? 5",". 1 / . 0 840","? 4","? 4","768","? 4","768"],"associatedMergedColumns":[":"]},{"number":"0.0","isBolded":false,"associatedRows":["R50+H / 32","*","12k","224","GAP","28k","256"],"associatedColumns":["rate","size","Droplayer","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768","? 5",". 1 / . 0 840","? 4","? 4","768","? 4","768","768"],"associatedMergedColumns":[":"]},{"number":"2?10","isBolded":false,"associatedRows":["B / 32","2B","16k","224 MAP","70k","256"],"associatedColumns":["rate","size","Learning","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768","? 5",". 1 / . 0 840","? 4"],"associatedMergedColumns":[":"]},{"number":"0","isBolded":false,"associatedRows":["L / 16","16B 16k","224 MAP","70k","256"],"associatedColumns":["decay","size","Weight","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"7?10","isBolded":false,"associatedRows":["R50+H / 32 24B 12k","16k"],"associatedColumns":["rate","size","Learning","Image - level pre - training",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768","? 5",". 1 / . 0 840"],"associatedMergedColumns":["Model used for one - shot detection ( Table 2 ) :"]},{"number":"0","isBolded":false,"associatedRows":["Models used in the scaling study ( Figures","224 MAP 140k 256","70k","256"],"associatedColumns":["decay","size","Weight","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768"],"associatedMergedColumns":["? 5"]},{"number":"0","isBolded":false,"associatedRows":["Models used in the scaling study ( Figures","224 MAP 140k 256","140k 256"],"associatedColumns":["decay","size","Weight","Detection fine - tuning"],"associatedMergedColumns":["CLIP - based OWL - ViT models from Table 1 :"]},{"number":"1?10","isBolded":false,"associatedRows":["R26+B / 32","16B 16k"],"associatedColumns":["decay","type","Weight","Image - level pre - training",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 5","768","? 5","768"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"1?10","isBolded":false,"associatedRows":["R50+H / 32 24B 12k","16k"],"associatedColumns":["decay","type","Weight","Image - level pre - training",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 5","768","? 5","768","? 5","768","? 5","768","? 5",". 1 / . 0 840"],"associatedMergedColumns":["Model used for one - shot detection ( Table 2 ) :"]},{"number":"2?10","isBolded":false,"associatedRows":["R50+H / 32","*","12k","224","GAP","28k","256"],"associatedColumns":["rate","type","Learning","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768","? 5",". 1 / . 0 840","? 4","? 4","768","? 4","768","768"],"associatedMergedColumns":[":"]},{"number":"2?10","isBolded":false,"associatedRows":["R26+B / 32","16B 16k","288 MAP 140k 256","256"],"associatedColumns":["rate","size","Learning","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"0.0","isBolded":false,"associatedRows":["R26+B / 32","16B 16k","288 MAP 140k 256","256"],"associatedColumns":["rate","size","Droplayer","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"3and4):","isBolded":true,"associatedRows":["Models used in the scaling study ( Figures"],"associatedColumns":["decay","size","Image","Image - level pre - training",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 5","768","? 5","768","? 5","768","? 5","768","? 5",". 1 / . 0 840","? 5","? 5","768","? 5","768"],"associatedMergedColumns":[":"]},{"number":"3?10?4","isBolded":false,"associatedRows":["H / 14","12B 16k"],"associatedColumns":["rate","size","Learning","Image - level pre - training",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"2?10","isBolded":false,"associatedRows":["Models used in the scaling study ( Figures","224","GAP","28k","256"],"associatedColumns":["rate","size","Learning","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768","? 5",". 1 / . 0 840"],"associatedMergedColumns":["Model used for one - shot detection ( Table 2 ) :"]},{"number":"1?10","isBolded":false,"associatedRows":["R26+B / 32","8B","16k"],"associatedColumns":["decay","type","Weight","Image - level pre - training",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 5","768","? 5","768","? 5","768","? 5","768","? 5",". 1 / . 0 840","? 5","? 5","768"],"associatedMergedColumns":[":"]},{"number":"7?10","isBolded":false,"associatedRows":["R50+H / 32","*","12k"],"associatedColumns":["rate","size","Learning","Image - level pre - training",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768","? 5",". 1 / . 0 840","? 4","? 4","768","? 4","768","768"],"associatedMergedColumns":[":"]},{"number":"0.0","isBolded":false,"associatedRows":["*","*","16k","*","*","*","MAP 140k 256","256","*"],"associatedColumns":["rate","size","Droplayer","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768","? 5",". 1 / . 0 840","? 4","? 4","768","? 4","768"],"associatedMergedColumns":[":"]},{"number":"2?10","isBolded":false,"associatedRows":["Models used in the scaling study ( Figures","224 MAP 140k 256","70k","256"],"associatedColumns":["rate","size","Learning","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768"],"associatedMergedColumns":["? 5"]},{"number":"5?10","isBolded":false,"associatedRows":["H / 14","12B 16k","224 MAP","70k","256"],"associatedColumns":["rate","size","Learning","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768"],"associatedMergedColumns":["LiT - based OWL - ViT models from Table 1 :"]},{"number":"0","isBolded":false,"associatedRows":["R50+H / 32","*","12k","224","GAP","28k","256"],"associatedColumns":["decay","size","Weight","Detection fine - tuning",". 2 / . 1 768",". 2 / . 1 768",". 2 / . 1 840","? 4","768","? 4","768","? 4","768","? 5","768","? 5",". 1 / . 0 840","? 4","? 4","768","? 4","768","768"],"associatedMergedColumns":[":"]},{"number":"5?10","isBolded":false,"associatedRows":["B / 32","16B 16k","224 MAP 140k 256","140k 256"],"associatedColumns":["rate","size","Learning","Detection fine - tuning"],"associatedMergedColumns":["CLIP - based OWL - ViT models from Table 1 :"]},{"number":"5?10","isBolded":false,"associatedRows":["B / 16","16B 16k","224 MAP 140k 256","140k 256"],"associatedColumns":["rate","size","Learning","Detection fine - tuning",". 2 / . 1 768"],"associatedMergedColumns":["? 5"]}]}]