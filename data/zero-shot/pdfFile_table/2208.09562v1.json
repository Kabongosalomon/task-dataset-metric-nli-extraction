[{"caption":"dataset. All models are optimized by the Adam op-\ntimizer (Kingma and Ba 2014). The learning rate is set as \n3 ? 10 ?4 for all models trained on 224 ? 224 and 336 ? 336 \n\nModel \nBackbone \nInput \nmAP(%) \nResolution \nML-GCN \nResNet101 \n448x448 \n83.0 \nKSSNET \nResNet101 \n448x448 \n83.7 \nSSGRL \nResNet101 \n576x576 \n83.8 \nMS-CMA \nResNet101 \n448x448 \n83.8 \nASL \nTResNet-L \n448x448 \n88.4 \nQ2L \nTResNet-L \n448x448 \n89.2 \nML-Decoder TResNet-M \n224x224 \n84.2 \nML-Decoder TResNet-L \n448x448 \n90.1 \nML-Decoder TResNet-XL \n640x640 \n91.4 \nOurs \nViT-L \n224x224 \n89.82 \nOurs \nViT-L-336 \n336x336 \n91.76 \nOurs \nViT-L-336 \n640x640 \n93.41 \nOurs \nViT-L-336 \n1344x1344 \n93.54 \n\nTable 1: Comparison on conventional multi-label classifi-\ncation for MS-COCO dataset. Our ADDS approach shows \nsignificant improvement over previous methods, including \nML-GCN (Chen et al. 2019b), KSSNET (Liu et al. 2018), \nSSGRL (Chen et al. 2019a), MS-CMA (You et al. 2020), \nASL (Ben-Baruch et al. 2020), Q2L (Liu et al. 2021), and \nML-Decoder (Ridnik et al. 2021b). \n\n","rows":["KSSNET","336x336","TResNet - M","Ours","TResNet - L","640x640","576x576","ML - GCN","ML - Decoder","TResNet - XL","ViT - L","Q2L","224x224","MS - CMA","448x448","ResNet101","ViT - L - 336","SSGRL","ASL","1344x1344"],"columns":["Input","mAP ( % )"],"mergedAllColumns":["Resolution"],"numberCells":[{"number":"90.1","isBolded":false,"associatedRows":["ML - Decoder","TResNet - L","448x448"],"associatedColumns":["Input","mAP ( % )"],"associatedMergedColumns":["Resolution"]},{"number":"83.8","isBolded":false,"associatedRows":["SSGRL","ResNet101","576x576"],"associatedColumns":["Input","mAP ( % )"],"associatedMergedColumns":["Resolution"]},{"number":"89.2","isBolded":false,"associatedRows":["Q2L","TResNet - L","448x448"],"associatedColumns":["Input","mAP ( % )"],"associatedMergedColumns":["Resolution"]},{"number":"83.8","isBolded":false,"associatedRows":["MS - CMA","ResNet101","448x448"],"associatedColumns":["Input","mAP ( % )"],"associatedMergedColumns":["Resolution"]},{"number":"83.7","isBolded":false,"associatedRows":["KSSNET","ResNet101","448x448"],"associatedColumns":["Input","mAP ( % )"],"associatedMergedColumns":["Resolution"]},{"number":"93.54","isBolded":true,"associatedRows":["Ours","ViT - L - 336","1344x1344"],"associatedColumns":["Input","mAP ( % )"],"associatedMergedColumns":["Resolution"]},{"number":"91.4","isBolded":false,"associatedRows":["ML - Decoder","TResNet - XL","640x640"],"associatedColumns":["Input","mAP ( % )"],"associatedMergedColumns":["Resolution"]},{"number":"88.4","isBolded":false,"associatedRows":["ASL","TResNet - L","448x448"],"associatedColumns":["Input","mAP ( % )"],"associatedMergedColumns":["Resolution"]},{"number":"83.0","isBolded":false,"associatedRows":["ML - GCN","ResNet101","448x448"],"associatedColumns":["Input","mAP ( % )"],"associatedMergedColumns":["Resolution"]},{"number":"84.2","isBolded":false,"associatedRows":["ML - Decoder","TResNet - M","224x224"],"associatedColumns":["Input","mAP ( % )"],"associatedMergedColumns":["Resolution"]},{"number":"89.82","isBolded":true,"associatedRows":["Ours","ViT - L","224x224"],"associatedColumns":["Input","mAP ( % )"],"associatedMergedColumns":["Resolution"]},{"number":"93.41","isBolded":true,"associatedRows":["Ours","ViT - L - 336","640x640"],"associatedColumns":["Input","mAP ( % )"],"associatedMergedColumns":["Resolution"]},{"number":"91.76","isBolded":true,"associatedRows":["Ours","ViT - L - 336","336x336"],"associatedColumns":["Input","mAP ( % )"],"associatedMergedColumns":["Resolution"]}]},{"caption":"Table 2. First, we use image-\nencoder ViT-B-32 and its corresponding text encoder with \nCLIP pre-trained weights. Using 224?224 input images, we \nalready achieve a 36.56% mAP, 5.46 points higher than the \nprevious SOTA method ML-Decoder. Then we use image-\nencoder ViT-L-336 and its corresponding text encoder with \nCLIP pre-trained weights on 336 ? 336 images. The re-\nsult further improves to a 39.01% mAP, 7.9 points higher \nthan ML-Decoder (which even uses higher resolution im-\nages than both of our settings). This clearly shows that our \napproach provides significantly better results than pre-\nvious methods on zero-shot multi-label classification. \n\n","rows":["CONSE","Fast0Tag","LESA","ML - Decoder","( TresNet -","LabelEM","Generative ML - ZSL","SDL","One Attention per Label","One Attention per Clus -","BiAM"],"columns":["mAP ( % )","F1 ( k\u003d3 )","F1 ( k\u003d5 )"],"mergedAllColumns":["ter ( M\u003d10 )","( ViT - B - 32 , 224x224 )","L , 448x448 )"],"numberCells":[{"number":"25.9","isBolded":false,"associatedRows":["SDL"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"30.5","isBolded":false,"associatedRows":["SDL"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"25.7","isBolded":false,"associatedRows":["Generative ML - ZSL"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"31.6","isBolded":false,"associatedRows":["LESA"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"26.3","isBolded":false,"associatedRows":["BiAM"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"20.2","isBolded":false,"associatedRows":["CONSE"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":[]},{"number":"25.8","isBolded":false,"associatedRows":["One Attention per Label"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":[]},{"number":"27.8","isBolded":false,"associatedRows":["SDL"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"23.6","isBolded":false,"associatedRows":["One Attention per Label"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":[]},{"number":"36.56","isBolded":true,"associatedRows":["Generative ML - ZSL"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":["L , 448x448 )"]},{"number":"9.4","isBolded":false,"associatedRows":["CONSE"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":[]},{"number":"26.4","isBolded":false,"associatedRows":["Fast0Tag"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":[]},{"number":"39.28","isBolded":true,"associatedRows":["Generative ML - ZSL"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":["( ViT - B - 32 , 224x224 )"]},{"number":"19.5","isBolded":false,"associatedRows":["LabelEM"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":[]},{"number":"19.4","isBolded":false,"associatedRows":["LESA"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"24.6","isBolded":false,"associatedRows":["One Attention per Clus -"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":[]},{"number":"30.7","isBolded":false,"associatedRows":["BiAM"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"36.65","isBolded":true,"associatedRows":["Generative ML - ZSL"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":["L , 448x448 )"]},{"number":"19.2","isBolded":false,"associatedRows":["LabelEM"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":[]},{"number":"22.9","isBolded":false,"associatedRows":["One Attention per Clus -"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":[]},{"number":"27.8","isBolded":false,"associatedRows":["Fast0Tag"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":[]},{"number":"32.8","isBolded":false,"associatedRows":["Generative ML - ZSL"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"29.3","isBolded":false,"associatedRows":["Generative ML - ZSL"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"34.1","isBolded":false,"associatedRows":["ML - Decoder","( TresNet -"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"34.22","isBolded":true,"associatedRows":["Generative ML - ZSL"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":["L , 448x448 )"]},{"number":"30.8","isBolded":false,"associatedRows":["ML - Decoder","( TresNet -"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"12.9","isBolded":false,"associatedRows":["One Attention per Clus -"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":[]},{"number":"33.1","isBolded":false,"associatedRows":["BiAM"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"7.1","isBolded":false,"associatedRows":["LabelEM"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":[]},{"number":"15.1","isBolded":false,"associatedRows":["Fast0Tag"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":[]},{"number":"31.1","isBolded":false,"associatedRows":["ML - Decoder","( TresNet -"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"28.7","isBolded":false,"associatedRows":["LESA"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"39.01","isBolded":true,"associatedRows":["Generative ML - ZSL"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":["( ViT - B - 32 , 224x224 )"]},{"number":"10.4","isBolded":false,"associatedRows":["One Attention per Label"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":[]},{"number":"36.96","isBolded":true,"associatedRows":["Generative ML - ZSL"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":["( ViT - B - 32 , 224x224 )"]},{"number":"21.6","isBolded":false,"associatedRows":["CONSE"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":[]}]},{"caption":"Table 2: Comparison on zero-shot multi-label classification \nmethods for NUS-WIDE dataset. Our method provides sig-\nnificantly better results than previous methods, including \nCONSE ","rows":["CONSE","Fast0Tag","LESA","ML - Decoder","( TresNet -","LabelEM","Generative ML - ZSL","SDL","One Attention per Label","One Attention per Clus -","BiAM"],"columns":["mAP ( % )","F1 ( k\u003d3 )","F1 ( k\u003d5 )"],"mergedAllColumns":["ter ( M\u003d10 )","( ViT - B - 32 , 224x224 )","L , 448x448 )"],"numberCells":[{"number":"30.7","isBolded":false,"associatedRows":["BiAM"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"39.01","isBolded":true,"associatedRows":["Generative ML - ZSL"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":["( ViT - B - 32 , 224x224 )"]},{"number":"28.7","isBolded":false,"associatedRows":["LESA"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"36.56","isBolded":true,"associatedRows":["Generative ML - ZSL"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":["L , 448x448 )"]},{"number":"26.4","isBolded":false,"associatedRows":["Fast0Tag"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":[]},{"number":"10.4","isBolded":false,"associatedRows":["One Attention per Label"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":[]},{"number":"25.8","isBolded":false,"associatedRows":["One Attention per Label"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":[]},{"number":"19.2","isBolded":false,"associatedRows":["LabelEM"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":[]},{"number":"19.4","isBolded":false,"associatedRows":["LESA"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"39.28","isBolded":true,"associatedRows":["Generative ML - ZSL"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":["( ViT - B - 32 , 224x224 )"]},{"number":"15.1","isBolded":false,"associatedRows":["Fast0Tag"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":[]},{"number":"27.8","isBolded":false,"associatedRows":["Fast0Tag"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":[]},{"number":"29.3","isBolded":false,"associatedRows":["Generative ML - ZSL"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"9.4","isBolded":false,"associatedRows":["CONSE"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":[]},{"number":"24.6","isBolded":false,"associatedRows":["One Attention per Clus -"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":[]},{"number":"31.6","isBolded":false,"associatedRows":["LESA"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"30.5","isBolded":false,"associatedRows":["SDL"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"36.96","isBolded":true,"associatedRows":["Generative ML - ZSL"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":["( ViT - B - 32 , 224x224 )"]},{"number":"25.9","isBolded":false,"associatedRows":["SDL"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"12.9","isBolded":false,"associatedRows":["One Attention per Clus -"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":[]},{"number":"25.7","isBolded":false,"associatedRows":["Generative ML - ZSL"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"30.8","isBolded":false,"associatedRows":["ML - Decoder","( TresNet -"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"19.5","isBolded":false,"associatedRows":["LabelEM"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":[]},{"number":"34.1","isBolded":false,"associatedRows":["ML - Decoder","( TresNet -"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"7.1","isBolded":false,"associatedRows":["LabelEM"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":[]},{"number":"20.2","isBolded":false,"associatedRows":["CONSE"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":[]},{"number":"26.3","isBolded":false,"associatedRows":["BiAM"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"36.65","isBolded":true,"associatedRows":["Generative ML - ZSL"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":["L , 448x448 )"]},{"number":"22.9","isBolded":false,"associatedRows":["One Attention per Clus -"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":[]},{"number":"33.1","isBolded":false,"associatedRows":["BiAM"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"21.6","isBolded":false,"associatedRows":["CONSE"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":[]},{"number":"23.6","isBolded":false,"associatedRows":["One Attention per Label"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":[]},{"number":"32.8","isBolded":false,"associatedRows":["Generative ML - ZSL"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"31.1","isBolded":false,"associatedRows":["ML - Decoder","( TresNet -"],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":["ter ( M\u003d10 )"]},{"number":"34.22","isBolded":true,"associatedRows":["Generative ML - ZSL"],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":["L , 448x448 )"]},{"number":"27.8","isBolded":false,"associatedRows":["SDL"],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":["ter ( M\u003d10 )"]}]},{"caption":"Table 3: Comparison on single-to-multi label classification. \nModels are trained on ImageNet-1k and tested on MS-\nCOCO. Our method greatly outperforms ML-Decoder. \nWith overlapped classes \nModel \nBackbone mAP(%) F1(k\u003d3) F1(k\u003d5) \nML-Decoder TResNet-L 14.15 \n7.07 \n7.30 \nOurs \nViT-B-32 \n27.34 \n20.39 \n20.39 \nOurs \nViT-L-336 \n31.07 \n24.69 \n24.69 \nWithout overlapped classes \nModel \nBackbone mAP(%) F1(k\u003d3) F1(k\u003d5) \nML-Decoder TResNet-L 13.19 \n6.26 \n6.27 \nOurs \nViT-B-32 \n26.96 \n16.07 \n16.07 \nOurs \nViT-L-336 \n30.66 \n19.18 \n19.18 \n\n","rows":["Ours","TResNet - L","ML - Decoder","ViT - B - 32","ViT - L - 336"],"columns":["mAP ( % )","F1 ( k\u003d3 )","F1 ( k\u003d5 )","With overlapped classes"],"mergedAllColumns":["Without overlapped classes"],"numberCells":[{"number":"50.86","isBolded":false,"associatedRows":["Ours","ViT - L - 336"],"associatedColumns":["With overlapped classes","F1 ( k\u003d5 )"],"associatedMergedColumns":[]},{"number":"6.90","isBolded":true,"associatedRows":["ML - Decoder","TResNet - L"],"associatedColumns":["With overlapped classes","F1 ( k\u003d3 )","F1 ( k\u003d3 )"],"associatedMergedColumns":["Without overlapped classes"]},{"number":"6.90","isBolded":true,"associatedRows":["ML - Decoder","TResNet - L"],"associatedColumns":["With overlapped classes","F1 ( k\u003d5 )","F1 ( k\u003d5 )"],"associatedMergedColumns":["Without overlapped classes"]},{"number":"67.10","isBolded":false,"associatedRows":["Ours","ViT - L - 336"],"associatedColumns":["With overlapped classes","mAP ( % )"],"associatedMergedColumns":[]},{"number":"33.16","isBolded":false,"associatedRows":["Ours","ViT - L - 336"],"associatedColumns":["With overlapped classes","F1 ( k\u003d3 )","F1 ( k\u003d3 )"],"associatedMergedColumns":["Without overlapped classes"]},{"number":"33.16","isBolded":false,"associatedRows":["Ours","ViT - L - 336"],"associatedColumns":["With overlapped classes","F1 ( k\u003d5 )","F1 ( k\u003d5 )"],"associatedMergedColumns":["Without overlapped classes"]},{"number":"30.90","isBolded":false,"associatedRows":["Ours","ViT - B - 32"],"associatedColumns":["With overlapped classes","F1 ( k\u003d5 )","F1 ( k\u003d5 )"],"associatedMergedColumns":["Without overlapped classes"]},{"number":"17.80","isBolded":true,"associatedRows":["ML - Decoder","TResNet - L"],"associatedColumns":["With overlapped classes","F1 ( k\u003d3 )"],"associatedMergedColumns":[]},{"number":"50.86","isBolded":false,"associatedRows":["Ours","ViT - L - 336"],"associatedColumns":["With overlapped classes","F1 ( k\u003d3 )"],"associatedMergedColumns":[]},{"number":"17.80","isBolded":true,"associatedRows":["ML - Decoder","TResNet - L"],"associatedColumns":["With overlapped classes","F1 ( k\u003d5 )"],"associatedMergedColumns":[]},{"number":"45.88","isBolded":false,"associatedRows":["Ours","ViT - B - 32"],"associatedColumns":["With overlapped classes","F1 ( k\u003d5 )"],"associatedMergedColumns":[]},{"number":"38.37","isBolded":true,"associatedRows":["ML - Decoder","TResNet - L"],"associatedColumns":["With overlapped classes","mAP ( % )","mAP ( % )"],"associatedMergedColumns":["Without overlapped classes"]},{"number":"45.88","isBolded":false,"associatedRows":["Ours","ViT - B - 32"],"associatedColumns":["With overlapped classes","F1 ( k\u003d3 )"],"associatedMergedColumns":[]},{"number":"64.32","isBolded":false,"associatedRows":["Ours","ViT - B - 32"],"associatedColumns":["With overlapped classes","mAP ( % )","mAP ( % )"],"associatedMergedColumns":["Without overlapped classes"]},{"number":"41.60","isBolded":true,"associatedRows":["ML - Decoder","TResNet - L"],"associatedColumns":["With overlapped classes","mAP ( % )"],"associatedMergedColumns":[]},{"number":"59.27","isBolded":false,"associatedRows":["Ours","ViT - B - 32"],"associatedColumns":["With overlapped classes","mAP ( % )"],"associatedMergedColumns":[]},{"number":"69.60","isBolded":false,"associatedRows":["Ours","ViT - L - 336"],"associatedColumns":["With overlapped classes","mAP ( % )","mAP ( % )"],"associatedMergedColumns":["Without overlapped classes"]},{"number":"30.90","isBolded":false,"associatedRows":["Ours","ViT - B - 32"],"associatedColumns":["With overlapped classes","F1 ( k\u003d3 )","F1 ( k\u003d3 )"],"associatedMergedColumns":["Without overlapped classes"]}]},{"caption":"Table 4: Comparisons of single-to-multi label classification \ntask which is trained on ImageNet-1k and tested on NUS-\nWIDE dataset. Our method shows the SOTA performance. \n\n","rows":["Ours","TResNet - L","ML - Decoder","ViT - B - 32","ViT - L - 336"],"columns":["mAP ( % )","ImageNet - 1k","F1 ( k\u003d3 )","and","F1 ( k\u003d5 )","Table 3 : Comparison on single - to - multi label classification .","on"],"mergedAllColumns":["Without overlapped classes","With overlapped classes"],"numberCells":[{"number":"31.07","isBolded":false,"associatedRows":["Ours","ViT - L - 336"],"associatedColumns":["Table 3 : Comparison on single - to - multi label classification .","ImageNet - 1k","mAP ( % )"],"associatedMergedColumns":["With overlapped classes"]},{"number":"24.69","isBolded":false,"associatedRows":["Ours","ViT - L - 336"],"associatedColumns":["Table 3 : Comparison on single - to - multi label classification .","on","F1 ( k\u003d5 )"],"associatedMergedColumns":["With overlapped classes"]},{"number":"30.66","isBolded":false,"associatedRows":["Ours","ViT - L - 336"],"associatedColumns":["Table 3 : Comparison on single - to - multi label classification .","ImageNet - 1k","mAP ( % )","mAP ( % )"],"associatedMergedColumns":["Without overlapped classes"]},{"number":"19.18","isBolded":false,"associatedRows":["Ours","ViT - L - 336"],"associatedColumns":["Table 3 : Comparison on single - to - multi label classification .","on","F1 ( k\u003d5 )","F1 ( k\u003d5 )"],"associatedMergedColumns":["Without overlapped classes"]},{"number":"13.19","isBolded":true,"associatedRows":["ML - Decoder","TResNet - L"],"associatedColumns":["Table 3 : Comparison on single - to - multi label classification .","ImageNet - 1k","mAP ( % )","mAP ( % )"],"associatedMergedColumns":["Without overlapped classes"]},{"number":"20.39","isBolded":false,"associatedRows":["Ours","ViT - B - 32"],"associatedColumns":["Table 3 : Comparison on single - to - multi label classification .","on","F1 ( k\u003d5 )"],"associatedMergedColumns":["With overlapped classes"]},{"number":"7.07","isBolded":true,"associatedRows":["ML - Decoder","TResNet - L"],"associatedColumns":["Table 3 : Comparison on single - to - multi label classification .","and","F1 ( k\u003d3 )"],"associatedMergedColumns":["With overlapped classes"]},{"number":"7.30","isBolded":true,"associatedRows":["ML - Decoder","TResNet - L"],"associatedColumns":["Table 3 : Comparison on single - to - multi label classification .","on","F1 ( k\u003d5 )"],"associatedMergedColumns":["With overlapped classes"]},{"number":"20.39","isBolded":false,"associatedRows":["Ours","ViT - B - 32"],"associatedColumns":["Table 3 : Comparison on single - to - multi label classification .","and","F1 ( k\u003d3 )"],"associatedMergedColumns":["With overlapped classes"]},{"number":"6.26","isBolded":true,"associatedRows":["ML - Decoder","TResNet - L"],"associatedColumns":["Table 3 : Comparison on single - to - multi label classification .","and","F1 ( k\u003d3 )","F1 ( k\u003d3 )"],"associatedMergedColumns":["Without overlapped classes"]},{"number":"6.27","isBolded":true,"associatedRows":["ML - Decoder","TResNet - L"],"associatedColumns":["Table 3 : Comparison on single - to - multi label classification .","on","F1 ( k\u003d5 )","F1 ( k\u003d5 )"],"associatedMergedColumns":["Without overlapped classes"]},{"number":"24.69","isBolded":false,"associatedRows":["Ours","ViT - L - 336"],"associatedColumns":["Table 3 : Comparison on single - to - multi label classification .","and","F1 ( k\u003d3 )"],"associatedMergedColumns":["With overlapped classes"]},{"number":"16.07","isBolded":false,"associatedRows":["Ours","ViT - B - 32"],"associatedColumns":["Table 3 : Comparison on single - to - multi label classification .","on","F1 ( k\u003d5 )","F1 ( k\u003d5 )"],"associatedMergedColumns":["Without overlapped classes"]},{"number":"16.07","isBolded":false,"associatedRows":["Ours","ViT - B - 32"],"associatedColumns":["Table 3 : Comparison on single - to - multi label classification .","and","F1 ( k\u003d3 )","F1 ( k\u003d3 )"],"associatedMergedColumns":["Without overlapped classes"]},{"number":"14.15","isBolded":true,"associatedRows":["ML - Decoder","TResNet - L"],"associatedColumns":["Table 3 : Comparison on single - to - multi label classification .","ImageNet - 1k","mAP ( % )"],"associatedMergedColumns":["With overlapped classes"]},{"number":"26.96","isBolded":false,"associatedRows":["Ours","ViT - B - 32"],"associatedColumns":["Table 3 : Comparison on single - to - multi label classification .","ImageNet - 1k","mAP ( % )","mAP ( % )"],"associatedMergedColumns":["Without overlapped classes"]},{"number":"27.34","isBolded":false,"associatedRows":["Ours","ViT - B - 32"],"associatedColumns":["Table 3 : Comparison on single - to - multi label classification .","ImageNet - 1k","mAP ( % )"],"associatedMergedColumns":["With overlapped classes"]},{"number":"19.18","isBolded":false,"associatedRows":["Ours","ViT - L - 336"],"associatedColumns":["Table 3 : Comparison on single - to - multi label classification .","and","F1 ( k\u003d3 )","F1 ( k\u003d3 )"],"associatedMergedColumns":["Without overlapped classes"]}]},{"caption":"Table 5: Comparison between DM-Decoder and ML-\nDecoder on NUS-WIDE dataset for zero-shot multi-label \nclassification. \n\n","rows":[],"columns":["mAP ( % )","F1 ( k\u003d3 )","F1 ( k\u003d5 )"],"mergedAllColumns":["( ViT - L - 336 , 336x336 )"],"numberCells":[{"number":"36.35","isBolded":false,"associatedRows":[],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":["( ViT - L - 336 , 336x336 )"]},{"number":"34.46","isBolded":true,"associatedRows":[],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":["( ViT - L - 336 , 336x336 )"]},{"number":"38.68","isBolded":true,"associatedRows":[],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":["( ViT - L - 336 , 336x336 )"]},{"number":"39.28","isBolded":true,"associatedRows":[],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":["( ViT - L - 336 , 336x336 )"]},{"number":"36.96","isBolded":true,"associatedRows":[],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":["( ViT - L - 336 , 336x336 )"]},{"number":"36.15","isBolded":false,"associatedRows":[],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":[]},{"number":"33.56","isBolded":false,"associatedRows":[],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":["( ViT - L - 336 , 336x336 )"]},{"number":"31.33","isBolded":false,"associatedRows":[],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":["( ViT - L - 336 , 336x336 )"]},{"number":"32.45","isBolded":false,"associatedRows":[],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":[]},{"number":"37.50","isBolded":true,"associatedRows":[],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":["( ViT - L - 336 , 336x336 )"]},{"number":"29.83","isBolded":false,"associatedRows":[],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":["( ViT - L - 336 , 336x336 )"]},{"number":"35.50","isBolded":true,"associatedRows":[],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":[]},{"number":"36.88","isBolded":true,"associatedRows":[],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":["( ViT - L - 336 , 336x336 )"]},{"number":"35.48","isBolded":false,"associatedRows":[],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":["( ViT - L - 336 , 336x336 )"]},{"number":"39.01","isBolded":true,"associatedRows":[],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":["( ViT - L - 336 , 336x336 )"]},{"number":"36.34","isBolded":false,"associatedRows":[],"associatedColumns":["mAP ( % )"],"associatedMergedColumns":["( ViT - L - 336 , 336x336 )"]},{"number":"32.95","isBolded":true,"associatedRows":[],"associatedColumns":["F1 ( k\u003d3 )"],"associatedMergedColumns":["( ViT - L - 336 , 336x336 )"]},{"number":"34.89","isBolded":false,"associatedRows":[],"associatedColumns":["F1 ( k\u003d5 )"],"associatedMergedColumns":["( ViT - L - 336 , 336x336 )"]}]},{"caption":"Table 6: Comparison of different components for validating \nthe effectiveness of textual-visual alignment. \n\n","rows":["RandMx","ViT - L","CLIP text","TresNet - L","224x224","ConvNeXt - XL"],"columns":["Image"],"mergedAllColumns":["Resolution"],"numberCells":[{"number":"85.78","isBolded":false,"associatedRows":["TresNet - L","RandMx","224x224"],"associatedColumns":["Image"],"associatedMergedColumns":["Resolution"]},{"number":"87.58","isBolded":false,"associatedRows":["ConvNeXt - XL","CLIP text","224x224"],"associatedColumns":["Image"],"associatedMergedColumns":["Resolution"]},{"number":"85.76","isBolded":false,"associatedRows":["TresNet - L","CLIP text","224x224"],"associatedColumns":["Image"],"associatedMergedColumns":["Resolution"]},{"number":"89.42","isBolded":false,"associatedRows":["ViT - L","CLIP text","224x224"],"associatedColumns":["Image"],"associatedMergedColumns":["Resolution"]},{"number":"87.57","isBolded":false,"associatedRows":["ViT - L","RandMx","224x224"],"associatedColumns":["Image"],"associatedMergedColumns":["Resolution"]}]},{"caption":"Table 7: Full vs. single-layer Pyramid-Forwarding. \n\n","rows":["336x336","Ours","[ 0 ]","[ 2 ]","[ 0 , 1 , 2 ]","1344x1344"],"columns":["mAP ( % )","Image"],"mergedAllColumns":["Resolution"],"numberCells":[{"number":"93.54","isBolded":false,"associatedRows":["Ours","[ 0 , 1 , 2 ]","1344x1344"],"associatedColumns":["Image","mAP ( % )"],"associatedMergedColumns":["Resolution"]},{"number":"91.79","isBolded":false,"associatedRows":["Ours","[ 2 ]","1344x1344"],"associatedColumns":["Image","mAP ( % )"],"associatedMergedColumns":["Resolution"]},{"number":"91.76","isBolded":false,"associatedRows":["Ours","[ 0 ]","336x336"],"associatedColumns":["Image","mAP ( % )"],"associatedMergedColumns":["Resolution"]}]},{"caption":"Table 8: Comparisons of our method training on ImageNet-\n1k/ImageNet-21k dataset (filter the overlapped classes with \nNUS-WIDE) and test on NUS-WIDE dataset, with all \n336x336 resolution and ViT-L-336 backbone. \n\n","rows":["each class , so that the selected dataset contains","ImageNet - 1k","ImageNet - 21k","ViT - L - 336","times than before , the model mAP increases"],"columns":["dataset . For a fair comparison , we filter out the overlapped","mAP ( % )","F1 ( k\u003d3 )","F1 ( k\u003d5 )"],"mergedAllColumns":["lapped classes with NUS - WIDE , and select 100 images for","With the number of available training classes becomes 15"],"numberCells":[{"number":"1.3Mimages","isBolded":false,"associatedRows":["each class , so that the selected dataset contains"],"associatedColumns":["dataset . For a fair comparison , we filter out the overlapped"],"associatedMergedColumns":["lapped classes with NUS - WIDE , and select 100 images for"]},{"number":"31.02","isBolded":false,"associatedRows":["ImageNet - 1k","ViT - L - 336"],"associatedColumns":["dataset . For a fair comparison , we filter out the overlapped","mAP ( % )"],"associatedMergedColumns":["With the number of available training classes becomes 15"]},{"number":"39.82","isBolded":true,"associatedRows":["ImageNet - 21k","ViT - L - 336"],"associatedColumns":["dataset . For a fair comparison , we filter out the overlapped","F1 ( k\u003d3 )"],"associatedMergedColumns":["With the number of available training classes becomes 15"]},{"number":"24.98","isBolded":false,"associatedRows":["ImageNet - 1k","ViT - L - 336"],"associatedColumns":["dataset . For a fair comparison , we filter out the overlapped","F1 ( k\u003d5 )"],"associatedMergedColumns":["With the number of available training classes becomes 15"]},{"number":"37.92","isBolded":true,"associatedRows":["ImageNet - 21k","ViT - L - 336"],"associatedColumns":["dataset . For a fair comparison , we filter out the overlapped","mAP ( % )"],"associatedMergedColumns":["With the number of available training classes becomes 15"]},{"number":"40.39","isBolded":true,"associatedRows":["ImageNet - 21k","ViT - L - 336"],"associatedColumns":["dataset . For a fair comparison , we filter out the overlapped","F1 ( k\u003d5 )"],"associatedMergedColumns":["With the number of available training classes becomes 15"]},{"number":"6.9points.","isBolded":false,"associatedRows":["times than before , the model mAP increases"],"associatedColumns":["dataset . For a fair comparison , we filter out the overlapped"],"associatedMergedColumns":["With the number of available training classes becomes 15"]},{"number":"24.98","isBolded":false,"associatedRows":["ImageNet - 1k","ViT - L - 336"],"associatedColumns":["dataset . For a fair comparison , we filter out the overlapped","F1 ( k\u003d3 )"],"associatedMergedColumns":["With the number of available training classes becomes 15"]}]},{"caption":"Table 9: Applying other VLP models for alignment to our \nmethod on NUS-WIDE zero-shot classification task. \n\n","rows":["SLIP","384x384","ViT - L","ViT - L ( COCO )","224x224","BLIP"],"columns":["Table 9 , the","mAP ( % )","Image","Resolution"],"mergedAllColumns":["boost , instead of the image or text encoder itself ."],"numberCells":[{"number":"2.52","isBolded":false,"associatedRows":["BLIP","ViT - L ( COCO )","384x384"],"associatedColumns":["Table 9 , the","Image","mAP ( % )","Resolution"],"associatedMergedColumns":["boost , instead of the image or text encoder itself ."]},{"number":"35.15","isBolded":false,"associatedRows":["BLIP","ViT - L","224x224"],"associatedColumns":["Table 9 , the","Image","mAP ( % )","Resolution"],"associatedMergedColumns":["boost , instead of the image or text encoder itself ."]},{"number":"34.15","isBolded":false,"associatedRows":["SLIP","ViT - L","224x224"],"associatedColumns":["Table 9 , the","Image","mAP ( % )","Resolution"],"associatedMergedColumns":["boost , instead of the image or text encoder itself ."]}]}]