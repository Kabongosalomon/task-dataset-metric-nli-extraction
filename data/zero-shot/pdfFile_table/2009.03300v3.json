[{"caption":"Social Science \nSTEM \nOther \nAverage \nRandom Baseline \n25.0 \n25.0 \n25.0 \n25.0 \n25.0 \nRoBERTa \n27.9 \n28.8 \n27.0 \n27.7 \n27.9 \nALBERT \n27.2 \n25.7 \n27.7 \n27.9 \n27.1 \nGPT-2 \n32.8 \n33.3 \n30.2 \n33.1 \n32.4 \nUnifiedQA \n45.6 \n56.6 \n40.2 \n54.6 \n48.9 \nGPT-3 Small (few-shot) \n24.4 \n30.9 \n26.0 \n24.1 \n25.9 \nGPT-3 Medium (few-shot) \n26.1 \n21.6 \n25.6 \n25.5 \n24.9 \nGPT-3 Large (few-shot) \n27.1 \n25.6 \n24.3 \n26.5 \n26.0 \nGPT-3 X-Large (few-shot) \n40.8 \n50.4 \n36.7 \n48.8 \n43.9 \n\nTable 1: Average weighted accuracy for each model on all four broad disciplines. All values are \npercentages. Some models proposed in the past few months can move several percent points beyond \nrandom chance. GPT-3 uses few-shot learning and UnifiedQA is tested under distribution shift. \n","rows":["RoBERTa","GPT - 3 X - Large ( few - shot )","GPT - 3 Small ( few - shot )","GPT - 2","GPT - 3 Medium ( few - shot )","ALBERT","Random Baseline","GPT - 3 Large ( few - shot )","UnifiedQA"],"columns":["Average","Social Science","STEM","Other"],"mergedAllColumns":[],"numberCells":[{"number":"25.7","isBolded":false,"associatedRows":["ALBERT"],"associatedColumns":["Social Science"],"associatedMergedColumns":[]},{"number":"21.6","isBolded":false,"associatedRows":["GPT - 3 Medium ( few - shot )"],"associatedColumns":["Social Science"],"associatedMergedColumns":[]},{"number":"30.9","isBolded":false,"associatedRows":["GPT - 3 Small ( few - shot )"],"associatedColumns":["Social Science"],"associatedMergedColumns":[]},{"number":"27.9","isBolded":false,"associatedRows":["RoBERTa"],"associatedColumns":["Average"],"associatedMergedColumns":[]},{"number":"27.9","isBolded":false,"associatedRows":["ALBERT"],"associatedColumns":["Other"],"associatedMergedColumns":[]},{"number":"33.3","isBolded":false,"associatedRows":["GPT - 2"],"associatedColumns":["Social Science"],"associatedMergedColumns":[]},{"number":"27.2","isBolded":false,"associatedRows":["ALBERT"],"associatedColumns":["Social Science"],"associatedMergedColumns":[]},{"number":"24.3","isBolded":false,"associatedRows":["GPT - 3 Large ( few - shot )"],"associatedColumns":["STEM"],"associatedMergedColumns":[]},{"number":"27.7","isBolded":false,"associatedRows":["ALBERT"],"associatedColumns":["STEM"],"associatedMergedColumns":[]},{"number":"24.1","isBolded":false,"associatedRows":["GPT - 3 Small ( few - shot )"],"associatedColumns":["Other"],"associatedMergedColumns":[]},{"number":"25.6","isBolded":false,"associatedRows":["GPT - 3 Large ( few - shot )"],"associatedColumns":["Social Science"],"associatedMergedColumns":[]},{"number":"27.1","isBolded":false,"associatedRows":["GPT - 3 Large ( few - shot )"],"associatedColumns":["Social Science"],"associatedMergedColumns":[]},{"number":"50.4","isBolded":false,"associatedRows":["GPT - 3 X - Large ( few - shot )"],"associatedColumns":["Social Science"],"associatedMergedColumns":[]},{"number":"40.2","isBolded":false,"associatedRows":["UnifiedQA"],"associatedColumns":["STEM"],"associatedMergedColumns":[]},{"number":"25.0","isBolded":false,"associatedRows":["Random Baseline"],"associatedColumns":["Average"],"associatedMergedColumns":[]},{"number":"26.5","isBolded":false,"associatedRows":["GPT - 3 Large ( few - shot )"],"associatedColumns":["Other"],"associatedMergedColumns":[]},{"number":"25.6","isBolded":false,"associatedRows":["GPT - 3 Medium ( few - shot )"],"associatedColumns":["STEM"],"associatedMergedColumns":[]},{"number":"32.4","isBolded":false,"associatedRows":["GPT - 2"],"associatedColumns":["Average"],"associatedMergedColumns":[]},{"number":"48.8","isBolded":false,"associatedRows":["GPT - 3 X - Large ( few - shot )"],"associatedColumns":["Other"],"associatedMergedColumns":[]},{"number":"25.0","isBolded":false,"associatedRows":["Random Baseline"],"associatedColumns":["STEM"],"associatedMergedColumns":[]},{"number":"32.8","isBolded":false,"associatedRows":["GPT - 2"],"associatedColumns":["Social Science"],"associatedMergedColumns":[]},{"number":"26.0","isBolded":false,"associatedRows":["GPT - 3 Large ( few - shot )"],"associatedColumns":["Average"],"associatedMergedColumns":[]},{"number":"24.4","isBolded":false,"associatedRows":["GPT - 3 Small ( few - shot )"],"associatedColumns":["Social Science"],"associatedMergedColumns":[]},{"number":"24.9","isBolded":false,"associatedRows":["GPT - 3 Medium ( few - shot )"],"associatedColumns":["Average"],"associatedMergedColumns":[]},{"number":"25.9","isBolded":false,"associatedRows":["GPT - 3 Small ( few - shot )"],"associatedColumns":["Average"],"associatedMergedColumns":[]},{"number":"27.0","isBolded":false,"associatedRows":["RoBERTa"],"associatedColumns":["STEM"],"associatedMergedColumns":[]},{"number":"27.9","isBolded":false,"associatedRows":["RoBERTa"],"associatedColumns":["Social Science"],"associatedMergedColumns":[]},{"number":"27.1","isBolded":false,"associatedRows":["ALBERT"],"associatedColumns":["Average"],"associatedMergedColumns":[]},{"number":"25.5","isBolded":false,"associatedRows":["GPT - 3 Medium ( few - shot )"],"associatedColumns":["Other"],"associatedMergedColumns":[]},{"number":"30.2","isBolded":false,"associatedRows":["GPT - 2"],"associatedColumns":["STEM"],"associatedMergedColumns":[]},{"number":"45.6","isBolded":false,"associatedRows":["UnifiedQA"],"associatedColumns":["Social Science"],"associatedMergedColumns":[]},{"number":"36.7","isBolded":false,"associatedRows":["GPT - 3 X - Large ( few - shot )"],"associatedColumns":["STEM"],"associatedMergedColumns":[]},{"number":"48.9","isBolded":false,"associatedRows":["UnifiedQA"],"associatedColumns":["Average"],"associatedMergedColumns":[]},{"number":"56.6","isBolded":false,"associatedRows":["UnifiedQA"],"associatedColumns":["Social Science"],"associatedMergedColumns":[]},{"number":"40.8","isBolded":false,"associatedRows":["GPT - 3 X - Large ( few - shot )"],"associatedColumns":["Social Science"],"associatedMergedColumns":[]},{"number":"33.1","isBolded":false,"associatedRows":["GPT - 2"],"associatedColumns":["Other"],"associatedMergedColumns":[]},{"number":"26.0","isBolded":false,"associatedRows":["GPT - 3 Small ( few - shot )"],"associatedColumns":["STEM"],"associatedMergedColumns":[]},{"number":"26.1","isBolded":false,"associatedRows":["GPT - 3 Medium ( few - shot )"],"associatedColumns":["Social Science"],"associatedMergedColumns":[]},{"number":"25.0","isBolded":false,"associatedRows":["Random Baseline"],"associatedColumns":["Other"],"associatedMergedColumns":[]},{"number":"28.8","isBolded":false,"associatedRows":["RoBERTa"],"associatedColumns":["Social Science"],"associatedMergedColumns":[]},{"number":"54.6","isBolded":false,"associatedRows":["UnifiedQA"],"associatedColumns":["Other"],"associatedMergedColumns":[]},{"number":"25.0","isBolded":false,"associatedRows":["Random Baseline"],"associatedColumns":["Social Science"],"associatedMergedColumns":[]},{"number":"27.7","isBolded":false,"associatedRows":["RoBERTa"],"associatedColumns":["Other"],"associatedMergedColumns":[]},{"number":"25.0","isBolded":false,"associatedRows":["Random Baseline"],"associatedColumns":["Social Science"],"associatedMergedColumns":[]},{"number":"43.9","isBolded":false,"associatedRows":["GPT - 3 X - Large ( few - shot )"],"associatedColumns":["Average"],"associatedMergedColumns":[]}]},{"caption":"Table 2. We also provide an example for each task starting with ","rows":["0","Accuracy","1500","2500","Confidence","500","Label","1000","True","2000","3000","Subject"],"columns":["GPT - 3 Question Length and Correctness","GPT - 3 Average Question Length and"],"mergedAllColumns":["of"],"numberCells":[{"number":"0.6","isBolded":false,"associatedRows":["True"],"associatedColumns":["GPT - 3 Average Question Length and","GPT - 3 Question Length and Correctness"],"associatedMergedColumns":[]},{"number":"0.2","isBolded":false,"associatedRows":["Confidence"],"associatedColumns":["GPT - 3 Average Question Length and","GPT - 3 Question Length and Correctness"],"associatedMergedColumns":["of"]},{"number":"1.0","isBolded":false,"associatedRows":["Label","Accuracy"],"associatedColumns":["GPT - 3 Average Question Length and"],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["Confidence"],"associatedColumns":["GPT - 3 Average Question Length and","GPT - 3 Question Length and Correctness"],"associatedMergedColumns":["of"]},{"number":"0.4","isBolded":false,"associatedRows":["Confidence","Subject"],"associatedColumns":["GPT - 3 Average Question Length and","GPT - 3 Question Length and Correctness"],"associatedMergedColumns":["of"]},{"number":"0.8","isBolded":false,"associatedRows":["Label"],"associatedColumns":["GPT - 3 Average Question Length and","GPT - 3 Question Length and Correctness"],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["Confidence","0","500","1000","1500","2000","2500","3000","Subject"],"associatedColumns":["GPT - 3 Average Question Length and","GPT - 3 Question Length and Correctness"],"associatedMergedColumns":["of"]},{"number":"0.2","isBolded":false,"associatedRows":["Confidence","Subject"],"associatedColumns":["GPT - 3 Average Question Length and","GPT - 3 Question Length and Correctness"],"associatedMergedColumns":["of"]},{"number":"0.6","isBolded":false,"associatedRows":["True","Accuracy"],"associatedColumns":["GPT - 3 Average Question Length and","GPT - 3 Question Length and Correctness"],"associatedMergedColumns":[]},{"number":"0.8","isBolded":false,"associatedRows":["Label","Accuracy"],"associatedColumns":["GPT - 3 Average Question Length and","GPT - 3 Question Length and Correctness"],"associatedMergedColumns":[]},{"number":"0.4","isBolded":false,"associatedRows":["Confidence"],"associatedColumns":["GPT - 3 Average Question Length and","GPT - 3 Question Length and Correctness"],"associatedMergedColumns":["of"]},{"number":"1.0","isBolded":false,"associatedRows":["Label"],"associatedColumns":["GPT - 3 Average Question Length and"],"associatedMergedColumns":[]}]}]