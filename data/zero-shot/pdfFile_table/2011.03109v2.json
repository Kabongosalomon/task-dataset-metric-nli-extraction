[{"caption":"Table 1. The amounts of audio data in hours. \nLanguage \nTrain Valid \nTest \nclean noisy \nRomanian \n161 \n5.2 \n5.1 \n10.2 \nTurkish \n3.1K \n13.6 \n21.2 \n23.4 \nGerman \n3.2K \n13.8 \n24.5 \n24.0 \n\n3. EXPERIMENTS \n\n3.1. Experimental setup \n\n3.1.1. Data \n\n","rows":["Romanian","German","161","Turkish"],"columns":["3 .","The amounts of audio data in hours .","Language","Valid","Test","clean","noisy","Train","Table 1 ."],"mergedAllColumns":[],"numberCells":[{"number":"13.6","isBolded":false,"associatedRows":["Turkish"],"associatedColumns":["The amounts of audio data in hours .","Valid","clean"],"associatedMergedColumns":[]},{"number":"24.5","isBolded":false,"associatedRows":["German"],"associatedColumns":["The amounts of audio data in hours .","Test","clean"],"associatedMergedColumns":[]},{"number":"23.4","isBolded":false,"associatedRows":["Turkish"],"associatedColumns":["The amounts of audio data in hours .","Test","noisy"],"associatedMergedColumns":[]},{"number":"10.2","isBolded":false,"associatedRows":["Romanian","161"],"associatedColumns":["The amounts of audio data in hours .","Test","noisy"],"associatedMergedColumns":[]},{"number":"3.1.","isBolded":true,"associatedRows":[],"associatedColumns":["Table 1 .","Language","clean","3 ."],"associatedMergedColumns":[]},{"number":"5.1","isBolded":false,"associatedRows":["Romanian","161"],"associatedColumns":["The amounts of audio data in hours .","Test","clean"],"associatedMergedColumns":[]},{"number":"13.8","isBolded":false,"associatedRows":["German"],"associatedColumns":["The amounts of audio data in hours .","Valid","clean"],"associatedMergedColumns":[]},{"number":"21.2","isBolded":false,"associatedRows":["Turkish"],"associatedColumns":["The amounts of audio data in hours .","Test","clean"],"associatedMergedColumns":[]},{"number":"5.2","isBolded":false,"associatedRows":["Romanian","161"],"associatedColumns":["The amounts of audio data in hours .","Valid","clean"],"associatedMergedColumns":[]},{"number":"3.1K","isBolded":false,"associatedRows":["Turkish"],"associatedColumns":["The amounts of audio data in hours .","Train","clean"],"associatedMergedColumns":[]},{"number":"3.2K","isBolded":false,"associatedRows":["German"],"associatedColumns":["The amounts of audio data in hours .","Train","clean"],"associatedMergedColumns":[]},{"number":"24.0","isBolded":false,"associatedRows":["German"],"associatedColumns":["The amounts of audio data in hours .","Test","noisy"],"associatedMergedColumns":[]}]},{"caption":"Table 2. WER results on Romanian dataset. ?aux is used in Eq. 9, \n13 and 14. \"aux\" and \"kl\" loss denote the auxiliary RNN-T (Sec-\ntion 2.2.1) and KL divergence criterion (Section 2.2.2) respectively. \n\"crosslingual pretrain\" denotes the encoder pretrained from a high-\nresource Spanish RNN-T. WERR (%) is the unweighted average of \nthe respective relative WER reductions on clean and noisy test sets. \nModel \n?aux valid clean noisy WERR \nbaseline \n-\n24.0 20.5 22.0 \n-\n0.1 23.2 \n+ aux loss \n0.3 22.8 19.6 21.0 4.5% \n0.6 23.1 \n0.3 22.9 \n+ kl loss \n0.6 22.6 19.3 20.6 6.1% \n0.9 22.7 \n+ aux + kl loss \n0.3 22.5 19.1 20.8 6.1% \n+ crosslingual pretrain \n-\n19.4 15.9 17.6 21.2% \n+ aux + kl loss \n0.3 18.9 15.7 17.2 22.6% \n\nTable 3. WER results on Romanian. ?ce is used in Eq. 17. \"ce \npretrain\" denotes encoder pretraining from graphemic hybrid CE \nmodel. \"ce loss\" denotes auxiliary chenone prediction objective \nfunction (Section 2.3). \"mid\" denotes connecting CE loss to the 3rd \n(middle) encoder layer, and \"top\" denotes connecting CE loss to the \n5th (topmost) encoder layer. \nModel \n?ce valid clean noisy WERR \nbaseline \n-24.0 20.5 22.0 \n-\n+ ce pretrain \n-22.8 19.3 20.9 5.4% \n0.3 23.2 \n+ ce loss, top \n0.6 22.9 19.8 21.2 3.5% \n0.9 23.1 \n0.3 22.3 \n+ ce loss, mid \n0.6 22.0 18.5 20.3 8.7% \n0.9 22.0 \n+ ce pretrain, ce loss, mid \n0.6 21.4 17.9 19.6 11.8% \n+ ce pretrain, ce loss, mid, top 0.6 21.2 17.8 19.5 12.3% \n\nthe 24-layer, and 2048 for the 36-layer, resulting in total model pa-\nrameters of 83.3M and 160.3M respectively. \n\n","rows":["+ aux + kl loss","+ crosslingual pretrain","rameters of","+ ce pretrain , ce loss , mid , top","baseline","-","+ kl loss","+ aux loss","+ ce loss , mid"],"columns":["valid","\" ce loss \" denotes auxiliary chenone prediction objective","WERR","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","Model","?aux","?ce valid clean noisy WERR","clean","noisy","WER results on Romanian dataset . ?aux is used in Eq . 9 ,"],"mergedAllColumns":["the 24 - layer , and 2048 for the 36 - layer , resulting in total model pa -","the respective relative WER reductions on clean and noisy test sets .","5th ( topmost ) encoder layer .","-"],"numberCells":[{"number":"22.9","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid"],"associatedMergedColumns":["-"]},{"number":"18.5","isBolded":false,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","clean","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"20.3","isBolded":false,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","noisy","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"-22.8","isBolded":false,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"0.6","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","?aux"],"associatedMergedColumns":["-"]},{"number":"21.2","isBolded":false,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","noisy","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"22.8","isBolded":true,"associatedRows":["baseline","+ aux loss","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid"],"associatedMergedColumns":["-"]},{"number":"22.6","isBolded":true,"associatedRows":["baseline","+ kl loss","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid"],"associatedMergedColumns":["-"]},{"number":"20.5","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","clean"],"associatedMergedColumns":["the respective relative WER reductions on clean and noisy test sets ."]},{"number":"22.6%","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","WERR"],"associatedMergedColumns":["-"]},{"number":"6.1%","isBolded":false,"associatedRows":["baseline","+ aux + kl loss","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","WERR"],"associatedMergedColumns":["-"]},{"number":"-24.0","isBolded":false,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["5th ( topmost ) encoder layer ."]},{"number":"4.5%","isBolded":false,"associatedRows":["baseline","+ aux loss","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","WERR"],"associatedMergedColumns":["-"]},{"number":"6.1%","isBolded":false,"associatedRows":["baseline","+ kl loss","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","WERR"],"associatedMergedColumns":["-"]},{"number":"0.3","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","?aux"],"associatedMergedColumns":["-"]},{"number":"0.3","isBolded":false,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"17.8","isBolded":false,"associatedRows":["rameters of","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","clean","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"21.0","isBolded":false,"associatedRows":["baseline","+ aux loss","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","noisy"],"associatedMergedColumns":["-"]},{"number":"15.7","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","clean"],"associatedMergedColumns":["-"]},{"number":"21.4","isBolded":false,"associatedRows":["rameters of","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"19.6","isBolded":false,"associatedRows":["rameters of","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","noisy","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"0.3","isBolded":false,"associatedRows":["baseline","+ aux + kl loss","+ aux + kl loss"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","?aux"],"associatedMergedColumns":["-"]},{"number":"20.5","isBolded":false,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","clean","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["5th ( topmost ) encoder layer ."]},{"number":"20.8","isBolded":false,"associatedRows":["baseline","+ aux + kl loss","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","noisy"],"associatedMergedColumns":["-"]},{"number":"23.1","isBolded":false,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"19.3","isBolded":false,"associatedRows":["baseline","+ kl loss","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","clean"],"associatedMergedColumns":["-"]},{"number":"83.3Mand","isBolded":false,"associatedRows":["rameters of"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","Model","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","Model"],"associatedMergedColumns":["the 24 - layer , and 2048 for the 36 - layer , resulting in total model pa -"]},{"number":"0.6","isBolded":true,"associatedRows":["baseline","+ kl loss","+ aux + kl loss"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","?aux"],"associatedMergedColumns":["-"]},{"number":"22.0","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","noisy"],"associatedMergedColumns":["the respective relative WER reductions on clean and noisy test sets ."]},{"number":"8.7%","isBolded":false,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","WERR","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"15.9","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","clean"],"associatedMergedColumns":["-"]},{"number":"19.3","isBolded":false,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","clean","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"0.3","isBolded":false,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"160.3Mrespectively.","isBolded":false,"associatedRows":["rameters of","+ ce loss , mid"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","?aux","valid","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["the 24 - layer , and 2048 for the 36 - layer , resulting in total model pa -"]},{"number":"22.9","isBolded":true,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"22.0","isBolded":true,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"19.5","isBolded":false,"associatedRows":["rameters of","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","noisy","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"0.6","isBolded":true,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"23.2","isBolded":false,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"17.2","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","noisy"],"associatedMergedColumns":["-"]},{"number":"12.3%","isBolded":false,"associatedRows":["rameters of","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","WERR","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"22.3","isBolded":false,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"22.0","isBolded":false,"associatedRows":["rameters of","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"5.4%","isBolded":false,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","WERR","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"17.9","isBolded":false,"associatedRows":["rameters of","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","clean","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"18.9","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid"],"associatedMergedColumns":["-"]},{"number":"20.6","isBolded":false,"associatedRows":["baseline","+ kl loss","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","noisy"],"associatedMergedColumns":["-"]},{"number":"19.6","isBolded":false,"associatedRows":["baseline","+ aux loss","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","clean"],"associatedMergedColumns":["-"]},{"number":"22.0","isBolded":false,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","noisy","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["5th ( topmost ) encoder layer ."]},{"number":"0.6","isBolded":false,"associatedRows":["rameters of","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"24.0","isBolded":false,"associatedRows":["baseline","+ aux loss","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid"],"associatedMergedColumns":["the respective relative WER reductions on clean and noisy test sets ."]},{"number":"23.1","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid"],"associatedMergedColumns":["-"]},{"number":"0.9","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","?aux"],"associatedMergedColumns":["-"]},{"number":"0.3","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","?aux"],"associatedMergedColumns":["-"]},{"number":"21.2%","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","WERR"],"associatedMergedColumns":["-"]},{"number":"19.1","isBolded":false,"associatedRows":["baseline","+ aux + kl loss","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","clean"],"associatedMergedColumns":["-"]},{"number":"3.5%","isBolded":false,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","WERR","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"22.7","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid"],"associatedMergedColumns":["-"]},{"number":"0.9","isBolded":false,"associatedRows":["rameters of","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"17.6","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","noisy"],"associatedMergedColumns":["-"]},{"number":"21.2","isBolded":false,"associatedRows":["rameters of","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"0.9","isBolded":false,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"0.6","isBolded":true,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"20.9","isBolded":false,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","noisy","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"11.8%","isBolded":false,"associatedRows":["rameters of","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","WERR","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"0.6","isBolded":false,"associatedRows":["rameters of","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"0.3","isBolded":true,"associatedRows":["baseline","+ aux loss","+ aux + kl loss"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","?aux"],"associatedMergedColumns":["-"]},{"number":"19.8","isBolded":false,"associatedRows":["baseline","+ ce pretrain , ce loss , mid , top"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","clean","WER results on Romanian . ?ce is used in Eq . 17 . \" ce","\" ce loss \" denotes auxiliary chenone prediction objective","?ce valid clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"23.2","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid"],"associatedMergedColumns":["-"]},{"number":"19.4","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid"],"associatedMergedColumns":["-"]},{"number":"22.5","isBolded":false,"associatedRows":["baseline","+ aux + kl loss","+ aux + kl loss","-"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","valid"],"associatedMergedColumns":["-"]},{"number":"0.1","isBolded":false,"associatedRows":["baseline","+ aux loss","+ aux + kl loss"],"associatedColumns":["WER results on Romanian dataset . ?aux is used in Eq . 9 ,","?aux"],"associatedMergedColumns":["-"]}]},{"caption":"Table 4. WER results on Turkish and German, with ?aux \u003d 0.3 and \n?ce \u003d 0.6. \nTurkish \nGerman \nModel \nclean noisy WERR clean noisy WERR \nbaseline \n17.1 18.9 \n-\n11.6 13.0 \n-\n+ aux loss \n16.8 18.8 1.1% 11.3 12.6 2.8% \n+ kl loss \n16.7 18.8 1.4% 11.5 12.8 1.2% \n+ aux + kl loss \n16.4 18.5 3.1% 11.3 12.6 2.8% \n+ crosslingual pretrain 16.6 18.6 2.3% 11.4 12.8 1.6% \n+ aux + kl loss \n16.1 18.1 5.0% 11.3 12.4 3.6% \n+ ce pretrain \n16.8 18.9 0.9% 11.5 12.8 1.2% \n+ ce loss, mid \n16.5 18.4 3.1% 11.3 12.5 3.2% \n+ ce loss, mid, top 16.3 18.2 4.2% 11.2 12.3 4.4% \n\n","rows":["+ aux + kl loss","Table 4 .","+ crosslingual pretrain","+ ce pretrain","?ce \u003d","baseline","+ ce loss , mid , top","-","+ kl loss","+ aux loss","WER results on Turkish and German , with ?aux \u003d","+ ce loss , mid"],"columns":["clean noisy WERR clean noisy WERR","German","Turkish"],"mergedAllColumns":["-"],"numberCells":[{"number":"11.3","isBolded":false,"associatedRows":["baseline","+ aux loss","-"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"12.8","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"4.2%","isBolded":true,"associatedRows":["baseline","+ crosslingual pretrain","+ ce loss , mid , top"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"11.5","isBolded":false,"associatedRows":["baseline","+ kl loss","-"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"11.3","isBolded":false,"associatedRows":["baseline","+ aux + kl loss","-"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"1.4%","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"1.2%","isBolded":false,"associatedRows":["baseline","+ kl loss","WER results on Turkish and German , with ?aux \u003d"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"3.1%","isBolded":true,"associatedRows":["baseline","+ crosslingual pretrain"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"18.1","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"1.6%","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"3.1%","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ ce loss , mid"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"3.6%","isBolded":true,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"18.9","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"2.8%","isBolded":false,"associatedRows":["baseline","+ aux loss","WER results on Turkish and German , with ?aux \u003d"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"16.4","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"16.7","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"11.5","isBolded":false,"associatedRows":["baseline","+ ce pretrain","-"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"12.4","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"12.8","isBolded":false,"associatedRows":["baseline","+ kl loss","-"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"17.1","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":[]},{"number":"16.1","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"11.3","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ ce loss , mid"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"18.6","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"4.4%","isBolded":true,"associatedRows":["baseline","+ crosslingual pretrain","+ ce loss , mid , top"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"11.3","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"16.3","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ ce loss , mid , top"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"1.1%","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"16.8","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"2.8%","isBolded":true,"associatedRows":["baseline","+ aux + kl loss","-"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"18.9","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":[]},{"number":"13.0","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","-"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":[]},{"number":"16.6","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"1.2%","isBolded":false,"associatedRows":["baseline","+ ce pretrain","-"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"2.3%","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"0.3and","isBolded":true,"associatedRows":["Table 4 .","+ crosslingual pretrain","WER results on Turkish and German , with ?aux \u003d"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"11.6","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","-"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":[]},{"number":"18.5","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"12.5","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ ce loss , mid"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"11.2","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ ce loss , mid , top"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"11.4","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"12.3","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ ce loss , mid , top"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"18.8","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"18.8","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"12.8","isBolded":false,"associatedRows":["baseline","+ ce pretrain","-"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"3.2%","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ ce loss , mid"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"0.6.","isBolded":true,"associatedRows":["?ce \u003d"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"18.2","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ ce loss , mid , top"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"12.6","isBolded":false,"associatedRows":["baseline","+ aux loss","-"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"16.5","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ ce loss , mid"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"12.6","isBolded":false,"associatedRows":["baseline","+ aux + kl loss","-"],"associatedColumns":["German","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"18.4","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain","+ ce loss , mid"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"0.9%","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"5.0%","isBolded":true,"associatedRows":["baseline","+ crosslingual pretrain","+ aux + kl loss"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]},{"number":"16.8","isBolded":false,"associatedRows":["baseline","+ crosslingual pretrain"],"associatedColumns":["Turkish","clean noisy WERR clean noisy WERR"],"associatedMergedColumns":["-"]}]},{"caption":"Table 5. WER results on LibriSpeech, with 24-layer transformer \nencoder and 83M total model parameters. \nModel \ntest-clean WERR test-other WERR \nbaseline \n2.77 \n-\n6.60 \n-\n+ aux + kl loss \n2.48 \n10.6% \n5.62 \n14.8% \n+ ce loss \n2.42 \n12.6% \n5.75 \n12.9% \n+ aux + kl + ce loss \n2.31 \n16.5% \n5.26 \n20.3% \n\nTable 6. Comparison of our models (with 36-layer transformer \nencoder and 160M total model parameters) with recently published \nbest results on LibriSpeech. \nModel \nw/o LM \nw/ LM \ntest-clean test-other test-clean test-other \nLAS \nLSTM [46] \n2.6 \n6.0 \n2.2 \n5.2 \nHybrid \nTransformer [32] \n2.6 \n5.6 \n2.3 \n4.9 \nCTC \nTransformer [47] \n2.3 \n4.8 \n2.1 \n4.2 \nSequence Transducer \nTransformer [33] \n2.4 \n5.6 \n2.0 \n4.6 \nConformer [48] \n2.1 \n4.3 \n1.9 \n3.9 \nTransformer (Ours) \n2.2 \n4.7 \n2.0 \n4.2 \n\n","rows":["+ aux + kl loss","+ ce loss","Transformer ( Ours )","Transformer [ 32 ]","Transformer [ 33 ]","Conformer [ 48 ]","Transformer [ 47 ]","+ aux + kl + ce loss","baseline","LSTM [ 46 ]","-"],"columns":["test - clean","w / o LM","w / LM","Comparison of our models ( with 36 - layer transformer","WERR","test - other","WER results on LibriSpeech , with 24 - layer transformer"],"mergedAllColumns":["CTC","encoder and 83M total model parameters .","Hybrid","Sequence Transducer","-","LAS"],"numberCells":[{"number":"4.6","isBolded":false,"associatedRows":["Transformer [ 33 ]"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","WERR","Comparison of our models ( with 36 - layer transformer","w / LM"],"associatedMergedColumns":["Sequence Transducer"]},{"number":"2.31","isBolded":false,"associatedRows":["baseline","+ aux + kl + ce loss"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","test - clean"],"associatedMergedColumns":["-"]},{"number":"2.1","isBolded":false,"associatedRows":["Conformer [ 48 ]"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","test - clean","Comparison of our models ( with 36 - layer transformer","w / o LM"],"associatedMergedColumns":["Sequence Transducer"]},{"number":"12.6%","isBolded":false,"associatedRows":["baseline","+ ce loss"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","WERR"],"associatedMergedColumns":["-"]},{"number":"5.75","isBolded":false,"associatedRows":["baseline","+ ce loss"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","test - other"],"associatedMergedColumns":["-"]},{"number":"12.9%","isBolded":false,"associatedRows":["baseline","+ ce loss"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","WERR"],"associatedMergedColumns":["-"]},{"number":"5.2","isBolded":false,"associatedRows":["Transformer [ 32 ]","LSTM [ 46 ]"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","WERR","Comparison of our models ( with 36 - layer transformer","w / LM"],"associatedMergedColumns":["LAS"]},{"number":"20.3%","isBolded":false,"associatedRows":["baseline","+ aux + kl + ce loss"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","WERR"],"associatedMergedColumns":["-"]},{"number":"2.42","isBolded":false,"associatedRows":["baseline","+ ce loss"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","test - clean"],"associatedMergedColumns":["-"]},{"number":"5.6","isBolded":false,"associatedRows":["Transformer [ 32 ]"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","WERR","Comparison of our models ( with 36 - layer transformer","w / o LM"],"associatedMergedColumns":["Hybrid"]},{"number":"2.0","isBolded":true,"associatedRows":["Transformer ( Ours )"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","test - other","Comparison of our models ( with 36 - layer transformer","w / LM"],"associatedMergedColumns":["Sequence Transducer"]},{"number":"4.9","isBolded":false,"associatedRows":["Transformer [ 32 ]"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","WERR","Comparison of our models ( with 36 - layer transformer","w / LM"],"associatedMergedColumns":["Hybrid"]},{"number":"14.8%","isBolded":false,"associatedRows":["baseline","+ aux + kl loss"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","WERR"],"associatedMergedColumns":["-"]},{"number":"2.0","isBolded":false,"associatedRows":["Transformer [ 33 ]"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","test - other","Comparison of our models ( with 36 - layer transformer","w / LM"],"associatedMergedColumns":["Sequence Transducer"]},{"number":"2.4","isBolded":false,"associatedRows":["Transformer [ 33 ]"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","test - clean","Comparison of our models ( with 36 - layer transformer","w / o LM"],"associatedMergedColumns":["Sequence Transducer"]},{"number":"2.48","isBolded":false,"associatedRows":["baseline","+ aux + kl loss"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","test - clean"],"associatedMergedColumns":["-"]},{"number":"4.2","isBolded":true,"associatedRows":["Transformer ( Ours )"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","WERR","Comparison of our models ( with 36 - layer transformer","w / LM"],"associatedMergedColumns":["Sequence Transducer"]},{"number":"6.0","isBolded":false,"associatedRows":["Transformer [ 32 ]","LSTM [ 46 ]"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","WERR","Comparison of our models ( with 36 - layer transformer","w / o LM"],"associatedMergedColumns":["LAS"]},{"number":"5.6","isBolded":false,"associatedRows":["Transformer [ 33 ]"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","WERR","Comparison of our models ( with 36 - layer transformer","w / o LM"],"associatedMergedColumns":["Sequence Transducer"]},{"number":"2.2","isBolded":false,"associatedRows":["Transformer [ 32 ]","LSTM [ 46 ]"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","test - other","Comparison of our models ( with 36 - layer transformer","w / LM"],"associatedMergedColumns":["LAS"]},{"number":"2.6","isBolded":false,"associatedRows":["Transformer [ 32 ]"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","test - clean","Comparison of our models ( with 36 - layer transformer","w / o LM"],"associatedMergedColumns":["Hybrid"]},{"number":"16.5%","isBolded":false,"associatedRows":["baseline","+ aux + kl + ce loss"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","WERR"],"associatedMergedColumns":["-"]},{"number":"4.7","isBolded":true,"associatedRows":["Transformer ( Ours )"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","WERR","Comparison of our models ( with 36 - layer transformer","w / o LM"],"associatedMergedColumns":["Sequence Transducer"]},{"number":"6.60","isBolded":false,"associatedRows":["baseline","+ aux + kl + ce loss","-"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","test - other"],"associatedMergedColumns":["encoder and 83M total model parameters ."]},{"number":"2.2","isBolded":true,"associatedRows":["Transformer ( Ours )"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","test - clean","Comparison of our models ( with 36 - layer transformer","w / o LM"],"associatedMergedColumns":["Sequence Transducer"]},{"number":"1.9","isBolded":false,"associatedRows":["Conformer [ 48 ]"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","test - other","Comparison of our models ( with 36 - layer transformer","w / LM"],"associatedMergedColumns":["Sequence Transducer"]},{"number":"3.9","isBolded":false,"associatedRows":["Conformer [ 48 ]"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","WERR","Comparison of our models ( with 36 - layer transformer","w / LM"],"associatedMergedColumns":["Sequence Transducer"]},{"number":"2.3","isBolded":false,"associatedRows":["Transformer [ 32 ]"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","test - other","Comparison of our models ( with 36 - layer transformer","w / LM"],"associatedMergedColumns":["Hybrid"]},{"number":"2.3","isBolded":false,"associatedRows":["Transformer [ 47 ]"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","test - clean","Comparison of our models ( with 36 - layer transformer","w / o LM"],"associatedMergedColumns":["CTC"]},{"number":"4.2","isBolded":false,"associatedRows":["Transformer [ 47 ]"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","WERR","Comparison of our models ( with 36 - layer transformer","w / LM"],"associatedMergedColumns":["CTC"]},{"number":"5.62","isBolded":false,"associatedRows":["baseline","+ aux + kl loss"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","test - other"],"associatedMergedColumns":["-"]},{"number":"2.6","isBolded":false,"associatedRows":["Transformer [ 32 ]","LSTM [ 46 ]"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","test - clean","Comparison of our models ( with 36 - layer transformer","w / o LM"],"associatedMergedColumns":["LAS"]},{"number":"10.6%","isBolded":false,"associatedRows":["baseline","+ aux + kl loss"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","WERR"],"associatedMergedColumns":["-"]},{"number":"4.3","isBolded":false,"associatedRows":["Conformer [ 48 ]"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","WERR","Comparison of our models ( with 36 - layer transformer","w / o LM"],"associatedMergedColumns":["Sequence Transducer"]},{"number":"2.77","isBolded":false,"associatedRows":["baseline","+ aux + kl + ce loss"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","test - clean"],"associatedMergedColumns":["encoder and 83M total model parameters ."]},{"number":"2.1","isBolded":false,"associatedRows":["Transformer [ 47 ]"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","test - other","Comparison of our models ( with 36 - layer transformer","w / LM"],"associatedMergedColumns":["CTC"]},{"number":"5.26","isBolded":false,"associatedRows":["baseline","+ aux + kl + ce loss"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","test - other"],"associatedMergedColumns":["-"]},{"number":"4.8","isBolded":false,"associatedRows":["Transformer [ 47 ]"],"associatedColumns":["WER results on LibriSpeech , with 24 - layer transformer","WERR","Comparison of our models ( with 36 - layer transformer","w / o LM"],"associatedMergedColumns":["CTC"]}]}]