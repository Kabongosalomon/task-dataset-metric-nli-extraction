[{"caption":"CSQA \n(Acc) \n\nWINOGRANDE \n(AUC) \n\nCODAH \n(Acc) \n\nHellaSwag-2K \n(Acc) \nAverage \n\nROBERTA (reported) 72.1 \n66.4 \n-\n-\n-\nROBERTA (ours) \n71.6 \n67.5 \n82.3 \n75.4 \n74.2 \nBACKTRANSLATION 70.2 \n67.2 \n81.8 \n73.0 \n73.1 \n\nG-DAUG c -Rand \n71.8 \n70.9 \n83.6 \n75.9 \n75.6 \nG-DAUG c -Influence \n72.1 \n70.9 \n84.3 \n75.8 \n75.8 \nG-DAUG c -Diversity \n72.3 \n71.2 \n83.5 \n76.1 \n75.8 \nG-DAUG c -Combo \n72.6 \n71.4 \n84.0 \n76.8 \n76.2 \n\nTable 1: Results on the test sets of four commonsense benchmarks. ROBERTA (reported) is the result for the \nROBERTA-large baseline reported on public leaderboards. 4 ROBERTA (ours) is re-evaluation of the ROBERTA-\nlarge model using our setup. All G-DAUG c methods outperform the baseline methods, and G-DAUG c -Combo \nperforms the best overall. \n\n","rows":["ROBERTA ( ours )","ROBERTA ( reported )","G - DAUG c - Diversity","G - DAUG c - Combo","G - DAUG c - Influence","G - DAUG c - Rand","BACKTRANSLATION"],"columns":["( Acc )","WINOGRANDE","HellaSwag - 2K","CODAH","-","CSQA","( AUC )"],"mergedAllColumns":["Average"],"numberCells":[{"number":"74.2","isBolded":false,"associatedRows":["ROBERTA ( ours )"],"associatedColumns":["HellaSwag - 2K","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"70.2","isBolded":false,"associatedRows":["BACKTRANSLATION"],"associatedColumns":["CSQA","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"81.8","isBolded":false,"associatedRows":["BACKTRANSLATION"],"associatedColumns":["CODAH","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"72.3","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["CSQA","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"76.1","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["HellaSwag - 2K","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"75.9","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["HellaSwag - 2K","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"75.8","isBolded":false,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["HellaSwag - 2K","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"76.8","isBolded":true,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["HellaSwag - 2K","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"73.1","isBolded":false,"associatedRows":["BACKTRANSLATION"],"associatedColumns":["HellaSwag - 2K","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"70.9","isBolded":false,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["WINOGRANDE","( AUC )","-"],"associatedMergedColumns":["Average"]},{"number":"75.6","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["HellaSwag - 2K","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"73.0","isBolded":false,"associatedRows":["BACKTRANSLATION"],"associatedColumns":["HellaSwag - 2K","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"71.2","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["WINOGRANDE","( AUC )","-"],"associatedMergedColumns":["Average"]},{"number":"76.2","isBolded":true,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["HellaSwag - 2K","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"67.2","isBolded":false,"associatedRows":["BACKTRANSLATION"],"associatedColumns":["WINOGRANDE","( AUC )","-"],"associatedMergedColumns":["Average"]},{"number":"67.5","isBolded":false,"associatedRows":["ROBERTA ( ours )"],"associatedColumns":["WINOGRANDE","( AUC )","-"],"associatedMergedColumns":["Average"]},{"number":"70.9","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["WINOGRANDE","( AUC )","-"],"associatedMergedColumns":["Average"]},{"number":"83.6","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["CODAH","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"75.8","isBolded":false,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["HellaSwag - 2K","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"66.4","isBolded":false,"associatedRows":["ROBERTA ( reported )"],"associatedColumns":["WINOGRANDE","( AUC )"],"associatedMergedColumns":["Average"]},{"number":"71.4","isBolded":true,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["WINOGRANDE","( AUC )","-"],"associatedMergedColumns":["Average"]},{"number":"72.1","isBolded":false,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["CSQA","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"72.6","isBolded":true,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["CSQA","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"84.0","isBolded":false,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["CODAH","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"72.1","isBolded":false,"associatedRows":["ROBERTA ( reported )"],"associatedColumns":["CSQA","( Acc )"],"associatedMergedColumns":["Average"]},{"number":"84.3","isBolded":true,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["CODAH","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"75.4","isBolded":false,"associatedRows":["ROBERTA ( ours )"],"associatedColumns":["HellaSwag - 2K","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"71.6","isBolded":false,"associatedRows":["ROBERTA ( ours )"],"associatedColumns":["CSQA","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"71.8","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["CSQA","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"75.8","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["HellaSwag - 2K","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"83.5","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["CODAH","( Acc )","-"],"associatedMergedColumns":["Average"]},{"number":"82.3","isBolded":false,"associatedRows":["ROBERTA ( ours )"],"associatedColumns":["CODAH","( Acc )","-"],"associatedMergedColumns":["Average"]}]},{"caption":"Table 2: Results on WordNet-based synonym replacement sets. For CODAH and HellaSwag-2K, we perturb test \nsets, as the labels are available. G-DAUG c -Combo achieves the highest average score. \n\n","rows":["ROBERTA ( ours )","G - DAUG c - Diversity","G - DAUG c - Combo","G - DAUG c - Influence","G - DAUG c - Rand","BACKTRANSLATION"],"columns":["WINOGRANDE","HellaSwag - 2K","Average","CODAH","CSQA"],"mergedAllColumns":[],"numberCells":[{"number":"74.7","isBolded":false,"associatedRows":["ROBERTA ( ours )"],"associatedColumns":["CODAH"],"associatedMergedColumns":[]},{"number":"76.0","isBolded":false,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["CODAH"],"associatedMergedColumns":[]},{"number":"65.4","isBolded":true,"associatedRows":["BACKTRANSLATION"],"associatedColumns":["HellaSwag - 2K"],"associatedMergedColumns":[]},{"number":"62.3","isBolded":false,"associatedRows":["BACKTRANSLATION"],"associatedColumns":["WINOGRANDE"],"associatedMergedColumns":[]},{"number":"69.4","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["Average"],"associatedMergedColumns":[]},{"number":"66.0","isBolded":true,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["WINOGRANDE"],"associatedMergedColumns":[]},{"number":"72.1","isBolded":true,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["CSQA"],"associatedMergedColumns":[]},{"number":"71.6","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["CSQA"],"associatedMergedColumns":[]},{"number":"76.0","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["CODAH"],"associatedMergedColumns":[]},{"number":"64.3","isBolded":false,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["HellaSwag - 2K"],"associatedMergedColumns":[]},{"number":"65.2","isBolded":false,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["HellaSwag - 2K"],"associatedMergedColumns":[]},{"number":"66.0","isBolded":true,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["WINOGRANDE"],"associatedMergedColumns":[]},{"number":"69.9","isBolded":false,"associatedRows":["ROBERTA ( ours )"],"associatedColumns":["CSQA"],"associatedMergedColumns":[]},{"number":"64.8","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["HellaSwag - 2K"],"associatedMergedColumns":[]},{"number":"69.3","isBolded":false,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["Average"],"associatedMergedColumns":[]},{"number":"69.6","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["Average"],"associatedMergedColumns":[]},{"number":"72.0","isBolded":false,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["CSQA"],"associatedMergedColumns":[]},{"number":"69.0","isBolded":false,"associatedRows":["BACKTRANSLATION"],"associatedColumns":["CSQA"],"associatedMergedColumns":[]},{"number":"75.5","isBolded":false,"associatedRows":["BACKTRANSLATION"],"associatedColumns":["CODAH"],"associatedMergedColumns":[]},{"number":"71.0","isBolded":false,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["CSQA"],"associatedMergedColumns":[]},{"number":"63.8","isBolded":false,"associatedRows":["ROBERTA ( ours )"],"associatedColumns":["WINOGRANDE"],"associatedMergedColumns":[]},{"number":"67.9","isBolded":false,"associatedRows":["ROBERTA ( ours )"],"associatedColumns":["Average"],"associatedMergedColumns":[]},{"number":"65.5","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["WINOGRANDE"],"associatedMergedColumns":[]},{"number":"63.2","isBolded":false,"associatedRows":["ROBERTA ( ours )"],"associatedColumns":["HellaSwag - 2K"],"associatedMergedColumns":[]},{"number":"75.9","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["CODAH"],"associatedMergedColumns":[]},{"number":"64.1","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["HellaSwag - 2K"],"associatedMergedColumns":[]},{"number":"76.2","isBolded":true,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["CODAH"],"associatedMergedColumns":[]},{"number":"69.8","isBolded":true,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["Average"],"associatedMergedColumns":[]},{"number":"68.1","isBolded":false,"associatedRows":["BACKTRANSLATION"],"associatedColumns":["Average"],"associatedMergedColumns":[]},{"number":"65.7","isBolded":false,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["WINOGRANDE"],"associatedMergedColumns":[]}]},{"caption":"Table 3: Robustness to TextFooler-based adversarial attacks (failure rate / average perturbation ratio, higher is \nbetter for both). Models trained with augmented data are more robust to TextFooler\u0027s attacks compared to models \nwithout data augmentation. On average, G-DAUG c -Diversity performs the best. \n\n","rows":["G - DAUG c - Diversity","G - DAUG c - Combo","G - DAUG c - Influence","Backtranslation","RoBERTa ( ours )","G - DAUG c - Rand"],"columns":["NLI Diag .","Test","Val .","TF : Fail","TF : Pert","Syn .","SNLI - 3K","ARC - Challenge Scientific QA"],"mergedAllColumns":[],"numberCells":[{"number":"8.1","isBolded":false,"associatedRows":["Backtranslation"],"associatedColumns":["SNLI - 3K","Test"],"associatedMergedColumns":[]},{"number":"13.1","isBolded":false,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["ARC - Challenge Scientific QA","TF : Fail"],"associatedMergedColumns":[]},{"number":"42.4","isBolded":false,"associatedRows":["Backtranslation"],"associatedColumns":["ARC - Challenge Scientific QA","Syn ."],"associatedMergedColumns":[]},{"number":"56.9","isBolded":false,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["SNLI - 3K","NLI Diag ."],"associatedMergedColumns":[]},{"number":"54.0","isBolded":false,"associatedRows":["Backtranslation"],"associatedColumns":["SNLI - 3K","NLI Diag ."],"associatedMergedColumns":[]},{"number":"17.0","isBolded":false,"associatedRows":["RoBERTa ( ours )"],"associatedColumns":["SNLI - 3K","TF : Fail"],"associatedMergedColumns":[]},{"number":"91.8","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["ARC - Challenge Scientific QA","Val ."],"associatedMergedColumns":[]},{"number":"88.6","isBolded":false,"associatedRows":["RoBERTa ( ours )"],"associatedColumns":["SNLI - 3K","Test"],"associatedMergedColumns":[]},{"number":"43.8","isBolded":false,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["ARC - Challenge Scientific QA","Syn ."],"associatedMergedColumns":[]},{"number":"20.6","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["SNLI - 3K","TF : Pert"],"associatedMergedColumns":[]},{"number":"79.4","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["SNLI - 3K","Syn ."],"associatedMergedColumns":[]},{"number":"81.0","isBolded":true,"associatedRows":["Backtranslation"],"associatedColumns":["SNLI - 3K","Syn ."],"associatedMergedColumns":[]},{"number":"91.8","isBolded":false,"associatedRows":["RoBERTa ( ours )"],"associatedColumns":["ARC - Challenge Scientific QA","Val ."],"associatedMergedColumns":[]},{"number":"13.9","isBolded":true,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["ARC - Challenge Scientific QA","TF : Fail"],"associatedMergedColumns":[]},{"number":"43.5","isBolded":false,"associatedRows":["RoBERTa ( ours )"],"associatedColumns":["ARC - Challenge Scientific QA","Val ."],"associatedMergedColumns":[]},{"number":"12.4","isBolded":false,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["ARC - Challenge Scientific QA","TF : Fail"],"associatedMergedColumns":[]},{"number":"48.1","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["ARC - Challenge Scientific QA","Test"],"associatedMergedColumns":[]},{"number":"42.2","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["ARC - Challenge Scientific QA","Syn ."],"associatedMergedColumns":[]},{"number":"91.9","isBolded":false,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["ARC - Challenge Scientific QA","Val ."],"associatedMergedColumns":[]},{"number":"78.7","isBolded":false,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["SNLI - 3K","Syn ."],"associatedMergedColumns":[]},{"number":"20.7","isBolded":false,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["SNLI - 3K","TF : Pert"],"associatedMergedColumns":[]},{"number":"6.6","isBolded":false,"associatedRows":["RoBERTa ( ours )"],"associatedColumns":["ARC - Challenge Scientific QA","TF : Fail"],"associatedMergedColumns":[]},{"number":"49.5","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["ARC - Challenge Scientific QA","Val ."],"associatedMergedColumns":[]},{"number":"89.0","isBolded":true,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["SNLI - 3K","Test"],"associatedMergedColumns":[]},{"number":"78.6","isBolded":false,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["SNLI - 3K","Syn ."],"associatedMergedColumns":[]},{"number":"10.7","isBolded":false,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["ARC - Challenge Scientific QA","TF : Pert"],"associatedMergedColumns":[]},{"number":"16.7","isBolded":false,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["SNLI - 3K","TF : Fail"],"associatedMergedColumns":[]},{"number":"88.7","isBolded":false,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["SNLI - 3K","Test"],"associatedMergedColumns":[]},{"number":"6.6","isBolded":false,"associatedRows":["Backtranslation"],"associatedColumns":["ARC - Challenge Scientific QA","TF : Fail"],"associatedMergedColumns":[]},{"number":"11.0","isBolded":true,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["ARC - Challenge Scientific QA","TF : Pert"],"associatedMergedColumns":[]},{"number":"91.2","isBolded":false,"associatedRows":["Backtranslation"],"associatedColumns":["ARC - Challenge Scientific QA","Val ."],"associatedMergedColumns":[]},{"number":"39.4","isBolded":false,"associatedRows":["RoBERTa ( ours )"],"associatedColumns":["ARC - Challenge Scientific QA","Test"],"associatedMergedColumns":[]},{"number":"92.0","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["ARC - Challenge Scientific QA","Val ."],"associatedMergedColumns":[]},{"number":"78.6","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["SNLI - 3K","Syn ."],"associatedMergedColumns":[]},{"number":"51.5","isBolded":true,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["ARC - Challenge Scientific QA","Val ."],"associatedMergedColumns":[]},{"number":"77.5","isBolded":false,"associatedRows":["RoBERTa ( ours )"],"associatedColumns":["SNLI - 3K","Syn ."],"associatedMergedColumns":[]},{"number":"50.8","isBolded":false,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["ARC - Challenge Scientific QA","Val ."],"associatedMergedColumns":[]},{"number":"18.0","isBolded":false,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["SNLI - 3K","TF : Fail"],"associatedMergedColumns":[]},{"number":"12.9","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["ARC - Challenge Scientific QA","TF : Fail"],"associatedMergedColumns":[]},{"number":"9.3","isBolded":false,"associatedRows":["Backtranslation"],"associatedColumns":["ARC - Challenge Scientific QA","TF : Pert"],"associatedMergedColumns":[]},{"number":"20.5","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["SNLI - 3K","TF : Pert"],"associatedMergedColumns":[]},{"number":"45.2","isBolded":true,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["ARC - Challenge Scientific QA","Syn ."],"associatedMergedColumns":[]},{"number":"57.4","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["SNLI - 3K","NLI Diag ."],"associatedMergedColumns":[]},{"number":"92.3","isBolded":true,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["ARC - Challenge Scientific QA","Val ."],"associatedMergedColumns":[]},{"number":"35.2","isBolded":false,"associatedRows":["RoBERTa ( ours )"],"associatedColumns":["ARC - Challenge Scientific QA","Syn ."],"associatedMergedColumns":[]},{"number":"50.8","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["ARC - Challenge Scientific QA","Val ."],"associatedMergedColumns":[]},{"number":"43.4","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["ARC - Challenge Scientific QA","Syn ."],"associatedMergedColumns":[]},{"number":"10.8","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["ARC - Challenge Scientific QA","TF : Pert"],"associatedMergedColumns":[]},{"number":"20.5","isBolded":false,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["SNLI - 3K","TF : Pert"],"associatedMergedColumns":[]},{"number":"9.3","isBolded":false,"associatedRows":["RoBERTa ( ours )"],"associatedColumns":["ARC - Challenge Scientific QA","TF : Pert"],"associatedMergedColumns":[]},{"number":"20.2","isBolded":false,"associatedRows":["RoBERTa ( ours )"],"associatedColumns":["SNLI - 3K","TF : Pert"],"associatedMergedColumns":[]},{"number":"19.0","isBolded":true,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["SNLI - 3K","TF : Fail"],"associatedMergedColumns":[]},{"number":"57.6","isBolded":false,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["SNLI - 3K","NLI Diag ."],"associatedMergedColumns":[]},{"number":"43.1","isBolded":false,"associatedRows":["Backtranslation"],"associatedColumns":["ARC - Challenge Scientific QA","Test"],"associatedMergedColumns":[]},{"number":"56.7","isBolded":false,"associatedRows":["RoBERTa ( ours )"],"associatedColumns":["SNLI - 3K","NLI Diag ."],"associatedMergedColumns":[]},{"number":"43.1","isBolded":false,"associatedRows":["Backtranslation"],"associatedColumns":["ARC - Challenge Scientific QA","Val ."],"associatedMergedColumns":[]},{"number":"47.5","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["ARC - Challenge Scientific QA","Test"],"associatedMergedColumns":[]},{"number":"48.5","isBolded":true,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["ARC - Challenge Scientific QA","Test"],"associatedMergedColumns":[]},{"number":"21.7","isBolded":true,"associatedRows":["Backtranslation"],"associatedColumns":["SNLI - 3K","TF : Pert"],"associatedMergedColumns":[]},{"number":"48.2","isBolded":false,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["ARC - Challenge Scientific QA","Test"],"associatedMergedColumns":[]},{"number":"88.7","isBolded":false,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["SNLI - 3K","Test"],"associatedMergedColumns":[]},{"number":"89.0","isBolded":true,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["SNLI - 3K","Test"],"associatedMergedColumns":[]},{"number":"57.7","isBolded":true,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["SNLI - 3K","NLI Diag ."],"associatedMergedColumns":[]},{"number":"10.8","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["ARC - Challenge Scientific QA","TF : Pert"],"associatedMergedColumns":[]},{"number":"17.7","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["SNLI - 3K","TF : Fail"],"associatedMergedColumns":[]},{"number":"18.8","isBolded":false,"associatedRows":["Backtranslation"],"associatedColumns":["SNLI - 3K","TF : Fail"],"associatedMergedColumns":[]}]},{"caption":"Table 5: Results comparing G-DAUG c \u0027s filtering meth-\nods against using the entire synthetic data pool for aug-\nmentation, on WINOGRANDE-M. \n\n","rows":["Acc","two - staged training method ( Section"],"columns":["G - DAUG c uses a","Random","log","Size","Diversity","127478","two stage training ( +1 . 8 increase ) outperforms both","Influence","Whole Pool","Validation results for different training set","380700","training on XS or S matches unaugmented"],"mergedAllColumns":["the XS size , but hurts performance on larger sizes .","mixing ( +0 . 0 ) and importance weighted loss ( +0 . 7 ) ."],"numberCells":[{"number":"74.4","isBolded":false,"associatedRows":["Acc"],"associatedColumns":["two stage training ( +1 . 8 increase ) outperforms both","Influence","127478"],"associatedMergedColumns":["mixing ( +0 . 0 ) and importance weighted loss ( +0 . 7 ) ."]},{"number":"73.0","isBolded":false,"associatedRows":["Acc"],"associatedColumns":["two stage training ( +1 . 8 increase ) outperforms both","Diversity","127478"],"associatedMergedColumns":["mixing ( +0 . 0 ) and importance weighted loss ( +0 . 7 ) ."]},{"number":"73.1","isBolded":false,"associatedRows":["Acc"],"associatedColumns":["two stage training ( +1 . 8 increase ) outperforms both","Whole Pool","380700"],"associatedMergedColumns":["mixing ( +0 . 0 ) and importance weighted loss ( +0 . 7 ) ."]},{"number":"3.2)aimed","isBolded":false,"associatedRows":["two - staged training method ( Section"],"associatedColumns":["two stage training ( +1 . 8 increase ) outperforms both","Random","Size","Validation results for different training set","log","training on XS or S matches unaugmented","G - DAUG c uses a"],"associatedMergedColumns":["the XS size , but hurts performance on larger sizes ."]},{"number":"71.7","isBolded":false,"associatedRows":["Acc"],"associatedColumns":["two stage training ( +1 . 8 increase ) outperforms both","Random","127478"],"associatedMergedColumns":["mixing ( +0 . 0 ) and importance weighted loss ( +0 . 7 ) ."]}]},{"caption":"Table 6: Validation accuracy of G-DAUG c with differ-\nent labeling methods on WINOGRANDE-L and COM-\nMONSENSEQA. Random labels hurt accuracy, and \nmodel relabeling helps on WINOGRANDE but not on \nCOMMONSENSEQA. \n\n","rows":["Random relabeling","Model relabeling","Generator label","Baseline"],"columns":["WINOGRANDE - L","CSQA"],"mergedAllColumns":[],"numberCells":[{"number":"78.1","isBolded":true,"associatedRows":["Generator label"],"associatedColumns":["CSQA"],"associatedMergedColumns":[]},{"number":"77.1","isBolded":false,"associatedRows":["Baseline"],"associatedColumns":["CSQA"],"associatedMergedColumns":[]},{"number":"77.7","isBolded":false,"associatedRows":["Model relabeling"],"associatedColumns":["CSQA"],"associatedMergedColumns":[]},{"number":"77.1","isBolded":false,"associatedRows":["Random relabeling"],"associatedColumns":["CSQA"],"associatedMergedColumns":[]},{"number":"76.2","isBolded":false,"associatedRows":["Generator label"],"associatedColumns":["WINOGRANDE - L"],"associatedMergedColumns":[]},{"number":"77.7","isBolded":true,"associatedRows":["Model relabeling"],"associatedColumns":["WINOGRANDE - L"],"associatedMergedColumns":[]},{"number":"75.9","isBolded":false,"associatedRows":["Baseline"],"associatedColumns":["WINOGRANDE - L"],"associatedMergedColumns":[]},{"number":"66.8","isBolded":false,"associatedRows":["Random relabeling"],"associatedColumns":["WINOGRANDE - L"],"associatedMergedColumns":[]}]},{"caption":"Table 7: Examples and prevalence of generated commonsense questions with different manually-assigned fluency \nratings, for the COMMONSENSEQA dataset. Ratings of 3 and higher correspond to questions that are answerable \nand address common sense, and most of G-DAUG c \u0027s generated questions fall into this category. \n\n","rows":["Coherent and Fluent","Ambiguous or unanswerable","138","306","889","Nonsensical","Minor errors ( e . g . , grammar )","54"],"columns":["Rating","Pct ."],"mergedAllColumns":["He hated flying , the controls were what ?","What country does a cow go to make a milk run ?","Where does a bugle call be played ?"],"numberCells":[{"number":"9.95%","isBolded":false,"associatedRows":["Minor errors ( e . g . , grammar )","138"],"associatedColumns":["Pct ."],"associatedMergedColumns":["He hated flying , the controls were what ?"]},{"number":"2","isBolded":false,"associatedRows":[],"associatedColumns":["Rating"],"associatedMergedColumns":["What country does a cow go to make a milk run ?"]},{"number":"3.89%","isBolded":false,"associatedRows":["Nonsensical","54"],"associatedColumns":["Pct ."],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":[],"associatedColumns":["Rating"],"associatedMergedColumns":[]},{"number":"64.10%","isBolded":false,"associatedRows":["Coherent and Fluent","889"],"associatedColumns":["Pct ."],"associatedMergedColumns":["Where does a bugle call be played ?"]},{"number":"22.06%","isBolded":false,"associatedRows":["Ambiguous or unanswerable","306"],"associatedColumns":["Pct ."],"associatedMergedColumns":["What country does a cow go to make a milk run ?"]},{"number":"4","isBolded":false,"associatedRows":[],"associatedColumns":["Rating"],"associatedMergedColumns":["Where does a bugle call be played ?"]},{"number":"3","isBolded":false,"associatedRows":[],"associatedColumns":["Rating"],"associatedMergedColumns":["He hated flying , the controls were what ?"]}]},{"caption":"Table 8: Results on the validation sets of four commonsense benchmarks. All G-DAUG c methods outperform the \nbaseline methods, in particular, G-DAUG c -Influence performs the best on all tasks, which is expected as it selects \nexamples which are helpful in reducing validation loss. \n\n","rows":["ROBERTA ( ours )","ROBERTA ( reported )","G - DAUG c - Diversity","G - DAUG c - Combo","G - DAUG c - Influence","Backtranslation","G - DAUG c - Rand"],"columns":["( Acc )","WINOGRANDE","Average","HellaSwag - 2K","CODAH","Method","-","CSQA","( AUC )"],"mergedAllColumns":[],"numberCells":[{"number":"72.7","isBolded":false,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["WINOGRANDE","Method","( AUC )","-"],"associatedMergedColumns":[]},{"number":"76.2","isBolded":false,"associatedRows":["ROBERTA ( ours )"],"associatedColumns":["HellaSwag - 2K","Average","( Acc )","-"],"associatedMergedColumns":[]},{"number":"73.0","isBolded":true,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["WINOGRANDE","Method","( AUC )","-"],"associatedMergedColumns":[]},{"number":"78.2","isBolded":false,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["CSQA","Method","( Acc )","-"],"associatedMergedColumns":[]},{"number":"78.3","isBolded":true,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["HellaSwag - 2K","Average","( Acc )","-"],"associatedMergedColumns":[]},{"number":"78.3","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["HellaSwag - 2K","Average","( Acc )","-"],"associatedMergedColumns":[]},{"number":"77.2","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["HellaSwag - 2K","Average","( Acc )","-"],"associatedMergedColumns":[]},{"number":"66.6","isBolded":false,"associatedRows":["ROBERTA ( reported )"],"associatedColumns":["WINOGRANDE","Method","( AUC )"],"associatedMergedColumns":[]},{"number":"72.8","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["WINOGRANDE","Method","( AUC )","-"],"associatedMergedColumns":[]},{"number":"76.6","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["HellaSwag - 2K","Average","( Acc )","-"],"associatedMergedColumns":[]},{"number":"86.7","isBolded":false,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["CODAH","Average","( Acc )","-"],"associatedMergedColumns":[]},{"number":"78.8","isBolded":true,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["CSQA","Method","( Acc )","-"],"associatedMergedColumns":[]},{"number":"67.7","isBolded":false,"associatedRows":["Backtranslation"],"associatedColumns":["WINOGRANDE","Method","( AUC )","-"],"associatedMergedColumns":[]},{"number":"83.4","isBolded":false,"associatedRows":["Backtranslation"],"associatedColumns":["CODAH","Average","( Acc )","-"],"associatedMergedColumns":[]},{"number":"78.1","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["CSQA","Method","( Acc )","-"],"associatedMergedColumns":[]},{"number":"77.1","isBolded":false,"associatedRows":["ROBERTA ( ours )"],"associatedColumns":["CSQA","Method","( Acc )","-"],"associatedMergedColumns":[]},{"number":"86.0","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["CODAH","Average","( Acc )","-"],"associatedMergedColumns":[]},{"number":"79.3","isBolded":true,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["HellaSwag - 2K","Average","( Acc )","-"],"associatedMergedColumns":[]},{"number":"75.4","isBolded":false,"associatedRows":["Backtranslation"],"associatedColumns":["HellaSwag - 2K","Average","( Acc )","-"],"associatedMergedColumns":[]},{"number":"85.7","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["CODAH","Average","( Acc )","-"],"associatedMergedColumns":[]},{"number":"78.4","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["HellaSwag - 2K","Average","( Acc )","-"],"associatedMergedColumns":[]},{"number":"87.2","isBolded":true,"associatedRows":["G - DAUG c - Influence"],"associatedColumns":["CODAH","Average","( Acc )","-"],"associatedMergedColumns":[]},{"number":"78.1","isBolded":false,"associatedRows":["G - DAUG c - Diversity"],"associatedColumns":["CSQA","Method","( Acc )","-"],"associatedMergedColumns":[]},{"number":"72.0","isBolded":false,"associatedRows":["G - DAUG c - Rand"],"associatedColumns":["WINOGRANDE","Method","( AUC )","-"],"associatedMergedColumns":[]},{"number":"68.4","isBolded":false,"associatedRows":["ROBERTA ( ours )"],"associatedColumns":["WINOGRANDE","Method","( AUC )","-"],"associatedMergedColumns":[]},{"number":"76.4","isBolded":false,"associatedRows":["Backtranslation"],"associatedColumns":["CSQA","Method","( Acc )","-"],"associatedMergedColumns":[]},{"number":"75.2","isBolded":false,"associatedRows":["ROBERTA ( ours )"],"associatedColumns":["HellaSwag - 2K","Average","( Acc )","-"],"associatedMergedColumns":[]},{"number":"77.5","isBolded":false,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["HellaSwag - 2K","Average","( Acc )","-"],"associatedMergedColumns":[]},{"number":"78.8","isBolded":false,"associatedRows":["G - DAUG c - Combo"],"associatedColumns":["HellaSwag - 2K","Average","( Acc )","-"],"associatedMergedColumns":[]},{"number":"84.2","isBolded":false,"associatedRows":["ROBERTA ( ours )"],"associatedColumns":["CODAH","Average","( Acc )","-"],"associatedMergedColumns":[]},{"number":"78.4","isBolded":false,"associatedRows":["ROBERTA ( reported )"],"associatedColumns":["CSQA","Method","( Acc )"],"associatedMergedColumns":[]},{"number":"74.2","isBolded":false,"associatedRows":["Backtranslation"],"associatedColumns":["HellaSwag - 2K","Average","( Acc )","-"],"associatedMergedColumns":[]}]},{"caption":"Table 11: Hyperparameter settings for finetuning GPT-2. \"q/a/d\" stands for \"question/answer/distractor\". Some \nhyperparameters for WINOGRANDE is shown in a separate table as they vary with the train size. \n\n","rows":["Weight Decay","Warmup Ratio"],"columns":["16","128","62 / 128 / 128","62 / 92 / 92","90 / 120","62 / 70 / 70","72 / 72 / -"],"mergedAllColumns":[],"numberCells":[{"number":"1.0","isBolded":false,"associatedRows":["Weight Decay"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"1.0","isBolded":false,"associatedRows":["Warmup Ratio"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"1.0","isBolded":false,"associatedRows":["Weight Decay"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.01","isBolded":false,"associatedRows":["Weight Decay"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"1.0","isBolded":false,"associatedRows":["Weight Decay"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.06","isBolded":false,"associatedRows":["Warmup Ratio"],"associatedColumns":["16","62 / 128 / 128"],"associatedMergedColumns":[]},{"number":"0.06","isBolded":false,"associatedRows":["Warmup Ratio"],"associatedColumns":["16","72 / 72 / -"],"associatedMergedColumns":[]},{"number":"1.0","isBolded":false,"associatedRows":["Weight Decay"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.01","isBolded":false,"associatedRows":["Weight Decay"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.06","isBolded":false,"associatedRows":["Warmup Ratio"],"associatedColumns":["16","62 / 70 / 70"],"associatedMergedColumns":[]},{"number":"0.01","isBolded":false,"associatedRows":["Weight Decay"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.01","isBolded":false,"associatedRows":["Weight Decay"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.01","isBolded":false,"associatedRows":["Weight Decay"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.06","isBolded":false,"associatedRows":["Warmup Ratio"],"associatedColumns":["16","62 / 92 / 92"],"associatedMergedColumns":[]},{"number":"1.0","isBolded":false,"associatedRows":["Weight Decay"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.06","isBolded":false,"associatedRows":["Warmup Ratio"],"associatedColumns":["16","90 / 120"],"associatedMergedColumns":[]},{"number":"0.06","isBolded":false,"associatedRows":["Warmup Ratio"],"associatedColumns":["16","128"],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["Weight Decay"],"associatedColumns":[],"associatedMergedColumns":[]}]},{"caption":"Table 14: Hyperparameter settings for finetuning ROBERTA. Some hyperparameters for WINOGRANDE are \nshown in a separate table as they vary with the training set size. \n\n","rows":["Adam ?2","Adam ?1","Weight Decay","Warmup Ratio","Grad Clipping"],"columns":["16","Yes","Large","WINOGRANDE","AdamW","CODAH","*","ARC - Challenge","1e - 6","1e - 5","RTX 2080Ti","120","RTX 8000","3","5","90","128","HellaSwag - 2K","70","SNLI - 3K","CSQA"],"mergedAllColumns":[],"numberCells":[{"number":"0.0","isBolded":false,"associatedRows":["Grad Clipping"],"associatedColumns":["HellaSwag - 2K","Large","RTX 2080Ti","AdamW","1e - 6","Yes","1e - 5","3"],"associatedMergedColumns":[]},{"number":"0.9","isBolded":false,"associatedRows":["Adam ?1"],"associatedColumns":["WINOGRANDE","Large","RTX 2080Ti","AdamW"],"associatedMergedColumns":[]},{"number":"0.06","isBolded":false,"associatedRows":["Warmup Ratio"],"associatedColumns":["ARC - Challenge","Large","RTX 2080Ti","AdamW","1e - 6","Yes","1e - 5","5","16","120"],"associatedMergedColumns":[]},{"number":"0.9","isBolded":false,"associatedRows":["Adam ?1"],"associatedColumns":["CODAH","Large","RTX 2080Ti","AdamW"],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["Grad Clipping"],"associatedColumns":["ARC - Challenge","Large","RTX 2080Ti","AdamW","1e - 6","Yes","1e - 5","5"],"associatedMergedColumns":[]},{"number":"0.01","isBolded":false,"associatedRows":["Weight Decay"],"associatedColumns":["CODAH","Large","RTX 2080Ti","AdamW","1e - 6","Yes","1e - 5","5"],"associatedMergedColumns":[]},{"number":"0.06","isBolded":false,"associatedRows":["Warmup Ratio"],"associatedColumns":["CSQA","Large","RTX 2080Ti","AdamW","1e - 6","Yes","1e - 5","5","16","70"],"associatedMergedColumns":[]},{"number":"0.98","isBolded":false,"associatedRows":["Adam ?2"],"associatedColumns":["CODAH","Large","RTX 2080Ti","AdamW"],"associatedMergedColumns":[]},{"number":"0.9","isBolded":false,"associatedRows":["Adam ?1"],"associatedColumns":["ARC - Challenge","Large","RTX 2080Ti","AdamW"],"associatedMergedColumns":[]},{"number":"0.98","isBolded":false,"associatedRows":["Adam ?2"],"associatedColumns":["CSQA","Large","RTX 2080Ti","AdamW"],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["Grad Clipping"],"associatedColumns":["WINOGRANDE","Large","RTX 2080Ti","AdamW","1e - 6","Yes","*","*"],"associatedMergedColumns":[]},{"number":"0.01","isBolded":false,"associatedRows":["Weight Decay"],"associatedColumns":["HellaSwag - 2K","Large","RTX 2080Ti","AdamW","1e - 6","Yes","1e - 5","3"],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["Grad Clipping"],"associatedColumns":["CODAH","Large","RTX 2080Ti","AdamW","1e - 6","Yes","1e - 5","5"],"associatedMergedColumns":[]},{"number":"0.98","isBolded":false,"associatedRows":["Adam ?2"],"associatedColumns":["HellaSwag - 2K","Large","RTX 2080Ti","AdamW"],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["Grad Clipping"],"associatedColumns":["SNLI - 3K","Large","RTX 8000","AdamW","1e - 6","Yes","1e - 5","5"],"associatedMergedColumns":[]},{"number":"0.98","isBolded":false,"associatedRows":["Adam ?2"],"associatedColumns":["ARC - Challenge","Large","RTX 2080Ti","AdamW"],"associatedMergedColumns":[]},{"number":"0.01","isBolded":false,"associatedRows":["Weight Decay"],"associatedColumns":["SNLI - 3K","Large","RTX 8000","AdamW","1e - 6","Yes","1e - 5","5"],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["Grad Clipping"],"associatedColumns":["CSQA","Large","RTX 2080Ti","AdamW","1e - 6","Yes","1e - 5","5"],"associatedMergedColumns":[]},{"number":"0.06","isBolded":false,"associatedRows":["Warmup Ratio"],"associatedColumns":["HellaSwag - 2K","Large","RTX 2080Ti","AdamW","1e - 6","Yes","1e - 5","3","16","128"],"associatedMergedColumns":[]},{"number":"0.98","isBolded":false,"associatedRows":["Adam ?2"],"associatedColumns":["WINOGRANDE","Large","RTX 2080Ti","AdamW"],"associatedMergedColumns":[]},{"number":"0.9","isBolded":false,"associatedRows":["Adam ?1"],"associatedColumns":["HellaSwag - 2K","Large","RTX 2080Ti","AdamW"],"associatedMergedColumns":[]},{"number":"0.9","isBolded":false,"associatedRows":["Adam ?1"],"associatedColumns":["CSQA","Large","RTX 2080Ti","AdamW"],"associatedMergedColumns":[]},{"number":"0.06","isBolded":false,"associatedRows":["Warmup Ratio"],"associatedColumns":["CODAH","Large","RTX 2080Ti","AdamW","1e - 6","Yes","1e - 5","5","16","90"],"associatedMergedColumns":[]},{"number":"0.9","isBolded":false,"associatedRows":["Adam ?1"],"associatedColumns":["SNLI - 3K","Large","RTX 8000","AdamW"],"associatedMergedColumns":[]},{"number":"0.01","isBolded":false,"associatedRows":["Weight Decay"],"associatedColumns":["WINOGRANDE","Large","RTX 2080Ti","AdamW","1e - 6","Yes","*","*"],"associatedMergedColumns":[]},{"number":"0.06","isBolded":false,"associatedRows":["Warmup Ratio"],"associatedColumns":["WINOGRANDE","Large","RTX 2080Ti","AdamW","1e - 6","Yes","*","*","16","70"],"associatedMergedColumns":[]},{"number":"0.01","isBolded":false,"associatedRows":["Weight Decay"],"associatedColumns":["ARC - Challenge","Large","RTX 2080Ti","AdamW","1e - 6","Yes","1e - 5","5"],"associatedMergedColumns":[]},{"number":"0.01","isBolded":false,"associatedRows":["Weight Decay"],"associatedColumns":["CSQA","Large","RTX 2080Ti","AdamW","1e - 6","Yes","1e - 5","5"],"associatedMergedColumns":[]},{"number":"0.98","isBolded":false,"associatedRows":["Adam ?2"],"associatedColumns":["SNLI - 3K","Large","RTX 8000","AdamW"],"associatedMergedColumns":[]},{"number":"0.06","isBolded":false,"associatedRows":["Warmup Ratio"],"associatedColumns":["SNLI - 3K","Large","RTX 8000","AdamW","1e - 6","Yes","1e - 5","5","16","128"],"associatedMergedColumns":[]}]},{"caption":"Table 15: Hyperparameter settings for finetuning ROBERTA on WINOGRANDE. \n\n","rows":["Epochs","10"],"columns":["S","XL","L","1e - 5","M"],"mergedAllColumns":[],"numberCells":[{"number":"8","isBolded":false,"associatedRows":["Epochs","10"],"associatedColumns":["S","1e - 5"],"associatedMergedColumns":[]},{"number":"5","isBolded":false,"associatedRows":["Epochs","10"],"associatedColumns":["XL","1e - 5"],"associatedMergedColumns":[]},{"number":"5","isBolded":false,"associatedRows":["Epochs","10"],"associatedColumns":["M","1e - 5"],"associatedMergedColumns":[]},{"number":"5","isBolded":false,"associatedRows":["Epochs","10"],"associatedColumns":["L","1e - 5"],"associatedMergedColumns":[]}]}]