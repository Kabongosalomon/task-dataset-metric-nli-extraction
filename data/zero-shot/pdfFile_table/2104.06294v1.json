[{"caption":"Median \nMean # Frames \n\n50.0% 1331.7% 4094.4% \n2000M \n95.0% 1006.4% 2856.2% \n200M \n99.5% 126.6% \n450.6% \n20M \n\nTable 1. Reanalyse scaling in Atari. Mean and median human \nnormalised scores over 57 Atari games, at different Reanalyse \nfractions. All other parameters are held constant. Varying the \nReanalyse fraction alone is enough to learn efficiently at data \nbudgets differing by orders of magnitude. \n\n","rows":[],"columns":["Mean","Median"],"mergedAllColumns":["2000M","200M"],"numberCells":[{"number":"2856.2%","isBolded":false,"associatedRows":[],"associatedColumns":["Mean"],"associatedMergedColumns":["2000M"]},{"number":"99.5%","isBolded":false,"associatedRows":[],"associatedColumns":["Median"],"associatedMergedColumns":["200M"]},{"number":"50.0%","isBolded":false,"associatedRows":[],"associatedColumns":["Median"],"associatedMergedColumns":[]},{"number":"1006.4%","isBolded":false,"associatedRows":[],"associatedColumns":["Median"],"associatedMergedColumns":["2000M"]},{"number":"95.0%","isBolded":false,"associatedRows":[],"associatedColumns":["Median"],"associatedMergedColumns":["2000M"]},{"number":"450.6%","isBolded":false,"associatedRows":[],"associatedColumns":["Mean"],"associatedMergedColumns":["200M"]},{"number":"126.6%","isBolded":false,"associatedRows":[],"associatedColumns":["Median"],"associatedMergedColumns":["200M"]},{"number":"1331.7%","isBolded":false,"associatedRows":[],"associatedColumns":["Median"],"associatedMergedColumns":[]},{"number":"4094.4%","isBolded":false,"associatedRows":[],"associatedColumns":["Mean"],"associatedMergedColumns":[]}]},{"caption":"Game \nQR-DQN \nREM CQL(H) \nMZ \n\nasterix (1%) \n166.3 \n386.5 \n592.4 27220.5 \nbreakout \n7.9 \n11.0 \n61.1 \n251.9 \npong \n-13.8 \n-6.9 \n19.3 \n-16.2 \nqbert \n383.6 \n343.4 14012.0 \n6953.2 \nseaquest \n672.9 \n499.8 \n779.4 \n4964.0 \n\nasterix (10%) 189.2 \n75.1 \n156.3 40554.0 \nbreakout \n151.2 \n86.7 \n269.3 \n485.8 \npong \n15.1 \n8.9 \n18.5 \n15.6 \nqbert \n7091.3 8624.3 13855.6 16817.9 \nseaquest \n2984.8 3936.6 \n3674.1 \n8556.3 \n\nTable 3. Low-data Atari setting. QR-DQN (Dabney et al., 2018), \nREM (Agarwal et al., 2020), CQL(H) (Kumar et al., 2020) and \nMuZero Unplugged results when trained on only 1% (top, 2 million \nframes) or 10% (bottom, 20 million frames) of Atari data. MuZero \nUnplugged performance improves consistently when trained on \nmore data. \n\n","rows":["qbert","seaquest","asterix ( 10% )","asterix ( 1% )","breakout","pong"],"columns":["CQL ( H )","REM","QR - DQN","MZ"],"mergedAllColumns":[],"numberCells":[{"number":"86.7","isBolded":false,"associatedRows":["breakout"],"associatedColumns":["REM"],"associatedMergedColumns":[]},{"number":"269.3","isBolded":false,"associatedRows":["breakout"],"associatedColumns":["CQL ( H )"],"associatedMergedColumns":[]},{"number":"61.1","isBolded":false,"associatedRows":["breakout"],"associatedColumns":["CQL ( H )"],"associatedMergedColumns":[]},{"number":"8556.3","isBolded":true,"associatedRows":["seaquest"],"associatedColumns":["MZ"],"associatedMergedColumns":[]},{"number":"499.8","isBolded":false,"associatedRows":["seaquest"],"associatedColumns":["REM"],"associatedMergedColumns":[]},{"number":"7091.3","isBolded":false,"associatedRows":["qbert"],"associatedColumns":["QR - DQN"],"associatedMergedColumns":[]},{"number":"14012.0","isBolded":true,"associatedRows":["qbert"],"associatedColumns":["CQL ( H )"],"associatedMergedColumns":[]},{"number":"7.9","isBolded":false,"associatedRows":["breakout"],"associatedColumns":["QR - DQN"],"associatedMergedColumns":[]},{"number":"343.4","isBolded":false,"associatedRows":["qbert"],"associatedColumns":["REM"],"associatedMergedColumns":[]},{"number":"75.1","isBolded":false,"associatedRows":["asterix ( 10% )"],"associatedColumns":["REM"],"associatedMergedColumns":[]},{"number":"18.5","isBolded":true,"associatedRows":["pong"],"associatedColumns":["CQL ( H )"],"associatedMergedColumns":[]},{"number":"3936.6","isBolded":false,"associatedRows":["seaquest"],"associatedColumns":["REM"],"associatedMergedColumns":[]},{"number":"8624.3","isBolded":false,"associatedRows":["qbert"],"associatedColumns":["REM"],"associatedMergedColumns":[]},{"number":"15.1","isBolded":false,"associatedRows":["pong"],"associatedColumns":["QR - DQN"],"associatedMergedColumns":[]},{"number":"-13.8","isBolded":false,"associatedRows":["pong"],"associatedColumns":["QR - DQN"],"associatedMergedColumns":[]},{"number":"2984.8","isBolded":false,"associatedRows":["seaquest"],"associatedColumns":["QR - DQN"],"associatedMergedColumns":[]},{"number":"156.3","isBolded":false,"associatedRows":["asterix ( 10% )"],"associatedColumns":["CQL ( H )"],"associatedMergedColumns":[]},{"number":"251.9","isBolded":true,"associatedRows":["breakout"],"associatedColumns":["MZ"],"associatedMergedColumns":[]},{"number":"485.8","isBolded":true,"associatedRows":["breakout"],"associatedColumns":["MZ"],"associatedMergedColumns":[]},{"number":"383.6","isBolded":false,"associatedRows":["qbert"],"associatedColumns":["QR - DQN"],"associatedMergedColumns":[]},{"number":"166.3","isBolded":false,"associatedRows":["asterix ( 1% )"],"associatedColumns":["QR - DQN"],"associatedMergedColumns":[]},{"number":"779.4","isBolded":false,"associatedRows":["seaquest"],"associatedColumns":["CQL ( H )"],"associatedMergedColumns":[]},{"number":"6953.2","isBolded":false,"associatedRows":["qbert"],"associatedColumns":["MZ"],"associatedMergedColumns":[]},{"number":"40554.0","isBolded":true,"associatedRows":["asterix ( 10% )"],"associatedColumns":["MZ"],"associatedMergedColumns":[]},{"number":"8.9","isBolded":false,"associatedRows":["pong"],"associatedColumns":["REM"],"associatedMergedColumns":[]},{"number":"16817.9","isBolded":true,"associatedRows":["qbert"],"associatedColumns":["MZ"],"associatedMergedColumns":[]},{"number":"151.2","isBolded":false,"associatedRows":["breakout"],"associatedColumns":["QR - DQN"],"associatedMergedColumns":[]},{"number":"592.4","isBolded":false,"associatedRows":["asterix ( 1% )"],"associatedColumns":["CQL ( H )"],"associatedMergedColumns":[]},{"number":"-6.9","isBolded":false,"associatedRows":["pong"],"associatedColumns":["REM"],"associatedMergedColumns":[]},{"number":"15.6","isBolded":false,"associatedRows":["pong"],"associatedColumns":["MZ"],"associatedMergedColumns":[]},{"number":"672.9","isBolded":false,"associatedRows":["seaquest"],"associatedColumns":["QR - DQN"],"associatedMergedColumns":[]},{"number":"189.2","isBolded":false,"associatedRows":["asterix ( 10% )"],"associatedColumns":["QR - DQN"],"associatedMergedColumns":[]},{"number":"13855.6","isBolded":false,"associatedRows":["qbert"],"associatedColumns":["CQL ( H )"],"associatedMergedColumns":[]},{"number":"3674.1","isBolded":false,"associatedRows":["seaquest"],"associatedColumns":["CQL ( H )"],"associatedMergedColumns":[]},{"number":"27220.5","isBolded":true,"associatedRows":["asterix ( 1% )"],"associatedColumns":["MZ"],"associatedMergedColumns":[]},{"number":"19.3","isBolded":true,"associatedRows":["pong"],"associatedColumns":["CQL ( H )"],"associatedMergedColumns":[]},{"number":"386.5","isBolded":false,"associatedRows":["asterix ( 1% )"],"associatedColumns":["REM"],"associatedMergedColumns":[]},{"number":"-16.2","isBolded":false,"associatedRows":["pong"],"associatedColumns":["MZ"],"associatedMergedColumns":[]},{"number":"4964.0","isBolded":true,"associatedRows":["seaquest"],"associatedColumns":["MZ"],"associatedMergedColumns":[]},{"number":"11.0","isBolded":false,"associatedRows":["breakout"],"associatedColumns":["REM"],"associatedMergedColumns":[]}]},{"caption":"Loss \nsupervised \nCRR Reanalyse \nUnroll \n0 \n1 \n5 \n5 \n5 \n\npolicy 60.6 \n61.4 \n54.0 155.6 \n203.2 \nvalue \n92.2 105.0 159.2 153.0 \n239.9 \nMCTS \n-\n137.3 169.7 172.5 \n265.3 \n\nTable 4. Median score in RL Unplugged Atari: ablations of \naction selection and training loss. Median normalized scores \nover the 46 Atari games from RL Unplugged. \nRows of the table correspond to different action selection methods: \nsampling according to the policy probabilities, selecting the action \nwith the highest value or selecting according to MCTS visit counts. \nColumns correspond to different number of unroll steps of the \nMuZero learned model and different losses. The leftmost three \ncolumns use the action from the training data as a supervised policy \ntarget, the rightmost two columns use the CRR and the Reanalyse \nloss respectively. For the case of 0 unroll steps, an action-value \nhead is used to predict action values, instead of the state-value \npredicted by the normal model. All columns use a 5-step TD \nbootstrap towards a target network as the value target. \nFor all action selection methods, Reanalyse loss led to the best \nperformance; for all losses, MCTS action selection also led to \nthe best performance. Overall, the combination of MCTS action \nselection and Reanalyse loss -the MuZero Unplugged algorithm -\nled to the best results. \n\n","rows":["MCTS","value","-","policy"],"columns":["0","1","supervised","CRR","5","Reanalyse"],"mergedAllColumns":[],"numberCells":[{"number":"159.2","isBolded":false,"associatedRows":["value"],"associatedColumns":["supervised","5"],"associatedMergedColumns":[]},{"number":"137.3","isBolded":false,"associatedRows":["MCTS","-"],"associatedColumns":["supervised","1"],"associatedMergedColumns":[]},{"number":"54.0","isBolded":false,"associatedRows":["policy"],"associatedColumns":["supervised","5"],"associatedMergedColumns":[]},{"number":"265.3","isBolded":false,"associatedRows":["MCTS","-"],"associatedColumns":["Reanalyse","5"],"associatedMergedColumns":[]},{"number":"61.4","isBolded":false,"associatedRows":["policy"],"associatedColumns":["supervised","1"],"associatedMergedColumns":[]},{"number":"172.5","isBolded":false,"associatedRows":["MCTS","-"],"associatedColumns":["CRR","5"],"associatedMergedColumns":[]},{"number":"92.2","isBolded":false,"associatedRows":["value"],"associatedColumns":["supervised","0"],"associatedMergedColumns":[]},{"number":"155.6","isBolded":false,"associatedRows":["policy"],"associatedColumns":["CRR","5"],"associatedMergedColumns":[]},{"number":"60.6","isBolded":false,"associatedRows":["policy"],"associatedColumns":["supervised","0"],"associatedMergedColumns":[]},{"number":"169.7","isBolded":false,"associatedRows":["MCTS","-"],"associatedColumns":["supervised","5"],"associatedMergedColumns":[]},{"number":"105.0","isBolded":false,"associatedRows":["value"],"associatedColumns":["supervised","1"],"associatedMergedColumns":[]},{"number":"153.0","isBolded":false,"associatedRows":["value"],"associatedColumns":["CRR","5"],"associatedMergedColumns":[]},{"number":"203.2","isBolded":false,"associatedRows":["policy"],"associatedColumns":["Reanalyse","5"],"associatedMergedColumns":[]},{"number":"239.9","isBolded":false,"associatedRows":["value"],"associatedColumns":["Reanalyse","5"],"associatedMergedColumns":[]}]},{"caption":"Task \n# dims # episodes \nBC D4PG BRAC RABM \nBC Unplugged \n\ncartpole.swingup \n1 \n40 386.0 856.0 \n869.0 \n798.0 143.7 \n343.3 \nfinger.turn hard \n2 \n500 238.0 714.0 \n227.0 \n433.0 308.8 \n405.0 \nfish.swim \n5 \n200 444.0 180.0 \n222.0 \n504.0 542.8 \n585.4 \nmanipulator.insert ball \n5 \n1500 385.0 154.0 \n55.6 \n409.0 412.7 \n557.0 \nmanipulator.insert peg \n5 \n1500 279.0 \n50.4 \n49.5 \n290.0 309.9 \n432.7 \nwalker.stand \n6 \n200 386.0 930.0 \n829.0 \n689.0 444.4 \n759.8 \nwalker.walk \n6 \n200 380.0 549.0 \n786.0 \n651.0 496.3 \n901.5 \ncheetah.run \n6 \n300 408.0 308.0 \n539.0 \n304.0 592.9 \n798.9 \nhumanoid.run \n21 \n3000 382.0 \n1.7 \n9.6 \n303.0 408.5 \n633.4 \n\nmean \n365.3 415.9 \n398.5 \n486.8 406.7 \n601.9 \n\nTable 5. Results for DM Control benchmark from RL Unplugged. Mean final score on 9 DM Control tasks, as well as mean score \nacross all tasks. First three columns indicate task, action dimensonality and dataset size, subsequent four columns reproduce baseline \nresults from (Gulcehre et al., 2020). Final columns show performance of Behaviour Cloning (BC) with the MuZero network and \nresults for MuZero Unplugged. As the data sets for the DM Control tasks are very small and vary a hundredfold between tasks, to \nkeep the number of model parameters per datapoint constant and prevent memorisation, we scaled the neural network according to \n\nchannels \u003d datapoints \nlayers . \n\n","rows":["humanoid . run","3000","walker . walk","1","manipulator . insert peg","2","200","1500","300","fish . swim","finger . turn hard","500","5","cheetah . run","6","mean","manipulator . insert ball","walker . stand","cartpole . swingup","40","21"],"columns":["BC","D4PG","RABM","BRAC","Unplugged"],"mergedAllColumns":[],"numberCells":[{"number":"308.0","isBolded":false,"associatedRows":["cheetah . run","6","300"],"associatedColumns":["D4PG"],"associatedMergedColumns":[]},{"number":"408.0","isBolded":false,"associatedRows":["cheetah . run","6","300"],"associatedColumns":["BC"],"associatedMergedColumns":[]},{"number":"303.0","isBolded":false,"associatedRows":["humanoid . run","21","3000"],"associatedColumns":["RABM"],"associatedMergedColumns":[]},{"number":"539.0","isBolded":false,"associatedRows":["cheetah . run","6","300"],"associatedColumns":["BRAC"],"associatedMergedColumns":[]},{"number":"415.9","isBolded":false,"associatedRows":["mean","21","3000"],"associatedColumns":["D4PG"],"associatedMergedColumns":[]},{"number":"496.3","isBolded":false,"associatedRows":["walker . walk","6","200"],"associatedColumns":["BC"],"associatedMergedColumns":[]},{"number":"380.0","isBolded":false,"associatedRows":["walker . walk","6","200"],"associatedColumns":["BC"],"associatedMergedColumns":[]},{"number":"542.8","isBolded":false,"associatedRows":["fish . swim","5","1500","200"],"associatedColumns":["BC"],"associatedMergedColumns":[]},{"number":"444.0","isBolded":false,"associatedRows":["fish . swim","5","1500","200"],"associatedColumns":["BC"],"associatedMergedColumns":[]},{"number":"309.9","isBolded":false,"associatedRows":["manipulator . insert peg","5","1500","200"],"associatedColumns":["BC"],"associatedMergedColumns":[]},{"number":"786.0","isBolded":false,"associatedRows":["walker . walk","6","200"],"associatedColumns":["BRAC"],"associatedMergedColumns":[]},{"number":"689.0","isBolded":false,"associatedRows":["walker . stand","6","200"],"associatedColumns":["RABM"],"associatedMergedColumns":[]},{"number":"869.0","isBolded":true,"associatedRows":["cartpole . swingup","1","1500","40"],"associatedColumns":["BRAC"],"associatedMergedColumns":[]},{"number":"633.4","isBolded":true,"associatedRows":["humanoid . run","21","3000"],"associatedColumns":["Unplugged"],"associatedMergedColumns":[]},{"number":"408.5","isBolded":false,"associatedRows":["humanoid . run","21","3000"],"associatedColumns":["BC"],"associatedMergedColumns":[]},{"number":"585.4","isBolded":true,"associatedRows":["fish . swim","5","1500","200"],"associatedColumns":["Unplugged"],"associatedMergedColumns":[]},{"number":"1.7","isBolded":false,"associatedRows":["humanoid . run","21","3000"],"associatedColumns":["D4PG"],"associatedMergedColumns":[]},{"number":"409.0","isBolded":false,"associatedRows":["manipulator . insert ball","5","1500","200"],"associatedColumns":["RABM"],"associatedMergedColumns":[]},{"number":"433.0","isBolded":false,"associatedRows":["finger . turn hard","2","1500","500"],"associatedColumns":["RABM"],"associatedMergedColumns":[]},{"number":"557.0","isBolded":true,"associatedRows":["manipulator . insert ball","5","1500","200"],"associatedColumns":["Unplugged"],"associatedMergedColumns":[]},{"number":"856.0","isBolded":false,"associatedRows":["cartpole . swingup","1","1500","40"],"associatedColumns":["D4PG"],"associatedMergedColumns":[]},{"number":"365.3","isBolded":false,"associatedRows":["mean","21","3000"],"associatedColumns":["BC"],"associatedMergedColumns":[]},{"number":"290.0","isBolded":false,"associatedRows":["manipulator . insert peg","5","1500","200"],"associatedColumns":["RABM"],"associatedMergedColumns":[]},{"number":"398.5","isBolded":false,"associatedRows":["mean","21","3000"],"associatedColumns":["BRAC"],"associatedMergedColumns":[]},{"number":"405.0","isBolded":false,"associatedRows":["finger . turn hard","2","1500","500"],"associatedColumns":["Unplugged"],"associatedMergedColumns":[]},{"number":"432.7","isBolded":true,"associatedRows":["manipulator . insert peg","5","1500","200"],"associatedColumns":["Unplugged"],"associatedMergedColumns":[]},{"number":"238.0","isBolded":false,"associatedRows":["finger . turn hard","2","1500","500"],"associatedColumns":["BC"],"associatedMergedColumns":[]},{"number":"714.0","isBolded":true,"associatedRows":["finger . turn hard","2","1500","500"],"associatedColumns":["D4PG"],"associatedMergedColumns":[]},{"number":"343.3","isBolded":false,"associatedRows":["cartpole . swingup","1","1500","40"],"associatedColumns":["Unplugged"],"associatedMergedColumns":[]},{"number":"9.6","isBolded":false,"associatedRows":["humanoid . run","21","3000"],"associatedColumns":["BRAC"],"associatedMergedColumns":[]},{"number":"486.8","isBolded":false,"associatedRows":["mean","21","3000"],"associatedColumns":["RABM"],"associatedMergedColumns":[]},{"number":"55.6","isBolded":false,"associatedRows":["manipulator . insert ball","5","1500","200"],"associatedColumns":["BRAC"],"associatedMergedColumns":[]},{"number":"49.5","isBolded":false,"associatedRows":["manipulator . insert peg","5","1500","200"],"associatedColumns":["BRAC"],"associatedMergedColumns":[]},{"number":"601.9","isBolded":true,"associatedRows":["mean","21","3000"],"associatedColumns":["Unplugged"],"associatedMergedColumns":[]},{"number":"504.0","isBolded":false,"associatedRows":["fish . swim","5","1500","200"],"associatedColumns":["RABM"],"associatedMergedColumns":[]},{"number":"308.8","isBolded":false,"associatedRows":["finger . turn hard","2","1500","500"],"associatedColumns":["BC"],"associatedMergedColumns":[]},{"number":"829.0","isBolded":false,"associatedRows":["walker . stand","6","200"],"associatedColumns":["BRAC"],"associatedMergedColumns":[]},{"number":"406.7","isBolded":false,"associatedRows":["mean","21","3000"],"associatedColumns":["BC"],"associatedMergedColumns":[]},{"number":"386.0","isBolded":false,"associatedRows":["walker . stand","6","200"],"associatedColumns":["BC"],"associatedMergedColumns":[]},{"number":"279.0","isBolded":false,"associatedRows":["manipulator . insert peg","5","1500","200"],"associatedColumns":["BC"],"associatedMergedColumns":[]},{"number":"143.7","isBolded":false,"associatedRows":["cartpole . swingup","1","1500","40"],"associatedColumns":["BC"],"associatedMergedColumns":[]},{"number":"651.0","isBolded":false,"associatedRows":["walker . walk","6","200"],"associatedColumns":["RABM"],"associatedMergedColumns":[]},{"number":"798.9","isBolded":true,"associatedRows":["cheetah . run","6","300"],"associatedColumns":["Unplugged"],"associatedMergedColumns":[]},{"number":"180.0","isBolded":false,"associatedRows":["fish . swim","5","1500","200"],"associatedColumns":["D4PG"],"associatedMergedColumns":[]},{"number":"227.0","isBolded":false,"associatedRows":["finger . turn hard","2","1500","500"],"associatedColumns":["BRAC"],"associatedMergedColumns":[]},{"number":"304.0","isBolded":false,"associatedRows":["cheetah . run","6","300"],"associatedColumns":["RABM"],"associatedMergedColumns":[]},{"number":"798.0","isBolded":false,"associatedRows":["cartpole . swingup","1","1500","40"],"associatedColumns":["RABM"],"associatedMergedColumns":[]},{"number":"901.5","isBolded":true,"associatedRows":["walker . walk","6","200"],"associatedColumns":["Unplugged"],"associatedMergedColumns":[]},{"number":"385.0","isBolded":false,"associatedRows":["manipulator . insert ball","5","1500","200"],"associatedColumns":["BC"],"associatedMergedColumns":[]},{"number":"759.8","isBolded":false,"associatedRows":["walker . stand","6","200"],"associatedColumns":["Unplugged"],"associatedMergedColumns":[]},{"number":"382.0","isBolded":false,"associatedRows":["humanoid . run","21","3000"],"associatedColumns":["BC"],"associatedMergedColumns":[]},{"number":"592.9","isBolded":false,"associatedRows":["cheetah . run","6","300"],"associatedColumns":["BC"],"associatedMergedColumns":[]},{"number":"222.0","isBolded":false,"associatedRows":["fish . swim","5","1500","200"],"associatedColumns":["BRAC"],"associatedMergedColumns":[]},{"number":"386.0","isBolded":false,"associatedRows":["cartpole . swingup","1","1500","40"],"associatedColumns":["BC"],"associatedMergedColumns":[]},{"number":"154.0","isBolded":false,"associatedRows":["manipulator . insert ball","5","1500","200"],"associatedColumns":["D4PG"],"associatedMergedColumns":[]},{"number":"444.4","isBolded":false,"associatedRows":["walker . stand","6","200"],"associatedColumns":["BC"],"associatedMergedColumns":[]},{"number":"549.0","isBolded":false,"associatedRows":["walker . walk","6","200"],"associatedColumns":["D4PG"],"associatedMergedColumns":[]},{"number":"50.4","isBolded":false,"associatedRows":["manipulator . insert peg","5","1500","200"],"associatedColumns":["D4PG"],"associatedMergedColumns":[]},{"number":"412.7","isBolded":false,"associatedRows":["manipulator . insert ball","5","1500","200"],"associatedColumns":["BC"],"associatedMergedColumns":[]},{"number":"930.0","isBolded":true,"associatedRows":["walker . stand","6","200"],"associatedColumns":["D4PG"],"associatedMergedColumns":[]}]},{"caption":"MuZero \nTask \nCRR \nBC Unplugged \n\ncartpole.swingup \n664.0 501.8 \n594.3 \nfinger.turn hard \n714.0 333.8 \n759.0 \nfish.swim \n517.0 556.8 \n681.6 \nmanipulator.insert ball 625.0 465.6 \n659.2 \nmanipulator.insert peg 387.0 325.9 \n556.0 \nwalker.stand \n797.0 473.3 \n887.2 \nwalker.walk \n901.0 637.9 \n949.5 \ncheetah.run \n577.0 765.3 \n869.9 \nhumanoid.run \n586.0 416.5 \n643.1 \n\nmean \n640.9 497.4 \n733.3 \n\nTable 6. Comparison of MuZero Unplugged and CRR. Results \nfor CRR (Wang et al., 2020) were reported by selecting the check-\npoint with the highest mean reward from each training run. Since \nthis does not follow the offline policy selection guidelines from RL \nUnplugged and is therefore not directly comparable to the baseline \nresults, we compared to it separately. The same highest mean \nreward evaluation scheme as used in CRR was used for MuZero \nUnplugged results in this table as well. All other tables report \nresults at the end of training. \n\n","rows":["manipulator . insert peg","fish . swim","finger . turn hard","cheetah . run","mean","manipulator . insert ball","walker . stand","humanoid . run","cartpole . swingup","walker . walk"],"columns":["MuZero","BC","CRR","Unplugged"],"mergedAllColumns":[],"numberCells":[{"number":"586.0","isBolded":false,"associatedRows":["humanoid . run"],"associatedColumns":["MuZero","CRR"],"associatedMergedColumns":[]},{"number":"640.9","isBolded":false,"associatedRows":["mean"],"associatedColumns":["MuZero","CRR"],"associatedMergedColumns":[]},{"number":"869.9","isBolded":true,"associatedRows":["cheetah . run"],"associatedColumns":["MuZero","Unplugged"],"associatedMergedColumns":[]},{"number":"556.8","isBolded":false,"associatedRows":["fish . swim"],"associatedColumns":["MuZero","BC"],"associatedMergedColumns":[]},{"number":"733.3","isBolded":true,"associatedRows":["mean"],"associatedColumns":["MuZero","Unplugged"],"associatedMergedColumns":[]},{"number":"664.0","isBolded":true,"associatedRows":["cartpole . swingup"],"associatedColumns":["MuZero","CRR"],"associatedMergedColumns":[]},{"number":"501.8","isBolded":false,"associatedRows":["cartpole . swingup"],"associatedColumns":["MuZero","BC"],"associatedMergedColumns":[]},{"number":"901.0","isBolded":false,"associatedRows":["walker . walk"],"associatedColumns":["MuZero","CRR"],"associatedMergedColumns":[]},{"number":"517.0","isBolded":false,"associatedRows":["fish . swim"],"associatedColumns":["MuZero","CRR"],"associatedMergedColumns":[]},{"number":"594.3","isBolded":false,"associatedRows":["cartpole . swingup"],"associatedColumns":["MuZero","Unplugged"],"associatedMergedColumns":[]},{"number":"659.2","isBolded":true,"associatedRows":["manipulator . insert ball"],"associatedColumns":["MuZero","Unplugged"],"associatedMergedColumns":[]},{"number":"643.1","isBolded":true,"associatedRows":["humanoid . run"],"associatedColumns":["MuZero","Unplugged"],"associatedMergedColumns":[]},{"number":"625.0","isBolded":false,"associatedRows":["manipulator . insert ball"],"associatedColumns":["MuZero","CRR"],"associatedMergedColumns":[]},{"number":"325.9","isBolded":false,"associatedRows":["manipulator . insert peg"],"associatedColumns":["MuZero","BC"],"associatedMergedColumns":[]},{"number":"556.0","isBolded":true,"associatedRows":["manipulator . insert peg"],"associatedColumns":["MuZero","Unplugged"],"associatedMergedColumns":[]},{"number":"949.5","isBolded":true,"associatedRows":["walker . walk"],"associatedColumns":["MuZero","Unplugged"],"associatedMergedColumns":[]},{"number":"465.6","isBolded":false,"associatedRows":["manipulator . insert ball"],"associatedColumns":["MuZero","BC"],"associatedMergedColumns":[]},{"number":"714.0","isBolded":false,"associatedRows":["finger . turn hard"],"associatedColumns":["MuZero","CRR"],"associatedMergedColumns":[]},{"number":"577.0","isBolded":false,"associatedRows":["cheetah . run"],"associatedColumns":["MuZero","CRR"],"associatedMergedColumns":[]},{"number":"637.9","isBolded":false,"associatedRows":["walker . walk"],"associatedColumns":["MuZero","BC"],"associatedMergedColumns":[]},{"number":"797.0","isBolded":false,"associatedRows":["walker . stand"],"associatedColumns":["MuZero","CRR"],"associatedMergedColumns":[]},{"number":"473.3","isBolded":false,"associatedRows":["walker . stand"],"associatedColumns":["MuZero","BC"],"associatedMergedColumns":[]},{"number":"387.0","isBolded":false,"associatedRows":["manipulator . insert peg"],"associatedColumns":["MuZero","CRR"],"associatedMergedColumns":[]},{"number":"681.6","isBolded":true,"associatedRows":["fish . swim"],"associatedColumns":["MuZero","Unplugged"],"associatedMergedColumns":[]},{"number":"333.8","isBolded":false,"associatedRows":["finger . turn hard"],"associatedColumns":["MuZero","BC"],"associatedMergedColumns":[]},{"number":"416.5","isBolded":false,"associatedRows":["humanoid . run"],"associatedColumns":["MuZero","BC"],"associatedMergedColumns":[]},{"number":"759.0","isBolded":true,"associatedRows":["finger . turn hard"],"associatedColumns":["MuZero","Unplugged"],"associatedMergedColumns":[]},{"number":"765.3","isBolded":false,"associatedRows":["cheetah . run"],"associatedColumns":["MuZero","BC"],"associatedMergedColumns":[]},{"number":"887.2","isBolded":true,"associatedRows":["walker . stand"],"associatedColumns":["MuZero","Unplugged"],"associatedMergedColumns":[]},{"number":"497.4","isBolded":false,"associatedRows":["mean"],"associatedColumns":["MuZero","BC"],"associatedMergedColumns":[]}]},{"caption":"Algorithm \nMedian \nMean \nFrames \n\nIMPALA 1 \n191.8% \n957.6% \n200M \nRainbow 2 \n231.1% \n-\n200M \nUNREAL 3 a \n250.0% a \n880% a \n250M \nLASER 4 \n431.0% \n-\n200M \nMuZero 5 \n741.7% \n2183.6% \n200M \n\nMuZero sticky \n692.9% \n2188.4% \n200M \nMuZero Res2 Adam 1006.4% 2856.2% \n200M \n\nTable 7. MuZero improvements in Atari. Mean and median \nhuman normalized scores over 57 Atari games, best results are \nhighlighted in bold. For per game scores, see Table 12. Top of \nthe table shows results for previously published work, bottom \nrow show results for our experiments. Using sticky actions does \nnot significantly hurt performance, and switching to ResNet v2 \nstyle pre-activation residual blocks with Layer Normalisation and \ntraining with Adam significantly improves performance. \n1 ","rows":["1","Rainbow 2","3 a","4","MuZero Res2 Adam","5","MuZero sticky","IMPALA"],"columns":["Mean","Median","880% a","-"],"mergedAllColumns":["200M"],"numberCells":[{"number":"957.6%","isBolded":false,"associatedRows":["MuZero Res2 Adam","1"],"associatedColumns":["Mean"],"associatedMergedColumns":[]},{"number":"2856.2%","isBolded":true,"associatedRows":["MuZero Res2 Adam"],"associatedColumns":["Mean","-","880% a","-"],"associatedMergedColumns":["200M"]},{"number":"741.7%","isBolded":false,"associatedRows":["MuZero Res2 Adam","5"],"associatedColumns":["Median","-","880% a","-"],"associatedMergedColumns":["200M"]},{"number":"692.9%","isBolded":false,"associatedRows":["MuZero sticky"],"associatedColumns":["Median","-","880% a","-"],"associatedMergedColumns":["200M"]},{"number":"231.1%","isBolded":false,"associatedRows":["Rainbow 2","1","3 a"],"associatedColumns":["Median"],"associatedMergedColumns":["200M"]},{"number":"1006.4%","isBolded":true,"associatedRows":["MuZero Res2 Adam"],"associatedColumns":["Median","-","880% a","-"],"associatedMergedColumns":["200M"]},{"number":"191.8%","isBolded":false,"associatedRows":["IMPALA","1","3 a"],"associatedColumns":["Median"],"associatedMergedColumns":[]},{"number":"2183.6%","isBolded":false,"associatedRows":["MuZero Res2 Adam","5"],"associatedColumns":["Mean","-","880% a","-"],"associatedMergedColumns":["200M"]},{"number":"2188.4%","isBolded":false,"associatedRows":["MuZero sticky"],"associatedColumns":["Mean","-","880% a","-"],"associatedMergedColumns":["200M"]},{"number":"250.0%a","isBolded":false,"associatedRows":["MuZero Res2 Adam","3 a"],"associatedColumns":["Median","-"],"associatedMergedColumns":["200M"]},{"number":"431.0%","isBolded":false,"associatedRows":["MuZero Res2 Adam","4"],"associatedColumns":["Median","-","880% a"],"associatedMergedColumns":["200M"]}]},{"caption":"Loss \nsupervised \nCRR Reanalyse \nUnroll \n0 \n1 \n5 \n5 \n5 \n\npolicy \n50.7 \n49.6 \n46.9 271.2 \n433.0 \nvalue \n143.2 151.2 359.4 346.7 \n549.7 \nMCTS \n-\n248.4 394.8 408.3 \n595.5 \n\nTable 9. Mean score in RL Unplugged Atari: ablations of ac-\ntion selection and training loss. As Table 4, but showing mean \nnormalized score instead of median normalized score. \n\n","rows":["MCTS","value","-","policy"],"columns":["0","1","supervised","CRR","5","Reanalyse"],"mergedAllColumns":[],"numberCells":[{"number":"271.2","isBolded":false,"associatedRows":["policy"],"associatedColumns":["CRR","5"],"associatedMergedColumns":[]},{"number":"595.5","isBolded":false,"associatedRows":["MCTS","-"],"associatedColumns":["Reanalyse","5"],"associatedMergedColumns":[]},{"number":"49.6","isBolded":false,"associatedRows":["policy"],"associatedColumns":["supervised","1"],"associatedMergedColumns":[]},{"number":"46.9","isBolded":false,"associatedRows":["policy"],"associatedColumns":["supervised","5"],"associatedMergedColumns":[]},{"number":"346.7","isBolded":false,"associatedRows":["value"],"associatedColumns":["CRR","5"],"associatedMergedColumns":[]},{"number":"248.4","isBolded":false,"associatedRows":["MCTS","-"],"associatedColumns":["supervised","1"],"associatedMergedColumns":[]},{"number":"549.7","isBolded":false,"associatedRows":["value"],"associatedColumns":["Reanalyse","5"],"associatedMergedColumns":[]},{"number":"50.7","isBolded":false,"associatedRows":["policy"],"associatedColumns":["supervised","0"],"associatedMergedColumns":[]},{"number":"408.3","isBolded":false,"associatedRows":["MCTS","-"],"associatedColumns":["CRR","5"],"associatedMergedColumns":[]},{"number":"394.8","isBolded":false,"associatedRows":["MCTS","-"],"associatedColumns":["supervised","5"],"associatedMergedColumns":[]},{"number":"359.4","isBolded":false,"associatedRows":["value"],"associatedColumns":["supervised","5"],"associatedMergedColumns":[]},{"number":"433.0","isBolded":false,"associatedRows":["policy"],"associatedColumns":["Reanalyse","5"],"associatedMergedColumns":[]},{"number":"143.2","isBolded":false,"associatedRows":["value"],"associatedColumns":["supervised","0"],"associatedMergedColumns":[]},{"number":"151.2","isBolded":false,"associatedRows":["value"],"associatedColumns":["supervised","1"],"associatedMergedColumns":[]}]},{"caption":"Task \n# dims no inject inject \n\nfish.swim \n5 \n79.7 585.4 \nmanipulator.insert ball \n5 \n21.2 557.0 \nmanipulator.insert peg \n5 \n90.1 432.7 \nwalker.stand \n6 \n735.6 759.8 \ncheetah.run \n6 \n101.3 798.9 \nhumanoid.run \n21 \n3.3 633.4 \n\nTable 10. Impact of injecting the trajectory action when re-\nanalysing. When reanalysing continuous action offline data or \ndemonstrations from other agents, the learned policy is unlikely \nto sample the same actions as occur in the data, preventing the \nMCTS from considering those actions. This effect is especially \npronounced in high dimensional tasks. Injecting the trajectory \nactions as one of the actions for MCTS to consider avoids this \nissue. \n","rows":["manipulator . insert peg","fish . swim","5","cheetah . run","6","manipulator . insert ball","walker . stand","humanoid . run","21"],"columns":["inject","no inject"],"mergedAllColumns":[],"numberCells":[{"number":"3.3","isBolded":false,"associatedRows":["humanoid . run","21"],"associatedColumns":["no inject"],"associatedMergedColumns":[]},{"number":"79.7","isBolded":false,"associatedRows":["fish . swim","5"],"associatedColumns":["no inject"],"associatedMergedColumns":[]},{"number":"101.3","isBolded":false,"associatedRows":["cheetah . run","6"],"associatedColumns":["no inject"],"associatedMergedColumns":[]},{"number":"21.2","isBolded":false,"associatedRows":["manipulator . insert ball","5"],"associatedColumns":["no inject"],"associatedMergedColumns":[]},{"number":"735.6","isBolded":false,"associatedRows":["walker . stand","6"],"associatedColumns":["no inject"],"associatedMergedColumns":[]},{"number":"633.4","isBolded":false,"associatedRows":["humanoid . run","21"],"associatedColumns":["inject"],"associatedMergedColumns":[]},{"number":"798.9","isBolded":false,"associatedRows":["cheetah . run","6"],"associatedColumns":["inject"],"associatedMergedColumns":[]},{"number":"759.8","isBolded":false,"associatedRows":["walker . stand","6"],"associatedColumns":["inject"],"associatedMergedColumns":[]},{"number":"432.7","isBolded":false,"associatedRows":["manipulator . insert peg","5"],"associatedColumns":["inject"],"associatedMergedColumns":[]},{"number":"557.0","isBolded":false,"associatedRows":["manipulator . insert ball","5"],"associatedColumns":["inject"],"associatedMergedColumns":[]},{"number":"585.4","isBolded":false,"associatedRows":["fish . swim","5"],"associatedColumns":["inject"],"associatedMergedColumns":[]},{"number":"90.1","isBolded":false,"associatedRows":["manipulator . insert peg","5"],"associatedColumns":["no inject"],"associatedMergedColumns":[]}]},{"caption":"# hidden size \nTask \n# dims # episodes \n32 \n64 \n128 \n256 \n512 \n\ncartpole.swingup \n1 \n40 605.0 343.3 293.2 302.6 259.3 \nfish.swim \n5 \n200 511.9 566.0 579.1 614.7 458.2 \nwalker.stand \n6 \n200 820.2 889.4 868.4 782.6 676.1 \nwalker.walk \n6 \n200 922.8 954.0 920.6 919.9 904.5 \n\nTable 11. Varying network size in DM Control benchmark from RL Unplugged. In tasks with very small amounts of training data, \nnetworks with a large number of parameters overfit easily. \n","rows":["1","200","fish . swim","5","6","walker . stand","cartpole . swingup","40","walker . walk"],"columns":["# hidden size","256","512","128","64","32"],"mergedAllColumns":[],"numberCells":[{"number":"511.9","isBolded":false,"associatedRows":["fish . swim","5","200"],"associatedColumns":["# hidden size","32"],"associatedMergedColumns":[]},{"number":"782.6","isBolded":false,"associatedRows":["walker . stand","6","200"],"associatedColumns":["# hidden size","256"],"associatedMergedColumns":[]},{"number":"920.6","isBolded":false,"associatedRows":["walker . walk","6","200"],"associatedColumns":["# hidden size","128"],"associatedMergedColumns":[]},{"number":"458.2","isBolded":false,"associatedRows":["fish . swim","5","200"],"associatedColumns":["# hidden size","512"],"associatedMergedColumns":[]},{"number":"605.0","isBolded":false,"associatedRows":["cartpole . swingup","1","40"],"associatedColumns":["# hidden size","32"],"associatedMergedColumns":[]},{"number":"566.0","isBolded":false,"associatedRows":["fish . swim","5","200"],"associatedColumns":["# hidden size","64"],"associatedMergedColumns":[]},{"number":"820.2","isBolded":false,"associatedRows":["walker . stand","6","200"],"associatedColumns":["# hidden size","32"],"associatedMergedColumns":[]},{"number":"259.3","isBolded":false,"associatedRows":["cartpole . swingup","1","40"],"associatedColumns":["# hidden size","512"],"associatedMergedColumns":[]},{"number":"919.9","isBolded":false,"associatedRows":["walker . walk","6","200"],"associatedColumns":["# hidden size","256"],"associatedMergedColumns":[]},{"number":"614.7","isBolded":false,"associatedRows":["fish . swim","5","200"],"associatedColumns":["# hidden size","256"],"associatedMergedColumns":[]},{"number":"676.1","isBolded":false,"associatedRows":["walker . stand","6","200"],"associatedColumns":["# hidden size","512"],"associatedMergedColumns":[]},{"number":"954.0","isBolded":false,"associatedRows":["walker . walk","6","200"],"associatedColumns":["# hidden size","64"],"associatedMergedColumns":[]},{"number":"293.2","isBolded":false,"associatedRows":["cartpole . swingup","1","40"],"associatedColumns":["# hidden size","128"],"associatedMergedColumns":[]},{"number":"302.6","isBolded":false,"associatedRows":["cartpole . swingup","1","40"],"associatedColumns":["# hidden size","256"],"associatedMergedColumns":[]},{"number":"922.8","isBolded":false,"associatedRows":["walker . walk","6","200"],"associatedColumns":["# hidden size","32"],"associatedMergedColumns":[]},{"number":"904.5","isBolded":false,"associatedRows":["walker . walk","6","200"],"associatedColumns":["# hidden size","512"],"associatedMergedColumns":[]},{"number":"343.3","isBolded":false,"associatedRows":["cartpole . swingup","1","40"],"associatedColumns":["# hidden size","64"],"associatedMergedColumns":[]},{"number":"579.1","isBolded":false,"associatedRows":["fish . swim","5","200"],"associatedColumns":["# hidden size","128"],"associatedMergedColumns":[]},{"number":"868.4","isBolded":false,"associatedRows":["walker . stand","6","200"],"associatedColumns":["# hidden size","128"],"associatedMergedColumns":[]},{"number":"889.4","isBolded":false,"associatedRows":["walker . stand","6","200"],"associatedColumns":["# hidden size","64"],"associatedMergedColumns":[]}]}]