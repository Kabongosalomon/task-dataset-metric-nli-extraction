[{"caption":"Training from scratch (i.e., random initialization) \nViT384 [23] \n-\n-\n77.9 76.5 \n-\n-\nDeiT [60] \n-\n-\n81.8 \n-\n-\n-\nViT [31] \n-\n-\n82.3 82.6 83.1 \n-\n\nSelf-Supervised Pre-Training on ImageNet-1K \nDINO [8] \nIN-1K \n300 \n82.8 \n-\n-\n-\nMoCo v3 [16] IN-1K \n300 \n83.2 84.1 \n-\n-\nBEiT [2] \nIN-1K \n800 \n83.2 85.2 \n-\n-\nMAE  ? [31] \nIN-1K \n1600 \n83.6 85.9 86.9 \n87.8 \nPeCo \nIN-1K \n800 \n84.5 86.5 87.5 \n88.3 \n\nTable 1. Image classification accuracy (%) comparison on \nImageNet-1K (IN-1K) of different self-supervised methods using \nvarious backbones. -B, -L, -H stands for using ViT-B, ViT-L, ViT-\nH model, respectively. We report Top-1 accuracy and our method \nPeCo outperforms previous self-supervised methods.  ? MAE is a \nconcurrent work of our PeCo. \n\n","rows":["PeCo","DeiT [ 60 ]","MAE ? [ 31 ]","DINO [ 8 ]","BEiT [ 2 ]","-","IN - 1K","1600","300","ViT [ 31 ]","MoCo v3 [ 16 ]","ViT384 [ 23 ]","800"],"columns":["Training from scratch ( i . e . , random initialization )","-"],"mergedAllColumns":["Self - Supervised Pre - Training on ImageNet - 1K"],"numberCells":[{"number":"82.6","isBolded":false,"associatedRows":["ViT [ 31 ]","-","-"],"associatedColumns":["Training from scratch ( i . e . , random initialization )","-","-"],"associatedMergedColumns":[]},{"number":"83.2","isBolded":false,"associatedRows":["MoCo v3 [ 16 ]","IN - 1K","300"],"associatedColumns":["Training from scratch ( i . e . , random initialization )","-","-","-"],"associatedMergedColumns":["Self - Supervised Pre - Training on ImageNet - 1K"]},{"number":"87.8","isBolded":false,"associatedRows":["MAE ? [ 31 ]","IN - 1K","1600"],"associatedColumns":["Training from scratch ( i . e . , random initialization )","-","-","-","-","-"],"associatedMergedColumns":["Self - Supervised Pre - Training on ImageNet - 1K"]},{"number":"76.5","isBolded":false,"associatedRows":["ViT384 [ 23 ]","-","-"],"associatedColumns":["Training from scratch ( i . e . , random initialization )"],"associatedMergedColumns":[]},{"number":"86.9","isBolded":false,"associatedRows":["MAE ? [ 31 ]","IN - 1K","1600"],"associatedColumns":["Training from scratch ( i . e . , random initialization )","-","-","-","-","-"],"associatedMergedColumns":["Self - Supervised Pre - Training on ImageNet - 1K"]},{"number":"88.3","isBolded":true,"associatedRows":["PeCo","IN - 1K","800"],"associatedColumns":["Training from scratch ( i . e . , random initialization )","-","-","-","-","-"],"associatedMergedColumns":["Self - Supervised Pre - Training on ImageNet - 1K"]},{"number":"81.8","isBolded":false,"associatedRows":["DeiT [ 60 ]","-","-"],"associatedColumns":["Training from scratch ( i . e . , random initialization )","-"],"associatedMergedColumns":[]},{"number":"87.5","isBolded":false,"associatedRows":["PeCo","IN - 1K","800"],"associatedColumns":["Training from scratch ( i . e . , random initialization )","-","-","-","-","-"],"associatedMergedColumns":["Self - Supervised Pre - Training on ImageNet - 1K"]},{"number":"83.6","isBolded":false,"associatedRows":["MAE ? [ 31 ]","IN - 1K","1600"],"associatedColumns":["Training from scratch ( i . e . , random initialization )","-","-","-","-","-"],"associatedMergedColumns":["Self - Supervised Pre - Training on ImageNet - 1K"]},{"number":"82.3","isBolded":false,"associatedRows":["ViT [ 31 ]","-","-"],"associatedColumns":["Training from scratch ( i . e . , random initialization )","-","-"],"associatedMergedColumns":[]},{"number":"83.1","isBolded":false,"associatedRows":["ViT [ 31 ]","-","-"],"associatedColumns":["Training from scratch ( i . e . , random initialization )","-","-"],"associatedMergedColumns":[]},{"number":"77.9","isBolded":false,"associatedRows":["ViT384 [ 23 ]","-","-"],"associatedColumns":["Training from scratch ( i . e . , random initialization )"],"associatedMergedColumns":[]},{"number":"83.2","isBolded":false,"associatedRows":["BEiT [ 2 ]","IN - 1K","800"],"associatedColumns":["Training from scratch ( i . e . , random initialization )","-","-","-","-"],"associatedMergedColumns":["Self - Supervised Pre - Training on ImageNet - 1K"]},{"number":"85.2","isBolded":false,"associatedRows":["BEiT [ 2 ]","IN - 1K","800"],"associatedColumns":["Training from scratch ( i . e . , random initialization )","-","-","-","-"],"associatedMergedColumns":["Self - Supervised Pre - Training on ImageNet - 1K"]},{"number":"84.5","isBolded":false,"associatedRows":["PeCo","IN - 1K","800"],"associatedColumns":["Training from scratch ( i . e . , random initialization )","-","-","-","-","-"],"associatedMergedColumns":["Self - Supervised Pre - Training on ImageNet - 1K"]},{"number":"84.1","isBolded":false,"associatedRows":["MoCo v3 [ 16 ]","IN - 1K","300"],"associatedColumns":["Training from scratch ( i . e . , random initialization )","-","-","-"],"associatedMergedColumns":["Self - Supervised Pre - Training on ImageNet - 1K"]},{"number":"86.5","isBolded":false,"associatedRows":["PeCo","IN - 1K","800"],"associatedColumns":["Training from scratch ( i . e . , random initialization )","-","-","-","-","-"],"associatedMergedColumns":["Self - Supervised Pre - Training on ImageNet - 1K"]},{"number":"85.9","isBolded":false,"associatedRows":["MAE ? [ 31 ]","IN - 1K","1600"],"associatedColumns":["Training from scratch ( i . e . , random initialization )","-","-","-","-","-"],"associatedMergedColumns":["Self - Supervised Pre - Training on ImageNet - 1K"]},{"number":"82.8","isBolded":false,"associatedRows":["DINO [ 8 ]","IN - 1K","300"],"associatedColumns":["Training from scratch ( i . e . , random initialization )","-","-"],"associatedMergedColumns":["Self - Supervised Pre - Training on ImageNet - 1K"]}]},{"caption":"Methods \ntokenizer \ntokenizer BERT pre-IN-1K \ndataset \nparams train epoch Top-1 \n\nBEiT [2] DALLE(400M) 53.8M \n300/800 82.8/83.2 \nPeCo \nIN-1K(1.3M) 37.5M \n300/800 84.1/84.5 \nPeCo lite IN-1K(1.3M) 25.7M \n300/800 84.0/84.5 \n\nTable 2. Tokenizer comparison with BEiT. Here we report and \ntokenizer training dataset and parameters. PeCo lite is a lite-weight \nversion of PeCo that reduces the feature dimension of tokenizer to \nhalf. \n\n","rows":["PeCo","DALLE ( 400M )","BEiT [ 2 ]","lite","IN - 1K ( 1 . 3M )"],"columns":["params","300 / 800","tokenizer BERT pre - IN - 1K"],"mergedAllColumns":["Methods"],"numberCells":[{"number":"37.5M","isBolded":false,"associatedRows":["PeCo","IN - 1K ( 1 . 3M )"],"associatedColumns":["tokenizer BERT pre - IN - 1K","params","300 / 800"],"associatedMergedColumns":["Methods"]},{"number":"53.8M","isBolded":false,"associatedRows":["BEiT [ 2 ]","DALLE ( 400M )"],"associatedColumns":["tokenizer BERT pre - IN - 1K","params"],"associatedMergedColumns":["Methods"]},{"number":"25.7M","isBolded":false,"associatedRows":["PeCo","lite","IN - 1K ( 1 . 3M )"],"associatedColumns":["tokenizer BERT pre - IN - 1K","params","300 / 800","300 / 800"],"associatedMergedColumns":["Methods"]}]},{"caption":"Models \n\npre-train \npre-train ADE-20K \ndataset \nepochs \nmIoU \n\nImageNet-1K supervised pre-training \n45.3 \n\nSelf-Supervised Pre-Training on ImageNet-1K \nBEiT [2] \nIN-1K+DALLE \n300 \n45.7 \nPeCo(ours) \nIN-1K \n300 \n46.7 \n\nTable 3. Semantic segmentation mIoU (%) comparison on \nADE20K of different self-supervised methods using the same \nbackbone ViT-B. Our method PeCo outperforms previous self-\nsupervised methods. \n\n","rows":["IN - 1K","IN - 1K+DALLE","ImageNet - 1K supervised pre - training","300","PeCo ( ours )","BEiT [ 2 ]"],"columns":["mIoU","comparison","COCO","ADE - 20K","mk","AP"],"mergedAllColumns":["Models","supervised methods .","Self - Supervised Pre - Training on ImageNet - 1K"],"numberCells":[{"number":"38.8","isBolded":false,"associatedRows":["BEiT [ 2 ]","IN - 1K+DALLE","300","300"],"associatedColumns":["ADE - 20K","mIoU","comparison","COCO","mk","AP"],"associatedMergedColumns":["supervised methods ."]},{"number":"46.7","isBolded":true,"associatedRows":["PeCo ( ours )","IN - 1K","300","300"],"associatedColumns":["ADE - 20K","mIoU"],"associatedMergedColumns":["Self - Supervised Pre - Training on ImageNet - 1K"]},{"number":"45.7","isBolded":false,"associatedRows":["BEiT [ 2 ]","IN - 1K+DALLE","300","300"],"associatedColumns":["ADE - 20K","mIoU"],"associatedMergedColumns":["Self - Supervised Pre - Training on ImageNet - 1K"]},{"number":"39.8","isBolded":true,"associatedRows":["PeCo ( ours )","IN - 1K","300","300"],"associatedColumns":["ADE - 20K","mIoU","comparison","COCO","mk","AP"],"associatedMergedColumns":["supervised methods ."]},{"number":"43.9","isBolded":true,"associatedRows":["PeCo ( ours )","IN - 1K","300","300"],"associatedColumns":["ADE - 20K","mIoU","comparison","COCO","mk"],"associatedMergedColumns":["supervised methods ."]},{"number":"45.3","isBolded":false,"associatedRows":["ImageNet - 1K supervised pre - training","300","300"],"associatedColumns":["ADE - 20K","mIoU"],"associatedMergedColumns":["Models"]},{"number":"42.6","isBolded":false,"associatedRows":["BEiT [ 2 ]","IN - 1K+DALLE","300","300"],"associatedColumns":["ADE - 20K","mIoU","comparison","COCO","mk"],"associatedMergedColumns":["supervised methods ."]}]},{"caption":"Models \npre-train \npre-train \nCOCO \ndataset \nepochs \nAP bb AP mk \n\nBEiT [2] \nIN-1K+DALLE \n300 \n42.6 \n38.8 \nPeCo(ours) \nIN-1K \n300 \n43.9 \n39.8 \n\nTable 4. Object detection and instance segmentation comparison \nin terms of box AP (AP bb ) and mask AP (AP mk ) on COCO of \ndifferent self-supervised methods using the same backbone ViT-\nB. Our method PeCo outperforms the strong competitor BEiT [2]. \n\n","rows":["IN - 1K","IN - 1K+DALLE","ImageNet - 1K supervised pre - training","300","PeCo ( ours )","BEiT [ 2 ]"],"columns":["mIoU","comparison","COCO","ADE - 20K","mk","AP"],"mergedAllColumns":["Models","supervised methods .","Self - Supervised Pre - Training on ImageNet - 1K"],"numberCells":[{"number":"42.6","isBolded":false,"associatedRows":["BEiT [ 2 ]","IN - 1K+DALLE","300","300"],"associatedColumns":["ADE - 20K","mIoU","comparison","COCO","mk"],"associatedMergedColumns":["supervised methods ."]},{"number":"46.7","isBolded":true,"associatedRows":["PeCo ( ours )","IN - 1K","300","300"],"associatedColumns":["ADE - 20K","mIoU"],"associatedMergedColumns":["Self - Supervised Pre - Training on ImageNet - 1K"]},{"number":"39.8","isBolded":true,"associatedRows":["PeCo ( ours )","IN - 1K","300","300"],"associatedColumns":["ADE - 20K","mIoU","comparison","COCO","mk","AP"],"associatedMergedColumns":["supervised methods ."]},{"number":"45.3","isBolded":false,"associatedRows":["ImageNet - 1K supervised pre - training","300","300"],"associatedColumns":["ADE - 20K","mIoU"],"associatedMergedColumns":["Models"]},{"number":"38.8","isBolded":false,"associatedRows":["BEiT [ 2 ]","IN - 1K+DALLE","300","300"],"associatedColumns":["ADE - 20K","mIoU","comparison","COCO","mk","AP"],"associatedMergedColumns":["supervised methods ."]},{"number":"43.9","isBolded":true,"associatedRows":["PeCo ( ours )","IN - 1K","300","300"],"associatedColumns":["ADE - 20K","mIoU","comparison","COCO","mk"],"associatedMergedColumns":["supervised methods ."]},{"number":"45.7","isBolded":false,"associatedRows":["BEiT [ 2 ]","IN - 1K+DALLE","300","300"],"associatedColumns":["ADE - 20K","mIoU"],"associatedMergedColumns":["Self - Supervised Pre - Training on ImageNet - 1K"]}]},{"caption":"Table 5. Evaluation of the semantics of the codewords from linear \nprobling accuracy (%) of codewords on ImageNet-1K and clas-\nsification accuracy (%) on the reconstructed ImageNet validation \nimages using Deit-T ","rows":["DALL - E [ 56 ]","PeCo ( ours )","PeCo ( w / o Lpercep )"],"columns":["Classifi . on recon .","LinearP . on codewords"],"mergedAllColumns":[],"numberCells":[{"number":"18.2","isBolded":false,"associatedRows":["DALL - E [ 56 ]"],"associatedColumns":["Classifi . on recon ."],"associatedMergedColumns":[]},{"number":"51.7","isBolded":false,"associatedRows":["PeCo ( ours )"],"associatedColumns":["Classifi . on recon ."],"associatedMergedColumns":[]},{"number":"17.9","isBolded":false,"associatedRows":["PeCo ( w / o Lpercep )"],"associatedColumns":["Classifi . on recon ."],"associatedMergedColumns":[]},{"number":"29.7","isBolded":false,"associatedRows":["PeCo ( ours )"],"associatedColumns":["LinearP . on codewords"],"associatedMergedColumns":[]},{"number":"6.1","isBolded":false,"associatedRows":["DALL - E [ 56 ]"],"associatedColumns":["LinearP . on codewords"],"associatedMergedColumns":[]},{"number":"10.2","isBolded":false,"associatedRows":["PeCo ( w / o Lpercep )"],"associatedColumns":["LinearP . on codewords"],"associatedMergedColumns":[]}]},{"caption":"The Effect of Training Dataset for Perceptual Code-\nbook. One strong advantage of our PeCo is that we only \nuse the ImageNet-1K with 1.28M images for the VQ-VAE \ntraining. This is really data efficient compared with DALL-\nE using 250M images from the Internet for the VQ-VAE \ntraining. To further validate this, we train a perceptual code-\n\nDeep models for perceptual similarity acc. on ImageNet-1K \nBaseline (w/o perceptual similarity) \n82.6 \nSelf-supervised ResNet-50 [16]  83.3 \nSelf-supervised ViT-B [16]  83.5 \nSupervised VGG [59]  83.4 \n\nTable 6. The performance comparison when using different archi-\ntectures for calculating the perceptual similarity. We evaluate on \nthe downstream ImageNet-1K classification (%). Using different \nmodels achieve comparable performance, all better than the base-\nline without the perceptual similarity. \n\n","rows":["ImageNet - 1K","ImageNet - 22K"],"columns":["acc . on ImageNet - 1K"],"mergedAllColumns":[],"numberCells":[{"number":"83.3","isBolded":false,"associatedRows":["ImageNet - 1K"],"associatedColumns":["acc . on ImageNet - 1K"],"associatedMergedColumns":[]},{"number":"83.3","isBolded":false,"associatedRows":["ImageNet - 22K"],"associatedColumns":["acc . on ImageNet - 1K"],"associatedMergedColumns":[]}]},{"caption":"Training dataset for perceptual codebook acc. on ImageNet-1K \nImageNet-1K \n83.3 \nImageNet-22K \n83.3 \n\nTable 7. Illustrating the effect of training dataset for learning per-\nceptual codebook. We show fine-tune accuracy (%) on ImageNet-\n1K. We can see that enlarging training set for learning perceptual \ncodebook does not bring gain for the transfer performance. \n\n","rows":["ImageNet - 1K","ImageNet - 22K"],"columns":["acc . on ImageNet - 1K"],"mergedAllColumns":[],"numberCells":[{"number":"83.3","isBolded":false,"associatedRows":["ImageNet - 22K"],"associatedColumns":["acc . on ImageNet - 1K"],"associatedMergedColumns":[]},{"number":"83.3","isBolded":false,"associatedRows":["ImageNet - 1K"],"associatedColumns":["acc . on ImageNet - 1K"],"associatedMergedColumns":[]}]},{"caption":"book over the ImageNet-22K dataset with 14M images fol-\nlowing our default setting. The BERT pre-training is still \nconducted on ImageNet-1K. We show the transfer perfor-\nmance in Table 7. It can be seen that training perceptual \ncodebook with more data seems not helpful for the follow-\ning BERT pre-training. This is particularly encouraging as \nwe only need to focus on designing a better perceptual code-\nbook, instead of scaling up the dataset. \nDifferent Architectures for VQ-VAE. Here we investigate \nthe performance when using different architectures for VQ-\nVAE. We consider several variants of the network architec-\nture. For encoder, we explore three models: 1) 16? down-\nsample encoder (our default setting); 2) 8? down sample \nencoder; 3) ViT-B (16? down-sample). For the 8? down-\nsample encoder, we remove one stage and train it with im-\nages of 112?112 resolution. For decoder, we use the in-\nversed version of the corresponding decoder. The results \n\n","rows":["ImageNet - 1K","ImageNet - 22K"],"columns":["acc . on ImageNet - 1K"],"mergedAllColumns":[],"numberCells":[{"number":"83.3","isBolded":false,"associatedRows":["ImageNet - 1K"],"associatedColumns":["acc . on ImageNet - 1K"],"associatedMergedColumns":[]},{"number":"83.3","isBolded":false,"associatedRows":["ImageNet - 22K"],"associatedColumns":["acc . on ImageNet - 1K"],"associatedMergedColumns":[]}]},{"caption":"Encoder of VQ-VAE \n\nDecoder of VQ-VAE \nacc. on ImageNet-1K \nViT-B \nViT-B \n82.9 \nCNN(8x) \nCNN(8x) \n83.1 \nCNN(16x) \nCNN(16x) \n83.3 \nCNN(16x) \nCNN(16x) (Half Channel) \n83.2 \nCNN(16x) \nCNN(16x) (Half Depth) \n83.3 \n\nTable 8. Illustrating the effect of different architectures for training \nPeCo. We show fine-tune accuracy (%) on ImageNet-1K. CNN \nbased encoders and decoders achieve better results than vision \nTransformer. \n\n","rows":["0","?","ImagenNet - 1K"],"columns":["1","3","10"],"mergedAllColumns":[],"numberCells":[{"number":"83.3","isBolded":false,"associatedRows":["ImagenNet - 1K"],"associatedColumns":["1"],"associatedMergedColumns":[]},{"number":"82.6","isBolded":false,"associatedRows":["ImagenNet - 1K"],"associatedColumns":["1"],"associatedMergedColumns":[]},{"number":"0.3","isBolded":false,"associatedRows":["?","0"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"83.3","isBolded":false,"associatedRows":["ImagenNet - 1K"],"associatedColumns":["1"],"associatedMergedColumns":[]},{"number":"82.9","isBolded":false,"associatedRows":["ImagenNet - 1K"],"associatedColumns":["3"],"associatedMergedColumns":[]},{"number":"82.8","isBolded":false,"associatedRows":["ImagenNet - 1K"],"associatedColumns":["10"],"associatedMergedColumns":[]}]},{"caption":"? \n0 \n0.3 \n1 \n3 \n10 \nImagenNet-1K 82.6 83.3 83.3 82.9 82.8 \nTable 9. Illustrating the effect of loss weight of perceptual similar-\nity. We show fine-tune accuracy (%) on ImageNet-1K. Enlarging \nthe loss weight can not get consistent improvement, may due to \nthe loss of local details. \n\n","rows":["0","?","ImagenNet - 1K"],"columns":["1","3","10"],"mergedAllColumns":[],"numberCells":[{"number":"0.3","isBolded":false,"associatedRows":["?","0"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"82.6","isBolded":false,"associatedRows":["ImagenNet - 1K"],"associatedColumns":["1"],"associatedMergedColumns":[]},{"number":"83.3","isBolded":false,"associatedRows":["ImagenNet - 1K"],"associatedColumns":["1"],"associatedMergedColumns":[]},{"number":"82.8","isBolded":false,"associatedRows":["ImagenNet - 1K"],"associatedColumns":["10"],"associatedMergedColumns":[]},{"number":"82.9","isBolded":false,"associatedRows":["ImagenNet - 1K"],"associatedColumns":["3"],"associatedMergedColumns":[]},{"number":"83.3","isBolded":false,"associatedRows":["ImagenNet - 1K"],"associatedColumns":["1"],"associatedMergedColumns":[]}]},{"caption":"Table 8. We observe \nthat CNN based encoders and decoders achieve better re-\nsults than vision Transformer. We further reduce the pa-\nrameters of decoder by decreasing the channel number or \ndecreasing the depth of the network by half. Results shown \nin ","rows":["0","?","ImagenNet - 1K"],"columns":["1","3","10"],"mergedAllColumns":[],"numberCells":[{"number":"82.9","isBolded":false,"associatedRows":["ImagenNet - 1K"],"associatedColumns":["3"],"associatedMergedColumns":[]},{"number":"83.3","isBolded":false,"associatedRows":["ImagenNet - 1K"],"associatedColumns":["1"],"associatedMergedColumns":[]},{"number":"82.6","isBolded":false,"associatedRows":["ImagenNet - 1K"],"associatedColumns":["1"],"associatedMergedColumns":[]},{"number":"83.3","isBolded":false,"associatedRows":["ImagenNet - 1K"],"associatedColumns":["1"],"associatedMergedColumns":[]},{"number":"0.3","isBolded":false,"associatedRows":["?","0"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"82.8","isBolded":false,"associatedRows":["ImagenNet - 1K"],"associatedColumns":["10"],"associatedMergedColumns":[]}]},{"caption":"Table 10. Performance comparison of our implicit way (percep-\ntual loss on images) and the explicit ways (classification/conrastive \nloss on codewords) for improving the perceptual level of code-\nbook. We show fine-tune accuracy (%) on ImageNet-1K. \n\n","rows":["L pixel + L percep + L adv","L pixel","L pixel + L percep"],"columns":["- 1 acc . on IN - 1K"],"mergedAllColumns":[],"numberCells":[{"number":"82.6","isBolded":false,"associatedRows":["L pixel"],"associatedColumns":["- 1 acc . on IN - 1K"],"associatedMergedColumns":[]},{"number":"83.3","isBolded":false,"associatedRows":["L pixel + L percep"],"associatedColumns":["- 1 acc . on IN - 1K"],"associatedMergedColumns":[]},{"number":"83.3","isBolded":false,"associatedRows":["L pixel + L percep + L adv"],"associatedColumns":["- 1 acc . on IN - 1K"],"associatedMergedColumns":[]}]},{"caption":"-1 acc. on IN-1K \nL pixel \n82.6 \nL pixel + L percep \n83.3 \nL pixel + L percep + L adv \n83.3 \n\nTable 11. Performance comparison when using different loss func-\ntions. We show fine-tune accuracy (%) on ImageNet-1K. Adding \nan extra adversarial loss which is useful for improving image qual-\nity brings little gain to the transfer performance. \n\n","rows":["L pixel + L percep + L adv","L pixel","L pixel + L percep"],"columns":["- 1 acc . on IN - 1K"],"mergedAllColumns":[],"numberCells":[{"number":"82.6","isBolded":false,"associatedRows":["L pixel"],"associatedColumns":["- 1 acc . on IN - 1K"],"associatedMergedColumns":[]},{"number":"83.3","isBolded":false,"associatedRows":["L pixel + L percep"],"associatedColumns":["- 1 acc . on IN - 1K"],"associatedMergedColumns":[]},{"number":"83.3","isBolded":false,"associatedRows":["L pixel + L percep + L adv"],"associatedColumns":["- 1 acc . on IN - 1K"],"associatedMergedColumns":[]}]},{"caption":"Table 10. We \nconjecture that the codebook may learn global semantics \nfrom the classification/contrastive loss and thus fail to dif-\nferentiate different codewords, which is not suitable for pre-\ntraining. In contrast, the features from a pre-trained deep \nmodel contain rich and local semantics. However, we still \nthink that it will be more intuitive to explicitly learn the per-\nceptual codebook and we leave that as future work. \nPerceptual Loss vs. GAN Loss. The perceptual loss is \nwidely used in generation tasks with the goal of improv-\ning the image quality. We ask the question that is there a \npositive relation with the image quality and the perceptual \nlevel of the codebook. In order to explore this, we adopt an-\nother technique, adversarial loss in Generative Adversarial \nNets(GANs) ","rows":["L pixel + L percep + L adv","L pixel","L pixel + L percep"],"columns":["- 1 acc . on IN - 1K"],"mergedAllColumns":[],"numberCells":[{"number":"83.3","isBolded":false,"associatedRows":["L pixel + L percep + L adv"],"associatedColumns":["- 1 acc . on IN - 1K"],"associatedMergedColumns":[]},{"number":"82.6","isBolded":false,"associatedRows":["L pixel"],"associatedColumns":["- 1 acc . on IN - 1K"],"associatedMergedColumns":[]},{"number":"83.3","isBolded":false,"associatedRows":["L pixel + L percep"],"associatedColumns":["- 1 acc . on IN - 1K"],"associatedMergedColumns":[]}]}]