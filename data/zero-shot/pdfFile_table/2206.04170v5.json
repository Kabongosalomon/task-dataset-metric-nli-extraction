[{"caption":"Table \n1, we compare the cumulative training times for self-supervised training of a CNN and Transformer \nwith DINO and CASS. We observed that CASS took an average of 69% less time compared to DINO. \nAnother point to note is that, CASS trained two architectures at the same time or in a single pass. \nWhile to train a CNN and Transformer with DINO it would take two separate passes. \n\nDataset \nDINO \nCASS \nAutoimmune 1 Hour 13 Mins \n21 Mins \nDermofit \n3 Hours 9 mins \n1 Hour 11 Mins \nBrain MRI \n26 Hours 21 Mins \n7 Hours 11 Mins \nISIC-2019 \n109 Hours 21 Mins 29 Hours 58 Mins \nTable 1: Self-supervised training time comparison for 100 epochs on a single RTX8000 GPU. \n\n","rows":["1 Hour","ISIC - 2019","109 Hours","3 Hours 9 mins","Autoimmune","7 Hours","Brain MRI","Dermofit"],"columns":["CASS","Table","DINO"],"mergedAllColumns":["While to train a CNN and Transformer with DINO it would take two separate passes ."],"numberCells":[{"number":"11Mins","isBolded":true,"associatedRows":["Dermofit","3 Hours 9 mins","1 Hour"],"associatedColumns":["Table","CASS"],"associatedMergedColumns":["While to train a CNN and Transformer with DINO it would take two separate passes ."]},{"number":"11Mins","isBolded":true,"associatedRows":["Brain MRI","3 Hours 9 mins","7 Hours"],"associatedColumns":["Table","CASS"],"associatedMergedColumns":["While to train a CNN and Transformer with DINO it would take two separate passes ."]},{"number":"21Mins","isBolded":false,"associatedRows":["ISIC - 2019","109 Hours"],"associatedColumns":["Table","CASS"],"associatedMergedColumns":["While to train a CNN and Transformer with DINO it would take two separate passes ."]},{"number":"13Mins","isBolded":false,"associatedRows":["Autoimmune","1 Hour"],"associatedColumns":["Table","DINO"],"associatedMergedColumns":["While to train a CNN and Transformer with DINO it would take two separate passes ."]},{"number":"26Hours","isBolded":false,"associatedRows":["Brain MRI"],"associatedColumns":["Table","DINO"],"associatedMergedColumns":["While to train a CNN and Transformer with DINO it would take two separate passes ."]},{"number":"21Mins","isBolded":true,"associatedRows":["Autoimmune","1 Hour"],"associatedColumns":["Table","CASS"],"associatedMergedColumns":["While to train a CNN and Transformer with DINO it would take two separate passes ."]},{"number":"21Mins","isBolded":false,"associatedRows":["Brain MRI","1 Hour"],"associatedColumns":["Table","DINO"],"associatedMergedColumns":["While to train a CNN and Transformer with DINO it would take two separate passes ."]},{"number":"58Mins","isBolded":true,"associatedRows":["ISIC - 2019","109 Hours","7 Hours"],"associatedColumns":["Table","CASS"],"associatedMergedColumns":["While to train a CNN and Transformer with DINO it would take two separate passes ."]},{"number":"29Hours","isBolded":true,"associatedRows":["ISIC - 2019","109 Hours"],"associatedColumns":["Table","CASS"],"associatedMergedColumns":["While to train a CNN and Transformer with DINO it would take two separate passes ."]}]},{"caption":"score \n10% \n100% \nDINO \nResnet-50 0.8237?0.001 \n0.84252?0.008 \nCASS \nResnet-50 0.8158?0.0055 0.8650?0.0001 \nSupervised Resnet-50 0.82095?0.007 0.819?0.0216 \nDINO \nViT B/16 \n0.8445?0.0008 0.8639? 0.002 \nCASS \nViT B/16 \n0.8717?0.005 \n0.8894?0.005 \nSupervised ViT B/16 \n0.8356?0.007 \n0.8420?0.009 \nTable 2: Results for autoimmune biopsy slides dataset. In this table we compare the F1 score on \ntest set. We observed that CASS outperformed the existing state-of-art self-supervised method using \n100% labels for CNN as well as for Transformers. Although DINO outperforms CASS for CNN with \n10% labeled fraction. Overall CASS outperforms DINO by 2.2% for 100% labeled training for CNN \nand Transformer. For Transformers in 10% labeled training CASS\u0027 performance was 2.7% better \nthan DINO. \n\n","rows":["0 . 8445?0 . 0008","ViT B / 16","10% labeled fraction . Overall CASS outperforms DINO by","10%","DINO","and Transformer . For Transformers in 10% labeled training CASS \u0027 performance was"],"columns":["score","0 . 8237?0 . 001","0 . 84252?0 . 008","ViT B / 16","0 . 8717?0 . 005","0 . 819?0 . 0216","0 . 8650?0 . 0001","0 . 8894?0 . 005","0 . 8158?0 . 0055","0 . 8356?0 . 007","0 . 8420?0 . 009","0 . 82095?0 . 007"],"mergedAllColumns":["test set . We observed that CASS outperformed the existing state - of - art self - supervised method using"],"numberCells":[{"number":"2.2%for","isBolded":false,"associatedRows":["10% labeled fraction . Overall CASS outperforms DINO by"],"associatedColumns":["score","0 . 84252?0 . 008","0 . 8158?0 . 0055","0 . 82095?0 . 007","0 . 8717?0 . 005","0 . 8356?0 . 007"],"associatedMergedColumns":["test set . We observed that CASS outperformed the existing state - of - art self - supervised method using"]},{"number":"100%","isBolded":false,"associatedRows":["10% labeled fraction . Overall CASS outperforms DINO by","ViT B / 16","10%","0 . 8445?0 . 0008"],"associatedColumns":["score"],"associatedMergedColumns":[]},{"number":"0.8639?","isBolded":false,"associatedRows":["10% labeled fraction . Overall CASS outperforms DINO by","DINO","ViT B / 16","0 . 8445?0 . 0008"],"associatedColumns":["score","0 . 84252?0 . 008","0 . 8650?0 . 0001","0 . 819?0 . 0216"],"associatedMergedColumns":[]},{"number":"100%labeledtrainingforCNN","isBolded":false,"associatedRows":["10% labeled fraction . Overall CASS outperforms DINO by","0 . 8445?0 . 0008"],"associatedColumns":["score","0 . 84252?0 . 008","0 . 8650?0 . 0001","0 . 819?0 . 0216","0 . 8894?0 . 005","0 . 8420?0 . 009"],"associatedMergedColumns":["test set . We observed that CASS outperformed the existing state - of - art self - supervised method using"]},{"number":"0.002","isBolded":false,"associatedRows":["10% labeled fraction . Overall CASS outperforms DINO by","DINO","ViT B / 16","0 . 8445?0 . 0008"],"associatedColumns":["score","0 . 84252?0 . 008","0 . 8650?0 . 0001","0 . 819?0 . 0216"],"associatedMergedColumns":[]},{"number":"2.7%better","isBolded":false,"associatedRows":["and Transformer . For Transformers in 10% labeled training CASS \u0027 performance was"],"associatedColumns":["score","0 . 84252?0 . 008","0 . 8650?0 . 0001","0 . 819?0 . 0216","0 . 8894?0 . 005","0 . 8420?0 . 009"],"associatedMergedColumns":["test set . We observed that CASS outperformed the existing state - of - art self - supervised method using"]},{"number":"100%labelsforCNNaswellasforTransformers.AlthoughDINOoutperformsCASSforCNNwith","isBolded":false,"associatedRows":[],"associatedColumns":["score","0 . 8237?0 . 001","0 . 84252?0 . 008","0 . 8650?0 . 0001","0 . 8158?0 . 0055","0 . 819?0 . 0216","0 . 8894?0 . 005","ViT B / 16","0 . 8420?0 . 009"],"associatedMergedColumns":["test set . We observed that CASS outperformed the existing state - of - art self - supervised method using"]}]},{"caption":"Table 3. \n\n","rows":["DINO ( ViT B / 16 )"],"columns":["score","0 . 33?0 . 0001","10%","0 . 4367?0 . 0002","0 . 3749?0 . 0011"],"mergedAllColumns":[],"numberCells":[{"number":"0.332?","isBolded":false,"associatedRows":["DINO ( ViT B / 16 )"],"associatedColumns":["score","10%","0 . 3749?0 . 0011","0 . 4367?0 . 0002","0 . 33?0 . 0001"],"associatedMergedColumns":[]},{"number":"0.0002","isBolded":false,"associatedRows":["DINO ( ViT B / 16 )"],"associatedColumns":["score","10%","0 . 3749?0 . 0011","0 . 4367?0 . 0002","0 . 33?0 . 0001"],"associatedMergedColumns":[]}]},{"caption":"score \n10% \n100% \nDINO (Resnet-50) \n0.3749?0.0011 0.6775?0.0005 \nCASS (Resnet-50) \n0.4367?0.0002 0.7132?0.0003 \nSupervised (Resnet-50) 0.33?0.0001 \n0.6341?0.0077 \nDINO (ViT B/16) \n0.332? 0.0002 0.4810?0.0012 \nCASS (ViT B/16) \n0.3896?0.0013 0.6667?0.0002 \nSupervised (ViT B/16) \n0.299?0.002 \n0.456?0.0077 \nTable 3: Results for the dermofit dataset. Parenthesis next to the techniques represent the architecture \nused, for example DINO(ViT B/16) represents ViT B/16 trianed with DINO. In this table we compare \nthe F1 score on test set. ","rows":["DINO ( ViT B / 16 )"],"columns":["score","0 . 33?0 . 0001","10%","0 . 4367?0 . 0002","0 . 3749?0 . 0011"],"mergedAllColumns":[],"numberCells":[{"number":"0.332?","isBolded":false,"associatedRows":["DINO ( ViT B / 16 )"],"associatedColumns":["score","10%","0 . 3749?0 . 0011","0 . 4367?0 . 0002","0 . 33?0 . 0001"],"associatedMergedColumns":[]},{"number":"0.0002","isBolded":false,"associatedRows":["DINO ( ViT B / 16 )"],"associatedColumns":["score","10%","0 . 3749?0 . 0011","0 . 4367?0 . 0002","0 . 33?0 . 0001"],"associatedMergedColumns":[]}]},{"caption":"Table 4: While DINO outperformed CASS for 1% and 10% labeled training for CNN, CASS main-\ntained its superiority for 100% labeled training, albeit by just 0.09%. Similarly, CASS outperformed \nDINO for all data regimes for Transformers, incrementally 1.34% in for 1%, 3.04% for 10%, and \n4.38% for 100% labeled training. We observe that this margin is more significant than for biopsy \nimages. Such results could be ascribed to the increase in dataset size and increasing learnable \ninformation. \n\n","rows":["CASS","0 . 9022?0 . 011","0 . 7529?0 . 044","0 . 40816?0 . 13","DINO","Resnet - 50","Supervised","0 . 7833?0 . 0259","0 . 747?0 . 0245","0 . 3345?0 . 11","ViT B / 16","0 . 8925?0 . 0254","0 . 52?0 . 018","0 . 3211?0 . 071"],"columns":["Backbone","Testing F1 score","0 . 9900?0 . 0058","100%","1%","10%","0 . 63405?0 . 09"],"mergedAllColumns":[],"numberCells":[{"number":"0.8841?","isBolded":false,"associatedRows":["DINO","ViT B / 16","0 . 3211?0 . 071","0 . 7529?0 . 044"],"associatedColumns":["Testing F1 score","Backbone","100%","0 . 9900?0 . 0058"],"associatedMergedColumns":[]},{"number":"0.3017?","isBolded":false,"associatedRows":["Supervised","ViT B / 16"],"associatedColumns":["Testing F1 score","Backbone","1%","0 . 63405?0 . 09"],"associatedMergedColumns":[]},{"number":"0.9899?","isBolded":false,"associatedRows":["Supervised","Resnet - 50","0 . 52?0 . 018","0 . 9022?0 . 011"],"associatedColumns":["Testing F1 score","Backbone","100%","0 . 9900?0 . 0058"],"associatedMergedColumns":[]},{"number":"0.0052","isBolded":false,"associatedRows":["DINO","ViT B / 16","0 . 3211?0 . 071","0 . 7529?0 . 044"],"associatedColumns":["Testing F1 score","Backbone","100%","0 . 9900?0 . 0058"],"associatedMergedColumns":[]},{"number":"0.017","isBolded":false,"associatedRows":["Supervised","ViT B / 16","0 . 3211?0 . 071","0 . 747?0 . 0245"],"associatedColumns":["Testing F1 score","Backbone","100%","0 . 9900?0 . 0058"],"associatedMergedColumns":[]},{"number":"0.9279?","isBolded":true,"associatedRows":["CASS","ViT B / 16","0 . 3345?0 . 11","0 . 7833?0 . 0259"],"associatedColumns":["Testing F1 score","Backbone","100%","0 . 9900?0 . 0058"],"associatedMergedColumns":[]},{"number":"0.077","isBolded":false,"associatedRows":["Supervised","ViT B / 16"],"associatedColumns":["Testing F1 score","Backbone","10%","0 . 63405?0 . 09"],"associatedMergedColumns":[]},{"number":"0.9909?","isBolded":true,"associatedRows":["CASS","Resnet - 50","0 . 40816?0 . 13","0 . 8925?0 . 0254"],"associatedColumns":["Testing F1 score","Backbone","100%","0 . 9900?0 . 0058"],"associatedMergedColumns":[]},{"number":"0.0032","isBolded":true,"associatedRows":["CASS","Resnet - 50","0 . 40816?0 . 13","0 . 8925?0 . 0254"],"associatedColumns":["Testing F1 score","Backbone","100%","0 . 9900?0 . 0058"],"associatedMergedColumns":[]},{"number":"0.003","isBolded":false,"associatedRows":["Supervised","Resnet - 50","0 . 52?0 . 018","0 . 9022?0 . 011"],"associatedColumns":["Testing F1 score","Backbone","100%","0 . 9900?0 . 0058"],"associatedMergedColumns":[]},{"number":"0.0213","isBolded":true,"associatedRows":["CASS","ViT B / 16","0 . 3345?0 . 11","0 . 7833?0 . 0259"],"associatedColumns":["Testing F1 score","Backbone","100%","0 . 9900?0 . 0058"],"associatedMergedColumns":[]},{"number":"0.8719?","isBolded":false,"associatedRows":["Supervised","ViT B / 16","0 . 3211?0 . 071","0 . 747?0 . 0245"],"associatedColumns":["Testing F1 score","Backbone","100%","0 . 9900?0 . 0058"],"associatedMergedColumns":[]}]},{"caption":"Table 5: Results for the ISIC-2019 dataset. Comparable to the official metrics used in the challenge \nhttps://challenge.isic-archive.com/landing/2019/. We use balanced multi-class accu-\nracy as our metric, which is semantically equal to recall value. We observed that CASS consistently \noutperforms DINO by approximately 4% for all label fractions with CNN and Transformer. \n\n","rows":["CASS","ViT B / 16","DINO"],"columns":["Backbone","0 . 2640?0 . 031","1%","0 . 3998?0 . 056","10%","Testing Balanced multi - class accuracy","0 . 328?0 . 0016","0 . 3617?0 . 0047"],"mergedAllColumns":[],"numberCells":[{"number":"0.0465","isBolded":true,"associatedRows":["CASS","ViT B / 16"],"associatedColumns":["Testing Balanced multi - class accuracy","Backbone","10%","0 . 328?0 . 0016","0 . 3617?0 . 0047","0 . 2640?0 . 031","0 . 3998?0 . 056"],"associatedMergedColumns":[]},{"number":"0.3676?","isBolded":false,"associatedRows":["DINO","ViT B / 16"],"associatedColumns":["Testing Balanced multi - class accuracy","Backbone","1%","0 . 328?0 . 0016","0 . 3617?0 . 0047","0 . 2640?0 . 031"],"associatedMergedColumns":[]},{"number":"0.012","isBolded":false,"associatedRows":["DINO","ViT B / 16"],"associatedColumns":["Testing Balanced multi - class accuracy","Backbone","10%","0 . 328?0 . 0016","0 . 3617?0 . 0047","0 . 2640?0 . 031"],"associatedMergedColumns":[]},{"number":"0.3973?","isBolded":true,"associatedRows":["CASS","ViT B / 16"],"associatedColumns":["Testing Balanced multi - class accuracy","Backbone","1%","0 . 328?0 . 0016","0 . 3617?0 . 0047","0 . 2640?0 . 031","0 . 3998?0 . 056"],"associatedMergedColumns":[]}]},{"caption":"Table 10 and 11, we observed that CASS trained ViT Transformer with the same CNN \nconsistently gained approximately 4.7% over its supervised counterpart. Furthermore, from Table 11 \nwe observed that although ViT L/16 performs better than ViT B/16 on ImageNet ( Wightman [2019]\u0027s \nresults), we observed that the trend is opposite on the autoimmune dataset. Hence, the supervised \nperformance of architecture must be considered before pairing it with CASS. \n\nTransformer \nCNN F1 Score Transformer F1 Score \nViT Base/16 \n0.8650?0.001 \n0.8894? 0.005 \nViT Large/16 0.8481?0.001 \n0.853?0.004 \nTable 10: In this table we show performance of CASS for ViT large/16 with ResNet-50 and ViT \nbase/16 with ResNet-50. We observed that CASS trained Transformers on average performed 4.7% \nbetter than their supervised counterparts. \n","rows":["ViT Base / 16","base / 16 with ResNet - 50 . We observed that CASS trained Transformers on average performed","consistently gained approximately","0 . 8650?0 . 001"],"columns":["Table 10 and 11 , we observed that CASS trained ViT Transformer with the same CNN","Transformer F1 Score","0 . 853?0 . 004"],"mergedAllColumns":["Table 10 : In this table we show performance of CASS for ViT large / 16 with ResNet - 50 and ViT","performance of architecture must be considered before pairing it with CASS ."],"numberCells":[{"number":"4.7%overitssupervisedcounterpart.Furthermore,fromTable11","isBolded":false,"associatedRows":["consistently gained approximately"],"associatedColumns":["Table 10 and 11 , we observed that CASS trained ViT Transformer with the same CNN"],"associatedMergedColumns":[]},{"number":"0.8894?","isBolded":false,"associatedRows":["ViT Base / 16","0 . 8650?0 . 001"],"associatedColumns":["Table 10 and 11 , we observed that CASS trained ViT Transformer with the same CNN","Transformer F1 Score"],"associatedMergedColumns":["performance of architecture must be considered before pairing it with CASS ."]},{"number":"4.7%","isBolded":false,"associatedRows":["base / 16 with ResNet - 50 . We observed that CASS trained Transformers on average performed"],"associatedColumns":["Table 10 and 11 , we observed that CASS trained ViT Transformer with the same CNN","Transformer F1 Score","0 . 853?0 . 004"],"associatedMergedColumns":["Table 10 : In this table we show performance of CASS for ViT large / 16 with ResNet - 50 and ViT"]},{"number":"0.005","isBolded":false,"associatedRows":["ViT Base / 16","0 . 8650?0 . 001"],"associatedColumns":["Table 10 and 11 , we observed that CASS trained ViT Transformer with the same CNN","Transformer F1 Score"],"associatedMergedColumns":["performance of architecture must be considered before pairing it with CASS ."]}]},{"caption":"Architecture Testing F1 Score \nResnNet-50 0.819?0.0216 \nViT Base/16 0.8420?0.009 \nViT large/16 0.80495?0.0077 \nTable 11: Supervised performance of ViT family on the autoimmune dataset. We observed that as \nopposed to ImageNet performance, ViT large/16 performs worse than ViT Base/16 on the autoimmune \ndataset. \n\n","rows":["Table","0 . 8680?0 . 001","ResNet - 50 ( 25 . 56M )","ViT Base / 16 ( 86 . 86M )"],"columns":["0 . 80495?0 . 0077","Transformer F1 score","Architecture","Testing F1 Score","0 . 8773?5 . 29e - 5","100% Label Fraction","ViT Base / 16","ViT large / 16","0 . 819?0 . 0216","ResnNet - 50","Changing CNN and keeping the Transformer same","0 . 8420?0 . 009"],"mergedAllColumns":["dataset .","[ 2019 ] doesn \u0027 t have ImageNet initialization hence used random initialization ."],"numberCells":[{"number":"12and","isBolded":false,"associatedRows":["Table"],"associatedColumns":["Architecture","ResnNet - 50","ViT Base / 16","ViT large / 16","Changing CNN and keeping the Transformer same"],"associatedMergedColumns":["dataset ."]},{"number":"0.0005","isBolded":false,"associatedRows":["ResNet - 50 ( 25 . 56M )","ViT Base / 16 ( 86 . 86M )","0 . 8680?0 . 001"],"associatedColumns":["Testing F1 Score","0 . 819?0 . 0216","0 . 8420?0 . 009","0 . 80495?0 . 0077","Changing CNN and keeping the Transformer same","100% Label Fraction","Transformer F1 score","0 . 8773?5 . 29e - 5"],"associatedMergedColumns":["[ 2019 ] doesn \u0027 t have ImageNet initialization hence used random initialization ."]},{"number":"13weobservedthatsimilartochangingTransformerwhilekeepingCNNsame,CASS","isBolded":false,"associatedRows":["Table"],"associatedColumns":["Testing F1 Score","0 . 819?0 . 0216","0 . 8420?0 . 009","0 . 80495?0 . 0077","Changing CNN and keeping the Transformer same"],"associatedMergedColumns":["dataset ."]},{"number":"0.8894?","isBolded":false,"associatedRows":["ResNet - 50 ( 25 . 56M )","ViT Base / 16 ( 86 . 86M )","0 . 8680?0 . 001"],"associatedColumns":["Testing F1 Score","0 . 819?0 . 0216","0 . 8420?0 . 009","0 . 80495?0 . 0077","Changing CNN and keeping the Transformer same","100% Label Fraction","Transformer F1 score","0 . 8773?5 . 29e - 5"],"associatedMergedColumns":["[ 2019 ] doesn \u0027 t have ImageNet initialization hence used random initialization ."]}]},{"caption":"Table 12: F1 metric comparison between the two arms of CASS trained over 100 epochs, following \nthe protocols and procedure listed in Appendix E. The numbers in parentheses show the parameters \nlearned by the network. We use Wightman [2019] implementation of CNN and transformers, with \nImageNet initialisation except for ResNet-200. \n\n","rows":["Table","0 . 8680?0 . 001","ResNet - 50 ( 25 . 56M )","ViT Base / 16 ( 86 . 86M )"],"columns":["Transformer F1 score","0 . 8773?5 . 29e - 5","100% Label Fraction","Changing CNN and keeping the Transformer same"],"mergedAllColumns":["[ 2019 ] doesn \u0027 t have ImageNet initialization hence used random initialization ."],"numberCells":[{"number":"0.0005","isBolded":false,"associatedRows":["ResNet - 50 ( 25 . 56M )","ViT Base / 16 ( 86 . 86M )","0 . 8680?0 . 001"],"associatedColumns":["Changing CNN and keeping the Transformer same","100% Label Fraction","Transformer F1 score","0 . 8773?5 . 29e - 5"],"associatedMergedColumns":["[ 2019 ] doesn \u0027 t have ImageNet initialization hence used random initialization ."]},{"number":"0.8894?","isBolded":false,"associatedRows":["ResNet - 50 ( 25 . 56M )","ViT Base / 16 ( 86 . 86M )","0 . 8680?0 . 001"],"associatedColumns":["Changing CNN and keeping the Transformer same","100% Label Fraction","Transformer F1 score","0 . 8773?5 . 29e - 5"],"associatedMergedColumns":["[ 2019 ] doesn \u0027 t have ImageNet initialization hence used random initialization ."]},{"number":"12and","isBolded":false,"associatedRows":["Table"],"associatedColumns":["Changing CNN and keeping the Transformer same"],"associatedMergedColumns":[]},{"number":"13weobservedthatsimilartochangingTransformerwhilekeepingCNNsame,CASS","isBolded":false,"associatedRows":["Table"],"associatedColumns":["Changing CNN and keeping the Transformer same"],"associatedMergedColumns":[]}]},{"caption":"Architecture Testing F1 Score \nResnNet-18 \n0.8299?0.0004 \nResnNet-50 \n0.831?0.0216 \nResnNet-200 0.823?0.0005 \nTable 13: Supervised performance of the ResNet CNN family on the autoimmune dataset. \n\n","rows":[],"columns":[],"mergedAllColumns":[],"numberCells":[]},{"caption":"Table \n15 displays results for this experimentation. \n\nEpochs CNN F1 Score \nTransformer F1 Score \n50 \n0.9795?0.0109 0.9262?0.0181 \n100 \n0.9909? 0.0032 0.9279? 0.0213 \n200 \n0.9864?0.008 \n0.9476?0.0012 \n300 \n0.9920?0.001 \n0.9484?0.017 \nTable 15: Performance comparison over varied number of epochs, from 50 to 300 epochs, the \ndownstream training procedure and the CNN-transformer combination is kept constant across all the \nfour experiments, only the number of self-supervised epochs has been changed. \n\n","rows":["Performance comparison over varied number of epochs , from 50 to","Table 15 :"],"columns":["CNN F1 Score","Table","0 . 9484?0 . 017","Epochs","Transformer F1 Score","0 . 9795?0 . 0109","50","0 . 9864?0 . 008","0 . 9476?0 . 0012","0 . 9262?0 . 0181"],"mergedAllColumns":["15 displays results for this experimentation ."],"numberCells":[{"number":"0.0032","isBolded":false,"associatedRows":[],"associatedColumns":["Table","CNN F1 Score","0 . 9795?0 . 0109"],"associatedMergedColumns":["15 displays results for this experimentation ."]},{"number":"100","isBolded":false,"associatedRows":[],"associatedColumns":["Table","Epochs","50"],"associatedMergedColumns":["15 displays results for this experimentation ."]},{"number":"0.9279?","isBolded":false,"associatedRows":[],"associatedColumns":["Table","Transformer F1 Score","0 . 9262?0 . 0181"],"associatedMergedColumns":["15 displays results for this experimentation ."]},{"number":"0.0213","isBolded":false,"associatedRows":[],"associatedColumns":["Table","Transformer F1 Score","0 . 9262?0 . 0181"],"associatedMergedColumns":["15 displays results for this experimentation ."]},{"number":"200","isBolded":false,"associatedRows":[],"associatedColumns":["Table","Epochs","50"],"associatedMergedColumns":["15 displays results for this experimentation ."]},{"number":"300","isBolded":false,"associatedRows":[],"associatedColumns":["Table","Epochs","50","0 . 9864?0 . 008"],"associatedMergedColumns":["15 displays results for this experimentation ."]},{"number":"300epochs,the","isBolded":false,"associatedRows":["Table 15 :","Performance comparison over varied number of epochs , from 50 to"],"associatedColumns":["Table","Transformer F1 Score","0 . 9262?0 . 0181","0 . 9476?0 . 0012","0 . 9484?0 . 017"],"associatedMergedColumns":["15 displays results for this experimentation ."]},{"number":"0.9909?","isBolded":false,"associatedRows":[],"associatedColumns":["Table","CNN F1 Score","0 . 9795?0 . 0109"],"associatedMergedColumns":["15 displays results for this experimentation ."]}]},{"caption":"Table 16. We observed that increase in performance of ResNet correlates to \nincrease in performance of Transformer, hence implying that there is information transfer between \nthe two. \n\n","rows":["0 . 9909?0 . 0032","ResNet - 50 ( 25 . 56M )","ViT Base / 16 ( 86 . 86M )"],"columns":["Transformer F1 score","0 . 9801?0 . 007","100% Label Fraction"],"mergedAllColumns":[],"numberCells":[{"number":"0.0213","isBolded":false,"associatedRows":["ResNet - 50 ( 25 . 56M )","ViT Base / 16 ( 86 . 86M )","0 . 9909?0 . 0032"],"associatedColumns":["100% Label Fraction","Transformer F1 score","0 . 9801?0 . 007"],"associatedMergedColumns":[]},{"number":"0.9279?","isBolded":false,"associatedRows":["ResNet - 50 ( 25 . 56M )","ViT Base / 16 ( 86 . 86M )","0 . 9909?0 . 0032"],"associatedColumns":["100% Label Fraction","Transformer F1 score","0 . 9801?0 . 007"],"associatedMergedColumns":[]}]},{"caption":"Table 16: F1 metric comparison between the two arms of CASS trained over 100 epochs, following \nthe protocols and procedure listed in Appendix E. The numbers in parentheses show the parameters \nlearned by the network. We use Wightman [2019] implementation of CNN and transformers, with \nImageNet initialisation except for ResNet-200. \n\n","rows":["0 . 9909?0 . 0032","ResNet - 50 ( 25 . 56M )","ViT Base / 16 ( 86 . 86M )"],"columns":["Transformer F1 score","0 . 9801?0 . 007","100% Label Fraction"],"mergedAllColumns":[],"numberCells":[{"number":"0.0213","isBolded":false,"associatedRows":["ResNet - 50 ( 25 . 56M )","ViT Base / 16 ( 86 . 86M )","0 . 9909?0 . 0032"],"associatedColumns":["100% Label Fraction","Transformer F1 score","0 . 9801?0 . 007"],"associatedMergedColumns":[]},{"number":"0.9279?","isBolded":false,"associatedRows":["ResNet - 50 ( 25 . 56M )","ViT Base / 16 ( 86 . 86M )","0 . 9909?0 . 0032"],"associatedColumns":["100% Label Fraction","Transformer F1 score","0 . 9801?0 . 007"],"associatedMergedColumns":[]}]},{"caption":"Table 17: For the same number of Transformer parameters, DEiT-base with ResNet-50 performed \nmuch better than ResNet-50 with ViT-base. The difference in their CNN arm is 0.10%. On ImageNet \nDEiT-base has a top1% accuracy of 83.106 while ViT-base has an accuracy of 86.006. We use both \nthe Transformers with 16 patches. [ResNet-50 has an accuracy of 80.374] \n\n","rows":["ResNet - 50","0 . 9909?0 . 0032","ViT Base / 16 ( 86 . 86M )"],"columns":["Transformer F1 Score","0 . 9844?0 . 0048"],"mergedAllColumns":[],"numberCells":[{"number":"0.0213","isBolded":false,"associatedRows":["ResNet - 50","ViT Base / 16 ( 86 . 86M )","0 . 9909?0 . 0032"],"associatedColumns":["Transformer F1 Score","0 . 9844?0 . 0048"],"associatedMergedColumns":[]},{"number":"0.9279?","isBolded":false,"associatedRows":["ResNet - 50","ViT Base / 16 ( 86 . 86M )","0 . 9909?0 . 0032"],"associatedColumns":["Transformer F1 Score","0 . 9844?0 . 0048"],"associatedMergedColumns":[]}]},{"caption":"Table 19: We present the results for using Transformers in both the arms and compare the results \nwith CNN-Transformer combination. \n\n","rows":["ResNet - 50","F1 Score of arm","Transfomer","arm","0 . 9909?0 . 0032"],"columns":["Architecture in"],"mergedAllColumns":["F1 Score of ViT - B / 16 arm"],"numberCells":[{"number":"0.9279?","isBolded":false,"associatedRows":["ResNet - 50","0 . 9909?0 . 0032"],"associatedColumns":["Architecture in"],"associatedMergedColumns":["F1 Score of ViT - B / 16 arm"]},{"number":"0.0213","isBolded":false,"associatedRows":["ResNet - 50","0 . 9909?0 . 0032"],"associatedColumns":["Architecture in"],"associatedMergedColumns":["F1 Score of ViT - B / 16 arm"]},{"number":"1","isBolded":false,"associatedRows":["arm"],"associatedColumns":["Architecture in"],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":["ResNet - 50","Transfomer","F1 Score of arm"],"associatedColumns":[],"associatedMergedColumns":[]}]}]