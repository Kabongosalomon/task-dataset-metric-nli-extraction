[{"caption":"System \n\nCoNLL-Aida TAC-KBP 2010 \nSil et al. 2018 \n94.0 \n87.4 \nYamada et al. 2016 \n91.5 \n85.5 \n-entity linking features \n81.1 \n80.1 \nYamada et al. 2017 \n94.3 \n87.7 \nRadhakrishnan et al. 2018 \n93.0 \n89.6 \nRaiman \u0026 Raiman 2018 \n94.9 \n90.9 \nRELIC \n81.9 \n87.5 \nRELIC + CoNLL-Aida tuning \n94.9 4 \n89.8 \n\nTable 1: RELIC achieves comparable precision to best performing dedicated entity-linking sys-\ntems despite using no external resources or task specific features. When given a standard CoNLL-\nAida alias table and tuned on the CoNLL-Aida training set, RELIC\u0027s learned representations match \nthe state-of-the-art DeepType system which relies on the large hand engineered Wikidata knowledge \nbase. \n\n","rows":["RELIC + CoNLL - Aida tuning","- entity linking features","Raiman \u0026 Raiman 2018","Sil et al . 2018","Radhakrishnan et al . 2018","Yamada et al . 2016","Yamada et al . 2017"],"columns":["CoNLL - Aida","TAC - KBP 2010"],"mergedAllColumns":[],"numberCells":[{"number":"81.1","isBolded":false,"associatedRows":["Radhakrishnan et al . 2018","- entity linking features"],"associatedColumns":["CoNLL - Aida"],"associatedMergedColumns":[]},{"number":"85.5","isBolded":false,"associatedRows":["Yamada et al . 2016","- entity linking features"],"associatedColumns":["TAC - KBP 2010"],"associatedMergedColumns":[]},{"number":"94.9","isBolded":false,"associatedRows":["Raiman \u0026 Raiman 2018"],"associatedColumns":["CoNLL - Aida"],"associatedMergedColumns":[]},{"number":"89.8","isBolded":false,"associatedRows":["RELIC + CoNLL - Aida tuning"],"associatedColumns":["TAC - KBP 2010"],"associatedMergedColumns":[]},{"number":"91.5","isBolded":false,"associatedRows":["Yamada et al . 2016","- entity linking features"],"associatedColumns":["CoNLL - Aida"],"associatedMergedColumns":[]},{"number":"89.6","isBolded":false,"associatedRows":["Radhakrishnan et al . 2018"],"associatedColumns":["TAC - KBP 2010"],"associatedMergedColumns":[]},{"number":"87.5","isBolded":false,"associatedRows":["RELIC + CoNLL - Aida tuning"],"associatedColumns":["TAC - KBP 2010"],"associatedMergedColumns":[]},{"number":"81.9","isBolded":false,"associatedRows":["RELIC + CoNLL - Aida tuning"],"associatedColumns":["CoNLL - Aida"],"associatedMergedColumns":[]},{"number":"93.0","isBolded":false,"associatedRows":["Radhakrishnan et al . 2018"],"associatedColumns":["CoNLL - Aida"],"associatedMergedColumns":[]},{"number":"90.9","isBolded":false,"associatedRows":["Raiman \u0026 Raiman 2018"],"associatedColumns":["TAC - KBP 2010"],"associatedMergedColumns":[]},{"number":"87.4","isBolded":false,"associatedRows":["Sil et al . 2018","- entity linking features"],"associatedColumns":["TAC - KBP 2010"],"associatedMergedColumns":[]},{"number":"94.3","isBolded":false,"associatedRows":["Yamada et al . 2017"],"associatedColumns":["CoNLL - Aida"],"associatedMergedColumns":[]},{"number":"80.1","isBolded":false,"associatedRows":["RELIC + CoNLL - Aida tuning","- entity linking features"],"associatedColumns":["TAC - KBP 2010"],"associatedMergedColumns":[]},{"number":"94.0","isBolded":false,"associatedRows":["Sil et al . 2018","- entity linking features"],"associatedColumns":["CoNLL - Aida"],"associatedMergedColumns":[]},{"number":"87.7","isBolded":false,"associatedRows":["Yamada et al . 2017"],"associatedColumns":["TAC - KBP 2010"],"associatedMergedColumns":[]},{"number":"94.94","isBolded":false,"associatedRows":["RELIC + CoNLL - Aida tuning"],"associatedColumns":["CoNLL - Aida"],"associatedMergedColumns":[]}]},{"caption":"Table 2: Performance on FIGMENT. We report P@1 (proportion of entities whose top ranked types \nare correct), Micro F1 aggregated over all (entity, type) compatibility decisions, and overall accuracy \nof entity labeling decisions. RELIC outperforms prior work, even with only 5% of the training data. \n\n","rows":["RELIC","Yaghoobzadeh et al . 2018","RELIC with 5% of FIGMENT training data"],"columns":["P@1","Acc","F1"],"mergedAllColumns":[],"numberCells":[{"number":"91.0","isBolded":false,"associatedRows":["Yaghoobzadeh et al . 2018"],"associatedColumns":["P@1"],"associatedMergedColumns":[]},{"number":"94.8","isBolded":false,"associatedRows":["RELIC"],"associatedColumns":["P@1"],"associatedMergedColumns":[]},{"number":"87.9","isBolded":false,"associatedRows":["RELIC"],"associatedColumns":["F1"],"associatedMergedColumns":[]},{"number":"59.3","isBolded":false,"associatedRows":["RELIC with 5% of FIGMENT training data"],"associatedColumns":["Acc"],"associatedMergedColumns":[]},{"number":"82.3","isBolded":false,"associatedRows":["Yaghoobzadeh et al . 2018"],"associatedColumns":["F1"],"associatedMergedColumns":[]},{"number":"56.5","isBolded":false,"associatedRows":["Yaghoobzadeh et al . 2018"],"associatedColumns":["Acc"],"associatedMergedColumns":[]},{"number":"68.3","isBolded":false,"associatedRows":["RELIC"],"associatedColumns":["Acc"],"associatedMergedColumns":[]},{"number":"83.3","isBolded":false,"associatedRows":["RELIC with 5% of FIGMENT training data"],"associatedColumns":["F1"],"associatedMergedColumns":[]},{"number":"90.9","isBolded":false,"associatedRows":["RELIC with 5% of FIGMENT training data"],"associatedColumns":["P@1"],"associatedMergedColumns":[]}]},{"caption":"Yamada Subset \n\nAll Entities \nTypeNet Wikipedia TypeNet Wikipedia \n# Entities \n291,663 \n707,588 \n323,347 3,667,933 \nRandom \n2.7 \n0.1 \n2.5 \n0.1 \nYamada et al. 2017 \n25.9 \n8.0 \n-\n-\nRELIC \n27.8 \n21.0 \n29.3 \n13.8 \n\nTable 4: Mean average precision on exemplar-based category completion (Section 5.4). The Yamada \nsubset is filtered to only contain entities that are covered by Yamada et al. 2017, and categories are \nfiltered to those which contain at least 300 entities (131 categories). For the \"All Entities\" setting, \nwe use all Wikipedia entities covered by RELIC, and filter to categories which contain at least 1000 \nentities (1083 categories). The embeddings learned by Yamada et al. 2017 are competitive with \nRELIC on the task of populating TypeNet categories, but they are much worse at capturing the \ncomplex, and compound, typing information present in Wikipedia categories. \n\n","rows":["Random","RELIC","Yamada et al . 2017"],"columns":["291 , 663","707 , 588","323 , 347","Wikipedia","TypeNet","3 , 667 , 933","-","All Entities","Yamada Subset"],"mergedAllColumns":[],"numberCells":[{"number":"2.7","isBolded":false,"associatedRows":["Random"],"associatedColumns":["Yamada Subset","TypeNet","291 , 663"],"associatedMergedColumns":[]},{"number":"29.3","isBolded":false,"associatedRows":["RELIC"],"associatedColumns":["All Entities","TypeNet","323 , 347","-"],"associatedMergedColumns":[]},{"number":"27.8","isBolded":false,"associatedRows":["RELIC"],"associatedColumns":["Yamada Subset","TypeNet","291 , 663","-"],"associatedMergedColumns":[]},{"number":"21.0","isBolded":false,"associatedRows":["RELIC"],"associatedColumns":["Yamada Subset","Wikipedia","707 , 588","-"],"associatedMergedColumns":[]},{"number":"25.9","isBolded":false,"associatedRows":["Yamada et al . 2017"],"associatedColumns":["Yamada Subset","TypeNet","291 , 663"],"associatedMergedColumns":[]},{"number":"13.8","isBolded":false,"associatedRows":["RELIC"],"associatedColumns":["All Entities","Wikipedia","3 , 667 , 933","-"],"associatedMergedColumns":[]},{"number":"0.1","isBolded":false,"associatedRows":["Random"],"associatedColumns":["All Entities","Wikipedia","3 , 667 , 933"],"associatedMergedColumns":[]},{"number":"0.1","isBolded":false,"associatedRows":["Random"],"associatedColumns":["Yamada Subset","Wikipedia","707 , 588"],"associatedMergedColumns":[]},{"number":"8.0","isBolded":false,"associatedRows":["Yamada et al . 2017"],"associatedColumns":["Yamada Subset","Wikipedia","707 , 588"],"associatedMergedColumns":[]},{"number":"2.5","isBolded":false,"associatedRows":["Random"],"associatedColumns":["All Entities","TypeNet","323 , 347"],"associatedMergedColumns":[]}]},{"caption":"Table 5: Answer exact match on TriviaQA. RELIC\u0027s fast nearest neighbor search over entities \nachieves 80% of the performance of ORQA, which runs a BERT-based reading comprehesion model \nover multiple retrieved evidence passages. Unlike ORQA and RELIC, the classifier baseline and \nSLQA have access to a single evidence document that is known to contain the answer. As a result \nthey are solving a much easier task. \n\n","rows":["Classifier baseline ( Joshi et al . , 2017 )","SLQA ( Wang et al . , 2018b )","RELIC","-","ORQA ( Lee et al . , 2019 )"],"columns":["Verified Web","Open - domain Unfiltered"],"mergedAllColumns":[],"numberCells":[{"number":"82.4","isBolded":false,"associatedRows":["SLQA ( Wang et al . , 2018b )","-"],"associatedColumns":["Verified Web"],"associatedMergedColumns":[]},{"number":"51.2","isBolded":false,"associatedRows":["RELIC","-"],"associatedColumns":["Verified Web"],"associatedMergedColumns":[]},{"number":"45.0","isBolded":false,"associatedRows":["ORQA ( Lee et al . , 2019 )"],"associatedColumns":["Open - domain Unfiltered"],"associatedMergedColumns":[]},{"number":"30.2","isBolded":false,"associatedRows":["Classifier baseline ( Joshi et al . , 2017 )","-"],"associatedColumns":["Verified Web"],"associatedMergedColumns":[]},{"number":"35.7","isBolded":false,"associatedRows":["RELIC"],"associatedColumns":["Open - domain Unfiltered"],"associatedMergedColumns":[]}]}]