[{"caption":"Table 1: Top-1 validation accuracy comparisons. variants were trained longer (see Table 2 ) \n\nModel \nC-10 \nC-100 \nFashion MNIST # Params \nMACs \n\nConvolutional Networks (Designed for ImageNet) \n\nResNet18 \n90.27% 66.46% 94.78% 99.80% \n11.18 M \n0.04 G \nResNet34 \n90.51% 66.84% 94.78% 99.77% \n21.29 M \n0.08 G \n\nMobileNetV2/0.5 \n84.78% 56.32% 93.93% 99.70% \n0.70 M \n\u003c 0.01 G \nMobileNetV2/2.0 \n91.02% 67.44% 95.26% 99.75% \n8.72 M \n0.02 G \n\nConvolutional Networks (Designed for CIFAR) \n\nResNet56[16] \n94.63% 74.81% 95.25% 99.27% \n0.85 M \n0.13 G \nResNet110[16] \n95.08% 76.63% 95.32% 99.28% \n1.73 M \n0.26 G \nResNet1k-v2 [17] 95.38% \n? \n? \n? \n10.33 M \n1.55 G \nProxyless-G[5] \n97.92% \n? \n? \n? \n5.7 M \n? \n\nVision Transformers \n\nViT-12/16 \n83.04% 57.97% 93.61% 99.63% \n85.63 M \n0.43 G \n\nViT-Lite-7/16 \n78.45% 52.87% 93.24% 99.68% \n3.89 M \n0.02 G \nViT-Lite-7/8 \n89.10% 67.27% 94.49% 99.69% \n3.74 M \n0.06 G \nViT-Lite-7/4 \n93.57% 73.94% 95.16% 99.77% \n3.72 M \n0.26 G \n\nCompact Vision Transformers \n\nCVT-7/8 \n89.79% 70.11% 94.50% 99.70% \n3.74 M \n0.06 G \nCVT-7/4 \n94.01% 76.49% 95.32% 99.76% \n3.72 M \n0.25 G \n\nCompact Convolutional Transformers \n\nCCT-2/3?2 \n89.75% 66.93% 94.08% 99.70% \n0.28 M \n0.04 G \nCCT-7/3?2 \n95.04% 77.72% 95.16% 99.76% \n3.85 M \n0.29 G \n\nCCT-7/3?1 \n96.53% 80.92% 95.56% 99.82% \n3.76 M \n1.19 G \nCCT-7/3?1 \n98.00% 82.72% \n? \n? \n3.76 M \n1.19 G \n\nTable 2: CCT-7/3?1 top-1 accuracy on CIFAR-10/100 \nwhen trained longer \n\n# Epochs Pos. Emb. CIFAR-10 CIFAR-100 \n\n300 \nLearnable \n96.53% \n80.92% \n1500 \nSinusoidal 97.48% \n82.72% \n5000 \nSinusoidal 98.00% \n82.87% \n\n","rows":["ResNet1k - v2 [ 17 ]","ViT - 12 / 16","ViT - Lite - 7 / 16","CVT - 7 / 4","5000","CVT - 7 / 8","Proxyless - G [ 5 ]","ResNet18","MobileNetV2 / 0 . 5","ResNet110 [ 16 ]","ViT - Lite - 7 / 4","?","ResNet56 [ 16 ]","CCT - 7 / 3?1","300","ResNet34","1500","CCT - 7 / 3?2","Learnable","CCT - 2 / 3?2","MobileNetV2 / 2 . 0","Sinusoidal","\u003c","ViT - Lite - 7 / 8"],"columns":["CIFAR - 10 / 100","C - 10","# Params","C - 100","Fashion","MACs","MNIST","accuracy","CIFAR - 10","variants were trained longer ( see Table 2 )","Table 1 : Top - 1 validation accuracy comparisons .","CIFAR - 100"],"mergedAllColumns":["Compact Convolutional Transformers","Compact Vision Transformers","Convolutional Networks ( Designed for CIFAR )","when trained longer","Vision Transformers","Convolutional Networks ( Designed for ImageNet )"],"numberCells":[{"number":"78.45%","isBolded":false,"associatedRows":["300","ViT - Lite - 7 / 16"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10"],"associatedMergedColumns":["Vision Transformers"]},{"number":"89.75%","isBolded":false,"associatedRows":["300","CCT - 2 / 3?2"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"95.26%","isBolded":false,"associatedRows":["300","MobileNetV2 / 2 . 0"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","Fashion"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"10.33M","isBolded":false,"associatedRows":["300","ResNet1k - v2 [ 17 ]","?","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"0.28M","isBolded":true,"associatedRows":["300","CCT - 2 / 3?2","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","# Params"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"99.76%","isBolded":false,"associatedRows":["300","CVT - 7 / 4","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MNIST"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"3.76M","isBolded":false,"associatedRows":["1500","CCT - 7 / 3?1","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","# Params"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"99.75%","isBolded":false,"associatedRows":["300","MobileNetV2 / 2 . 0","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MNIST"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"0.70M","isBolded":false,"associatedRows":["300","MobileNetV2 / 0 . 5","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"99.28%","isBolded":false,"associatedRows":["300","ResNet110 [ 16 ]","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MNIST"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"93.57%","isBolded":false,"associatedRows":["300","ViT - Lite - 7 / 4"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10"],"associatedMergedColumns":["Vision Transformers"]},{"number":"66.93%","isBolded":false,"associatedRows":["300","CCT - 2 / 3?2"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 100"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"3.89M","isBolded":false,"associatedRows":["300","ViT - Lite - 7 / 16","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","# Params"],"associatedMergedColumns":["Vision Transformers"]},{"number":"99.69%","isBolded":false,"associatedRows":["300","ViT - Lite - 7 / 8","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MNIST"],"associatedMergedColumns":["Vision Transformers"]},{"number":"95.32%","isBolded":false,"associatedRows":["300","CVT - 7 / 4"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","Fashion"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"82.87%","isBolded":true,"associatedRows":["5000","Sinusoidal"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10","CIFAR - 10 / 100","CIFAR - 100"],"associatedMergedColumns":["when trained longer"]},{"number":"11.18M","isBolded":false,"associatedRows":["300","ResNet18","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"67.44%","isBolded":false,"associatedRows":["300","MobileNetV2 / 2 . 0"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 100"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"99.70%","isBolded":false,"associatedRows":["300","CVT - 7 / 8","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MNIST"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"21.29M","isBolded":false,"associatedRows":["300","ResNet34","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"93.61%","isBolded":false,"associatedRows":["300","ViT - 12 / 16"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","Fashion"],"associatedMergedColumns":["Vision Transformers"]},{"number":"94.63%","isBolded":false,"associatedRows":["300","ResNet56 [ 16 ]"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"74.81%","isBolded":false,"associatedRows":["300","ResNet56 [ 16 ]"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 100"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"67.27%","isBolded":false,"associatedRows":["300","ViT - Lite - 7 / 8"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 100"],"associatedMergedColumns":["Vision Transformers"]},{"number":"3.72M","isBolded":false,"associatedRows":["300","ViT - Lite - 7 / 4","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","# Params"],"associatedMergedColumns":["Vision Transformers"]},{"number":"1.19G","isBolded":false,"associatedRows":["1500","CCT - 7 / 3?1","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MACs"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"99.77%","isBolded":false,"associatedRows":["300","ResNet34","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MNIST"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"93.93%","isBolded":false,"associatedRows":["300","MobileNetV2 / 0 . 5"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","Fashion"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"0.13G","isBolded":false,"associatedRows":["300","ResNet56 [ 16 ]","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"0.06G","isBolded":false,"associatedRows":["300","ViT - Lite - 7 / 8","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MACs"],"associatedMergedColumns":["Vision Transformers"]},{"number":"0.25G","isBolded":false,"associatedRows":["300","CVT - 7 / 4","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MACs"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"0.26G","isBolded":false,"associatedRows":["300","ViT - Lite - 7 / 4","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MACs"],"associatedMergedColumns":["Vision Transformers"]},{"number":"80.92%","isBolded":false,"associatedRows":["300","CCT - 7 / 3?1"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 100"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"0.04G","isBolded":false,"associatedRows":["1500","CCT - 2 / 3?2","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MACs"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"95.56%","isBolded":true,"associatedRows":["300","CCT - 7 / 3?1"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","Fashion"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"95.04%","isBolded":false,"associatedRows":["300","CCT - 7 / 3?2"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"99.76%","isBolded":false,"associatedRows":["300","CCT - 7 / 3?2","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MNIST"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"95.32%","isBolded":false,"associatedRows":["300","ResNet110 [ 16 ]"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","Fashion"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"73.94%","isBolded":false,"associatedRows":["300","ViT - Lite - 7 / 4"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 100"],"associatedMergedColumns":["Vision Transformers"]},{"number":"99.82%","isBolded":true,"associatedRows":["300","CCT - 7 / 3?1","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MNIST"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"8.72M","isBolded":false,"associatedRows":["300","MobileNetV2 / 2 . 0","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"97.48%","isBolded":false,"associatedRows":["1500","Sinusoidal"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10","accuracy","CIFAR - 10"],"associatedMergedColumns":["when trained longer"]},{"number":"90.27%","isBolded":false,"associatedRows":["300","ResNet18"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"84.78%","isBolded":false,"associatedRows":["300","MobileNetV2 / 0 . 5"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"99.63%","isBolded":false,"associatedRows":["300","ViT - 12 / 16","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MNIST"],"associatedMergedColumns":["Vision Transformers"]},{"number":"1.19G","isBolded":false,"associatedRows":["1500","CCT - 7 / 3?1","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MACs"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"1.73M","isBolded":false,"associatedRows":["300","ResNet110 [ 16 ]","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"98.00%","isBolded":true,"associatedRows":["5000","Sinusoidal"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10","accuracy","CIFAR - 10"],"associatedMergedColumns":["when trained longer"]},{"number":"95.08%","isBolded":false,"associatedRows":["300","ResNet110 [ 16 ]"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"3.74M","isBolded":false,"associatedRows":["300","ViT - Lite - 7 / 8","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","# Params"],"associatedMergedColumns":["Vision Transformers"]},{"number":"94.78%","isBolded":false,"associatedRows":["300","ResNet18"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","Fashion"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"52.87%","isBolded":false,"associatedRows":["300","ViT - Lite - 7 / 16"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 100"],"associatedMergedColumns":["Vision Transformers"]},{"number":"0.02G","isBolded":false,"associatedRows":["300","MobileNetV2 / 2 . 0","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"0.26G","isBolded":false,"associatedRows":["300","ResNet110 [ 16 ]","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"0.08G","isBolded":false,"associatedRows":["300","ResNet34","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"3.74M","isBolded":false,"associatedRows":["300","CVT - 7 / 8","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","# Params"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"77.72%","isBolded":false,"associatedRows":["300","CCT - 7 / 3?2"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 100"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"85.63M","isBolded":false,"associatedRows":["300","ViT - 12 / 16","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","# Params"],"associatedMergedColumns":["Vision Transformers"]},{"number":"94.50%","isBolded":false,"associatedRows":["300","CVT - 7 / 8"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","Fashion"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"96.53%","isBolded":false,"associatedRows":["300","CCT - 7 / 3?1"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"82.72%","isBolded":false,"associatedRows":["1500","Sinusoidal"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10","CIFAR - 10 / 100","CIFAR - 100"],"associatedMergedColumns":["when trained longer"]},{"number":"57.97%","isBolded":false,"associatedRows":["300","ViT - 12 / 16"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 100"],"associatedMergedColumns":["Vision Transformers"]},{"number":"95.16%","isBolded":false,"associatedRows":["300","CCT - 7 / 3?2"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","Fashion"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"94.78%","isBolded":false,"associatedRows":["300","ResNet34"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","Fashion"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"76.63%","isBolded":false,"associatedRows":["300","ResNet110 [ 16 ]"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 100"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"0.43G","isBolded":false,"associatedRows":["300","ViT - 12 / 16","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MACs"],"associatedMergedColumns":["Vision Transformers"]},{"number":"3.76M","isBolded":false,"associatedRows":["1500","CCT - 7 / 3?1","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","# Params"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"99.27%","isBolded":false,"associatedRows":["300","ResNet56 [ 16 ]","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MNIST"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"93.24%","isBolded":false,"associatedRows":["300","ViT - Lite - 7 / 16"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","Fashion"],"associatedMergedColumns":["Vision Transformers"]},{"number":"99.80%","isBolded":false,"associatedRows":["300","ResNet18","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MNIST"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"99.68%","isBolded":false,"associatedRows":["300","ViT - Lite - 7 / 16","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MNIST"],"associatedMergedColumns":["Vision Transformers"]},{"number":"89.10%","isBolded":false,"associatedRows":["300","ViT - Lite - 7 / 8"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10"],"associatedMergedColumns":["Vision Transformers"]},{"number":"82.72%","isBolded":true,"associatedRows":["300","CCT - 7 / 3?1"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 100"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"0.04G","isBolded":false,"associatedRows":["300","ResNet18","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"97.92%","isBolded":false,"associatedRows":["300","Proxyless - G [ 5 ]"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"3.72M","isBolded":false,"associatedRows":["300","CVT - 7 / 4","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","# Params"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"80.92%","isBolded":false,"associatedRows":["300","Learnable"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10","CIFAR - 10 / 100","CIFAR - 100"],"associatedMergedColumns":["when trained longer"]},{"number":"95.38%","isBolded":false,"associatedRows":["300","ResNet1k - v2 [ 17 ]"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"99.70%","isBolded":false,"associatedRows":["300","MobileNetV2 / 0 . 5","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MNIST"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"99.70%","isBolded":false,"associatedRows":["300","CCT - 2 / 3?2","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MNIST"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"96.53%","isBolded":false,"associatedRows":["300","Learnable"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10","accuracy","CIFAR - 10"],"associatedMergedColumns":["when trained longer"]},{"number":"83.04%","isBolded":false,"associatedRows":["300","ViT - 12 / 16"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10"],"associatedMergedColumns":["Vision Transformers"]},{"number":"5.7M","isBolded":false,"associatedRows":["300","Proxyless - G [ 5 ]","?","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"0.06G","isBolded":false,"associatedRows":["300","CVT - 7 / 8","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MACs"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"98.00%","isBolded":true,"associatedRows":["300","CCT - 7 / 3?1"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"94.49%","isBolded":false,"associatedRows":["300","ViT - Lite - 7 / 8"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","Fashion"],"associatedMergedColumns":["Vision Transformers"]},{"number":"1.55G","isBolded":false,"associatedRows":["300","ResNet1k - v2 [ 17 ]","?","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"0.85M","isBolded":false,"associatedRows":["300","ResNet56 [ 16 ]","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"66.46%","isBolded":false,"associatedRows":["300","ResNet18"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 100"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"76.49%","isBolded":false,"associatedRows":["300","CVT - 7 / 4"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 100"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"99.77%","isBolded":false,"associatedRows":["300","ViT - Lite - 7 / 4","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MNIST"],"associatedMergedColumns":["Vision Transformers"]},{"number":"91.02%","isBolded":false,"associatedRows":["300","MobileNetV2 / 2 . 0"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"95.16%","isBolded":false,"associatedRows":["300","ViT - Lite - 7 / 4"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","Fashion"],"associatedMergedColumns":["Vision Transformers"]},{"number":"70.11%","isBolded":false,"associatedRows":["300","CVT - 7 / 8"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 100"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"56.32%","isBolded":false,"associatedRows":["300","MobileNetV2 / 0 . 5"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 100"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"95.25%","isBolded":false,"associatedRows":["300","ResNet56 [ 16 ]"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","Fashion"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"94.01%","isBolded":false,"associatedRows":["300","CVT - 7 / 4"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"0.29G","isBolded":false,"associatedRows":["1500","CCT - 7 / 3?2","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MACs"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"3.85M","isBolded":false,"associatedRows":["1500","CCT - 7 / 3?2","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","# Params"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"66.84%","isBolded":false,"associatedRows":["300","ResNet34"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 100"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"0.02G","isBolded":false,"associatedRows":["300","ViT - Lite - 7 / 16","?","?"],"associatedColumns":["variants were trained longer ( see Table 2 )","MACs"],"associatedMergedColumns":["Vision Transformers"]},{"number":"89.79%","isBolded":false,"associatedRows":["300","CVT - 7 / 8"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"0.01G","isBolded":true,"associatedRows":["300","MobileNetV2 / 0 . 5","?","?","\u003c"],"associatedColumns":["variants were trained longer ( see Table 2 )","MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"94.08%","isBolded":false,"associatedRows":["300","CCT - 2 / 3?2"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","Fashion"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"90.51%","isBolded":false,"associatedRows":["300","ResNet34"],"associatedColumns":["Table 1 : Top - 1 validation accuracy comparisons .","C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]}]},{"caption":"Table 3: ImageNet Top-1 validation accuracy comparison (no extra data or pretraining). This shows that larger variants of \nCCT could also be applicable to medium-sized datasets \n\nModel \nTop-1 \n# Params MACs Training Epochs \n\nResNet50 [16] \n77.15% \n25.55 M \n4.15 G \n120 \nResNet50 (2021) [44] 79.80% \n25.55 M \n4.15 G \n300 \nViT-S [19] \n79.85% \n22.05 M \n4.61 G \n300 \nCCT-14/7?2 \n80.67% \n22.36 M \n5.53 G \n300 \n\nDeiT-S [19] \n81.16% \n22.44M \n4.63 G \n300 \nCCT-14/7?2 Distilled 81.34% \n22.36 M \n5.53 G \n300 \n\n","rows":["ImageNet - 1k","CCT - 14 / 7?2 Distilled","JFT - 300M","DeiT - S [ 19 ]","DeiT - B","ResNet50 [ 16 ]","ViT - S [ 19 ]","ViT - L / 16","-","ResNet50 ( 2021 ) [ 44 ]","ViT - H / 14","CCT - 14 / 7?2"],"columns":["# Params","Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","MACs","CCT outperforms other competitive models , having signifi -","Top - 1","Training Epochs","Table 4 : Flowers - 102 Top - 1 validation accuracy comparison .","Resolution"],"mergedAllColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images","CCT could also be applicable to medium - sized datasets","Appendix E . Unless stated otherwise , all tests were run for"],"numberCells":[{"number":"79.80%","isBolded":false,"associatedRows":["ResNet50 ( 2021 ) [ 44 ]"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"22.36M","isBolded":false,"associatedRows":["CCT - 14 / 7?2","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"99.76%","isBolded":true,"associatedRows":["CCT - 14 / 7?2","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params","CCT outperforms other competitive models , having signifi -","Top - 1"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"384","isBolded":true,"associatedRows":["CCT - 14 / 7?2"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1","Table 4 : Flowers - 102 Top - 1 validation accuracy comparison .","Resolution"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"97.19%","isBolded":false,"associatedRows":["CCT - 14 / 7?2","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params","CCT outperforms other competitive models , having signifi -","Top - 1"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"22.36M","isBolded":true,"associatedRows":["CCT - 14 / 7?2 Distilled","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"55.68G","isBolded":false,"associatedRows":["DeiT - B","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs","CCT outperforms other competitive models , having signifi -","MACs"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"81.16%","isBolded":false,"associatedRows":["DeiT - S [ 19 ]"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"384","isBolded":true,"associatedRows":["ViT - H / 14"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1","Table 4 : Flowers - 102 Top - 1 validation accuracy comparison .","Resolution"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"300","isBolded":false,"associatedRows":["CCT - 14 / 7?2 Distilled","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"5.53G","isBolded":false,"associatedRows":["CCT - 14 / 7?2 Distilled","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","MACs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"22.05M","isBolded":true,"associatedRows":["ViT - S [ 19 ]","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"80.67%","isBolded":true,"associatedRows":["CCT - 14 / 7?2"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"300","isBolded":false,"associatedRows":["ResNet50 ( 2021 ) [ 44 ]","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"5.53G","isBolded":false,"associatedRows":["CCT - 14 / 7?2","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","MACs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"22.17M","isBolded":true,"associatedRows":["CCT - 14 / 7?2","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs","CCT outperforms other competitive models , having signifi -","# Params"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"191.30G","isBolded":false,"associatedRows":["ViT - L / 16","JFT - 300M"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs","CCT outperforms other competitive models , having signifi -","MACs"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"79.85%","isBolded":false,"associatedRows":["ViT - S [ 19 ]"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"661.00M","isBolded":false,"associatedRows":["ViT - H / 14","JFT - 300M"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","MACs","CCT outperforms other competitive models , having signifi -","# Params"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"98.80%","isBolded":false,"associatedRows":["DeiT - B","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params","CCT outperforms other competitive models , having signifi -","Top - 1"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"304.71M","isBolded":false,"associatedRows":["ViT - L / 16","JFT - 300M"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","MACs","CCT outperforms other competitive models , having signifi -","# Params"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"22.44M","isBolded":false,"associatedRows":["DeiT - S [ 19 ]","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"99.74%","isBolded":false,"associatedRows":["ViT - L / 16","JFT - 300M"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params","CCT outperforms other competitive models , having signifi -","Top - 1"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"384","isBolded":true,"associatedRows":["ViT - L / 16"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1","Table 4 : Flowers - 102 Top - 1 validation accuracy comparison .","Resolution"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"18.63G","isBolded":true,"associatedRows":["CCT - 14 / 7?2","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs","CCT outperforms other competitive models , having signifi -","MACs"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"25.55M","isBolded":false,"associatedRows":["ResNet50 ( 2021 ) [ 44 ]","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"4.15G","isBolded":false,"associatedRows":["ResNet50 [ 16 ]","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","MACs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"504.00G","isBolded":false,"associatedRows":["ViT - H / 14","JFT - 300M"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs","CCT outperforms other competitive models , having signifi -","MACs"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"81.34%","isBolded":true,"associatedRows":["CCT - 14 / 7?2 Distilled"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"120","isBolded":false,"associatedRows":["ResNet50 [ 16 ]","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"300","isBolded":false,"associatedRows":["DeiT - S [ 19 ]","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"4.61G","isBolded":true,"associatedRows":["ViT - S [ 19 ]","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","MACs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"25.55M","isBolded":false,"associatedRows":["ResNet50 [ 16 ]","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"300","isBolded":false,"associatedRows":["ViT - S [ 19 ]","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"224","isBolded":true,"associatedRows":["CCT - 14 / 7?2"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1","Table 4 : Flowers - 102 Top - 1 validation accuracy comparison .","Resolution"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"22.17M","isBolded":false,"associatedRows":["CCT - 14 / 7?2","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs","CCT outperforms other competitive models , having signifi -","# Params"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"99.68%","isBolded":false,"associatedRows":["ViT - H / 14","JFT - 300M"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params","CCT outperforms other competitive models , having signifi -","Top - 1"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"4.63G","isBolded":true,"associatedRows":["DeiT - S [ 19 ]","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","MACs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"300","isBolded":false,"associatedRows":["CCT - 14 / 7?2","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"384","isBolded":true,"associatedRows":["DeiT - B"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1","Table 4 : Flowers - 102 Top - 1 validation accuracy comparison .","Resolution"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"77.15%","isBolded":false,"associatedRows":["ResNet50 [ 16 ]"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"4.15G","isBolded":false,"associatedRows":["ResNet50 ( 2021 ) [ 44 ]","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","MACs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"300epochs,andthelearningrateisreducedperepochbased","isBolded":true,"associatedRows":[],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1","Table 4 : Flowers - 102 Top - 1 validation accuracy comparison .","Resolution"],"associatedMergedColumns":["Appendix E . Unless stated otherwise , all tests were run for"]},{"number":"86.25M","isBolded":false,"associatedRows":["DeiT - B","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs","CCT outperforms other competitive models , having signifi -","# Params"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"18.63G","isBolded":false,"associatedRows":["CCT - 14 / 7?2","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs","CCT outperforms other competitive models , having signifi -","MACs"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]}]},{"caption":"Table 4: Flowers-102 Top-1 validation accuracy comparison. CCT outperforms other competitive models, having signifi-\ncantly fewer parameters and GMACs. This demonstrates the compactness on small datasets even with large images \n\nModel \nResolution Pretraining \nTop-1 \n# Params \nMACs \n\nCCT-14/7?2 \n224 \n-\n97.19% \n22.17 M \n18.63 G \n\nDeiT-B \n384 \nImageNet-1k 98.80% \n86.25 M \n55.68 G \nViT-L/16 \n384 \nJFT-300M \n99.74% 304.71 M 191.30 G \nViT-H/14 \n384 \nJFT-300M \n99.68% 661.00 M 504.00 G \nCCT-14/7?2 \n384 \nImageNet-1k 99.76% \n22.17 M \n18.63 G \n\ncode. We also provide a report on hyperparamter settings in \nAppendix E. Unless stated otherwise, all tests were run for \n300 epochs, and the learning rate is reduced per epoch based \non cosine annealing [26]. All transformer based models \n(ViT-Lite, CVT, and CCT) were trained using the AdamW \noptimizer. \n\n","rows":["ImageNet - 1k","CCT - 14 / 7?2 Distilled","JFT - 300M","DeiT - S [ 19 ]","DeiT - B","ResNet50 [ 16 ]","ViT - S [ 19 ]","ViT - L / 16","-","ResNet50 ( 2021 ) [ 44 ]","ViT - H / 14","CCT - 14 / 7?2"],"columns":["# Params","Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","MACs","CCT outperforms other competitive models , having signifi -","Top - 1","Training Epochs","Table 4 : Flowers - 102 Top - 1 validation accuracy comparison .","Resolution"],"mergedAllColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images","CCT could also be applicable to medium - sized datasets","Appendix E . Unless stated otherwise , all tests were run for"],"numberCells":[{"number":"81.16%","isBolded":false,"associatedRows":["DeiT - S [ 19 ]"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"5.53G","isBolded":false,"associatedRows":["CCT - 14 / 7?2","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","MACs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"22.44M","isBolded":false,"associatedRows":["DeiT - S [ 19 ]","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"81.34%","isBolded":true,"associatedRows":["CCT - 14 / 7?2 Distilled"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"384","isBolded":true,"associatedRows":["ViT - L / 16"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1","Table 4 : Flowers - 102 Top - 1 validation accuracy comparison .","Resolution"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"25.55M","isBolded":false,"associatedRows":["ResNet50 [ 16 ]","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"300","isBolded":false,"associatedRows":["ResNet50 ( 2021 ) [ 44 ]","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"191.30G","isBolded":false,"associatedRows":["ViT - L / 16","JFT - 300M"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs","CCT outperforms other competitive models , having signifi -","MACs"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"22.36M","isBolded":false,"associatedRows":["CCT - 14 / 7?2","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"224","isBolded":true,"associatedRows":["CCT - 14 / 7?2"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1","Table 4 : Flowers - 102 Top - 1 validation accuracy comparison .","Resolution"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"504.00G","isBolded":false,"associatedRows":["ViT - H / 14","JFT - 300M"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs","CCT outperforms other competitive models , having signifi -","MACs"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"77.15%","isBolded":false,"associatedRows":["ResNet50 [ 16 ]"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"4.61G","isBolded":true,"associatedRows":["ViT - S [ 19 ]","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","MACs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"99.74%","isBolded":false,"associatedRows":["ViT - L / 16","JFT - 300M"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params","CCT outperforms other competitive models , having signifi -","Top - 1"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"22.17M","isBolded":true,"associatedRows":["CCT - 14 / 7?2","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs","CCT outperforms other competitive models , having signifi -","# Params"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"384","isBolded":true,"associatedRows":["DeiT - B"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1","Table 4 : Flowers - 102 Top - 1 validation accuracy comparison .","Resolution"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"80.67%","isBolded":true,"associatedRows":["CCT - 14 / 7?2"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"300","isBolded":false,"associatedRows":["ViT - S [ 19 ]","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"300","isBolded":false,"associatedRows":["CCT - 14 / 7?2","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"22.36M","isBolded":true,"associatedRows":["CCT - 14 / 7?2 Distilled","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"18.63G","isBolded":false,"associatedRows":["CCT - 14 / 7?2","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs","CCT outperforms other competitive models , having signifi -","MACs"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"4.63G","isBolded":true,"associatedRows":["DeiT - S [ 19 ]","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","MACs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"22.17M","isBolded":false,"associatedRows":["CCT - 14 / 7?2","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs","CCT outperforms other competitive models , having signifi -","# Params"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"55.68G","isBolded":false,"associatedRows":["DeiT - B","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs","CCT outperforms other competitive models , having signifi -","MACs"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"99.68%","isBolded":false,"associatedRows":["ViT - H / 14","JFT - 300M"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params","CCT outperforms other competitive models , having signifi -","Top - 1"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"99.76%","isBolded":true,"associatedRows":["CCT - 14 / 7?2","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params","CCT outperforms other competitive models , having signifi -","Top - 1"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"22.05M","isBolded":true,"associatedRows":["ViT - S [ 19 ]","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"97.19%","isBolded":false,"associatedRows":["CCT - 14 / 7?2","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params","CCT outperforms other competitive models , having signifi -","Top - 1"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"304.71M","isBolded":false,"associatedRows":["ViT - L / 16","JFT - 300M"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","MACs","CCT outperforms other competitive models , having signifi -","# Params"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"300epochs,andthelearningrateisreducedperepochbased","isBolded":true,"associatedRows":[],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1","Table 4 : Flowers - 102 Top - 1 validation accuracy comparison .","Resolution"],"associatedMergedColumns":["Appendix E . Unless stated otherwise , all tests were run for"]},{"number":"300","isBolded":false,"associatedRows":["CCT - 14 / 7?2 Distilled","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"661.00M","isBolded":false,"associatedRows":["ViT - H / 14","JFT - 300M"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","MACs","CCT outperforms other competitive models , having signifi -","# Params"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"4.15G","isBolded":false,"associatedRows":["ResNet50 [ 16 ]","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","MACs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"120","isBolded":false,"associatedRows":["ResNet50 [ 16 ]","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"5.53G","isBolded":false,"associatedRows":["CCT - 14 / 7?2 Distilled","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","MACs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"86.25M","isBolded":false,"associatedRows":["DeiT - B","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs","CCT outperforms other competitive models , having signifi -","# Params"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"25.55M","isBolded":false,"associatedRows":["ResNet50 ( 2021 ) [ 44 ]","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"18.63G","isBolded":true,"associatedRows":["CCT - 14 / 7?2","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs","CCT outperforms other competitive models , having signifi -","MACs"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"98.80%","isBolded":false,"associatedRows":["DeiT - B","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","# Params","CCT outperforms other competitive models , having signifi -","Top - 1"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"384","isBolded":true,"associatedRows":["CCT - 14 / 7?2"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1","Table 4 : Flowers - 102 Top - 1 validation accuracy comparison .","Resolution"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"79.80%","isBolded":false,"associatedRows":["ResNet50 ( 2021 ) [ 44 ]"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"384","isBolded":true,"associatedRows":["ViT - H / 14"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1","Table 4 : Flowers - 102 Top - 1 validation accuracy comparison .","Resolution"],"associatedMergedColumns":["cantly fewer parameters and GMACs . This demonstrates the compactness on small datasets even with large images"]},{"number":"79.85%","isBolded":false,"associatedRows":["ViT - S [ 19 ]"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Top - 1"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"300","isBolded":false,"associatedRows":["DeiT - S [ 19 ]","ImageNet - 1k"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","Training Epochs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]},{"number":"4.15G","isBolded":false,"associatedRows":["ResNet50 ( 2021 ) [ 44 ]","-"],"associatedColumns":["Table 3 : ImageNet Top - 1 validation accuracy comparison ( no extra data or pretraining ) . This shows that larger variants of","MACs"],"associatedMergedColumns":["CCT could also be applicable to medium - sized datasets"]}]},{"caption":"Table 5: Transformer backbones in each variant. \n\n","rows":["CCT - 4","CCT - 6","14","CVT - 7","CVT - 6","CCT - 2","ViT - Lite - 7","ViT - Lite - 6","CCT - 7","CCT - 14"],"columns":["Ratio","# Heads","# Layers"],"mergedAllColumns":["256","128"],"numberCells":[{"number":"4","isBolded":false,"associatedRows":["CVT - 6"],"associatedColumns":["# Heads"],"associatedMergedColumns":["256"]},{"number":"1","isBolded":false,"associatedRows":["CCT - 4"],"associatedColumns":["Ratio"],"associatedMergedColumns":["128"]},{"number":"4","isBolded":false,"associatedRows":["CCT - 6"],"associatedColumns":["# Heads"],"associatedMergedColumns":["128"]},{"number":"2","isBolded":false,"associatedRows":["CCT - 2"],"associatedColumns":["# Heads"],"associatedMergedColumns":["256"]},{"number":"4","isBolded":false,"associatedRows":["CVT - 7"],"associatedColumns":["# Heads"],"associatedMergedColumns":["256"]},{"number":"4","isBolded":false,"associatedRows":["ViT - Lite - 7"],"associatedColumns":["# Heads"],"associatedMergedColumns":["256"]},{"number":"2","isBolded":false,"associatedRows":["CVT - 6"],"associatedColumns":["Ratio"],"associatedMergedColumns":["256"]},{"number":"7","isBolded":false,"associatedRows":["CVT - 7"],"associatedColumns":["# Layers"],"associatedMergedColumns":["256"]},{"number":"6","isBolded":false,"associatedRows":["CCT - 6"],"associatedColumns":["# Layers"],"associatedMergedColumns":["128"]},{"number":"2","isBolded":false,"associatedRows":["CCT - 7"],"associatedColumns":["Ratio"],"associatedMergedColumns":["256"]},{"number":"6","isBolded":false,"associatedRows":["CCT - 14","14"],"associatedColumns":["# Heads"],"associatedMergedColumns":["256"]},{"number":"3","isBolded":false,"associatedRows":["CCT - 14","14"],"associatedColumns":["Ratio"],"associatedMergedColumns":["256"]},{"number":"7","isBolded":false,"associatedRows":["CCT - 7"],"associatedColumns":["# Layers"],"associatedMergedColumns":["256"]},{"number":"2","isBolded":false,"associatedRows":["CCT - 2"],"associatedColumns":["# Layers"],"associatedMergedColumns":["256"]},{"number":"1","isBolded":false,"associatedRows":["CCT - 2"],"associatedColumns":["Ratio"],"associatedMergedColumns":["256"]},{"number":"4","isBolded":false,"associatedRows":["ViT - Lite - 6"],"associatedColumns":["# Heads"],"associatedMergedColumns":[]},{"number":"6","isBolded":false,"associatedRows":["ViT - Lite - 6"],"associatedColumns":["# Layers"],"associatedMergedColumns":[]},{"number":"7","isBolded":false,"associatedRows":["ViT - Lite - 7"],"associatedColumns":["# Layers"],"associatedMergedColumns":["256"]},{"number":"6","isBolded":false,"associatedRows":["CVT - 6"],"associatedColumns":["# Layers"],"associatedMergedColumns":["256"]},{"number":"2","isBolded":false,"associatedRows":["CCT - 6"],"associatedColumns":["Ratio"],"associatedMergedColumns":["128"]},{"number":"2","isBolded":false,"associatedRows":["ViT - Lite - 7"],"associatedColumns":["Ratio"],"associatedMergedColumns":["256"]},{"number":"4","isBolded":false,"associatedRows":["CCT - 7"],"associatedColumns":["# Heads"],"associatedMergedColumns":["256"]},{"number":"2","isBolded":false,"associatedRows":["CVT - 7"],"associatedColumns":["Ratio"],"associatedMergedColumns":["256"]},{"number":"4","isBolded":false,"associatedRows":["CCT - 4"],"associatedColumns":["# Layers"],"associatedMergedColumns":["128"]},{"number":"2","isBolded":false,"associatedRows":["ViT - Lite - 6"],"associatedColumns":["Ratio"],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["CCT - 4"],"associatedColumns":["# Heads"],"associatedMergedColumns":["128"]}]},{"caption":"Table 6: Tokenizers in each variant. \n\n","rows":["ViT - Lite - 7 / 4","CVT - 7 / 4","CCT - 7 / 3x1","CVT - 7 / 8","CCT - 2 / 3x2","ViT - Lite - 7 / 8","CCT - 7 / 7x2"],"columns":["4?4","8?8","# Convs","# Layers","3?3"],"mergedAllColumns":[],"numberCells":[{"number":"2","isBolded":false,"associatedRows":["CCT - 2 / 3x2"],"associatedColumns":["# Layers","8?8","4?4","8?8","4?4"],"associatedMergedColumns":[]},{"number":"7","isBolded":false,"associatedRows":["CCT - 7 / 7x2"],"associatedColumns":["# Layers","8?8","4?4","8?8","4?4","3?3","3?3"],"associatedMergedColumns":[]},{"number":"7","isBolded":false,"associatedRows":["CVT - 7 / 8"],"associatedColumns":["# Layers","8?8","4?4"],"associatedMergedColumns":[]},{"number":"7","isBolded":false,"associatedRows":["ViT - Lite - 7 / 4"],"associatedColumns":["# Layers","8?8"],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":["CVT - 7 / 4"],"associatedColumns":["# Convs","8?8","4?4","8?8"],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["CCT - 2 / 3x2"],"associatedColumns":["# Convs","8?8","4?4","8?8","4?4"],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["CCT - 7 / 7x2"],"associatedColumns":["# Convs","8?8","4?4","8?8","4?4","3?3","3?3"],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":["CVT - 7 / 8"],"associatedColumns":["# Convs","8?8","4?4"],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":["ViT - Lite - 7 / 4"],"associatedColumns":["# Convs","8?8"],"associatedMergedColumns":[]},{"number":"7","isBolded":false,"associatedRows":["CVT - 7 / 4"],"associatedColumns":["# Layers","8?8","4?4","8?8"],"associatedMergedColumns":[]},{"number":"7","isBolded":false,"associatedRows":["CCT - 7 / 3x1"],"associatedColumns":["# Layers","8?8","4?4","8?8","4?4","3?3"],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":["CCT - 7 / 3x1"],"associatedColumns":["# Convs","8?8","4?4","8?8","4?4","3?3"],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":["ViT - Lite - 7 / 8"],"associatedColumns":["# Convs"],"associatedMergedColumns":[]},{"number":"7","isBolded":false,"associatedRows":["ViT - Lite - 7 / 8"],"associatedColumns":["# Layers"],"associatedMergedColumns":[]}]},{"caption":"Table 8: CIFAR Top-1 validation accuracy when transforming ViT into CCT step by step. We disabled advanced training \ntechniques and augmentations for these runs. \n\n","rows":["ViT - 12 / 16","ViT - Lite - 7 / 16","CVT - 7 / 4","CVT - 7 / 8","CVT - 7 / 2","CT","1","ViT - Lite - 7 / 4","2","CCT - 7 / 3?1","CCT - 7 / 3?2","CVT - 7 / 16","7 ? 7","ViT - Lite - 7 / 8","SP","3 ? 3","CCT - 7 / 7?1"],"columns":["C - 10","# Params","C - 100","MACs"],"mergedAllColumns":[],"numberCells":[{"number":"1.18G","isBolded":false,"associatedRows":["CVT - 7 / 2","SP","1","7 ? 7"],"associatedColumns":["MACs"],"associatedMergedColumns":[]},{"number":"74.77%","isBolded":false,"associatedRows":["CCT - 7 / 3?2","SP","2","3 ? 3"],"associatedColumns":["C - 100"],"associatedMergedColumns":[]},{"number":"58.43%","isBolded":false,"associatedRows":["ViT - Lite - 7 / 4","CT","1","7 ? 7"],"associatedColumns":["C - 100"],"associatedMergedColumns":[]},{"number":"3.76M","isBolded":false,"associatedRows":["CCT - 7 / 3?1","SP","1","3 ? 3"],"associatedColumns":["# Params"],"associatedMergedColumns":[]},{"number":"83.59%","isBolded":false,"associatedRows":["ViT - Lite - 7 / 4","CT","1","7 ? 7"],"associatedColumns":["C - 10"],"associatedMergedColumns":[]},{"number":"0.25G","isBolded":false,"associatedRows":["CVT - 7 / 4","SP","1","7 ? 7"],"associatedColumns":["MACs"],"associatedMergedColumns":[]},{"number":"0.43G","isBolded":false,"associatedRows":["ViT - 12 / 16","CT","1","7 ? 7"],"associatedColumns":["MACs"],"associatedMergedColumns":[]},{"number":"57.98%","isBolded":false,"associatedRows":["CVT - 7 / 2","SP","1","7 ? 7"],"associatedColumns":["C - 100"],"associatedMergedColumns":[]},{"number":"0.02G","isBolded":false,"associatedRows":["ViT - Lite - 7 / 16","CT","1","7 ? 7"],"associatedColumns":["MACs"],"associatedMergedColumns":[]},{"number":"3.85M","isBolded":false,"associatedRows":["CCT - 7 / 3?2","SP","2","3 ? 3"],"associatedColumns":["# Params"],"associatedMergedColumns":[]},{"number":"3.74M","isBolded":false,"associatedRows":["CCT - 7 / 7?1","SP","1","7 ? 7"],"associatedColumns":["# Params"],"associatedMergedColumns":[]},{"number":"55.49%","isBolded":false,"associatedRows":["CVT - 7 / 8","SP","1","7 ? 7"],"associatedColumns":["C - 100"],"associatedMergedColumns":[]},{"number":"0.26G","isBolded":false,"associatedRows":["CCT - 7 / 7?1","SP","1","7 ? 7"],"associatedColumns":["MACs"],"associatedMergedColumns":[]},{"number":"75.59%","isBolded":true,"associatedRows":["CCT - 7 / 3?1","SP","1","3 ? 3"],"associatedColumns":["C - 100"],"associatedMergedColumns":[]},{"number":"3.74M","isBolded":false,"associatedRows":["CVT - 7 / 8","SP","1","7 ? 7"],"associatedColumns":["# Params"],"associatedMergedColumns":[]},{"number":"91.72%","isBolded":false,"associatedRows":["CVT - 7 / 4","SP","1","7 ? 7"],"associatedColumns":["C - 10"],"associatedMergedColumns":[]},{"number":"0.25G","isBolded":false,"associatedRows":["CVT - 7 / 4","SP","1","7 ? 7"],"associatedColumns":["MACs"],"associatedMergedColumns":[]},{"number":"72.46%","isBolded":false,"associatedRows":["CCT - 7 / 7?1","SP","1","7 ? 7"],"associatedColumns":["C - 100"],"associatedMergedColumns":[]},{"number":"91.85%","isBolded":false,"associatedRows":["CCT - 7 / 7?1","SP","1","7 ? 7"],"associatedColumns":["C - 10"],"associatedMergedColumns":[]},{"number":"3.72M","isBolded":false,"associatedRows":["CVT - 7 / 4","SP","1","7 ? 7"],"associatedColumns":["# Params"],"associatedMergedColumns":[]},{"number":"92.29%","isBolded":false,"associatedRows":["CCT - 7 / 7?1","SP","1","7 ? 7"],"associatedColumns":["C - 10"],"associatedMergedColumns":[]},{"number":"42.37%","isBolded":false,"associatedRows":["CVT - 7 / 16","SP","1","7 ? 7"],"associatedColumns":["C - 100"],"associatedMergedColumns":[]},{"number":"71.78%","isBolded":false,"associatedRows":["ViT - Lite - 7 / 16","CT","1","7 ? 7"],"associatedColumns":["C - 10"],"associatedMergedColumns":[]},{"number":"3.72M","isBolded":false,"associatedRows":["CVT - 7 / 4","SP","1","7 ? 7"],"associatedColumns":["# Params"],"associatedMergedColumns":[]},{"number":"73.01%","isBolded":false,"associatedRows":["CVT - 7 / 4","SP","1","7 ? 7"],"associatedColumns":["C - 100"],"associatedMergedColumns":[]},{"number":"0.26G","isBolded":false,"associatedRows":["CCT - 7 / 7?1","SP","1","7 ? 7"],"associatedColumns":["MACs"],"associatedMergedColumns":[]},{"number":"69.82%","isBolded":false,"associatedRows":["ViT - 12 / 16","CT","1","7 ? 7"],"associatedColumns":["C - 10"],"associatedMergedColumns":[]},{"number":"69.43%","isBolded":false,"associatedRows":["CCT - 7 / 7?1","SP","1","7 ? 7"],"associatedColumns":["C - 100"],"associatedMergedColumns":[]},{"number":"40.57%","isBolded":false,"associatedRows":["ViT - 12 / 16","CT","1","7 ? 7"],"associatedColumns":["C - 100"],"associatedMergedColumns":[]},{"number":"63.14%","isBolded":false,"associatedRows":["CVT - 7 / 8","SP","1","7 ? 7"],"associatedColumns":["C - 100"],"associatedMergedColumns":[]},{"number":"1.19G","isBolded":false,"associatedRows":["CCT - 7 / 3?1","SP","1","3 ? 3"],"associatedColumns":["MACs"],"associatedMergedColumns":[]},{"number":"62.83%","isBolded":false,"associatedRows":["CCT - 7 / 7?1","SP","1","7 ? 7"],"associatedColumns":["C - 100"],"associatedMergedColumns":[]},{"number":"94.47%","isBolded":true,"associatedRows":["CCT - 7 / 3?1","SP","1","3 ? 3"],"associatedColumns":["C - 10"],"associatedMergedColumns":[]},{"number":"3.74M","isBolded":false,"associatedRows":["CVT - 7 / 8","SP","1","7 ? 7"],"associatedColumns":["# Params"],"associatedMergedColumns":[]},{"number":"0.02G","isBolded":false,"associatedRows":["CVT - 7 / 16","SP","1","7 ? 7"],"associatedColumns":["MACs"],"associatedMergedColumns":[]},{"number":"87.15%","isBolded":false,"associatedRows":["CVT - 7 / 8","SP","1","7 ? 7"],"associatedColumns":["C - 10"],"associatedMergedColumns":[]},{"number":"41.59%","isBolded":false,"associatedRows":["ViT - Lite - 7 / 16","CT","1","7 ? 7"],"associatedColumns":["C - 100"],"associatedMergedColumns":[]},{"number":"3.72M","isBolded":false,"associatedRows":["ViT - Lite - 7 / 4","CT","1","7 ? 7"],"associatedColumns":["# Params"],"associatedMergedColumns":[]},{"number":"62.06%","isBolded":false,"associatedRows":["CVT - 7 / 4","SP","1","7 ? 7"],"associatedColumns":["C - 100"],"associatedMergedColumns":[]},{"number":"87.81%","isBolded":false,"associatedRows":["CCT - 7 / 7?1","SP","1","7 ? 7"],"associatedColumns":["C - 10"],"associatedMergedColumns":[]},{"number":"3.74M","isBolded":false,"associatedRows":["ViT - Lite - 7 / 8","CT","1","7 ? 7"],"associatedColumns":["# Params"],"associatedMergedColumns":[]},{"number":"84.24%","isBolded":false,"associatedRows":["CVT - 7 / 8","SP","1","7 ? 7"],"associatedColumns":["C - 10"],"associatedMergedColumns":[]},{"number":"88.06%","isBolded":false,"associatedRows":["CVT - 7 / 4","SP","1","7 ? 7"],"associatedColumns":["C - 10"],"associatedMergedColumns":[]},{"number":"3.72M","isBolded":false,"associatedRows":["CVT - 7 / 4","SP","1","7 ? 7"],"associatedColumns":["# Params"],"associatedMergedColumns":[]},{"number":"93.65%","isBolded":false,"associatedRows":["CCT - 7 / 3?2","SP","2","3 ? 3"],"associatedColumns":["C - 10"],"associatedMergedColumns":[]},{"number":"0.06G","isBolded":false,"associatedRows":["CVT - 7 / 8","SP","1","7 ? 7"],"associatedColumns":["MACs"],"associatedMergedColumns":[]},{"number":"3.74M","isBolded":false,"associatedRows":["CCT - 7 / 7?1","SP","1","7 ? 7"],"associatedColumns":["# Params"],"associatedMergedColumns":[]},{"number":"3.76M","isBolded":false,"associatedRows":["CVT - 7 / 2","SP","1","7 ? 7"],"associatedColumns":["# Params"],"associatedMergedColumns":[]},{"number":"69.59%","isBolded":false,"associatedRows":["CVT - 7 / 4","SP","1","7 ? 7"],"associatedColumns":["C - 100"],"associatedMergedColumns":[]},{"number":"85.63M","isBolded":false,"associatedRows":["ViT - 12 / 16","CT","1","7 ? 7"],"associatedColumns":["# Params"],"associatedMergedColumns":[]},{"number":"0.06G","isBolded":false,"associatedRows":["CVT - 7 / 8","SP","1","7 ? 7"],"associatedColumns":["MACs"],"associatedMergedColumns":[]},{"number":"92.43%","isBolded":false,"associatedRows":["CVT - 7 / 4","SP","1","7 ? 7"],"associatedColumns":["C - 10"],"associatedMergedColumns":[]},{"number":"3.74M","isBolded":false,"associatedRows":["CCT - 7 / 7?1","SP","1","7 ? 7"],"associatedColumns":["# Params"],"associatedMergedColumns":[]},{"number":"0.06G","isBolded":false,"associatedRows":["ViT - Lite - 7 / 8","CT","1","7 ? 7"],"associatedColumns":["MACs"],"associatedMergedColumns":[]},{"number":"0.25G","isBolded":false,"associatedRows":["CVT - 7 / 4","SP","1","7 ? 7"],"associatedColumns":["MACs"],"associatedMergedColumns":[]},{"number":"72.26%","isBolded":false,"associatedRows":["CVT - 7 / 16","SP","1","7 ? 7"],"associatedColumns":["C - 10"],"associatedMergedColumns":[]},{"number":"3.89M","isBolded":false,"associatedRows":["ViT - Lite - 7 / 16","CT","1","7 ? 7"],"associatedColumns":["# Params"],"associatedMergedColumns":[]},{"number":"83.38%","isBolded":false,"associatedRows":["ViT - Lite - 7 / 8","CT","1","7 ? 7"],"associatedColumns":["C - 10"],"associatedMergedColumns":[]},{"number":"55.69%","isBolded":false,"associatedRows":["ViT - Lite - 7 / 8","CT","1","7 ? 7"],"associatedColumns":["C - 100"],"associatedMergedColumns":[]},{"number":"84.80%","isBolded":false,"associatedRows":["CVT - 7 / 2","SP","1","7 ? 7"],"associatedColumns":["C - 10"],"associatedMergedColumns":[]},{"number":"0.29G","isBolded":false,"associatedRows":["CCT - 7 / 3?2","SP","2","3 ? 3"],"associatedColumns":["MACs"],"associatedMergedColumns":[]},{"number":"0.26G","isBolded":false,"associatedRows":["CCT - 7 / 7?1","SP","1","7 ? 7"],"associatedColumns":["MACs"],"associatedMergedColumns":[]},{"number":"0.26G","isBolded":false,"associatedRows":["ViT - Lite - 7 / 4","CT","1","7 ? 7"],"associatedColumns":["MACs"],"associatedMergedColumns":[]},{"number":"3.89M","isBolded":false,"associatedRows":["CVT - 7 / 16","SP","1","7 ? 7"],"associatedColumns":["# Params"],"associatedMergedColumns":[]}]},{"caption":"Table 9: Difference between tuned and not tuned runs in \n","rows":["MLP Dropout","0","Stochastic Depth","MSA Dropout"],"columns":["Tuned","Not Tuned"],"mergedAllColumns":["0"],"numberCells":[{"number":"0.1","isBolded":false,"associatedRows":["MLP Dropout"],"associatedColumns":["Not Tuned"],"associatedMergedColumns":[]},{"number":"0.1","isBolded":false,"associatedRows":["MSA Dropout","0"],"associatedColumns":["Tuned"],"associatedMergedColumns":["0"]},{"number":"0.1","isBolded":false,"associatedRows":["Stochastic Depth","0"],"associatedColumns":["Tuned"],"associatedMergedColumns":["0"]}]},{"caption":"Table 8. \n\n","rows":["MLP Dropout","0","Stochastic Depth","MSA Dropout"],"columns":["Tuned","Not Tuned"],"mergedAllColumns":["0"],"numberCells":[{"number":"0.1","isBolded":false,"associatedRows":["MSA Dropout","0"],"associatedColumns":["Tuned"],"associatedMergedColumns":["0"]},{"number":"0.1","isBolded":false,"associatedRows":["MLP Dropout"],"associatedColumns":["Not Tuned"],"associatedMergedColumns":[]},{"number":"0.1","isBolded":false,"associatedRows":["Stochastic Depth","0"],"associatedColumns":["Tuned"],"associatedMergedColumns":["0"]}]},{"caption":"Table 11. \n","rows":["MLP Dropout","0","Stochastic Depth","MSA Dropout"],"columns":["Tuned","Not Tuned"],"mergedAllColumns":["0"],"numberCells":[{"number":"0.1","isBolded":false,"associatedRows":["MLP Dropout"],"associatedColumns":["Not Tuned"],"associatedMergedColumns":[]},{"number":"0.1","isBolded":false,"associatedRows":["MSA Dropout","0"],"associatedColumns":["Tuned"],"associatedMergedColumns":["0"]},{"number":"0.1","isBolded":false,"associatedRows":["Stochastic Depth","0"],"associatedColumns":["Tuned"],"associatedMergedColumns":["0"]}]},{"caption":"Table 11: Top-1 comparisons. were trained longer (see Tab 2). \n\n","rows":["MobileNetV2 / 1 . 25","CCT - 4 / 3?2","CVT - 7 / 4","CVT - 7 / 8","Proxyless - G [ 5 ]","ResNet18","MobileNetV2 / 0 . 5","ResNet110 [ 16 ]","ViT - Lite - 6 / 16","?","ResNet56 [ 16 ]","CCT - 7 / 3?1","CCT - 7 / 3?2","CCT - 2 / 3?2","ResNet1k - v2 [ 17 ]","ViT - 12 / 16","ViT - Lite - 7 / 16","ViT - Lite - 6 / 8","ViT - Lite - 6 / 4","ResNet164 - v1 [ 17 ]","ViT - Lite - 7 / 4","ResNet34","CCT - 6 / 3?1","MobileNetV2 / 1 . 0","CCT - 6 / 3?2","ResNet50","ResNet164 - v2 [ 17 ]","CVT - 6 / 8","MobileNetV2 / 2 . 0","CVT - 6 / 4","\u003c","ViT - Lite - 7 / 8","ResNet1k - v1 [ 17 ]"],"columns":["C - 10","# Params","C - 100","Fashion","MACs","MNIST"],"mergedAllColumns":["Compact Convolutional Transformers","Compact Vision Transformers","Convolutional Networks ( Designed for CIFAR )","Vision Transformers","Convolutional Networks ( Designed for ImageNet )"],"numberCells":[{"number":"95.38%","isBolded":false,"associatedRows":["ResNet1k - v2 [ 17 ]"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"3.47M","isBolded":false,"associatedRows":["MobileNetV2 / 1 . 25","?","?","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"3.21M","isBolded":false,"associatedRows":["CVT - 6 / 8","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"99.75%","isBolded":false,"associatedRows":["MobileNetV2 / 1 . 0","?","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"2.24M","isBolded":false,"associatedRows":["MobileNetV2 / 1 . 0","?","?","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"0.48M","isBolded":false,"associatedRows":["CCT - 4 / 3?2","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"77.14%","isBolded":false,"associatedRows":["CCT - 6 / 3?2"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"0.04G","isBolded":false,"associatedRows":["CCT - 2 / 3?2","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"3.19M","isBolded":false,"associatedRows":["ViT - Lite - 6 / 4","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Vision Transformers"]},{"number":"89.10%","isBolded":false,"associatedRows":["ViT - Lite - 7 / 8"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Vision Transformers"]},{"number":"94.63%","isBolded":false,"associatedRows":["ResNet56 [ 16 ]"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"1.70M","isBolded":false,"associatedRows":["ResNet164 - v1 [ 17 ]","?","?","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"89.07%","isBolded":false,"associatedRows":["MobileNetV2 / 1 . 0"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"1.73M","isBolded":false,"associatedRows":["ResNet110 [ 16 ]","?","?","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"94.08%","isBolded":false,"associatedRows":["CCT - 2 / 3?2"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"99.70%","isBolded":false,"associatedRows":["CVT - 7 / 8","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"93.61%","isBolded":false,"associatedRows":["ViT - 12 / 16"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Vision Transformers"]},{"number":"65.24%","isBolded":false,"associatedRows":["MobileNetV2 / 1 . 25"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"99.75%","isBolded":false,"associatedRows":["CCT - 6 / 3?2","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"95.08%","isBolded":false,"associatedRows":["ResNet110 [ 16 ]"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"3.72M","isBolded":false,"associatedRows":["CVT - 7 / 4","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"74.23%","isBolded":false,"associatedRows":["CVT - 6 / 4"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"66.46%","isBolded":false,"associatedRows":["ResNet18"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"94.43%","isBolded":false,"associatedRows":["CCT - 6 / 3?2"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"95.32%","isBolded":false,"associatedRows":["CVT - 7 / 4"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"95.16%","isBolded":false,"associatedRows":["CCT - 7 / 3?2"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"91.02%","isBolded":false,"associatedRows":["MobileNetV2 / 2 . 0"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"94.78%","isBolded":false,"associatedRows":["ResNet18"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"89.75%","isBolded":false,"associatedRows":["CCT - 2 / 3?2"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"99.79%","isBolded":false,"associatedRows":["ResNet50","?","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"10.33M","isBolded":false,"associatedRows":["ResNet1k - v1 [ 17 ]","?","?","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"99.75%","isBolded":false,"associatedRows":["CVT - 6 / 4","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"10.33M","isBolded":false,"associatedRows":["ResNet1k - v2 [ 17 ]","?","?","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"95.32%","isBolded":false,"associatedRows":["ResNet110 [ 16 ]"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"99.70%","isBolded":false,"associatedRows":["CCT - 2 / 3?2","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"10.33M","isBolded":false,"associatedRows":["ResNet1k - v2 [ 17 ]","?","?","?","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"79.40%","isBolded":false,"associatedRows":["CCT - 6 / 3?1"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"0.70M","isBolded":false,"associatedRows":["MobileNetV2 / 0 . 5","?","?","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"80.92%","isBolded":false,"associatedRows":["CCT - 7 / 3?1"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"99.28%","isBolded":false,"associatedRows":["ResNet110 [ 16 ]","?","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"0.04G","isBolded":false,"associatedRows":["ResNet18","?","?","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"0.05G","isBolded":false,"associatedRows":["CCT - 4 / 3?2","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"99.82%","isBolded":true,"associatedRows":["CCT - 7 / 3?1","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"1.55G","isBolded":false,"associatedRows":["ResNet1k - v2 [ 17 ]","?","?","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"95.04%","isBolded":false,"associatedRows":["CCT - 7 / 3?2"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"73.94%","isBolded":false,"associatedRows":["ViT - Lite - 7 / 4"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Vision Transformers"]},{"number":"71.51%","isBolded":false,"associatedRows":["CCT - 4 / 3?2"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"3.89M","isBolded":false,"associatedRows":["ViT - Lite - 7 / 16","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Vision Transformers"]},{"number":"95.56%","isBolded":true,"associatedRows":["CCT - 7 / 3?1"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"94.36%","isBolded":false,"associatedRows":["ViT - Lite - 6 / 8"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Vision Transformers"]},{"number":"99.70%","isBolded":false,"associatedRows":["MobileNetV2 / 0 . 5","?","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"99.74%","isBolded":false,"associatedRows":["ViT - Lite - 6 / 4","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Vision Transformers"]},{"number":"99.66%","isBolded":false,"associatedRows":["ViT - Lite - 6 / 16","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Vision Transformers"]},{"number":"0.22G","isBolded":false,"associatedRows":["CVT - 6 / 4","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"0.26G","isBolded":false,"associatedRows":["ViT - Lite - 7 / 4","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Vision Transformers"]},{"number":"75.67%","isBolded":false,"associatedRows":["ResNet164 - v2 [ 17 ]"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"1.55G","isBolded":false,"associatedRows":["ResNet1k - v1 [ 17 ]","?","?","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"99.76%","isBolded":false,"associatedRows":["CCT - 7 / 3?2","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"23.53M","isBolded":false,"associatedRows":["ResNet50","?","?","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"94.50%","isBolded":false,"associatedRows":["CVT - 7 / 8"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"95.00%","isBolded":false,"associatedRows":["CVT - 6 / 4"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"3.74M","isBolded":false,"associatedRows":["CVT - 7 / 8","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"89.50%","isBolded":false,"associatedRows":["CVT - 6 / 8"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"11.18M","isBolded":false,"associatedRows":["ResNet18","?","?","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"85.63M","isBolded":false,"associatedRows":["ViT - 12 / 16","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Vision Transformers"]},{"number":"93.09%","isBolded":false,"associatedRows":["ViT - Lite - 6 / 16"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Vision Transformers"]},{"number":"3.22M","isBolded":false,"associatedRows":["ViT - Lite - 6 / 8","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Vision Transformers"]},{"number":"83.04%","isBolded":false,"associatedRows":["ViT - 12 / 16"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Vision Transformers"]},{"number":"99.69%","isBolded":false,"associatedRows":["ViT - Lite - 7 / 8","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Vision Transformers"]},{"number":"94.07%","isBolded":false,"associatedRows":["ResNet164 - v1 [ 17 ]"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"94.78%","isBolded":false,"associatedRows":["ResNet34"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"91.63%","isBolded":false,"associatedRows":["ResNet50"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"77.72%","isBolded":false,"associatedRows":["CCT - 7 / 3?2"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"66.40%","isBolded":false,"associatedRows":["ViT - Lite - 6 / 8"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Vision Transformers"]},{"number":"52.87%","isBolded":false,"associatedRows":["ViT - Lite - 7 / 16"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Vision Transformers"]},{"number":"3.23M","isBolded":false,"associatedRows":["CCT - 6 / 3?1","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"68.27%","isBolded":false,"associatedRows":["ResNet50"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"8.72M","isBolded":false,"associatedRows":["MobileNetV2 / 2 . 0","?","?","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"93.57%","isBolded":false,"associatedRows":["ViT - Lite - 7 / 4"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Vision Transformers"]},{"number":"73.33%","isBolded":false,"associatedRows":["ViT - Lite - 6 / 4"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Vision Transformers"]},{"number":"76.49%","isBolded":false,"associatedRows":["CVT - 7 / 4"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"1.19G","isBolded":false,"associatedRows":["CCT - 7 / 3?1","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"0.01G","isBolded":true,"associatedRows":["MobileNetV2 / 0 . 5","?","?","?","?","\u003c"],"associatedColumns":["MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"92.39%","isBolded":false,"associatedRows":["ResNet1k - v1 [ 17 ]"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"0.08G","isBolded":false,"associatedRows":["ResNet50","?","?","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"70.11%","isBolded":false,"associatedRows":["CVT - 7 / 8"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"94.53%","isBolded":false,"associatedRows":["CVT - 6 / 8"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"78.12%","isBolded":false,"associatedRows":["ViT - Lite - 6 / 16"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Vision Transformers"]},{"number":"3.33M","isBolded":false,"associatedRows":["CCT - 6 / 3?2","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"99.77%","isBolded":false,"associatedRows":["MobileNetV2 / 1 . 25","?","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"0.01G","isBolded":false,"associatedRows":["MobileNetV2 / 1 . 25","?","?","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"3.36M","isBolded":false,"associatedRows":["ViT - Lite - 6 / 16","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Vision Transformers"]},{"number":"93.24%","isBolded":false,"associatedRows":["ViT - Lite - 7 / 16"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Vision Transformers"]},{"number":"93.60%","isBolded":false,"associatedRows":["CVT - 6 / 4"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"90.60%","isBolded":false,"associatedRows":["MobileNetV2 / 1 . 25"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"91.97%","isBolded":false,"associatedRows":["CCT - 4 / 3?2"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"99.27%","isBolded":false,"associatedRows":["ResNet56 [ 16 ]","?","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"99.73%","isBolded":false,"associatedRows":["CCT - 4 / 3?2","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"0.02G","isBolded":false,"associatedRows":["ViT - Lite - 7 / 16","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Vision Transformers"]},{"number":"94.74%","isBolded":false,"associatedRows":["CCT - 4 / 3?2"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"99.63%","isBolded":false,"associatedRows":["ViT - 12 / 16","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Vision Transformers"]},{"number":"0.08G","isBolded":false,"associatedRows":["ResNet34","?","?","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"98.00%","isBolded":true,"associatedRows":["CCT - 7 / 3?1"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"0.02G","isBolded":false,"associatedRows":["ViT - Lite - 6 / 16","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Vision Transformers"]},{"number":"5.7M","isBolded":false,"associatedRows":["Proxyless - G [ 5 ]","?","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"68.80%","isBolded":false,"associatedRows":["CVT - 6 / 8"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"93.93%","isBolded":false,"associatedRows":["MobileNetV2 / 0 . 5"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"99.75%","isBolded":false,"associatedRows":["MobileNetV2 / 2 . 0","?","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"74.84%","isBolded":false,"associatedRows":["ResNet164 - v1 [ 17 ]"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"66.93%","isBolded":false,"associatedRows":["CCT - 2 / 3?2"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"0.06G","isBolded":false,"associatedRows":["ViT - Lite - 7 / 8","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Vision Transformers"]},{"number":"67.44%","isBolded":false,"associatedRows":["MobileNetV2 / 2 . 0"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"63.69%","isBolded":false,"associatedRows":["MobileNetV2 / 1 . 0"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"95.14%","isBolded":false,"associatedRows":["ViT - Lite - 6 / 4"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Vision Transformers"]},{"number":"0.26G","isBolded":false,"associatedRows":["ResNet164 - v2 [ 17 ]","?","?","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"1.70M","isBolded":false,"associatedRows":["ResNet164 - v2 [ 17 ]","?","?","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"21.29M","isBolded":false,"associatedRows":["ResNet34","?","?","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"94.85%","isBolded":false,"associatedRows":["MobileNetV2 / 1 . 0"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"94.54%","isBolded":false,"associatedRows":["ResNet164 - v2 [ 17 ]"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"0.43G","isBolded":false,"associatedRows":["ViT - 12 / 16","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Vision Transformers"]},{"number":"0.26G","isBolded":false,"associatedRows":["ResNet164 - v1 [ 17 ]","?","?","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"3.85M","isBolded":false,"associatedRows":["CCT - 7 / 3?2","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"99.79%","isBolded":false,"associatedRows":["CCT - 6 / 3?1","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"90.51%","isBolded":false,"associatedRows":["ResNet34"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"56.32%","isBolded":false,"associatedRows":["MobileNetV2 / 0 . 5"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"0.02G","isBolded":false,"associatedRows":["MobileNetV2 / 2 . 0","?","?","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"99.76%","isBolded":false,"associatedRows":["CVT - 7 / 4","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"77.29%","isBolded":false,"associatedRows":["ResNet1k - v2 [ 17 ]"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"82.72%","isBolded":true,"associatedRows":["CCT - 7 / 3?1"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"95.05%","isBolded":false,"associatedRows":["MobileNetV2 / 1 . 25"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"67.27%","isBolded":false,"associatedRows":["ViT - Lite - 7 / 8"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Vision Transformers"]},{"number":"89.79%","isBolded":false,"associatedRows":["CVT - 7 / 8"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"0.05G","isBolded":false,"associatedRows":["CVT - 6 / 8","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"3.19M","isBolded":false,"associatedRows":["CVT - 6 / 4","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"0.22G","isBolded":false,"associatedRows":["ViT - Lite - 6 / 4","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Vision Transformers"]},{"number":"52.68%","isBolded":false,"associatedRows":["ViT - Lite - 6 / 16"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Vision Transformers"]},{"number":"99.68%","isBolded":false,"associatedRows":["ViT - Lite - 7 / 16","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Vision Transformers"]},{"number":"66.84%","isBolded":false,"associatedRows":["ResNet34"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"74.81%","isBolded":false,"associatedRows":["ResNet56 [ 16 ]"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"97.92%","isBolded":false,"associatedRows":["Proxyless - G [ 5 ]"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"3.74M","isBolded":false,"associatedRows":["ViT - Lite - 7 / 8","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Vision Transformers"]},{"number":"3.72M","isBolded":false,"associatedRows":["ViT - Lite - 7 / 4","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Vision Transformers"]},{"number":"0.29G","isBolded":false,"associatedRows":["CCT - 7 / 3?2","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"1.02G","isBolded":false,"associatedRows":["CCT - 6 / 3?1","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"99.77%","isBolded":false,"associatedRows":["ResNet34","?","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"95.08%","isBolded":false,"associatedRows":["ResNet1k - v2 [ 17 ]"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"76.63%","isBolded":false,"associatedRows":["ResNet110 [ 16 ]"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"94.49%","isBolded":false,"associatedRows":["ViT - Lite - 7 / 8"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Vision Transformers"]},{"number":"3.76M","isBolded":false,"associatedRows":["CCT - 7 / 3?1","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"94.99%","isBolded":false,"associatedRows":["ResNet50"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"0.01G","isBolded":false,"associatedRows":["MobileNetV2 / 1 . 0","?","?","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"0.06G","isBolded":false,"associatedRows":["CVT - 7 / 8","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"57.97%","isBolded":false,"associatedRows":["ViT - 12 / 16"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Vision Transformers"]},{"number":"99.73%","isBolded":false,"associatedRows":["ViT - Lite - 6 / 8","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Vision Transformers"]},{"number":"3.76M","isBolded":false,"associatedRows":["CCT - 7 / 3?1","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"95.25%","isBolded":false,"associatedRows":["ResNet56 [ 16 ]"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"0.13G","isBolded":false,"associatedRows":["ResNet56 [ 16 ]","?","?","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"78.45%","isBolded":false,"associatedRows":["ViT - Lite - 7 / 16"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Vision Transformers"]},{"number":"95.41%","isBolded":false,"associatedRows":["CCT - 6 / 3?1"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"95.70%","isBolded":false,"associatedRows":["CCT - 6 / 3?1"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"84.78%","isBolded":false,"associatedRows":["MobileNetV2 / 0 . 5"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"0.25G","isBolded":false,"associatedRows":["CVT - 7 / 4","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"95.16%","isBolded":false,"associatedRows":["ViT - Lite - 7 / 4"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Vision Transformers"]},{"number":"0.26G","isBolded":false,"associatedRows":["ResNet110 [ 16 ]","?","?","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"1.55G","isBolded":false,"associatedRows":["ResNet1k - v2 [ 17 ]","?","?","?","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"99.74%","isBolded":false,"associatedRows":["CVT - 6 / 8","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"94.01%","isBolded":false,"associatedRows":["CVT - 7 / 4"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Compact Vision Transformers"]},{"number":"99.77%","isBolded":false,"associatedRows":["ViT - Lite - 7 / 4","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Vision Transformers"]},{"number":"0.06G","isBolded":false,"associatedRows":["ViT - Lite - 6 / 8","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Vision Transformers"]},{"number":"0.25G","isBolded":false,"associatedRows":["CCT - 6 / 3?2","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"99.80%","isBolded":false,"associatedRows":["ResNet18","?","?"],"associatedColumns":["MNIST"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"96.53%","isBolded":false,"associatedRows":["CCT - 7 / 3?1"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"72.18%","isBolded":false,"associatedRows":["ResNet1k - v1 [ 17 ]"],"associatedColumns":["C - 100"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"1.19G","isBolded":false,"associatedRows":["CCT - 7 / 3?1","?","?"],"associatedColumns":["MACs"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"0.28M","isBolded":true,"associatedRows":["CCT - 2 / 3?2","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Compact Convolutional Transformers"]},{"number":"93.08%","isBolded":false,"associatedRows":["ViT - Lite - 6 / 4"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Vision Transformers"]},{"number":"0.85M","isBolded":false,"associatedRows":["ResNet56 [ 16 ]","?","?","?","?"],"associatedColumns":["# Params"],"associatedMergedColumns":["Convolutional Networks ( Designed for CIFAR )"]},{"number":"88.29%","isBolded":false,"associatedRows":["ViT - Lite - 6 / 8"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Vision Transformers"]},{"number":"90.27%","isBolded":false,"associatedRows":["ResNet18"],"associatedColumns":["C - 10"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"95.26%","isBolded":false,"associatedRows":["MobileNetV2 / 2 . 0"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Convolutional Networks ( Designed for ImageNet )"]},{"number":"95.34%","isBolded":false,"associatedRows":["CCT - 6 / 3?2"],"associatedColumns":["Fashion"],"associatedMergedColumns":["Compact Convolutional Transformers"]}]}]