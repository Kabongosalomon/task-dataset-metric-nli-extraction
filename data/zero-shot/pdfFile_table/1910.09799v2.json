[{"caption":"Table 1: Effect of Positional Embeddings (PE) for Transformer. \nPE \ntest-clean test-other \nNone \n3.11 \n6.94 \nSinusoid \n3.13 \n6.67 \nFrame Stacking \n3.04 \n6.64 \nConvolution \n2.87 \n6.46 \n\n","rows":["Convolution","Frame Stacking","None","Sinusoid"],"columns":["test - clean","test - other","Table 1 : Effect of Positional Embeddings ( PE ) for Transformer ."],"mergedAllColumns":[],"numberCells":[{"number":"3.11","isBolded":false,"associatedRows":["None"],"associatedColumns":["Table 1 : Effect of Positional Embeddings ( PE ) for Transformer .","test - clean"],"associatedMergedColumns":[]},{"number":"6.67","isBolded":false,"associatedRows":["Sinusoid"],"associatedColumns":["Table 1 : Effect of Positional Embeddings ( PE ) for Transformer .","test - other"],"associatedMergedColumns":[]},{"number":"6.46","isBolded":false,"associatedRows":["Convolution"],"associatedColumns":["Table 1 : Effect of Positional Embeddings ( PE ) for Transformer .","test - other"],"associatedMergedColumns":[]},{"number":"3.04","isBolded":false,"associatedRows":["Frame Stacking"],"associatedColumns":["Table 1 : Effect of Positional Embeddings ( PE ) for Transformer .","test - clean"],"associatedMergedColumns":[]},{"number":"6.64","isBolded":false,"associatedRows":["Frame Stacking"],"associatedColumns":["Table 1 : Effect of Positional Embeddings ( PE ) for Transformer .","test - other"],"associatedMergedColumns":[]},{"number":"6.94","isBolded":false,"associatedRows":["None"],"associatedColumns":["Table 1 : Effect of Positional Embeddings ( PE ) for Transformer .","test - other"],"associatedMergedColumns":[]},{"number":"3.13","isBolded":false,"associatedRows":["Sinusoid"],"associatedColumns":["Table 1 : Effect of Positional Embeddings ( PE ) for Transformer .","test - clean"],"associatedMergedColumns":[]},{"number":"2.87","isBolded":false,"associatedRows":["Convolution"],"associatedColumns":["Table 1 : Effect of Positional Embeddings ( PE ) for Transformer .","test - clean"],"associatedMergedColumns":[]}]},{"caption":"Table 2: Architecture comparison on the Librispeech benchmark \nModel Arch \n#Params (M) test-clean test-other \nBLSTM (800,5) \n79 \n3.11 \n7.44 \nTrf-FS (768,12) \n91 \n3.04 \n6.64 \nvggBLSTM (800,5) \n95 \n2.99 \n6.95 \nvggTrf. (768,12) \n93 \n2.87 \n6.46 \nvggBLSTM (1000,6) \n163 \n2.86 \n6.63 \nvggTrf. (768, 20) \n149 \n2.77 \n6.10 \n\nlayers further. To make the model size manageable, we use a smaller \nembedding dimension, 512, for deep transformer models. Our initial \nattempt was not successful; deep transformer models (deeper than \n20 layers) often got stuck in training and made little progress for a \nlong time. We solved the problem with the iterated loss used in [29]: \nthe output embeddings of the 6/12/18-th transformer layers are non-\nlinearly transformed (projected to a 256-dimensional space with a \nlinear transformation followed by a Relu non-linearity) and auxiliary \nCE losses are calculated separately. These additional CE losses are \ninterpolated with the original CE loss with a 0.3 weight. With this \niterated loss, we were able to train a 24-layer transformer model with \nonly 81M model parameters in decoding 5 and obtain a 7% and 13% \nWER reduction on test-clean and test-other, respectively, \nover the vggTrf(768, 12) baseline. \n\n","rows":["vggTrf . ( 768 , 12 )","79","vggBLSTM ( 800 , 5 )","vggTrf . ( 768 , 20 )","Trf - FS ( 768 , 12 )","163","BLSTM ( 800 , 5 )","interpolated with the original CE loss with a","149","91","93","95","vggBLSTM ( 1000 , 6 )"],"columns":["test - clean","test - other","Table 2 : Architecture comparison on the Librispeech benchmark"],"mergedAllColumns":["CE losses are calculated separately . These additional CE losses are"],"numberCells":[{"number":"2.99","isBolded":false,"associatedRows":["vggBLSTM ( 800 , 5 )","95"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - clean"],"associatedMergedColumns":[]},{"number":"6.46","isBolded":false,"associatedRows":["vggTrf . ( 768 , 12 )","93"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - other"],"associatedMergedColumns":[]},{"number":"3.11","isBolded":false,"associatedRows":["BLSTM ( 800 , 5 )","79"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - clean"],"associatedMergedColumns":[]},{"number":"2.77","isBolded":false,"associatedRows":["vggTrf . ( 768 , 20 )","149"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - clean"],"associatedMergedColumns":[]},{"number":"6.10","isBolded":false,"associatedRows":["vggTrf . ( 768 , 20 )","149"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - other"],"associatedMergedColumns":[]},{"number":"3.04","isBolded":false,"associatedRows":["Trf - FS ( 768 , 12 )","91"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - clean"],"associatedMergedColumns":[]},{"number":"7.44","isBolded":false,"associatedRows":["BLSTM ( 800 , 5 )","79"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - other"],"associatedMergedColumns":[]},{"number":"2.87","isBolded":false,"associatedRows":["vggTrf . ( 768 , 12 )","93"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - clean"],"associatedMergedColumns":[]},{"number":"6.64","isBolded":false,"associatedRows":["Trf - FS ( 768 , 12 )","91"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - other"],"associatedMergedColumns":[]},{"number":"6.63","isBolded":false,"associatedRows":["vggBLSTM ( 1000 , 6 )","163"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - other"],"associatedMergedColumns":[]},{"number":"6.95","isBolded":false,"associatedRows":["vggBLSTM ( 800 , 5 )","95"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - other"],"associatedMergedColumns":[]},{"number":"0.3weight.Withthis","isBolded":false,"associatedRows":["interpolated with the original CE loss with a"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - other"],"associatedMergedColumns":["CE losses are calculated separately . These additional CE losses are"]},{"number":"2.86","isBolded":false,"associatedRows":["vggBLSTM ( 1000 , 6 )","163"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - clean"],"associatedMergedColumns":[]}]},{"caption":"Table 3: Using iterated loss to train deep transformer models. \nModel Arch \nIter Loss test-clean test-other \nvggTrf. (768, 12) \nN \n2.87 \n6.46 \n(Params: 93M) \nY \n2.77 \n6.10 \nvggTrf. (512, 24) \nN \nnot converged \n(Params: 81M) \nY \n2.66 \n5.64 \n\n","rows":["vggTrf . ( 768 , 12 )","79","vggBLSTM ( 800 , 5 )","vggTrf . ( 768 , 20 )","Trf - FS ( 768 , 12 )","163","BLSTM ( 800 , 5 )","interpolated with the original CE loss with a","149","91","93","95","vggBLSTM ( 1000 , 6 )"],"columns":["test - clean","test - other","Table 2 : Architecture comparison on the Librispeech benchmark"],"mergedAllColumns":["CE losses are calculated separately . These additional CE losses are"],"numberCells":[{"number":"3.11","isBolded":false,"associatedRows":["BLSTM ( 800 , 5 )","79"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - clean"],"associatedMergedColumns":[]},{"number":"6.95","isBolded":false,"associatedRows":["vggBLSTM ( 800 , 5 )","95"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - other"],"associatedMergedColumns":[]},{"number":"3.04","isBolded":false,"associatedRows":["Trf - FS ( 768 , 12 )","91"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - clean"],"associatedMergedColumns":[]},{"number":"2.86","isBolded":false,"associatedRows":["vggBLSTM ( 1000 , 6 )","163"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - clean"],"associatedMergedColumns":[]},{"number":"0.3weight.Withthis","isBolded":false,"associatedRows":["interpolated with the original CE loss with a"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - other"],"associatedMergedColumns":["CE losses are calculated separately . These additional CE losses are"]},{"number":"7.44","isBolded":false,"associatedRows":["BLSTM ( 800 , 5 )","79"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - other"],"associatedMergedColumns":[]},{"number":"2.87","isBolded":false,"associatedRows":["vggTrf . ( 768 , 12 )","93"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - clean"],"associatedMergedColumns":[]},{"number":"6.46","isBolded":false,"associatedRows":["vggTrf . ( 768 , 12 )","93"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - other"],"associatedMergedColumns":[]},{"number":"6.10","isBolded":false,"associatedRows":["vggTrf . ( 768 , 20 )","149"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - other"],"associatedMergedColumns":[]},{"number":"6.63","isBolded":false,"associatedRows":["vggBLSTM ( 1000 , 6 )","163"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - other"],"associatedMergedColumns":[]},{"number":"2.77","isBolded":false,"associatedRows":["vggTrf . ( 768 , 20 )","149"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - clean"],"associatedMergedColumns":[]},{"number":"2.99","isBolded":false,"associatedRows":["vggBLSTM ( 800 , 5 )","95"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - clean"],"associatedMergedColumns":[]},{"number":"6.64","isBolded":false,"associatedRows":["Trf - FS ( 768 , 12 )","91"],"associatedColumns":["Table 2 : Architecture comparison on the Librispeech benchmark","test - other"],"associatedMergedColumns":[]}]},{"caption":"Table \n4: when the standard 4-gram LM is used in decoding, our sys-\ntem achieves 19% and 26% WER reduction on test-clean and \ntest-other respectively, over previous best 4-gram only hybrid \nsystem ","rows":["vggTrf . ( 768 , 12 )","Y","( Params : 93M )","( Params : 81M )","N"],"columns":["test - clean","not converged","Table 3 : Using iterated loss to train deep transformer models .","test - other"],"mergedAllColumns":[],"numberCells":[{"number":"6.46","isBolded":false,"associatedRows":["vggTrf . ( 768 , 12 )","N"],"associatedColumns":["Table 3 : Using iterated loss to train deep transformer models .","test - other"],"associatedMergedColumns":[]},{"number":"6.10","isBolded":false,"associatedRows":["( Params : 93M )","Y"],"associatedColumns":["Table 3 : Using iterated loss to train deep transformer models .","test - other"],"associatedMergedColumns":[]},{"number":"2.66","isBolded":false,"associatedRows":["( Params : 81M )","Y"],"associatedColumns":["Table 3 : Using iterated loss to train deep transformer models .","test - clean","not converged"],"associatedMergedColumns":[]},{"number":"2.77","isBolded":false,"associatedRows":["( Params : 93M )","Y"],"associatedColumns":["Table 3 : Using iterated loss to train deep transformer models .","test - clean"],"associatedMergedColumns":[]},{"number":"5.64","isBolded":false,"associatedRows":["( Params : 81M )","Y"],"associatedColumns":["Table 3 : Using iterated loss to train deep transformer models .","test - other","not converged"],"associatedMergedColumns":[]},{"number":"2.87","isBolded":false,"associatedRows":["vggTrf . ( 768 , 12 )","N"],"associatedColumns":["Table 3 : Using iterated loss to train deep transformer models .","test - clean"],"associatedMergedColumns":[]}]},{"caption":"Table 4: Comparison with previous best results on Librispeech. \n\"4g\" means the stand 4-gram LM is used; \"NNLM\" means a neu-\nral LM is used. \n\n","rows":["Karita et al . [ 30 ]","4g","NNLM","Park et al . [ 10 ]","Hybrid","NNLM + 4g","+NNLM","LAS","Le et al . [ 24 ]"],"columns":["LM","other","test -","clean"],"mergedAllColumns":["RWTH [ 38 ]","Han et al . [ 41 ]"],"numberCells":[{"number":"5.8","isBolded":false,"associatedRows":["LAS","Park et al . [ 10 ]","NNLM + 4g"],"associatedColumns":["test -","LM","other"],"associatedMergedColumns":[]},{"number":"2.3","isBolded":false,"associatedRows":["Hybrid","Karita et al . [ 30 ]","+NNLM"],"associatedColumns":["test -","LM","clean"],"associatedMergedColumns":[]},{"number":"5.59","isBolded":true,"associatedRows":["Hybrid","Le et al . [ 24 ]","4g"],"associatedColumns":["test -","LM","other"],"associatedMergedColumns":["Han et al . [ 41 ]"]},{"number":"8.8","isBolded":false,"associatedRows":["Hybrid","Karita et al . [ 30 ]","4g"],"associatedColumns":["test -","LM","other"],"associatedMergedColumns":[]},{"number":"2.60","isBolded":true,"associatedRows":["Hybrid","Le et al . [ 24 ]","4g"],"associatedColumns":["test -","LM","clean"],"associatedMergedColumns":["Han et al . [ 41 ]"]},{"number":"2.5","isBolded":false,"associatedRows":["LAS","Park et al . [ 10 ]","NNLM + 4g"],"associatedColumns":["test -","LM","clean"],"associatedMergedColumns":[]},{"number":"3.8","isBolded":false,"associatedRows":["Hybrid","Karita et al . [ 30 ]","4g"],"associatedColumns":["test -","LM","clean"],"associatedMergedColumns":[]},{"number":"5.0","isBolded":false,"associatedRows":["Hybrid","Karita et al . [ 30 ]","+NNLM"],"associatedColumns":["test -","LM","other"],"associatedMergedColumns":[]},{"number":"2.9","isBolded":false,"associatedRows":["Hybrid","Karita et al . [ 30 ]","4g"],"associatedColumns":["test -","LM","clean"],"associatedMergedColumns":["RWTH [ 38 ]"]},{"number":"7.6","isBolded":false,"associatedRows":["Hybrid","Le et al . [ 24 ]","4g"],"associatedColumns":["test -","LM","other"],"associatedMergedColumns":["Han et al . [ 41 ]"]},{"number":"5.8","isBolded":false,"associatedRows":["Hybrid","Karita et al . [ 30 ]","+NNLM"],"associatedColumns":["test -","LM","other"],"associatedMergedColumns":["RWTH [ 38 ]"]},{"number":"2.26","isBolded":false,"associatedRows":["Hybrid","Le et al . [ 24 ]","+NNLM"],"associatedColumns":["test -","LM","clean"],"associatedMergedColumns":["Han et al . [ 41 ]"]},{"number":"8.3","isBolded":false,"associatedRows":["Hybrid","Karita et al . [ 30 ]","4g"],"associatedColumns":["test -","LM","other"],"associatedMergedColumns":["RWTH [ 38 ]"]},{"number":"3.2","isBolded":false,"associatedRows":["Hybrid","Le et al . [ 24 ]","4g"],"associatedColumns":["test -","LM","clean"],"associatedMergedColumns":["Han et al . [ 41 ]"]},{"number":"4.85","isBolded":true,"associatedRows":["Hybrid","Le et al . [ 24 ]","+NNLM"],"associatedColumns":["test -","LM","other"],"associatedMergedColumns":["Han et al . [ 41 ]"]},{"number":"5.7","isBolded":false,"associatedRows":["Hybrid","Karita et al . [ 30 ]","NNLM"],"associatedColumns":["test -","LM","other"],"associatedMergedColumns":[]},{"number":"2.6","isBolded":false,"associatedRows":["Hybrid","Karita et al . [ 30 ]","NNLM"],"associatedColumns":["test -","LM","clean"],"associatedMergedColumns":[]},{"number":"2.2","isBolded":true,"associatedRows":["Hybrid","Le et al . [ 24 ]","+NNLM"],"associatedColumns":["test -","LM","clean"],"associatedMergedColumns":["RWTH [ 38 ]"]}]},{"caption":"Table 5: Forcing transformer models to use limited right context \n(RC) per layer during inference. Given a 12-layer transformer, an \nRC of 10 frames translates to 2.48 seconds of total lookahead. \nRC \ntest-clean test-other \n? \n2.87 \n6.45 \n50 \n3.01 \n7.12 \n20 \n3.29 \n8.10 \n10 \n3.65 \n9.01 \n4.5 Limited Right Context \nAll the transformer-based experiments so far used full context. To \nunderstand to what extent the transformer relies on future frames to \nderive embeddings for the current frames, we take the vggTrf(768, \n12) model (row 4, ","rows":["RC of 10 frames translates to","50","?","20","10"],"columns":["test - clean","RC","test - other","Forcing transformer models to use limited right context","Table 5 :"],"mergedAllColumns":["( RC ) per layer during inference . Given a 12 - layer transformer , an"],"numberCells":[{"number":"2.48secondsoftotallookahead.","isBolded":false,"associatedRows":["RC of 10 frames translates to"],"associatedColumns":["Forcing transformer models to use limited right context"],"associatedMergedColumns":["( RC ) per layer during inference . Given a 12 - layer transformer , an"]},{"number":"3.01","isBolded":false,"associatedRows":["50"],"associatedColumns":["Forcing transformer models to use limited right context","test - clean"],"associatedMergedColumns":["( RC ) per layer during inference . Given a 12 - layer transformer , an"]},{"number":"3.65","isBolded":false,"associatedRows":["10"],"associatedColumns":["Forcing transformer models to use limited right context","test - clean"],"associatedMergedColumns":["( RC ) per layer during inference . Given a 12 - layer transformer , an"]},{"number":"6.45","isBolded":false,"associatedRows":["?"],"associatedColumns":["Forcing transformer models to use limited right context","test - other"],"associatedMergedColumns":["( RC ) per layer during inference . Given a 12 - layer transformer , an"]},{"number":"4.5","isBolded":true,"associatedRows":[],"associatedColumns":["Table 5 :","RC"],"associatedMergedColumns":["( RC ) per layer during inference . Given a 12 - layer transformer , an"]},{"number":"7.12","isBolded":false,"associatedRows":["50"],"associatedColumns":["Forcing transformer models to use limited right context","test - other"],"associatedMergedColumns":["( RC ) per layer during inference . Given a 12 - layer transformer , an"]},{"number":"9.01","isBolded":false,"associatedRows":["10"],"associatedColumns":["Forcing transformer models to use limited right context","test - other"],"associatedMergedColumns":["( RC ) per layer during inference . Given a 12 - layer transformer , an"]},{"number":"8.10","isBolded":false,"associatedRows":["20"],"associatedColumns":["Forcing transformer models to use limited right context","test - other"],"associatedMergedColumns":["( RC ) per layer during inference . Given a 12 - layer transformer , an"]},{"number":"3.29","isBolded":false,"associatedRows":["20"],"associatedColumns":["Forcing transformer models to use limited right context","test - clean"],"associatedMergedColumns":["( RC ) per layer during inference . Given a 12 - layer transformer , an"]},{"number":"2.87","isBolded":false,"associatedRows":["?"],"associatedColumns":["Forcing transformer models to use limited right context","test - clean"],"associatedMergedColumns":["( RC ) per layer during inference . Given a 12 - layer transformer , an"]}]},{"caption":"Table 6: Experiment results on our internal English video ASR task. \nModel \ncurated clean noisy \nvggBLSTM(800,5) \n10.72 \n15.97 22.13 \nvggTrf(768,12) \n9.90 \n15.26 21.25 \n\n","rows":["vggTrf ( 768 , 12 )","vggBLSTM ( 800 , 5 )"],"columns":["curated","Table 6 : Experiment results on our internal English video ASR task .","clean","noisy"],"mergedAllColumns":[],"numberCells":[{"number":"15.97","isBolded":false,"associatedRows":["vggBLSTM ( 800 , 5 )"],"associatedColumns":["Table 6 : Experiment results on our internal English video ASR task .","clean"],"associatedMergedColumns":[]},{"number":"9.90","isBolded":false,"associatedRows":["vggTrf ( 768 , 12 )"],"associatedColumns":["Table 6 : Experiment results on our internal English video ASR task .","curated"],"associatedMergedColumns":[]},{"number":"21.25","isBolded":false,"associatedRows":["vggTrf ( 768 , 12 )"],"associatedColumns":["Table 6 : Experiment results on our internal English video ASR task .","noisy"],"associatedMergedColumns":[]},{"number":"22.13","isBolded":false,"associatedRows":["vggBLSTM ( 800 , 5 )"],"associatedColumns":["Table 6 : Experiment results on our internal English video ASR task .","noisy"],"associatedMergedColumns":[]},{"number":"15.26","isBolded":false,"associatedRows":["vggTrf ( 768 , 12 )"],"associatedColumns":["Table 6 : Experiment results on our internal English video ASR task .","clean"],"associatedMergedColumns":[]},{"number":"10.72","isBolded":false,"associatedRows":["vggBLSTM ( 800 , 5 )"],"associatedColumns":["Table 6 : Experiment results on our internal English video ASR task .","curated"],"associatedMergedColumns":[]}]}]