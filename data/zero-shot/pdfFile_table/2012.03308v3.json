[{"caption":"Layerwise Analysis. The pre-trained StyleGAN we used \nin most experiments is to generate images of 256 ? 256 \n(i.e., size 256), whose has 14 layers of the intermediate vec-\ntor. For a synthesis network trained to generate images of \n512 ? 512, the intermediate vector would be of shape (16, \n512) (and (18, 512) for 1024 ? 1024), where the number of \nthe layers L is determined by 2 log 2 R?2 and R is the image \nsize. In general, layers in the generator at lower resolutions \n(e.g., 4 ? 4 and 8 ? 8) control high-level styles such as eye-\nglasses and head pose, layers in the middle (e.g., as 16 ? 16 \nand 32 ? 32) control hairstyle and facial expression, while \nthe final layers (e.g., 64 ? 64 to 1024 ? 1024) control color \nschemes and fine-grained details. Based on empirical obser-\nvations, we list the attributes represented by different layers \nof a 14-layer StyleGAN in Table 1. The layers from 11-\n14 represent micro features or fine structures, such as stub-\nble, freckles, or skin pores, which can be regarded as the \nstochastic variation. High-resolution images contain lots of \nfacial details and cannot be obtained by simply upsampling \nfrom the lower-resolutions, making the stochastic variations \nespecially important as they improve the visual perception \nwithout affecting the main structures and attributes of the \nsynthesized image. \n\n","rows":["face shape","eye glasses","head pose"],"columns":["hair length , nose , lip","chin","n - th","cheekbones","attribute"],"mergedAllColumns":["hair color","face color","4 . Experiments","age"],"numberCells":[{"number":"1","isBolded":false,"associatedRows":[],"associatedColumns":["n - th"],"associatedMergedColumns":[]},{"number":"4.1.ExperimentsSetup","isBolded":true,"associatedRows":[],"associatedColumns":["n - th","attribute","hair length , nose , lip","cheekbones","chin"],"associatedMergedColumns":["4 . Experiments"]},{"number":"2","isBolded":false,"associatedRows":[],"associatedColumns":["n - th"],"associatedMergedColumns":["hair color"]},{"number":"4","isBolded":false,"associatedRows":[],"associatedColumns":["n - th"],"associatedMergedColumns":["age"]},{"number":"5","isBolded":false,"associatedRows":[],"associatedColumns":["n - th","hair length , nose , lip"],"associatedMergedColumns":["age"]},{"number":"8","isBolded":false,"associatedRows":["head pose"],"associatedColumns":["n - th"],"associatedMergedColumns":["hair color"]},{"number":"7","isBolded":false,"associatedRows":["eye glasses"],"associatedColumns":["n - th"],"associatedMergedColumns":[]},{"number":"3","isBolded":false,"associatedRows":[],"associatedColumns":["n - th"],"associatedMergedColumns":["face color"]},{"number":"9","isBolded":false,"associatedRows":["face shape"],"associatedColumns":["n - th"],"associatedMergedColumns":["face color"]},{"number":"6","isBolded":false,"associatedRows":[],"associatedColumns":["n - th","hair length , nose , lip","cheekbones"],"associatedMergedColumns":["age"]}]},{"caption":"Table 1. The Empirical Layerwise Analysis of a 14-layer Style-\nGAN Generator. The 13-th and 14-th layers are omitted since there \nis basically no visible difference. \n\n","rows":["face shape","eye glasses","head pose"],"columns":["hair length , nose , lip","chin","n - th","cheekbones","attribute"],"mergedAllColumns":["hair color","face color","4 . Experiments","age"],"numberCells":[{"number":"2","isBolded":false,"associatedRows":[],"associatedColumns":["n - th"],"associatedMergedColumns":["hair color"]},{"number":"4.1.ExperimentsSetup","isBolded":true,"associatedRows":[],"associatedColumns":["n - th","attribute","hair length , nose , lip","cheekbones","chin"],"associatedMergedColumns":["4 . Experiments"]},{"number":"9","isBolded":false,"associatedRows":["face shape"],"associatedColumns":["n - th"],"associatedMergedColumns":["face color"]},{"number":"8","isBolded":false,"associatedRows":["head pose"],"associatedColumns":["n - th"],"associatedMergedColumns":["hair color"]},{"number":"4","isBolded":false,"associatedRows":[],"associatedColumns":["n - th"],"associatedMergedColumns":["age"]},{"number":"6","isBolded":false,"associatedRows":[],"associatedColumns":["n - th","hair length , nose , lip","cheekbones"],"associatedMergedColumns":["age"]},{"number":"1","isBolded":false,"associatedRows":[],"associatedColumns":["n - th"],"associatedMergedColumns":[]},{"number":"7","isBolded":false,"associatedRows":["eye glasses"],"associatedColumns":["n - th"],"associatedMergedColumns":[]},{"number":"3","isBolded":false,"associatedRows":[],"associatedColumns":["n - th"],"associatedMergedColumns":["face color"]},{"number":"5","isBolded":false,"associatedRows":[],"associatedColumns":["n - th","hair length , nose , lip"],"associatedMergedColumns":["age"]}]},{"caption":"Table 2. Quantitative Comparison of Text-to-Image Generation. \nWe use FID, LPIPS, accuracy (Acc.), and realism (Real.) to com-\npare the state of the art and our method on the proposed Multi-\nmodal CelebA-HQ dataset. ? means the lower the better while ? \nmeans the opposite. \n\n","rows":["DFGAN [ 32 ]","ControlGAN [ 19 ]","TediGAN","AttnGAN [ 39 ]","DM - GAN [ 47 ]"],"columns":["Real . ( % ) ?","Acc . ( % ) ?","FID ?","LPIPS ?"],"mergedAllColumns":[],"numberCells":[{"number":"22.8","isBolded":false,"associatedRows":["DFGAN [ 32 ]"],"associatedColumns":["Acc . ( % ) ?"],"associatedMergedColumns":[]},{"number":"19.5","isBolded":false,"associatedRows":["DM - GAN [ 47 ]"],"associatedColumns":["Acc . ( % ) ?"],"associatedMergedColumns":[]},{"number":"131.05","isBolded":false,"associatedRows":["DM - GAN [ 47 ]"],"associatedColumns":["FID ?"],"associatedMergedColumns":[]},{"number":"0.456","isBolded":true,"associatedRows":["TediGAN"],"associatedColumns":["LPIPS ?"],"associatedMergedColumns":[]},{"number":"14.2","isBolded":false,"associatedRows":["AttnGAN [ 39 ]"],"associatedColumns":["Acc . ( % ) ?"],"associatedMergedColumns":[]},{"number":"0.522","isBolded":false,"associatedRows":["ControlGAN [ 19 ]"],"associatedColumns":["LPIPS ?"],"associatedMergedColumns":[]},{"number":"20.3","isBolded":false,"associatedRows":["AttnGAN [ 39 ]"],"associatedColumns":["Real . ( % ) ?"],"associatedMergedColumns":[]},{"number":"0.544","isBolded":false,"associatedRows":["DM - GAN [ 47 ]"],"associatedColumns":["LPIPS ?"],"associatedMergedColumns":[]},{"number":"106.37","isBolded":true,"associatedRows":["TediGAN"],"associatedColumns":["FID ?"],"associatedMergedColumns":[]},{"number":"18.2","isBolded":false,"associatedRows":["ControlGAN [ 19 ]"],"associatedColumns":["Acc . ( % ) ?"],"associatedMergedColumns":[]},{"number":"137.60","isBolded":false,"associatedRows":["DFGAN [ 32 ]"],"associatedColumns":["FID ?"],"associatedMergedColumns":[]},{"number":"22.5","isBolded":false,"associatedRows":["ControlGAN [ 19 ]"],"associatedColumns":["Real . ( % ) ?"],"associatedMergedColumns":[]},{"number":"0.512","isBolded":false,"associatedRows":["AttnGAN [ 39 ]"],"associatedColumns":["LPIPS ?"],"associatedMergedColumns":[]},{"number":"25.3","isBolded":true,"associatedRows":["TediGAN"],"associatedColumns":["Acc . ( % ) ?"],"associatedMergedColumns":[]},{"number":"0.581","isBolded":false,"associatedRows":["DFGAN [ 32 ]"],"associatedColumns":["LPIPS ?"],"associatedMergedColumns":[]},{"number":"31.7","isBolded":true,"associatedRows":["TediGAN"],"associatedColumns":["Real . ( % ) ?"],"associatedMergedColumns":[]},{"number":"12.8","isBolded":false,"associatedRows":["DM - GAN [ 47 ]"],"associatedColumns":["Real . ( % ) ?"],"associatedMergedColumns":[]},{"number":"125.98","isBolded":false,"associatedRows":["AttnGAN [ 39 ]"],"associatedColumns":["FID ?"],"associatedMergedColumns":[]},{"number":"25.5","isBolded":false,"associatedRows":["DFGAN [ 32 ]"],"associatedColumns":["Real . ( % ) ?"],"associatedMergedColumns":[]},{"number":"116.32","isBolded":false,"associatedRows":["ControlGAN [ 19 ]"],"associatedColumns":["FID ?"],"associatedMergedColumns":[]}]},{"caption":"Table 3. Quantitative Comparison of Text-Guided Image Manipu-\nlation. We use FID, accuracy (Acc.), and realism (Real.) to com-\npare with the state of the art ManiGAN [20]. \n\n","rows":["Real . ( % ) ?","Acc . ( % ) ?","FID ?"],"columns":["Ours","ManiGAN [ 20 ]","Non - CelebA","CelebA"],"mergedAllColumns":[],"numberCells":[{"number":"78.3","isBolded":false,"associatedRows":["Real . ( % ) ?"],"associatedColumns":["Non - CelebA","Ours"],"associatedMergedColumns":[]},{"number":"117.89","isBolded":true,"associatedRows":["FID ?"],"associatedColumns":["CelebA","ManiGAN [ 20 ]"],"associatedMergedColumns":[]},{"number":"12.8","isBolded":true,"associatedRows":["Acc . ( % ) ?"],"associatedColumns":["Non - CelebA","ManiGAN [ 20 ]"],"associatedMergedColumns":[]},{"number":"36.2","isBolded":true,"associatedRows":["Real . ( % ) ?"],"associatedColumns":["CelebA","ManiGAN [ 20 ]"],"associatedMergedColumns":[]},{"number":"135.47","isBolded":false,"associatedRows":["FID ?"],"associatedColumns":["Non - CelebA","Ours"],"associatedMergedColumns":[]},{"number":"4.2.ComparisonwithState-of-the-ArtMethods","isBolded":false,"associatedRows":[],"associatedColumns":["Non - CelebA","ManiGAN [ 20 ]"],"associatedMergedColumns":[]},{"number":"59.1","isBolded":false,"associatedRows":["Acc . ( % ) ?"],"associatedColumns":["CelebA","Ours"],"associatedMergedColumns":[]},{"number":"107.25","isBolded":false,"associatedRows":["FID ?"],"associatedColumns":["CelebA","Ours"],"associatedMergedColumns":[]},{"number":"40.9","isBolded":true,"associatedRows":["Acc . ( % ) ?"],"associatedColumns":["CelebA","ManiGAN [ 20 ]"],"associatedMergedColumns":[]},{"number":"87.2","isBolded":false,"associatedRows":["Acc . ( % ) ?"],"associatedColumns":["Non - CelebA","Ours"],"associatedMergedColumns":[]},{"number":"21.7","isBolded":true,"associatedRows":["Real . ( % ) ?"],"associatedColumns":["Non - CelebA","ManiGAN [ 20 ]"],"associatedMergedColumns":[]},{"number":"143.39","isBolded":true,"associatedRows":["FID ?"],"associatedColumns":["Non - CelebA","ManiGAN [ 20 ]"],"associatedMergedColumns":[]},{"number":"63.8","isBolded":false,"associatedRows":["Real . ( % ) ?"],"associatedColumns":["CelebA","Ours"],"associatedMergedColumns":[]}]}]