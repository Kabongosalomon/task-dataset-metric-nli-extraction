[{"caption":"Table 1: Human Evaluation on Kinetics-600 \n\nPrefer Video VQ-VAE Prefer [27] Indifferent \n\n65.7% \n12.8% \n21.5% \n\n","rows":[],"columns":["Prefer Video VQ - VAE","Table 1 : Human Evaluation on Kinetics - 600","Prefer [ 27 ]","Indifferent"],"mergedAllColumns":[],"numberCells":[{"number":"12.8%","isBolded":false,"associatedRows":[],"associatedColumns":["Table 1 : Human Evaluation on Kinetics - 600","Prefer [ 27 ]"],"associatedMergedColumns":[]},{"number":"21.5%","isBolded":false,"associatedRows":[],"associatedColumns":["Table 1 : Human Evaluation on Kinetics - 600","Indifferent"],"associatedMergedColumns":[]},{"number":"65.7%","isBolded":false,"associatedRows":[],"associatedColumns":["Table 1 : Human Evaluation on Kinetics - 600","Prefer Video VQ - VAE"],"associatedMergedColumns":[]}]},{"caption":"Table 3: VQ-VAE Architecture Details \n\nParameter \nValue \n\nInput Size \n256?256?16?3 \nLatent layers \n32?32?4, 64?64?8 \n? (commitment loss coefficient) \n0.25 \nBatch size \n16 \nHidden units \n128 \nResidual units \n32 \nLayers \n2 \nCodebook size \n512 \nCodebook dimension \n64 \nFirst Stage Encoder twh-Conv Filter Size \n4 8 8 \nFirst Stage Encoder twh-Conv Filter Stride \n2 4 4 \nSecond Stage Encoder twh-Conv Filter Size \n4 4 4 \nSecond Stage Encoder twh-Conv Filter Stride \n2 2 2 \nUpsampling twh-Conv Filter Size \n4 8 8 \nUpsampling twh-Conv Filter Stride \n2 4 4 \nTraining steps \n1000000 \n","rows":["Upsampling twh - Conv Filter Size","Upsampling twh - Conv Filter Stride","? ( commitment loss coefficient )","First Stage Encoder twh - Conv Filter Stride","First Stage Encoder twh - Conv Filter Size","Second Stage Encoder twh - Conv Filter Size","Layers","Second Stage Encoder twh - Conv Filter Stride"],"columns":["32?32?4 , 64?64?8","256?256?16?3","Table 3 : VQ - VAE Architecture Details","16","512","128","Value","64","32"],"mergedAllColumns":[],"numberCells":[{"number":"4","isBolded":false,"associatedRows":["Upsampling twh - Conv Filter Stride"],"associatedColumns":["Table 3 : VQ - VAE Architecture Details","Value","256?256?16?3","32?32?4 , 64?64?8","16","128","32","512","64"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["Second Stage Encoder twh - Conv Filter Size"],"associatedColumns":["Table 3 : VQ - VAE Architecture Details","Value","256?256?16?3","32?32?4 , 64?64?8","16","128","32","512","64"],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["Second Stage Encoder twh - Conv Filter Stride"],"associatedColumns":["Table 3 : VQ - VAE Architecture Details","Value","256?256?16?3","32?32?4 , 64?64?8","16","128","32","512","64"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":["First Stage Encoder twh - Conv Filter Size"],"associatedColumns":["Table 3 : VQ - VAE Architecture Details","Value","256?256?16?3","32?32?4 , 64?64?8","16","128","32","512","64"],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["Second Stage Encoder twh - Conv Filter Stride"],"associatedColumns":["Table 3 : VQ - VAE Architecture Details","Value","256?256?16?3","32?32?4 , 64?64?8","16","128","32","512","64"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["Upsampling twh - Conv Filter Size"],"associatedColumns":["Table 3 : VQ - VAE Architecture Details","Value","256?256?16?3","32?32?4 , 64?64?8","16","128","32","512","64"],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["Upsampling twh - Conv Filter Stride"],"associatedColumns":["Table 3 : VQ - VAE Architecture Details","Value","256?256?16?3","32?32?4 , 64?64?8","16","128","32","512","64"],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["Second Stage Encoder twh - Conv Filter Stride"],"associatedColumns":["Table 3 : VQ - VAE Architecture Details","Value","256?256?16?3","32?32?4 , 64?64?8","16","128","32","512","64"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":["First Stage Encoder twh - Conv Filter Size"],"associatedColumns":["Table 3 : VQ - VAE Architecture Details","Value","256?256?16?3","32?32?4 , 64?64?8","16","128","32","512","64"],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["First Stage Encoder twh - Conv Filter Stride"],"associatedColumns":["Table 3 : VQ - VAE Architecture Details","Value","256?256?16?3","32?32?4 , 64?64?8","16","128","32","512","64"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["First Stage Encoder twh - Conv Filter Stride"],"associatedColumns":["Table 3 : VQ - VAE Architecture Details","Value","256?256?16?3","32?32?4 , 64?64?8","16","128","32","512","64"],"associatedMergedColumns":[]},{"number":"0.25","isBolded":false,"associatedRows":["? ( commitment loss coefficient )"],"associatedColumns":["Table 3 : VQ - VAE Architecture Details","Value","256?256?16?3","32?32?4 , 64?64?8"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["Second Stage Encoder twh - Conv Filter Size"],"associatedColumns":["Table 3 : VQ - VAE Architecture Details","Value","256?256?16?3","32?32?4 , 64?64?8","16","128","32","512","64"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["Upsampling twh - Conv Filter Stride"],"associatedColumns":["Table 3 : VQ - VAE Architecture Details","Value","256?256?16?3","32?32?4 , 64?64?8","16","128","32","512","64"],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["Layers"],"associatedColumns":["Table 3 : VQ - VAE Architecture Details","Value","256?256?16?3","32?32?4 , 64?64?8","16","128","32"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["First Stage Encoder twh - Conv Filter Stride"],"associatedColumns":["Table 3 : VQ - VAE Architecture Details","Value","256?256?16?3","32?32?4 , 64?64?8","16","128","32","512","64"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":["Upsampling twh - Conv Filter Size"],"associatedColumns":["Table 3 : VQ - VAE Architecture Details","Value","256?256?16?3","32?32?4 , 64?64?8","16","128","32","512","64"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":["Upsampling twh - Conv Filter Size"],"associatedColumns":["Table 3 : VQ - VAE Architecture Details","Value","256?256?16?3","32?32?4 , 64?64?8","16","128","32","512","64"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["Second Stage Encoder twh - Conv Filter Size"],"associatedColumns":["Table 3 : VQ - VAE Architecture Details","Value","256?256?16?3","32?32?4 , 64?64?8","16","128","32","512","64"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["First Stage Encoder twh - Conv Filter Size"],"associatedColumns":["Table 3 : VQ - VAE Architecture Details","Value","256?256?16?3","32?32?4 , 64?64?8","16","128","32","512","64"],"associatedMergedColumns":[]}]},{"caption":"Table 4: PixelCNN Prior Details \n\nParameter \nTop Prior Bottom Prior \n\nInput size \n32?32?3 \n64?64 \nBatch size \n512 \n32 \nHidden units \n512 \n512 \nResidual units \n1024 \n1024 \nLayers \n40 \n20 \nAttention layers \n8 \n4 \nAttention heads \n8 \n8 \nConv Filter size \n3 \n5 \nDropout \n0.5 \n0.0 \nTraining steps \n1016000 \n950000 \n\nParameter \nValues \n\nInput size \n32?32 (upsampled to 64?64?2), 64?64?2 \nHidden units \n512 \nResidual units \n128 \nLayers \n4 \n\n","rows":["Conv Filter size","Attention heads","Layers","Attention layers","Dropout"],"columns":["64?64","32?32 ( upsampled to 64?64?2 ) , 64?64?2","Values","1016000","32?32?3","Bottom Prior","512","1024","Top Prior","128","40","20","Table 4 : PixelCNN Prior Details","32"],"mergedAllColumns":[],"numberCells":[{"number":"8","isBolded":false,"associatedRows":["Attention layers"],"associatedColumns":["Table 4 : PixelCNN Prior Details","Top Prior","32?32?3","512","512","1024","40"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":["Attention heads"],"associatedColumns":["Table 4 : PixelCNN Prior Details","Top Prior","32?32?3","512","512","1024","40"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":["Attention heads"],"associatedColumns":["Table 4 : PixelCNN Prior Details","Bottom Prior","64?64","32","512","1024","20"],"associatedMergedColumns":[]},{"number":"3","isBolded":false,"associatedRows":["Conv Filter size"],"associatedColumns":["Table 4 : PixelCNN Prior Details","Top Prior","32?32?3","512","512","1024","40"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["Attention layers"],"associatedColumns":["Table 4 : PixelCNN Prior Details","Bottom Prior","64?64","32","512","1024","20"],"associatedMergedColumns":[]},{"number":"0.5","isBolded":false,"associatedRows":["Dropout"],"associatedColumns":["Table 4 : PixelCNN Prior Details","Top Prior","32?32?3","512","512","1024","40"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["Layers","Dropout"],"associatedColumns":["Table 4 : PixelCNN Prior Details","Bottom Prior","32?32?3","512","512","1024","20","1016000","Values","32?32 ( upsampled to 64?64?2 ) , 64?64?2","512","128"],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["Dropout"],"associatedColumns":["Table 4 : PixelCNN Prior Details","Bottom Prior","64?64","32","512","1024","20"],"associatedMergedColumns":[]},{"number":"5","isBolded":false,"associatedRows":["Conv Filter size"],"associatedColumns":["Table 4 : PixelCNN Prior Details","Bottom Prior","64?64","32","512","1024","20"],"associatedMergedColumns":[]}]},{"caption":"Table 5: Top Prior Conditioning Stack. The conditioning frames 256?256?4?3 are compressed \nusing our VQ-VAE into a 32?32 and 64?64?2 space. These two layers are then concatenated, \ndownsampled back down to 32?32?256 by a convolutional layer, tiled to 32?32?3?256, and \nfinally fed through another convolutional layer to 32?32?3?512 before being fed through four \nresidual blocks. \n\n","rows":["Conv Filter size","Attention heads","Layers","Attention layers","Dropout"],"columns":["64?64","32?32 ( upsampled to 64?64?2 ) , 64?64?2","Values","1016000","32?32?3","Bottom Prior","512","1024","Top Prior","128","40","20","Table 4 : PixelCNN Prior Details","32"],"mergedAllColumns":[],"numberCells":[{"number":"4","isBolded":false,"associatedRows":["Attention layers"],"associatedColumns":["Table 4 : PixelCNN Prior Details","Bottom Prior","64?64","32","512","1024","20"],"associatedMergedColumns":[]},{"number":"5","isBolded":false,"associatedRows":["Conv Filter size"],"associatedColumns":["Table 4 : PixelCNN Prior Details","Bottom Prior","64?64","32","512","1024","20"],"associatedMergedColumns":[]},{"number":"0.0","isBolded":false,"associatedRows":["Dropout"],"associatedColumns":["Table 4 : PixelCNN Prior Details","Bottom Prior","64?64","32","512","1024","20"],"associatedMergedColumns":[]},{"number":"3","isBolded":false,"associatedRows":["Conv Filter size"],"associatedColumns":["Table 4 : PixelCNN Prior Details","Top Prior","32?32?3","512","512","1024","40"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":["Attention layers"],"associatedColumns":["Table 4 : PixelCNN Prior Details","Top Prior","32?32?3","512","512","1024","40"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["Layers","Dropout"],"associatedColumns":["Table 4 : PixelCNN Prior Details","Bottom Prior","32?32?3","512","512","1024","20","1016000","Values","32?32 ( upsampled to 64?64?2 ) , 64?64?2","512","128"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":["Attention heads"],"associatedColumns":["Table 4 : PixelCNN Prior Details","Top Prior","32?32?3","512","512","1024","40"],"associatedMergedColumns":[]},{"number":"0.5","isBolded":false,"associatedRows":["Dropout"],"associatedColumns":["Table 4 : PixelCNN Prior Details","Top Prior","32?32?3","512","512","1024","40"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":["Attention heads"],"associatedColumns":["Table 4 : PixelCNN Prior Details","Bottom Prior","64?64","32","512","1024","20"],"associatedMergedColumns":[]}]},{"caption":"Table 6: Bottom Prior Conditioning Stack. Let n be the timestep to be modeled by the bottom prior. \nThe 32?32?4 top layer is upsampled to 64?64?8?1024 through a series of three convolutional \nlayers with kernel sizes (4, 3, 3)?(3,4,4)?(3, 3, 3) and strides (2, 1, 1)?(1, 2, 2) ?(1, 1, 1) \n","rows":["Hidden units"],"columns":["Values","32?32?4 , 64?64?4"],"mergedAllColumns":[],"numberCells":[{"number":"1024","isBolded":false,"associatedRows":["Hidden units"],"associatedColumns":["Values","32?32?4 , 64?64?4"],"associatedMergedColumns":[]}]}]