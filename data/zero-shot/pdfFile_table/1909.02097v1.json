[{"caption":"892 0.944 \n0.261 \n0.190 \nG + B-Ultra + L \n0.937 0.984 \n0.265 \n0.195 \n\nTable 1: Automatic metric scores for the image cap-\ntioning task on Conceptual Captions. Ablation results \nare reported for our model using different sets of visual \nfeatures. The top two baselines are from the Concep-\ntual Captions Leaderboard as of August 30, 2019. \n\n","rows":["892","G + B - Ultra + L","+"],"columns":["-"],"mergedAllColumns":[],"numberCells":[{"number":"0.195","isBolded":true,"associatedRows":["G + B - Ultra + L"],"associatedColumns":["-"],"associatedMergedColumns":[]},{"number":"0.190","isBolded":false,"associatedRows":["G + B - Ultra + L","892"],"associatedColumns":["-"],"associatedMergedColumns":[]},{"number":"0.984","isBolded":true,"associatedRows":["G + B - Ultra + L"],"associatedColumns":["-"],"associatedMergedColumns":[]},{"number":"0.937","isBolded":true,"associatedRows":["G + B - Ultra + L"],"associatedColumns":["-"],"associatedMergedColumns":[]},{"number":"0.888","isBolded":false,"associatedRows":["+"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.265","isBolded":true,"associatedRows":["G + B - Ultra + L"],"associatedColumns":["-"],"associatedMergedColumns":[]},{"number":"0.944","isBolded":false,"associatedRows":["G + B - Ultra + L","892"],"associatedColumns":["-"],"associatedMergedColumns":[]},{"number":"0.261","isBolded":false,"associatedRows":["G + B - Ultra + L","892"],"associatedColumns":["-"],"associatedMergedColumns":[]}]},{"caption":"Table 2: Accuracy (%) on the test-standard split for \nthe VQA task on the VizWiz dataset. Additionally, we \nprovide accuracy per answer type: yes/no (y/n), num-\nber (num), unanswerable (unans), and the rest (other). \nThe baselines include VizWiz ","rows":["VizWiz","Ours ( Ultra )","Ours ( FRCNN )","BAN"],"columns":["all","other","Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","num","unans","Ultra is more capable than FRCNN of dealing with images with unfamiliar","along with ground - truth captions .","y / n"],"mergedAllColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."],"numberCells":[{"number":"24.3","isBolded":false,"associatedRows":["Ours ( FRCNN )"],"associatedColumns":["Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","along with ground - truth captions .","num"],"associatedMergedColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."]},{"number":"35.4","isBolded":true,"associatedRows":["Ours ( Ultra )"],"associatedColumns":["Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","Ultra is more capable than FRCNN of dealing with images with unfamiliar","other"],"associatedMergedColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."]},{"number":"27.3","isBolded":false,"associatedRows":["VizWiz"],"associatedColumns":["Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","Ultra is more capable than FRCNN of dealing with images with unfamiliar","other"],"associatedMergedColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."]},{"number":"51.6","isBolded":false,"associatedRows":["BAN"],"associatedColumns":["Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","along with ground - truth captions .","all"],"associatedMergedColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."]},{"number":"85.3","isBolded":true,"associatedRows":["BAN"],"associatedColumns":["Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","Ultra is more capable than FRCNN of dealing with images with unfamiliar","unans"],"associatedMergedColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."]},{"number":"21.0","isBolded":false,"associatedRows":["VizWiz"],"associatedColumns":["Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","along with ground - truth captions .","num"],"associatedMergedColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."]},{"number":"31.5","isBolded":false,"associatedRows":["BAN"],"associatedColumns":["Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","Ultra is more capable than FRCNN of dealing with images with unfamiliar","other"],"associatedMergedColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."]},{"number":"32.1","isBolded":false,"associatedRows":["Ours ( FRCNN )"],"associatedColumns":["Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","Ultra is more capable than FRCNN of dealing with images with unfamiliar","other"],"associatedMergedColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."]},{"number":"28.8","isBolded":true,"associatedRows":["Ours ( Ultra )"],"associatedColumns":["Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","along with ground - truth captions .","num"],"associatedMergedColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."]},{"number":"46.9","isBolded":false,"associatedRows":["VizWiz"],"associatedColumns":["Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","along with ground - truth captions .","all"],"associatedMergedColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."]},{"number":"17.9","isBolded":false,"associatedRows":["BAN"],"associatedColumns":["Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","along with ground - truth captions .","num"],"associatedMergedColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."]},{"number":"66.7","isBolded":false,"associatedRows":["Ours ( FRCNN )"],"associatedColumns":["Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","along with ground - truth captions .","y / n"],"associatedMergedColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."]},{"number":"68.1","isBolded":true,"associatedRows":["BAN"],"associatedColumns":["Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","along with ground - truth captions .","y / n"],"associatedMergedColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."]},{"number":"53.7","isBolded":true,"associatedRows":["Ours ( Ultra )"],"associatedColumns":["Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","along with ground - truth captions .","all"],"associatedMergedColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."]},{"number":"59.6","isBolded":false,"associatedRows":["VizWiz"],"associatedColumns":["Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","along with ground - truth captions .","y / n"],"associatedMergedColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."]},{"number":"84.0","isBolded":false,"associatedRows":["Ours ( Ultra )"],"associatedColumns":["Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","Ultra is more capable than FRCNN of dealing with images with unfamiliar","unans"],"associatedMergedColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."]},{"number":"80.5","isBolded":false,"associatedRows":["VizWiz"],"associatedColumns":["Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","Ultra is more capable than FRCNN of dealing with images with unfamiliar","unans"],"associatedMergedColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."]},{"number":"68.1","isBolded":true,"associatedRows":["Ours ( Ultra )"],"associatedColumns":["Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","along with ground - truth captions .","y / n"],"associatedMergedColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."]},{"number":"85.0","isBolded":false,"associatedRows":["Ours ( FRCNN )"],"associatedColumns":["Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","Ultra is more capable than FRCNN of dealing with images with unfamiliar","unans"],"associatedMergedColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."]},{"number":"51.9","isBolded":false,"associatedRows":["Ours ( FRCNN )"],"associatedColumns":["Figure 2 : Qualitative results from our image captioning models using B - FRCNN vs . B - Ultra ( see text for details ) ,","along with ground - truth captions .","all"],"associatedMergedColumns":["objects , those that do not perfectly fall into the domain where the Faster R - CNN object detector is trained on ."]}]},{"caption":"Table 3: Accuracy (%) for the VQA task on the VizWiz dataset. Additionally, we provide accuracy per answer type \non the test-dev and test-standard splits: yes/no (y/n), number (num), unanswerable (unans), and the rest (other). \n","rows":["Ours ( Ultra )","VizWiz ( Gurari et al . , 2018 )","Ours ( FRCNN )","BAN ( Kim et al . , 2018 )","-"],"columns":["other","Tagger","num","Label","Embedder","unans","Image","Feature","\" Hat \" , \" Sunglasses \"","test - standard","Network","Box Feat 1","\" Woman \" , \" Man \" ,","Box Feat 2","Extractor",". . .","Label Emb 2","Trainable","all","val","Label Emb 1","Pipeline for converting an image to a sequence of image features in our highest performing image","Captioning","\u0026","y / n","test - dev","Model","Region","Global Feat","Proposal"],"mergedAllColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."],"numberCells":[{"number":"35.4","isBolded":true,"associatedRows":["Ours ( Ultra )","-","-","-","-","-","-"],"associatedColumns":["Label Emb 1","Label Emb 2","Trainable","\u0026","Captioning","Model",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - standard","other"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"71.7","isBolded":false,"associatedRows":["Ours ( Ultra )","-","-"],"associatedColumns":["\" Woman \" , \" Man \" ,","\" Hat \" , \" Sunglasses \"","Tagger","Global Feat","Region","Proposal","Network","Pipeline for converting an image to a sequence of image features in our highest performing image","test - dev","y / n"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"59.6","isBolded":false,"associatedRows":["VizWiz ( Gurari et al . , 2018 )","-","-","-","-","-","-"],"associatedColumns":["Label Emb 1","Label","Embedder","Global Feat","Box Feat 1","Box Feat 2",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - standard","y / n"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"68.1","isBolded":true,"associatedRows":["BAN ( Kim et al . , 2018 )","-","-","-","-","-","-"],"associatedColumns":["Label Emb 1","Label","Embedder","Global Feat","Box Feat 1","Box Feat 2",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - standard","y / n"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"55.1","isBolded":false,"associatedRows":["Ours ( Ultra )","-"],"associatedColumns":["\" Woman \" , \" Man \" ,","Image","Tagger","Global Feat","Region","Proposal","Network","Pipeline for converting an image to a sequence of image features in our highest performing image","val","all"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"36.7","isBolded":false,"associatedRows":["Ours ( Ultra )","-","-","-","-","-"],"associatedColumns":["\" Woman \" , \" Man \" ,","Label","Embedder","Global Feat","Feature","Extractor",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - dev","other"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"85.3","isBolded":true,"associatedRows":["BAN ( Kim et al . , 2018 )","-","-","-","-","-","-"],"associatedColumns":["Label Emb 1","Label Emb 2","Trainable","\u0026","Captioning","Box Feat 2",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - standard","unans"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"53.7","isBolded":true,"associatedRows":["Ours ( Ultra )","-","-","-","-","-","-"],"associatedColumns":["Label Emb 1","Label","Embedder","Global Feat","Feature","Box Feat 2",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - standard","all"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"53.6","isBolded":false,"associatedRows":["Ours ( FRCNN )","-"],"associatedColumns":["\" Woman \" , \" Man \" ,","Image","Tagger","Global Feat","Region","Proposal","Network","Pipeline for converting an image to a sequence of image features in our highest performing image","val","all"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"84.4","isBolded":false,"associatedRows":["Ours ( Ultra )","-","-","-","-"],"associatedColumns":["\" Woman \" , \" Man \" ,","Label","Embedder","Global Feat","Feature","Extractor","Network","Pipeline for converting an image to a sequence of image features in our highest performing image","test - dev","unans"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"21.0","isBolded":false,"associatedRows":["VizWiz ( Gurari et al . , 2018 )","-","-","-","-","-","-"],"associatedColumns":["Label Emb 1","Label Emb 2",". . .","Global Feat","Box Feat 1","Box Feat 2",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - standard","num"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"33.3","isBolded":false,"associatedRows":["Ours ( FRCNN )","-","-","-","-","-"],"associatedColumns":["\" Woman \" , \" Man \" ,","Label","Embedder","Global Feat","Feature","Extractor",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - dev","other"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"84.0","isBolded":false,"associatedRows":["Ours ( Ultra )","-","-","-","-","-","-"],"associatedColumns":["Label Emb 1","Label Emb 2","Trainable","\u0026","Captioning","Box Feat 2",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - standard","unans"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"56.8","isBolded":false,"associatedRows":["Ours ( Ultra )"],"associatedColumns":["\" Woman \" , \" Man \" ,","Image","Tagger","Global Feat","Region","Proposal","Network","Pipeline for converting an image to a sequence of image features in our highest performing image","val","all"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"51.9","isBolded":false,"associatedRows":["Ours ( FRCNN )","-","-","-","-","-","-"],"associatedColumns":["Label Emb 1","Label","Embedder","Global Feat","Feature","Box Feat 2",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - standard","all"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"85.9","isBolded":false,"associatedRows":["Ours ( FRCNN )","-","-","-","-"],"associatedColumns":["\" Woman \" , \" Man \" ,","Label","Embedder","Global Feat","Feature","Extractor","Network","Pipeline for converting an image to a sequence of image features in our highest performing image","test - dev","unans"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"55.2","isBolded":false,"associatedRows":["Ours ( FRCNN )"],"associatedColumns":["\" Woman \" , \" Man \" ,","Image","Tagger","Global Feat","Region","Proposal","Network","Pipeline for converting an image to a sequence of image features in our highest performing image","val","all"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"68.1","isBolded":true,"associatedRows":["Ours ( Ultra )","-","-","-","-","-","-"],"associatedColumns":["Label Emb 1","Label","Embedder","Global Feat","Box Feat 1","Box Feat 2",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - standard","y / n"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"80.5","isBolded":false,"associatedRows":["VizWiz ( Gurari et al . , 2018 )","-","-","-","-","-","-"],"associatedColumns":["Label Emb 1","Label Emb 2","Trainable","\u0026","Captioning","Box Feat 2",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - standard","unans"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"72.7","isBolded":false,"associatedRows":["Ours ( FRCNN )","-","-"],"associatedColumns":["\" Woman \" , \" Man \" ,","\" Hat \" , \" Sunglasses \"","Tagger","Global Feat","Region","Proposal","Network","Pipeline for converting an image to a sequence of image features in our highest performing image","test - dev","y / n"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"27.3","isBolded":false,"associatedRows":["VizWiz ( Gurari et al . , 2018 )","-","-","-","-","-","-"],"associatedColumns":["Label Emb 1","Label Emb 2","Trainable","\u0026","Captioning","Model",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - standard","other"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"17.9","isBolded":false,"associatedRows":["BAN ( Kim et al . , 2018 )","-","-","-","-","-","-"],"associatedColumns":["Label Emb 1","Label Emb 2",". . .","Global Feat","Box Feat 1","Box Feat 2",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - standard","num"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"31.5","isBolded":false,"associatedRows":["BAN ( Kim et al . , 2018 )","-","-","-","-","-","-"],"associatedColumns":["Label Emb 1","Label Emb 2","Trainable","\u0026","Captioning","Model",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - standard","other"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"32.1","isBolded":false,"associatedRows":["Ours ( FRCNN )","-","-","-","-","-","-"],"associatedColumns":["Label Emb 1","Label Emb 2","Trainable","\u0026","Captioning","Model",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - standard","other"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"85.0","isBolded":false,"associatedRows":["Ours ( FRCNN )","-","-","-","-","-","-"],"associatedColumns":["Label Emb 1","Label Emb 2","Trainable","\u0026","Captioning","Box Feat 2",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - standard","unans"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"31.6","isBolded":false,"associatedRows":["Ours ( Ultra )","-","-","-"],"associatedColumns":["\" Woman \" , \" Man \" ,","\" Hat \" , \" Sunglasses \"","Embedder","Global Feat","Region","Proposal","Network","Pipeline for converting an image to a sequence of image features in our highest performing image","test - dev","num"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"51.6","isBolded":false,"associatedRows":["BAN ( Kim et al . , 2018 )","-","-","-","-","-","-"],"associatedColumns":["Label Emb 1","Label","Embedder","Global Feat","Feature","Box Feat 2",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - standard","all"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"22.7","isBolded":false,"associatedRows":["Ours ( FRCNN )","-","-","-"],"associatedColumns":["\" Woman \" , \" Man \" ,","\" Hat \" , \" Sunglasses \"","Embedder","Global Feat","Region","Proposal","Network","Pipeline for converting an image to a sequence of image features in our highest performing image","test - dev","num"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"46.9","isBolded":false,"associatedRows":["VizWiz ( Gurari et al . , 2018 )","-","-","-","-","-","-"],"associatedColumns":["Label Emb 1","Label","Embedder","Global Feat","Feature","Box Feat 2",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - standard","all"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"24.3","isBolded":false,"associatedRows":["Ours ( FRCNN )","-","-","-","-","-","-"],"associatedColumns":["Label Emb 1","Label Emb 2",". . .","Global Feat","Box Feat 1","Box Feat 2",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - standard","num"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"28.8","isBolded":true,"associatedRows":["Ours ( Ultra )","-","-","-","-","-","-"],"associatedColumns":["Label Emb 1","Label Emb 2",". . .","Global Feat","Box Feat 1","Box Feat 2",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - standard","num"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]},{"number":"66.7","isBolded":false,"associatedRows":["Ours ( FRCNN )","-","-","-","-","-","-"],"associatedColumns":["Label Emb 1","Label","Embedder","Global Feat","Box Feat 1","Box Feat 2",". . .","Pipeline for converting an image to a sequence of image features in our highest performing image","test - standard","y / n"],"associatedMergedColumns":["captioning model on the Conceptual Captions benchmark , used as input to the Transformer - based model ."]}]}]