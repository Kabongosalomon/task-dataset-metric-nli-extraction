[{"caption":"NMT \n\nVQA \nModel \nObjective \nE \nPPL BLEU \nNLL \nEval \n\nSoft Attention \nlog p(y | E[z]) \n-\n7.17 \n32.77 \n1.76 58.93 \nMarginal Likelihood \nlog E[p] \nEnum \n6.34 \n33.29 \n1.69 60.33 \nHard Attention \nEp[log p] \nEnum \n7.37 \n31.40 \n1.78 57.60 \nHard Attention \nEp[log p] \nSample 7.38 \n31.00 \n1.82 56.30 \nVariational Relaxed Attention Eq[log p] ? KL Sample 7.58 \n30.05 \n-\n-\nVariational Attention \nEq[log p] ? KL \nEnum \n6.08 \n33.68 \n1.69 58.44 \nVariational Attention \nEq[log p] ? KL Sample 6.17 \n33.30 \n1.75 57.52 \n\nTable 1: Evaluation on NMT and VQA for the various models. E column indicates whether the expectation \nis calculated via enumeration (Enum) or a single sample (Sample) during training. For NMT we evaluate \nintrinsically on perplexity (PPL) (lower is better) and extrinsically on BLEU (higher is better), where for BLEU \nwe perform beam search with beam size 10 and length penalty (see Appendix B for further details). For VQA \nwe evaluate intrinsically on negative log-likelihood (NLL) (lower is better) and extrinsically on VQA evaluation \nmetric (higher is better). All results except for relaxed attention use enumeration at test time. \n\n","rows":["Soft Attention","Enum","Eq [ log p ] ? KL","log p ( y | E [ z ] )","Hard Attention","Sample","Ep [ log p ]","Variational Attention","log E [ p ]","-","Marginal Likelihood","Variational Relaxed Attention"],"columns":["NMT","BLEU","VQA","PPL","Eval","-","NLL"],"mergedAllColumns":[],"numberCells":[{"number":"7.58","isBolded":false,"associatedRows":["Variational Relaxed Attention","Eq [ log p ] ? KL","Sample"],"associatedColumns":["NMT","PPL"],"associatedMergedColumns":[]},{"number":"31.00","isBolded":false,"associatedRows":["Hard Attention","Ep [ log p ]","Sample"],"associatedColumns":["NMT","BLEU"],"associatedMergedColumns":[]},{"number":"6.08","isBolded":false,"associatedRows":["Variational Attention","Eq [ log p ] ? KL","Enum"],"associatedColumns":["NMT","PPL","-"],"associatedMergedColumns":[]},{"number":"7.38","isBolded":false,"associatedRows":["Hard Attention","Ep [ log p ]","Sample"],"associatedColumns":["NMT","PPL"],"associatedMergedColumns":[]},{"number":"30.05","isBolded":false,"associatedRows":["Variational Relaxed Attention","Eq [ log p ] ? KL","Sample"],"associatedColumns":["NMT","BLEU"],"associatedMergedColumns":[]},{"number":"33.30","isBolded":false,"associatedRows":["Variational Attention","Eq [ log p ] ? KL","Sample"],"associatedColumns":["NMT","BLEU","-"],"associatedMergedColumns":[]},{"number":"31.40","isBolded":false,"associatedRows":["Hard Attention","Ep [ log p ]","Enum"],"associatedColumns":["NMT","BLEU"],"associatedMergedColumns":[]},{"number":"1.78","isBolded":false,"associatedRows":["Hard Attention","Ep [ log p ]","Enum"],"associatedColumns":["VQA","NLL"],"associatedMergedColumns":[]},{"number":"56.30","isBolded":false,"associatedRows":["Hard Attention","Ep [ log p ]","Sample"],"associatedColumns":["VQA","Eval"],"associatedMergedColumns":[]},{"number":"60.33","isBolded":false,"associatedRows":["Marginal Likelihood","log E [ p ]","Enum"],"associatedColumns":["VQA","Eval"],"associatedMergedColumns":[]},{"number":"57.60","isBolded":false,"associatedRows":["Hard Attention","Ep [ log p ]","Enum"],"associatedColumns":["VQA","Eval"],"associatedMergedColumns":[]},{"number":"1.69","isBolded":false,"associatedRows":["Variational Attention","Eq [ log p ] ? KL","Enum"],"associatedColumns":["VQA","NLL","-"],"associatedMergedColumns":[]},{"number":"1.75","isBolded":false,"associatedRows":["Variational Attention","Eq [ log p ] ? KL","Sample"],"associatedColumns":["VQA","NLL","-"],"associatedMergedColumns":[]},{"number":"1.82","isBolded":false,"associatedRows":["Hard Attention","Ep [ log p ]","Sample"],"associatedColumns":["VQA","NLL"],"associatedMergedColumns":[]},{"number":"58.93","isBolded":false,"associatedRows":["Soft Attention","log p ( y | E [ z ] )","-"],"associatedColumns":["VQA","Eval"],"associatedMergedColumns":[]},{"number":"58.44","isBolded":false,"associatedRows":["Variational Attention","Eq [ log p ] ? KL","Enum"],"associatedColumns":["VQA","Eval","-"],"associatedMergedColumns":[]},{"number":"1.69","isBolded":false,"associatedRows":["Marginal Likelihood","log E [ p ]","Enum"],"associatedColumns":["VQA","NLL"],"associatedMergedColumns":[]},{"number":"33.68","isBolded":false,"associatedRows":["Variational Attention","Eq [ log p ] ? KL","Enum"],"associatedColumns":["NMT","BLEU","-"],"associatedMergedColumns":[]},{"number":"6.17","isBolded":false,"associatedRows":["Variational Attention","Eq [ log p ] ? KL","Sample"],"associatedColumns":["NMT","PPL","-"],"associatedMergedColumns":[]},{"number":"7.17","isBolded":false,"associatedRows":["Soft Attention","log p ( y | E [ z ] )","-"],"associatedColumns":["NMT","PPL"],"associatedMergedColumns":[]},{"number":"57.52","isBolded":false,"associatedRows":["Variational Attention","Eq [ log p ] ? KL","Sample"],"associatedColumns":["VQA","Eval","-"],"associatedMergedColumns":[]},{"number":"1.76","isBolded":false,"associatedRows":["Soft Attention","log p ( y | E [ z ] )","-"],"associatedColumns":["VQA","NLL"],"associatedMergedColumns":[]},{"number":"6.34","isBolded":false,"associatedRows":["Marginal Likelihood","log E [ p ]","Enum"],"associatedColumns":["NMT","PPL"],"associatedMergedColumns":[]},{"number":"32.77","isBolded":false,"associatedRows":["Soft Attention","log p ( y | E [ z ] )","-"],"associatedColumns":["NMT","BLEU"],"associatedMergedColumns":[]},{"number":"33.29","isBolded":false,"associatedRows":["Marginal Likelihood","log E [ p ]","Enum"],"associatedColumns":["NMT","BLEU"],"associatedMergedColumns":[]},{"number":"7.37","isBolded":false,"associatedRows":["Hard Attention","Ep [ log p ]","Enum"],"associatedColumns":["NMT","PPL"],"associatedMergedColumns":[]}]},{"caption":"Table 2: (Left) Performance change on NMT from exact decoding to K-Max decoding with K \u003d 5. (see section \n5 for definition of K-max decoding). (Right) Test perplexity of different approaches while varying K to estimate \nEz[p(y|x,x)]. Dotted lines compare soft baseline and variational with full enumeration. \n\n","rows":["Variational + Enum","Hard + Sample","Variational + Sample","Hard + Enum","Marginal Likelihood"],"columns":["K - Max","BLEU","Exact","PPL"],"mergedAllColumns":[],"numberCells":[{"number":"7.38","isBolded":false,"associatedRows":["Hard + Sample"],"associatedColumns":["PPL","K - Max"],"associatedMergedColumns":[]},{"number":"6.17","isBolded":false,"associatedRows":["Variational + Sample"],"associatedColumns":["PPL","Exact"],"associatedMergedColumns":[]},{"number":"7.37","isBolded":false,"associatedRows":["Hard + Enum"],"associatedColumns":["PPL","K - Max"],"associatedMergedColumns":[]},{"number":"33.68","isBolded":false,"associatedRows":["Variational + Enum"],"associatedColumns":["BLEU","Exact"],"associatedMergedColumns":[]},{"number":"31.37","isBolded":false,"associatedRows":["Hard + Enum"],"associatedColumns":["BLEU","K - Max"],"associatedMergedColumns":[]},{"number":"33.27","isBolded":false,"associatedRows":["Variational + Sample"],"associatedColumns":["BLEU","K - Max"],"associatedMergedColumns":[]},{"number":"6.42","isBolded":false,"associatedRows":["Variational + Enum"],"associatedColumns":["PPL","K - Max"],"associatedMergedColumns":[]},{"number":"33.29","isBolded":false,"associatedRows":["Marginal Likelihood"],"associatedColumns":["BLEU","Exact"],"associatedMergedColumns":[]},{"number":"7.38","isBolded":false,"associatedRows":["Hard + Sample"],"associatedColumns":["PPL","Exact"],"associatedMergedColumns":[]},{"number":"31.00","isBolded":false,"associatedRows":["Hard + Sample"],"associatedColumns":["BLEU","Exact"],"associatedMergedColumns":[]},{"number":"6.51","isBolded":false,"associatedRows":["Variational + Sample"],"associatedColumns":["PPL","K - Max"],"associatedMergedColumns":[]},{"number":"33.31","isBolded":false,"associatedRows":["Marginal Likelihood"],"associatedColumns":["BLEU","K - Max"],"associatedMergedColumns":[]},{"number":"33.69","isBolded":false,"associatedRows":["Variational + Enum"],"associatedColumns":["BLEU","K - Max"],"associatedMergedColumns":[]},{"number":"6.90","isBolded":false,"associatedRows":["Marginal Likelihood"],"associatedColumns":["PPL","K - Max"],"associatedMergedColumns":[]},{"number":"31.04","isBolded":false,"associatedRows":["Hard + Sample"],"associatedColumns":["BLEU","K - Max"],"associatedMergedColumns":[]},{"number":"31.40","isBolded":false,"associatedRows":["Hard + Enum"],"associatedColumns":["BLEU","Exact"],"associatedMergedColumns":[]},{"number":"6.08","isBolded":false,"associatedRows":["Variational + Enum"],"associatedColumns":["PPL","Exact"],"associatedMergedColumns":[]},{"number":"33.30","isBolded":false,"associatedRows":["Variational + Sample"],"associatedColumns":["BLEU","Exact"],"associatedMergedColumns":[]},{"number":"7.37","isBolded":false,"associatedRows":["Hard + Enum"],"associatedColumns":["PPL","Exact"],"associatedMergedColumns":[]},{"number":"6.34","isBolded":false,"associatedRows":["Marginal Likelihood"],"associatedColumns":["PPL","Exact"],"associatedMergedColumns":[]}]},{"caption":"Table 3: (Left) Comparison against the best prior work for NMT on the IWSLT 2014 German-English test set. \n(Upper Right) Comparison of inference alternatives of variational attention on IWSLT 2014. (Lower Right) \nComparison of different models in terms of implied discrete entropy (lower \u003d more certain alignment). \n\n","rows":["Soft Attention","Neural PBMT + LM [ 29 ]","Variational Attention + Enum","BLEU","Beam Search Optimization [ 77 ]","Minimum Risk Training [ 21 ]","Hard Attention + Enum","REINFORCE","RWS","1","Gumbel - Softmax","5","Hard Attention + Sample","Variational Attention + Sample","Model","Actor - Critic [ 5 ]","Marginal Likelihood","Variational Relaxed Attention"],"columns":["NMT","BLEU","VQA","Model",") . Both models imply a similar alignment , but variational attention has lower entropy .","Inference Method","PPL"],"mergedAllColumns":["IWSLT","Entropy","-"],"numberCells":[{"number":"6.41","isBolded":false,"associatedRows":["Beam Search Optimization [ 77 ]","RWS","5"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","PPL"],"associatedMergedColumns":["IWSLT"]},{"number":"2.66","isBolded":false,"associatedRows":["Marginal Likelihood","Variational Relaxed Attention"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","BLEU","VQA"],"associatedMergedColumns":["Entropy"]},{"number":"26.36","isBolded":false,"associatedRows":["Beam Search Optimization [ 77 ]"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","Inference Method"],"associatedMergedColumns":["IWSLT"]},{"number":"0.54","isBolded":false,"associatedRows":["Variational Attention + Enum","Variational Attention + Enum"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","PPL","NMT"],"associatedMergedColumns":["-"]},{"number":"28.53","isBolded":false,"associatedRows":["Actor - Critic [ 5 ]"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","Inference Method"],"associatedMergedColumns":["IWSLT"]},{"number":"0.52","isBolded":false,"associatedRows":["Variational Attention + Sample","Variational Attention + Sample"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","PPL","NMT"],"associatedMergedColumns":["-"]},{"number":"32.77","isBolded":false,"associatedRows":["Soft Attention"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","Inference Method","Model"],"associatedMergedColumns":["Entropy"]},{"number":"32.84","isBolded":false,"associatedRows":["Minimum Risk Training [ 21 ]"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","Inference Method"],"associatedMergedColumns":["Entropy"]},{"number":"2.44","isBolded":false,"associatedRows":["Variational Attention + Sample","Variational Attention + Sample"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","BLEU","VQA"],"associatedMergedColumns":["-"]},{"number":"6.51","isBolded":false,"associatedRows":["Beam Search Optimization [ 77 ]","Gumbel - Softmax","1"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","PPL"],"associatedMergedColumns":["IWSLT"]},{"number":"33.08","isBolded":false,"associatedRows":["Beam Search Optimization [ 77 ]","Gumbel - Softmax","1"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","BLEU"],"associatedMergedColumns":["IWSLT"]},{"number":"0.07","isBolded":false,"associatedRows":["Hard Attention + Sample","Variational Relaxed Attention"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","PPL","NMT"],"associatedMergedColumns":["Entropy"]},{"number":"2.07","isBolded":false,"associatedRows":["Variational Attention + Enum","Variational Attention + Enum"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","BLEU","VQA"],"associatedMergedColumns":["-"]},{"number":"0.73","isBolded":false,"associatedRows":["Hard Attention + Enum","Variational Relaxed Attention"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","BLEU","VQA"],"associatedMergedColumns":["Entropy"]},{"number":"2.70","isBolded":false,"associatedRows":["Soft Attention","Variational Relaxed Attention"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","BLEU","VQA"],"associatedMergedColumns":["Entropy"]},{"number":"0.05","isBolded":false,"associatedRows":["Hard Attention + Enum","Variational Relaxed Attention"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","PPL","NMT"],"associatedMergedColumns":["Entropy"]},{"number":"30.08","isBolded":false,"associatedRows":["Neural PBMT + LM [ 29 ]"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","Inference Method"],"associatedMergedColumns":["IWSLT"]},{"number":"32.96","isBolded":false,"associatedRows":["Beam Search Optimization [ 77 ]","RWS","5"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","BLEU"],"associatedMergedColumns":["IWSLT"]},{"number":"33.29","isBolded":false,"associatedRows":["Marginal Likelihood"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","Inference Method","Model"],"associatedMergedColumns":["Entropy"]},{"number":"33.30","isBolded":false,"associatedRows":["Variational Attention + Sample"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","Inference Method","Model"],"associatedMergedColumns":["-"]},{"number":"30.42","isBolded":false,"associatedRows":["Hard Attention + Sample"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","Inference Method","Model"],"associatedMergedColumns":["Entropy"]},{"number":"0.82","isBolded":false,"associatedRows":["Marginal Likelihood","Variational Relaxed Attention"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","PPL","NMT"],"associatedMergedColumns":["Entropy"]},{"number":"33.69","isBolded":false,"associatedRows":["Variational Attention + Enum"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","Inference Method","Model"],"associatedMergedColumns":["-"]},{"number":"30.05","isBolded":false,"associatedRows":["Variational Relaxed Attention"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","Inference Method","Model"],"associatedMergedColumns":["Entropy"]},{"number":"1.24","isBolded":false,"associatedRows":["Soft Attention","Variational Relaxed Attention"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","PPL","NMT"],"associatedMergedColumns":["Entropy"]},{"number":"0.58","isBolded":false,"associatedRows":["Hard Attention + Sample","Variational Relaxed Attention"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","BLEU","VQA"],"associatedMergedColumns":["Entropy"]},{"number":"6.17","isBolded":false,"associatedRows":["Model","BLEU","REINFORCE","1"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","PPL"],"associatedMergedColumns":[]},{"number":"33.30","isBolded":false,"associatedRows":["Model","BLEU","REINFORCE","1"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","BLEU"],"associatedMergedColumns":[]},{"number":"2.02","isBolded":false,"associatedRows":["Variational Relaxed Attention","Variational Relaxed Attention"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","PPL","NMT"],"associatedMergedColumns":["Entropy"]},{"number":"31.40","isBolded":false,"associatedRows":["Hard Attention + Enum"],"associatedColumns":[") . Both models imply a similar alignment , but variational attention has lower entropy .","Inference Method","Model"],"associatedMergedColumns":["Entropy"]}]}]