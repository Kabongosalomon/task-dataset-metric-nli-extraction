[{"caption":"Table 1: Top-1 test accuracy when training on ImageNet from scratch using DP-SGD. We train a \nNF-ResNet-50 with ? \u003d 2.5, hyper-parameters of Table 2 and (B, S) \u003d (32768, 18k) (Table 4). \n\n","rows":["( 13 . 2 , 10 ? 6 )","Ours","De et al . ( 2022 ) ( original paper )","De et al . ( 2022 ) ( our reproduction )","Kurakin et al . ( 2022 )","? 7 )","( 8 ,"],"columns":["Accuracy","( ? , ? )"],"mergedAllColumns":[],"numberCells":[{"number":"30.2%","isBolded":true,"associatedRows":["De et al . ( 2022 ) ( our reproduction )","( 8 ,","? 7 )"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"32.4%","isBolded":true,"associatedRows":["De et al . ( 2022 ) ( original paper )","( 8 ,","? 7 )"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"8.10","isBolded":false,"associatedRows":["Ours","( 8 ,"],"associatedColumns":["( ? , ? )"],"associatedMergedColumns":[]},{"number":"39.2%","isBolded":true,"associatedRows":["Ours","( 8 ,","? 7 )"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]},{"number":"8.10","isBolded":false,"associatedRows":["De et al . ( 2022 ) ( our reproduction )","( 8 ,"],"associatedColumns":["( ? , ? )"],"associatedMergedColumns":[]},{"number":"8.10","isBolded":false,"associatedRows":["De et al . ( 2022 ) ( original paper )","( 8 ,"],"associatedColumns":["( ? , ? )"],"associatedMergedColumns":[]},{"number":"6.2%","isBolded":true,"associatedRows":["Kurakin et al . ( 2022 )","( 13 . 2 , 10 ? 6 )"],"associatedColumns":["Accuracy"],"associatedMergedColumns":[]}]},{"caption":"Table 2: Comparing optimal hyper-parameters. Keeping ? step and S constant, we compare various \nchanges in the training pipeline. We compare with the baseline of De et al. (2022) (blue line in Fig-\nure 1: NFResNet-50, learning rate at 4, EMA decay at 0.99999, 4 random augmentations averaged \nover 3 runs). Each gain is compared to the previous column. \n\n","rows":["16384","256","512","1024","128","Imagenet : ? ref \u003d","( 8 , 0 , 0 )","-","( Ours , 8 )"],"columns":["Total","decay","AugTest","( lr , ? , d )","AugMult"],"mergedAllColumns":[],"numberCells":[{"number":"+5.7%","isBolded":false,"associatedRows":["256","( 8 , 0 , 0 )","Imagenet : ? ref \u003d","( Ours , 8 )"],"associatedColumns":["Total"],"associatedMergedColumns":[]},{"number":"0.999","isBolded":false,"associatedRows":["512","( 8 , 0 , 0 )","-"],"associatedColumns":["decay"],"associatedMergedColumns":[]},{"number":"2.5,Bref\u003d16,384,S\u003d72K","isBolded":true,"associatedRows":["128","( 8 , 0 , 0 )","Imagenet : ? ref \u003d"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.999","isBolded":false,"associatedRows":["1024","( 8 , 0 , 0 )","-"],"associatedColumns":["decay"],"associatedMergedColumns":[]},{"number":"+3.0","isBolded":false,"associatedRows":["128","( 8 , 0 , 0 )","Imagenet : ? ref \u003d","( Ours , 8 )"],"associatedColumns":["AugMult"],"associatedMergedColumns":[]},{"number":"+6.2%","isBolded":false,"associatedRows":["512","( 8 , 0 , 0 )","Imagenet : ? ref \u003d","( Ours , 8 )"],"associatedColumns":["Total"],"associatedMergedColumns":[]},{"number":"+1.2","isBolded":false,"associatedRows":["256","( 8 , 0 , 0 )","Imagenet : ? ref \u003d"],"associatedColumns":["decay"],"associatedMergedColumns":[]},{"number":"+1.1","isBolded":false,"associatedRows":["512","( 8 , 0 , 0 )","-"],"associatedColumns":["decay"],"associatedMergedColumns":[]},{"number":"+5.6%","isBolded":false,"associatedRows":["128","( 8 , 0 , 0 )","Imagenet : ? ref \u003d","( Ours , 8 )"],"associatedColumns":["Total"],"associatedMergedColumns":[]},{"number":"+0.8","isBolded":false,"associatedRows":["256","( 8 , 0 , 0 )"],"associatedColumns":["( lr , ? , d )"],"associatedMergedColumns":[]},{"number":"+1.6","isBolded":false,"associatedRows":["1024","( 8 , 0 , 0 )"],"associatedColumns":["( lr , ? , d )"],"associatedMergedColumns":[]},{"number":"+5.9%","isBolded":false,"associatedRows":["1024","( 8 , 0 , 0 )","Imagenet : ? ref \u003d","( Ours , 8 )"],"associatedColumns":["Total"],"associatedMergedColumns":[]},{"number":"+2.3","isBolded":false,"associatedRows":["1024","( 8 , 0 , 0 )","-","( Ours , 8 )"],"associatedColumns":["AugMult"],"associatedMergedColumns":[]},{"number":"+0.8","isBolded":false,"associatedRows":["1024","( 8 , 0 , 0 )","Imagenet : ? ref \u003d","( Ours , 8 )"],"associatedColumns":["AugTest"],"associatedMergedColumns":[]},{"number":"0.999","isBolded":false,"associatedRows":["128","( 8 , 0 , 0 )","-"],"associatedColumns":["decay"],"associatedMergedColumns":[]},{"number":"0.999","isBolded":false,"associatedRows":["256","( 8 , 0 , 0 )","-"],"associatedColumns":["decay"],"associatedMergedColumns":[]},{"number":"+2.8","isBolded":false,"associatedRows":["512","( 8 , 0 , 0 )","Imagenet : ? ref \u003d","( Ours , 8 )"],"associatedColumns":["AugMult"],"associatedMergedColumns":[]},{"number":"+0.4","isBolded":false,"associatedRows":["128","( 8 , 0 , 0 )","Imagenet : ? ref \u003d","( Ours , 8 )"],"associatedColumns":["AugTest"],"associatedMergedColumns":[]},{"number":"+1.2","isBolded":false,"associatedRows":["1024","( 8 , 0 , 0 )","-"],"associatedColumns":["decay"],"associatedMergedColumns":[]},{"number":"+0.8","isBolded":false,"associatedRows":["16384","-","-","-","-","-","-"],"associatedColumns":["AugTest"],"associatedMergedColumns":[]},{"number":"+1.2","isBolded":false,"associatedRows":["512","( 8 , 0 , 0 )"],"associatedColumns":["( lr , ? , d )"],"associatedMergedColumns":[]},{"number":"+0.7","isBolded":false,"associatedRows":["256","( 8 , 0 , 0 )","Imagenet : ? ref \u003d","( Ours , 8 )"],"associatedColumns":["AugTest"],"associatedMergedColumns":[]},{"number":"+1.1","isBolded":false,"associatedRows":["512","( 8 , 0 , 0 )","Imagenet : ? ref \u003d","( Ours , 8 )"],"associatedColumns":["AugTest"],"associatedMergedColumns":[]},{"number":"+6.7%","isBolded":true,"associatedRows":["16384","-","-","-","-","-","-"],"associatedColumns":["Total"],"associatedMergedColumns":[]},{"number":"+1.0","isBolded":false,"associatedRows":["128","( 8 , 0 , 0 )"],"associatedColumns":["( lr , ? , d )"],"associatedMergedColumns":[]},{"number":"+3.0","isBolded":false,"associatedRows":["256","( 8 , 0 , 0 )","Imagenet : ? ref \u003d","( Ours , 8 )"],"associatedColumns":["AugMult"],"associatedMergedColumns":[]},{"number":"+1.2","isBolded":false,"associatedRows":["128","( 8 , 0 , 0 )","Imagenet : ? ref \u003d"],"associatedColumns":["decay"],"associatedMergedColumns":[]}]},{"caption":"Table 3: Low compute simulation of privacy parameter search. We start from B \u003d 256 \u003d 16384/64 \nand S \u003d 72K. We use ? \u003d 2.5/64 for all runs and no data augmentation. \n\n","rows":["and dampening factors d ? [ 0 ,","9K","2","weights ( Tan \u0026 Le , 2019 ) with a decay parameter in [ 0 . 9 ,","18K","512","128","756","We search over learning rates lr ? [ 1 , 2 , 4 , 8 , 12 , 16 ] , momentum parameters ? ? [ 0 ,","64","288K","32"],"columns":["HYPER - PARAMETER TUNING AT CONSTANT TAN","72K","ref \u003d 72K","We run a large hyper - parameter search and report the best hyper -","S","lr","Gain","/"],"mergedAllColumns":["? step and S ) . Each gain is compared to the optimal hyper - parameters find at the previous column ."],"numberCells":[{"number":"0.5,","isBolded":true,"associatedRows":["We search over learning rates lr ? [ 1 , 2 , 4 , 8 , 12 , 16 ] , momentum parameters ? ? [ 0 ,"],"associatedColumns":["ref \u003d 72K","Gain","/","HYPER - PARAMETER TUNING AT CONSTANT TAN","We run a large hyper - parameter search and report the best hyper -"],"associatedMergedColumns":["? step and S ) . Each gain is compared to the optimal hyper - parameters find at the previous column ."]},{"number":"0.9999,0.99999].","isBolded":true,"associatedRows":["weights ( Tan \u0026 Le , 2019 ) with a decay parameter in [ 0 . 9 ,"],"associatedColumns":["ref \u003d 72K","Gain","/","HYPER - PARAMETER TUNING AT CONSTANT TAN","We run a large hyper - parameter search and report the best hyper -"],"associatedMergedColumns":["? step and S ) . Each gain is compared to the optimal hyper - parameters find at the previous column ."]},{"number":"0.1,","isBolded":true,"associatedRows":["We search over learning rates lr ? [ 1 , 2 , 4 , 8 , 12 , 16 ] , momentum parameters ? ? [ 0 ,"],"associatedColumns":["ref \u003d 72K","Gain","/","HYPER - PARAMETER TUNING AT CONSTANT TAN","We run a large hyper - parameter search and report the best hyper -"],"associatedMergedColumns":["? step and S ) . Each gain is compared to the optimal hyper - parameters find at the previous column ."]},{"number":"-6.22%","isBolded":false,"associatedRows":["and dampening factors d ? [ 0 ,","9K","756","64"],"associatedColumns":["ref \u003d 72K","Gain"],"associatedMergedColumns":[]},{"number":"0.1,","isBolded":true,"associatedRows":["and dampening factors d ? [ 0 ,"],"associatedColumns":["ref \u003d 72K","S","72K","HYPER - PARAMETER TUNING AT CONSTANT TAN","We run a large hyper - parameter search and report the best hyper -"],"associatedMergedColumns":["? step and S ) . Each gain is compared to the optimal hyper - parameters find at the previous column ."]},{"number":"-1.88%","isBolded":false,"associatedRows":["and dampening factors d ? [ 0 ,","288K","128","2"],"associatedColumns":["ref \u003d 72K","Gain","/"],"associatedMergedColumns":[]},{"number":"0.99,","isBolded":true,"associatedRows":["weights ( Tan \u0026 Le , 2019 ) with a decay parameter in [ 0 . 9 ,"],"associatedColumns":["ref \u003d 72K","Gain","/","HYPER - PARAMETER TUNING AT CONSTANT TAN","We run a large hyper - parameter search and report the best hyper -"],"associatedMergedColumns":["? step and S ) . Each gain is compared to the optimal hyper - parameters find at the previous column ."]},{"number":"0.9,1].Weuseexponentialmovingaverage(EMA)onthe","isBolded":true,"associatedRows":["and dampening factors d ? [ 0 ,","9K"],"associatedColumns":["ref \u003d 72K","lr","Gain","/","HYPER - PARAMETER TUNING AT CONSTANT TAN","We run a large hyper - parameter search and report the best hyper -"],"associatedMergedColumns":["? step and S ) . Each gain is compared to the optimal hyper - parameters find at the previous column ."]},{"number":"0.5,","isBolded":true,"associatedRows":["and dampening factors d ? [ 0 ,"],"associatedColumns":["ref \u003d 72K","S","72K","HYPER - PARAMETER TUNING AT CONSTANT TAN","We run a large hyper - parameter search and report the best hyper -"],"associatedMergedColumns":["? step and S ) . Each gain is compared to the optimal hyper - parameters find at the previous column ."]},{"number":"+1.32%","isBolded":true,"associatedRows":["and dampening factors d ? [ 0 ,","18K","512","32"],"associatedColumns":["ref \u003d 72K","Gain"],"associatedMergedColumns":[]},{"number":"0.999,","isBolded":true,"associatedRows":["weights ( Tan \u0026 Le , 2019 ) with a decay parameter in [ 0 . 9 ,"],"associatedColumns":["ref \u003d 72K","Gain","/","HYPER - PARAMETER TUNING AT CONSTANT TAN","We run a large hyper - parameter search and report the best hyper -"],"associatedMergedColumns":["? step and S ) . Each gain is compared to the optimal hyper - parameters find at the previous column ."]},{"number":"0.9,1]","isBolded":true,"associatedRows":["We search over learning rates lr ? [ 1 , 2 , 4 , 8 , 12 , 16 ] , momentum parameters ? ? [ 0 ,"],"associatedColumns":["ref \u003d 72K","Gain","/","HYPER - PARAMETER TUNING AT CONSTANT TAN","We run a large hyper - parameter search and report the best hyper -"],"associatedMergedColumns":["? step and S ) . Each gain is compared to the optimal hyper - parameters find at the previous column ."]}]},{"caption":"Table 2. Overall, we improved performance by 9% when training from scratch on Im-\nageNet with DP-SGD under ? \u003d 8. We compare to our reproduction of the previous SOTA of ","rows":["Accuracy","plotted with two standard deviations over","Top","Test","post - activation in a 16 - 4 - WideResnet on CIFAR - 10 . We compare simulation at ( B , ? ) \u003d ( 512 ,","Final"],"columns":["GN - ReLU , B\u003d4096","GN - ReLU , B\u003d512 ( simulation )","ReLU - GN , B\u003d4096","Pre Activation VS Post Activation Normalization","ReLU - GN , B\u003d512 ( simulation )","Impact of data augmentation on the test accuracy using pre - activation normalization vs"],"mergedAllColumns":["ref ) \u003d ( 4096 , 3 ) , both trained for S \u003d 2 , 500 steps . Confidence intervals are","Augmentation Multiplicity Order"],"numberCells":[{"number":"0.75","isBolded":false,"associatedRows":["Accuracy"],"associatedColumns":["Pre Activation VS Post Activation Normalization"],"associatedMergedColumns":[]},{"number":"0.60","isBolded":false,"associatedRows":["Top"],"associatedColumns":["Pre Activation VS Post Activation Normalization"],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":[],"associatedColumns":["Pre Activation VS Post Activation Normalization"],"associatedMergedColumns":[]},{"number":"5runs.AugmentationMultiplicityOrdercorrespondsto","isBolded":true,"associatedRows":["plotted with two standard deviations over"],"associatedColumns":["Pre Activation VS Post Activation Normalization","ReLU - GN , B\u003d4096","GN - ReLU , B\u003d4096","Impact of data augmentation on the test accuracy using pre - activation normalization vs"],"associatedMergedColumns":["ref ) \u003d ( 4096 , 3 ) , both trained for S \u003d 2 , 500 steps . Confidence intervals are"]},{"number":"0.65","isBolded":false,"associatedRows":["Test"],"associatedColumns":["Pre Activation VS Post Activation Normalization"],"associatedMergedColumns":[]},{"number":"0.80","isBolded":false,"associatedRows":["Accuracy"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"0.55","isBolded":false,"associatedRows":["Final"],"associatedColumns":["Pre Activation VS Post Activation Normalization"],"associatedMergedColumns":[]},{"number":"8","isBolded":true,"associatedRows":["post - activation in a 16 - 4 - WideResnet on CIFAR - 10 . We compare simulation at ( B , ? ) \u003d ( 512 ,"],"associatedColumns":["Pre Activation VS Post Activation Normalization","ReLU - GN , B\u003d4096","GN - ReLU , B\u003d4096","Impact of data augmentation on the test accuracy using pre - activation normalization vs"],"associatedMergedColumns":["Augmentation Multiplicity Order"]},{"number":"8","isBolded":false,"associatedRows":["plotted with two standard deviations over"],"associatedColumns":["Pre Activation VS Post Activation Normalization","ReLU - GN , B\u003d4096","GN - ReLU , B\u003d4096"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["plotted with two standard deviations over"],"associatedColumns":["Pre Activation VS Post Activation Normalization","ReLU - GN , B\u003d4096","GN - ReLU , B\u003d4096"],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["plotted with two standard deviations over"],"associatedColumns":["Pre Activation VS Post Activation Normalization","ReLU - GN , B\u003d512 ( simulation )","GN - ReLU , B\u003d512 ( simulation )"],"associatedMergedColumns":[]},{"number":"0.50","isBolded":false,"associatedRows":["Final"],"associatedColumns":["Pre Activation VS Post Activation Normalization","ReLU - GN , B\u003d512 ( simulation )"],"associatedMergedColumns":[]},{"number":"0","isBolded":false,"associatedRows":["Final"],"associatedColumns":["Pre Activation VS Post Activation Normalization","ReLU - GN , B\u003d512 ( simulation )","GN - ReLU , B\u003d512 ( simulation )"],"associatedMergedColumns":[]},{"number":"0.70","isBolded":false,"associatedRows":["Accuracy"],"associatedColumns":["Pre Activation VS Post Activation Normalization"],"associatedMergedColumns":[]},{"number":"3","isBolded":true,"associatedRows":["post - activation in a 16 - 4 - WideResnet on CIFAR - 10 . We compare simulation at ( B , ? ) \u003d ( 512 ,"],"associatedColumns":["Pre Activation VS Post Activation Normalization","ReLU - GN , B\u003d4096","GN - ReLU , B\u003d4096","Impact of data augmentation on the test accuracy using pre - activation normalization vs"],"associatedMergedColumns":["Augmentation Multiplicity Order"]}]},{"caption":"Table 4: Privacy parameter search. We use the optimal parameters described in Section 4.2.1 with \n? \u003d 2.5 for one expensive run and compare it with our optimal result \n\nB ref \u003d 16384, S ref \u003d 72K \n\nS \nB \n? \nlr Test acc \n\n18K 32,768 8.00 32 \n39.2% \n72K 16,384 7.97 8 \n36.9% \n","rows":["72K","16 , 384","18K","8","? \u003d","32 , 768","32"],"columns":["Privacy parameter search . We use the optimal parameters described in Section 4 . 2 . 1 with","?","Test acc"],"mergedAllColumns":["ref \u003d 72K"],"numberCells":[{"number":"8.00","isBolded":false,"associatedRows":["18K","32 , 768"],"associatedColumns":["Privacy parameter search . We use the optimal parameters described in Section 4 . 2 . 1 with","?"],"associatedMergedColumns":["ref \u003d 72K"]},{"number":"2.5foroneexpensiverunandcompareitwithouroptimalresult","isBolded":true,"associatedRows":["? \u003d"],"associatedColumns":["Privacy parameter search . We use the optimal parameters described in Section 4 . 2 . 1 with"],"associatedMergedColumns":[]},{"number":"36.9%","isBolded":false,"associatedRows":["72K","16 , 384","8"],"associatedColumns":["Privacy parameter search . We use the optimal parameters described in Section 4 . 2 . 1 with","Test acc"],"associatedMergedColumns":["ref \u003d 72K"]},{"number":"7.97","isBolded":false,"associatedRows":["72K","16 , 384"],"associatedColumns":["Privacy parameter search . We use the optimal parameters described in Section 4 . 2 . 1 with","?"],"associatedMergedColumns":["ref \u003d 72K"]},{"number":"39.2%","isBolded":true,"associatedRows":["18K","32 , 768","32"],"associatedColumns":["Privacy parameter search . We use the optimal parameters described in Section 4 . 2 . 1 with","Test acc"],"associatedMergedColumns":["ref \u003d 72K"]}]},{"caption":"Table 5: Impact of the training set size N on the privacy/utility trade-off. We start training on \n10% of the data (N 0 \u003d 5K). We use B \u003d 4,096, ? \u003d 3 and S \u003d 2,500, with post-activation \nnormalization, and no augmentation. Standard deviations are computed over 3 runs. \n\n","rows":["5K","25K"],"columns":["Test acc ( % )","CIFAR - 10 : ? \u003d 3 , B \u003d 4 , 096 , S \u003d 2 , 500","?"],"mergedAllColumns":[],"numberCells":[{"number":"71.1(?0.4)","isBolded":false,"associatedRows":["25K"],"associatedColumns":["CIFAR - 10 : ? \u003d 3 , B \u003d 4 , 096 , S \u003d 2 , 500","Test acc ( % )"],"associatedMergedColumns":[]},{"number":"150.3","isBolded":false,"associatedRows":["5K"],"associatedColumns":["CIFAR - 10 : ? \u003d 3 , B \u003d 4 , 096 , S \u003d 2 , 500","?"],"associatedMergedColumns":[]},{"number":"13.7","isBolded":false,"associatedRows":["25K"],"associatedColumns":["CIFAR - 10 : ? \u003d 3 , B \u003d 4 , 096 , S \u003d 2 , 500","?"],"associatedMergedColumns":[]},{"number":"59.9(?1)","isBolded":false,"associatedRows":["5K"],"associatedColumns":["CIFAR - 10 : ? \u003d 3 , B \u003d 4 , 096 , S \u003d 2 , 500","Test acc ( % )"],"associatedMergedColumns":[]}]},{"caption":"Table 6: Impact of adding more data on ImageNet. The \"Simulated Gain\" column corresponds \nto the accuracy gain we observe when simulating at lower compute using our scaling strategy for \nB \u003d 256. The \"Gain\" column corresponds to the real gain at B \u003d 16384. \n\n","rows":[": ? ref \u003d"],"columns":["Simulated Gain","?","? T AN","?","N","Gain","/"],"mergedAllColumns":[],"numberCells":[{"number":"17.98","isBolded":false,"associatedRows":[],"associatedColumns":["?"],"associatedMergedColumns":[]},{"number":"8.10?7","isBolded":true,"associatedRows":[],"associatedColumns":["?","/"],"associatedMergedColumns":[]},{"number":"+1.3%","isBolded":false,"associatedRows":[],"associatedColumns":["Gain","/"],"associatedMergedColumns":[]},{"number":"18.06","isBolded":false,"associatedRows":[],"associatedColumns":["? T AN"],"associatedMergedColumns":[]},{"number":"0.6M","isBolded":false,"associatedRows":[],"associatedColumns":["N"],"associatedMergedColumns":[]},{"number":"8.00","isBolded":false,"associatedRows":[],"associatedColumns":["?","/"],"associatedMergedColumns":[]},{"number":"8.26","isBolded":false,"associatedRows":[],"associatedColumns":["? T AN","/"],"associatedMergedColumns":[]},{"number":"16.10?7","isBolded":true,"associatedRows":[],"associatedColumns":["?"],"associatedMergedColumns":[]},{"number":"2.5,Bref\u003d16384,S\u003d72k","isBolded":true,"associatedRows":[": ? ref \u003d"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"1.2M","isBolded":false,"associatedRows":[],"associatedColumns":["N","/"],"associatedMergedColumns":[]},{"number":"+1.5%","isBolded":false,"associatedRows":[],"associatedColumns":["Simulated Gain","/"],"associatedMergedColumns":[]}]}]