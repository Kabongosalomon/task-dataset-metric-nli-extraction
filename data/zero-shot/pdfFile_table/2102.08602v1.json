[{"caption":"Table 3: Comparison of the lambda layer and attention mechanisms on ImageNet classification \nwith a ResNet50 architecture. The lambda layer strongly outperforms attention alternatives at a \nfraction of the parameter cost. All models are trained in mostly similar setups (see Appendix E.2) \nand we include the reported improvements compared to the convolution baseline in parentheses. See \nAppendix B.4 for a description of the |u| hyperparameter.  ? Our implementation. \n\n","rows":["Conv + linear attention ( Chen et al . , 2018 )","Lambda layer","Conv ( He et al . , 2016 ) ?","Conv + linear attention ( Shen et al . , 2018 )","Conv + relative self - attention ( Bello et al . , 2019 )","Lambda layer ( |u|\u003d4 )","Conv + channel attention ( Hu et al . , 2018c ) ?","Local relative self - attention ( Hu et al . , 2019 )","-","Local relative self - attention ( Zhao et al . , 2020 )","Local relative self - attention ( Ramachandran et al . , 2019 )"],"columns":["top - 1","Params ( M )"],"mergedAllColumns":[],"numberCells":[{"number":"77.0","isBolded":false,"associatedRows":["Conv + linear attention ( Chen et al . , 2018 )"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"77.7(+1.3)","isBolded":true,"associatedRows":["Conv + relative self - attention ( Bello et al . , 2019 )"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"15.0","isBolded":true,"associatedRows":["Lambda layer"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"78.4(+1.5)","isBolded":true,"associatedRows":["Lambda layer"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"78.9(+2.0)","isBolded":true,"associatedRows":["Lambda layer ( |u|\u003d4 )"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"18.0","isBolded":false,"associatedRows":["Local relative self - attention ( Ramachandran et al . , 2019 )"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"33.0","isBolded":false,"associatedRows":["Conv + linear attention ( Chen et al . , 2018 )"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"77.3(+1.2)","isBolded":true,"associatedRows":["Conv + linear attention ( Shen et al . , 2018 )","-"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"77.4(+0.5)","isBolded":true,"associatedRows":["Local relative self - attention ( Ramachandran et al . , 2019 )"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"23.3","isBolded":false,"associatedRows":["Local relative self - attention ( Hu et al . , 2019 )"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"16.0","isBolded":true,"associatedRows":["Lambda layer ( |u|\u003d4 )"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"76.9","isBolded":false,"associatedRows":["Conv ( He et al . , 2016 ) ?"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"25.8","isBolded":false,"associatedRows":["Conv + relative self - attention ( Bello et al . , 2019 )"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"20.5","isBolded":false,"associatedRows":["Local relative self - attention ( Zhao et al . , 2020 )"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"78.2(+1.3)","isBolded":true,"associatedRows":["Local relative self - attention ( Zhao et al . , 2020 )"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"25.6","isBolded":false,"associatedRows":["Conv ( He et al . , 2016 ) ?"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"28.1","isBolded":false,"associatedRows":["Conv + channel attention ( Hu et al . , 2018c ) ?"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"77.6(+0.7)","isBolded":true,"associatedRows":["Conv + channel attention ( Hu et al . , 2018c ) ?"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"77.3(+1.0)","isBolded":true,"associatedRows":["Local relative self - attention ( Hu et al . , 2019 )"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]}]},{"caption":"Table 4: The lambda layer reaches higher ImageNet accuracies while being faster and more \nmemory-efficient than self-attention alternatives. Memory is reported assuming full precision \nfor a batch of 128 inputs using default hyperparameters. The memory cost for storing the lambdas \nmatches the memory cost of activations in the rest of the network and is therefore ignored. b: batch \nsize, h: number of heads/queries, n: input length, m: context length, k: query/key depth, l: number \nof layers. \n\n","rows":["Axial self - attention","? ( blhn","? ( blhnm )",")","? ( lkn","-","1210 ex / s","960 ex / s","440 ex / s","Lambda layer","Lambda layer ( shared embeddings )","? ( lknm )","Lambda layer ( |k|\u003d8 )","n )","Local self - attention ( 7x7 )","? ( kn 2 )","Lambda convolution ( 7x7 )","1100 ex / s","1160ex / s","1640 ex / s"],"columns":["top - 1","Memory ( GB )","OOM","120"],"mergedAllColumns":["2"],"numberCells":[{"number":"77.9","isBolded":false,"associatedRows":["Lambda layer ( |k|\u003d8 )","? ( lkn",")","-","1640 ex / s"],"associatedColumns":["top - 1","OOM"],"associatedMergedColumns":["2"]},{"number":"0.95","isBolded":false,"associatedRows":["Lambda layer ( |k|\u003d8 )","? ( lkn",")"],"associatedColumns":["Memory ( GB )","120"],"associatedMergedColumns":["2"]},{"number":"1.9","isBolded":false,"associatedRows":["Lambda layer","? ( lkn",")"],"associatedColumns":["Memory ( GB )","120"],"associatedMergedColumns":[]},{"number":"0.63","isBolded":true,"associatedRows":["Lambda layer ( shared embeddings )","? ( kn 2 )"],"associatedColumns":["Memory ( GB )","120"],"associatedMergedColumns":["2"]},{"number":"4.8","isBolded":false,"associatedRows":["Axial self - attention","? ( blhn","? ( blhnm )","n )"],"associatedColumns":["Memory ( GB )","120"],"associatedMergedColumns":[]},{"number":"78.4","isBolded":true,"associatedRows":["Lambda layer","? ( lkn",")","-","1160ex / s"],"associatedColumns":["top - 1","OOM"],"associatedMergedColumns":[]},{"number":"77.4","isBolded":false,"associatedRows":["Local self - attention ( 7x7 )","? ( blhnm )","? ( kn 2 )","-","440 ex / s"],"associatedColumns":["top - 1","OOM"],"associatedMergedColumns":[]},{"number":"78.1","isBolded":false,"associatedRows":["Lambda convolution ( 7x7 )","? ( lknm )","? ( kn 2 )","-","1100 ex / s"],"associatedColumns":["top - 1","OOM"],"associatedMergedColumns":["2"]},{"number":"77.5","isBolded":false,"associatedRows":["Axial self - attention","? ( blhn","? ( blhnm )","n )","-","960 ex / s"],"associatedColumns":["top - 1","OOM"],"associatedMergedColumns":[]},{"number":"78.0","isBolded":false,"associatedRows":["Lambda layer ( shared embeddings )","? ( kn 2 )","-","1210 ex / s"],"associatedColumns":["top - 1","OOM"],"associatedMergedColumns":["2"]}]},{"caption":"Table 5: Comparison of models trained on extra data. ViT-L/16 is pre-trained on JFT and fine-\ntuned on ImageNet at resolution 384x384, while EfficientNet and LambdaResNet are co-trained on \nImageNet and JFT pseudo-labels. Training and inference throughput is shown for 8 TPUv3 cores. \n\n","rows":["66","faster than EfficientNets and","trained at image size 320 , reaches a strong","6100","1620","EfficientNet - B7","LambdaResNet - 152","ViT - L / 16","51","matching the training and regularization setup of EfficientNets , LambdaResNets are"],"columns":["B3","When","ResNet - RS wo / SE","Infer ( ex / s )","( 420 , 320 )","ImageNet top - 1","Speed - accuracy comparison between LambdaResNets and EfficientNets .","Params ( M )","Train ( ex / s )"],"mergedAllColumns":["ResNet - RS w / SE","daResNets are annotated with ( depth , image size ) . Our largest LambdaResNet , LambdaResNet - 420"],"numberCells":[{"number":"3.2-4.4x","isBolded":false,"associatedRows":["matching the training and regularization setup of EfficientNets , LambdaResNets are"],"associatedColumns":["( 420 , 320 )","ResNet - RS wo / SE","When"],"associatedMergedColumns":["ResNet - RS w / SE"]},{"number":"980(6.2x)","isBolded":false,"associatedRows":["EfficientNet - B7","66"],"associatedColumns":["( 420 , 320 )","ResNet - RS wo / SE","Speed - accuracy comparison between LambdaResNets and EfficientNets .","Infer ( ex / s )"],"associatedMergedColumns":["daResNets are annotated with ( depth , image size ) . Our largest LambdaResNet , LambdaResNet - 420"]},{"number":"307","isBolded":false,"associatedRows":["ViT - L / 16"],"associatedColumns":["( 420 , 320 )","B3","Speed - accuracy comparison between LambdaResNets and EfficientNets .","Params ( M )"],"associatedMergedColumns":["daResNets are annotated with ( depth , image size ) . Our largest LambdaResNet , LambdaResNet - 420"]},{"number":"640(9.5x)","isBolded":false,"associatedRows":["ViT - L / 16"],"associatedColumns":["( 420 , 320 )","ResNet - RS wo / SE","Speed - accuracy comparison between LambdaResNets and EfficientNets .","Infer ( ex / s )"],"associatedMergedColumns":["daResNets are annotated with ( depth , image size ) . Our largest LambdaResNet , LambdaResNet - 420"]},{"number":"87.1","isBolded":true,"associatedRows":["ViT - L / 16"],"associatedColumns":["( 420 , 320 )","ResNet - RS wo / SE","Speed - accuracy comparison between LambdaResNets and EfficientNets .","ImageNet top - 1"],"associatedMergedColumns":["daResNets are annotated with ( depth , image size ) . Our largest LambdaResNet , LambdaResNet - 420"]},{"number":"86.7","isBolded":false,"associatedRows":["LambdaResNet - 152","51","1620","6100"],"associatedColumns":["( 420 , 320 )","ResNet - RS wo / SE","Speed - accuracy comparison between LambdaResNets and EfficientNets .","ImageNet top - 1"],"associatedMergedColumns":["daResNets are annotated with ( depth , image size ) . Our largest LambdaResNet , LambdaResNet - 420"]},{"number":"180(9.0x)","isBolded":false,"associatedRows":["ViT - L / 16"],"associatedColumns":["( 420 , 320 )","B3","Speed - accuracy comparison between LambdaResNets and EfficientNets .","Train ( ex / s )"],"associatedMergedColumns":["daResNets are annotated with ( depth , image size ) . Our largest LambdaResNet , LambdaResNet - 420"]},{"number":"1.6-2.3xfasterthanResNet-RSwithsqueeze-and-excitation.Lamb-","isBolded":false,"associatedRows":["faster than EfficientNets and"],"associatedColumns":["( 420 , 320 )","ResNet - RS wo / SE","B3","Speed - accuracy comparison between LambdaResNets and EfficientNets .","When"],"associatedMergedColumns":["ResNet - RS w / SE"]},{"number":"86.7","isBolded":false,"associatedRows":["EfficientNet - B7","66"],"associatedColumns":["( 420 , 320 )","ResNet - RS wo / SE","Speed - accuracy comparison between LambdaResNets and EfficientNets .","ImageNet top - 1"],"associatedMergedColumns":["daResNets are annotated with ( depth , image size ) . Our largest LambdaResNet , LambdaResNet - 420"]},{"number":"170(9.5x)","isBolded":false,"associatedRows":["EfficientNet - B7","66"],"associatedColumns":["( 420 , 320 )","B3","Speed - accuracy comparison between LambdaResNets and EfficientNets .","Train ( ex / s )"],"associatedMergedColumns":["daResNets are annotated with ( depth , image size ) . Our largest LambdaResNet , LambdaResNet - 420"]},{"number":"84.9%top-1accuracy.","isBolded":false,"associatedRows":["trained at image size 320 , reaches a strong"],"associatedColumns":["( 420 , 320 )","B3","Speed - accuracy comparison between LambdaResNets and EfficientNets ."],"associatedMergedColumns":["daResNets are annotated with ( depth , image size ) . Our largest LambdaResNet , LambdaResNet - 420"]}]},{"caption":"Table 6: COCO object detection and instance segmentation with Mask-RCNN architecture on \n1024x1024 inputs. Mean Average Precision (AP) for small, medium, large objects (s/m/l). Using \nlambda layers yields consistent gains across all object sizes, especially small objects. \n","rows":["LambdaResNet - 101","LambdaResNet - 152","ResNet - 101","ResNet - 101 + SE","ResNet - 152 + SE","ResNet - 152"],"columns":["bb","Using lambda layers yields consistent","coco","s / m / l","AP bb","( He et al . , 2017 ) on the","COCO object detection and instance segmentation tasks .","Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","AP","mask"],"mergedAllColumns":["information ."],"numberCells":[{"number":"46.5/","isBolded":true,"associatedRows":["LambdaResNet - 101"],"associatedColumns":["( He et al . , 2017 ) on the","Using lambda layers yields consistent","mask","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"30.0/","isBolded":false,"associatedRows":["ResNet - 152 + SE"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"25.9/","isBolded":true,"associatedRows":["LambdaResNet - 101"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","Using lambda layers yields consistent","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"61.2","isBolded":false,"associatedRows":["ResNet - 152"],"associatedColumns":["( He et al . , 2017 ) on the","Using lambda layers yields consistent","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"48.2","isBolded":false,"associatedRows":["ResNet - 101"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","AP bb"],"associatedMergedColumns":["information ."]},{"number":"29.9/","isBolded":false,"associatedRows":["ResNet - 101"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","AP"],"associatedMergedColumns":["information ."]},{"number":"64.9","isBolded":false,"associatedRows":["ResNet - 101"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"24.6/","isBolded":false,"associatedRows":["ResNet - 152 + SE"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","Using lambda layers yields consistent","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"62.0","isBolded":true,"associatedRows":["LambdaResNet - 152"],"associatedColumns":["( He et al . , 2017 ) on the","Using lambda layers yields consistent","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"47.3/","isBolded":true,"associatedRows":["LambdaResNet - 152"],"associatedColumns":["( He et al . , 2017 ) on the","Using lambda layers yields consistent","mask","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"29.9/","isBolded":false,"associatedRows":["ResNet - 152"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"66.0","isBolded":false,"associatedRows":["ResNet - 152"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"49.4","isBolded":true,"associatedRows":["LambdaResNet - 101"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","AP bb"],"associatedMergedColumns":["information ."]},{"number":"45.6/","isBolded":false,"associatedRows":["ResNet - 101"],"associatedColumns":["( He et al . , 2017 ) on the","Using lambda layers yields consistent","mask","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"46.8/","isBolded":false,"associatedRows":["ResNet - 152 + SE"],"associatedColumns":["( He et al . , 2017 ) on the","Using lambda layers yields consistent","mask","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"31.8/","isBolded":true,"associatedRows":["LambdaResNet - 152"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"50.0","isBolded":true,"associatedRows":["LambdaResNet - 152"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","AP bb"],"associatedMergedColumns":["information ."]},{"number":"53.4/","isBolded":true,"associatedRows":["LambdaResNet - 152"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","bb","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"46.0/","isBolded":false,"associatedRows":["ResNet - 101 + SE"],"associatedColumns":["( He et al . , 2017 ) on the","Using lambda layers yields consistent","mask","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"43.5","isBolded":false,"associatedRows":["ResNet - 152 + SE"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","Using lambda layers yields consistent","mask","coco"],"associatedMergedColumns":["information ."]},{"number":"65.3","isBolded":false,"associatedRows":["ResNet - 101 + SE"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"60.0","isBolded":false,"associatedRows":["ResNet - 101"],"associatedColumns":["( He et al . , 2017 ) on the","Using lambda layers yields consistent","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"25.5/","isBolded":true,"associatedRows":["LambdaResNet - 152"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","Using lambda layers yields consistent","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"67.0","isBolded":true,"associatedRows":["LambdaResNet - 152"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"43.5","isBolded":true,"associatedRows":["LambdaResNet - 101"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","Using lambda layers yields consistent","mask","coco"],"associatedMergedColumns":["information ."]},{"number":"29.9/","isBolded":false,"associatedRows":["ResNet - 101 + SE"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"48.9","isBolded":false,"associatedRows":["ResNet - 152"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","AP bb"],"associatedMergedColumns":["information ."]},{"number":"48.5","isBolded":false,"associatedRows":["ResNet - 101 + SE"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","AP bb"],"associatedMergedColumns":["information ."]},{"number":"42.8","isBolded":false,"associatedRows":["ResNet - 101 + SE"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","Using lambda layers yields consistent","mask","coco"],"associatedMergedColumns":["information ."]},{"number":"43.2","isBolded":false,"associatedRows":["ResNet - 152"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","Using lambda layers yields consistent","mask","coco"],"associatedMergedColumns":["information ."]},{"number":"46.1/","isBolded":false,"associatedRows":["ResNet - 152"],"associatedColumns":["( He et al . , 2017 ) on the","Using lambda layers yields consistent","mask","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"24.2/","isBolded":false,"associatedRows":["ResNet - 101"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","Using lambda layers yields consistent","AP"],"associatedMergedColumns":["information ."]},{"number":"66.7","isBolded":false,"associatedRows":["ResNet - 152 + SE"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"42.6","isBolded":false,"associatedRows":["ResNet - 101"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","Using lambda layers yields consistent","mask","coco"],"associatedMergedColumns":["information ."]},{"number":"61.8","isBolded":false,"associatedRows":["ResNet - 152 + SE"],"associatedColumns":["( He et al . , 2017 ) on the","Using lambda layers yields consistent","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"24.0/","isBolded":false,"associatedRows":["ResNet - 101 + SE"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","Using lambda layers yields consistent","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"49.4","isBolded":false,"associatedRows":["ResNet - 152 + SE"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","AP bb"],"associatedMergedColumns":["information ."]},{"number":"51.5/","isBolded":false,"associatedRows":["ResNet - 101 + SE"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","bb","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"65.6","isBolded":true,"associatedRows":["LambdaResNet - 101"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"52.3/","isBolded":false,"associatedRows":["ResNet - 152 + SE"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","bb","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"52.2/","isBolded":true,"associatedRows":["LambdaResNet - 101"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","bb","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"60.8","isBolded":true,"associatedRows":["LambdaResNet - 101"],"associatedColumns":["( He et al . , 2017 ) on the","Using lambda layers yields consistent","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"24.2/","isBolded":false,"associatedRows":["ResNet - 152"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","Using lambda layers yields consistent","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"43.9","isBolded":true,"associatedRows":["LambdaResNet - 152"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","Using lambda layers yields consistent","mask","coco"],"associatedMergedColumns":["information ."]},{"number":"31.7/","isBolded":true,"associatedRows":["LambdaResNet - 101"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"60.2","isBolded":false,"associatedRows":["ResNet - 101 + SE"],"associatedColumns":["( He et al . , 2017 ) on the","Using lambda layers yields consistent","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"50.9/","isBolded":false,"associatedRows":["ResNet - 101"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","bb","s / m / l"],"associatedMergedColumns":["information ."]},{"number":"51.8/","isBolded":false,"associatedRows":["ResNet - 152"],"associatedColumns":["Table 6 , we evaluate LambdaResNets as a backbone in Mask - RCNN","COCO object detection and instance segmentation tasks .","bb","s / m / l"],"associatedMergedColumns":["information ."]}]},{"caption":"Table 9: Ablations on the ImageNet classification task when using the lambda layer in a \nResNet50 architecture. All configurations outpeform the convolutional baseline at a lower pa-\nrameter cost. As expected, we get additional improvements by increasing the query depth |k| or \nintra-depth |u|. The number of heads is best set to intermediate values such as |h|\u003d4. A large num-\nber of heads |h| excessively decreases the value depth |v| \u003d d/|h|, while a small number of heads \ntranslates to too few queries, both of which hurt performance. \n\n","rows":["16","ResNet baseline","32"],"columns":["|h|","|u|","top - 1","Params ( M )"],"mergedAllColumns":[],"numberCells":[{"number":"8","isBolded":false,"associatedRows":["32"],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":[],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["16"],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":["16"],"associatedColumns":["|u|"],"associatedMergedColumns":[]},{"number":"77.4","isBolded":false,"associatedRows":["ResNet baseline"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["16"],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":[],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":["16"],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"77.9","isBolded":false,"associatedRows":["16"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"78.9","isBolded":false,"associatedRows":["16"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"15.7","isBolded":false,"associatedRows":["32"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"14.7","isBolded":false,"associatedRows":["16"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"14.8","isBolded":false,"associatedRows":["ResNet baseline"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["32"],"associatedColumns":["|u|"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":[],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"14.7","isBolded":false,"associatedRows":["32"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":[],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":["16"],"associatedColumns":["|u|"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":["32"],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":["16"],"associatedColumns":["|u|"],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":[],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"78.4","isBolded":false,"associatedRows":["32"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["16"],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"77.9","isBolded":false,"associatedRows":["ResNet baseline"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":["16"],"associatedColumns":["|u|"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["32"],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"78.6","isBolded":false,"associatedRows":["16"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["16"],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":["16"],"associatedColumns":["|u|"],"associatedMergedColumns":[]},{"number":"15.6","isBolded":false,"associatedRows":["16"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"77.6","isBolded":false,"associatedRows":["ResNet baseline"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"78.4","isBolded":false,"associatedRows":["32"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"76.9","isBolded":false,"associatedRows":["ResNet baseline"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":[],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["16"],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":[],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"14.8","isBolded":false,"associatedRows":["ResNet baseline"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":["16"],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"78.4","isBolded":false,"associatedRows":["16"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"25.6","isBolded":false,"associatedRows":["ResNet baseline"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":["16"],"associatedColumns":["|u|"],"associatedMergedColumns":[]},{"number":"77.7","isBolded":false,"associatedRows":["16"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":[],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"16.0","isBolded":false,"associatedRows":["16"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":["16"],"associatedColumns":["|u|"],"associatedMergedColumns":[]},{"number":"14.7","isBolded":false,"associatedRows":["16"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":["16"],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"14.7","isBolded":false,"associatedRows":["ResNet baseline"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":["32"],"associatedColumns":["|u|"],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":["32"],"associatedColumns":["|u|"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["16"],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":["16"],"associatedColumns":["|u|"],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":["16"],"associatedColumns":["|u|"],"associatedMergedColumns":[]},{"number":"77.8","isBolded":false,"associatedRows":["ResNet baseline"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"78.5","isBolded":false,"associatedRows":["32"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":[],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":["32"],"associatedColumns":["|u|"],"associatedMergedColumns":[]},{"number":"78.1","isBolded":false,"associatedRows":["16"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"16.0","isBolded":false,"associatedRows":["16"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"4","isBolded":false,"associatedRows":[],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":["16"],"associatedColumns":["|u|"],"associatedMergedColumns":[]},{"number":"15.0","isBolded":false,"associatedRows":["16"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"15.3","isBolded":false,"associatedRows":["32"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"77.9","isBolded":false,"associatedRows":["16"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"14.7","isBolded":false,"associatedRows":["ResNet baseline"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"15.4","isBolded":false,"associatedRows":["32"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":["16"],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"1","isBolded":false,"associatedRows":["16"],"associatedColumns":["|u|"],"associatedMergedColumns":[]},{"number":"15.1","isBolded":false,"associatedRows":["16"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"8","isBolded":false,"associatedRows":["32"],"associatedColumns":["|h|"],"associatedMergedColumns":[]},{"number":"77.2","isBolded":false,"associatedRows":["ResNet baseline"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]}]},{"caption":"Table 10: Contributions of content and positional interactions. As expected, positional interac-\ntions are crucial to perform well on the image classification task. \n","rows":["?"],"columns":["top - 1","FLOPS ( B )","Params ( M )"],"mergedAllColumns":[],"numberCells":[{"number":"68.8","isBolded":false,"associatedRows":["?"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"12.0","isBolded":false,"associatedRows":[],"associatedColumns":["FLOPS ( B )"],"associatedMergedColumns":[]},{"number":"14.9","isBolded":false,"associatedRows":[],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"14.9","isBolded":false,"associatedRows":["?"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"5.0","isBolded":false,"associatedRows":["?"],"associatedColumns":["FLOPS ( B )"],"associatedMergedColumns":[]},{"number":"78.4","isBolded":false,"associatedRows":[],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"78.1","isBolded":false,"associatedRows":["?"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"14.9","isBolded":false,"associatedRows":["?"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"11.9","isBolded":false,"associatedRows":["?"],"associatedColumns":["FLOPS ( B )"],"associatedMergedColumns":[]}]},{"caption":"Table 11: Impact of varying the scope size for positional lambdas on the ImageNet classification \ntask. We replace the 3x3 spatial convolutions in the last 2 stages of a ResNet-50 with lambda layers \n(input image size is 224x224). Flops significantly increase with the scope size, however we stress \nthat larger scopes do not translate to slower latencies when using the einsum implementation (see \n","rows":["Top - 1 Accuracy","FLOPS ( B )"],"columns":["23x23","15x15","7x7","global","3x3","31x31"],"mergedAllColumns":[],"numberCells":[{"number":"78.5","isBolded":false,"associatedRows":["Top - 1 Accuracy"],"associatedColumns":["15x15"],"associatedMergedColumns":[]},{"number":"78.4","isBolded":false,"associatedRows":["Top - 1 Accuracy"],"associatedColumns":["global"],"associatedMergedColumns":[]},{"number":"10.0","isBolded":false,"associatedRows":["FLOPS ( B )"],"associatedColumns":["23x23"],"associatedMergedColumns":[]},{"number":"78.5","isBolded":false,"associatedRows":["Top - 1 Accuracy"],"associatedColumns":["31x31"],"associatedMergedColumns":[]},{"number":"19.4","isBolded":false,"associatedRows":["FLOPS ( B )"],"associatedColumns":["global"],"associatedMergedColumns":[]},{"number":"78.2","isBolded":false,"associatedRows":["Top - 1 Accuracy"],"associatedColumns":["7x7"],"associatedMergedColumns":[]},{"number":"78.3","isBolded":false,"associatedRows":["Top - 1 Accuracy"],"associatedColumns":["23x23"],"associatedMergedColumns":[]},{"number":"5.7","isBolded":false,"associatedRows":["FLOPS ( B )"],"associatedColumns":["3x3"],"associatedMergedColumns":[]},{"number":"7.8","isBolded":false,"associatedRows":["FLOPS ( B )"],"associatedColumns":["15x15"],"associatedMergedColumns":[]},{"number":"77.6","isBolded":false,"associatedRows":["Top - 1 Accuracy"],"associatedColumns":["3x3"],"associatedMergedColumns":[]},{"number":"6.1","isBolded":false,"associatedRows":["FLOPS ( B )"],"associatedColumns":["7x7"],"associatedMergedColumns":[]},{"number":"12.4","isBolded":false,"associatedRows":["FLOPS ( B )"],"associatedColumns":["31x31"],"associatedMergedColumns":[]}]},{"caption":"Table 12: Impact of normalization schemes in the lambda layer. Normalization of the keys along \nthe context spatial dimension m, normalization of the queries along the query depth k. \n\n","rows":["No normalization on keys","L2 normalization on keys","Softmax on keys \u0026 Softmax on queries","Softmax on keys ( default )","No batch normalization on queries and values"],"columns":["top - 1"],"mergedAllColumns":[],"numberCells":[{"number":"78.1","isBolded":false,"associatedRows":["Softmax on keys \u0026 Softmax on queries"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"78.4","isBolded":false,"associatedRows":["Softmax on keys ( default )"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"78.0","isBolded":false,"associatedRows":["L2 normalization on keys"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"70.0","isBolded":false,"associatedRows":["No normalization on keys"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"76.2","isBolded":false,"associatedRows":["No batch normalization on queries and values"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]}]},{"caption":"Table 13: Hybrid models achieve a better speed-accuracy trade-off. Inference throughput and \ntop-1 accuracy as a function of lambda (L) vs convolution (C) layers\u0027 placement in a ResNet50 \narchitecture on 224x224 inputs. Lambda layers in the c5 stage incur almost no speed decrease \ncompared to standard 3x3 convolutions. Lambda layers in the c4 stage are relatively slower than \nstandard 3x3 convolutions but yield significant accuracy gains. \n","rows":["L ? L ? L ? C","1880 ex / s","7160 ex / s","1280 ex / s","C ? C ? L ? L","2200 ex / s","L ? C ? C ? C","4980 ex / s","L ? L ? C ? C","C ? C ? C ? L","L ? L ? L ? L","C ? L ? L ? L","7240 ex / s","1160 ex / s","C ? C ? C ? C"],"columns":["top - 1","Params ( M )"],"mergedAllColumns":[],"numberCells":[{"number":"76.9","isBolded":false,"associatedRows":["C ? C ? C ? C","7240 ex / s"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"25.6","isBolded":false,"associatedRows":["C ? C ? C ? C"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"77.3","isBolded":false,"associatedRows":["L ? C ? C ? C","1880 ex / s"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"78.3","isBolded":false,"associatedRows":["C ? C ? L ? L","4980 ex / s"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"25.5","isBolded":false,"associatedRows":["L ? C ? C ? C"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"15.1","isBolded":false,"associatedRows":["C ? L ? L ? L"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"77.3","isBolded":false,"associatedRows":["C ? C ? C ? L","7160 ex / s"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"78.4","isBolded":false,"associatedRows":["L ? L ? L ? L","1160 ex / s"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"77.2","isBolded":false,"associatedRows":["L ? L ? C ? C","1280 ex / s"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"21.7","isBolded":false,"associatedRows":["L ? L ? L ? C"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"15.4","isBolded":false,"associatedRows":["C ? C ? L ? L"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"18.8","isBolded":false,"associatedRows":["C ? C ? C ? L"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"77.8","isBolded":false,"associatedRows":["L ? L ? L ? C","1160 ex / s"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"25.0","isBolded":false,"associatedRows":["L ? L ? C ? C"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"15.0","isBolded":false,"associatedRows":["L ? L ? L ? L"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"78.3","isBolded":false,"associatedRows":["C ? L ? L ? L","2200 ex / s"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]}]},{"caption":"Table 15: Parameter-efficiency comparison between LambdaResNet-C4 and EfficientNet-B6. \nLambdaResNet-C4 is more parameter-efficient in spite of using a smaller image size. Increasing the \nimage size would likely result in improved accuracy while keeping the number of parameters fixed. \nModels are trained for 350 epochs. \n\n","rows":["EfficientNet - B6","LambdaResNet - 152 - C4","320x320","35","528x528","LambdaResNet - 200 - C4","42","43"],"columns":["top - 1"],"mergedAllColumns":[],"numberCells":[{"number":"84.0","isBolded":false,"associatedRows":["LambdaResNet - 152 - C4","320x320","35"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"84.0","isBolded":false,"associatedRows":["EfficientNet - B6","528x528","43"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"84.3","isBolded":true,"associatedRows":["LambdaResNet - 200 - C4","320x320","42"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]}]},{"caption":"Table 16: Flops-efficiency comparison between LambdaResNet-C4 and EfficientNet-B6. We \nuse smaller local scopes (|m|\u003d7x7) to reduce FLOPS in the lambda layers. Models are trained for \n350 epochs. \n\n","rows":["EfficientNet - B6","34","LambdaResNet - 270 - C4 ( |m|\u003d7x7 )","38","256x256","528x528"],"columns":["top - 1"],"mergedAllColumns":[],"numberCells":[{"number":"84.0","isBolded":false,"associatedRows":["EfficientNet - B6","528x528","38"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"84.0","isBolded":false,"associatedRows":["LambdaResNet - 270 - C4 ( |m|\u003d7x7 )","256x256","34"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]}]},{"caption":"Table 17: Lambda layers improve ImageNet accuracy in a resource-constrained scenario. \nReplacing the 10-th and 16-th inverted bottleneck blocks with lightweight lambda blocks in the \nMobileNet-v2 architecture reduces parameters and flops by ?10% while improving ImageNet ac-\ncuracy by 0.6%. \n","rows":["MobileNet - v2 with 2 lightweight lambda blocks","563","603","MobileNet - v2"],"columns":["top - 1","Params ( M )"],"mergedAllColumns":[],"numberCells":[{"number":"3.21","isBolded":true,"associatedRows":["MobileNet - v2 with 2 lightweight lambda blocks"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"3.50","isBolded":false,"associatedRows":["MobileNet - v2"],"associatedColumns":["Params ( M )"],"associatedMergedColumns":[]},{"number":"73.3","isBolded":true,"associatedRows":["MobileNet - v2 with 2 lightweight lambda blocks","563"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]},{"number":"72.7","isBolded":false,"associatedRows":["MobileNet - v2","603"],"associatedColumns":["top - 1"],"associatedMergedColumns":[]}]},{"caption":"Table 19: Detailed LambdaResNets results. Latency refers to the time per training step for a batch \nsize of 1024 on 8 TPU-v3 cores using bfloat16 activations. \n\n","rows":["50","-"],"columns":["Pseudo - labels top - 1","Supervised top - 1","Image size","Depth","Latency ( s )"],"mergedAllColumns":["-"],"numberCells":[{"number":"320","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image size"],"associatedMergedColumns":["-"]},{"number":"152","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":[]},{"number":"350","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":["-"]},{"number":"1.16","isBolded":false,"associatedRows":["50"],"associatedColumns":["Latency ( s )"],"associatedMergedColumns":["-"]},{"number":"1.91","isBolded":false,"associatedRows":["50"],"associatedColumns":["Latency ( s )"],"associatedMergedColumns":["-"]},{"number":"0.63","isBolded":false,"associatedRows":["50"],"associatedColumns":["Latency ( s )"],"associatedMergedColumns":["-"]},{"number":"350","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":["-"]},{"number":"288","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image size"],"associatedMergedColumns":["-"]},{"number":"82.5","isBolded":false,"associatedRows":["50"],"associatedColumns":["Supervised top - 1"],"associatedMergedColumns":[]},{"number":"84.9","isBolded":false,"associatedRows":["50"],"associatedColumns":["Supervised top - 1"],"associatedMergedColumns":["-"]},{"number":"84.4","isBolded":false,"associatedRows":["50"],"associatedColumns":["Supervised top - 1"],"associatedMergedColumns":["-"]},{"number":"152","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":[]},{"number":"83.4","isBolded":false,"associatedRows":["50"],"associatedColumns":["Pseudo - labels top - 1"],"associatedMergedColumns":[]},{"number":"80.8","isBolded":false,"associatedRows":["50"],"associatedColumns":["Supervised top - 1"],"associatedMergedColumns":[]},{"number":"192","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image size"],"associatedMergedColumns":[]},{"number":"192","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image size"],"associatedMergedColumns":[]},{"number":"2.25","isBolded":false,"associatedRows":["50"],"associatedColumns":["Latency ( s )"],"associatedMergedColumns":["-"]},{"number":"160","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image size"],"associatedMergedColumns":[]},{"number":"86.5","isBolded":false,"associatedRows":["50"],"associatedColumns":["Pseudo - labels top - 1"],"associatedMergedColumns":[]},{"number":"1.48","isBolded":false,"associatedRows":["50"],"associatedColumns":["Latency ( s )"],"associatedMergedColumns":["-"]},{"number":"420","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":["-"]},{"number":"0.91","isBolded":false,"associatedRows":["50"],"associatedColumns":["Latency ( s )"],"associatedMergedColumns":["-"]},{"number":"86.1","isBolded":false,"associatedRows":["50"],"associatedColumns":["Pseudo - labels top - 1"],"associatedMergedColumns":[]},{"number":"101","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":[]},{"number":"288","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image size"],"associatedMergedColumns":["-"]},{"number":"86.7","isBolded":false,"associatedRows":["50","-"],"associatedColumns":["Pseudo - labels top - 1"],"associatedMergedColumns":["-"]},{"number":"83.8","isBolded":false,"associatedRows":["50"],"associatedColumns":["Supervised top - 1"],"associatedMergedColumns":[]},{"number":"0.38","isBolded":false,"associatedRows":["50"],"associatedColumns":["Latency ( s )"],"associatedMergedColumns":[]},{"number":"81.9","isBolded":false,"associatedRows":["50"],"associatedColumns":["Supervised top - 1"],"associatedMergedColumns":[]},{"number":"0.28","isBolded":false,"associatedRows":["50"],"associatedColumns":["Latency ( s )"],"associatedMergedColumns":[]},{"number":"79.2","isBolded":false,"associatedRows":["50"],"associatedColumns":["Supervised top - 1"],"associatedMergedColumns":[]},{"number":"84.5","isBolded":false,"associatedRows":["50"],"associatedColumns":["Supervised top - 1"],"associatedMergedColumns":["-"]},{"number":"101","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":[]},{"number":"84.7","isBolded":false,"associatedRows":["50"],"associatedColumns":["Pseudo - labels top - 1"],"associatedMergedColumns":[]},{"number":"350","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":["-"]},{"number":"84.7","isBolded":false,"associatedRows":["50"],"associatedColumns":["Supervised top - 1"],"associatedMergedColumns":["-"]},{"number":"85.4","isBolded":false,"associatedRows":["50"],"associatedColumns":["Pseudo - labels top - 1"],"associatedMergedColumns":[]},{"number":"152","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":[]},{"number":"256","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image size"],"associatedMergedColumns":[]},{"number":"82.1","isBolded":false,"associatedRows":["50"],"associatedColumns":["Pseudo - labels top - 1"],"associatedMergedColumns":[]},{"number":"84.2","isBolded":false,"associatedRows":["50"],"associatedColumns":["Supervised top - 1"],"associatedMergedColumns":["-"]},{"number":"224","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image size"],"associatedMergedColumns":[]},{"number":"0.49","isBolded":false,"associatedRows":["50"],"associatedColumns":["Latency ( s )"],"associatedMergedColumns":[]},{"number":"0.14","isBolded":false,"associatedRows":["50"],"associatedColumns":["Latency ( s )"],"associatedMergedColumns":[]},{"number":"256","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image size"],"associatedMergedColumns":["-"]},{"number":"160","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image size"],"associatedMergedColumns":[]},{"number":"0.058","isBolded":false,"associatedRows":["50"],"associatedColumns":["Latency ( s )"],"associatedMergedColumns":[]},{"number":"320","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image size"],"associatedMergedColumns":["-"]},{"number":"128","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image size"],"associatedMergedColumns":[]},{"number":"152","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":["-"]},{"number":"77.4","isBolded":false,"associatedRows":["50"],"associatedColumns":["Supervised top - 1"],"associatedMergedColumns":[]},{"number":"83.2","isBolded":false,"associatedRows":["50"],"associatedColumns":["Supervised top - 1"],"associatedMergedColumns":[]},{"number":"0.089","isBolded":false,"associatedRows":["50"],"associatedColumns":["Latency ( s )"],"associatedMergedColumns":[]},{"number":"0.20","isBolded":false,"associatedRows":["50"],"associatedColumns":["Latency ( s )"],"associatedMergedColumns":[]},{"number":"270","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":["-"]},{"number":"256","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image size"],"associatedMergedColumns":["-"]}]},{"caption":"Table 20: Hyperparameters used to train LambdaResNets. We train for 350 epochs with Ran-\ndAugment, dropout and stochastic depth. \n\n","rows":["15","50","10"],"columns":["Stochastic depth rate","Image Size","Depth","Dropout"],"mergedAllColumns":["0"],"numberCells":[{"number":"320","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image Size"],"associatedMergedColumns":["0"]},{"number":"0.2","isBolded":false,"associatedRows":["50","15"],"associatedColumns":["Stochastic depth rate"],"associatedMergedColumns":["0"]},{"number":"0.3","isBolded":false,"associatedRows":["50","10"],"associatedColumns":["Dropout"],"associatedMergedColumns":["0"]},{"number":"350","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":["0"]},{"number":"0.3","isBolded":false,"associatedRows":["50","15"],"associatedColumns":["Dropout"],"associatedMergedColumns":["0"]},{"number":"0.3","isBolded":false,"associatedRows":["50","15"],"associatedColumns":["Dropout"],"associatedMergedColumns":["0"]},{"number":"256","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image Size"],"associatedMergedColumns":["0"]},{"number":"0.1","isBolded":false,"associatedRows":["50","15"],"associatedColumns":["Stochastic depth rate"],"associatedMergedColumns":["0"]},{"number":"0.3","isBolded":false,"associatedRows":["50","15"],"associatedColumns":["Dropout"],"associatedMergedColumns":["0"]},{"number":"101","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":["0"]},{"number":"0.3","isBolded":false,"associatedRows":["50","15"],"associatedColumns":["Dropout"],"associatedMergedColumns":["0"]},{"number":"0.2","isBolded":false,"associatedRows":["50","15"],"associatedColumns":["Stochastic depth rate"],"associatedMergedColumns":["0"]},{"number":"350","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":["0"]},{"number":"320","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image Size"],"associatedMergedColumns":["0"]},{"number":"192","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image Size"],"associatedMergedColumns":["0"]},{"number":"256","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image Size"],"associatedMergedColumns":["0"]},{"number":"152","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":["0"]},{"number":"256","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image Size"],"associatedMergedColumns":["0"]},{"number":"0.2","isBolded":false,"associatedRows":["50","15"],"associatedColumns":["Dropout"],"associatedMergedColumns":["0"]},{"number":"350","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":["0"]},{"number":"0.2","isBolded":false,"associatedRows":["50","10"],"associatedColumns":["Dropout"],"associatedMergedColumns":["0"]},{"number":"152","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":["0"]},{"number":"160","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image Size"],"associatedMergedColumns":["0"]},{"number":"160","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image Size"],"associatedMergedColumns":["0"]},{"number":"0.2","isBolded":false,"associatedRows":["50","15"],"associatedColumns":["Stochastic depth rate"],"associatedMergedColumns":["0"]},{"number":"0.2","isBolded":false,"associatedRows":["50","10"],"associatedColumns":["Dropout"],"associatedMergedColumns":[]},{"number":"0.3","isBolded":false,"associatedRows":["50","15"],"associatedColumns":["Dropout"],"associatedMergedColumns":["0"]},{"number":"192","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image Size"],"associatedMergedColumns":["0"]},{"number":"152","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":["0"]},{"number":"0.1","isBolded":false,"associatedRows":["50","15"],"associatedColumns":["Stochastic depth rate"],"associatedMergedColumns":["0"]},{"number":"101","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":["0"]},{"number":"128","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image Size"],"associatedMergedColumns":[]},{"number":"0.1","isBolded":false,"associatedRows":["50","15"],"associatedColumns":["Stochastic depth rate"],"associatedMergedColumns":["0"]},{"number":"152","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":["0"]},{"number":"0.3","isBolded":false,"associatedRows":["50","15"],"associatedColumns":["Dropout"],"associatedMergedColumns":["0"]},{"number":"0.3","isBolded":false,"associatedRows":["50","15"],"associatedColumns":["Dropout"],"associatedMergedColumns":["0"]},{"number":"270","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":["0"]},{"number":"0.3","isBolded":false,"associatedRows":["50","15"],"associatedColumns":["Dropout"],"associatedMergedColumns":["0"]},{"number":"288","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image Size"],"associatedMergedColumns":["0"]},{"number":"420","isBolded":false,"associatedRows":[],"associatedColumns":["Depth"],"associatedMergedColumns":["0"]},{"number":"288","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image Size"],"associatedMergedColumns":["0"]},{"number":"224","isBolded":false,"associatedRows":["50"],"associatedColumns":["Image Size"],"associatedMergedColumns":["0"]},{"number":"0.1","isBolded":false,"associatedRows":["50","15"],"associatedColumns":["Stochastic depth rate"],"associatedMergedColumns":["0"]},{"number":"0.2","isBolded":false,"associatedRows":["50","15"],"associatedColumns":["Stochastic depth rate"],"associatedMergedColumns":["0"]},{"number":"0.3","isBolded":false,"associatedRows":["50","15"],"associatedColumns":["Dropout"],"associatedMergedColumns":["0"]}]}]