[{"caption":"wav2vec: Unsupervised Pre-training for Speech Recognition \n\nnov93dev \nnov92 \nLER \nWER LER \nWER \n\nDeep Speech 2 (12K h labeled speech; Amodei et al., 2016) -\n4.42 \n-\n3.1 \nTrainable frontend (Zeghidour et al., 2018a) \n-\n6.8 \n-\n3.5 \nLattice-free MMI (Hadian et al., 2018) \n-\n5.66  ? -\n2.8  ? \nSupervised transfer-learning (Ghahremani et al., 2017) \n-\n4.99  ? -\n2.53  ? \n\n4-GRAM LM (Heafield et al., 2013) \nBaseline \n-\n-3.32 \n8.57 \n2.19 \n5.64 \nwav2vec \nLibrispeech \n80 h 3.71 \n9.11 \n2.17 \n5.55 \nwav2vec \nLibrispeech \n960 h 2.85 \n7.40 \n1.76 \n4.57 \nwav2vec \nLibri + WSJ \n1,041 h 2.91 \n7.59 \n1.67 \n4.61 \nwav2vec large \nLibrispeech \n960 h 2.73 \n6.96 \n1.57 \n4.32 \n\nWORD CONVLM (Zeghidour et al., 2018b) \nBaseline \n-\n-2.57 \n6.27 \n1.51 \n3.60 \nwav2vec \nLibrispeech \n960 h 2.22 \n5.39 \n1.25 \n2.87 \nwav2vec large \nLibrispeech \n960 h 2.13 \n5.16 \n1.02 \n2.53 \n\nCHAR CONVLM (Likhomanenko et al., 2019) \nBaseline \n-\n-2.77 \n6.67 \n1.53 \n3.46 \nwav2vec \nLibrispeech \n960 h 2.14 \n5.31 \n1.15 \n2.78 \nwav2vec large \nLibrispeech \n960 h 2.11 \n5.10 \n0.99 \n2.43 \n\nTable 1: Replacing log-mel filterbanks (Baseline) by pre-trained embeddings improves WSJ per-\nformance on test (nov92) and validation (nov93dev) in terms of both LER and WER. We evaluate \npre-training on the acoustic data of part of clean and full Librispeech as well as the combination of \nall of them.  ? indicates results with phoneme-based models. \n\n","rows":["Libri + WSJ","960 h","Baseline","wav2vec","-","Deep Speech 2 ( 12K h labeled speech ; Amodei et al . , 2016 )","Lattice - free MMI ( Hadian et al . , 2018 )","Supervised transfer - learning ( Ghahremani et al . , 2017 )","wav2vec large","Trainable frontend ( Zeghidour et al . , 2018a )","80 h","1 , 041 h","Librispeech"],"columns":["WER","LER","wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","nov92"],"mergedAllColumns":["CHAR CONVLM ( Likhomanenko et al . , 2019 )","4 - GRAM LM ( Heafield et al . , 2013 )","WORD CONVLM ( Zeghidour et al . , 2018b )"],"numberCells":[{"number":"1.51","isBolded":false,"associatedRows":["Baseline","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","LER"],"associatedMergedColumns":["WORD CONVLM ( Zeghidour et al . , 2018b )"]},{"number":"5.55","isBolded":false,"associatedRows":["wav2vec","Librispeech","80 h","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","WER"],"associatedMergedColumns":["4 - GRAM LM ( Heafield et al . , 2013 )"]},{"number":"2.43","isBolded":false,"associatedRows":["wav2vec large","Librispeech","960 h","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","WER"],"associatedMergedColumns":["CHAR CONVLM ( Likhomanenko et al . , 2019 )"]},{"number":"2.53?","isBolded":false,"associatedRows":["Supervised transfer - learning ( Ghahremani et al . , 2017 )","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","WER"],"associatedMergedColumns":[]},{"number":"1.57","isBolded":false,"associatedRows":["wav2vec large","Librispeech","960 h","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","LER"],"associatedMergedColumns":["4 - GRAM LM ( Heafield et al . , 2013 )"]},{"number":"8.57","isBolded":false,"associatedRows":["Baseline","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","WER"],"associatedMergedColumns":["4 - GRAM LM ( Heafield et al . , 2013 )"]},{"number":"2.53","isBolded":false,"associatedRows":["wav2vec large","Librispeech","960 h","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","WER"],"associatedMergedColumns":["WORD CONVLM ( Zeghidour et al . , 2018b )"]},{"number":"6.27","isBolded":false,"associatedRows":["Baseline","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","WER"],"associatedMergedColumns":["WORD CONVLM ( Zeghidour et al . , 2018b )"]},{"number":"2.17","isBolded":false,"associatedRows":["wav2vec","Librispeech","80 h","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","LER"],"associatedMergedColumns":["4 - GRAM LM ( Heafield et al . , 2013 )"]},{"number":"5.39","isBolded":false,"associatedRows":["wav2vec","Librispeech","960 h","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","WER"],"associatedMergedColumns":["WORD CONVLM ( Zeghidour et al . , 2018b )"]},{"number":"4.32","isBolded":false,"associatedRows":["wav2vec large","Librispeech","960 h","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","WER"],"associatedMergedColumns":["4 - GRAM LM ( Heafield et al . , 2013 )"]},{"number":"2.78","isBolded":false,"associatedRows":["wav2vec","Librispeech","960 h","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","WER"],"associatedMergedColumns":["CHAR CONVLM ( Likhomanenko et al . , 2019 )"]},{"number":"4.99?","isBolded":false,"associatedRows":["Supervised transfer - learning ( Ghahremani et al . , 2017 )","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","WER"],"associatedMergedColumns":[]},{"number":"3.60","isBolded":false,"associatedRows":["Baseline","-","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","WER"],"associatedMergedColumns":["WORD CONVLM ( Zeghidour et al . , 2018b )"]},{"number":"2.13","isBolded":false,"associatedRows":["wav2vec large","Librispeech","960 h"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","LER"],"associatedMergedColumns":["WORD CONVLM ( Zeghidour et al . , 2018b )"]},{"number":"2.85","isBolded":false,"associatedRows":["wav2vec","Librispeech","960 h"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","LER"],"associatedMergedColumns":["4 - GRAM LM ( Heafield et al . , 2013 )"]},{"number":"1.25","isBolded":false,"associatedRows":["wav2vec","Librispeech","960 h","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","LER"],"associatedMergedColumns":["WORD CONVLM ( Zeghidour et al . , 2018b )"]},{"number":"6.96","isBolded":false,"associatedRows":["wav2vec large","Librispeech","960 h","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","WER"],"associatedMergedColumns":["4 - GRAM LM ( Heafield et al . , 2013 )"]},{"number":"-2.77","isBolded":false,"associatedRows":["Baseline","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","LER"],"associatedMergedColumns":["CHAR CONVLM ( Likhomanenko et al . , 2019 )"]},{"number":"-3.32","isBolded":false,"associatedRows":["Baseline","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","LER"],"associatedMergedColumns":["4 - GRAM LM ( Heafield et al . , 2013 )"]},{"number":"0.99","isBolded":false,"associatedRows":["wav2vec large","Librispeech","960 h","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","LER"],"associatedMergedColumns":["CHAR CONVLM ( Likhomanenko et al . , 2019 )"]},{"number":"2.22","isBolded":false,"associatedRows":["wav2vec","Librispeech","960 h"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","LER"],"associatedMergedColumns":["WORD CONVLM ( Zeghidour et al . , 2018b )"]},{"number":"2.8?","isBolded":false,"associatedRows":["Lattice - free MMI ( Hadian et al . , 2018 )","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","WER"],"associatedMergedColumns":[]},{"number":"1.15","isBolded":false,"associatedRows":["wav2vec","Librispeech","960 h","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","LER"],"associatedMergedColumns":["CHAR CONVLM ( Likhomanenko et al . , 2019 )"]},{"number":"1.02","isBolded":false,"associatedRows":["wav2vec large","Librispeech","960 h","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","LER"],"associatedMergedColumns":["WORD CONVLM ( Zeghidour et al . , 2018b )"]},{"number":"4.42","isBolded":false,"associatedRows":["Deep Speech 2 ( 12K h labeled speech ; Amodei et al . , 2016 )","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","WER"],"associatedMergedColumns":[]},{"number":"5.10","isBolded":false,"associatedRows":["wav2vec large","Librispeech","960 h","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","WER"],"associatedMergedColumns":["CHAR CONVLM ( Likhomanenko et al . , 2019 )"]},{"number":"6.67","isBolded":false,"associatedRows":["Baseline","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","WER"],"associatedMergedColumns":["CHAR CONVLM ( Likhomanenko et al . , 2019 )"]},{"number":"5.64","isBolded":false,"associatedRows":["Baseline","-","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","WER"],"associatedMergedColumns":["4 - GRAM LM ( Heafield et al . , 2013 )"]},{"number":"4.61","isBolded":false,"associatedRows":["wav2vec","Libri + WSJ","1 , 041 h","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","WER"],"associatedMergedColumns":["4 - GRAM LM ( Heafield et al . , 2013 )"]},{"number":"2.73","isBolded":false,"associatedRows":["wav2vec large","Librispeech","960 h"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","LER"],"associatedMergedColumns":["4 - GRAM LM ( Heafield et al . , 2013 )"]},{"number":"3.46","isBolded":false,"associatedRows":["Baseline","-","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","WER"],"associatedMergedColumns":["CHAR CONVLM ( Likhomanenko et al . , 2019 )"]},{"number":"1.67","isBolded":false,"associatedRows":["wav2vec","Libri + WSJ","1 , 041 h","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","LER"],"associatedMergedColumns":["4 - GRAM LM ( Heafield et al . , 2013 )"]},{"number":"2.11","isBolded":false,"associatedRows":["wav2vec large","Librispeech","960 h"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","LER"],"associatedMergedColumns":["CHAR CONVLM ( Likhomanenko et al . , 2019 )"]},{"number":"2.14","isBolded":false,"associatedRows":["wav2vec","Librispeech","960 h"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","LER"],"associatedMergedColumns":["CHAR CONVLM ( Likhomanenko et al . , 2019 )"]},{"number":"3.5","isBolded":false,"associatedRows":["Trainable frontend ( Zeghidour et al . , 2018a )","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","WER"],"associatedMergedColumns":[]},{"number":"2.19","isBolded":false,"associatedRows":["Baseline","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","LER"],"associatedMergedColumns":["4 - GRAM LM ( Heafield et al . , 2013 )"]},{"number":"7.59","isBolded":false,"associatedRows":["wav2vec","Libri + WSJ","1 , 041 h","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","WER"],"associatedMergedColumns":["4 - GRAM LM ( Heafield et al . , 2013 )"]},{"number":"7.40","isBolded":false,"associatedRows":["wav2vec","Librispeech","960 h","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","WER"],"associatedMergedColumns":["4 - GRAM LM ( Heafield et al . , 2013 )"]},{"number":"6.8","isBolded":false,"associatedRows":["Trainable frontend ( Zeghidour et al . , 2018a )","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","WER"],"associatedMergedColumns":[]},{"number":"-2.57","isBolded":false,"associatedRows":["Baseline","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","LER"],"associatedMergedColumns":["WORD CONVLM ( Zeghidour et al . , 2018b )"]},{"number":"5.16","isBolded":false,"associatedRows":["wav2vec large","Librispeech","960 h","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","WER"],"associatedMergedColumns":["WORD CONVLM ( Zeghidour et al . , 2018b )"]},{"number":"4.57","isBolded":false,"associatedRows":["wav2vec","Librispeech","960 h","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","WER"],"associatedMergedColumns":["4 - GRAM LM ( Heafield et al . , 2013 )"]},{"number":"3.71","isBolded":false,"associatedRows":["wav2vec","Librispeech","80 h"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","LER"],"associatedMergedColumns":["4 - GRAM LM ( Heafield et al . , 2013 )"]},{"number":"5.66?","isBolded":false,"associatedRows":["Lattice - free MMI ( Hadian et al . , 2018 )","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","WER"],"associatedMergedColumns":[]},{"number":"1.76","isBolded":false,"associatedRows":["wav2vec","Librispeech","960 h","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","LER"],"associatedMergedColumns":["4 - GRAM LM ( Heafield et al . , 2013 )"]},{"number":"1.53","isBolded":false,"associatedRows":["Baseline","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","LER"],"associatedMergedColumns":["CHAR CONVLM ( Likhomanenko et al . , 2019 )"]},{"number":"3.1","isBolded":false,"associatedRows":["Deep Speech 2 ( 12K h labeled speech ; Amodei et al . , 2016 )","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","WER"],"associatedMergedColumns":[]},{"number":"2.91","isBolded":false,"associatedRows":["wav2vec","Libri + WSJ","1 , 041 h"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","LER"],"associatedMergedColumns":["4 - GRAM LM ( Heafield et al . , 2013 )"]},{"number":"5.31","isBolded":false,"associatedRows":["wav2vec","Librispeech","960 h","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","WER"],"associatedMergedColumns":["CHAR CONVLM ( Likhomanenko et al . , 2019 )"]},{"number":"9.11","isBolded":false,"associatedRows":["wav2vec","Librispeech","80 h","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov93dev","WER"],"associatedMergedColumns":["4 - GRAM LM ( Heafield et al . , 2013 )"]},{"number":"2.87","isBolded":false,"associatedRows":["wav2vec","Librispeech","960 h","-","-"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","nov92","WER"],"associatedMergedColumns":["WORD CONVLM ( Zeghidour et al . , 2018b )"]}]},{"caption":"Table 2: Results for phoneme recognition on TIMIT in terms of PER. All our models use the CNN-\n8L-PReLU-do0.7 architecture (Zeghidour et al., 2018a). \n\n","rows":["1","2","5","20","10"],"columns":["train time ( h )","dev PER"],"mergedAllColumns":[],"numberCells":[{"number":"10.5","isBolded":false,"associatedRows":["10"],"associatedColumns":["train time ( h )"],"associatedMergedColumns":[]},{"number":"15.7","isBolded":false,"associatedRows":["20"],"associatedColumns":["dev PER"],"associatedMergedColumns":[]},{"number":"6.1","isBolded":false,"associatedRows":["1"],"associatedColumns":["train time ( h )"],"associatedMergedColumns":[]},{"number":"15.3","isBolded":false,"associatedRows":["20"],"associatedColumns":["train time ( h )"],"associatedMergedColumns":[]},{"number":"6.3","isBolded":false,"associatedRows":["2"],"associatedColumns":["train time ( h )"],"associatedMergedColumns":[]},{"number":"15.8","isBolded":false,"associatedRows":["2"],"associatedColumns":["dev PER"],"associatedMergedColumns":[]},{"number":"15.9","isBolded":false,"associatedRows":["5"],"associatedColumns":["dev PER"],"associatedMergedColumns":[]},{"number":"16.3","isBolded":false,"associatedRows":["1"],"associatedColumns":["dev PER"],"associatedMergedColumns":[]},{"number":"15.5","isBolded":false,"associatedRows":["10"],"associatedColumns":["dev PER"],"associatedMergedColumns":[]},{"number":"8.2","isBolded":false,"associatedRows":["5"],"associatedColumns":["train time ( h )"],"associatedMergedColumns":[]}]},{"caption":"Table 3: Effect of different number of negative samples during pre-training for TIMIT on the devel-\nopment set. \n\n","rows":["1","2","5","20","10"],"columns":["train time ( h )","dev PER"],"mergedAllColumns":[],"numberCells":[{"number":"15.8","isBolded":false,"associatedRows":["2"],"associatedColumns":["dev PER"],"associatedMergedColumns":[]},{"number":"15.9","isBolded":false,"associatedRows":["5"],"associatedColumns":["dev PER"],"associatedMergedColumns":[]},{"number":"8.2","isBolded":false,"associatedRows":["5"],"associatedColumns":["train time ( h )"],"associatedMergedColumns":[]},{"number":"16.3","isBolded":false,"associatedRows":["1"],"associatedColumns":["dev PER"],"associatedMergedColumns":[]},{"number":"6.3","isBolded":false,"associatedRows":["2"],"associatedColumns":["train time ( h )"],"associatedMergedColumns":[]},{"number":"6.1","isBolded":false,"associatedRows":["1"],"associatedColumns":["train time ( h )"],"associatedMergedColumns":[]},{"number":"10.5","isBolded":false,"associatedRows":["10"],"associatedColumns":["train time ( h )"],"associatedMergedColumns":[]},{"number":"15.7","isBolded":false,"associatedRows":["20"],"associatedColumns":["dev PER"],"associatedMergedColumns":[]},{"number":"15.5","isBolded":false,"associatedRows":["10"],"associatedColumns":["dev PER"],"associatedMergedColumns":[]},{"number":"15.3","isBolded":false,"associatedRows":["20"],"associatedColumns":["train time ( h )"],"associatedMergedColumns":[]}]},{"caption":"wav2vec: Unsupervised Pre-training for Speech Recognition \n\nCrop size \ndev PER \n\nNone (Avg. 207k) \n16.3 \n100k \n16.1 \n150k \n15.5 \n200k \n16.0 \n\nTable 4: Effect of different crop sizes \n(cf. Table 3). \n\n","rows":["None ( Avg . 207k )","150k","100k","200k"],"columns":["dev PER","wav2vec : Unsupervised Pre - training for Speech Recognition"],"mergedAllColumns":[],"numberCells":[{"number":"16.3","isBolded":false,"associatedRows":["None ( Avg . 207k )"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","dev PER"],"associatedMergedColumns":[]},{"number":"15.5","isBolded":true,"associatedRows":["150k"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","dev PER"],"associatedMergedColumns":[]},{"number":"16.0","isBolded":false,"associatedRows":["200k"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","dev PER"],"associatedMergedColumns":[]},{"number":"16.1","isBolded":false,"associatedRows":["100k"],"associatedColumns":["wav2vec : Unsupervised Pre - training for Speech Recognition","dev PER"],"associatedMergedColumns":[]}]},{"caption":"Table 5: Effect of different number of \ntasks K (cf. Table 3). \n\n","rows":["12","16","8"],"columns":["dev PER"],"mergedAllColumns":[],"numberCells":[{"number":"15.9","isBolded":false,"associatedRows":["8"],"associatedColumns":["dev PER"],"associatedMergedColumns":[]},{"number":"15.5","isBolded":false,"associatedRows":["16"],"associatedColumns":["dev PER"],"associatedMergedColumns":[]},{"number":"15.5","isBolded":true,"associatedRows":["12"],"associatedColumns":["dev PER"],"associatedMergedColumns":[]}]}]