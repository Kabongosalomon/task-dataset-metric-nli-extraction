[{"caption":"Table 1: IWSLT\u002714 En?De BLEU scores utilizing \ncontextualized embeddings from various pre-trained \nlanguage models. random represents the embedding \nlayer of the NMT encoder that is randomly initial-\nized but uses the same vocabulary of the assigned pre-\ntrained language model. pre-trained means the em-\nbedding layer of the NMT encoder use the output of \nthe assigned frozen pre-trained language model dur-\ning MT training. Numbers in the bracket show the \nincrement/deduction compared with the corresponding \nmodel compared to randomly initialized embeddings. \n\n","rows":["pre - trained","random","27 . 85 ( ? 0 . 02 )","GOTTBERT","-","BIBERT"],"columns":["De?En","score","En?De","34 . 26 ( +0 . 25 )","-","28 . 74 ( +1 . 44 )","27 . 37 ( ? 0 . 43 )","36 . 32 ( +2 . 76 )"],"mergedAllColumns":["-","More training details are described in Appendix A ."],"numberCells":[{"number":"34.01","isBolded":false,"associatedRows":["GOTTBERT","random","27 . 85 ( ? 0 . 02 )"],"associatedColumns":["score","De?En","-","36 . 32 ( +2 . 76 )"],"associatedMergedColumns":["-"]},{"number":"27.3","isBolded":false,"associatedRows":["GOTTBERT","random"],"associatedColumns":["score","En?De"],"associatedMergedColumns":["More training details are described in Appendix A ."]},{"number":"33.67","isBolded":false,"associatedRows":["GOTTBERT","random","27 . 85 ( ? 0 . 02 )"],"associatedColumns":["score","De?En","-","36 . 32 ( +2 . 76 )","34 . 26 ( +0 . 25 )"],"associatedMergedColumns":["-"]},{"number":"35.38(+1.71)","isBolded":false,"associatedRows":["GOTTBERT","pre - trained","27 . 85 ( ? 0 . 02 )"],"associatedColumns":["score","De?En","-","36 . 32 ( +2 . 76 )","34 . 26 ( +0 . 25 )"],"associatedMergedColumns":["-"]},{"number":"27.53","isBolded":false,"associatedRows":["BIBERT","random"],"associatedColumns":["score","En?De","28 . 74 ( +1 . 44 )","-","27 . 37 ( ? 0 . 43 )"],"associatedMergedColumns":["-"]},{"number":"33.52","isBolded":false,"associatedRows":["GOTTBERT","random","27 . 85 ( ? 0 . 02 )"],"associatedColumns":["score","De?En","-","36 . 32 ( +2 . 76 )","34 . 26 ( +0 . 25 )"],"associatedMergedColumns":["-"]},{"number":"27.80","isBolded":false,"associatedRows":["GOTTBERT","random"],"associatedColumns":["score","En?De","28 . 74 ( +1 . 44 )","-"],"associatedMergedColumns":["-"]},{"number":"33.56","isBolded":false,"associatedRows":["GOTTBERT","random","-"],"associatedColumns":["score","De?En","-"],"associatedMergedColumns":["-"]},{"number":"27.87","isBolded":false,"associatedRows":["GOTTBERT","random"],"associatedColumns":["score","En?De","28 . 74 ( +1 . 44 )","-","27 . 37 ( ? 0 . 43 )"],"associatedMergedColumns":["-"]}]},{"caption":"Table 1. We first conduct experiments \nwith randomly initialized embeddings to obtain \nbaselines. Feeding the output of a pre-trained lan-\nguage model into an NMT model necessitates that \nthe vocabulary of the encoder should be the same \nas the one used for the language model. To en-\nsure that improvements are not the result of choos-\ning a better vocabulary, we train randomly ini-\ntialized baseline systems using identical vocabu-\nlaries for each encoder. For these experiments, \nthe decoder\u0027s vocabulary size is fixed to 8K in \norder to make fair comparisons. We investigate \ndecoder vocabulary size selection in more detail \nin Section 2.5. When the embedding layer of the \nMT encoder is randomly initialized, as opposed \nto using the pre-trained language model, we ob-\n\nserve similar BLEU scores for all baselines from \nEnglish-to-German (around 27.6) and German-to-\nEnglish (around 33.7). By replacing the embed-\nding layer with contextualized embeddings, GOT-\nTBERT boosts the BLEU scores of De?En from \n33.56 to 36.32, and ROBERTA strengthens the \nEn?De translation from 27.3 to 28.74. However, \nthe MBERT and XLM-R only provide modest \nimprovement in De?En translation and even de-\ngenerate the performance of En?De translation. \n\n","rows":["pre - trained","random","27 . 85 ( ? 0 . 02 )","GOTTBERT","-","BIBERT"],"columns":["De?En","score","En?De","34 . 26 ( +0 . 25 )","-","28 . 74 ( +1 . 44 )","27 . 37 ( ? 0 . 43 )","36 . 32 ( +2 . 76 )"],"mergedAllColumns":["-","More training details are described in Appendix A ."],"numberCells":[{"number":"34.01","isBolded":false,"associatedRows":["GOTTBERT","random","27 . 85 ( ? 0 . 02 )"],"associatedColumns":["score","De?En","-","36 . 32 ( +2 . 76 )"],"associatedMergedColumns":["-"]},{"number":"27.3","isBolded":false,"associatedRows":["GOTTBERT","random"],"associatedColumns":["score","En?De"],"associatedMergedColumns":["More training details are described in Appendix A ."]},{"number":"33.56","isBolded":false,"associatedRows":["GOTTBERT","random","-"],"associatedColumns":["score","De?En","-"],"associatedMergedColumns":["-"]},{"number":"27.87","isBolded":false,"associatedRows":["GOTTBERT","random"],"associatedColumns":["score","En?De","28 . 74 ( +1 . 44 )","-","27 . 37 ( ? 0 . 43 )"],"associatedMergedColumns":["-"]},{"number":"27.80","isBolded":false,"associatedRows":["GOTTBERT","random"],"associatedColumns":["score","En?De","28 . 74 ( +1 . 44 )","-"],"associatedMergedColumns":["-"]},{"number":"33.67","isBolded":false,"associatedRows":["GOTTBERT","random","27 . 85 ( ? 0 . 02 )"],"associatedColumns":["score","De?En","-","36 . 32 ( +2 . 76 )","34 . 26 ( +0 . 25 )"],"associatedMergedColumns":["-"]},{"number":"35.38(+1.71)","isBolded":false,"associatedRows":["GOTTBERT","pre - trained","27 . 85 ( ? 0 . 02 )"],"associatedColumns":["score","De?En","-","36 . 32 ( +2 . 76 )","34 . 26 ( +0 . 25 )"],"associatedMergedColumns":["-"]},{"number":"27.53","isBolded":false,"associatedRows":["BIBERT","random"],"associatedColumns":["score","En?De","28 . 74 ( +1 . 44 )","-","27 . 37 ( ? 0 . 43 )"],"associatedMergedColumns":["-"]},{"number":"33.52","isBolded":false,"associatedRows":["GOTTBERT","random","27 . 85 ( ? 0 . 02 )"],"associatedColumns":["score","De?En","-","36 . 32 ( +2 . 76 )","34 . 26 ( +0 . 25 )"],"associatedMergedColumns":["-"]}]},{"caption":"Table 2: Comparison of our work and most recent ex-\nisting methods on IWSLT\u002714 De?En. \n\n","rows":["BERT - Fuse ( Zhu et al . , 2020 )","DynamicConv ( Wu et al . , 2019 )","Ours , GOTTBERT","Ours , BIBERT","Mixed Representations ( Wu et al . , 2020 )","MAT ( Fan et al . , 2020 )","UniDrop ( Wu et al . , 2021 )"],"columns":[],"mergedAllColumns":[],"numberCells":[{"number":"36.41","isBolded":false,"associatedRows":["Mixed Representations ( Wu et al . , 2020 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"36.32","isBolded":false,"associatedRows":["Ours , GOTTBERT"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"36.11","isBolded":false,"associatedRows":["BERT - Fuse ( Zhu et al . , 2020 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"36.88","isBolded":false,"associatedRows":["UniDrop ( Wu et al . , 2021 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"37.58","isBolded":true,"associatedRows":["Ours , BIBERT"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"35.20","isBolded":false,"associatedRows":["DynamicConv ( Wu et al . , 2019 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"35.40","isBolded":false,"associatedRows":["Mixed Representations ( Wu et al . , 2020 )"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"36.22","isBolded":false,"associatedRows":["MAT ( Fan et al . , 2020 )"],"associatedColumns":[],"associatedMergedColumns":[]}]},{"caption":"Table 3. We first discuss the \nmodels trained without stochastic layer selection. \nThe dual-directional model substantially outper-\nforms the one-way model by obtaining a gain of \n0.52 in En?De and 0.72 in De?En. Moreover, \nfine-tuning on the in-domain data further improves \nBLEU from 29.89 to 30.33 in En?De and from \n37.97 to 38.12 in De?En. Both positive results \nindicated by the dual-directional model and fine-\ntuning approach show their effectiveness in help-\ning translation. A similar discussion holds for the \nmodels with the stochastic layer selection method. \n","rows":["+ Fine - Tuning","One - Way ( vocab size\u003d12K )","Dual - Directional Training"],"columns":["De?En","Compared with our previous models in Section 3","En?De"],"mergedAllColumns":["Stochastic Layer Selection , K \u003d 8 :","En?De and De?En , which respectively obtain","No Stochastic Layer Selection :"],"numberCells":[{"number":"38.61","isBolded":true,"associatedRows":["+ Fine - Tuning"],"associatedColumns":["Compared with our previous models in Section 3","De?En"],"associatedMergedColumns":["Stochastic Layer Selection , K \u003d 8 :"]},{"number":"37.97","isBolded":false,"associatedRows":["Dual - Directional Training"],"associatedColumns":["Compared with our previous models in Section 3","De?En"],"associatedMergedColumns":["No Stochastic Layer Selection :"]},{"number":"37.25","isBolded":false,"associatedRows":["One - Way ( vocab size\u003d12K )"],"associatedColumns":["Compared with our previous models in Section 3","De?En"],"associatedMergedColumns":["No Stochastic Layer Selection :"]},{"number":"37.69","isBolded":false,"associatedRows":["One - Way ( vocab size\u003d12K )"],"associatedColumns":["Compared with our previous models in Section 3","De?En"],"associatedMergedColumns":["Stochastic Layer Selection , K \u003d 8 :"]},{"number":"30.30","isBolded":false,"associatedRows":["Dual - Directional Training"],"associatedColumns":["Compared with our previous models in Section 3","En?De"],"associatedMergedColumns":["Stochastic Layer Selection , K \u003d 8 :"]},{"number":"37.94inDe?En),ourbest","isBolded":false,"associatedRows":["+ Fine - Tuning"],"associatedColumns":["Compared with our previous models in Section 3"],"associatedMergedColumns":[]},{"number":"38.12","isBolded":true,"associatedRows":["+ Fine - Tuning"],"associatedColumns":["Compared with our previous models in Section 3","De?En"],"associatedMergedColumns":["No Stochastic Layer Selection :"]},{"number":"38.37","isBolded":false,"associatedRows":["Dual - Directional Training"],"associatedColumns":["Compared with our previous models in Section 3","De?En"],"associatedMergedColumns":["Stochastic Layer Selection , K \u003d 8 :"]},{"number":"(30.04En?Deand","isBolded":false,"associatedRows":[],"associatedColumns":["Compared with our previous models in Section 3"],"associatedMergedColumns":[]},{"number":"38.61BLEU.","isBolded":true,"associatedRows":[],"associatedColumns":["Compared with our previous models in Section 3"],"associatedMergedColumns":["En?De and De?En , which respectively obtain"]},{"number":"30.00","isBolded":false,"associatedRows":["One - Way ( vocab size\u003d12K )"],"associatedColumns":["Compared with our previous models in Section 3","En?De"],"associatedMergedColumns":["Stochastic Layer Selection , K \u003d 8 :"]},{"number":"30.33","isBolded":true,"associatedRows":["+ Fine - Tuning"],"associatedColumns":["Compared with our previous models in Section 3","En?De"],"associatedMergedColumns":["No Stochastic Layer Selection :"]},{"number":"29.89","isBolded":false,"associatedRows":["Dual - Directional Training"],"associatedColumns":["Compared with our previous models in Section 3","En?De"],"associatedMergedColumns":["No Stochastic Layer Selection :"]},{"number":"29.37","isBolded":false,"associatedRows":["One - Way ( vocab size\u003d12K )"],"associatedColumns":["Compared with our previous models in Section 3","En?De"],"associatedMergedColumns":["No Stochastic Layer Selection :"]},{"number":"30.45and","isBolded":true,"associatedRows":[],"associatedColumns":["Compared with our previous models in Section 3"],"associatedMergedColumns":["En?De and De?En , which respectively obtain"]},{"number":"30.45","isBolded":true,"associatedRows":["+ Fine - Tuning"],"associatedColumns":["Compared with our previous models in Section 3","En?De"],"associatedMergedColumns":["Stochastic Layer Selection , K \u003d 8 :"]}]},{"caption":"Table 3: Comparison of dual-directional and ordi-\nnary (one-way) translation models, with and without \nstochastic layer selection, on IWSLT\u002714 En?De. \n\n","rows":["+ Fine - Tuning","One - Way ( vocab size\u003d12K )","Dual - Directional Training"],"columns":["De?En","Compared with our previous models in Section 3","En?De"],"mergedAllColumns":["Stochastic Layer Selection , K \u003d 8 :","En?De and De?En , which respectively obtain","No Stochastic Layer Selection :"],"numberCells":[{"number":"37.97","isBolded":false,"associatedRows":["Dual - Directional Training"],"associatedColumns":["Compared with our previous models in Section 3","De?En"],"associatedMergedColumns":["No Stochastic Layer Selection :"]},{"number":"30.45and","isBolded":true,"associatedRows":[],"associatedColumns":["Compared with our previous models in Section 3"],"associatedMergedColumns":["En?De and De?En , which respectively obtain"]},{"number":"38.61BLEU.","isBolded":true,"associatedRows":[],"associatedColumns":["Compared with our previous models in Section 3"],"associatedMergedColumns":["En?De and De?En , which respectively obtain"]},{"number":"38.37","isBolded":false,"associatedRows":["Dual - Directional Training"],"associatedColumns":["Compared with our previous models in Section 3","De?En"],"associatedMergedColumns":["Stochastic Layer Selection , K \u003d 8 :"]},{"number":"30.33","isBolded":true,"associatedRows":["+ Fine - Tuning"],"associatedColumns":["Compared with our previous models in Section 3","En?De"],"associatedMergedColumns":["No Stochastic Layer Selection :"]},{"number":"30.00","isBolded":false,"associatedRows":["One - Way ( vocab size\u003d12K )"],"associatedColumns":["Compared with our previous models in Section 3","En?De"],"associatedMergedColumns":["Stochastic Layer Selection , K \u003d 8 :"]},{"number":"37.25","isBolded":false,"associatedRows":["One - Way ( vocab size\u003d12K )"],"associatedColumns":["Compared with our previous models in Section 3","De?En"],"associatedMergedColumns":["No Stochastic Layer Selection :"]},{"number":"38.12","isBolded":true,"associatedRows":["+ Fine - Tuning"],"associatedColumns":["Compared with our previous models in Section 3","De?En"],"associatedMergedColumns":["No Stochastic Layer Selection :"]},{"number":"(30.04En?Deand","isBolded":false,"associatedRows":[],"associatedColumns":["Compared with our previous models in Section 3"],"associatedMergedColumns":[]},{"number":"30.30","isBolded":false,"associatedRows":["Dual - Directional Training"],"associatedColumns":["Compared with our previous models in Section 3","En?De"],"associatedMergedColumns":["Stochastic Layer Selection , K \u003d 8 :"]},{"number":"37.69","isBolded":false,"associatedRows":["One - Way ( vocab size\u003d12K )"],"associatedColumns":["Compared with our previous models in Section 3","De?En"],"associatedMergedColumns":["Stochastic Layer Selection , K \u003d 8 :"]},{"number":"30.45","isBolded":true,"associatedRows":["+ Fine - Tuning"],"associatedColumns":["Compared with our previous models in Section 3","En?De"],"associatedMergedColumns":["Stochastic Layer Selection , K \u003d 8 :"]},{"number":"29.37","isBolded":false,"associatedRows":["One - Way ( vocab size\u003d12K )"],"associatedColumns":["Compared with our previous models in Section 3","En?De"],"associatedMergedColumns":["No Stochastic Layer Selection :"]},{"number":"38.61","isBolded":true,"associatedRows":["+ Fine - Tuning"],"associatedColumns":["Compared with our previous models in Section 3","De?En"],"associatedMergedColumns":["Stochastic Layer Selection , K \u003d 8 :"]},{"number":"37.94inDe?En),ourbest","isBolded":false,"associatedRows":["+ Fine - Tuning"],"associatedColumns":["Compared with our previous models in Section 3"],"associatedMergedColumns":[]},{"number":"29.89","isBolded":false,"associatedRows":["Dual - Directional Training"],"associatedColumns":["Compared with our previous models in Section 3","En?De"],"associatedMergedColumns":["No Stochastic Layer Selection :"]}]},{"caption":"Table 4: WMT\u002714 En?De results on newstest2014 test set. \n\n","rows":["BIBERT Contextualized Embeddings + Stochastic Layer Selection","+ Fine - Tuning","+ Dual - Directional Training"],"columns":["-"],"mergedAllColumns":[],"numberCells":[{"number":"34.54","isBolded":false,"associatedRows":["+ Dual - Directional Training"],"associatedColumns":["-"],"associatedMergedColumns":[]},{"number":"30.31","isBolded":false,"associatedRows":["+ Dual - Directional Training"],"associatedColumns":["-"],"associatedMergedColumns":[]},{"number":"31.26","isBolded":true,"associatedRows":["+ Fine - Tuning"],"associatedColumns":["-"],"associatedMergedColumns":[]},{"number":"30.75","isBolded":false,"associatedRows":["BIBERT Contextualized Embeddings + Stochastic Layer Selection"],"associatedColumns":[],"associatedMergedColumns":[]},{"number":"30.91","isBolded":false,"associatedRows":["BIBERT Contextualized Embeddings + Stochastic Layer Selection"],"associatedColumns":["-"],"associatedMergedColumns":[]},{"number":"34.94","isBolded":true,"associatedRows":["BIBERT Contextualized Embeddings + Stochastic Layer Selection"],"associatedColumns":["-"],"associatedMergedColumns":[]},{"number":"34.68","isBolded":false,"associatedRows":["+ Fine - Tuning"],"associatedColumns":["-"],"associatedMergedColumns":[]}]}]