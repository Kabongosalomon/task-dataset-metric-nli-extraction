[{"caption":"stage \nInput audio waveform \n(Ta ? 1) \n\nInput image sequence \n(Tv ? W ? H) \n\nconv1 \nconv1d, 80, 64, stride 4 \nconv3d, 5 ? 7 2 , 64, stride 1 ? 2 2 \n\nmaxpool, 1 ? 3 2 \n\nres2 \nconv1d, 3, 64 \n\nconv1d, 3, 64 \n\n? 2 \nconv2d, 3 2 , 64 \n\nconv2d, 3 2 , 64 \n\n? 2 \n\nres3 \nconv1d, 3, 128 \n\nconv1d, 3, 128 \n\n? 2 \nconv2d, 3 2 , 128 \n\nconv2d, 3 2 , 128 \n\n? 2 \n\nres4 \nconv1d, 3, 256 \n\nconv1d, 3, 256 \n\n? 2 \nconv2d, 3 2 , 256 \n\nconv2d, 3 2 , 256 \n\n? 2 \n\nres5 \nconv1d, 3, 512 \n\nconv1d, 3, 512 \n\n? 2 \nconv2d, 3 2 , 512 \n\nconv2d, 3 2 , 512 \n\n? 2 \n\npool 6 average pooling, stride 20 \nglobal average pooling \n\nTable 1. The architecture of acoustic and visual Front-end. \nThe dimensions of kernels are denoted by {temporal size ? \nspatial size 2 , channels}. The acoustic model and visual backbones \nhave 3.85 M and 11.18 M parameters, respectively. Ta and Tv denote \nthe number of input samples and frames, respectively. \n\n","rows":["res5","res4","conv3d ,","conv2d ,","res3","res2","pool","conv1d , 80 , 64 , stride","spatial size","conv1d , 3 , 128","?","conv1d , 3 , 512","have","conv1","conv1d , 3 , 256",", 64 , stride","maxpool ,","conv1d , 3 , 64"],"columns":["( Ta ? 1 )","The","Input audio waveform","stage","by","Input image sequence","( Tv ? W ? H )","global average pooling","visual","average pooling , stride 20","dimensions","1 ."],"mergedAllColumns":[", 256",", 64",", 128",", 512",", channels} . The acoustic model and visual backbones"],"numberCells":[{"number":"2","isBolded":false,"associatedRows":["res4","conv1d , 3 , 256","conv2d ,"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[", 256"]},{"number":"2","isBolded":false,"associatedRows":["spatial size","conv1d , 3 , 512","conv2d ,"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[", 512"]},{"number":"2","isBolded":false,"associatedRows":["res2","?","conv2d ,","?"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["conv1","conv1d , 80 , 64 , stride","conv2d ,","maxpool ,","?"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[]},{"number":"5?","isBolded":false,"associatedRows":["conv1","conv1d , 80 , 64 , stride","conv3d ,"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[]},{"number":"1?","isBolded":false,"associatedRows":["conv1","conv1d , 80 , 64 , stride","conv3d ,","conv2d ,",", 64 , stride"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["conv1","conv1d , 80 , 64 , stride","conv3d ,","conv2d ,"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[]},{"number":"3","isBolded":false,"associatedRows":["conv1","conv1d , 80 , 64 , stride","conv2d ,","maxpool ,","?"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[]},{"number":"3","isBolded":false,"associatedRows":["res4","conv1d , 3 , 256","conv2d ,"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[", 128"]},{"number":"2","isBolded":false,"associatedRows":["conv1","conv1d , 3 , 64","conv2d ,"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["res3","conv1d , 3 , 128","conv2d ,"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[", 128"]},{"number":"2","isBolded":false,"associatedRows":["spatial size","conv1d , 3 , 512","conv2d ,"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[", 256"]},{"number":"7","isBolded":false,"associatedRows":["conv1","conv1d , 80 , 64 , stride","conv3d ,","conv2d ,"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["res3","?","conv2d ,","?"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[", 64"]},{"number":"4","isBolded":false,"associatedRows":["conv1","conv1d , 80 , 64 , stride"],"associatedColumns":["Input audio waveform","( Ta ? 1 )"],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["conv1","conv1d , 3 , 64","conv2d ,"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[", 64"]},{"number":"2","isBolded":false,"associatedRows":["conv1","conv1d , 80 , 64 , stride","conv3d ,","conv2d ,",", 64 , stride"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[]},{"number":"3","isBolded":false,"associatedRows":["res4","conv1d , 3 , 256","conv2d ,"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[", 256"]},{"number":"6","isBolded":false,"associatedRows":["pool"],"associatedColumns":["stage","( Ta ? 1 )"],"associatedMergedColumns":[", 512"]},{"number":"2","isBolded":false,"associatedRows":["res4","?"],"associatedColumns":["Input audio waveform","( Ta ? 1 )"],"associatedMergedColumns":[", 128"]},{"number":"3","isBolded":false,"associatedRows":["spatial size","conv1d , 3 , 512","conv2d ,"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[", 512"]},{"number":"11.18Mparameters,respectively.TaandTvdenote","isBolded":true,"associatedRows":["spatial size"],"associatedColumns":["Input image sequence","( Tv ? W ? H )","global average pooling","visual","by"],"associatedMergedColumns":[", channels} . The acoustic model and visual backbones"]},{"number":"3","isBolded":false,"associatedRows":["conv1","conv1d , 3 , 64","conv2d ,"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[]},{"number":"3","isBolded":false,"associatedRows":["res3","conv1d , 3 , 128","conv2d ,"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[", 64"]},{"number":"3","isBolded":false,"associatedRows":["res3","conv1d , 3 , 128","conv2d ,"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[", 128"]},{"number":"2","isBolded":false,"associatedRows":["spatial size"],"associatedColumns":["Input audio waveform","( Ta ? 1 )","average pooling , stride 20","The","dimensions"],"associatedMergedColumns":[", 512"]},{"number":"2","isBolded":false,"associatedRows":["res4","conv1d , 3 , 256","conv2d ,"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[", 128"]},{"number":"2","isBolded":false,"associatedRows":["res5","?","conv2d ,","?"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[", 256"]},{"number":"2","isBolded":false,"associatedRows":["res3","conv1d , 3 , 128","conv2d ,"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[", 64"]},{"number":"3","isBolded":false,"associatedRows":["spatial size","conv1d , 3 , 512","conv2d ,"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[", 256"]},{"number":"2","isBolded":false,"associatedRows":["conv1","conv1d , 80 , 64 , stride","conv3d ,","conv2d ,",", 64 , stride"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[]},{"number":"2","isBolded":false,"associatedRows":["res5","?"],"associatedColumns":["Input audio waveform","( Ta ? 1 )"],"associatedMergedColumns":[", 256"]},{"number":"2","isBolded":false,"associatedRows":["res2","?"],"associatedColumns":["Input audio waveform","( Ta ? 1 )"],"associatedMergedColumns":[]},{"number":"3.85Mand","isBolded":true,"associatedRows":["have"],"associatedColumns":["stage","( Ta ? 1 )","average pooling , stride 20","1 .","dimensions"],"associatedMergedColumns":[", channels} . The acoustic model and visual backbones"]},{"number":"2","isBolded":false,"associatedRows":["res4","?","conv2d ,","?"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[", 128"]},{"number":"1?","isBolded":false,"associatedRows":["conv1","conv1d , 80 , 64 , stride","conv2d ,","maxpool ,"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[]},{"number":"3","isBolded":false,"associatedRows":["conv1","conv1d , 3 , 64","conv2d ,"],"associatedColumns":["Input image sequence","( Tv ? W ? H )"],"associatedMergedColumns":[", 64"]},{"number":"2","isBolded":false,"associatedRows":["res3","?"],"associatedColumns":["Input audio waveform","( Ta ? 1 )"],"associatedMergedColumns":[", 64"]}]},{"caption":"Table 2. Ablation study on visual speech recognition performance \non LRS2. \n\n","rows":["+ Conformer encoder","0","+ Transformer LM","- 5","Word","5","+ LRW pre - training","Baseline [ 24 ]","+ E2E"],"columns":["WER","AV","V","log - Mel fbank","Method"],"mergedAllColumns":["Error"],"numberCells":[{"number":"20","isBolded":false,"associatedRows":["0","- 5","0","5"],"associatedColumns":["WER","log - Mel fbank"],"associatedMergedColumns":["Error"]},{"number":"42.4","isBolded":true,"associatedRows":["Baseline [ 24 ]","+ Conformer encoder"],"associatedColumns":["WER"],"associatedMergedColumns":[]},{"number":"63.5","isBolded":true,"associatedRows":["Baseline [ 24 ]","+ Conformer encoder"],"associatedColumns":["WER"],"associatedMergedColumns":[]},{"number":"40","isBolded":false,"associatedRows":[],"associatedColumns":["Method","V"],"associatedMergedColumns":[]},{"number":"20","isBolded":false,"associatedRows":["Word"],"associatedColumns":["Method","V"],"associatedMergedColumns":["Error"]},{"number":"37.9","isBolded":true,"associatedRows":["Baseline [ 24 ]","+ Transformer LM"],"associatedColumns":["WER"],"associatedMergedColumns":[]},{"number":"50.9","isBolded":true,"associatedRows":["+ E2E","+ Conformer encoder"],"associatedColumns":["WER"],"associatedMergedColumns":[]},{"number":"15","isBolded":false,"associatedRows":["0","- 5","0","5"],"associatedColumns":["WER","log - Mel fbank"],"associatedMergedColumns":["Error"]},{"number":"10","isBolded":false,"associatedRows":[],"associatedColumns":["Method","V"],"associatedMergedColumns":["Error"]},{"number":"50","isBolded":false,"associatedRows":[],"associatedColumns":["Method"],"associatedMergedColumns":[]},{"number":"10","isBolded":false,"associatedRows":["0","- 5","0","5"],"associatedColumns":["WER","AV"],"associatedMergedColumns":["Error"]},{"number":"46.2","isBolded":true,"associatedRows":["Baseline [ 24 ]","+ LRW pre - training"],"associatedColumns":["WER"],"associatedMergedColumns":[]},{"number":"30","isBolded":false,"associatedRows":[],"associatedColumns":["Method","V"],"associatedMergedColumns":[]}]},{"caption":"Training Data (Hours) \nWER \n\nVisual-only (?) \n\nMV-WAS [5] \nLRS2 (224) \n70.4 \n\nLIBS [37] \nMVLRS (730) + LRS2 (224) \n65.3 \n\nCTC/Attention [25] \nLRW (157) + LRS2 (224) \n63.5 \n\nConv-seq2seq [36] \nLRW (157) + LRS2\u00263 v0.0 (698) \n51.7 \n\nKD + CTC [2] \nVC2 clean (334) + LRS2\u00263 v0.4 (632) 51.3 \n\nTDNN [34] \nLRS2 (224) \n48.9 \n\nTM-seq2seq [1] \nMVLRS (730) + LRS2\u00263 v0.4 (632) 48.3 \n\nOurs (V) \nLRS2 (224) \n39.1 \n\nOurs (V) \nLRW (157) + LRS2 (224) \n37.9 \n\nAudio-only (?) \n\nTM-seq2seq [1] \nMVLRS (730) + LRS2\u00263 v0.4 (632) 9.7 \n\nCTC/Attention [25] \nLRS2 (224) \n8.3 \n\nCTC/Attention [16] \nLibriSpeech (960) + LRS2 (224) \n8.2 \n\nTDNN [34] \nLRS2 (224) \n6.7 \n\nOurs (filter-bank) \nLRS2 (224) \n4.3 \n\nOurs (raw A) \nLRS2 (224) \n4.3 \n\nOurs (raw A) \nLRW (157) + LRS2 (224) \n3.9 \n\nAudio-visual (?) \n\nTM-seq2seq [1] \nMVLRS (730) + LRS2\u00263 v0.4 (632) 8.5 \n\nCTC/Attention [25] \nLRW (157) + LRS2 (224) \n7.0 \n\nTDNN [34] \nLRS2 (224) \n5.9 \n\nOurs (raw A + V) \nLRS2 (224) \n4.2 \n\nOurs (raw A + V) \nLRW (157) + LRS2 (224) \n3.7 \n\nTable 3. Word Error Rate (WER) of the audio-only, visual-only and \naudio-visual models on LRS2. VC2 clean denotes the filtered version \nof VoxCeleb2. LRS2\u00263 consists of LRS2 and LRS3. LRS3 v0.4 is \nthe updated version of LRS3 with speaker-independent settings. \n\n","rows":["V2P [ 27 ]","( 632 )","EG - seq2seq [ 33 ]","VC2 clean ( 334 ) + LRS3","RNN - T [ 19 ]","LRW ( 157 ) +LRS3","LRW ( 157 ) + LRS3","( 698 )","KD + CTC [ 2 ]","Ours ( V )","of VoxCeleb2 .","LRW ( 157 ) + LRS2\u00263","Conv - seq2seq [ 36 ]","Ours ( raw A )","TM - seq2seq [ 1 ]","( 438 )","LRS2\u00263 consists of LRS2 and LRS3 .","LRS3","( 474 )","YT ( 31 000 )","The best visual - only model has a WER of","YT ( 3 886 )","Ours ( raw A + V )","Results on LRS3","MVLRS ( 730 ) + LRS2\u00263","Ours ( filter - bank )"],"columns":["audio - visual models on LRS3 . VC2","denotes the filtered version","WER","model .","Results on LRS3","are reported in Table 4 .","The gap between raw audio - only and audio - visual models","Method","Training Data ( Hours )"],"mergedAllColumns":["Audio - only ( ? )","Audio - visual ( ? )","clean","experiments , our model pushes the state - of - the - art performance to","Visual - only ( ? )","audio modality is heavily corrupted by background noise ."],"numberCells":[{"number":"v0.4","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Ours ( raw A )","LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"2.3","isBolded":true,"associatedRows":["The best visual - only model has a WER of","Ours ( filter - bank )","LRS3","( 438 )","( 438 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"v0.4","isBolded":false,"associatedRows":["The best visual - only model has a WER of","TM - seq2seq [ 1 ]","MVLRS ( 730 ) + LRS2\u00263"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"2.3","isBolded":true,"associatedRows":["The best visual - only model has a WER of","Ours ( raw A )","LRS3","( 438 )","( 438 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"v0.4","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Ours ( raw A + V )","LRW ( 157 ) + LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Audio - visual ( ? )"]},{"number":"57.8","isBolded":false,"associatedRows":["The best visual - only model has a WER of","EG - seq2seq [ 33 ]","LRW ( 157 ) + LRS3","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"2.3","isBolded":true,"associatedRows":["The best visual - only model has a WER of","Ours ( raw A + V )","LRW ( 157 ) + LRS3","( 438 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - visual ( ? )"]},{"number":"7.2","isBolded":false,"associatedRows":["The best visual - only model has a WER of","TM - seq2seq [ 1 ]","MVLRS ( 730 ) + LRS2\u00263","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - visual ( ? )"]},{"number":"v0.4","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Ours ( filter - bank )","LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"43.3%.Weobservethat","isBolded":false,"associatedRows":["The best visual - only model has a WER of"],"associatedColumns":["Method","audio - visual models on LRS3 . VC2","The gap between raw audio - only and audio - visual models","are reported in Table 4 ."],"associatedMergedColumns":["audio modality is heavily corrupted by background noise ."]},{"number":"v0.0","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Ours ( V )","LRW ( 157 ) + LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"58.9","isBolded":false,"associatedRows":["The best visual - only model has a WER of","TM - seq2seq [ 1 ]","MVLRS ( 730 ) + LRS2\u00263","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"8.3","isBolded":false,"associatedRows":["The best visual - only model has a WER of","TM - seq2seq [ 1 ]","MVLRS ( 730 ) + LRS2\u00263","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"7.2","isBolded":false,"associatedRows":["The best visual - only model has a WER of","EG - seq2seq [ 33 ]","LRS3","( 474 )","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"v0.4","isBolded":false,"associatedRows":["The best visual - only model has a WER of","of VoxCeleb2 .","LRS2\u00263 consists of LRS2 and LRS3 .","( 438 )","( 632 )","LRS3"],"associatedColumns":["WER","denotes the filtered version"],"associatedMergedColumns":["clean"]},{"number":"2.5%and","isBolded":false,"associatedRows":["The best visual - only model has a WER of"],"associatedColumns":["Method","audio - visual models on LRS3 . VC2","The gap between raw audio - only and audio - visual models","are reported in Table 4 ."],"associatedMergedColumns":["experiments , our model pushes the state - of - the - art performance to"]},{"number":"60.1","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Conv - seq2seq [ 36 ]","LRW ( 157 ) + LRS2\u00263","( 474 )","( 698 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"v0.4","isBolded":false,"associatedRows":["The best visual - only model has a WER of","TM - seq2seq [ 1 ]","MVLRS ( 730 ) + LRS2\u00263"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"1.3","isBolded":true,"associatedRows":["The best visual - only model has a WER of","Ours ( raw A )","LRW ( 157 ) +LRS3","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"v0.0","isBolded":false,"associatedRows":["The best visual - only model has a WER of","EG - seq2seq [ 33 ]","LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"v0.0","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Ours ( raw A )","LRW ( 157 ) +LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"43.3","isBolded":true,"associatedRows":["The best visual - only model has a WER of","Ours ( V )","LRW ( 157 ) + LRS3","( 438 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"30.4","isBolded":true,"associatedRows":["The best visual - only model has a WER of","Ours ( V )","LRW ( 157 ) + LRS3","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"2.3","isBolded":true,"associatedRows":["The best visual - only model has a WER of","Ours ( raw A )","LRW ( 157 ) +LRS3","( 438 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"v0.4","isBolded":false,"associatedRows":["The best visual - only model has a WER of","TM - seq2seq [ 1 ]","MVLRS ( 730 ) + LRS2\u00263"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Audio - visual ( ? )"]},{"number":"v0.0","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Ours ( raw A + V )","LRW ( 157 ) + LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Audio - visual ( ? )"]},{"number":"4.5","isBolded":false,"associatedRows":["The best visual - only model has a WER of","RNN - T [ 19 ]","YT ( 31 000 )","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - visual ( ? )"]},{"number":"33.6","isBolded":false,"associatedRows":["The best visual - only model has a WER of","RNN - T [ 19 ]","YT ( 31 000 )","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"46.9","isBolded":true,"associatedRows":["The best visual - only model has a WER of","Ours ( V )","LRS3","( 438 )","( 438 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"2.3%,respectively,outperforming[19]by","isBolded":false,"associatedRows":[],"associatedColumns":["Method","audio - visual models on LRS3 . VC2","The gap between raw audio - only and audio - visual models","Results on LRS3"],"associatedMergedColumns":["experiments , our model pushes the state - of - the - art performance to"]},{"number":"v0.4","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Ours ( V )","LRW ( 157 ) + LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"2.2%,respectively.Itisworthpointingoutthatourmodelistrained","isBolded":false,"associatedRows":[],"associatedColumns":["Method","audio - visual models on LRS3 . VC2","The gap between raw audio - only and audio - visual models","Results on LRS3"],"associatedMergedColumns":["experiments , our model pushes the state - of - the - art performance to"]},{"number":"v0.0","isBolded":false,"associatedRows":["The best visual - only model has a WER of","EG - seq2seq [ 33 ]","LRW ( 157 ) + LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"55.1","isBolded":false,"associatedRows":["The best visual - only model has a WER of","V2P [ 27 ]","YT ( 3 886 )","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"6.8","isBolded":false,"associatedRows":["The best visual - only model has a WER of","EG - seq2seq [ 33 ]","LRW ( 157 ) + LRS3","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - visual ( ? )"]},{"number":"59.8","isBolded":false,"associatedRows":["The best visual - only model has a WER of","KD + CTC [ 2 ]","VC2 clean ( 334 ) + LRS3","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"v0.4","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Ours ( raw A )","LRW ( 157 ) +LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"1.2","isBolded":true,"associatedRows":["The best visual - only model has a WER of","Ours ( raw A + V )","LRW ( 157 ) + LRS3","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - visual ( ? )"]},{"number":"v0.0","isBolded":false,"associatedRows":["The best visual - only model has a WER of","EG - seq2seq [ 33 ]","LRW ( 157 ) + LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Audio - visual ( ? )"]},{"number":"v0.4","isBolded":true,"associatedRows":["The best visual - only model has a WER of","KD + CTC [ 2 ]","VC2 clean ( 334 ) + LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"2.3%and","isBolded":false,"associatedRows":[],"associatedColumns":["Method","audio - visual models on LRS3 . VC2","model .","Results on LRS3"],"associatedMergedColumns":["experiments , our model pushes the state - of - the - art performance to"]},{"number":"v0.4","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Ours ( V )","LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"v0.4","isBolded":true,"associatedRows":["Results on LRS3"],"associatedColumns":["Method","audio - visual models on LRS3 . VC2","The gap between raw audio - only and audio - visual models"],"associatedMergedColumns":["audio modality is heavily corrupted by background noise ."]},{"number":"4.8","isBolded":false,"associatedRows":["The best visual - only model has a WER of","RNN - T [ 19 ]","YT ( 31 000 )","( 438 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"v0.0","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Conv - seq2seq [ 36 ]","LRW ( 157 ) + LRS2\u00263"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Visual - only ( ? )"]}]},{"caption":"The results are shown in Fig 2. Note that both audio-only and audio-\nvisual models are augmented with noise injection. It is clear that the \naudio-visual model achieves better performance than the audio-only \nmodel. The gap between raw audio-only and audio-visual models \nbecomes larger by the presence of high level of noise. This demon-\nstrates that the audio-visual model is particularly beneficial when the \naudio modality is heavily corrupted by background noise. \nResults on LRS3 Results on LRS3 v0.4 are reported in Table 4. \nThe best visual-only model has a WER of 43.3 %. We observe that \nour visual-only model outperforms other methods by a large margin \nwhile using fewer training data. For the audio-only and audio-visual \nexperiments, our model pushes the state-of-the-art performance to \n2.3 % and 2.3 %, respectively, outperforming [19] by 2.5 % and \n2.2 %, respectively. It is worth pointing out that our model is trained \non a dataset which is 52? smaller than [19], 595 vs 31000 hours. \nWe should note that some works use the old version of LRS3 \n(denoted as v0.0), where some speakers appear both in the training \nand test sets. For fair comparisons, we also report the performance \nof audio-only, visual-only, and audio-visual model on this version of \nLRS3 as well. Specifically, the audio-only model achieves a WER \n\nMethod \nTraining Data (Hours) \nWER \n\nVisual-only (?) \n\nConv-seq2seq [36] \nLRW (157) + LRS2\u00263 v0.0 (698) \n60.1 \n\nKD + CTC [2] \nVC2 clean (334) + LRS3 v0.4 (438) \n59.8 \n\nTM-seq2seq [1] \nMVLRS (730) + LRS2\u00263 v0.4 (632) 58.9 \n\nEG-seq2seq [33] \nLRW (157) + LRS3 v0.0 (474) \n57.8 \n\nV2P [27] \nYT (3 886) \n55.1 \n\nRNN-T [19] \nYT (31 000) \n33.6 \n\nOurs (V) \nLRS3 v0.4 (438) \n46.9 \n\nOurs (V) \nLRW (157) + LRS3 v0.4 (438) \n43.3 \n\nOurs (V) \nLRW (157) + LRS3 v0.0 (474) \n30.4 \n\nAudio-only (?) \n\nTM-seq2seq [1] \nMVLRS (730) + LRS2\u00263 v0.4 (632) 8.3 \n\nEG-seq2seq [33] \nLRS3 v0.0 (474) \n7.2 \n\nRNN-T [19] \nYT (31 000) \n4.8 \n\nOurs (filter-bank) \nLRS3 v0.4 (438) \n2.3 \n\nOurs (raw A) \nLRS3 v0.4 (438) \n2.3 \n\nOurs (raw A) \nLRW (157) +LRS3 v0.4 (438) \n2.3 \n\nOurs (raw A) \nLRW (157) +LRS3 v0.0 (474) \n1.3 \n\nAudio-visual (?) \n\nTM-seq2seq [1] \nMVLRS (730) + LRS2\u00263 v0.4 (632) 7.2 \n\nEG-seq2seq [33] \nLRW (157) + LRS3 v0.0 (474) \n6.8 \n\nRNN-T [19] \nYT (31 000) \n4.5 \n\nOurs (raw A + V) \nLRW (157) + LRS3 v0.4 (438) \n2.3 \n\nOurs (raw A + V) \nLRW (157) + LRS3 v0.0 (474) \n1.2 \n\nTable 4. Word Error Rate (WER) of the audio-only, visual-only and \naudio-visual models on LRS3. VC2 clean denotes the filtered version \nof VoxCeleb2. LRS2\u00263 consists of LRS2 and LRS3. LRS3 v0.4 is \nthe updated version of LRS3 with speaker-independent settings. \n\n","rows":["V2P [ 27 ]","( 632 )","EG - seq2seq [ 33 ]","VC2 clean ( 334 ) + LRS3","RNN - T [ 19 ]","LRW ( 157 ) +LRS3","LRW ( 157 ) + LRS3","( 698 )","KD + CTC [ 2 ]","Ours ( V )","of VoxCeleb2 .","LRW ( 157 ) + LRS2\u00263","Conv - seq2seq [ 36 ]","Ours ( raw A )","TM - seq2seq [ 1 ]","( 438 )","LRS2\u00263 consists of LRS2 and LRS3 .","LRS3","( 474 )","YT ( 31 000 )","The best visual - only model has a WER of","YT ( 3 886 )","Ours ( raw A + V )","Results on LRS3","MVLRS ( 730 ) + LRS2\u00263","Ours ( filter - bank )"],"columns":["audio - visual models on LRS3 . VC2","denotes the filtered version","WER","model .","Results on LRS3","are reported in Table 4 .","The gap between raw audio - only and audio - visual models","Method","Training Data ( Hours )"],"mergedAllColumns":["Audio - only ( ? )","Audio - visual ( ? )","clean","experiments , our model pushes the state - of - the - art performance to","Visual - only ( ? )","audio modality is heavily corrupted by background noise ."],"numberCells":[{"number":"4.8","isBolded":false,"associatedRows":["The best visual - only model has a WER of","RNN - T [ 19 ]","YT ( 31 000 )","( 438 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"v0.4","isBolded":false,"associatedRows":["The best visual - only model has a WER of","of VoxCeleb2 .","LRS2\u00263 consists of LRS2 and LRS3 .","( 438 )","( 632 )","LRS3"],"associatedColumns":["WER","denotes the filtered version"],"associatedMergedColumns":["clean"]},{"number":"v0.4","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Ours ( V )","LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"v0.4","isBolded":false,"associatedRows":["The best visual - only model has a WER of","TM - seq2seq [ 1 ]","MVLRS ( 730 ) + LRS2\u00263"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Audio - visual ( ? )"]},{"number":"8.3","isBolded":false,"associatedRows":["The best visual - only model has a WER of","TM - seq2seq [ 1 ]","MVLRS ( 730 ) + LRS2\u00263","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"1.3","isBolded":true,"associatedRows":["The best visual - only model has a WER of","Ours ( raw A )","LRW ( 157 ) +LRS3","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"v0.4","isBolded":false,"associatedRows":["The best visual - only model has a WER of","TM - seq2seq [ 1 ]","MVLRS ( 730 ) + LRS2\u00263"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"v0.0","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Conv - seq2seq [ 36 ]","LRW ( 157 ) + LRS2\u00263"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"4.5","isBolded":false,"associatedRows":["The best visual - only model has a WER of","RNN - T [ 19 ]","YT ( 31 000 )","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - visual ( ? )"]},{"number":"6.8","isBolded":false,"associatedRows":["The best visual - only model has a WER of","EG - seq2seq [ 33 ]","LRW ( 157 ) + LRS3","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - visual ( ? )"]},{"number":"v0.4","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Ours ( filter - bank )","LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"2.3%and","isBolded":false,"associatedRows":[],"associatedColumns":["Method","audio - visual models on LRS3 . VC2","model .","Results on LRS3"],"associatedMergedColumns":["experiments , our model pushes the state - of - the - art performance to"]},{"number":"55.1","isBolded":false,"associatedRows":["The best visual - only model has a WER of","V2P [ 27 ]","YT ( 3 886 )","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"30.4","isBolded":true,"associatedRows":["The best visual - only model has a WER of","Ours ( V )","LRW ( 157 ) + LRS3","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"59.8","isBolded":false,"associatedRows":["The best visual - only model has a WER of","KD + CTC [ 2 ]","VC2 clean ( 334 ) + LRS3","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"v0.0","isBolded":false,"associatedRows":["The best visual - only model has a WER of","EG - seq2seq [ 33 ]","LRW ( 157 ) + LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"v0.0","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Ours ( V )","LRW ( 157 ) + LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"v0.0","isBolded":false,"associatedRows":["The best visual - only model has a WER of","EG - seq2seq [ 33 ]","LRW ( 157 ) + LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Audio - visual ( ? )"]},{"number":"58.9","isBolded":false,"associatedRows":["The best visual - only model has a WER of","TM - seq2seq [ 1 ]","MVLRS ( 730 ) + LRS2\u00263","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"v0.4","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Ours ( V )","LRW ( 157 ) + LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"v0.0","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Ours ( raw A )","LRW ( 157 ) +LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"57.8","isBolded":false,"associatedRows":["The best visual - only model has a WER of","EG - seq2seq [ 33 ]","LRW ( 157 ) + LRS3","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"43.3%.Weobservethat","isBolded":false,"associatedRows":["The best visual - only model has a WER of"],"associatedColumns":["Method","audio - visual models on LRS3 . VC2","The gap between raw audio - only and audio - visual models","are reported in Table 4 ."],"associatedMergedColumns":["audio modality is heavily corrupted by background noise ."]},{"number":"43.3","isBolded":true,"associatedRows":["The best visual - only model has a WER of","Ours ( V )","LRW ( 157 ) + LRS3","( 438 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"v0.4","isBolded":false,"associatedRows":["The best visual - only model has a WER of","TM - seq2seq [ 1 ]","MVLRS ( 730 ) + LRS2\u00263"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"7.2","isBolded":false,"associatedRows":["The best visual - only model has a WER of","EG - seq2seq [ 33 ]","LRS3","( 474 )","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"46.9","isBolded":true,"associatedRows":["The best visual - only model has a WER of","Ours ( V )","LRS3","( 438 )","( 438 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"v0.4","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Ours ( raw A )","LRW ( 157 ) +LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"v0.4","isBolded":true,"associatedRows":["The best visual - only model has a WER of","KD + CTC [ 2 ]","VC2 clean ( 334 ) + LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"v0.0","isBolded":false,"associatedRows":["The best visual - only model has a WER of","EG - seq2seq [ 33 ]","LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"33.6","isBolded":false,"associatedRows":["The best visual - only model has a WER of","RNN - T [ 19 ]","YT ( 31 000 )","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"2.3","isBolded":true,"associatedRows":["The best visual - only model has a WER of","Ours ( raw A )","LRW ( 157 ) +LRS3","( 438 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"60.1","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Conv - seq2seq [ 36 ]","LRW ( 157 ) + LRS2\u00263","( 474 )","( 698 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Visual - only ( ? )"]},{"number":"v0.4","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Ours ( raw A )","LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"2.3%,respectively,outperforming[19]by","isBolded":false,"associatedRows":[],"associatedColumns":["Method","audio - visual models on LRS3 . VC2","The gap between raw audio - only and audio - visual models","Results on LRS3"],"associatedMergedColumns":["experiments , our model pushes the state - of - the - art performance to"]},{"number":"7.2","isBolded":false,"associatedRows":["The best visual - only model has a WER of","TM - seq2seq [ 1 ]","MVLRS ( 730 ) + LRS2\u00263","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - visual ( ? )"]},{"number":"v0.0","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Ours ( raw A + V )","LRW ( 157 ) + LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Audio - visual ( ? )"]},{"number":"2.2%,respectively.Itisworthpointingoutthatourmodelistrained","isBolded":false,"associatedRows":[],"associatedColumns":["Method","audio - visual models on LRS3 . VC2","The gap between raw audio - only and audio - visual models","Results on LRS3"],"associatedMergedColumns":["experiments , our model pushes the state - of - the - art performance to"]},{"number":"v0.4","isBolded":true,"associatedRows":["Results on LRS3"],"associatedColumns":["Method","audio - visual models on LRS3 . VC2","The gap between raw audio - only and audio - visual models"],"associatedMergedColumns":["audio modality is heavily corrupted by background noise ."]},{"number":"2.5%and","isBolded":false,"associatedRows":["The best visual - only model has a WER of"],"associatedColumns":["Method","audio - visual models on LRS3 . VC2","The gap between raw audio - only and audio - visual models","are reported in Table 4 ."],"associatedMergedColumns":["experiments , our model pushes the state - of - the - art performance to"]},{"number":"2.3","isBolded":true,"associatedRows":["The best visual - only model has a WER of","Ours ( raw A )","LRS3","( 438 )","( 438 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - only ( ? )"]},{"number":"v0.4","isBolded":false,"associatedRows":["The best visual - only model has a WER of","Ours ( raw A + V )","LRW ( 157 ) + LRS3"],"associatedColumns":["Training Data ( Hours )"],"associatedMergedColumns":["Audio - visual ( ? )"]},{"number":"2.3","isBolded":true,"associatedRows":["The best visual - only model has a WER of","Ours ( raw A + V )","LRW ( 157 ) + LRS3","( 438 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - visual ( ? )"]},{"number":"1.2","isBolded":true,"associatedRows":["The best visual - only model has a WER of","Ours ( raw A + V )","LRW ( 157 ) + LRS3","( 474 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - visual ( ? )"]},{"number":"2.3","isBolded":true,"associatedRows":["The best visual - only model has a WER of","Ours ( filter - bank )","LRS3","( 438 )","( 438 )","( 438 )","( 632 )"],"associatedColumns":["WER"],"associatedMergedColumns":["Audio - only ( ? )"]}]}]