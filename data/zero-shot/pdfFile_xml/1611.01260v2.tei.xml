<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEARNING IDENTITY MAPPINGS WITH RESIDUAL GATES</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">H P</forename><surname>Savarese</surname></persName>
							<email>savarese@land.ufrj.br</email>
							<affiliation key="aff0">
								<orgName type="department">COPPE/PESC Federal</orgName>
								<orgName type="institution">University of Rio de Janeiro Rio de Janeiro</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><forename type="middle">O</forename><surname>Mazza</surname></persName>
							<email>leonardomazza@poli.ufrj.br</email>
							<affiliation key="aff1">
								<orgName type="institution">Poli Federal University of Rio</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">R</forename><surname>Figueiredo</surname></persName>
							<email>daniel@land.ufrj.br</email>
							<affiliation key="aff2">
								<orgName type="department">COPPE/PESC Federal</orgName>
								<orgName type="institution">University of Rio</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LEARNING IDENTITY MAPPINGS WITH RESIDUAL GATES</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Under review as a conference paper at ICLR 2017</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a new layer design by adding a linear gating mechanism to shortcut connections. By using a scalar parameter to control each gate, we provide a way to learn identity mappings by optimizing only one parameter. We build upon the motivation behind Residual Networks, where a layer is reformulated in order to make learning identity mappings less problematic to the optimizer. The augmentation introduces only one extra parameter per layer, and provides easier optimization by making degeneration into identity mappings simpler. We propose a new model, the Gated Residual Network, which is the result when augmenting Residual Networks. Experimental results show that augmenting layers provides better optimization, increased performance, and more layer independence. We evaluate our method on MNIST using fully-connected networks, showing empirical indications that our augmentation facilitates the optimization of deep models, and that it provides high tolerance to full layer removal: the model retains over 90% of its performance even after half of its layers have been randomly removed. We also evaluate our model on CIFAR-10 and CIFAR-100 using Wide Gated ResNets, achieving 3.65% and 18.27% error, respectively.</p><p>Recently, models such as Residual Networks <ref type="bibr" target="#b9">(He et al. (2015b)</ref>) and Highway Neural Networks <ref type="bibr" target="#b22">(Srivastava et al. (2015)</ref>) permitted the design of networks with hundreds of layers. A key idea of these models is to allow for information to flow more freely through the layers, by using shortcut connections between the layer's input and output. This layer design greatly facilitates training, due to shorter paths between the lower layers and the network's error function. In particular, these models can more easily learn identity mappings in the layers, thus allowing the network to be deeper and learn more abstract representations (?). Such networks have been highly successful in many computer vision tasks.</p><p>On the theoretical side, it is suggested that depth contributes exponentially more to the representational capacity of networks than width <ref type="bibr" target="#b5">(Eldan &amp; Shamir (2015)</ref> Telgarsky <ref type="formula">(2016)</ref> Bianchini &amp; 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>As the number of layers of neural networks increase, effectively training its parameters becomes a fundamental problem <ref type="bibr" target="#b15">(Larochelle et al. (2009)</ref>). Many obstacles challenge the training of neural networks, including vanishing/exploding gradients <ref type="bibr" target="#b0">(Bengio et al. (1994)</ref>), saturating activation functions <ref type="bibr" target="#b25">(Xu et al. (2016)</ref>) and poor weight initialization <ref type="bibr" target="#b6">(Glorot &amp; Bengio (2010)</ref>). Techniques such as unsupervised pre-training <ref type="bibr" target="#b1">(Bengio et al. (2007)</ref>), non-saturating activation functions <ref type="bibr" target="#b19">(Nair &amp; Hinton (2010)</ref>) and normalization <ref type="bibr" target="#b12">(Ioffe &amp; Szegedy (2015)</ref>) target these issues and enable the training of deeper networks. However, stacking more than a dozen layers still lead to a hard to train model. <ref type="bibr" target="#b2">Scarselli (2014)</ref>  <ref type="bibr" target="#b18">Mont?far et al. (2014)</ref>). This agrees with the increasing depth of winning architectures on challenges such as ImageNet <ref type="bibr" target="#b9">(He et al. (2015b)</ref>  <ref type="bibr" target="#b23">Szegedy et al. (2014)</ref>).</p><p>Increasing the depth of networks significantly increases its representational capacity and consequently its performance, an observation supported by theory <ref type="bibr" target="#b5">(Eldan &amp; Shamir (2015)</ref>  <ref type="bibr" target="#b24">Telgarsky (2016)</ref>  <ref type="bibr" target="#b2">Bianchini &amp; Scarselli (2014)</ref>  <ref type="bibr" target="#b18">Mont?far et al. (2014)</ref>) and practice <ref type="bibr" target="#b9">(He et al. (2015b)</ref>  <ref type="bibr" target="#b23">Szegedy et al. (2014)</ref>). Moreover, <ref type="bibr" target="#b9">He et al. (2015b)</ref> showed that, by construction, one can increase a network's depth while preserving its performance. These two observations suggest that it suffices to stack more layers to a network in order to increase its performance. However, this behavior is not observed in practice even with recently proposed models, in part due to the challenge of training ever deeper networks.</p><p>In this work we aim to improve the training of deep networks by proposing a layer design that builds on Residual Networks and Highway Neural Networks. The key idea is to facilitate the learning of identity mappings by introducing a gating mechanism to the shortcut connection, as illustrated in <ref type="figure">Figure 1</ref>. Note that the shortcut connection is controlled by a gate that is parameterized with a scalar, k. This is a key difference from Highway Networks, where a tensor is used to regulate the shortcut connection, along with the incoming data. The idea of using a scalar is simple: it is easier to learn k = 0 than to learn W g = 0 for a weight tensor W g controlling the gate. Indeed, this single scalar allows for stronger supervision on lower layers, by making gradients flow more smoothly in the optimization. <ref type="figure">Figure 1</ref>: Gating mechanism applied to the shortcut connection of a layer. The key difference with Highway Networks is that only a scalar k is used to regulate the gates instead of a tensor.</p><formula xml:id="formula_0">x ) , ( W x f u ? ? ) ( k g ? ? 1 x ) , ( W x f u</formula><p>We apply our proposed network design to Residual Networks, as illustrated in <ref type="figure">Figure 2</ref>. Note that in this case the layer becomes simply u = g(k)f r (x, W ) + x, where f r denotes the layer's residual function. Thus, the shortcut connection allows the input to flow freely without any interference of g(k) through the layer. We will call this model Gated Residual Network, or GResNets. Again, note that learning identity mappings is again much easier in comparison to the original ResNets.</p><p>Note that layers that degenerated into identity mappings have no impact in the signal propagating through the network, and thus can be removed without affecting performance. The removal of such layers can be seen as a transposed application of sparse encoding <ref type="bibr" target="#b7">(Glorot et al. (2011)</ref>): transposing the sparsity from neurons to layers provides a form to prune them entirely from the network. Indeed, we show that performance decays slowly in GResNets when layers are removed, when compared to ResNets.</p><p>We evaluate the performance of the proposed design in two experiments. First, we evaluate fullyconnected GResNets on MNIST and compare it with fully-connected ResNets, showing superior performance and robustness to layer removal. Second, we apply our model to Wide ResNets <ref type="bibr" target="#b26">(Zagoruyko &amp; Komodakis (2016)</ref>) and test its performance on CIFAR, obtaining results that are superior to all previously published results (to the best of our knowledge). These findings indicate <ref type="figure">Figure 2</ref>: Proposed network design applied to Residual Networks. Note that the joint network design results in a shortcut path where the input remains unchanged. In this case, g(k) can be interpreted as an amplifier or suppressor for the residual f r (x, W ).</p><formula xml:id="formula_1">x ) , ( W x fr u ? ? ) ( k g x ) , ( W x fr u ? ? ) ( k g ? ? 1 ? ) , ( W x f</formula><p>that learning identity mappings is a fundamental aspect of learning in deep networks, and designing models where this is easier seems highly effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">AUGMENTATION WITH RESIDUAL GATES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">THEORETICAL INTUITION</head><p>Recall that a network's depth can always be increased without affecting its performance -it suffices to add layers that perform identity mappings. Consider a classic fully-connected ReLU network with layers defined as u = ReLU ( x, W ). When adding a new layer, if we initialize W to the identity matrix I, we have:</p><formula xml:id="formula_2">u = ReLU ( x, I ) = ReLU (x) = x</formula><p>The last step holds since x is an output of a previous ReLU layer, and ReLU (ReLU (x)) = ReLU (x). Thus, adding more layers should only improve performance. However, how can a network with more layers learn to yield performance superior than a network with less layers? A key observation is that if learning identity mapping is easy, then the network with more layers is more likely to yield superior performance, as it can more easily recover the performance of a smaller network through identity mappings. This new layer will perform an identity mapping, therefore the two models are equivalent.</p><p>The layer design of Residual Networks allows for deeper models to be trained due to its shortcut connections. Note that in ResNets the identity mapping is learned when W = 0 instead of W = I. Considering a residual layer u = ReLU ( x, W ) + x, we have:</p><formula xml:id="formula_3">u = ReLU ( x, 0 ) + x = ReLU (0) + x = x</formula><p>Intuitively, residual layers can degenerate into identity mappings more effectively since learning an all-zero matrix is easier than learning the identity matrix. To support this argument, consider weight parameters randomly initialized with zero mean. Hence, the point W = 0 is located exactly in the center of the probability mass distribution used to initialize the weights.</p><p>However, assuming that residual layers can trivially learn the parameter set W = 0 implies ignoring the randomness when initializing the weights. We demonstrate this by calculating the expected component-wise distance between W o and the origin. Here, W o denotes the weight tensor after initialization and prior to any optimization. Note that the distance between W o and the origin captures the effort for a network to learn identity mappings:</p><formula xml:id="formula_4">E (W o ? 0) 2 = E W 2 o = V ar W o</formula><p>Note that the distance is given by the distribution's variance, and there is no reason to assume it to be negligible. Additionally, the fact that Residual Networks still suffer from optimization issues caused by depth ) further supports this claim.</p><p>Some initialization schemes propose a variance in the order of O( 1 n ) (?, <ref type="bibr" target="#b8">He et al. (2015a)</ref>), however this represents the distance for each individual parameter in W . For tensors with O(n 2 ) parameters, the total distance -either absolute or Euclidean -between W o and the origin will be in the order of O(n).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">RESIDUAL GATES</head><p>As previously mentioned, the key contribution in this work is the proposal of a layer design where learning a single scalar parameter suffices in order for the layer to degenerate into an identity mapping. As in Highway Networks, we propose the addition of gated shortcut connections. Our gates, however, are parametrized by a single scalar value, being easier to analyze and learn. In our model, the effort required to learn identity mappings does not depend on any parameter, such as the layer width, in sharp contrast to prior models.</p><p>Our design is as follows: a layer u = f (x, W ) becomes u = g(k)f (x, W ) + (1 ? g(k))x, where k is a scalar parameter. This design is illustrated in <ref type="figure">Figure 1</ref>. Note that such layer can quickly degenerate by setting g(k) to 0. Using the ReLU activation function as g, it suffices that k ? 0 for g(k) = 0.</p><p>By adding an extra parameter, the dimensionality of the cost surface also grows by one. This new dimension, however, can be easily understood due to the specific nature of the layer reformulation. The original surface is maintained on the k = 1 slice, since the gated model becomes equivalent to the original one. On the k = 0 slice we have an identity mapping, and the associated cost for all points in such slice is the same cost associated with the point {k = 1, W = I}: this follows since both parameter configurations correspond to identity mappings, therefore being equivalent. Lastly, due to the linear nature of g(k) and consequently of the gates, all other slices k = 0, k = 1 will be a linear combination between the slices k = 0 and k = 1.</p><p>We proceed to use residual layers as the basis for our design, for two reasons. First, they are the current standard for computer vision tasks. Second, ResNets lack means to regulate the residuals, therefore a linear gating mechanism might not only allow deeper models, but could also improve performance. Thus, the residual layer is given by:</p><formula xml:id="formula_5">u = f (x, W ) = f r (x, W ) + x where f r (x, W ) is the layer's residual function -in our case, BN-ReLU-Conv-BN-ReLU-Conv.</formula><p>Our approach changes this layer by adding a linear gate, yielding:</p><formula xml:id="formula_6">u = g(k)f (x, W ) + (1 ? g(k))x = g(k)(f r (x, W ) + x) + (1 ? g(k))x = g(k)f r (x, W ) + x</formula><p>Our approach applied to residual layers is shown in <ref type="figure">Figure 2</ref>. The resulting layer maintains the shortcut connection unaltered, which according to <ref type="bibr" target="#b10">He et al. (2016)</ref> is a desired property when designing residual blocks. As (1 ? g(k)) vanishes from the formulation, g(k) stops acting as a dual gating mechanism and can be interpreted as a flow regulator. Note that this model introduces a single scalar parameter per layer block. This new dimension can be interpreted as discussed above, except that the slice k = 0 is equivalent to {k = 1, W = 0}, since an identity mapping is learned when W = 0 in ResNets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">EXPERIMENTS</head><p>All models were implemented on Keras <ref type="bibr" target="#b3">(Chollet (2015)</ref>) or on Torch (?), and were executed on a Geforce GTX 1070. Larger models or more complex datasets, such as the ImageNet <ref type="bibr">(Russakovsky et al. (2015)</ref>), were not explored due to hardware limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">MNIST</head><p>The MNIST dataset <ref type="bibr" target="#b17">(Lecun et al. (1998)</ref>) is composed of 60, 000 greyscale images with 28 ? 28 pixels. Images represent handwritten digits, resulting in a total of 10 classes. We trained three types of fully-connected models: classical plain networks, ResNets and GResNets.</p><p>The networks consist of a linear layer with 50 neurons, followed by d layers with 50 neurons each, and lastly a softmax layer for classification. Only the d middle layers differ between the three architectures -the first linear layer and the softmax layer are the same in all experiments.</p><p>For plain networks, each layer performs dot product, followed by Batch Normalization and a ReLU activation function.</p><p>Initial tests with pre-activations <ref type="bibr" target="#b10">(He et al. (2016)</ref>) resulted in poor performance on the validation set, therefore we opted for the traditional Dot-BN-ReLU layer when designing Residual Networks. Each residual block is consists of two layers, as conventional.</p><p>All networks were trained using Adam <ref type="bibr" target="#b13">(Kingma &amp; Ba (2014)</ref>) with Nesterov momentum <ref type="bibr">(Dozat)</ref> for a total of 100 epochs using mini-batches of size 128. No learning rate decay was used: we kept the learning rate and momentum fixed to 0.002 and 0.9 during the whole training.</p><p>For preprocessing, we divided each pixel value by 255, normalizing their values to [0, 1].</p><p>The training curves for classical plain networks, ResNets and GResNets with varying depth are shown in <ref type="figure" target="#fig_1">Figure 4</ref>. The distance between the curves increase with the depth, showing that the augmentation helps the training of deeper models.   As observed in <ref type="table" target="#tab_1">Table 2</ref>, the mean values of k decrease as the model gets deeper, showing that shortcut connections have less impact on shallow networks. This agrees with empirical results that ResNets perform better than classical plain networks as the depth increases.</p><p>We also analyzed how layer removal affects ResNets and GResNets. We compared how the deepest networks (d = 100) behave as residual blocks composed of 2 layers are completely removed from the models. The final values for each k parameter, according to its corresponding residual block, is shown in <ref type="figure">Figure 5</ref>. We can observe that layers close to the middle of the network have a smaller k than these in the beginning or the end. Therefore, the middle layers have less importance by due to being closer to identity mappings. <ref type="figure">Figure 5</ref>: Left: Values for k according to ascending order of residual blocks. The first block, consisted of the first two layers of the network, has index 1, while the last block -right before the softmax layer -has index 50. Right: Test accuracy (%) according to the number of removed layers. Gated Residual Networks are more robust to layer removal, and maintain decent results even after half of the layers have been removed.</p><p>Results are shown in <ref type="figure">Figure 5</ref>. For Gated Residual Networks, we prune pairs of layers following two strategies. One consists of pruning layers in a greedy fashion, where blocks with the smallest k are removed first. In the other we remove blocks randomly. We present results using both strategies for GResNets, and only random pruning for ResNets since they lack the k parameter.</p><p>The greedy strategy is slightly better for Gated Residual Networks, showing that the k parameter is indeed a good indicator of a layer's importance for the model, but that layers tend to assume the same level of significance. In a fair comparison, where both models are pruned randomly, GResNets retain a satisfactory performance even after half of its layers have been removed, while ResNets suffer performance decrease after just a few layers.</p><p>Therefore augmented models are not only more robust to layer removal, but can have a fair share of their layers pruned and still perform well. Faster predictions can be generated by using a pruned version of an original model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CIFAR</head><p>The CIFAR datasets <ref type="bibr" target="#b14">(Krizhevsky (2009)</ref>) consists of 60, 000 color images with 32 ? 32 pixels each. CIFAR-10 has a total of 10 classes, including pictures of cats, birds and airplanes. The CIFAR-100 dataset is composed of the same number of images, however with a total of 100 classes.</p><p>Residual Networks have surpassed state-of-the-art results on CIFAR. We test GResNets, Wide GResNets <ref type="bibr" target="#b26">(Zagoruyko &amp; Komodakis (2016)</ref>) and compare them with their original, non-augmented models.</p><p>For pre-activation ResNets, as described in <ref type="bibr" target="#b10">He et al. (2016)</ref>, we follow the original implementation details. We set an initial learning rate of 0.1, and decrease it by a factor of 10 after 50% and 75% epochs. SGD with Nesterov momentum of 0.9 are used for optimization, and the only preprocessing consists of mean subtraction. Weight decay of 0.0001 is used for regularization, and Batch Normalization's momentum is set to 0.9.</p><p>We follow the implementation from <ref type="bibr" target="#b26">Zagoruyko &amp; Komodakis (2016)</ref> for Wide ResNets. The learning rate is initialized as 0.1, and decreases by a factor of 5 after 30%, 60% and 80% epochs. Images are mean/std normalized, and a weight decay of 0.0005 is used for regularization. When dropout is specified, we apply 0.3 dropout (?) between convolutions. All other details are the same as for ResNets. For both architectures we use moderate data augmentation: images are padded with 4 pixels, and we take random crops of size 32 ? 32 during training. Additionally, each image is horizontally flipped with 50% probability. We use batch size 128 for all experiments.</p><p>For all gated networks, we initialize k with a constant value of 1. One crucial question is whether weight decay should be applied to the k parameters. We call this "k decay", and also compare GResNets and Wide GResNets when it is applied with the same magnitude of the weight decay: 0.0001 for GResNet and 0.0005 for Wide GResNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acc</head><p>Original Gated Gated (k decay) Resnet 5 7.16 6.67 7.04 Wide ResNet (4,10) + Dropout 3.89 3.65 3.74 <ref type="table">Table 3</ref>: Test error (%) on the CIFAR-10 dataset, for ResNets, Wide ResNets and their augmented counterparts. k decay is when weight decay is also applied to the k parameters in an augmented network. <ref type="table">Table 3</ref> shows the test error for two architectures: a ResNet with n = 5, and a Wide ResNet with n = 4, n = 10. Augmenting each model adds 15 and 12 parameters, respectively. We observe that k decay hurts performance in both cases, indicating that they should either remain unregularized or suffer a more subtle regularization compared to the weight parameters. Due to its direct connection to layer degeneration, regularizing k results in enforcing identity mappings, which might harm the model. <ref type="figure">Figure 7</ref>: Values for k according to ascending order of residual blocks. The first block, consisted of the first two layers of the network, has index 1, while the last block -right before the softmax layer -has index 12.</p><p>As in the previous experiment, in <ref type="figure">Figure 7</ref> we present the final k values for each block. We can observe that the k values follow an intriguing pattern: the lowest values are for the blocks of index 1, 5 and 9, which are exactly the ones that increase the feature map dimension. This indicates that, in such residual blocks, the convolution performed in the shortcut connection to increase dimension is more important than the residual block itself. Additionally, the peak value for the last residual block suggests that its shortcut connection is of little importance, and could as well be fully removed without greatly impacting the model.</p><p>Results of different models on the CIFAR datasets are shown in <ref type="table" target="#tab_3">Table 4</ref>. The training and test errors are presented in <ref type="figure" target="#fig_2">Figure 6</ref>. To the authors' knowledge, those are the best results on CIFAR-10 and CIFAR-100 with moderate data augmentation -only random flips and translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CONCLUSION</head><p>We have proposed a novel layer design based on Highway Neural Networks, which can be applied to provide general layers a quick way to learn identity mappings. Unlike Highway or Residual Networks, layers generated by our technique require optimizing only one parameter to degenerate into identity. By designing our method such that randomly initialized parameter sets are always close to identity mappings, our design offers less issues with optimization issues caused by depth.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>A network can have layers added to it without losing performance. Initially, a network has m ReLU layers with parameters {W 1 , . . . , W m }. A new, (m+1)-th layer is added with W m+1 = I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Train loss for plain (classical), residual and gated residual networks (GResNet), with d = {2, 10, 20, 50, 100}. As the models get deeper, the error reduction due to the augmentation increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Training and test curves, showing error (%) on training and test sets. Dashed lines represent training error, whereas solid lines represent test error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Test error (%) on the MNIST dataset for fully-connected networks. GResNets achieve lower error than ResNets in all experiments. Classical fully-connected networks perform worse and fail to converge for d = 50 and d = 100.Gated Residual Networks perform better in all settings, and the performance boost is more noticeable with increased depths. The relative error decreased approximately 2.5% for d = {2, 10, 20}, 8.7% for d = 50 and 16% for d = 100.</figDesc><table><row><cell cols="4">Depth = d + 2 Classical ResNet GResNet</cell></row><row><cell>d = 2</cell><cell>2.29</cell><cell>2.20</cell><cell>2.17</cell></row><row><cell>d = 10</cell><cell>2.22</cell><cell>1.64</cell><cell>1.60</cell></row><row><cell>d = 20</cell><cell>2.21</cell><cell>1.61</cell><cell>1.57</cell></row><row><cell>d = 50</cell><cell>60.37</cell><cell>1.62</cell><cell>1.48</cell></row><row><cell>d = 100</cell><cell>90.20</cell><cell>1.50</cell><cell>1.26</cell></row><row><cell cols="3">Table 1: Depth = d + 2 Mean k</cell><cell></cell></row><row><cell>d = 2</cell><cell></cell><cell>5.58</cell><cell></cell></row><row><cell>d = 10</cell><cell></cell><cell>2.54</cell><cell></cell></row><row><cell>d = 20</cell><cell></cell><cell>1.73</cell><cell></cell></row><row><cell>d = 50</cell><cell></cell><cell>1.04</cell><cell></cell></row><row><cell cols="2">d = 100</cell><cell>0.67</cell><cell></cell></row></table><note>shows the test error for each depth and architecture. ResNets converge in experiments with d = 50 and d = 100 (52 and 102 layers, respectively), while classical models do not.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>Mean k for increasingly deep Gated Residual Networks.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Test error (%) on the CIFAR-10 and CIFAR-100 dataset. All results are with standard data augmentation (crops and flips).We have shown that applying our technique to ResNets yield a model that can regulate the residuals, named Gated Residual Networks. This model performed better in all our experiments with negligible extra training time and parameters. Lastly, we have shown how it can be used for layer pruning, effectively removing large numbers of parameters from a network without necessarily harming its performance.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning long-term dependencies with gradient descent is difficult</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Greedy layer-wise training of deep networks. NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Popovici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On the complexity of neural network classifiers: A comparison between shallow and deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Monica</forename><surname>Bianchini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<idno type="DOI">10.1109/TNNLS.2013.2293637</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1553" to="1565" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franois</forename><surname>Chollet</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Incorporating nesterov momentum into adam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The Power of Depth for Feedforward Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://www.jmlr.org/proceedings/papers/v15/glorot11a/glorot11a.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11)</title>
		<editor>Geoffrey J. Gordon and David B. Dunson</editor>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
	<note>Journal of Machine Learning Research -Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-02" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Identity Mappings in Deep Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep Networks with Stochastic Depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-03" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploring strategies for training deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<idno>1532-4435</idno>
		<ptr target="http://dl.acm.org/citation.cfm?id=1577069.1577070" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2009-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">FractalNet: Ultra-Deep Neural Networks without Residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-05" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">On the Number of Linear Regions of Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mont?far</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014-02" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.icml2010.org/papers/432.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10)</title>
		<editor>Johannes Frnkranz and Thorsten Joachims</editor>
		<meeting>the 27th International Conference on Machine Learning (ICML-10)</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<idno type="DOI">10.1007/s11263-015-0816-y</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Rupesh Kumar Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
		<idno>abs/1507.06228</idno>
		<ptr target="http://arxiv.org/abs/1507.06228" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions. CoRR, abs/1409</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1409.4842" />
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">4842</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Benefits of depth in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Telgarsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Revise Saturated Activation Functions. ArXiv e-prints</title>
		<imprint>
			<date type="published" when="2016-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Wide residual networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno>abs/1605.07146</idno>
		<ptr target="http://arxiv.org/abs/1605.07146" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
