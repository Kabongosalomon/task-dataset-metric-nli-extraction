<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hybrid Dynamic-static Context-aware Attention Network for Action Assess- ment in Long Videos Hybrid Dynamic-static Context-aware Attention Network for Action Assessment in Long Videos Bib: @inproceedings{zeng2020hybrid, title={Hybrid Dynamic-static Context-aware Attention Network for Action Assessment in Long Videos}, author={Ling- Hybrid Dynamic-static Context-aware Attention Network for Action Assessment in Long Videos Dynamic features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 12-16, 2020. October 12-16, 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-An</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fa-Ting</forename><surname>Hong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Zhi</forename><surname>Yu</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zeng</surname></persName>
							<email>weizeng@pku.edu.cnwangyw@pcl.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Wei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-An</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fa-Ting</forename><surname>Hong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Zhi</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Wei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fa-Ting</forename><surname>Hong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Zhi</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zeng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Wei</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai}</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-An</forename><surname>Zeng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fa-Ting</forename><surname>Hong</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi-Zhi</forename><surname>Yu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Zhejiang Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zeng</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Peking University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao-Wei</forename><surname>Wang</surname></persName>
							<affiliation key="aff4">
								<orgName type="department">PengCheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">booktitle={ACM International Conference on Multimedia}</orgName>
								<address>
									<postCode>year={2020} }</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hybrid Dynamic-static Context-aware Attention Network for Action Assess- ment in Long Videos Hybrid Dynamic-static Context-aware Attention Network for Action Assessment in Long Videos Bib: @inproceedings{zeng2020hybrid, title={Hybrid Dynamic-static Context-aware Attention Network for Action Assessment in Long Videos}, author={Ling- Hybrid Dynamic-static Context-aware Attention Network for Action Assessment in Long Videos Dynamic features</title>
					</analytic>
					<monogr>
						<title level="m">Proc. of ACM International Conference on Multimedia (ACM MM), 2020. MM &apos;20</title>
						<meeting>. of ACM International Conference on Multimedia (ACM MM), 2020. MM &apos;20 <address><addrLine>Seattle, WA, USA MM &apos;20; Seattle, WA, USA</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">October 12-16, 2020. October 12-16, 2020</date>
						</imprint>
					</monogr>
					<note>For reference of this work, please cite: Zeng et al. ACM ISBN 978-1-4503-7988-5/20/10. . . $15.00</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Activity recognition and un- derstanding; Scene understanding KEYWORDS Computer vision</term>
					<term>Action quality assessment</term>
					<term>Hybrid dynamic-static architecture</term>
					<term>Context-aware attention</term>
					<term>Rhythmic Gymnastics dataset * Corresponding author</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The objective of action quality assessment is to score sports videos. However, most existing works focus only on video dynamic information (i.e., motion information) but ignore the specific postures that an athlete is performing in a video, which is important for action assessment in long videos. In this work, we present a novel hybrid dynAmic-static Context-aware attenTION NETwork (ACTION-NET) for action assessment in long videos. To learn more discriminative representations for videos, we not only learn the video dynamic information but also focus on the static postures of the detected athletes in specific frames, which represent the action quality at certain moments, along with the help of the proposed hybrid dynamic-static architecture. Moreover, we leverage a context-aware attention module consisting of a temporal instancewise graph convolutional network unit and an attention unit for both streams to extract more robust stream features, where the former is for exploring the relations between instances and the latter for assigning a proper weight to each instance. Finally, we combine the features of the two streams to regress the final video score, supervised by ground-truth scores given by experts. Additionally, we have collected and annotated the new Rhythmic Gymnastics dataset, which contains videos of four different types of gymnastics routines, for evaluation of action quality assessment in long videos. Extensive experimental results validate the efficacy of our proposed method, which outperforms related approaches. The codes and dataset are available at https://github.com/lingan1996/ACTION-NET.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Action quality assessment by ACTION-NET. ACTION-NET takes both motion information and posture information as inputs. Here, we utilize posture information to effectively supplement the motion information. We also adopt an attention mechanism to exploit the important part in the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In certain types of sports competitions, athletes' scores are given by experts, who have trained in the corresponding field for many years, based on the movements made by the athletes being scored. However, without onsite evaluations by such experts, athletes cannot obtain rapid feedback to improve their performance. It is therefore natural to ask whether a computer vision model can be built to automatically assess the quality of the actions performed by athletes. It is already known that such action quality assessment can be beneficial in other fields, such as medical treatment and the teaching of experimental methods in a scholastic setting.</p><p>Recently, action quality assessment has received increasing interest in the computer vision community <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref>. However, most existing works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref> focus only on dynamic information (i.e., motion information), which mainly reflects the category of an action, but ignore static information, which reflects the quality of an action at a certain moment. For instance, an insufficient angle of leg lift will result in a substandard action and penalty points for a leg-lifting action. Although an incorrect angle of leg lift may be easily missed on dynamic information, it can be clearly identified from a specific video frame. Moreover, for long videos, existing works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b29">30]</ref> simply treat the importance of each part of a video as the same. However, actions performed at different times may have markedly different effects on the final score issued by experts, meaning that the different parts of a video are not of equal importance. For example, in artistic gymnastics, the highlights of a routine should be more important than other parts. Consequently, these works have poor performance for action assessment in long videos.</p><p>To address the first problem mentioned above, we propose a hybrid dynamic-static architecture for learning video representations, as depicted in <ref type="figure">Figure 1</ref>. There are two streams in the proposed architecture: a dynamic stream for extracting motion information from video, which takes video segments as inputs, and a static stream for exploring the postures of detected athletes in specific frames. The same network structure is used for both streams, but the parameters are not shared. By fusing these two separate streams, we can model more discriminative features to represent the actions. Additionally, rather than treating each segment in a video equally, we apply a context-aware attention module to each stream to generate more informative dynamic/static features before they are concatenated to regress the final score for the video. More specifically, the context-aware attention module consists of a temporal instancewise graph convolutional network (GCN) unit and an attention unit, the former for exploring the relations between instances and the latter for assigning a proper weight to each instance, for both streams to extract more robust stream features. Subsequently, we aggregate the fused local-context features of all segments/frames (or motions/postures) weighted with the weights estimated by the attention unit to generate the final dynamic/static features.</p><p>To support research on action quality assessment for long videos, we have constructed a new Rhythmic Gymnastics dataset. The Rhythmic Gymnastics dataset contains videos of four different types of gymnastics routines: ball, clubs, hoop and ribbon. Each type of routine has 250 associated videos, and the length of each video is approximately 1 min 35 s. We chose high-standard international competition videos, including videos from the 36th and 37th International Artistic Gymnastics Competitions, to construct the dataset. We have edited out the irrelevant parts of the original videos (such as replay shots and athlete warmups). We have annotated each video with three scores (a difficulty score, an execution score and a total score), which were given by the referee in accordance with the official scoring system. This dataset will be released soon.</p><p>In summary, we propose a hybrid dynAmic-statiC conText-aware attentION NETwork (ACTION-NET) to predict athlete scores from long videos. To the best of our knowledge, ACTION-NET is the first such technology to incorporate both video motion information and posture information for detected people in static frames for action quality assessment. Experiments conducted on the public MIT-Skating dataset and our Rhythmic Gymnastics dataset clearly show that our method achieves state-of-the-art results. The codes and dataset are available at https://github.com/lingan1996/ACTION-NET.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Action Quality Assessment. The challenging problem of action quality assessment in videos has been extensively explored in previous works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>. The existing methods can be divided into two strategies based on the information used as the input-namely, pose-based methods and vision-based methods. Pose-based methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18]</ref> analyze human pose information to assess action quality. Pirsiavash et al. <ref type="bibr" target="#b17">[18]</ref> used the DCT features of joint trajectories as the input for SVR to predict final scores supervised by ground-truth scores given by referees. Pan et al. <ref type="bibr" target="#b12">[13]</ref> considered the relations among joints and proposed two modulesnamely, a joint commonality module and a joint difference moduleto analyze joint motion. However, due to the atypical body postures involved, it is difficult to estimate the human poses executed during sports such as diving. Moreover, joint motion alone cannot reflect appearance information, which serves as the basis for points scored in gymnastics, or background information, such as the splash size in diving. By contrast, vision-based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref> assess the quality of actions based on visual features extracted from videos by 3D convolutional neural networks (CNNs). For instance, to model more informative features from specific fragments, key fragment segmentation <ref type="bibr" target="#b9">[10]</ref> has been proposed to obtain key fragments while discarding irrelevant fragments. In addition, Li et al. <ref type="bibr" target="#b10">[11]</ref> proposed a novel recurrent neural network (RNN)-based spatial attention model based on the attention mechanism used by humans when assessing videos. However, because these video feature extraction methods are designed for action classification, the dynamic information they extract mainly reflects the action category rather than the action quality.</p><p>For long videos, actions performed at different times may have markedly different effects on the final score issued by experts. However, most existing works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref> simply treat the importance of each part from a video equally, so the important part may not be explored effectively in such a way. To assess the quality of actions in long videos, several different methods have been proposed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b24">25]</ref>. Bertasius et al. <ref type="bibr" target="#b1">[2]</ref> used a convolutional long short-term memory (LSTM) network to detect atomic basketball events, such as shooting or passing the ball during a basketball game, from first-person videos. However, because that method is a strongly task-related method, it is difficult to generalize. Xu et al. <ref type="bibr" target="#b24">[25]</ref> used a self-attentive LSTM network and a multiscale skip LSTM network to learn local (technical movements) and global (athlete performance) scores, respectively. Doughty et al. <ref type="bibr" target="#b4">[5]</ref> proposed a rank-aware temporal attention mechanism to attend to the skill-relevant parts of a video. However, these works did not consider the temporal relations between adjacent parts of a video.</p><p>In contrast to previous work, we propose a hybrid dynamic-static architecture for learning both dynamic information and static information related to specific moments. We argue that in sports, static information is also important for exploring the postures of the athletes detected in specific frames to determine whether those postures are correct. Additionally, we propose a contextaware attention module to aggregate all video segments/frames (motions/postures) to produce dynamic/static features.</p><p>Video Understanding. Video understanding has been studied for a long time in the field of computer vision and provides fundamental tools for action quality assessment. A number of deep neural network architectures for action classification have been proposed <ref type="bibr">[3, 22-24, 26, 27]</ref>, including two-stream networks with multiple modalities <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref>, 3D CNNs for extracting spatial and temporal features <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b22">23]</ref>, and RNNs for capturing temporal relations in variable-length videos <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. In addition to action classification, many works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31]</ref> have studied temporal action localization, with the aim of generating sequences of bounding boxes related to the locations of the actors.</p><p>In this paper, we employ the I3D architecture <ref type="bibr" target="#b2">[3]</ref> to obtain the basic video features from the video segments. Because the video features extracted by I3D are not designed for action quality assessment and mainly reflect the action category, we especially add a static stream, from which features are extracted using the ResNet architecture <ref type="bibr" target="#b5">[6]</ref> to explore the postures of detected athletes in specific frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>In this work, we develop ACTION-NET for action quality assessment. By leveraging two streams-i.e., a dynamic stream and a static stream-ACTION-NET generates more informative representations for videos by exploiting video motion information and the posture information of detected athletes in specific frames, respectively. Moreover, we propose a context-aware attention module to aggregate all video segments/frames to produce dynamic/static features in each stream. More details of the proposed network can be seen in <ref type="figure" target="#fig_0">Figure 2</ref>. In this section, we present detailed descriptions of the steps of action quality assessment using ACTION-NET. We introduce the proposed context-aware attention module in section 3.2 after first introducing the dynamic and static streams of the hybrid dynamic-static architecture in section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hybrid Dynamic-Static Architecture</head><p>Most existing works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25]</ref> mainly focus on the dynamic temporal information contained in videos and thus ignore incorrect postures at specific moments during an athlete's routine because of the fleeting nature of these postures. To address this problem, we propose a hybrid dynamic-static architecture for learning both dynamic information and posture information at specific moments, as depicted in <ref type="figure">Figure 3</ref>. Specifically, the network branches corresponding to the two streams have the same structure but do not share parameters.</p><p>Dynamic Stream. We leverage a stream of segments from the same video as the input to explore the motion information of the video. Instead of operating on the basis of optical flow, our dynamic stream structure utilizes a sequence of video segments sampled from the raw video. For the dynamic stream, we first feed the segments into an I3D network pretrained on the Kinetics dataset <ref type="bibr" target="#b2">[3]</ref> to extract 1024-dimensional segment features, which are then passed to the context-aware attention module. In the context-aware attention module, we compute the context-related features for each segment by a temporal instance-wise graph convolutional network unit (TCG-U), which is leveraged to model the relationship between each segment and the corresponding global information (with all segment features as inputs). An attention unit (ATT-U) then takes the fused local-context features obtained by concatenating the segment features and the context-related feature as inputs to generate an attention weight for each segment. Finally, we aggregate the fused local-context features of all segments weighted with the attention weights estimated by the ATT-U to generate dynamic features.</p><p>Static Stream. As mentioned before, the video segments can yield dynamic information, but slightly incorrect postures can easily be ignored because of their fleeting nature. To address this challenge, we adopt a static stream, for which the network structure is the same as that for the dynamic stream, to provide supplemental posture and appearance information of the detected athletes. For the static stream, we sample frames from raw videos to explore the posture information. Since the athletes in sports videos often occupy only a small part of the image, it is difficult to extract high-quality spatial features of an athlete when the entire image is taken as the input. Therefore, we first perform human detection on the sampled frames and then extract features from the detected people. In this way, we can obtain high-quality spatial information on the exhibited postures. Subsequently, we feed the extracted posture features from each frame into the context-aware attention module for the static stream, which is the same as that for the dynamic stream, to generate static features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Context-Aware Attention Module</head><p>As described in section 3.1, a context-aware attention module is applied to both streams to aggregate all segment/frame (motion/posture) features to produce the features of the corresponding stream (i.e., dynamic features and static features). Our proposed context-aware attention module consists of two units: the TCG-U for modeling the relationships between the segments/frames and the ATT-U for estimating the attention weights of these segments/frames. Temporal Clipwise GCN Unit (TCG-U). After obtaining the features {f i I } N i=1 of each instance (here, we refer to both segments and frames as instances for convenience of description), we use a GCN to output the context-related features {f i H } N i=1 for each instance by aggregating all instance features. To iteratively learn the relationships between all instances, we construct a graph G with all instances as vertices. We then adopt an exponential kernel <ref type="bibr" target="#b27">[28]</ref> to compute the adjacency matrix A ? R N ?N for graph G:</p><formula xml:id="formula_0">A (i, j) = exp ?||f i I ?f j I || K ,<label>(1)</label></formula><p>where K is a positive hyperparameter used to adjust the scale of the distance between two vertices, and element A (i, j) of the adjacency matrix A represents the temporal relationship between the i th and j th instances. In our experiment, K is set to 1.</p><p>It is known that the adjacent instances in a graph exchange information through iterative graph-Laplacian operations. To maintain the original distribution of the matrix A in the process of information transmission, we normalize the adjacency matrix, following Kipf and Welling <ref type="bibr" target="#b6">[7]</ref>:</p><formula xml:id="formula_1">A = D ? 1 2 A D ? 1 2 ,<label>(2)</label></formula><p>where the matrix A is defined as A = A+I , with I ? R N ?N being the identity matrix, and D (i,i) = i, j A i, j is the degree matrix of the   <ref type="figure">Figure 3</ref>: Hybrid dynamic-static architecture. Two streams have different purposes but identical streams. Here, the dynamic stream mainly extracts the motion information of video, and the static stream extracts the posture information of specific frames.</p><p>adjacency matrix variant A. To learn more context knowledge from the graph G consisting of all instances, we iteratively update the representation of each vertex by multiple GCN layers to generate more robust context-related features.</p><formula xml:id="formula_2">H l +1 = ? ( AH l W l ),<label>(3)</label></formula><p>where ? (?) is the ReLU (?) activation function in this work, W l is the trainable weight matrix in the l t h layer, and H l is the activation matrix in the l t h layer, with H 0 = {f i I } N i=1 . We then use two GCN layers to capture the context information among all instances based on the temporal relations of adjacent instances. Finally, we denote the last H l by H = {f i R } N i=1 as the context-related feature for each instance.</p><p>Attention Unit (ATT-U). After obtaining the context-related fea-</p><formula xml:id="formula_3">tures {f i H } N i=1</formula><p>, we combine the corresponding instance features to obtain the fused local-context features {f i C } N i=1 , which contain both instance and global context information and are more representative of each instance in the video:</p><formula xml:id="formula_4">f i C = concatenate(f i I , f i R ).<label>(4)</label></formula><p>Here, we concatenate the instance features f i I and the contextrelated features f i R to obtain the fused local-context features f i C . We then calculate the weighted sum of all fused local-context features using the attention weights {? i } N i=1 estimated by our simple ATT-U, which consists of two fully connected layers and corresponding activation function with fused local-context features as inputs, to produce the corresponding stream features f D/S (i.e., dynamic features f D or static features f S ):</p><formula xml:id="formula_5">f D/S = i ? i f i C , ? i = ?(f i C ).<label>(5)</label></formula><p>where ?(?) represents the ATT-U. In this way, we utilize all instances in a video to construct fused local-context features that capture not only the corresponding instance attributes but also the context information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Combination of Dynamic and Static Streams</head><p>Finally, we obtain the dynamic and static features from their respective streams. We concatenate these two types of features and regress the final score for the input video as follows:</p><formula xml:id="formula_6">s = f r ([f D ; f S ]),<label>(6)</label></formula><p>where f r (?) represents two fully connected layers with the sigmoid activation function. The sigmoid activation function is used to normalize the estimated score to the range of [0, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">RHYTHMIC GYMNASTICS DATASET</head><p>In addition, we construct a new dataset for action quality assessmenti.e., the Rhythmic Gymnastics dataset-which contains videos of four types of gymnastics routines: ball, clubs, hoop and ribbon. We collected these videos from the internet by searching for specific topics. All videos in the Rhythmic Gymnastics dataset are of high quality and were downloaded from the official account on YouTube. The Rhythmic Gymnastics dataset consists of 1000 videos spanning all types of routines-?.e., ball, clubs, hoop and ribbon-with 250 videos per routine type. More specifically, these videos were obtained from high-standard international competitions-i.e., the 36th and 37th International Artistic Gymnastics Competitions. Because the scoring criteria for rhythmic gymnastics changed significantly after the 35th International Rhythmic Gymnastics Competition, we did not select any videos from before the scoring change to ensure the uniformity of the scoring criteria.</p><p>Preprocessing. We first collected approximately 37 h of videos and removed abnormal videos, such as those in which the athlete retired for atypical reasons. We edited out the irrelevant parts of each original video (such as bowing to the audience and warming up). We preserved only the duration of each video from the moment of the beginning pose to the moment of the ending pose. The length of each video is approximately 1 min 35 s, corresponding to approximately 2375 frames at a frame rate of 25 frames per second. Score Annotation. We annotated each video with three scores (a difficulty score, an execution score and a total score), which were given by the referee in accordance with the scoring system. The final score is the sum of the difficulty score and the execution score, with deductions for any penalties incurred. The difficulty score consists of subscores for body difficulties, dynamic elements with rotation (commonly known as risks), and apparatus difficulties. The execution score represents the degree to which the gymnast performs with aesthetic and technical perfection. Finally, for each event, we randomly split the dataset into 200 training videos and 50 testing videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we first describe the implementation details of our ACTION-NET model and then present the experimental results obtained on two datasets-i.e., the public MIT-Skating dataset and our Rhythmic Gymnastics dataset-and compare them with the results of several baselines. Finally, we conduct an ablation study to analyze the contributions of the hybrid dynamic-static architecture and the context-aware attention module as well as the necessity of human detection on the static stream.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Evaluation Metric</head><p>Datasets. We evaluated our ACTION-NET model on both the MIT-Skating dataset <ref type="bibr" target="#b17">[18]</ref> and our newly constructed Rhythmic Gymnastics dataset. The MIT-Skating dataset contains 150 figure skating videos, each of which is approximately 2 min 55 s. Each one contains an average of 4200 frames. Each video is labeled with three scores-i.e., a total element score, a total program component score and a final score; the final score, which ranges from 0 (worst) to 100 (best), is the sum of the total element score and the total program score. We used 100 videos for training and the remaining 50 videos for testing. <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> repeated the experiment 200 times with different random data splits and averaged the results. Due to the heavy computing cost, we repeated the experiment on five random data splits instead. On the Rhythmic Gymnastics dataset, for each type of routine, we used 200 videos for training and the remaining 50 videos for testing.</p><p>Evaluation Metric. Following existing works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref>, we used the standard evaluation metric known as Spearman's rank correlation coefficient ?, which represents the strength of the relation between two series of data. Its value ranges from -1 to 1, and it is computed as follows:</p><formula xml:id="formula_7">? = i (x i ? x)(y i ? y) i (x i ? x) 2 i (y i ? y) 2 ,<label>(7)</label></formula><p>where x and y represent the rankings of the two series. The higher the value of ?, the higher the rank correlation between the predicted and ground-truth scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>Data Preprocessing. Similar to previous works <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref>, our method is composed of two stages: feature extraction and score prediction. We first extracted features from videos and then used our network to predict scores. For the dynamic stream, we sampled 5 frames per second on average, and each segment contained 16 sampled frames. All frames were rescaled to have a shortest side length of 256 and were then center cropped to 224 ? 224. We then extracted 1024-dimensional segment features from the avg_pool layer of I3D pretrained on Kinetics <ref type="bibr" target="#b2">[3]</ref>. For the static stream, we sampled 1 frame per second and cropped the region with the detected athlete in the sampled frame using YOLOv3 <ref type="bibr" target="#b18">[19]</ref> pretrained on COCO <ref type="bibr" target="#b11">[12]</ref>. To filter out the action-irrelevant audience, we chose only the largest human bounding box in each frame and discarded all frames in which no athlete was detected. All cropped image regions were rescaled to 224 ? 224. We then extracted the 2048-dimensional features from the avg_pool layer of ResNet <ref type="bibr" target="#b5">[6]</ref> pretrained on ImageNet <ref type="bibr" target="#b19">[20]</ref>.</p><p>Experimental Settings. In the context-aware attention module, two fully connected layers with ReLU activation first embedded the input features into a low-dimensional space. For the dynamic stream, the dimensions of these two layers were 1024 ? 512 and 512 ? 256. For the static stream, their dimensions were 2048 ? 1024 and 1024?256. Two GCN layers were used for the TCG-U, both with dimensions of 256 ? 256. We used two fully connected layers as f r in Eq. 6 to predict the final score; the first fully connected layer had dimensions of 1024 ? 128 (followed by ReLU activation and dropout = 0.5), and the second layer had dimensions of 128 ? 1 with sigmoid activation. To ensure stable training, we adopted different learning rates for the context-aware attention module (lr = 0.01) and the score prediction module (lr = 0.05). We used minibatch stochastic gradient descent (SGD) to train the network, with a momentum of 0.9 and a weight decay of 10  <ref type="bibr" target="#b24">[25]</ref>, all video segments and cropped frames were augmented by shifting the starting segment and frame.</p><p>Competitors. We considered several baseline methods for comparison to validate the effectiveness of our proposed method. We first compared our method with those presented in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b24">25]</ref>. Because the Rhythmic Gymnastics dataset is a new dataset, to facilitate comparison, we newly evaluated the performance of some previous methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref> on this dataset. Following the settings for competitors given in <ref type="bibr" target="#b24">[25]</ref>, we considered different combinations of the following model components:</p><p>? Input features: We used either frame-level features or videosegment-level features as the input features. As the framelevel features, we extracted 2048-dimensional features from the avg_pool layer of ResNet <ref type="bibr" target="#b5">[6]</ref>, which is a common feature extractor for images. For the video-segment-level features, we extracted them as described for the dynamic stream. ? LSTM and Bi-LSTM-based models: Similar to Parmar et al.</p><p>[ <ref type="bibr" target="#b15">16]</ref>, we applied an LSTM architecture to generate videolevel descriptions. Due to the very long duration of the videos and to ensure fair comparisons, we also tested the use of a bidirectional LSTM (Bi-LSTM) architecture in place of the LSTM architecture. The hidden dimensions of the LSTM/Bi-LSTM layers were 256/128, and to avoid over-fitting, we used one fully connected layer for regression. ? SVR-based models: We first used either average or maximum pooling for video-level description. Because the effect of the linear kernel was better than that of the radial basis function (RBF) kernel in <ref type="bibr" target="#b24">[25]</ref>, we applied SVR only with a linear kernel to regress the final scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIT-Skating Rhythmic Gymnastics Ball Clubs Hoop Ribbon</head><p>Pose + DCT <ref type="bibr" target="#b14">[15]</ref> 0.350 ----ConvISA <ref type="bibr" target="#b7">[8]</ref> 0.450 ----C3D + SVR <ref type="bibr" target="#b15">[16]</ref> 0.530 0.357* 0.551* 0.495* 0.516* Li et al. <ref type="bibr" target="#b8">[9]</ref> 0.575 ----Pan et al. <ref type="bibr" target="#b12">[13]</ref> 0.384 ----Xu et al. <ref type="bibr" target="#b24">[25]</ref> 0   <ref type="table">Table 2</ref>: Ablation study showing the contributions of the dynamic and static streams in our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Results and Comparisons</head><p>The experimental results obtained by our method and the compared methods on the MIT-Skating and Rhythmic Gymnastics datasets are presented in <ref type="table" target="#tab_2">Table 1</ref>. The results marked with * are those obtained through the reimplementation of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b24">25]</ref> on our Rhythmic Gymnastics dataset. Although Pan et al. <ref type="bibr" target="#b12">[13]</ref> has achieved state-ofthe-art results on the AQA-7 dataset <ref type="bibr" target="#b13">[14]</ref>, which simply contains short videos (102 frames per video on average), their proposal performs poorly on the MIT-Skating dataset 1 . This is because Pan's work simply assumes all parts of a long video are equally important. Our ACTION-NET model achieves the best performance and outperforms the state-of-the-art method presented in <ref type="bibr" target="#b24">[25]</ref> by 2.5%. These results of compared methods clearly demonstrate that only utilizing video temporal information is insufficient to model a more comprehensive action assessment. In contrast, the results of our ACTION-NET indicate that the static information helps understand the quality of an action. Additionally, we further explored different variants of our model, as shown in <ref type="table" target="#tab_2">Table 1</ref>   <ref type="figure">Figure 6</ref>: The top four frames with high or low attention weights in static stream from two videos. The number below each image frame is the attention weight of the corresponding frame. The video frames with high attention show that the gymnast is performing technical movements or making a mistake (e.g., the loss of the apparatus is shown in the 5 th and 8 th images in the first row). The video frames with low attention weights show no such important, highly technical postures.</p><p>with average pooling also works well on the Rhythmic Gymnastics dataset but not on MIT-Skating, which is most likely because the videos in MIT-Skating are longer than those in the Rhythmic Gymnastics dataset. Moreover, models using I3D features can produce better prediction results than those using ResNet features extracted from the whole scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Model Analysis</head><p>Ablation Study of the Hybrid Dynamic-Static Architecture.</p><p>To show the effectiveness of the proposed hybrid dynamic-static architecture, we tried to remove either the dynamic stream or the static stream from the full model. The results of this ablation study are shown in <ref type="table">Table 2</ref>. To save space, we use the abbreviations DS, SS, TS, and CAA to represent the dynamic stream only, the static stream only, the two streams together (the hybrid dynamic-static  architecture) and our context-aware attention module, respectively. Interestingly, the results of SS + CAA show that methods using only the static stream can still achieve a good effect, even outperforming the methods using only the dynamic stream on the Hoop and Ribbon subsets of the Rhythmic Gymnastics dataset. These findings indicate that there is considerable redundant information in the dynamic stream and that the quality of actions performed by athletes can already be roughly assessed from only a few video frames. However, combining the dynamic stream with the static stream can boost the performance, as shown by the results of our full model (i.e., TS + CAA).</p><p>Ablation Study of the Context-Aware Attention Module. We also constructed four baseline methods by replacing the contextaware attention module in our method to evaluate its effectiveness:</p><p>? Avg Pooling: We used average pooling at the temporal level to generate video-level descriptions. With this approach, the temporal evolution and timing of an action is lost. ? SAU: We also used a standard attention unit (SAU) without context features as the input to evaluate the effectiveness of the context-aware attention module. The SAU was the same as the ATT-U used in the context-aware attention module. ? RTA: We also compared our approach with multi-filter attention module from the Rank-aware Temporal Attention (RTA) <ref type="bibr" target="#b4">[5]</ref> in our ablation study, in which the number of filters was set to three. ? LSTM/Bi-LSTM + SAU: Because an LSTM architecture is often used for tasks with a sequential structure, we used an LSTM architecture to capture context information as the input to the SAU. We also used a Bi-LSTM architecture in place of the LSTM architecture. The hidden dimensions of the LSTM/Bi-LSTM layers were set to 256/128.</p><p>In <ref type="table" target="#tab_5">Table 3</ref>, we compare the results of our method with those of the different baselines. Importantly, our proposed method shows improvements over all the alternative variants listed above on all datasets (i.e., our full method outperforms the Avg Pooling by 2.3 % on the MIT-Skating dataset). When the context information captured by the TCG-U is removed, the performance drops considerably. Using an LSTM/Bi-LSTM module instead of the TCG-U to capture context information results in a decrease in model performance. By contrast, using average pooling alone also results in competitive performance, particularly for the Ball and Hoop subsets. Similar to the experimental results of Doughty et al. <ref type="bibr" target="#b4">[5]</ref>, although the SAU achieves higher accuracy than average pooling for some tasks, we find the SAU results to be highly inconsistent for action quality assessment in long videos. Moreover, our context-aware attention slightly outperforms the multi-filter attention in the Rank-aware Temporal Attention <ref type="bibr" target="#b4">[5]</ref>, and the success of our context-aware attention and multi-filter attention indicate that the attention mechanism is an effective method for action assessment in long videos.</p><p>Additionally, to visualize the operation of the context-aware attention module, we present the attention weights computed for several segments of two videos in <ref type="figure">Figure 4</ref>. In <ref type="figure">Figure 5</ref>, we show further details of video clips with high and low attention weights from both videos; in this figure, each video segment is compressed to eight frames. We believe that if a particular video segment or frame has a high weight, then the video segment or frame shows an important technical movement that is likely to contribute to the final score; otherwise, it is not significant. With uniform weighting, the weight of each video segment is 1/28 (0.0384), where the value</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MIT-Skating</head><p>Rhythmic Gymnastics Ball Clubs Hoop Ribbon SS + CAA w/o detection 0.565 0.334 0.582 0.602 0.391 SS + CAA w detection 0.572 0.443 0.581 0.686 0.540 TS + CAA w/o detection 0.592 0.365 0.632 0.656 0.540 TS + CAA w detection 0.615 0.528 0.657 0.708 0.578 <ref type="table">Table 4</ref>: Ablation study showing the contribution of human detection in our method.</p><p>of 28 corresponds to the number of segments per video used to train our network. Notably, due to the scoring rules of gymnastics and figure skating, it is difficult to accurately assess the quality of an athlete's actions by observing only a few video segments. Therefore, there are no order-of-magnitude differences in the weights between significant and insignificant segments. From <ref type="figure">Figure 5</ref>, we find that segments that show the athlete performing movements such as rolling or lifting one leg represent key technical movements and thus have very high attention values. From <ref type="figure">Figure 6</ref>, we can also reach a similar conclusion in the static stream.</p><p>Is it necessary to perform human detection on the static stream? In this paragraph, we discuss whether it is necessary to perform human detection on the static stream. To show the effectiveness of human detection on the static stream, we used features extracted from the whole scene as the input for our full model. As seen in <ref type="table">Table 4</ref>, without human detection on the static stream, the performance achieved using either the static stream alone or the two streams combined is significantly decreased. Since athletes in sports videos often occupy only a small part of the image, it is difficult to extract high-quality spatial features of such an athlete when the entire image is used as the input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this work, we have proposed a hybrid dynamic-static contextaware attention network (ACTION-NET) for learning both video motion information and specific posture information from sampled video frames for action quality assessment. The context-aware attention module, which is applied to both streams in the proposed network, is able to learn useful representations by learning the relations between instances. The experimental results clearly show that our proposed ACTION-NET method can achieve state-of-theart performance on two datasets. Additionally, to support research on action quality assessment in long videos, we have collected and annotated the new Rhythmic Gymnastics dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGEMENT</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The pipeline of our ACTION-NET. Based on the dynamic stream, motion information and background information are extracted from video segments. The static stream provides spatial information about the postures and appearance of detected athletes in specific frames. Both streams are fed into ACTION-NET branches with identical structures, but these branches do not share parameters. The proposed context-aware attention module learns the relations between all segments/frames and generates dynamic/static features by aggregating the fused local-context features of all segments/frames. Finally, we concatenate the features from both streams and feed them into the score prediction module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Visualization of the attention weights generated by our context-aware attention module on two videos. The numbers below the images are the attention weights of the corresponding video segments. Visualization of video segments with high and low attention weights from two videos. The first row shows video segments with high attention weights, while the bottom row shows video segments with low attention weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>This work was supported partially by the National Key Research and Development Program of China (2018YFB1004903), NSFC(U1911401, U1811461), Guangdong Province Science and Technology Innovation Leading Talents (2016TX03X157), Guangdong NSF Project (No. 2018B030312002), Guangzhou Research Project (201902010037), Research Projects of Zhejiang Lab (No. 2019KD0AB03), the Key-Area Research and Development Program of Guangzhou (202007030004), and Pearl River S&amp;T Nova Program of Guangzhou (201806010056). We thank Jia-Chang Feng for useful feedback and suggestions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>?4 . The batch size was 32 for the Rhythmic Gymnastics dataset and 16 for the MIT-Skating dataset. The number of training epochs was 200 for the MIT-Skating dataset, and for better convergence, we set different training epochs on four types of gymnastics routines (400/300/500/300 for ball/clubs/hoop/ribbon).</figDesc><table><row><cell>For the Rhythmic Gymnastics dataset, we applied stepwise decay</cell></row><row><cell>in the last 100 and last 50 epochs with a decay rate of 0.1. For the</cell></row><row><cell>MIT-Skating dataset, we applied stepwise decay after 150 and 180</cell></row><row><cell>epochs with a decay rate of 0.1. Remarkably, although our model is</cell></row><row><cell>a hybrid dynamic-static architecture, our model is small, with only</cell></row><row><cell>3.54 M parameters.</cell></row></table><note>Data Augmentation. For the dynamic stream, the number of video segments per video was approximately 55 on the MIT-Skating dataset and 28 on the Rhythmic Gymnastics dataset. Accordingly, we used 48 video segments per video to train the network on the MIT-Skating dataset and 26 video segments per video on the Rhythmic Gymnastics dataset. For the static stream, the number of cropped frames per video was almost 160 on the MIT-Skating dataset and 80 on the Rhythmic Gymnastics dataset. We used 150 cropped frames per video for training on the MIT-Skating dataset and 80 cropped frames per video on the Rhythmic Gymnastics dataset. Similar to Xu et al.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell cols="3">Results in terms of Spearman's rank correlation co-</cell></row><row><cell cols="3">efficient (higher values are better) on the MIT-Skating and</cell></row><row><cell cols="3">Rhythmic Gymnastics datasets. The results marked with *</cell></row><row><cell cols="3">are those obtained through the reimplementation of [16, 25]</cell></row><row><cell cols="3">on our Rhythmic Gymnastics dataset.</cell></row><row><cell>Method</cell><cell>MIT-Skating</cell><cell>Rhythmic Gymnastics Ball Clubs Hoop Ribbon</cell></row><row><cell>DS + CAA</cell><cell>0.603</cell><cell>0.346 0.583 0.643 0.394</cell></row><row><cell>SS + CAA</cell><cell>0.575</cell><cell>0.443 0.581 0.686 0.540</cell></row><row><cell>TS + CAA (ours)</cell><cell>0.615</cell><cell>0.528 0.657 0.708 0.578</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>. By comparing the experimental results of the different variants, we can reach several interesting conclusions. First, in contrast to the results of Xu et al.<ref type="bibr" target="#b24">[25]</ref>, average pooling is obviously superior to maximum pooling on all datasets. SVR</figDesc><table><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>0.0473</cell><cell>0.0604</cell><cell>0.0319</cell><cell>0.0460</cell><cell>0.0153</cell><cell></cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row><row><cell>0.0310</cell><cell>0.0676</cell><cell>0.0190</cell><cell>0.0510</cell><cell>0.0489</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ablation study showing the contribution of the context-aware attention module in our method.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">This result was obtained by Pan through communication. Thanks for their contribution.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Cnnbased multiple path search for action tube detection in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erick</forename><surname>Hendra Putra Alwando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yie-Tarng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Hsien</forename><surname>Fang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Am I a baller? Basketball performance assessment from first-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun</forename><forename type="middle">Soo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Who&apos;s Better? Who&apos;s Best? Pairwise Deep Ranking for Skill Determination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walterio</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6057" to="6066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">The pros and cons: Rank-aware temporal attention for skill determination in long videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walterio</forename><surname>Mayol-Cuevas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7862" to="7871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><forename type="middle">Y</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Serena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3361" to="3368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">End-to-end learning for action quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujuan</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific Rim Conference on Multimedia</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="125" to="134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ScoringNet: Learning Key Fragment for Action Quality Assessment with Ranking Loss in Skilled Sports</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujuan</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="149" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Manipulation-skill assessment from videos with spatial attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minjie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoichi</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Action Assessment by Joint Relation Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jibin</forename><surname>Jia-Hui Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6331" to="6340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Action Quality Assessment Across Multiple Actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paritosh</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1468" to="1476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Measuring the quality of exercises</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paritosh</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">Tran</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2241" to="2244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to score olympic events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paritosh</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><forename type="middle">Tran</forename><surname>Morris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="20" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-region two-stream R-CNN for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="744" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Assessing the quality of actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="556" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep learning for detecting multiple space-time action tubes in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suman</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurkirt</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Sapienza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cuzzolin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.01529</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning to Score Figure Skating Sport Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengming</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zitian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Making convolutional networks recurrent for visual sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavlo</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6469" to="6478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Xing</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nannan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1237" to="1246" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Automated assessment of surgical skills using frequency analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aneeq</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yachna</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Bettadapura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">L</forename><surname>Sarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Clements</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="430" to="438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video and accelerometer-based motion analysis for automated surgical skills assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aneeq</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yachna</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinay</forename><surname>Bettadapura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">L</forename><surname>Sarin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irfan</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer assisted radiology and surgery</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="443" to="455" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Chained multi-stream networks exploiting pose, motion, and appearance for action classification and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nima</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Sedaghat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2904" to="2913" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
