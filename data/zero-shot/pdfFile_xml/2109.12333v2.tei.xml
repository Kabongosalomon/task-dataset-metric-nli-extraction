<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hard-sample Guided Hybrid Contrast Learning for Unsupervised Person Re-Identification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Hu</surname></persName>
							<email>huzheng95@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Laboratory of Advanced Information Networks Beijing Key Laboratory of Network System Architecture and Convergence</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhu</surname></persName>
							<email>czhu@bupt.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Laboratory of Advanced Information Networks Beijing Key Laboratory of Network System Architecture and Convergence</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Beijing Laboratory of Advanced Information Networks Beijing Key Laboratory of Network System Architecture and Convergence</orgName>
								<orgName type="institution">Beijing University of Posts and Telecommunications</orgName>
								<address>
									<postCode>100876</postCode>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hard-sample Guided Hybrid Contrast Learning for Unsupervised Person Re-Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Unsupervised Learning</term>
					<term>Person Re-ID</term>
					<term>Hard Sample</term>
					<term>Contrastive Learning, Pseudo Label</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised person re-identification (Re-ID) is a promising and very challenging research problem in computer vision. Learning robust and discriminative features with unlabeled data is of central importance to Re-ID. Recently, more attention has been paid to unsupervised Re-ID algorithms based on clustered pseudo-label. However, the previous approaches did not fully exploit information of hard samples, simply using cluster centroid or all instances for contrastive learning. In this paper, we propose a Hard-sample Guided Hybrid Contrast Learning (HHCL) approach combining cluster-level loss with instance-level loss for unsupervised person Re-ID. Our approach applies cluster centroid contrastive loss to ensure that the network is updated in a more stable way. Meanwhile, introduction of a hard instance contrastive loss further mines the discriminative information. Extensive experiments on two popular large-scale Re-ID benchmarks demonstrate that our HHCL outperforms previous state-of-the-art methods and significantly improves the performance of unsupervised person Re-ID. The code of our work and dataset are available soon at https://github.com/bupt-ai-cz/ HHCL-ReID.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Person Re-ID aims to identify the same person under different cameras views. It has been used extensively in largescale surveillance systems. Though great progress has been made in supervised person Re-ID tasks, the reliance on extensive manual annotation greatly constrains its application. Nevertheless, collecting pedestrian images without annotation is much cheaper and easier. Thus, increasing research attention has been drawn to unsupervised person Re-ID, directly learning from unlabeled data, which is more scalable and has more potential to deployments in the real world.</p><p>The extant unsupervised person re-ID methods can be broadly divided into two categories, unsupervised domain adaptation Re-ID methods and purely unsupervised Re-ID methods. The first type methods are based on unsupervised domain adaption (UDA) where the source domain dataset is fully annotated and the target domain is an unlabeled dataset. Most of these UDA-based methods address this task by learning the knowledge in the labeled source domain dataset and transferring them to the unlabeled target domain dataset <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b7">8]</ref>. The second type of unsupervised Re-ID method is pseudo-label-based fully unsuper-vised learning that directly learn from unlabeled data in the target domain and use representation features to estimate pseudo labels <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b8">9]</ref>. This method does not require any annotations and is more challenging. Existing fully unsupervised Re-ID works mainly aim to exploit pseudo labels from clustering and apply contrastive learning which has shown excellent performance in unsupervised representation learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>The performance of the unsupervised methods relies on feature representation learning. More recently, the Stateof-the-art method <ref type="bibr" target="#b10">[11]</ref> using a memory bank unit <ref type="bibr" target="#b27">[28]</ref> to store all instance features, treats each image as an individual class, and learns the representation by matching features of the same instance in different augmented views. However, each class usually contains more than one positive instance in Re-ID datasets. SpCL <ref type="bibr" target="#b8">[9]</ref> method alleviates this problem by matching an instance with the centroid of the multiple positives. To further ensure each positive converges to its centroid at a uniform pace, cluster contrast learning <ref type="bibr" target="#b3">[4]</ref> updates the memory dictionary and computes contrastive loss in the cluster level.</p><p>Although cluster contrast learning <ref type="bibr" target="#b3">[4]</ref> has achieved impressive performance, the method of applying contrastive learning only in the cluster level does not consider the the relationship between hard instances in the instance level. In fact, previous works in deep metric learning have focused on hard sample mining to lay more emphasis on hard samples inside a class. These methods aim to distinguish samples from different categories and bring samples from the same category closer together. However, these methods usually adopt a mini-batch-based deep metric loss, such as hard triplet loss <ref type="bibr" target="#b12">[13]</ref> and multi-similarity loss <ref type="bibr" target="#b24">[25]</ref>. Meanwhile, these losses only utilized a small portion of data without considering the information of all categories.</p><p>To learn discriminative feature representation for Re-ID and address the lack of adequately exploring information of hard samples, this paper introduces a novel hard-sample mining strategy and proposes a simple and effective method of hard-sample guided hybrid contrast learning for unsupervised Re-ID. In summary, this paper makes the following contributions:</p><p>? We propose a hybrid contrast learning framework for unsupervised person Re-ID which combines both cluster-level contrastive loss and instance-level contrastive loss.</p><p>? We introduce a novel hard instance mining strategy, which is based on an instance memory bank, to explore more discriminative information by selecting global hard samples online for each input instance.</p><p>? Extensive experiments on two popular large-scale Re-ID benchmarks demonstrate that our HHCL outper-forms previous state-of-the-art methods and significantly improves the performance of unsupervised person Re-ID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Unsupervised Re-ID</head><p>The domain adaptation strategy has been widely used for unsupervised person Re-ID tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>. The transfer-based methods follow the strategy of UDA, which uses the pretrained model in the labeled source domain dataset as the initialization of the target domain, or uses the style transfer method to transfer labeled images to the target domain. However, the UDA approach can be very challenging when the categories in the two domains are quite different. The drawback with pseudo-labels is that if the domains are not similar enough, it is not easy for us to obtain high quality pseudo labels, because the labeling noise might be too high to hurt the performance.</p><p>More recently, researchers have given more attention to pseudo-label-based methods that do not require source domain data. The pseudo labels can be generated by a pretrained classifier or by a feature similarity-based clustering algorithm, such as K-means, DB-SCAN <ref type="bibr" target="#b5">[6]</ref>. In this way, the pseudo labels are applied to fine-tuning the Re-ID model in a supervised manner. HCT <ref type="bibr" target="#b30">[31]</ref> combined hierarchical clustering with hard-batch triplet loss to improve the quality of pseudo labels. MMCL <ref type="bibr" target="#b21">[22]</ref> formulated unsupervised person re-ID as a multi-label classification task to progressively seek true labels. SpCL <ref type="bibr" target="#b8">[9]</ref> adopted the selfpaced contrastive learning strategy to form more reliable clusters. CACL <ref type="bibr" target="#b15">[16]</ref> designed an asymmetric contrastive learning framework to help the siamese network effectively mine the invariance in feature learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Mining Schemes</head><p>Sampling is a fundamental operation for reducing bias during model learning. Random sampling is one of the commonly used approaches, and different sampling methods are proposed to facilitate the learning of various loss functions. For the person re-ID task, identity sampling is widely used during the training stage, such as pair-wise sampling for contrastive loss and semi-hard negative mining method for triplet loss.</p><p>Hard sample mining is considered as a vital component of many deep metric learning algorithms <ref type="bibr" target="#b27">[28]</ref> to accelerate network convergence or to improve the final discriminative ability of the neural network because hard samples are more informative for training. The training should focus more on hard samples than easy samples. However, existing hard mining schemes of deep metric learning based on mini-batch training data often suffer from slow convergence, because they employ only one negative or partial negative example in mini-batch while not interacting with the other negative classes that have not been sampled into the current mini-batch in each update. In this paper, we propose a new strategy selecting the global hard samples from a memory bank for each input feature, to improve the model performance. Our hard mining strategy considers the relationship between each query instance and other clusters of different pseudo labels rather than taking into account only the inter-instance relationship with a small fraction of the categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>Given an unlabeled training set X = {x 1 , x 2 , ..., x n } consisting of n image samples, the goal is to learn ?(?; ?)-an encoder parameterized by ? used to extract features from input images. For inference, this encoder is applied to the gallery set X g = {x g 1 , x g 2 , ..., x g ng } and query set X q = {x q 1 , x q 2 , ..., x q nq }. The gallery set contains the total collection of retrieval images in the database and representations of the query images ?(x q i ; ?) are used to search the gallery set to retrieve the most similar matches to x q i according to Euclidean distance between the query and gallery embeddings, d(x q , x g ) = ||?(x q ; ?) ? ?(x g ; ?)||, where a smaller distance implies increased similarity between the images. Thus, feature representations of the same person are supposed to be as close as possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Architecture</head><p>Our hybrid contrast learning framework for fully unsupervised Re-ID consists of two main components: Cluster Centroid Contrastive Loss (CCCL) and Hard Instance Contrastive Loss (HICL). As shown in <ref type="figure" target="#fig_1">Fig.2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Hybrid Contrast Learning</head><p>To increase intra-class compactness and inter-class separability, state-of-the-art contrastive learning methods minimize the distance between samples of the same category and maximize the distance between samples of different categories with InfoNCE loss <ref type="bibr" target="#b20">[21]</ref>.</p><formula xml:id="formula_0">L q = E[? log exp(q ? k + )/? K i=1 exp(q ? k i )/? ],<label>(1)</label></formula><p>where q is an encoded query and k + is a positive feature which has the same label with q selected from a set of candidates k 1 , k 2 , ...k K . ? is a temperature hyper-parameter that controls the scale of similarities.</p><p>Comparing the non-parametric loss functions of different approaches based on the memory dictionary, the SSL <ref type="bibr" target="#b16">[17]</ref> considers each image as an individual instance and computes the loss and updates the memory dictionary both in the instance level so that all features of the training data need to be saved. To decrease memory usage and take full advantage of clustering outliers, SPCL <ref type="bibr" target="#b8">[9]</ref> computes the loss in cluster level but updates the memory dictionary in the instance level. However, the updating progress for each cluster is inconsistent due to the varying cluster size and randomness of sampling. ClusterNCE loss <ref type="bibr" target="#b3">[4]</ref> updates the feature vectors and computes the loss both in the cluster level. Although only a smaller storage space needs to be created to hold a cluster size amount of features for ClusterNCE, a single feature vector is not enough for a cluster representation. The averaged momentum representations calculated from all instances belonging to one cluster may lose the intraclass diversity. If updating cluster representation with only an instance feature, would introduce more biases because of noisy pseudo labels generated by unsupervised clustering.</p><p>Thus, we proposed a new unsupervised Re-ID framework that combines cluster-level loss with instance-level loss. The overall loss function of our method is as follow:</p><formula xml:id="formula_1">L ReID = ?L cls + (1 ? ?)L ins ,<label>(2)</label></formula><p>where ? is a balancing factor and we set ? = 0.5 by default. In the following, we will detail the objective function Eq. <ref type="formula" target="#formula_1">(2)</ref>. Cluster Centroid Contrastive Loss Some instancelevel memory dictionary techniques, such as <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8]</ref> maintaining each instance feature of the dataset and update corresponding memory dictionary with its own instance features in each mini-batch, have the problem of memory updating consistency <ref type="bibr" target="#b3">[4]</ref>. Since different instances within the same cluster will have different updating states. In every training iteration, due to the unbalanced distribution of cluster size, a smaller cluster could have a higher proportion of instances updated than a larger cluster. Unlike the previous instancelevel memory dictionary, we use cluster-level memory dictionary M cls to keep one cluster feature for each cluster instead of preserving every instance feature. The corresponding memory dictionary is updated regardless of whether the clusters are large or small, ensuring updating consistency of features within the same cluster.</p><formula xml:id="formula_2">L cls = E[? log exp(&lt; q ? c + &gt;)/? c C i=1 exp(&lt; q ? c i &gt;)/? c ],<label>(3)</label></formula><p>where C is the number of clusters in a training epoch and ? c is a temperature hyper-parameter. Different from unified contrastive loss, outliers are dropped out. We calculate cluster centroids c 1 , c 2 , ...c C and store them in a memory for the cluster centroid contrastive loss. We update the cluster memory bank as follows:</p><formula xml:id="formula_3">c i ? ?c i + (1 ? ?)c i ,<label>(4)</label></formula><p>wherec i is the average of i-th class instance features in the mini-batch.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Memory Based Hard Mining Scheme</head><p>To further distinguish easily confused sample pairs and explore the inter-instance relationship, we propose a novel hard sample mining strategy based on a memory dictionary. We construct another memory-based dictionary M ins to store P ? C instance features, which contains C pseudo identities and each identity has P instances. As shown in <ref type="figure" target="#fig_0">Fig.1</ref>, unlike traditional hard mining strategies such as hard triplet loss <ref type="bibr" target="#b12">[13]</ref>, which is based on pairwise loss calculating the distance of the hardest positive and the hardest negative instances within a mini-batch, our proposed method is based on all pseudo-labeled categories and contains C ? 1 negative samples for each query. Our hard mining strategy considers the relationship between each query instance and other clusters of different pseudo labels rather than taking into account only the inter-instance relationship with a small fraction of the categories.</p><p>For the same query, we construct C sample pairs which include one positive pair and C ? 1 hard negative pairs. We define hard instance contrastive loss as follows:</p><formula xml:id="formula_4">L ins = E[? log exp(&lt; q ? z + hard &gt;)/? ins C i=1 exp(&lt; q ? z i hard &gt;)/? ins ],<label>(5)</label></formula><p>where ? ins is an instance temperature hyper-parameter, z + hard is the hard positive instance feature that has the lowest cosine similarity with query q within the same cluster, and z i hard is hard negative instance feature that has the highest cosine similarity that belongs to i-th class. They are respectively defined as z + hard = argmin(&lt; q ? z + k &gt;), k = 1, ..., K;</p><p>z i hard = argmax(&lt; q ? z i k &gt;), k = 1, ..., K.</p><p>Similarly, to ensure memory updating consistency, all instance features of the corresponding K identities in the minibatch are updated in each training iteration. We update the instance memory bank as follows:</p><formula xml:id="formula_7">m i k ? z i k ,<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Data and Metrics</head><p>We evaluate our approach on two large-scale benchmark datasets: Market1501 <ref type="bibr" target="#b32">[33]</ref>, and DukeMTMC-reID <ref type="bibr" target="#b34">[35]</ref> which are widely used real-world person Re-ID tasks.</p><p>Market1501 contains 1,501 person identities with 32,668 images which are captured by 6 cameras in front of the Tsinghua University campus. It contains 12,936 images of 751 identities for training and 19,732 images of 750 identities for testing. All of the images were cropped by a pedestrian detector which inevitably introduced little misalignment, part missing and false positives. Evaluation Metrics. We followed the standard training/test split and evaluation protocol to evaluate the performance of our method. For the evaluation metrics, we used the Rank-k (for k = 1, 5, and 10) matching accuracy, which means the query picture has the match in the top-k list. And we use the mean Average Precision (mAP), which is computed from the Cumulated Matching Characteristics (CMC) <ref type="bibr" target="#b9">[10]</ref>. Moreover, results reported in this paper are under the single-query setting, and no post-processing technique is applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation</head><p>We adopt ResNet-50 <ref type="bibr" target="#b11">[12]</ref> as the backbone of the feature extractor and initialize the model with the parameters pre-trained on ImageNet <ref type="bibr" target="#b4">[5]</ref>. After layer-4, we remove all sub-module layers and add global average pooling (GAP) followed by batch normalization layer <ref type="bibr" target="#b13">[14]</ref> and L2normalization layer, which will produce 2048-dimensional features. During testing, we take the features of the global average pooling layer to calculate the distance. For the beginning of each epoch, we use DB-SCAN <ref type="bibr" target="#b5">[6]</ref> for clustering to generate pseudo labels. The input image is resized 256 ? 128. For training images, we perform random horizontal flipping, padding with 10 pixels, random cropping, and random erasing. Each mini-batch contains 256 images of 16 pseudo person identities (16 instances for each person). We adopt Adam optimizer to train the Re-ID model with weight decay 5e-4. The initial learning rate is set to 3.5e-4, and is reduced to 1/10 of its previous value every 20 epoch in a total of 50 epoch. As the same with the cluster method of <ref type="bibr" target="#b8">[9]</ref> paper, we use DB-SCAN and Jaccard distance <ref type="bibr" target="#b35">[36]</ref> to cluster with k nearest neighbors, where k = 30. For DB-SCAN, the maximum distance d between two samples is set as 0.45 and the minimal number of neighbors in a core point is set as 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1">Comparison with unsupervised method</head><p>We compare our proposed method with state-of-theart ReID methods including: 1) the unsupervised domain adaptation methods for person Re-ID(e.g. ECN <ref type="bibr" target="#b36">[37]</ref>, MAR <ref type="bibr" target="#b36">[37]</ref>, SSG <ref type="bibr" target="#b6">[7]</ref>, MMCL <ref type="bibr" target="#b21">[22]</ref>, JVTC <ref type="bibr" target="#b14">[15]</ref>, DG-Net++ <ref type="bibr" target="#b40">[40]</ref>, ECN+ <ref type="bibr" target="#b37">[38]</ref>, MMT <ref type="bibr" target="#b7">[8]</ref>, DCML <ref type="bibr" target="#b0">[1]</ref>, MEB <ref type="bibr" target="#b31">[32]</ref>, SpCL <ref type="bibr" target="#b8">[9]</ref>; 2) the purely unsupervised methods for person Re-ID SSL <ref type="bibr" target="#b8">[9]</ref>, MMCL <ref type="bibr" target="#b21">[22]</ref>, JVTC <ref type="bibr" target="#b14">[15]</ref>, HCT <ref type="bibr" target="#b30">[31]</ref>, CycAs <ref type="bibr" target="#b25">[26]</ref>, SpCL <ref type="bibr" target="#b8">[9]</ref>, CAP <ref type="bibr" target="#b23">[24]</ref>, CACL <ref type="bibr" target="#b15">[16]</ref>, CCL <ref type="bibr" target="#b3">[4]</ref> and ICE <ref type="bibr" target="#b1">[2]</ref>). The comparison results of the state-of-the-art unsupervised domain adaptation methods and purely unsupervised methods on Market-1501 and DukeMTMC-reID are reported in Tab. 1.</p><p>As shown in Tab.1, we observe our method is competitive with all the state-of-the-art methods. On the three datasets, our proposed HHCL without any identity annotation achieves better performance than all of UDA methods that use of the additional labeled source dataset. It can be found that we not only perform better than all unsupervised domain adaptation methods and also achieve competitive performance with purely unsupervised methods. Under the fully unsupervised setting, HHCL achieves 84.2% in mAP and 93.4% in rank-1 accuracy on Market-1501, which is 1.9% higher than the current state of the art (ICE <ref type="bibr" target="#b3">[4]</ref>). On DukeMTMC-reID, our method also achieves a high performance of 73.3/85.1% in mAP/rank-1. These results indicate that our method is effective for unsupervised person Re-ID learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2">Comparison with supervised method</head><p>Our HHCL method can be easily implemented as a supervised approach when we replace the pseudo-labels with ground truth. We further find that our proposed unsupervised method is already comparable to some excellent su- pervised methods, such as PCB <ref type="bibr" target="#b19">[20]</ref> and DG-Net <ref type="bibr" target="#b33">[34]</ref>, when ground truth is not used. And our HHCL even achieves a better performance under supervised setting. This result shows that our proposed method achieves better results when using the ground truth to avoid introducing noisy pseudo-labels. And it also further demonstrates the effectiveness of our method for the person Re-ID problem, both unsupervised and supervised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Ablation Study</head><p>Influence of Hyper-Parameter ? Tab. 2 reports the experiment result under different value of hyper-parameter?. As mentioned in 2, ? is a balancing factor between 0 and 1, which plays an important role in affecting the weights of the cluster-level loss and instance-level loss. When ? is equal to 0, the loss function contains only the hard instance contrastive loss. From the <ref type="figure" target="#fig_3">fig.3</ref>. we can find that the model converges very slowly in the early stage of the training pro-cess, and using only the hard samples for comparison is not benefit for learning generalized features and obtaining better clustering pseudo labels. On the contrary, when ?=1 and cluster-level loss only is used, although a faster convergence can be achieved, only one feature is retained for each cluster, which loses the diversity of intra class and is still not conducive to facilitating the network to learn more discriminative features. It can be seen that combining both kind of contrastive loss leads to better performance obviously. And when ? = 0.5, we get the best performance 84.2% in mAP, indicating that our proposed hybrid contrastive learning method has a distinct advantage over others during the training process.   <ref type="table">Table 3</ref>. Comparison of HHCL with other tricks on Market1501 and DukeMTMC-reID datasets. 'IBN' denotes that the backbone applies IBN-ResNet50. 'GeM' and 'LS' represent GeM pooling layer and label smoothing respectively.</p><p>Instance-batch normalization (IBN) <ref type="bibr" target="#b17">[18]</ref> and Generalized Mean Pooling (GeM) <ref type="bibr" target="#b18">[19]</ref> has been proved effective in both supervised and UDA based Re-ID methods. We compare the performance of HCCL under different settings in Tab.3. The performance of our proposed HHCL can be further improved with an IBN-ResNet50 backbone network and GeM pooling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a novel method for the fully unsupervised person re-ID. The new concepts and techniques introduced include a more efficient hybrid contrast learning framework and a memory based hard sample mining scheme. Specifically, our proposed HHCL approach comprehensively consider both of cluster level and instance level information. For effectively exploiting the invariance within and between clusters, HHCL leverages hard samples to guide network to learn more robust and discriminative features. Extensive experiments on two benchmark datasets demonstrated that HHCL achieves the best results comparing with all existing purely unsupervised and UDA-based Re-ID methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Hard-sample guided hybrid contrast learning. According to the features saved in the memory bank, we calculate clusterlevel contrastive loss and hard instance-level contrastive loss, respectively. (a) Cluster centroid leads the optimization trend of features, resulting in features belonging to the same cluster being more compact and strengthen identity similarity. (b) Hard instance contrastive loss compares input sample with hard positive that belong to the same cluster and hard negative instances from other clusters, thereby learning to distinguish easily confusing samples. (Best viewed in color)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Hybrid Contrast Learning Framework. 1) Initialization: clustering algorithm divides all features extracted from the training set into different clusters as pseudo labels and initialize instance memory bank and cluster centroid memory bank; 2) forward propagate: calculate cluster contrastive loss between the input and the clustering centroids and the hard instance contrastive loss of the hard samples selected by hard mining strategy respectively; 3) back propagate and update the encoder model; 4) update instance memory bank and cluster centroid memory bank.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 : 5 for iter = 1 to B do 6 Sample 7 Forward to extract the features of the samples; 8 9 Backward to update model ? ? ; 10 Update</head><label>15678910</label><figDesc>Hard-sample Guided Hybrid Contrast Learning for Unsupervised Re-Identification Data: An unlabeled training set X Input: ImageNet pre-trained model ?(?; ?), the iteration number N , the training batch size B. Result: trained model ?(?; ?) 1 for epoch = 1 to N do 2 Extract feature embedding x from X by ?(?; ?); 3 Clustering x into c clusters with DB-Scan; 4 Initialize cluster memory bank M cls and hard instance memory bank M ins with Eq.1; P ? K mini-batch images from X ; Compute the total loss L ReID in Eq.(2) which combines cluster centroid contrastive loss L cls Eq.(3) and hard instance contrastive loss L ins Eq.(5); cluster memory M cls and instance memory M ins via Eq.(4) and Eq.(8); consists a total of 36,411 images of people from 1404 different identities collected by 8 cameras. Specifically, The dataset is split by randomly selecting 702 identities as the training set and 702 identities as the testing set. it contains 16,522 images for training, 2,228 query images and 17,661 gallery images for testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Ablation study on Market1501: Result comparisons of different settings in mAP and Rank-1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Feature embedding Cluster Centroid Memory Instance Memory Back Propagate Back Propagate UpdateHard Samples Select Pseudo Labels Update Mini Batch Samples</head><label></label><figDesc></figDesc><table><row><cell>Flow</cell></row><row><cell>Back Propagate</cell></row><row><cell>Cluster Centroid Feature</cell></row><row><cell>Instance Embedding Feature</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Experimental results of the proposed HHCL and state-of-the-art methods on Market-1501 and DukeMTMC-reID. Note that the best results are bolded.</figDesc><table><row><cell>Method</cell><cell>Reference</cell><cell cols="3">Market1501 mAP R1 R5</cell><cell cols="4">DukeMTMC-reID R10 mAP R1 R5</cell><cell>R10</cell></row><row><cell cols="2">Unsupervised Domain Adaptation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ECN [37]</cell><cell>CVPR'19</cell><cell>43.0</cell><cell cols="4">75.1 87.6 91.6 40.4</cell><cell cols="3">63.3 75.8 80.4</cell></row><row><cell>MAR[30]</cell><cell>CVPR'19</cell><cell>40.0</cell><cell cols="2">67.7 81.9</cell><cell>-</cell><cell>48.0</cell><cell cols="2">67.1 79.8</cell><cell>-</cell></row><row><cell>SSG[7]</cell><cell>ICCV'19</cell><cell>58.3</cell><cell cols="4">80.0 90.0 92.4 53.4</cell><cell cols="3">73.0 80.6 83.2</cell></row><row><cell>MMCL [22]</cell><cell>CVPR'20</cell><cell>60.4</cell><cell cols="4">84.4 92.8 95.0 51.4</cell><cell cols="3">72.4 82.9 85.0</cell></row><row><cell>JVTC [15]</cell><cell>ECCV'20</cell><cell>61.1</cell><cell cols="4">83.8 93.0 95.2 56.2</cell><cell cols="3">75.0 85.1 88.2</cell></row><row><cell cols="2">DG-Net++ [40] ECCV'20</cell><cell>61.7</cell><cell cols="4">82.1 90.2 92.7 63.8</cell><cell cols="3">78.9 87.8 90.4</cell></row><row><cell>ECN+ [38]</cell><cell>PAMI'20</cell><cell>63.8</cell><cell cols="4">84.1 92.8 95.4 54.4</cell><cell cols="3">74.0 83.7 87.4</cell></row><row><cell>MMT [8]</cell><cell>ICLR'20</cell><cell>71.2</cell><cell cols="4">87.7 94.9 96.9 65.1</cell><cell cols="3">78.0 88.8 92.5</cell></row><row><cell>DCML [1]</cell><cell>ECCV'20</cell><cell>72.6</cell><cell cols="4">87.9 95.0 96.7 63.3</cell><cell cols="3">79.1 87.2 89.4</cell></row><row><cell>MEB [32]</cell><cell>ECCV'20</cell><cell>76.0</cell><cell cols="4">89.9 96.0 97.5 66.1</cell><cell cols="3">79.6 88.3 92.2</cell></row><row><cell>SpCL [9]</cell><cell cols="2">NeurIPS'20 76.7</cell><cell cols="4">90.3 96.2 97.7 68.8</cell><cell cols="3">82.9 90.1 92.5</cell></row><row><cell cols="2">Fully Unsupervised</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SSL [17]</cell><cell>CVPR'20</cell><cell>37.8</cell><cell cols="4">71.7 83.8 87.4 28.6</cell><cell cols="3">52.5 63.5 68.9</cell></row><row><cell>JVTC [15]</cell><cell>ECCV'20</cell><cell>41.8</cell><cell cols="4">72.9 84.2 88.7 42.2</cell><cell cols="3">67.6 78.0 81.6</cell></row><row><cell>MMCL [22]</cell><cell>CVPR'20</cell><cell>45.5</cell><cell cols="4">80.3 89.4 92.3 40.2</cell><cell cols="3">65.2 75.9 80.0</cell></row><row><cell>HCT [31]</cell><cell>CVPR'20</cell><cell>56.4</cell><cell cols="4">80.0 91.6 95.2 50.7</cell><cell cols="3">69.6 83.4 87.4</cell></row><row><cell>CycAs [26]</cell><cell>ECCV'20</cell><cell>64.8</cell><cell>84.8</cell><cell>-</cell><cell>-</cell><cell>60.1</cell><cell>77.9</cell><cell>-</cell><cell>-</cell></row><row><cell>SpCL [9]</cell><cell cols="2">NeurIPS'20 73.1</cell><cell cols="4">88.1 95.1 97.0 65.3</cell><cell cols="3">81.2 90.3 92.2</cell></row><row><cell>CAP [24]</cell><cell>AAAI'21</cell><cell>79.2</cell><cell cols="4">91.4 96.3 97.7 67.3</cell><cell cols="3">81.1 89.3 91.8</cell></row><row><cell>CACL [16]</cell><cell>Arxiv'21</cell><cell>80.9</cell><cell cols="4">92.7 97.4 98.5 69.6</cell><cell cols="3">82.6 91.2 93.8</cell></row><row><cell>ICE[2]</cell><cell>ICCV' 21</cell><cell>82.3</cell><cell cols="4">93.8 97.6 98.4 69.9</cell><cell cols="3">83.3 91.5 94.1</cell></row><row><cell>CCL[4]</cell><cell>Arxiv'21</cell><cell>82.6</cell><cell cols="4">93.0 97.0 98.1 72.8</cell><cell cols="3">85.7 92.0 93.5</cell></row><row><cell>HHCL</cell><cell>This paper</cell><cell>84.2</cell><cell cols="4">93.4 97.7 98.5 73.3</cell><cell cols="3">85.1 92.4 94.6</cell></row><row><cell>Supervised</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PCB[20]</cell><cell>ECCV'18</cell><cell>81.6</cell><cell cols="7">93.8 97.5 98.5 69.2 83.3 90.5 92.5</cell></row><row><cell>OSNet [39]</cell><cell>ICCV' 19</cell><cell>84.9</cell><cell>94.8</cell><cell>-</cell><cell>-</cell><cell>73.5</cell><cell>88.6</cell><cell>-</cell><cell>-</cell></row><row><cell>DG-Net [34]</cell><cell>CVPR'19</cell><cell>86.0</cell><cell>94.8</cell><cell>-</cell><cell>-</cell><cell cols="2">74.8 86.6</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">ICE [2] (w/ GT) ICCV' 21</cell><cell>86.6</cell><cell cols="4">95.1 98.3 98.9 76.5</cell><cell cols="3">88.2 94.1 95.7</cell></row><row><cell>HHCL(w/ GT)</cell><cell>This paper</cell><cell>87.2</cell><cell cols="4">94.6 98.5 99.1 80.0</cell><cell cols="3">89.8 95.2 96.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Evaluation of parameter ? on Market1501.</figDesc><table><row><cell>?</cell><cell>Market1501 mAP R1 R5</cell><cell>R10</cell></row><row><cell>0(hard)</cell><cell cols="2">78.5 90.5 96.0 97.4</cell></row><row><cell>0.25</cell><cell cols="2">82.7 92.9 97.0 98.2</cell></row><row><cell>0.5</cell><cell cols="2">84.2 93.4 97.7 98.5</cell></row><row><cell>0.75</cell><cell cols="2">81.7 92.1 97.1 98.2</cell></row><row><cell cols="3">1.(mean) 80.8 91.3 96.3 97.6</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by 111 Project of China (B17007), and in part by the National Natural Science Foundation of China (61602011).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep credible metric learning for unsupervised domain adaptation person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Ice: Interinstance contrastive encoding for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Lagadec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Br?mond</surname></persName>
		</author>
		<idno>abs/2103.16364</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/2002.05709</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Cluster contrast for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuozhuo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<idno>abs/2103.11568</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A density-based algorithm for discovering clusters in large spatial databases with noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Self-similarity grouping: A simple unsupervised cross domain adaptation approach for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanshuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humphrey</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6111" to="6120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Mutual mean-teaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Self-paced contrastive learning with hybrid memory for domain adaptive object re-id. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Evaluating appearance models for recognition, reacquisition, and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Brennan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Tao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9726" to="9735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kaiming He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno>abs/1703.07737</idno>
		<title level="m">defense of the triplet loss for person re-identification. ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint visual and temporal consistency for unsupervised domain adaptive person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Cluster-guided asymmetric contrastive learning for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Guang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Guo</surname></persName>
		</author>
		<idno>abs/2106.07846</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification via softened similarity learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3387" to="3396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Two at once: Enhancing learning and generalization capacities via ibn-net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Finetuning cnn image retrieval with no human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1655" to="1668" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Beyond part models: Person retrieval with refined part pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno>abs/1807.03748</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unsupervised person reidentification via multi-label classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongkai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiliang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10981" to="10990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards unsupervised open-set person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="769" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Xiaojin Gong, and Xiansheng Hua. Camera-aware proxies for unsupervised person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-similarity loss with general pair weighting for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengke</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5017" to="5025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cycas: Selfsupervised cycle association for learning re-identifiable descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongdao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="72" to="88" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XI 16</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3733" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instancelevel discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<idno>abs/1805.01978</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-view asymmetric metric learning for unsupervised person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="994" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised person re-identification by soft multilabel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Xing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2143" to="2152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hierarchical clustering with hard-batch triplet loss for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwei</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munan</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13657" to="13665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Multiple expert brainstorming for domain adaptive person re-identification. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxi</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Scalable person re-identification: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liyue</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1116" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Joint discriminative and generative learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2133" to="2142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unlabeled samples generated by gan improve the person re-identification baseline in vitro</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhedong Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3774" to="3782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Reranking person re-identification with k-reciprocal encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3652" to="3661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Invariance matters: Exemplar memory for domain adaptive person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="598" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to adapt invariance in memory for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhun</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaozi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yezhou</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="2723" to="2738" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Omni-scale feature learning for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3701" to="3711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Joint disentangling and adaptation for cross-domain person re-identification. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
