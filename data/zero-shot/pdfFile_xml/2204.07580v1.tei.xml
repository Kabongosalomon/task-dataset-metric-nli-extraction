<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">mGPT: Few-Shot Learners Go Multilingual</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleh</forename><surname>Shliazhko</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sber</forename><surname>Sberdevices</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhailov</forename><surname>Vladislav</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Sber</roleName><surname>Sberdevices</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alena</forename><forename type="middle">Fenogenova</forename><surname>Sberdevices</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sber</forename><surname>Anastasia</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Sber</roleName><forename type="first">Kozlova</forename><surname>Sberdevices</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Sber</roleName><forename type="first">Maria</forename><forename type="middle">Tikhonova</forename><surname>Sberdevices</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">HSE University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Sber AI Research Institute (AIRI)</orgName>
								<orgName type="institution">HSE University Tatiana Shavrina SberDevices</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">mGPT: Few-Shot Learners Go Multilingual</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:54+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies report that autoregressive language models can successfully solve many NLP tasks via zero-and few-shot learning paradigms, which opens up new possibilities for using the pre-trained language models. This paper introduces two autoregressive GPT-like models with 1.3 billion and 13 billion parameters trained on 60 languages from 25 language families using Wikipedia and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using GPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron frameworks allow us to parallelize the training and inference steps effectively. The resulting models show performance on par with the recently released XGLM models by Facebook, covering more languages and enhancing NLP possibilities for low resource languages of CIS countries and Russian small nations. We detail the motivation for the choices of the architecture design, thoroughly describe the data preparation pipeline, and train five small versions of the model to choose the most optimal multilingual tokenization strategy. We measure the model perplexity in all covered languages, and evaluate it on the wide spectre of multilingual tasks, including classification, generative, sequence labeling and knowledge probing. The models were evaluated with the zero-shot and few-shot methods. Furthermore, we compared the classification tasks with the state-of-the-art multilingual model XGLM. The source code and the mGPT XL model are publicly released.</p><p>Giusepppe Attardi. 2015. Wikiextractor. https:// github.com/attardi/wikiextractor.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The advent of the Transformer architecture <ref type="bibr" target="#b4">(Vaswani et al., 2017)</ref> has facilitated the development of various monolingual and multilingual language models (LMs; <ref type="bibr">Liu et al., 2020a;</ref><ref type="bibr">Doddapaneni et al., 2021)</ref>. Although the well-established "pre-train &amp; fine-tune" paradigm * Corresponding author olehshliazhko@gmail.com has led to rapid progress in NLP <ref type="bibr" target="#b5">(Wang et al., 2019)</ref>, it imposes several limitations. First, fine-tuning relies on an extensive amount of labeled data. Collecting high-quality labeled data for new tasks and languages is expensive, timeand resource-consuming <ref type="bibr" target="#b7">(Wang et al., 2021)</ref>. Second, LMs can learn spurious correlations from fine-tuning data <ref type="bibr">(Naik et al., 2018;</ref><ref type="bibr">Niven and Kao, 2019)</ref> and demonstrate inconsistent generalization, catastrophic forgetting or brittleness to fine-tuning data order <ref type="bibr">(McCoy et al., 2020;</ref><ref type="bibr">Dodge et al., 2020)</ref>. Last but not least, fine-tuning requires additional computational resources and, therefore, aggravates the problem of a large carbon footprint <ref type="bibr">(Bender et al., 2021)</ref>.</p><p>Latest approaches address the limitations with zero-and few-shot learning, that is, performing a new task by maximizing an LM scoring function or using a small amount of in-context examples without parameter updates <ref type="bibr">(Brown et al., 2020)</ref>. Autoregressive LMs adopted via these paradigms have been widely applied in many NLP tasks <ref type="bibr">(Schick and Sch?tze, 2021;</ref><ref type="bibr">Perez et al., 2021)</ref>, most notably in cross-lingual transfer <ref type="bibr" target="#b10">(Winata et al., 2021)</ref> and low-resource language scenarios <ref type="bibr" target="#b10">(Lin et al., 2021)</ref>. However, there are several less explored directions in the interconnected areas of zero/few-shot learning and multilingual NLP, including model development for typologically distant and low-resource languages underrepresented in LMs' pre-training corpora <ref type="bibr" target="#b12">(Wu and Dredze, 2020;</ref><ref type="bibr">Lauscher et al., 2020;</ref><ref type="bibr">Hedderich et al., 2021)</ref>, and multilingual zero/fewshot capabilities of autoregressive LMs <ref type="bibr">(Erdem et al., 2022)</ref>.</p><p>This paper expands such concepts to the multilingual setting and introduces a family of mGPT models of 1.3 billion and 13 billion parameters trained on 60 languages from 25 language families using Wikipedia and Colossal Clean Crawled Corpus (C4) <ref type="bibr">(Raffel et al., 2020)</ref>. We have measured the models' perplexities on all covered languages and evaluated its in-context learning and pattern recognition abilities on a multitude of multilingual NLP tasks, including ones from XGLUE Benchmark, <ref type="bibr">XWINO, XCOPA, etc. (Liang et al., 2020;</ref><ref type="bibr" target="#b2">Tikhonov and Ryabinin, 2021a;</ref><ref type="bibr">Ponti et al., 2020)</ref> with "zero-shot" and "few-shot" methods. We also performed knowledge probing on the mLAMA dataset <ref type="bibr">(Kassner et al., 2021b)</ref> and assessed generative capabilities for a low resource CIS and small nations' languages with perplexity score. We describe the data preparation and cleaning pipeline and train five small mGPT models (163M parameters) to choose the most optimal multilingual tokenization strategy.</p><p>We are publicly releasing the code and weights of 1.3B model to facilitate the research on the applicability of autoregressive LMs in languages other than English.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A wide range of works is dedicated to expanding the possibilities and application range of language models, including such emerging areas as multilingual, multitask, and even multimodal extensions. While works like <ref type="bibr">(Jain et al., 2021)</ref> capture new domains, we focus on the textual modality and the development of model performance in the problem of multilingual generalization.</p><p>Multilingual Transformers Several studies have introduced multilingual versions of the transformer models initially designed for English. Multilingualism allows testing the models in more challenging settings, such as zero-shot crosslingual transfer, in which task-specific supervision in a source language is used to fine-tune the model for evaluation in a target language <ref type="bibr">(Pires et al., 2019)</ref>. Despite the differences in the architecture design and pre-training objectives, mBERT <ref type="bibr">(Devlin et al., 2018)</ref>, mBART <ref type="bibr">(Liu et al., 2020b)</ref> and mT5 <ref type="bibr">(Xue et al., 2021)</ref> models have pushed state-of-the-art results on many NLP tasks in multiple languages <ref type="bibr">(Conneau et al., 2018b;</ref><ref type="bibr" target="#b0">Artetxe et al., 2020)</ref>.</p><p>GPT-based Models In spite of the fact that the "pre-train &amp; fine-tune" paradigm remains the dominant approach in the field, novel learning concepts and methods such as "zero-shot", "one-shot" and "few-shot" techniques have been designed to balance the cost of pre-training extensive LMs and accelerate their acquisition of new skills with less supervision towards the development of general AI <ref type="bibr">(Chollet, 2019)</ref>. GPT-2 and GPT-3 models have achieved impressive performance with the "fewshot" method on various downstream tasks given a tiny amount of supervision. While this approach allows avoiding additional training and expands the application of LMs, it still can lead to inconsistent results caused by the imbalance of classes in the provided supervision, shifts in frequency distributions, sub-optimal prompts, and even word order. However, this disadvantage can be overcome via a more optimal text interaction with the model <ref type="bibr" target="#b19">(Zhao et al., 2021)</ref>.</p><p>As the original source code and checkpoints of the GPT-3 model are not publicly unavailable, the research community has put a joint effort towards the task of re-creating the model. In parallel with this work, EleutherAI provides freely available transformer models 1 designed with replication of the GPT-3 architecture: GPT-Neo 1.3B and GPT-Neo 2.7B. The models were trained on the 825GB Pile dataset <ref type="bibr">(Gao et al., 2020)</ref> which contains 22 subsets from diverse domains <ref type="bibr">(including Wikipedia,</ref><ref type="bibr">Github,</ref><ref type="bibr">Books3,</ref><ref type="bibr">etc.)</ref>. The authors reproduce the GPT-3 architecture using Tensorflow Mesh framework for the model training and data parallelism 2 . Despite being publicly released, the models support only the English language.</p><p>Both multilingual and autoregressive methods are fruitfully embedded in the XGLM model <ref type="bibr" target="#b10">(Lin et al., 2021)</ref>. The authors presented five models of different sizes. The largest model in a row (7.5 billion parameters) is capable of transferring knowledge and tasks in over 30 languages. The work mostly focused on few-shot and zero-shot applications on classification tasks, some of them outperforming bigger monolingual models. Since there are not many references yet in the field of multilingual transformers, we will further compare our results with this work, namely the XGLM model of size 1.7B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section we describe the chosen model architectures, dataset collection, preparation and various aspects of training of large autoregressive models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Models and Training</head><p>Since the source code of the GPT-3 model and the model weights are not publicly released, we use the available information on the architecture description <ref type="bibr">(Brown et al., 2020)</ref> and the source code of the previous version, GPT-2 model <ref type="bibr">(Radford et al., 2019)</ref> replicated in HuggingFace Transformers <ref type="bibr">(Wolf et al., 2019) 3 and</ref><ref type="bibr">Nvidia Megatron-LM(Shoeybi et al., 2019)</ref> </p><formula xml:id="formula_0">4 .</formula><p>Comparison of the GPT-2 and GPT-3 architectures with the comparable number of parameters (see <ref type="table" target="#tab_0">Table 1)</ref> shows that, with all other hyperparameters being equal, the latest version (GPT-3) has fewer layers (Layers: 48 -&gt; 24), but the context window has become wider (Context: 1600 -&gt; 2048 tokens). In addition, GPT-3 adds the alternation of the classic dense and sparse attention mechanisms <ref type="bibr">(Child et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Params Layers Context GPT-2 1.5B 48 1600 GPT-3 XL 1.3B 24 2048 GPT-3 13B 13B 40 5120 The training procedure mostly follows <ref type="bibr">(Brown et al., 2020)</ref>. To reproduce the model's architecture, we use the implementation of the GPT-2 model code by Megatron-LM with desired parameters of sequence length, number of layers and context size. Our model also includes the sparse attention mechanism from DeepSpeed library 5 implementing alternating layers with dense and sparse attention. We train with total batch size of 2048 and the context window of 512 tokens for the first 400 000 steps. After that, we additionally train the model on the updated cleaned version of the dataset for <ref type="formula">200</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data</head><p>Training extensive multilingual models requires large volumes of data. The explosive growth of web corpora has allowed neural network methods to revolutionize, with the data volume starting from 1.6 billion words <ref type="bibr">(Mikolov et al., 2013)</ref> and now exceeding 6 trillion words <ref type="bibr">(Xue et al., 2020)</ref>. Training the mGPT model is based on a register variation approach: it is essential to limit the list of languages and resources, consistently curate them, and then carefully clean the collected texts to receive a representative and linguistically diverse training corpora of high quality that covers multiple domains (genres, web-sites, authorship info, etc.).</p><p>To this end, we limited the list of considered languages and collected the data from Wikipedia and C4 for each of them (see Section 3.2). The list consists of typologically weighted sample that covers languages from standard multilingual benchmarks such as XGLUE <ref type="bibr">(Liang et al., 2020)</ref>, XTREME <ref type="bibr">(Hu et al., 2020), and</ref><ref type="bibr">XNLI (Conneau et al., 2018b)</ref>. In addition, we included 20 languages from the tail of the list of Common Crawl languages and the list of minor languages of <ref type="bibr">Russia(Orekhov et al., 2016)</ref>, as well as the official and minor languages spoken in the countries of the former USSR <ref type="bibr">(Orekhov et al., 2016)</ref>. The resulting list consists of 60 languages from 25 language families. The list of the languages grouped by the family is outlined in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Preparation Pipeline</head><p>The Common Crawl and Wikipedia data was collected and processed as follows:</p><p>? Data Collection: C4 was obtained using standard scripts from Tensorflow datasets 6 , with the manually patched list of languages. The C4 download time was limited to two weeks. The Wikipedia plain texts were extracted from the dumps 7 with the help of WikiExtractor (Attardi, 2015).</p><p>? De-duplication: The primary de-duplication was conducted using 64-bit hashing of each text in the corpus, leaving texts with a unique hash.</p><p>? Filtration: C4 documents were filtered by entropy measured through text compression rate Further exploration of the dataset highlights the necessity of more thorough filtration. We decide to extend the C4 dataset and clean it using a set of language-agnostic heuristics.</p><p>? Dataset Statistics: The resulting size of the corpora after all preparation steps is 442 billion UTF characters (C4) and 46 billion UTF characters (Wikipedia). Detailed statistics on the number of documents and characters is shown in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Tokenizer Selection</head><p>We decide to choose tokenization approach based on a comparative evaluation of the generative models with different tokenization strategies. We've trained a bunch of small GPT models on different tokenizations of the subset of original dataset. Then we compare resulting models based on their character perplexity. We inferred the perplexity of an input text as follows <ref type="bibr">(Cotterell et al., 2018;</ref><ref type="bibr">Lau et al., 2020)</ref>:</p><formula xml:id="formula_1">P P L(t) = exp(? 1 |c| |t| i=0 log p ? (x i |x &lt;i )) (1)</formula><p>where t is an input text, |t| is the length of the text in tokens, |c| is the length of the text in characters, and log p ? (x i |x &lt;i ) is the log-likelihood of the i-th token in t conditioned on the preceding ones. The evaluation results are presented in <ref type="table" target="#tab_3">Table  2</ref>, where the score is averaged over the number of languages. We aware that different tokenizers split the same string to a different number of tokens, so to be comparable, perplexity was normalized to a number of characters in the original string, not a number of tokens.  The DEFAULT model achieved the best results, outperforming the rest of the models by up to a 2.5 perplexity score. Keeping in mind that the solely perplexity-based evaluation builds an incomplete picture of the model performance regarding the tokenization, we selected the DEFAULT strategy to train the mGPT XL model. We discuss this limitation in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation Setup</head><p>As typological diversity becomes a key focus of training our model, then, in addition to the standard metrics for evaluating language modeling, it is also necessary to conduct a full-fledged study of the generalizing abilities of the model in different languages. A big step in this direction was made in <ref type="bibr" target="#b10">(Lin et al., 2021)</ref>, where much attention is paid to how successfully the current model handles language variation.</p><p>Our evaluation approach consists of three main stages: We estimate the language modeling performance on the prediction of string continuation for each of the used languages. We used the average perplexity <ref type="bibr">(Jelinek, 1990)</ref> for each language and then aggregated this score for each language family to estimate this ability. We computed the average perplexity of tokenized documents on held-out samples for each language as described in Equation 1, with the exception of normalization to the length of a string in tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Knowledge Probing</head><p>We probe our models for factual knowledge in 23 languages using the mLAMA dataset <ref type="bibr">(Kassner et al., 2021a)</ref>. The task is to complete a knowledge triplet ?subject, relation, object? converted to templates for querying the LMs. Consider an example from the original LAMA <ref type="bibr">(Petroni et al., 2019)</ref>, where the triplet &lt;Dante, born-in, X&gt; is converted to the template "Dante was born in [MASK]". We follow Lin et al. (2021) to design the probing task as follows. Each such query contains hundreds of negative candidates on average, we limit the number of candidates to three, i.e., one is the ground truth candidate and the other two candidates are randomly sampled from the provided knowledge source. The probing performance is evaluated with precision@1 averaged over all relations per language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Downstream Tasks</head><p>We take on a wide range of tasks available for major languages, including those that are not included in the XGLM assessment. These tasks cover various areas of NLP, from POS and NER tagging to free-form text generation and text understanding. Generalization abilities were evaluated with the zero-shot and few-shot approaches, which helps to check inherent model abilities and compare with XGLM results. We focused on the following set of tasks: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation Method</head><p>We evaluate mGPT in a few-and zero-shot settings. Following the methodology of <ref type="bibr">(Brown et al., 2020)</ref>, in the zero-shot setting, the model is shown a formatted test prompt in natural language, while in the few-shot setting, the prompt is augmented with several demonstrations, i. e., examples with answers taken from the training data and formatted in the same way as the current test sample. We used slightly different variations of these approaches for each type of task. These details are described in more detail below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Classification</head><p>For the classification tasks, we select the label associated with the prompt string that has the lowest sum of losses per each token. Namely, mGPT uses per token cross-entropy loss, which is reduced to negative log probability due to the target token onehot encoding. Thus, the most probable string would lead to the lowest sum of neg. log probabilities of its tokens. To replicate the XGLM model results for the comparison, we use weights and code released by Facebook in it's Fairseq GitHub repository 8 . Evaluation procedure was not included in this code, so we reproduce the 1.7B and 7.5B evaluation process based on the description given in XGLM paper, including 5 runs with different random seeds for a few-shot setup. For correct comparison with our models, we select prompts according to the templates in the XGLM paper. For languages other </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Sequence labeling</head><p>For sequence labeling tasks, we used a slightly modified version of the previous approach predicting each tag separately for each word. Namely, for a sentence that contains k words, we created k samples of the phrase prefix ending with the current word augmented with an underscore. For example, a tagged sentence for the POS task "I PRON want VERB to PART go VERB to ADP the DET cafeteria . PUNCT" is transformed into 8 test samples of the following format: 1)&lt;s&gt;lang: en \n Tagged sentence: I_, 2)&lt;s&gt;lang: en \n Tagged sentence: I_PRON want_, etc.</p><p>For the NER task, we use the same scheme of forming test examples (see Appendix G for prompt formats).</p><p>Then, following the classification approach, for each obtained sample, we computed mGPT loss on this sample united with each of the possible golden labels and chose the one which yielded the lowest loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Text generation</head><p>The sequence-to-sequence task formalization can be considered quite universal. However, we include tasks requiring a text answer in this category of tasks, such as question-answer tasks, paraphrasing, and generative tasks from benchmarks. The evaluation methods described use sampling and task-specific restrictions of the number of tokens generated, top_p and top_k parameters.</p><p>The zero-shot generation method implies generation based on minimal task-specific prompts, e.g., special start and end tokens, a task-specific prompt in a target language, a paragraph, and a question for SQuAD-like datasets, and no answer given. The few-shot generation method implies an extended generation prompt, consisting of up to 3 examples with answers, joined by newlines, plus a target example with no answer. The examples for few-shot prompting are randomly taken from the training split of the dataset for each target example. For each task, the prompt is automatically translated into a target language with TextBlob library 9 .</p><p>Both types of the generative approach include post-processing, where the generated results are split by a newline. Tapaco paraphrasing prompt examples:</p><p>? Zero-shot: 'Rephrase: I eat cheese ==&gt;'</p><p>? Few-shot: 'Rephrase: Today, it's Monday ==&gt; It is Monday Where is the library? ==&gt; Where's the library? Do you know English ==&gt; Do you speak English? I eat cheese ==&gt;' Generative task specifications:</p><p>? tapaco. zero-shot: top_p=0.85, top_k=0, max_tokens=50. few-shot: num fewshot examples=5, top_p=0.95, top_k=0, max_tokens=250, hf dataset split=train.</p><p>? question generation. zero-shot: top_p=0.9, top_k=0, max_tokens=150, hf dataset split=test few-shot:</p><p>num few-shot examples=3, top_p=0.95, top_k=1, max_tokens=150, huggingface split=test.</p><p>? xquad. zero-shot: top_p=0.95, top_k=1, hf dataset split=validation. few-shot: num few-shot examples=3, top_p=0.95, top_k=0, max_tokens=1250, hf dataset split=validation. Higher perplexity has been demonstrated for only 4 languages with a score exceeding 16. The XL model also demonstrate the perplexities following the same distribution, consistently higher then the ones from 13B model. <ref type="figure" target="#fig_1">Figure 2</ref> outlines the family-wise perplexity scores. The 13B model has shown the best perplexity scores within the 2-to-10 perplexity range for most of the language families. Remaining 8 families fall under the 10-to-15 range, such as Afro-Asiatic, Austro-Asiatic, Austronesian, Germanic, Romance, Uralic, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Language modeling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Classification Tasks</head><p>The comparative results of the presented models and XGLM on classification tasks are reported in the <ref type="table" target="#tab_8">Table 3</ref>. We attempted to repeat the scores reported in the XGLM paper for 7.5 billion parameters and 1.7B parameters models as they are the nearest to ours in terms of size. For zero-shot setup we reproduce all the metrics, some of scores are even slightly higher. As for the few-shot setup not all of the metrics were successfully replicated (for instance, PAWSX task and XNLI task). We assume there could be differences in writing of the translated prompts. Thus, the reproducibility of the XGLM results remains an open question. For our model we report the scores for one run in few-shot setup, however the variances among few-shot trials using different random seeds could be significant. We still can see that mGPT 1.3B is comparable with XGLM 1.7B despite having less weights and being forced to learn more different languages at once. See the appendix <ref type="table" target="#tab_0">Tables 12, 13</ref> <ref type="bibr">, 15, 14, 16</ref> for the detailed comparison.</p><p>Our few-shot results suggests that it is not always beneficial to introduce more examples before the estimated one. For some tasks increasing the number of examples in sample leads to the decreasing of accuracy. This issue is discussed in the Limitations section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Sequence Labeling Tasks</head><p>We evaluated mGPT ability to solve sequence labeling problems on two downstream tasks: NER and POS, from the XGLUE benchmark. Following the original XGLUE methodology, we evaluated NER using the F1 score and POS using the precision score. Aggregated results are presented in <ref type="table" target="#tab_9">Table 4</ref> and detailed per language results can be found in Appendix H.</p><p>Taking into account that tag sets for both tasks are quite numerous: 17 unique tags for POS task and 5 for NER (see Appendix F for the complete list), results for sequence labeling tasks seem pretty promising. It should also be noted that the fact that the NER zero-shot is better than the few-shot could be quite misleading due to class imbalance in the NER dataset with the predominant empty tag, representing the absence of the named entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Generation Tasks</head><p>Using standard metrics for each of the generative tasks, we can evaluate both semantic similarity (bert score, rouge, bleu) and exact matching (f1, EM) of the answers given by the model and the golden answers.</p><p>Comparing the results achieved in the different experimental setups, we can conclude, firstly, that few specific languages are prevalent in the overall quality of all of the tasks: thus, German, English, and Spanish achieve higher scores, and this is much more evident within the zero-shot setup. Some unexpected quality peaks can be found for languages such as Danish and Azeri. At the same time, this result is not consistent, which can be established by comparing the results in different experimental setups. Languages more represented in pre-training tend to increase quality from zeroto few-shot setup.    Secondly, we conclude that the similarity metrics are, expectedly, higher than the exact metrics, as the generative model tends to give lexically rich answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Knowledge Probing</head><p>Figure 1 outlines the results for mGPT-XL and mGPT-13B. The overall pattern is that the performance is equal or above 0.6 for Germanic, Romance, Austro-Asiatic, Japonic, and Chinese languages. However, Uralic, Slavic, Koreanic, and Afro-Asiatic languages receive scores of lower than 0.5. We also find that scaling the number of model parameters typically boost the performance for high-resource languages up to 5 points, while no significant improvements are observed on the other languages.</p><p>Comparing our results with Lin et al. <ref type="formula">(2021)</ref>, we conclude that our models achieve lower performance than XGLM (7.5B) almost on all languages, and perform on par with GPT3-Curie 10 (6.7B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>This section focuses on identified issues and limitations of the model, which we hope to fix in future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Evaluation</head><p>The overall quality of the model is achieving some promising results. The sequence labeling tasks on zero-shot and few-shot settings show high scores. Results of the XL model with 1.3 billion parameters on most of the classification tasks are on par or better than the results of   the XGLM model of 1.7B parameters despite the sharing model capacity between the 60 languages instead of just 30 for the XGLM one. This can suggest that a larger variety of languages leads to a better inner cross-lingual representation of texts inside a model that allows for better scores even with a fewer number of weights. Language modeling results for 1.3B and 13B models suggest that an increase in the model size improves its generation abilities for all given languages. However, as other downstream tasks showed, this is not always lead to improvements in more complex language understanding tasks. Knowledge probing results show that the model retains factual knowledge. This ability for any given language is correlated with the amount of data on this language in a training set. However, we observe a poor performance on a few classification tasks with many classes and sequence generation tasks. The model has a set of limitations that we list below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Limitations</head><p>In this section we discuss the limitations that was found during pretraining and evaluation of our models on a variety of natural language processing tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Languages</head><p>The number of the considered languages is incomplete and only partially covers the diversity of typologically contrasting languages. Besides, the distribution of the data by size and domain for the considered languages is inconsistent due to the absence of such resources. Extending the set of lan-guages requires scaling the number of the model parameters and, consequently, the number of sufficient computational resources.</p><p>An exciting direction for future work is to evaluate the model on a larger set of the tasks such as the XTREME <ref type="figure">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Tokenization</head><p>The choice of the most optimal tokenization method for a multilingual LM, precisely the vocabulary size, remains an open question. One line to solve this is to evaluate the model perplexity with respect to a given set of hyperparameters and tokenization strategy. However, such an experiment also requires additional computational resources, which is expensive in the case of large LMs. Another drawback of the perplexity-based evaluation is that it only partially assesses the model performance with respect to the learned representations. The representations, specifically the numerical ones, have been recently explored thanks to the creation of the numeracy tasks <ref type="bibr" target="#b1">(Thawani et al., 2021)</ref>, but such datasets exist solely for English, leaving other languages unattended.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Few-shot Performance</head><p>During downstream evaluation of given models we discover that increasing the size of a few-shot sample not always leads to improvements, but in fact decreasing the resulting score on some tasks. This observation is in line with similar results reported by XGLM paper <ref type="bibr" target="#b10">(Lin et al., 2021)</ref>. We hypothesize that for some of complex NLU tasks the variety and adversarial nature of examples leads to this undesired effect. Also for some tasks we observe metrics that not exceed the random guess which points to a failure of a model to capture noticeable relations difference between right and wrong text continuations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Model Robustness and Stability</head><p>Even though the superior performance for some tasks is obtained without fine-tuning, the most optimal generation hyperparameters and prompts are still unknown. The prompting approach itself is shown to be unstable <ref type="bibr" target="#b19">(Zhao et al., 2021)</ref> and is hardly universal across languages. Prompttuning approach <ref type="bibr">Konodyuk and Tikhonova, 2021)</ref> is the next step in that direction, as well as the adversarial benchmarking <ref type="bibr" target="#b7">(Xu et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion</head><p>We introduced the mGPT model and its variations with 1.3B parameters and 13B parameters, which are a reproduced counterpart of the GPT-3 architecture (Brown et al., 2020) trained on 60 languages from 25 language families. We present languagewise perplexities for every language; for some, mGPT is the first generative autoregressive architecture. The perplexities of the presented multilingual models for most languages fall within the same range of values as the monolingual models. At the same time, this is the first available model with measured perplexity for low-resource languages.</p><p>The models show abilities to reproduce the zeroshot and few-shot methodology stated for the GPT-3 1.3B model for English in the original work. We conduct the experiments for choosing the optimal multilingual tokenization strategy by training five small mGPT models (163M parameters) and evaluating their perplexity-based performance.</p><p>We provide the results of the model evaluation in various scenarios. These are language modeling, downstream evaluation on three different kinds of tasks, and knowledge probing. We use the zero-shot and few-shot approaches combined with the neg. log probability ranking to solve the tasks, putting the model in more challenging settings. Specifically, the zero-shot approach is solely based on the model's prior knowledge without finetuning on the source language data. Despite the ample space for further quality growth and solving the highlighted limitations, the model shows significant potential and can become the basis for developing the generative pipelines for languages other than English, especially the low-resource ones. The presented models show comparable to state-of-theart zero-and few-shot results for languages in a row of classification benchmarks (XWINO, PAWSX, XNLI, Twitter Hate Speech). We also discussed this class of models' ethical considerations and social impacts.</p><p>The code and the models are publicly available.</p><p>9 Ethical Considerations and Social Impacts 9.1 Low-resource Languages NLP for resource-lean scenarios is one of the leading research directions nowadays (Ruder and Korashy, 2019). The topic relevance has led to proactive research on low-resource languages, which has proposed various transfer techniques and massively multilingual LMs. Our work falls under this scope, introducing the first autoregressive LM for 60 languages. To the best of our knowledge, we make the first attempt to address this problem for 20 languages of the Commonwealth of Independent States and the small peoples in Russia, such as Kazakh, Tajik, Tatar, Kalmyk, Yakut, etc. We hope to benefit cross-lingual knowledge transfer, annotation projection, and other potential applications for economically challenged and underrepresented languages, and diversify the research field by shifting from the Anglo-centric paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Energy Efficiency and Usage</head><p>Pre-training large-scale LMs requires a large number of computational resources, which is energyintensive and expensive. To address this issue we used sparse attention approach that is suggested by <ref type="bibr">(Brown et al., 2020)</ref> and reduce the required amount of computational resources needed to achieve desired model performance. Following recommendations by Bommasani et al. <ref type="formula">(2021)</ref>, we report computational, energy, and carbon costs of pre-training our models. We use the CO2 emission formula proposed in <ref type="bibr">Strubell et al. (2019)</ref>. Reported power usage effectiveness for our datacenters is not more than 1.3, reported CO2 energy intensity for the region is 400 gramms per kwh.</p><p>Using this basis we calculate estimates of CO2 released to atmosphere as 15 934 kg for mGPT-XL and 47 524 kg for mGPT-13B respectively. This numbers are comparable to a single medium range flight of a modern aircraft, which usually releases around 12 000 kg of CO2 per 1000 km of flight. Despite the costs, mGPT can be efficiently adapted to the user needs via few-shot learning, bringing down potential budget costs in the scope of applications in multiple languages, such as generating the content, augmenting labeled data, or rewriting news articles. Lack of necessity for fine-tuning saves on data annotation and energy consumption, alleviating the carbon footprint consequences. Model compression techniques, e.g., pruning and distillation, can further reduce the model inference costs and production restrictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Social Risks of Harm</head><p>Stereotypes and unjust discrimination present in pre-training corpora can lead to representation biases in LMs. LMs can reflect historical prejudices against disadvantaged social groups and reproduce harmful stereotypes about gender, race, religion, or sexual orientation <ref type="bibr">(Weidinger et al., 2021)</ref>. We have conducted evaluation on the Twitter Hate-Speech detection dataset to analyze mGPT's limitations on social risks of harm involving hate speech.</p><p>Our results are quite similar to <ref type="bibr" target="#b10">Lin et al. (2021)</ref> in that the Polish language receives a near-zero accuracy in a zero-shot classification task. Our models also achieve performance lower than the random guessing on this language. We believe that this may indicate a significant bias in the training corpus, a mutual influence of languages during training, or methodological problems in the test set. We do not claim that our bias evaluation setup is exhaustive, and we assume that other biases can be revealed through a direct model application or an extended evaluation setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4">Potential Misuse</head><p>The misuse potential of LMs increases with their abilities to generate high-quality texts. Malicious users can perform a socially harmful activity that involves generating texts, e.g., spreading propaganda and misinformation, abusing legal and governmental procedures, and facilitating fraud, scams, and other targeted manipulation (Jawahar et al., 2020). Ease of model deployment and the increasing difficulty of distinguishing between human-written and generated texts contribute to the spread of such misuse. We recognize that our models can be misused in all supported languages. However, adversarial defense and artificial text detection models can mitigate ethical and social risks of harm. Our primary purpose is to propose multilingual GPTstyle LMs for research and development needs, and we hope to work on the misuse problem with other developers and experts in mitigation research in future. <ref type="bibr">Zhang</ref>  <ref type="figure" target="#fig_1">2, , B, i, r, d, s, , +, , 3, , b, i, r, d, s, , =, , 2, 5, , b</ref>, i, r, d, s <ref type="table">Table 6</ref>: Different tokenization strategies applied to the sentence "22 Birds + 3 birds = 25 birds". The resulting tokens are highlighted in the corresponding colors.</p><p>The choice of the tokenization method has a significant impact on learning efficient representations, especially in the multilingual setting. There is a large body of works on the limitations of transformerbased LMs, ranging from low-resource languages and unseen language scripts <ref type="bibr">(Pfeiffer et al., 2021)</ref> to numerical representations <ref type="bibr">(Nogueira et al., 2021)</ref>. The most common tokenization approaches such as <ref type="bibr">BPE (Sennrich et al., 2016)</ref>, SentencePiece <ref type="bibr">(Wu et al., 2016)</ref> and BBPE <ref type="bibr" target="#b6">(Wang et al., 2020)</ref> tend to be sub-optimal for representing numbers <ref type="bibr" target="#b1">(Thawani et al., 2021)</ref> which has facilitated various improvements of the model architectures including their tokenization strategies <ref type="bibr">(Polu and Sutskever, 2020;</ref><ref type="bibr">Geva et al., 2020;</ref><ref type="bibr" target="#b18">Zhang et al., 2020)</ref>. In line with these studies, we investigated the effect of the tokenization strategy on the perplexity of the multilingual model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Tokenization Strategies</head><p>We considered five tokenization strategies that incorporate specific representations of uppercase characters, numbers, punctuation marks and whitespaces (see <ref type="table">Table 6</ref>):</p><p>? DEFAULT: BBPE tokenization <ref type="bibr" target="#b6">(Wang et al., 2020)</ref>;</p><p>? CASE: Each uppercase character is replaced with a special token &lt;case&gt; followed by the corresponding lowercase character;</p><p>? ARITHMETIC: The CASE strategy combined with representing numbers and arithmetic operations as individual tokens;</p><p>? COMBINED: The ARITHMETIC strategy combined with representing punctuation marks and whitespaces as individual tokens;</p><p>? CHAR: Character-level tokenization.</p><p>We trained five strategy-specific small versions of the mGPT model (163M parameters) on Wikipedia in 40 languages and evaluate the model perplexity on a held-out Wikipedia sample (approx. 10.7MB of texts) to choose the most optimal tokenization strategy.The models were trained on 16 V100 GPUs for 2 epochs with a set of fixed hyperparameters: vocabulary size of 100k, 2048 token context window, LR = 2e ?4 , the batch size of 4.      XNLI "&lt;s&gt;" + sentence1 + ", right? " + mask + ", " + sentence2 + "&lt;/s&gt;", where mask is the string that matches the label: entailment -Yes, neural -Also, contradiction -No. Example: &lt;s&gt;The girl that can help me is all the way across town, right? Yes, The girl I need help from lives a ways away.&lt;/s&gt; PAWSX "&lt;s&gt;" + sentence1 + ", right? " + mask + ", " + sentence2 + "&lt;/s&gt;", where mask is the string that matches the label: Yes, No. Example: &lt;s&gt; The Tabaci River is a tributary of the River Leurda in Romania, right? No, The Leurda River is a tributary of the River Tabaci in Romania.&lt;/s&gt; XWINO "&lt;s&gt;" + start_of_sentence + mask + end_of_sentence + "&lt;/s&gt;", where mask is the string that matches the label. Example: &lt;s&gt; The sculpture rolled off the shelf because the sculpture wasn't anchored.&lt;/s&gt; XCOPA "&lt;s&gt;" + sentence + " because " + mask + "&lt;/s&gt;" or "&lt;s&gt;" + sentence + " so " + mask + "&lt;/s&gt;", where mask is the string that matches the label. Example: &lt;s&gt; The office was closed because it was holiday.&lt;/s&gt; Twitter Hate Speech "&lt;s&gt;The sentence is " + mask + sentence + "&lt;/s&gt;", where mask including 5 negative (normal., common., ok., usual., acceptable.) and 5 positive (sexist., racist., offensive., abusive., hateful.) candidates. Example: &lt;s&gt; The sentence is normal. user i'm decent at editing no worries.&lt;/s&gt; QG "&lt;s&gt;/ nlang: eng/ npassage: "+text+"/ nlabel: "+ str(tgt)+"&lt;/s&gt;" Example: &lt;s&gt; lang: eng passage: John Fitzgerald Kennedy <ref type="bibr">(May 29, 1917</ref><ref type="bibr">-November 22, 1963</ref>, commonly known as "Jack" or by his initials JFK, was the <ref type="formula">35th</ref>    <ref type="table" target="#tab_0">Table 13</ref>: XCOPA task comparative results per language. Accuracy is used as a metric.   <ref type="table">Mode  ru  en  ar  bg  de  el  es  fr  hi  sw  th  tr  ur  vi  zh  Mean   mGPT XL</ref> zero-shot 0,450 0,498 0,346 0,440 0,450 0,367 0,425 0,448 0,396 0,408 0,380 0,377 0,380 0,342 0,385 0,406 1-shot 0,393 0,422 0,330 0,381 0,411 0,333 0,359 0,367 0,343 0,354 0,334 0,337 0,347 0,363 0,331 0,360 4-shot 0,395 0,432 0,349 0,384 0,417 0,355 0,360 0,404 0,337 0,357 0,349 0,351 0,349 0,382 0,358 0,372 16-shot 0,388 0,442 0,351 0,387 0,425 0,367 0,388 0,410 0,350 0,358 0,346 0,347 0,337 0,403 0,370 0,378 mGPT 13B zero-shot 0,476 0,517 0,339 0,471 0,465 0,357 0,459 0,482 0,414 0,429 0,405 0,364 0,407 0,377 0,414 0,425 1-shot 0,393 0,421 0,342 0,381 0,415 0,331 0,361 0,369 0,350 0,346 0,337 0,337 0,353 0,380 0,351 0,364 4-shot 0,422 0,460 0,353 0,408 0,431 0,371 0,428 0,456 0,382 0,378 0,371 0,347 0,372 0,395 0,396 0,398 16-shot 0,424 0,476 0,340 0,420 0,459 0,382 0,431 0,476 0,382 0,366 0,379 0,356 0,368 0,421 0,415 0  mGPT XL zero-shot 0,551 0,521 0,423 0,500 0,502 0,500 4-shot 0,493 0,497 0,515 0,556 0,498 0,512 mGPT 13B zero-shot 0,568 0,542 0,562 0,500 0,534 0,541 4-shot 0,525 0,491 0,554 0,511 0,568 0,530 XGLM 1.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B List of Languages</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Number of tokens for each language in the dataset on a logarithmic scale for a document. Outliers were dropped. The remaining documents were filtered by a small binary classifier trained on wiki documents as positive samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Family-wise perplexity results. The perplexity scores are averaged over the number of the languages within the family. than English we used prompts translated from English with Google Translator. The examples of prompts are listed in Appendix G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Perplexity of language modeling task for 60 languages, for both XL and 13B models. Lower is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Knowledge probing results for 25 languages. The performance of a random baseline is 0.33.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Hu et al., 2020) benchmark and the monolingual non-English SuperGLUE-style benchmarks, such as RussianSuperGLUE (Shavrina et al., 2020), KLEJ (Rybak et al., 2020) and FLUE (Le et al., 2020).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table /><note>Comparison of the GPT-2 and GPT-3 parame- ters. Params refers to the number of parameters; Lay- ers corresponds to the number of layers. Context is the size of the context window.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Evaluation results in the tokenization experiments for each small mGPT model.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9</head><label>9</label><figDesc></figDesc><table /><note>presents the perplexity scores for each lan- guage on a held-out set (see Appendix D). The mGPT 13B model has achieved the best perplex- ity results within the 2-to-12 score range for the majority of languages, including Dravidian (Malay- alam, Tamil, Telugu), Indo-Aryan (Bengali, Hindi, Marathi), Slavic (Belarusian, Ukrainian, Rus- sian, Bulgarian), Sino-Tibetan (Burmese), Kipchak (Bashkir, Kazakh) and others. 4 languages re- ceive a acceptable score within the 12-to-16 range, predominantly Romance (Spanish, Portuguese).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>The evaluation accuracy results for classification tasks averages across languages. We compare the results of mGPT and XGLM.</figDesc><table><row><cell>Model</cell><cell>Mode</cell><cell cols="2">POS NER</cell></row><row><cell>mGPT</cell><cell cols="2">zero-shot 0.43</cell><cell>0.86</cell></row><row><cell>XL 1.3B</cell><cell>4-shot</cell><cell>0.56</cell><cell>0.85</cell></row><row><cell>mGPT</cell><cell cols="2">zero-shot 0.42</cell><cell>0.86</cell></row><row><cell>13B</cell><cell>4-shot</cell><cell>0.50</cell><cell>0.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Evaluation results on sequence labeling tasks NER and POS from XGLUE benchmark. Precision is used as a metric for POS; F1 score for NER.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>ModelMode XQUAD F1 XQUAD ExactMatch QG Bertscore QG ROUGE QG BLEUTapaco Bertscore Tapaco ROUGE Tapaco BLEU</figDesc><table><row><cell></cell><cell>zeroshot</cell><cell>8.45</cell><cell>2.23</cell><cell>0.71</cell><cell>0.22</cell><cell>5.67</cell><cell>0.66</cell><cell>0.07</cell><cell>5.12</cell></row><row><cell>MGPT XL 1.3B</cell><cell>3-shot</cell><cell>1.95</cell><cell>0.95</cell><cell>0.71</cell><cell>0.19</cell><cell>6.33</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>5-shot</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.57</cell><cell>0.02</cell><cell>4.22</cell></row><row><cell></cell><cell>zeroshot</cell><cell>5.52</cell><cell>2.02</cell><cell>0.70</cell><cell>0.19</cell><cell>5.11</cell><cell>0.68</cell><cell>0.08</cell><cell>7.60</cell></row><row><cell>MGPT 13B</cell><cell>3-shot</cell><cell>0.2</cell><cell>0.0</cell><cell>0.71</cell><cell>0.19</cell><cell>6.33</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>5-shot</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.61</cell><cell>0.02</cell><cell>4.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>The overall evaluation results for generation tasks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>, Xikun Zhang, Yuhui Zhang, Lucia Zheng, Kaitlyn Zhou, and Percy Liang. 2021. On the opportunities and risks of foundation models. Berend, and G. Korvel. 2022. Neural Natural Language Generation: A Survey on Multilinguality, Multimodality, Controllability and Learning. Journal of Artificial Intelligence Research, in press. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, et al. 2020. The pile: An 800gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Nikita Konodyuk and Maria Tikhonova. 2021. Continuous prompt tuning for russian: how to learn prompts efficiently with rugpt3? In Proceedings of the International Conference on Analysis of Images, Social Networks and Texts. pages 217-227, Online. Association for Computational Linguistics. Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. Stanislas Polu and Ilya Sutskever. 2020. Generative language modeling for automated theorem proving. arXiv preprint arXiv:2009.03393. Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053.</figDesc><table><row><cell>Appendices</cell><cell>Mohammad Shoeybi, Mostofa Patwary, Raul Puri,</cell></row><row><cell cols="2">Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language Models are Few-shot Learners. Advances in neural information process-ing systems, 33:1877-1901. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long se-quences with sparse transformers. arXiv preprint arXiv:1904.10509. Jey Han Lau, Carlos Santos Armendariz, Shalom Lap-pin, Matthew Purver, and Chang Shu. 2020. How furiously can colourless green ideas sleep? sentence acceptability in context. CoRR, abs/2004.00881. Goran Glava?. 2020. From zero to hero: On the limitations of zero-shot language transfer with mul-tilingual Transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Lan-OpenAI blog, 1(8):9. guage models are unsupervised multitask learners. Dario Amodei, Ilya Sutskever, et al. 2019. Lan-Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Anne Lauscher, Vinit Ravishankar, Ivan Vuli?, and Edoardo Maria Ponti, Goran Glava?, Olga Majewska, Qianchu Liu, Ivan Vuli?, and Anna Korhonen. 2020. XCOPA: A multilingual dataset for causal common-sense reasoning. In Proceedings of the 2020 Con-ference on Empirical Methods in Natural Language Processing (EMNLP), pages 2362-2376, Online. As-sociation for Computational Linguistics. A Exploring Tokenization Strategies Strategy Tokenization Example DEFAULT 22, Birds, +, 3, birds, =, 25, birds Mor Geva, Ankit Gupta, and Jonathan Berant. 2020. Injecting numerical reasoning skills into language models. In Proceedings of the 58th Annual Meet-ing of the Association for Computational Linguis-tics, pages 946-958, Online. Association for Com-In Proceedings of the 27th International Conference putational Linguistics. Santa Fe, New Mexico, USA. Association for Com-on Computational Linguistics, pages 2340-2353, Association for Computational Linguistics. Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, and Graham Neubig. 2018. Stress test evaluation for natural language inference. Patrick LeGresley, Emma Strubell, Ananya Ganesh, and Andrew McCal-CASE 22, &lt;case&gt;, birds, +, 3, birds, =, 25, birds lum. 2019. Energy and policy considerations for ARITHMETIC 2, 2, &lt;case&gt;, birds, , +, , 3, birds, , =, , 2, 5, birds deep learning in NLP. In Proceedings of the 57th COMBINED 2, 2, &lt;case&gt;, birds, , +, , 3, , birds, , =, , 2, 5, , birds Annual Meeting of the Association for Computa-tional Linguistics, pages 3645-3650, Florence, Italy. CHAR 2,</cell></row><row><cell>Fran?ois Chollet. 2019. On the measure of intelligence. arXiv preprint arXiv:1911.01547. Alexis Conneau, Guillaume Lample, Ruty Rinott, Ad-ina Williams, Samuel R. Bowman, Holger Schwenk, and Veselin Stoyanov. 2018a. Xnli: Evaluating cross-lingual sentence representations. guage Processing (EMNLP), pages 4483-4499, On-line. Association for Computational Linguistics. Hang Le, Lo?c Vial, Jibril Frej, Vincent Segonne, Max-imin Coavoux, Benjamin Lecouteux, Alexandre Al-lauzen, Beno?t Crabb?, Laurent Besacier, and Didier Schwab. 2020. Flaubert: Unsupervised language Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the lim-its of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21:1-67.</cell><cell>putational Linguistics. Michael A. Hedderich, Lukas Lange, Heike Adel, Jan-nik Str?tgen, and Dietrich Klakow. 2021. A survey on recent approaches for natural language process-ing in low-resource scenarios. In Proceedings of the 2021 Conference of the North American Chapter of Timothy Niven and Hung-Yu Kao. 2019. Probing neu-ral network comprehension of natural language ar-guments. In Proceedings of the 57th Annual Meet-ing of the Association for Computational Linguis-tics, pages 4658-4664, Florence, Italy. Association for Computational Linguistics.</cell></row><row><cell>Alexis Conneau, Ruty Rinott, Guillaume Lample, Ad-ina Williams, Samuel R. Bowman, Holger Schwenk, and Veselin Stoyanov. 2018b. Xnli: Evaluating cross-lingual sentence representations. In Proceed-ings of the 2018 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics. Ryan Cotterell, Sabrina J. Mielke, Jason Eisner, and Brian Roark. 2018. Are all languages equally hard to language-model? In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 536-541, New Orleans, Louisiana. Associa-tion for Computational Linguistics. model pre-training for french. Sebastian Ruder and H Korashy. 2019. The Four Yaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fen-Biggest Open Problems in NLP. Ain Shams Eng. J. fei Guo, Weizhen Qi, Ming Gong, Linjun Shou, Piotr Rybak, Robert Mroczkowski, Janusz Tracz, and Daxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei Ireneusz Gawlik. 2020. KLEJ: Comprehensive Zhang, Rahul Agrawal, Edward Cui, Sining Wei, benchmark for Polish language understanding. In Taroon Bharti, Ying Qiao, Jiun-Hung Chen, Winnie Proceedings of the 58th Annual Meeting of the Asso-Wu, Shuguang Liu, Fan Yang, Daniel Campos, Ran-ciation for Computational Linguistics, pages 1191-gan Majumder, and Ming Zhou. 2020. XGLUE: A 1201, Online. Association for Computational Lin-new benchmark datasetfor cross-lingual pre-training, guistics. understanding and generation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Yves Scherrer et al. 2020. Tapaco: A corpus of senten-Language Processing (EMNLP), pages 6008-6018, Online. Association for Computational Linguistics. Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Na-tial paraphrases for 73 languages. In Proceedings of The 12th Language Resources and Evaluation Con-ference. European Language Resources Association (ELRA).</cell><cell>the Association for Computational Linguistics: Hu-Rodrigo Nogueira, Zhiying Jiang, and Jimmy Li. man Language Technologies, pages 2545-2568, On-2021. Investigating the limitations of the transform-line. Association for Computational Linguistics. Junjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-ham Neubig, Orhan Firat, and Melvin Johnson. 2020. Xtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual gener-alisation. In International Conference on Machine Learning, pages 4411-4421. PMLR. ers with simple arithmetic tasks. arXiv preprint arXiv:2102.13019. Boris Orekhov, I Krylova, I Popov, E Stepanova, and L Zaydelman. 2016. Russian minority languages on the web: Descriptive statistics. In Vladimir Selegey (chief ed.), Computational linguistics and intellec-tual technologies: Proceedings of the international Aashi Jain, Mandy Guo, Krishna Srinivasan, Ting Chen, Sneha Kudugunta, Chao Jia, Yinfei Yang, and Jason Baldridge. 2021. MURAL: Multimodal, mul-titask representations across languages. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3449-3463, Punta Cana, Do-conference "Dialogue, pages 498-508. Ethan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True Few-shot Learning with Language Models. Ad-vances in Neural Information Processing Systems, 34.</cell></row><row><cell>Thomas Davidson, Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. Automated hate speech detection and the problem of offensive language. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understand-ing. Sumanth Doddapaneni, Gowtham Ramesh, Anoop man Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O'Horo, Jeff Wang, Luke Zettle-moyer, Zornitsa Kozareva, Mona T. Diab, Veselin Stoyanov, and Xian Li. 2021. Few-shot learn-ing with multilingual language models. CoRR, abs/2112.10668. Timo Schick and Hinrich Sch?tze. 2021. It's not just size that matters: Small language models are also few-shot learners. In Proceedings of the 2021 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 2339-2352, Online. As-sociation for Computational Linguistics. Qi Liu, Matt J. Kusner, and Phil Blunsom. 2020a. A Rico Sennrich, Barry Haddow, and Alexandra Birch. survey on contextual embeddings. 2016. Neural machine translation of rare words</cell><cell>minican Republic. Association for Computational Linguistics. Ganesh Jawahar, Muhammad Abdul-Mageed, and Laks Lakshmanan, V.S. 2020. Automatic detection of machine generated text: A critical survey. In Proceedings of the 28th International Conference on Computational Linguistics, pages 2296-2309, Barcelona, Spain (Online). International Committee on Computational Linguistics. Fabio Petroni, Tim Rockt?schel, Sebastian Riedel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller. 2019. Language models as knowl-edge bases? In Proceedings of the 2019 Confer-ence on Empirical Methods in Natural Language Processing and the 9th International Joint Confer-ence on Natural Language Processing (EMNLP-IJCNLP), pages 2463-2473, Hong Kong, China. As-sociation for Computational Linguistics.</cell></row><row><cell>Kunchukuttan, Pratyush Kumar, and Mitesh M Khapra. 2021. A Primer on Pretrained Mul-tilingual Language Models. arXiv preprint arXiv:2107.00676. Jesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah Smith. 2020. Fine-tuning pretrained language models: Weight initializations, data orders, and early stop-ping. arXiv preprint arXiv:2002.06305. Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, with subword units. In Proceedings of the 54th An-nual Meeting of the Association for Computational Yujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt understands, too. Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Linguistics (Volume 1: Long Papers), pages 1715-1725, Berlin, Germany. Association for Computa-tional Linguistics. Edunov, Marjan Ghazvininejad, Mike Lewis, and Tatiana Shavrina, Alena Fenogenova, Emelyanov An-Luke Zettlemoyer. 2020b. Multilingual denoising ton, Denis Shevelev, Ekaterina Artemova, Valentin pre-training for neural machine translation. Malykh, Vladislav Mikhailov, Maria Tikhonova, An-drey Chertok, and Andrey Evlampiev. 2020. Rus-sianSuperGLUE: A Russian language understanding evaluation benchmark. In Proceedings of the 2020</cell><cell>F Jelinek. 1990. Self-organized modelling for speech recognition,". Readings in Speech Recognition, A. Waibel and K. Lee (eds.), pages 450-503. Nora Kassner, Philipp Dufter, and Hinrich Sch?tze. 2021a. Multilingual LAMA: Investigating knowl-edge in multilingual pretrained language models. In Proceedings of the 16th Conference of the European Jonas Pfeiffer, Ivan Vuli?, Iryna Gurevych, and Sebas-tian Ruder. 2021. UNKs everywhere: Adapting mul-tilingual language models to new scripts. In Pro-ceedings of the 2021 Conference on Empirical Meth-ods in Natural Language Processing, pages 10186-10203, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Chapter of the Association for Computational Lin-Telmo Pires, Eva Schlinger, and Dan Garrette. 2019. guistics: Main Volume, pages 3250-3258, Online. How multilingual is multilingual BERT? In Pro-Association for Computational Linguistics. ceedings of the 57th Annual Meeting of the Asso-</cell></row><row><cell>Conference on Empirical Methods in Natural Lan-</cell><cell>Nora Kassner, Philipp Dufter, and Hinrich Sch?tze. ciation for Computational Linguistics, pages 4996-</cell></row><row><cell>guage Processing (EMNLP), pages 4717-4726, On-</cell><cell>2021b. Multilingual lama: Investigating knowledge 5001, Florence, Italy. Association for Computa-</cell></row><row><cell>line. Association for Computational Linguistics.</cell><cell>in multilingual pretrained language models. tional Linguistics.</cell></row></table><note>E. Erdem, M. Kuyu, S. Yagcioglu, A. Frank, L. Par- calabescu, B. Plank, A. Babii, O. Turuta, A. Erdem, I. Calixto, E. Lloret, E.-S. Apostol, C.-O. Truica, B. Sandrih, A. Gatt, S. Martincic-Ipsic, G.R. Thomas McCoy, Junghyun Min, and Tal Linzen. 2020. BERTs of a feather do not generalize to- gether: Large variability in generalization across models with similar test set performance. In Pro- ceedings of the Third BlackboxNLP Workshop on An- alyzing and Interpreting Neural Networks for NLP,</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 7 :</head><label>7</label><figDesc>A list of languages grouped by the language family.</figDesc><table><row><cell cols="2">C Dataset Statistics</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>ISO</cell><cell cols="4">Wiki Chars Wiki Docs MC4 Chars MC4 Docs</cell><cell>ISO</cell><cell cols="4">Wiki Chars Wiki Docs MC4 Chars MC4 Docs</cell></row><row><cell>Code</cell><cell>(B)</cell><cell>(M)</cell><cell>(B)</cell><cell cols="2">(M) Code</cell><cell>(B)</cell><cell>(M)</cell><cell>(B)</cell><cell>(M)</cell></row><row><cell>af</cell><cell>0.13</cell><cell>0.09</cell><cell>0.51</cell><cell>2.15</cell><cell>ar</cell><cell>0.86</cell><cell>0.67</cell><cell>3.91</cell><cell>0.00</cell></row><row><cell>az</cell><cell>0.24</cell><cell>0.13</cell><cell>1.18</cell><cell>5.29</cell><cell>ba</cell><cell>0.08</cell><cell>0.05</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>be</cell><cell>0.47</cell><cell>0.32</cell><cell>4.95</cell><cell>1.74</cell><cell>bg</cell><cell>0.39</cell><cell>0.24</cell><cell>6.14</cell><cell>23.41</cell></row><row><cell>bn</cell><cell>0.18</cell><cell>0.09</cell><cell>1.69</cell><cell>7.44</cell><cell>bxr</cell><cell>0.02</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>cv</cell><cell>0.02</cell><cell>0.04</cell><cell>0.00</cell><cell>0.00</cell><cell>da</cell><cell>0.36</cell><cell>0.23</cell><cell>9.15</cell><cell>28.78</cell></row><row><cell>de</cell><cell>5.89</cell><cell>2.21</cell><cell>34.48</cell><cell>0.00</cell><cell>el</cell><cell>0.45</cell><cell>0.17</cell><cell>3.16</cell><cell>0.00</cell></row><row><cell>en</cell><cell>5.84</cell><cell>2.88</cell><cell>35.37</cell><cell>0.00</cell><cell>es</cell><cell>3.67</cell><cell>1.48</cell><cell>20.78</cell><cell>0.00</cell></row><row><cell>eu</cell><cell>0.29</cell><cell>0.26</cell><cell>4.06</cell><cell>1.56</cell><cell>fa</cell><cell>0.45</cell><cell>0.50</cell><cell>3.82</cell><cell>0.00</cell></row><row><cell>fi</cell><cell>0.73</cell><cell>0.46</cell><cell>2.26</cell><cell>0.00</cell><cell>fr</cell><cell>4.36</cell><cell>1.93</cell><cell>27.95</cell><cell>0.00</cell></row><row><cell>he</cell><cell>0.69</cell><cell>0.26</cell><cell>0.00</cell><cell>0.00</cell><cell>hi</cell><cell>0.17</cell><cell>0.09</cell><cell>4.27</cell><cell>18.51</cell></row><row><cell>hu</cell><cell>0.81</cell><cell>0.39</cell><cell>10.84</cell><cell>36.82</cell><cell>hy</cell><cell>0.12</cell><cell>0.36</cell><cell>0.61</cell><cell>2.40</cell></row><row><cell>id</cell><cell>0.59</cell><cell>0.49</cell><cell>6.66</cell><cell>0.00</cell><cell>it</cell><cell>2.98</cell><cell>1.30</cell><cell>29.96</cell><cell>0.00</cell></row><row><cell>ja</cell><cell>0.33</cell><cell>0.97</cell><cell>8.28</cell><cell>0.00</cell><cell>ka</cell><cell>0.16</cell><cell>0.12</cell><cell>0.63</cell><cell>2.30</cell></row><row><cell>kk</cell><cell>0.18</cell><cell>0.22</cell><cell>0.84</cell><cell>2.39</cell><cell>ko</cell><cell>0.28</cell><cell>0.28</cell><cell>6.00</cell><cell>15.60</cell></row><row><cell>ky</cell><cell>0.07</cell><cell>0.07</cell><cell>2.68</cell><cell>1.00</cell><cell>lt</cell><cell>0.20</cell><cell>0.16</cell><cell>3.25</cell><cell>11.27</cell></row><row><cell>lv</cell><cell>0.12</cell><cell>0.09</cell><cell>1.90</cell><cell>6.41</cell><cell>ml</cell><cell>0.12</cell><cell>0.07</cell><cell>0.56</cell><cell>2.04</cell></row><row><cell>mn</cell><cell>0.03</cell><cell>0.02</cell><cell>0.67</cell><cell>2.05</cell><cell>mr</cell><cell>0.05</cell><cell>0.04</cell><cell>0.94</cell><cell>7.77</cell></row><row><cell>ms</cell><cell>0.22</cell><cell>0.17</cell><cell>4.42</cell><cell>13.18</cell><cell>my</cell><cell>0.06</cell><cell>0.03</cell><cell>2.70</cell><cell>0.81</cell></row><row><cell>nl</cell><cell>1.68</cell><cell>1.82</cell><cell>14.28</cell><cell>0.00</cell><cell>os</cell><cell>0.02</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>pl</cell><cell>1.66</cell><cell>1.21</cell><cell>10.30</cell><cell>0.00</cell><cell>pt</cell><cell>1.64</cell><cell>0.88</cell><cell>25.40</cell><cell>0.00</cell></row><row><cell>ro</cell><cell>0.45</cell><cell>0.28</cell><cell>16.49</cell><cell>45.74</cell><cell>ru</cell><cell>3.47</cell><cell>1.47</cell><cell>35.92</cell><cell>0.00</cell></row><row><cell>sah</cell><cell>0.02</cell><cell>0.01</cell><cell>0.00</cell><cell>0.00</cell><cell>sv</cell><cell>1.98</cell><cell>3.21</cell><cell>13.96</cell><cell>48.57</cell></row><row><cell>sw</cell><cell>0.03</cell><cell>0.04</cell><cell>3.12</cell><cell>0.99</cell><cell>ta</cell><cell>0.20</cell><cell>0.13</cell><cell>1.13</cell><cell>3.51</cell></row><row><cell>te</cell><cell>0.19</cell><cell>0.06</cell><cell>3.31</cell><cell>1.19</cell><cell>tg</cell><cell>0.04</cell><cell>0.06</cell><cell>3.99</cell><cell>1.28</cell></row><row><cell>th</cell><cell>0.20</cell><cell>0.12</cell><cell>3.15</cell><cell>15.46</cell><cell>tk</cell><cell>0.02</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>tr</cell><cell>0.47</cell><cell>0.32</cell><cell>22.05</cell><cell>87.60</cell><cell>tt</cell><cell>0.06</cell><cell>0.15</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>tyv</cell><cell>0.02</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell><cell>uk</cell><cell>1.55</cell><cell>0.93</cell><cell>6.62</cell><cell>0.00</cell></row><row><cell>ur</cell><cell>0.10</cell><cell>0.11</cell><cell>6.02</cell><cell>1.95</cell><cell>uz</cell><cell>0.08</cell><cell>0.12</cell><cell>2.62</cell><cell>0.80</cell></row><row><cell>vi</cell><cell>0.70</cell><cell>0.61</cell><cell>21.36</cell><cell>78.59</cell><cell>xal</cell><cell>0.02</cell><cell>0.00</cell><cell>0.00</cell><cell>0.00</cell></row><row><cell>yo</cell><cell>0.02</cell><cell>0.00</cell><cell>0.13</cell><cell>0.05</cell><cell>zh</cell><cell>0.47</cell><cell>0.55</cell><cell>8.20</cell><cell>54.54</cell></row><row><cell>SUM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>46.70</cell><cell>28.97</cell><cell>442.67</cell><cell>533.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 8 :</head><label>8</label><figDesc>Statistics of the collected dataset. Chars=the number of billions of characters for a language; Docs=the number of millions of documents.</figDesc><table><row><cell cols="3">D Language-wise Perplexity</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Language</cell><cell cols="3">ISO mGPT XL mGPT 13B Language</cell><cell cols="3">ISO mGPT XL mGPT 13B</cell></row><row><cell>Afrikaans</cell><cell>af</cell><cell>17.16</cell><cell>12.29 Arabic</cell><cell>ar</cell><cell>13.60</cell><cell>10.56</cell></row><row><cell>Azerbaijani</cell><cell>az</cell><cell>4.81</cell><cell>3.97 Bashkir</cell><cell>ba</cell><cell>3.91</cell><cell>3.34</cell></row><row><cell>Belarusian</cell><cell>be</cell><cell>8.62</cell><cell>7.77 Bulgarian</cell><cell>bg</cell><cell>11.96</cell><cell>10.36</cell></row><row><cell>Bengali</cell><cell>bn</cell><cell>3.55</cell><cell>3.38 Buriat</cell><cell>bxr</cell><cell>11.66</cell><cell>10.65</cell></row><row><cell>Chuvash</cell><cell>cv</cell><cell>12.26</cell><cell>9.91 Danish</cell><cell>da</cell><cell>12.65</cell><cell>10.41</cell></row><row><cell>German</cell><cell>de</cell><cell>13.24</cell><cell>10.88 Greek, Modern</cell><cell>el</cell><cell>8.82</cell><cell>7.56</cell></row><row><cell>English</cell><cell>en</cell><cell>21.95</cell><cell cols="2">16.40 Spanish; Castilian es</cell><cell>15.71</cell><cell>12.93</cell></row><row><cell>Basque</cell><cell>eu</cell><cell>4.83</cell><cell>4.57 Persian</cell><cell>fa</cell><cell>6.05</cell><cell>5.60</cell></row><row><cell>Finnish</cell><cell>fi</cell><cell>18.96</cell><cell>15.05 French</cell><cell>fr</cell><cell>13.83</cell><cell>11.89</cell></row><row><cell>Hebrew (modern)</cell><cell>he</cell><cell>14.90</cell><cell>11.01 Hindi</cell><cell>hi</cell><cell>4.47</cell><cell>4.19</cell></row><row><cell>Hungarian</cell><cell>hu</cell><cell>13.49</cell><cell>11.50 Armenian</cell><cell>hy</cell><cell>2.01</cell><cell>1.92</cell></row><row><cell>Indonesian</cell><cell>id</cell><cell>13.90</cell><cell>11.47 Italian</cell><cell>it</cell><cell>12.79</cell><cell>10.53</cell></row><row><cell>Japanese</cell><cell>ja</cell><cell>6.97</cell><cell>5.98 Georgian</cell><cell>ka</cell><cell>8.14</cell><cell>7.32</cell></row><row><cell>Kazakh</cell><cell>kk</cell><cell>2.34</cell><cell>2.45 Korean</cell><cell>ko</cell><cell>12.53</cell><cell>10.92</cell></row><row><cell>Kirghiz, Kyrgyz</cell><cell>ky</cell><cell>4.96</cell><cell>4.54 Lithuanian</cell><cell>lt</cell><cell>7.13</cell><cell>6.08</cell></row><row><cell>Latvian</cell><cell>lv</cell><cell>8.23</cell><cell>6.91 Malayalam</cell><cell>ml</cell><cell>4.37</cell><cell>4.09</cell></row><row><cell>Mongolian</cell><cell>mn</cell><cell>6.53</cell><cell>5.70 Marathi</cell><cell>mr</cell><cell>3.63</cell><cell>3.02</cell></row><row><cell>Malay</cell><cell>ms</cell><cell>7.77</cell><cell>7.15 Burmese</cell><cell>my</cell><cell>3.42</cell><cell>3.30</cell></row><row><cell>Dutch</cell><cell>nl</cell><cell>9.90</cell><cell>8.78 Ossetian, Ossetic</cell><cell>os</cell><cell>31.51</cell><cell>21.73</cell></row><row><cell>Polish</cell><cell>pl</cell><cell>12.98</cell><cell>10.51 Portuguese</cell><cell>pt</cell><cell>15.02</cell><cell>12.74</cell></row><row><cell cols="2">Romanian, Moldavan ro</cell><cell>7.72</cell><cell>6.73 Russian</cell><cell>ru</cell><cell>11.52</cell><cell>9.15</cell></row><row><cell>Yakut</cell><cell>sah</cell><cell>7.43</cell><cell>5.99 Swedish</cell><cell>sv</cell><cell>6.22</cell><cell>5.60</cell></row><row><cell>Swahili</cell><cell>sw</cell><cell>10.28</cell><cell>9.17 Tamil</cell><cell>ta</cell><cell>3.43</cell><cell>3.25</cell></row><row><cell>Telugu</cell><cell>te</cell><cell>2.96</cell><cell>2.85 Tajik</cell><cell>tg</cell><cell>5.06</cell><cell>4.52</cell></row><row><cell>Thai</cell><cell>th</cell><cell>7.50</cell><cell>6.51 Turkmen</cell><cell>tk</cell><cell>21.04</cell><cell>18.41</cell></row><row><cell>Turkish</cell><cell>tr</cell><cell>10.32</cell><cell>9.79 Tatar</cell><cell>tt</cell><cell>1.31</cell><cell>1.23</cell></row><row><cell>Tuvinian</cell><cell>tyv</cell><cell>10.34</cell><cell>9.81 Ukrainian</cell><cell>uk</cell><cell>4.96</cell><cell>4.31</cell></row><row><cell>Urdu</cell><cell>ur</cell><cell>10.43</cell><cell>9.54 Uzbek</cell><cell>uz</cell><cell>7.64</cell><cell>6.91</cell></row><row><cell>Vietnamese</cell><cell>vi</cell><cell>11.88</cell><cell>10.34 Kalmyk</cell><cell>xal</cell><cell>27.65</cell><cell>25.26</cell></row><row><cell>Yoruba</cell><cell>yo</cell><cell>14.53</cell><cell>14.62 Chinese</cell><cell>zh</cell><cell>12.43</cell><cell>11.56</cell></row><row><cell>AVG</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>9.92</cell><cell>8.44</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 9 :</head><label>9</label><figDesc>The language-wise perplexity results for XL and 13B models respectively. AVG is the average perplexity score over the number of languages.</figDesc><table><row><cell cols="2">E mLAMA Knowledge Probing Results</cell><cell></cell><cell></cell></row><row><cell cols="4">Language No. of Examples mGPT XL mGPT 13B</cell></row><row><cell>ar</cell><cell>17 129</cell><cell>0,45</cell><cell>0,44</cell></row><row><cell>bg</cell><cell>10 639</cell><cell>0,45</cell><cell>0,46</cell></row><row><cell>bn</cell><cell>7 520</cell><cell>0,44</cell><cell>0,37</cell></row><row><cell>de</cell><cell>29 354</cell><cell>0,60</cell><cell>0,62</cell></row><row><cell>el</cell><cell>10 883</cell><cell>0,48</cell><cell>0,45</cell></row><row><cell>en</cell><cell>33 981</cell><cell>0,65</cell><cell>0,70</cell></row><row><cell>es</cell><cell>28 169</cell><cell>0,56</cell><cell>0,63</cell></row><row><cell>eu</cell><cell>11 788</cell><cell>0,53</cell><cell>0,54</cell></row><row><cell>fi</cell><cell>18 374</cell><cell>0,53</cell><cell>0,58</cell></row><row><cell>fr</cell><cell>30 643</cell><cell>0,58</cell><cell>0,62</cell></row><row><cell>hi</cell><cell>7 276</cell><cell>0,44</cell><cell>0,42</cell></row><row><cell>id</cell><cell>14 094</cell><cell>0,61</cell><cell>0,66</cell></row><row><cell>it</cell><cell>25 662</cell><cell>0,58</cell><cell>0,63</cell></row><row><cell>ja</cell><cell>22 920</cell><cell>0,61</cell><cell>0,63</cell></row><row><cell>ko</cell><cell>14 217</cell><cell>0,51</cell><cell>0,48</cell></row><row><cell>pt</cell><cell>20 319</cell><cell>0,60</cell><cell>0,65</cell></row><row><cell>ru</cell><cell>22 723</cell><cell>0,47</cell><cell>0,49</cell></row><row><cell>ta</cell><cell>7 241</cell><cell>0,41</cell><cell>0,42</cell></row><row><cell>th</cell><cell>8 327</cell><cell>0,43</cell><cell>0,44</cell></row><row><cell>tr</cell><cell>13 993</cell><cell>0,55</cell><cell>0,60</cell></row><row><cell>ur</cell><cell>7 295</cell><cell>0,45</cell><cell>0,45</cell></row><row><cell>vi</cell><cell>11 425</cell><cell>0,57</cell><cell>0,63</cell></row><row><cell>zh</cell><cell>21 449</cell><cell>0,60</cell><cell>0,63</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 10 :</head><label>10</label><figDesc>mLAMA Knowledge Probing results for each language. Accuracy is used as a metric. ADP, ADV, AUX, CCONJ, DET, INTJ,NOUN, NUM, PART, PRON, PROPN, PUNCT, SCONJ, SYM, VERB, X</figDesc><table><row><cell cols="3">F Sequence Labeling Tag Sets</cell></row><row><cell cols="2">Task Unique tags</cell><cell>Tags</cell></row><row><cell>NER</cell><cell>5</cell><cell>I-LOC, I-MISC, I-ORG, I-PER, O</cell></row><row><cell>POS</cell><cell>17</cell><cell>ADJ,</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 11 :</head><label>11</label><figDesc>Tag sets for sequence labeling tasks.</figDesc><table><row><cell>G Prompts Formats</cell></row><row><cell>NER "&lt;s&gt;lang: current_lang \n Tagged sentence:"+" ".join(tagged_words[:i-1])+ " word[i]"+"_"</cell></row><row><cell>Example: &lt;s&gt;lang: en \n Tagged sentence: Kenneth_I-PER Carlsen_I-PER (_O Denmark_I-LOC )_O</cell></row><row><cell>beat_O Patrick_</cell></row><row><cell>POS "&lt;s&gt;lang: current_lang \n Tagged sentence:"+" ".join(tagged_words[:i-1])+ " word[i]"+"_"</cell></row><row><cell>Example: &lt;s&gt;lang: en \n Tagged sentence: I_PRON want_VERB to_PART go_</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>President of the United States, serving from January 1961 until he was assassinated in November 1963. label: when is john f kennedy"s birthday&lt;/s&gt; H Detailed Downstream Evaluation Results</figDesc><table><row><cell>Model</cell><cell>Mode</cell><cell>en</cell><cell>jp</cell><cell>ru</cell><cell>pt</cell><cell>Mean</cell></row><row><cell></cell><cell cols="2">zero-shot 0.572</cell><cell>0.554</cell><cell>0.559</cell><cell>0.562</cell><cell>0.562</cell></row><row><cell>mGPT XL 1.3B</cell><cell>1-shot 4-shot</cell><cell>0,567 0,588</cell><cell>0,574 0,585</cell><cell>0,559 0,551</cell><cell>0,575 0,559</cell><cell>0,569 0,571</cell></row><row><cell></cell><cell>16-shot</cell><cell>0,584</cell><cell>0,555</cell><cell>0,540</cell><cell>0,524</cell><cell>0,551</cell></row><row><cell></cell><cell cols="2">zero-shot 0.608</cell><cell>0.555</cell><cell>0.597</cell><cell>0.568</cell><cell>0.582</cell></row><row><cell>mGPT 13B</cell><cell>1-shot 4-shot</cell><cell>0,623 0,642</cell><cell>0,595 0,560</cell><cell>0,589 0,578</cell><cell>0,562 0,559</cell><cell>0,592 0,585</cell></row><row><cell></cell><cell>16-shot</cell><cell>0,634</cell><cell>0,568</cell><cell>0,540</cell><cell>0,552</cell><cell>0,574</cell></row><row><cell></cell><cell cols="2">zero-shot 0.563</cell><cell>0.528</cell><cell>0.542</cell><cell>0.536</cell><cell>0.542</cell></row><row><cell>XGLM 1.7B</cell><cell>1-shot</cell><cell>0.59 (? 0.0048)</cell><cell cols="4">0.601 (? 0.0096) 0.557 (? 0.0076) 0.573 (? 0.0156) 0.580</cell></row><row><cell></cell><cell>4-shot</cell><cell cols="5">0.608 (? 0.0037) 0.600 (? 0.0025) 0.551 (? 0.0067) 0.558 (? 0.0138) 0.579</cell></row><row><cell></cell><cell>zero</cell><cell>0.611</cell><cell>0.567</cell><cell>0.59</cell><cell>0.60</cell><cell>0.592</cell></row><row><cell>XGLM 7.5B</cell><cell>1-shot</cell><cell>0.654 (? 0.002)</cell><cell>0.667 (? 0.012)</cell><cell cols="2">0.582 (? 0.0124) 0.645 (? 0.015)</cell><cell>0.637</cell></row><row><cell></cell><cell>4-shot</cell><cell>0.687 (? 0.008)</cell><cell>0.653 (? 0.008)</cell><cell cols="2">0.596 (? 0.0122) 0.631 (? 0.023)</cell><cell>0.642</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table 12 :</head><label>12</label><figDesc>XWINO task comparative results per language. Accuracy is used as a metric.</figDesc><table><row><cell>Model</cell><cell>Mode</cell><cell>et</cell><cell>ht</cell><cell>it</cell><cell>id</cell><cell>qu</cell><cell>sw</cell><cell>zh</cell><cell>ta</cell><cell>th</cell><cell>tr</cell><cell>vi</cell><cell>Mean</cell></row><row><cell></cell><cell cols="2">zero-shot 0.520</cell><cell>0.508</cell><cell>0.580</cell><cell>0.592</cell><cell>0.522</cell><cell>0.572</cell><cell>0.524</cell><cell>0.554</cell><cell>0.574</cell><cell>0.566</cell><cell>0.596</cell><cell>0.555</cell></row><row><cell>mGPT XL 1.3B</cell><cell>1-shot 4-shot</cell><cell>0,490 0,496</cell><cell>0,508 0,506</cell><cell>0,582 0,548</cell><cell>0,582 0,582</cell><cell>0,496 0,514</cell><cell>0,548 0,554</cell><cell>0,530 0,544</cell><cell>0,532 0,530</cell><cell>0,562 0,568</cell><cell>0,564 0,576</cell><cell>0,576 0,566</cell><cell>0,543 0,544</cell></row><row><cell></cell><cell>16-shot</cell><cell>0,518</cell><cell>0,516</cell><cell>0,554</cell><cell>0,558</cell><cell>0,498</cell><cell>0,552</cell><cell>0,566</cell><cell>0,528</cell><cell>0,570</cell><cell>0,568</cell><cell>0,584</cell><cell>0,547</cell></row><row><cell></cell><cell cols="2">zero-shot 0.498</cell><cell>0.504</cell><cell>0.616</cell><cell>0.634</cell><cell>0.504</cell><cell>0.576</cell><cell>0.546</cell><cell>0.570</cell><cell>0.540</cell><cell>0.582</cell><cell>0.604</cell><cell>0.562</cell></row><row><cell>mGPT 13B</cell><cell>1-shot 4-shot</cell><cell>0,502 0,486</cell><cell>0,488 0,486</cell><cell>0,608 0,608</cell><cell>0,612 0,626</cell><cell>0,492 0,506</cell><cell>0,564 0,566</cell><cell>0,572 0,584</cell><cell>0,554 0,554</cell><cell>0,526 0,548</cell><cell>0,592 0,574</cell><cell>0,614 0,618</cell><cell>0,557 0,560</cell></row><row><cell></cell><cell>16-shot</cell><cell>0,484</cell><cell>0,522</cell><cell>0,594</cell><cell>0,612</cell><cell>0,490</cell><cell>0,566</cell><cell>0,586</cell><cell>0,560</cell><cell>0,568</cell><cell>0,576</cell><cell>0,618</cell><cell>0,561</cell></row><row><cell></cell><cell cols="2">zero-shot 0.576</cell><cell>0.57</cell><cell>0.492</cell><cell>0.59</cell><cell>0.524</cell><cell>0.55</cell><cell>0.536</cell><cell>0.556</cell><cell>0.578</cell><cell>0.55</cell><cell>0.59</cell><cell>0.555</cell></row><row><cell>XGLM 1.7B</cell><cell>1-shot 4-shot</cell><cell cols="3">0.572 (0.012) 0.568 (0.0074) 0.554 (0.0046) 0.555 (0.005) 0.569 (0.0075) 0.561 (0.010)</cell><cell>0.634 (0.0117) 0.6104 (0.007)</cell><cell>0.503 (0.006) 0.502 (0.009)</cell><cell cols="2">0.561 (0.007) 0.556 (0.006) 0.564 (0.007) 0.555 (0.009)</cell><cell>0.546 (0.003) 0.548 (0.011)</cell><cell>0.572 (0.010) 0.575 (0.009)</cell><cell>0.544 (0.002) 0.539 (0.008)</cell><cell>0.624 (0.004) 0.609 (0.004)</cell><cell>0.568 0.562</cell></row><row><cell></cell><cell>16-shot</cell><cell cols="9">0.579 (0.0029) 0.554 (0.0098) 0.556 (0.0129) 0.6087 (0.0071) 0.503 (0.0058) 0.561 (0.008) 0.540 (0.0077) 0.546 (0.0085) 0.571 (0.008)</cell><cell>0.541 (0.006)</cell><cell>0.607 (0.007)</cell><cell>0.561</cell></row><row><cell></cell><cell cols="2">zero-shot 0.576</cell><cell>0.57</cell><cell>0.492</cell><cell>0.59</cell><cell>0.524</cell><cell>0.55</cell><cell>0.536</cell><cell>0.556</cell><cell>0.578</cell><cell>0.55</cell><cell>0.59</cell><cell>0.555</cell></row><row><cell>XGLM 7.5B</cell><cell>1-shot 4-shot</cell><cell>0.619 (0.010) 0.647 (0.004)</cell><cell cols="2">0.603 (0.0095) 0.638 (0.005) 0.604 (0.011) 0.640 (0.017)</cell><cell>0.674 (0.009) 0.673 (0.005)</cell><cell>0.5 (0.011) 0.5 (0.011)</cell><cell cols="3">0.619 (0.005) 0.581 (0.007) 0.618 (0.009) 0.599 (0.0059) 0.567 (0.004) 0.576 (0.008)</cell><cell cols="2">0.586 (0.0061) 0.599 (0.008) 0.615 (0.0087) 0.601 (0.010)</cell><cell cols="2">0.672 (0.007) 0.685 (0.0041) 0.614 0.606</cell></row><row><cell></cell><cell>16-shot</cell><cell>0.662 (0.004)</cell><cell>0.601 (0.004)</cell><cell>0.659 (0.004)</cell><cell>0.678 (0.008)</cell><cell>0.5 (0.011)</cell><cell cols="3">0.628 (0.002) 0.642 (0.0052) 0.567 (0.003)</cell><cell>0.622 (0.009)</cell><cell cols="2">0.601 (0.0082) 0.717 (0.004)</cell><cell>0.625</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 14 :</head><label>14</label><figDesc>PAWSX task comparative results per language; Accuracy is used as a metric.</figDesc><table><row><cell>Model</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table 15 :</head><label>15</label><figDesc>XNLI task comparative results per language; Accuracy is used as a metric.</figDesc><table><row><cell>Model</cell><cell>Mode</cell><cell>en</cell><cell>es</cell><cell>pt</cell><cell>pl</cell><cell>it</cell><cell>Mean</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table 16 :</head><label>16</label><figDesc>Hate Speech task comparative results per language; Accuracy is used as a metric.</figDesc><table><row><cell>Model</cell><cell>Mode</cell><cell>de</cell><cell>en</cell><cell>es</cell><cell>nl</cell><cell>Mean</cell></row><row><cell>mGPT XL</cell><cell cols="6">zero-shot 0.89 0.82 0.84 0.89 0.86 4-shot 0.87 0.85 0.81 0.87 0.85</cell></row><row><cell>mGPT 13B</cell><cell cols="6">zero-shot 0.88 0.82 0.85 0.89 0.86 4-shot 0.86 0.84 0.80 0.86 0.84</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head>Table 17 :</head><label>17</label><figDesc>XGLUE NER dataset comparative results per language; F1 score is used as a metric.</figDesc><table><row><cell>Model</cell><cell>Mode</cell><cell>ar</cell><cell>bg</cell><cell>de</cell><cell>el</cell><cell>en</cell><cell>es</cell><cell>hi</cell><cell>it</cell><cell>nl</cell><cell>pl</cell><cell>pt</cell><cell>ru</cell><cell>th</cell><cell>tr</cell><cell>ur</cell><cell>vi</cell><cell>zh</cell><cell>Mean</cell></row><row><cell>mGPT XL</cell><cell>zero-shot 4-shot</cell><cell>0.56 0.56</cell><cell>0.39 0.58</cell><cell>0.39 0.59</cell><cell>0.44 0.57</cell><cell>0.39 0.62</cell><cell>0.51 0.63</cell><cell>0.38 0.46</cell><cell>0.48 0.60</cell><cell>0.40 0.62</cell><cell>0.40 0.53</cell><cell>0.51 0.65</cell><cell>0.47 0.58</cell><cell>0.34 0.44</cell><cell>0.40 0.50</cell><cell>0.43 0.49</cell><cell>0.38 0.49</cell><cell>0.46 0.55</cell><cell>0.43 0.56</cell></row><row><cell>mGPT 13B</cell><cell>zero-shot 4-shot</cell><cell>0.56 0.53</cell><cell>0.39 0.51</cell><cell>0.38 0.53</cell><cell>0.46 0.51</cell><cell>0.38 0.60</cell><cell>0.51 0.58</cell><cell>0.38 0.42</cell><cell>0.45 0.56</cell><cell>0.40 0.54</cell><cell>0.39 0.47</cell><cell>0.49 0.61</cell><cell>0.46 0.53</cell><cell>0.34 0.41</cell><cell>0.37 0.43</cell><cell>0.41 0.41</cell><cell>0.37 0.44</cell><cell>0.42 0.48</cell><cell>0.42 0.50</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head>Table 18 :</head><label>18</label><figDesc>XGLUE POS dataset comparative results per language; Precision is used as a metric.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://huggingface.co/EleutherAI 2 https://github.com.com/tensorflow/ mesh</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">https://tensorflow.org/datasets/ catalog/c4 7 We used the 20201101 dump version for each language.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8">https://github.com/pytorch/fairseq/tree/6b770134a2f4474e0a4ddf6c1085c0e06</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">https://textblob.readthedocs.io/en/ dev/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10">https://blog.eleuther.ai/ gpt3-model-sizes/</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I Example Generation</head><p>los animales se clasifican en: (a) pertenecientes al Emperador, (b) asociados con procesos y problemas diversos al comportamiento de los mismos animales, (c) pertenecientes a razas peligrosas, (d) asociadas con la investigaci?n biol?gica y tecnol?gica, (e) asociadas con los m?todos de participaci?n en relaci?n con los animales en los procesos humanos, (f) inmortales, los animales se clasifican en: (a) pertenecientes al Emperador, (b) perteneciente a su ministro y a su aliado militar, (c) con la alianza del emperador.</p><p>French les animaux sont class?s en : (a) appartenant ? l'Empereur, bestoj estas klasifikitaj en: (a) apartenanta al la imperiestro, (b) asociada con procesos estadisticos retrospectivos, (c) con la forma del procedimiento probatorio mas amplia (tercero).</p><p>Afrikaans diere word geklassifiseer in: (a) wat aan die keiser behoort,</p><p>diere word geklassifiseer in: (a) wat aan die keiser behoort, (b) aan die kerkorde onderworpenes, en (c) aan die gereformeerde kerke.</p><p>diere word geklassifiseer in: (a) wat aan die keiser behoort, (b) aan staatgenote, aan elkeen wat buite die raad van presidentstand is, (c) aan masjiene en aan burgers.</p><p>1</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On the cross-lingual transferability of monolingual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representing numbers in NLP: a survey and a vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avijit</forename><surname>Thawani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Pujara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Szekely</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.53</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="644" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">It&apos;s all in the heads: Using attention heads as a baseline for cross-lingual transfer in commonsense reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Tikhonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Ryabinin</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2106.12066</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">It&apos;s All in the Heads: Using Attention Heads as a Baseline for Cross-Lingual Transfer in Commonsense Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Tikhonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Ryabinin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.310</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3534" to="3546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Attention is All You Need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural machine translation with byte-level subwords</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="9154" to="9160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Want to reduce labeling cost? GPT-3 can help</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuohang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-emnlp.354</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2021</title>
		<meeting><address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4195" to="4205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Weidinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Mellor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maribeth</forename><surname>Rauh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Conor</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myra</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Glaese</surname></persName>
		</author>
		<imprint>
			<pubPlace>Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Geoffrey Irving, and Iason Gabriel. 2021. Ethical and social risks of harm from language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">S</forename><surname>Isaac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Legassick</surname></persName>
		</author>
		<idno>abs/2112.04359</idno>
		<imprint>
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Language models are few-shot multilingual learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Genta Indra Winata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojiang</forename><surname>Madotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosanne</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascale</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fung</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.mrl-1.1</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Multilingual Representation Learning</title>
		<meeting>the 1st Workshop on Multilingual Representation Learning<address><addrLine>Punta Cana, Dominican Republic</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Huggingface&apos;s transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Louf</surname></persName>
		</author>
		<idno>abs/1910.03771</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
	<note>Morgan Funtowicz, and Jamie Brew</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Are all languages created equal in multilingual BERT?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Dredze</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.repl4nlp-1.16</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Workshop on Representation Learning for NLP</title>
		<meeting>the 5th Workshop on Representation Learning for NLP</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Klingner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apurva</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshikiyo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideto</forename><surname>Kazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keith</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Kurian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nishant</forename><surname>Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oriol Vinyals</title>
		<imprint>
			<publisher>Greg Corrado, Macduff Hughes</publisher>
		</imprint>
	</monogr>
	<note>and Jeffrey Dean. 2016. Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huilin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoao</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2107.07498</idno>
		<title level="m">Xiang Pan, Xin Tian, Libo Qin, and Hu Hai. 2021. Fewclue: A chinese few-shot learning evaluation benchmark</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11934</idno>
		<title level="m">Aditya Barua, and Colin Raffel. 2020. mt5: A massively multilingual pre-trained text-to-text transformer</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linting</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihir</forename><surname>Kale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Barua</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Paws-x: A cross-lingual adversarial dataset for paraphrase identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1908.11828</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Do language embeddings capture scales?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xikun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanai</forename><surname>Elazar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.439</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4889" to="4896" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Calibrate before use: Improving few-shot performance of language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12697" to="12706" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
