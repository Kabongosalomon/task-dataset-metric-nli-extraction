<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CHANG, LIU, LEE AND HSU: LGTSM FOR DEEP VIDEO INPAINTING Learnable Gated Temporal Shift Module for Deep Video Inpainting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya-Liang</forename><surname>Chang</surname></persName>
							<email>yaliangchang@cmlab.csie.ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><forename type="middle">Yu</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Ying</forename><surname>Lee</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><surname>Hsu</surname></persName>
							<email>whsu@ntu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
								<address>
									<settlement>Taipei</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CHANG, LIU, LEE AND HSU: LGTSM FOR DEEP VIDEO INPAINTING Learnable Gated Temporal Shift Module for Deep Video Inpainting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>How to efficiently utilize temporal information to recover videos in a consistent way is the main issue for video inpainting problems. Conventional 2D CNNs have achieved good performance on image inpainting but often lead to temporally inconsistent results where frames will flicker when applied to videos (see video:Edge-Connect); 3D CNNs can capture temporal information but are computationally intensive and hard to train. In this paper, we present a novel component termed Learnable Gated Temporal Shift Module (LGTSM) for video inpainting models that could effectively tackle arbitrary video masks without additional parameters from 3D convolutions.</p><p>LGTSM is designed to let 2D convolutions make use of neighboring frames more efficiently, which is crucial for video inpainting. Specifically, in each layer, LGTSM learns to shift some channels to its temporal neighbors so that 2D convolutions could be enhanced to handle temporal information. Meanwhile, a gated convolution is applied to the layer to identify the masked areas that are poisoning for conventional convolutions. On the FaceForensics and Free-form Video Inpainting (FVI) dataset, our model achieves state-of-the-art results with simply 33% of parameters and inference time. The source code is available on https://github.com/amjltc295/Free-Form-Video-Inpainting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Free-form video inpainting, to fill in arbitrary missing regions in a video, is a very challenging task. It could be widely used for movie post-processing, damaged video recovery and video editing. For humans, it takes tremendous efforts to recover these missing areas like <ref type="figure">Fig. 1</ref>, while an autonomous method may complete it easily.</p><p>The key for free-form video inpainting is to model spatial-temporal features. That is, a model needs to capture the content of masked areas according to its surroundings and fill in these areas with related pixels. Traditional patch-based methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref> fill in these areas by finding similar patches from other parts of the videos. However, the searching algorithms c 2019. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms. *The two authors contributed equally to this paper. arXiv:1907.01131v2 [cs.CV] 9 Jul 2019 <ref type="figure">Figure 1</ref>: Our model takes videos with free-form masks (first row) and fills in the missing areas with proposed LGTSM to generate realistic completed results (second row) compared to the original videos (third row). It could be applied to video editing tasks such as video object removal, as shown in the first two columns. Best viewed in color and zoom-in. See corresponding videos in the following links: object removal, free-form masks, and faces.</p><p>usually have high computational complexity and the missing area may not be found for complex objects or masks (see <ref type="figure" target="#fig_1">Fig. 4</ref>).</p><p>On the other hand, deep learning methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28]</ref> could fill in unseen masked areas by the encoding and decoding process, based on the structures learned from the training data. Still, compared to the success in image inpainting, deep learning methods struggle to model these video features due to the additional temporal dimension. Using 3D convolution to model spatial-temporal features is the most intuitive way but it requires plenty parameters and is hard to train.</p><p>In this paper, we propose a novel component termed Learnable Gated Temporal Shift Module (LGTSM) to handle free-form video masks with 2D convolutions, motivated by the TSM originally for action recognition. Though inspired by TSM, we found that TSM perfectly suitable for free-form video inpainting as it cannot totally make use of neighboring frames from the beginning layers nor handle irregular masks, which makes us propose LGTSM. Specifically, in each layer, LGTSM learns to shift a part of feature channels in a frame to its neighboring frame, and then attends on masked/inpainted/unmasked areas by a gating convolutional filter. LGTSM enables 2D convolutions to process masked videos and generate state-of-the-art results as 3D convolutions with only 33% parameters and inference time.</p><p>Our paper makes the following contributions:</p><p>? We propose the Gated Temporal Shift Module that could recover videos with freeform masks in 2D convolutions by temporally shifting and gating features in each layer, which reduce the model size and computational time to 33% compared to 3D convolutions.</p><p>? Given that video inpainting requires more information from neighboring frames, we propose a novel Learnable Gated Temporal Shift Module that could learn the temporally shifting kernels and achieved state-of-the-art performance.</p><p>? We propose the TSMGAN loss which significantly improves the model performance for free-form video inpainting. LGTSM, temporal shifting kernels are also learnable and the size could be different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Image Inpainting. Recently, deep learning based methods have taken over the image inpainting task. Xie et al. <ref type="bibr" target="#b26">[27]</ref> firstly apply convolutional neural networks (CNNs) on smallregion image inpainting and denoising. Pathak et al. <ref type="bibr" target="#b18">[19]</ref> then extended <ref type="bibr" target="#b26">[27]</ref> to larger region with an encoder-decoder structure. Moreover, <ref type="bibr" target="#b18">[19]</ref> adopted the generative adversarial network (GAN) <ref type="bibr" target="#b5">[6]</ref>, where a generator learning to create realistic images and a discriminator striving to tell fake ones are trained together to improve the image quality. Subsequently, <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> also developed new GAN architectures with different components for image inpainting. Among these deep methods, Yu et al. <ref type="bibr" target="#b27">[28]</ref> proposed gated convolutions for image inpainting that uses an additional gating convolution to learn the difference between masked, inpainted and unmasked areas in each layer. We integrate such gating mechanism to our model. Also, Nazeri et al. <ref type="bibr" target="#b16">[17]</ref> developed a two-stage model that generates image edges first before recovering the whole images conditioned on edges. Their model achieved state-ofthe-art results, and we set it as one of our baselines. Video Inpainting. Generally, video inpainting could be viewed as an extension of image inpainting with temporal constraints (i.e content in different frames need to be consistent.) However, different from image inpainting, patch-based methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26]</ref> still play a role in video inpainting as more patches are available in videos. Among them, Huang et al. <ref type="bibr" target="#b8">[9]</ref> jointly estimate optical flow and colors in the masked region to fix the moving camera problem and reached state-of-the-art results, so we also set it as one of our baselines.</p><p>Although patch-based methods have made great success in video inpainting, they are highly limited in computational time due to the search algorithms. In addition, the masked areas still need to be patchable; these methods do not work on complex objects such as faces.</p><p>To address these problems, Wang et al. <ref type="bibr" target="#b24">[25]</ref> proposed the first deep learning based method for video inpainting, with a two-stage CombCN that uses 3D convolutions to generate coarse but temporally consistent videos and then refines with 2D convolutions. Their model could learn to recover face videos so we also set it as a baseline. Temporal Modeling. As most state-of-the-art deep video inpainting learning methods adopt the encoder-decoder structure, the key is to model the spatial-temporal structures in videos. Over the past few years, a variety of deep learning architectures have been proposed to model video structures, especially for action recognition. These architectures include applying temporal pooling <ref type="bibr" target="#b10">[11]</ref> or recurrent networks <ref type="bibr" target="#b29">[30]</ref> on top of 2D convolutions to model temporal features structures, combining optical flows and RGB frames to make two-stream networks <ref type="bibr" target="#b21">[22]</ref>, directly using 3D convolutions <ref type="bibr" target="#b23">[24]</ref> and variants of these models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4]</ref>. For more details, we refer readers to <ref type="bibr" target="#b0">[1]</ref>.</p><p>Despite the great performance, many architectures for action recognition cannot be applied to video inpainting since 1) the input video is corrupted, so it is hard to derive optical flows or apply naive convolutions 2) these architectures only need an encoder, while for video inpainting requires a decoder to recover the missing areas.</p><p>Aside from architecture-level temporal modeling, there are also works that focus on the module-level <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref>, which is more applicable for video inpainting. Lin et al. <ref type="bibr" target="#b13">[14]</ref> proposed Temporal Shit Module that shifts part of feature channels in each frame to its neighboring frames so that 2D convolutions could handle temporal information. Similarly, Li et al. <ref type="bibr" target="#b12">[13]</ref> developed Temporal Bilinear (TB) that applies factorized bilinear operation on features to model interactions between frames. Note that these models are for action recognition that all input frames are valid, while for video inpainting, many pixels are masked. Based on these ideas of integrating temporal information to 2D convolutions, we propose the Learnable Gated Temporal Shift Module for video inpainting.</p><p>3 Proposed Method</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learnable Gated Temporal Shift Module</head><p>Models with 3D convolutions could capture temporal information in an intuitive way but are hard to train due to the large number of parameters. To this end, we extend the residual Temporal Shift Module (TSM) <ref type="bibr" target="#b13">[14]</ref>, originally designed for action recognition, to video inpainting. TSM tackles temporal information in 2D convolutions. The input activation F t,x,y for each convolutional layer in video inpainting is in shape of (B, C, L, H, W) where B is the batch size, C is the channel number, L is the temporal length, H is the height and W is the width of the input activation. For each frame in L, TSM shifts a portion of channels to its previous and next frame before the convolutional operations, as shown in <ref type="figure" target="#fig_0">Fig. 2(b)</ref>. These shifted channels contain features from other frames, so together with unshifted features, the original 2D convolutions could learn the temporal structures accordingly.</p><p>However, for free-form video inpainting, not every feature point is valid as many areas are masked. These masked areas are harmful to naive TSM as convolutions cannot tell the difference between valid and invalid feature points. To address this issue, we design the Gated Temporal Shift Module (GTSM) for free-form video inpainting (see <ref type="figure">Fig. 3</ref>). Specifically, in addition to the TSM module, a gating convolutional filter W g is applied to input features F t,x,y to obtain a gating Gating t,x,y . This gating will serve as a soft validity map to identify the masked/unmasked/inpainted areas for the output features Features t,x,y from the TSM module with original convolutional filter W f .</p><p>Mathematically, GTSM could be expressed as:  <ref type="figure">Figure 3</ref>: Module design. We integrate (a) Residual TSM <ref type="bibr" target="#b13">[14]</ref> and (b) gated convolution <ref type="bibr" target="#b27">[28]</ref> to (c) Gated Temporal Shift Module (GTSM) and design learnable temporal shifting kernel <ref type="figure" target="#fig_0">(Fig. 2c)</ref> to make (d) the proposed Learnable Gated Temporal Shift Module (LGTSM).</p><formula xml:id="formula_0">Gating t,x,y = ?? W g ? F t,x,y (1) Features t,x,y = ?? W f ? T SM(F t?1,x,y , F t,x,y , F t+1,x,y )<label>(2)</label></formula><formula xml:id="formula_1">Out put t,x,y = ? (Gating t,x,y )? (Features t,x,y )<label>(3)</label></formula><p>where ? is the sigmoid function that transforms gating to values between 0 (invalid) and 1 (valid), and ? is the activation function for the convolution. Note that the TSM module could be easily modified to online settings (without peeking future frames) for real-time applications.</p><p>Note that the temporal shifting operation in TSM is similar to applying forward/backward shifting kernels on the channel-temporal map, as shown in <ref type="figure" target="#fig_0">Fig. 2(c)</ref>. In TSM, these kernels are fixed; a frame could only get features from its one-frame neighbors in each layer. Such fixed shifting kernels in TSM are insufficient to make use of further neighboring frames as temporal information could only be aggregated through deeper layers. Unlike action recognition, video inpainting models sorely need information from the beginning layers to capture the spatial-temporal structures so that deeper layers could recover the missing areas accordingly.</p><p>Therefore, we also propose the Learnable Gated Temporal Shift Module (LGTSM), where the temporal shifting kernels are also learnable and the kernel size could be larger (see <ref type="figure" target="#fig_0">Fig. 2(d)</ref>). With LGTSM, the model could learn to shift and scale features from specific temporal neighbors in each layer (or not). For example, the model could get features from more temporally further neighbors in the first few layers and remain unshifted in the deeper layers. This greatly enhances the model capability with very little cost.</p><p>In practice, the shifting operation only uses an additional buffer in size of 1/4 channels, so it has little cost in terms of computational time and run-time memory compared to traditional 2D convolutions. Note that the number of kernels could also be flexible (there are only two for TSM: forward and backward). Moreover, LGTSM could learn temporal information with very few extra parameters, and we found that it could achieve state-of-the-art results as the 3D convolutions with only 33% parameters and inference time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Loss Functions</head><p>Design the combination of loss functions to train a video inpainting model is non-trivial due to the uncertainty of the free-form masks and the high complexity of video features. We use l 1 loss for low-level features, perceptual loss and style loss for image content, and propose TSMGAN loss for handle high-level features and enhance realness. In this section, we will introduce these loss functions to train our model.</p><p>Masked l 1 loss. The l 1 loss focuses on the pixel-level features, which is widely used for generative tasks on image <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>, and videos <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b24">25]</ref>:</p><formula xml:id="formula_2">L l 1 = E t,x,y [|O t,x,y ?V t,x,y |]<label>(4)</label></formula><p>Perceptual loss and style loss. l 1 loss often leads to blurry results as it only focus on low-level features. To address this problem, we adopt the perceptual and style loss <ref type="bibr" target="#b4">[5]</ref> to keep image contents. Similar loss functions could be found in many generative tasks such as image inpainting <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref> and super-resolution <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>. The perceptual loss could be viewed as the l 1 loss in feature level:</p><formula xml:id="formula_3">L perc = n ? t=1 P?1 ? p=0 |? O t p ? ? V t p | N ? V t p (5)</formula><p>where V t is the input video, ? V t p is the activation from the pth selected layer of the pretrained network, and N ? V t p is the number of elements in the pth layer. We choose layer relu 2_2 , relu 3_3 and relu4_3 from the VGG <ref type="bibr" target="#b22">[23]</ref> network pre-trained on ImageNet <ref type="bibr" target="#b20">[21]</ref>.</p><p>Style loss is a variant of perceptual loss, with an auto-correlation (Gram matrix) applied to the features first:</p><formula xml:id="formula_4">L style = n ? t=1 P?1 ? p=0 1 C p C p |(? O t p ) T (? O t p ) ? (? V t p ) T (? V t p ))| C p H p W p (6)</formula><p>where ? O t p and ? V t p are both features from the pre-trained VGG network, as the ones in the perceptual loss 5.</p><p>TSMGAN loss. All the aforementioned loss functions are for image only, which do not take temporal consistency into consideration. Therefore, we develop TSMGAN to learn temporal consistency. We set up a generative adversarial network (GAN) with Gated Temporal Shit Module integrated on both the generator and discriminator as stated in 3.1.</p><p>The TSMGAN discriminator is composed of six 2D convolutional layers with TSM. Also, we apply the recently proposed spectral normalization <ref type="bibr" target="#b15">[16]</ref> to both the generator and discriminator as <ref type="bibr" target="#b16">[17]</ref> to enhance the training stability. The TSMGAN loss L D for the discriminator to tell if the input video z is real or fake and L G for the generator to fool the discriminator are defined as:</p><formula xml:id="formula_5">L D = E x?P data (x) [ReLU(1 + D(x))] + E z?P z (z) [ReLU(1 ? D(G(z)))]<label>(7)</label></formula><formula xml:id="formula_6">L G = ?E z?P z (z) [D(G(z))]<label>(8)</label></formula><p>As <ref type="bibr" target="#b27">[28]</ref>, the kernel size is 5 ? 5, stride 2 ? 2 and the shifting operation is applied to all 11 convolutional layers for the TSMGAN discriminator, so the receptive field of each output feature point includes the whole video. It could be viewed as several GANs on different feature points, and a local-global discriminator structure <ref type="bibr" target="#b28">[29]</ref> is thus not needed.</p><p>Besides, the TSMGAN learns to classify real or fake for each spatial-temporal feature point from the last convolutional layer in the discriminator, which mostly consists of highlevel features. Since the l 1 loss already focuses on low-level features, using TSMGAN could improve the model in an efficient way.</p><p>Overall loss. The overall loss function to train the model is defined as:</p><formula xml:id="formula_7">L total = ? l 1 L l 1 + ? perc L perc + ? style L style + ? G L G<label>(9)</label></formula><p>where ? l 1 , ? perc , ? style and ? G are the weights for l 1 loss, perceptual loss, style loss and TSMGAN loss, respectively.  <ref type="table">Table 1</ref>: Quantitative comparison with baseline models on the FaceForensics and FVI dataset based on <ref type="bibr" target="#b1">[2]</ref>. The results of FVI dataset are averaged of seven mask-to-frame ratios; detailed results of each ratio could be found in the supplementary materials. *TCCDS failed on some cases and the results are averaged of successful ones. # runs on CPU; others are on GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network Design</head><p>The model has a U-net like generator and a TSMGAN discriminator. The generator is composed of 11 convolutional layers with the proposed Gated Temporal Shift Module, including down-sampling, dilated and up-sampling ones. Similar structures are also adopted for state-of-the-art image inpainting models <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b27">28]</ref>. Unlike U-net, there is no skip connection as there are many masked areas in the down-sampling layers. For down-sampling and up-sampling layers, we apply bilinear interpolation before convolutions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setups</head><p>Datasets. To compare with the baselines <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b24">25]</ref>, we train and test our model on the FaceForensics <ref type="bibr" target="#b19">[20]</ref> and Free-form Video Inpainting (FVI) <ref type="bibr" target="#b1">[2]</ref> dataset. Both datasets are based on videos from YouTube, so they are close to real world scenarios. FaceForensics is composed of 1,004 videos with face, news-caster or newsprogram tags. There are only frontal faces cropped to 128 ? 128 in the FaceForensics dataset, so it is rather simple for learning based models. Amongst, 150 videos are for evaluation while the rest are for training.</p><p>On the other hand, the FVI dataset contains 15,000 high-resolution videos with human activities, animals, natural scenes, etc. It also provides algorithms to generate free-form video masks for training. We re-size videos to 320 ? 180 and split 100 videos for evaluation following the setup in <ref type="bibr" target="#b1">[2]</ref>. Note the FVI dataset is considered more challenging as the videos are very diverse. Training and Testing. Empirically we found that the model converges slower when directly trained as a whole. Thus, during training, we first pre-train the generator without the TSMGAN loss until convergence, and then fine-tune with the TSMGAN. We initialize the temporal shifting kernels in the LGTSM with the values that are equivalent to the original TSM. The pre-train stage takes about 1 day, while the fine-tune stage takes about 3 days on the FVI dataset, which is 3 times faster, reducing training time from 10 days to 3 days, than the model with 3D convolutions <ref type="bibr" target="#b1">[2]</ref>, demonstrating the merits of the proposed module. For other implementation details, please see the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Results</head><p>As <ref type="bibr" target="#b1">[2]</ref>, mean square error (MSE) and Learned Perceptual Image Patch Similarity (LPIPS) <ref type="bibr" target="#b30">[31]</ref> are used to evaluate the image quality; the Fr?chet Inception Distance (FID) <ref type="bibr" target="#b7">[8]</ref> with pre-trained I3D <ref type="bibr" target="#b0">[1]</ref> is used to evaluate video quality and temporal consistency.</p><p>We compare with state-of-the-art baselines with different strategies: the patch-based video inpainting method TCCDS by Huang et al. <ref type="bibr" target="#b8">[9]</ref>, two-stage deep image inpainting method Edge-Connect (EC) by Nazeri et al. <ref type="bibr" target="#b16">[17]</ref>, two-stage deep video inpainting method CombCN by Wang et al. <ref type="bibr" target="#b24">[25]</ref>, and one-stage deep video inpainting method with 3D gated convolutions (3DGated) by Chang et al. <ref type="bibr" target="#b1">[2]</ref>. We train all learning based models on the FaceForensics and FVI datasets with free-form masks from <ref type="bibr" target="#b1">[2]</ref> for fair comparison.</p><p>The averaged results of 7 ranges of mask-to-frame ratio from 0 -10% to 60 -70% on the FaceForensics and FVI testing are shown in <ref type="table">Table 1</ref>. We could see that our model is on par with the state-of-the-art method 3DGated <ref type="bibr" target="#b1">[2]</ref> in terms of perceptual distance (LPIPS and FID) and video quality (FID) with only 33% of parameters and inference time (note that the results are averaged; our model performs better for some mask-to-frame ratios). TCCDS failed on many cases since the masks are irregular and it cannot properly recover partially masked objects. Edege-connect (EC) performs better on FaceForensics dataset with bounding box masks because faces are all aligned and the generated edges could be stable. Still, it has serious temporal inconsistent problem under other circumstances (see <ref type="figure" target="#fig_1">Fig. 4</ref>  <ref type="table">Table 2</ref>: Ablation study on the FVI dataset with object-like masks. Number of parameters are shown as generator/discriminator. *We increase the channel of vanilla convolution to fairly compare with gated convolutions.</p><p>FVI dataset (see <ref type="figure" target="#fig_1">Fig. 4</ref>) and require more parameters for the generator than our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Results</head><p>From <ref type="figure" target="#fig_1">Fig. 4</ref> we could observe that the proposed model outperforms TCCDS (wrong patches), Edge-connect (temporally inconsistent) and CombCN (blurry), while almost the same as 3DGated. More visual comparison could be found in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In order to validate the contribution of each component, we also conduct an ablation study on the FVI dataset ( <ref type="table">Table 2</ref>). We could observe that both the gated convolution and TSMGAN play important roles in our model; without them, the model will have a significant drop of performance. The proposed learnable shifting kernel further improves the performance with almost no additional parameters and achieve state-of-the-art results. Visual comparison be found in the supplementary materials and videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Future Work</head><p>Our model achieves state-of-the-art performance with LGTSM and 2D convolutions. However, the performance is still a bit lower than the 3D convolutions in <ref type="bibr" target="#b1">[2]</ref>. How to design a module that could handle temporal information in a more efficient way is still a challenging future work.</p><p>Another future work is to extend the input videos to higher or arbitrary resolution. For now, deep learning models are limited to a fixed resolution, which is not sufficient for exquisite videos. Simply applying these models to different parts of a high-resolution video may cause in spatial inconsistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper presented a novel Learnable Gated Temporal Shift Module (LGTSM) for freeform video inpainting. LGTSM learns to shift some channels to its temporal neighbors in each frame and apply gating filter to attend on masked/inpainted/unmasked areas, which enables 2D convolutions to process temporal information and tackle poisoning masked areas at the same time. In addition, LGTSM is highly efficient, using only 33% of parameters and inference time compared to the state-of-the-art model with 3D convolutions. Experiments on the FaceForensics and FVI dataset suggest that the proposed model reach state-of-the-art performance in terms of evaluation metrics and visual results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgement</head><p>This work was supported in part by the Ministry of Science and Technology, Taiwan, under Grant MOST 108-2634-F-002-004. We also benefit from the NVIDIA grants and the DGX-1 AI Supercomputer. We are grateful to the National Center for High-performance Computing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Explanation of the learnable shifting kernels in the proposed LGTSM. (a) Input features for the layer. We will do shifting operation on channel ? time dimensions. (b) Original TSM from [14]. (c) Equivalent TSM by temporal shifting kernels. (d) In the proposed</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Visual comparison with the baselines on the FVI testing set with object-like masks. Best viewed in color and zoom-in. See video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>). Although CombCN has lowest MSE scores, it could only generates blurry results for the</figDesc><table><row><cell>3D conv.</cell><cell>TSM</cell><cell>Learnable shifting</cell><cell>Gated conv.</cell><cell cols="2">GAN LPIPS? FID ?</cell><cell>Param. ?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1209</cell><cell cols="2">1.034 36M+18M</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.2048</cell><cell cols="2">1.303 12M*+6M</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1660</cell><cell>1.198</cell><cell>12M</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.1256</cell><cell>1.091</cell><cell>12M+6M</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.1213 1.039</cell><cell>12M+6M</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Free-form video inpainting with 3d gated convolution and temporal patchgan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya-Liang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><forename type="middle">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><surname>Hsu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10247</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya-Liang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><forename type="middle">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vornet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.06726</idno>
		<title level="m">Spatio-temporally consistent video inpainting for object removal</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A neural algorithm of artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How not to be seen???object removal from videos of crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Granados</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Tompkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Grau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Temporally coherent completion of dynamic video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Sing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narendra</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">196</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4681" to="4690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Temporal bilinear networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.09974</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08383</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Image inpainting for irregular holes using partial convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitsum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">J</forename><surname>Reda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Catanzaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07723</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Edgeconnect: Generative image inpainting with adversarial edge learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamyar</forename><surname>Nazeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faisal</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehran</forename><surname>Ebrahimi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Video inpainting of complex scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alasdair</forename><surname>Newson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?s</forename><surname>Almansa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Fradet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Gousseau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1993" to="2019" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2536" to="2544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Faceforensics: A large-scale video dataset for forgery detection in human faces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>R?ssler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Cozzolino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Verdoliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Riess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justus</forename><surname>Thies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nie?ner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09179</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for largescale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video inpainting by jointly learning temporal structure and spatial details</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 33th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Space-time completion of video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="463" to="476" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyuan</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linli</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03589</idno>
		<title level="m">Freeform image inpainting with gated convolution</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Huang</surname></persName>
		</author>
		<title level="m">Generative image inpainting with contextual attention</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe Yue-Hei</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
