<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PYSKL: Towards Good Practices for Skeleton Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-19">19 May 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haodong</forename><surname>Duan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of HongKong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of HongKong</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Shanghai AI Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PYSKL: Towards Good Practices for Skeleton Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-19">19 May 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present PYSKL: an open-source toolbox for skeletonbased action recognition based on PyTorch. The toolbox supports a wide variety of skeleton action recognition algorithms, including approaches based on GCN and CNN.   In contrast to existing open-source skeleton action recognition projects that include only one or two algorithms, PYSKL implements six different algorithms under a unified framework with both the latest and original good practices to ease the comparison of efficacy and efficiency. We also provide an original GCN-based skeleton action recognition model named ST-GCN++, which achieves competitive recognition performance without any complicated attention schemes, serving as a strong baseline. Meanwhile, PYSKL supports the training and testing of nine skeletonbased action recognition benchmarks and achieves stateof-the-art recognition performance on eight of them. To facilitate future research on skeleton action recognition, we also provide a large number of trained models and detailed benchmark results to give some insights. PYSKL is released at https://github.com/kennymckormick/pyskl and is actively maintained. We will update this report when we add new features or benchmarks. The current version corresponds to PYSKL v0.2.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Skeleton-based action recognition focuses on performing action recognition and video understanding based on human skeleton sequences. Compared to other modalities (like RGB / Flow), skeleton data (2D / 3D human joint coordinates) are compact yet informative and robust to illumination changes or scene variations. Due to these good properties, skeleton action recognition has attracted increasing attention in recent years. Various algorithms are developed to perform skeleton-based action recognition, which can be mainly categorized as GCN-based approaches and CNN-based approaches.</p><p>Since ST-GCN [23] first proposed to use Graph Convolutional Networks for skeleton processing, GCN-based approaches soonly became the most popular paradigm in skeleton-based action recognition. ST-GCN directly takes the sequence of joint coordinates as inputs and models skeleton data with a GCN backbone. The GCN backbone consists of alternating spatial graph convolutions and temporal convolutions for spatial and temporal modeling. Following works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr">24,</ref><ref type="bibr">25]</ref> inherited its basic design and made different improvements by: 1) developing better graph topologies, either manual <ref type="bibr" target="#b12">[13]</ref> or learnable <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr">24,</ref><ref type="bibr">25]</ref>; 2) training skeleton-based action recognition jointly with other auxiliary tasks <ref type="bibr" target="#b9">[10]</ref>; 3) adopting better data pre-processing, training, and testing strategies <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18]</ref>. With these improvements, there comes great progress in recognition performance. For instance, on NTURGB+D-Xsub <ref type="bibr" target="#b14">[15]</ref> benchmark, the improvement of Top-1 Acc is over 10%: from 81.5% (ST-GCN [23], 2018) to 92.4% (CTR-GCN <ref type="bibr" target="#b2">[3]</ref>, 2021).</p><p>Despite the considerable improvements, the settings of different GCN approaches do not align well. For example, ST-GCN only reports the recognition performance with a single joint-stream, while most following works report the performance with an ensemble of joint / bone-stream (first proposed by 2s-AGCN <ref type="bibr" target="#b16">[17]</ref>) or even four streams (first proposed by MS-AAGCN <ref type="bibr" target="#b17">[18]</ref>). Besides, the pre-processing techniques (skeleton alignments, temporal padding, denoising, etc.) also differ a lot. However, existing open-source repositories <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr">23]</ref> only implement a single algorithm with their own practices. To the best of our knowledge, none of them had ever compared these architectures under a unified setting. Therefore, we developed PYSKL, which includes implementations of representative GCN approaches under a unified framework. We trained and tested each algorithm with all the latest and original good practices on multiple benchmarks. Surprisingly, we find that the recognition performance of different GCNs does not vary a lot: the extreme deviations of Top-1 Acc are less than 2% on all four NTURGB+D benchmarks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>. Especially, on NTURGB+D XView, the current state-of-the-art CTR-GCN <ref type="bibr" target="#b2">[3]</ref> only outperforms the original ST-GCN [23] by 0.5%.</p><p>Given the pilot experiments, we find that good practices contribute more to achieving strong recognition performance rather than a complicated architectural design. This report presents all the good practices we adopted for training GCN-based models for skeleton-based action recognition. The practices include different aspects, including data pre-processing, spatial / temporal data augmentations, and hyper-parameter settings. Besides that, we also propose an original GCN algorithm named ST-GCN++. With simple modifications on top of the ST-GCN, we achieve strong recognition performance comparable with the stateof-the-art without any complicated attention mechanism. ST-GCN++ can serve as a strong baseline for future research on skeleton-based action recognition.</p><p>Another paradigm for skeleton-based action recognition leverages Convolutional Neural Networks to process skeleton data. These approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr">22]</ref> represent human joints as Gaussian maps and aggregate them as pseudo images or video clips. The generated inputs are processed by 2D-CNN or 3D-CNN. We implement a recent state-of-the-art 3D-CNN based approach PoseC3D <ref type="bibr" target="#b5">[6]</ref> in PYSKL. PoseC3D can achieve strong recognition performance on skeletonbased action recognition benchmarks and has unique advantages (like robustness, scalability, interoperability) compared to GCNs. However, it is much heavier than most of the existing GCN approaches due to its 3D-CNN backbone.</p><p>To summarize, PYSKL implements six representative skeleton-based action recognition approaches and supports nine different benchmarks. It provides extensive benchmark results for five GCNs on skeleton-based action recognition, including four NTURGB+D benchmarks, four skeleton modalities, and two annotation types (3D / 2D skeleton).</p><p>PYSKL is released at https://github.com/kennymckormick/pyskl under the Apache-2.0 License. The repository contains all the source code, a large-scale model zoo, detailed instructions for installation, dataset preparation, and (distributed) training and testing. PYSKL also provides tools for visualizing 2D / 3D skeletons and performing skeleton-based action recognition on custom datasets with no skeleton information available.</p><p>2. GCN-based approaches 2.1. Good Practices for GCN-based approaches 2.1.1 Data Pre-processing Skeleton sequences of different videos may have different temporal lengths, different numbers of persons, and may be captured by sensors from different views or with different setups. For 3D skeletons <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref> captured with Kinect sensors [26], various pre-processing approaches are adopted. ST-GCN uses no extra pre-processing and pads all sequences to a maximum length with zero padding. 2s-AGCN, however, performs pre-normalization by 1) aligning the center point of the person in the 1 st frame with the origin of the 3D-Cartesian coordinate system; 2) rotating all skeletons so that the spine of the person in the first frame is parallel with the z-axis in the 3D-Cartesian coordinate system. Besides, 2s-AGCN pads skeleton sequences to a maximum length with loop padding. CTR-GCN follows the spatial pre-processing used by 2s-AGCN. However, it keeps the original length of each skeleton sequence and uses different criteria for manual denoising. PYSKL follows the pre-processing approach of CTR-GCN. For 2D skeletons predicted by pose estimators <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20]</ref>, we pre-normalize them into a fixed range (like [0, 1] or [-1, 1]) following <ref type="bibr">[23]</ref>. We also perform simple pose-based tracking to form 1 or 2 skeleton sequences for NTURGB+D data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2">Temporal Augmentations</head><p>Most GCN works do not use any temporal augmentations. Among representative GCN approaches, CTR-GCN adopts random cropping as temporal augmentations. It crops a substring from the entire skeleton sequence (substring length ratio may vary from 50% to 100%) and resize the substring to a fixed length of 64 with bilinear interpolation. Inspired by <ref type="bibr" target="#b5">[6]</ref>, we use Uniform Sampling as the temporal augmentation strategy. To generate a skeleton sequence of length M (M=100 in PYSKL), we divide the original sequence uniformly into M splits with equal lengths and randomly sample one frame per split. The sampled skeletons will be joined again and form the input sequence. With Uniform Sampling, we can generate numerous data samples with similar distribution to the source data (no interpolation used).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3">Hyper Parameter Setting</head><p>The hyper parameter settings differ a lot in previous works for skeleton-based action recognition using GCN. In PYSKL, we use the same hyper parameter setting to train all GCN models. We set the initial learning rate to 0.1, batch size to 128, and train each model for 80 epochs with the CosineAnnealing LR scheduler. For the optimizer, we set the momentum to 0.9, weight decay to 5?10 ?4 , and use the Nesterov momentum. We find that for most GCN networks, the new hyper parameter setting leads to better recognition performance than previous settings that use the MultiStep LR scheduler.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The Design of ST-GCN++</head><p>We also propose an original GCN model named ST-GCN++. With only simple modifications to the original ST-GCN, ST-GCN++ achieves strong recognition performance comparable with the state-of-the-art approach with a complicated attention mechanism. Meanwhile, the computational overhead is also reduced. ST-GCN++ modifies the design of the interleaving spatial modules (spatial graph convolutions) and temporal modules (temporal 1D convolutions).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Spatial Module Design</head><p>In ST-GCN, pre-defined sparse coefficient matrices are used for fusing features of different joints belonging to the same person, while the coefficient matrices are derived from a pre-defined joint topology. Meanwhile, ST-GCN also reweights each element in coefficient matrices with a set of learnable weights. However, in ST-GCN++, we only use the pre-defined joint topology to initialize the coefficient matrices. We update the coefficient matrices iteratively with gradient descent during training without any sparse constraints. Besides, we also add a residual link in the spatial module, which further improves the spatial modeling capability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2">Temporal Module Design</head><p>A vanilla ST-GCN uses a single 1D convolution on the temporal dimension with kernel size 9 for temporal modeling. The large kernel covers a wide temporal receptive field. However, this design lacks flexibility and results in redundant computations and parameters. Inspired by <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref>, we use a multi-branch temporal ConvNet (TCN) to replace the single branch design. The adopted multi-branch TCN consists of six branches: a '1x1' Conv branch, a Max-Pooling branch, and four temporal 1D Conv branches with kernel size 3 and dilations from 1 to 4. It first transforms features with '1x1' Conv and divides them into six groups with equal  channel width. Then, each feature group is processed with a single branch. The six outputs are concatenated together and processed by another '1x1' Conv to form the output of the multi-branch TCN. The new TCN design not only improves the temporal modeling capabililty, but also saves the computational cost and parameters, due to the reduced channel width for every single branch. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Benchmarking GCN Algorithms</head><p>In PYSKL, we benchmark four representative GCN approaches: ST-GCN [23], AAGCN <ref type="bibr" target="#b17">[18]</ref>, MS-G3D <ref type="bibr" target="#b12">[13]</ref>, CTR-GCN <ref type="bibr" target="#b2">[3]</ref>, as well as the original ST-GCN++ on four NTURGB+D benchmarks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b14">15]</ref>. For skeleton annotations, we consider 3D skeletons generated with CTR-GCN pre-processing and 2D skeletons estimated by HRNet <ref type="bibr" target="#b19">[20]</ref>. We report the Top-1 Accuracy of joint-stream, bone-stream, two-stream fusion (joint + bone), and four-stream fusion (joint + bone + joint motion + bone motion), respectively. This report lists the benchmark results when using 3D skeletons in <ref type="table">Table 1</ref>, 2. The results for 2D skeletons can be found in the repository.</p><p>Unlike the performance originally reported, we find that the accuracy gaps between different GCN approaches are much smaller. On all NTURGB+D benchmarks, the extreme deviation of Top-1 Accuracy is less than 2%. For all algorithms except CTR-GCN, the reproduced results are better than reported due to the adopted good practices 1 . Moreover, our ST-GCN++ achieves strong recognition performance competitive with state-of-the-art GCN approaches, with a much simpler design, fewer parameters, and fewer FLOPs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Spatial Augmentations</head><p>We also adopt spatial augmentations in skeleton action recognition. We implement three augmentations in PYSKL: 1) Random Rotating: Rotate all skeletons (2D / 3D) with the same random angle</p><formula xml:id="formula_0">? (? = (? x , ? y ) ? R 2 or ? = (? x , ? y , ? z ) ? R 3 ), each element in ? is sampled from a uniform distribution [-0.3, 0.3].</formula><p>2) Random Scaling: Scale all joint coordinates in a skeleton sequence with the same scale factor r (r = (r x , r y ) ? R 2 or r = (r x , r y , r z ) ? R 3 ), each element in r is sampled from a uniform distribution [-0.1, 0.1] or [-0.2, 0.2] (for 3D / 2D skeletons).</p><p>3) Random Gaussian Noise: Randomly add a small Gaussian noise for each joint. The noise can be framespecific or frame-agnostic.</p><p>Extensive experiments are conducted to validate the effi-cacy of three spatial augmentations (results in <ref type="table" target="#tab_1">Table 3</ref>). We find that among the three augmentations, random rotating works for 3D skeletons; random scaling works for both 2D and 3D skeletons; while random Gaussian noise does not work for any kinds of skeletons. We train ST-GCN++ with 3D skeletons for 120 epochs with random rotating and random scaling. <ref type="table" target="#tab_2">Table 4</ref> shows that on 3 of 4 NTURGB+D benchmarks, ST-GCN++ surpasses the current state-of-theart CTR-GCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CNN-based approaches</head><p>PYSKL also implements the 3D-CNN based approach PoseC3D <ref type="bibr" target="#b5">[6]</ref>.</p><p>PoseC3D takes 2D human skeletons as inputs. It first generates Gaussian maps given the 2D joint coordinates and then organizes them as a 3D heatmap volume. PoseC3D can use an arbitrary 3D-CNN for 3D heatmap volume processing. In PYSKL, we support three backbones: C3D [21], SlowOnly <ref type="bibr" target="#b7">[8]</ref>, X3D <ref type="bibr" target="#b6">[7]</ref>, and release PoseC3D trained on seven different datasets: NTURGB+D <ref type="bibr" target="#b14">[15]</ref>, NTURGB+D 120 <ref type="bibr" target="#b11">[12]</ref>, Kinetics-400 <ref type="bibr" target="#b1">[2]</ref>, UCF101 <ref type="bibr" target="#b18">[19]</ref>, HMDB51 <ref type="bibr" target="#b8">[9]</ref>, Fine-GYM <ref type="bibr" target="#b15">[16]</ref>, and Diving48 <ref type="bibr" target="#b10">[11]</ref>. PoseC3D has good spatiotemporal modeling capability and achieves state-of-the-art recognition performance on 6 of 9 benchmarks. However, using 3D-CNN for skeleton processing consumes more computations and is much slower than representative GCN approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusion</head><p>We have publicly released PYSKL, which has extensive benchmarks for skeleton-based action recognition. PYSKL has implemented six representative algorithms under a unified framework, trained them on nine different skeletonbased action recognition benchmarks, and achieved stateof-the-art recognition performance on eight benchmarks <ref type="table" target="#tab_3">(Table 5</ref>). It has offered good practices for training skeleton-based action recognition models and provided extensive benchmarks. Besides, it also introduced a simple and strong baseline named ST-GCN++, which surpasses previous state-of-the-art on the NTURGB+D benchmarks. We hope this repository, along with all the released training configurations and model weights will facilitate future research in this area.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Benchmarking GCN skeleton-based action recognition algorithms on the NTURGB+D benchmark. Inputs are 3D skeletons with 25 joints. We set the input length to 100, input person number to 2, and apply all good practices introduced in Sec 2.1. Benchmarking GCN skeleton-based action recognition algorithms on the NTURGB+D 120 benchmark. Inputs are 3D skeletons with 25 joints. We set the input length to 100, input person number to 2, and apply all good practices introduced in Sec 2.1.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">NTURGB+D XSub</cell><cell></cell><cell></cell><cell cols="2">NTURGB+D XView</cell><cell></cell><cell cols="2">Computational Efficiency</cell></row><row><cell>Model</cell><cell>Joint</cell><cell>Bone</cell><cell>2s</cell><cell>4s</cell><cell>Joint</cell><cell>Bone</cell><cell>2s</cell><cell>4s</cell><cell>GFLOPs</cell><cell>MParams</cell></row><row><cell>ST-GCN [23]</cell><cell>87.8</cell><cell>88.6</cell><cell>90.0</cell><cell>90.7</cell><cell>95.5</cell><cell>95.0</cell><cell>96.2</cell><cell>96.5</cell><cell>5.34</cell><cell>3.08</cell></row><row><cell>AAGCN [18]</cell><cell>89.0</cell><cell>89.2</cell><cell>90.8</cell><cell>91.5</cell><cell>95.7</cell><cell>95.2</cell><cell>96.4</cell><cell>96.7</cell><cell>6.07</cell><cell>3.77</cell></row><row><cell>MS-G3D [13]</cell><cell>89.6</cell><cell>89.3</cell><cell>91.0</cell><cell>91.7</cell><cell>95.9</cell><cell>95.0</cell><cell>96.4</cell><cell>96.9</cell><cell>10.27</cell><cell>3.17</cell></row><row><cell>CTR-GCN [3]</cell><cell>89.6</cell><cell>90.0</cell><cell>91.5</cell><cell>92.1</cell><cell>95.6</cell><cell>95.4</cell><cell>96.6</cell><cell>97.0</cell><cell>2.82</cell><cell>1.43</cell></row><row><cell>ST-GCN++</cell><cell>89.3</cell><cell>90.1</cell><cell>91.4</cell><cell>92.1</cell><cell>95.6</cell><cell>95.5</cell><cell>96.7</cell><cell>97.0</cell><cell>2.80</cell><cell>1.39</cell></row><row><cell></cell><cell></cell><cell cols="2">NTURGB+D 120 XSub</cell><cell></cell><cell></cell><cell cols="2">NTURGB+D 120 XSet</cell><cell></cell><cell cols="2">Computational Efficiency</cell></row><row><cell>Model</cell><cell>Joint</cell><cell>Bone</cell><cell>2s</cell><cell>4s</cell><cell>Joint</cell><cell>Bone</cell><cell>2s</cell><cell>4s</cell><cell>GFLOPs</cell><cell>MParams</cell></row><row><cell>ST-GCN [23]</cell><cell>82.1</cell><cell>83.7</cell><cell>85.6</cell><cell>86.2</cell><cell>84.5</cell><cell>85.8</cell><cell>87.5</cell><cell>88.4</cell><cell>5.34</cell><cell>3.08</cell></row><row><cell>AAGCN [18]</cell><cell>82.8</cell><cell>84.7</cell><cell>86.3</cell><cell>86.9</cell><cell>84.8</cell><cell>86.2</cell><cell>88.1</cell><cell>88.8</cell><cell>6.07</cell><cell>3.77</cell></row><row><cell>MS-G3D [13]</cell><cell>84.0</cell><cell>85.3</cell><cell>86.9</cell><cell>87.8</cell><cell>86.0</cell><cell>87.3</cell><cell>88.9</cell><cell>89.6</cell><cell>10.27</cell><cell>3.17</cell></row><row><cell>CTR-GCN [3]</cell><cell>84.0</cell><cell>85.9</cell><cell>87.5</cell><cell>88.1</cell><cell>85.9</cell><cell>87.4</cell><cell>89.2</cell><cell>89.9</cell><cell>2.82</cell><cell>1.43</cell></row><row><cell>ST-GCN++</cell><cell>83.2</cell><cell>85.6</cell><cell>87.0</cell><cell>87.5</cell><cell>85.6</cell><cell>87.5</cell><cell>89.1</cell><cell>89.8</cell><cell>2.80</cell><cell>1.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 .</head><label>3</label><figDesc>Benchmarking spatial augmentations using ST-GCN++ on two NTURGB+D 120 benchmarks. Random rotation works for 3D skeletons, while random scaling works for both 2D and 3D skeletons.</figDesc><table><row><cell></cell><cell></cell><cell cols="4">NTURGB+D 120 XSub NTURGB+D 120 XSet</cell></row><row><cell cols="3">Spatial Augs Anno Joint Bone</cell><cell>2s</cell><cell>Joint Bone</cell><cell>2s</cell></row><row><cell>None</cell><cell>3D</cell><cell>83.2 85.6</cell><cell>87.0</cell><cell>85.6 87.5</cell><cell>89.1</cell></row><row><cell>Rot</cell><cell>3D</cell><cell>83.8 86.0</cell><cell>87.7</cell><cell>86.8 88.0</cell><cell>89.9</cell></row><row><cell>Scale</cell><cell>3D</cell><cell>84.0 85.9</cell><cell>87.7</cell><cell>86.3 87.3</cell><cell>89.2</cell></row><row><cell>Rot + Scale</cell><cell>3D</cell><cell>84.7 86.0</cell><cell>87.9</cell><cell>86.6 88.0</cell><cell>89.8</cell></row><row><cell>None</cell><cell>2D</cell><cell>84.4 84.8</cell><cell>86.4</cell><cell>88.1 88.5</cell><cell>90.0</cell></row><row><cell>Scale</cell><cell>2D</cell><cell>85.1 85.7</cell><cell>87.1</cell><cell>88.7 90.0</cell><cell>90.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>ST-GCN++ trained with good practices and spatial augmenatations surpasses CTR-GCN (official performance) on 3 of 4 NTURGB+D benchmarks.</figDesc><table><row><cell></cell><cell cols="2">NTURGB+D</cell><cell cols="2">NTURGB+D 120</cell></row><row><cell>Model</cell><cell>XSub</cell><cell>XView</cell><cell>XSub</cell><cell>XSet</cell></row><row><cell>CTR-GCN</cell><cell>92.4</cell><cell>96.8</cell><cell>88.9</cell><cell>90.6</cell></row><row><cell>ST-GCN++</cell><cell>92.6</cell><cell>97.4</cell><cell>88.6</cell><cell>90.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc>The best performance achieved by PYSKL on nine benchmarks. On 8 of 9 benchmarks for skeleton action recognition, PYSKL achieves the best recognition accuracy. We report the Top-1 accuracy for all benchmarks except FineGYM (for which we report the mean class accuracy). For Diving48, we use the V2 annotations.</figDesc><table><row><cell></cell><cell cols="4">NTURGB+D (3D) NTURGB+D 120 (3D)</cell><cell></cell><cell cols="3">Datasets with 2D skeleton annotations</cell><cell></cell></row><row><cell></cell><cell>XSub</cell><cell>XView</cell><cell>XSub</cell><cell>XSet</cell><cell cols="5">Kinetics-400 UCF101 HMDB51 FineGYM Diving48</cell></row><row><cell cols="4">Previous SOTA 92.4 [3] 96.8 [3] 88.9 [3]</cell><cell>90.6 [3]</cell><cell>38.6 [14]</cell><cell cols="2">69.1 [22] 53.5 [22]</cell><cell>N.A.</cell><cell>N.A.</cell></row><row><cell>PYSKL</cell><cell>92.6</cell><cell>97.4</cell><cell>88.6</cell><cell>90.8</cell><cell>49.1</cell><cell>86.9</cell><cell>69.4</cell><cell>94.1</cell><cell>54.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For CTR-GCN, the performance dropped a little, since spatial augmentations (like random rotation) are used in the original paper, but not used in this benchmark.</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conference on Computer Vision and</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Channel-wise topology refinement graph convolution for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with shift graph convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="183" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Potion: Pose motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Choutas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philippe</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7024" to="7033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Revisiting skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haodong</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.13586</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Est?baliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Actional-structural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3595" to="3603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Resound: Towards action recognition without representation bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="513" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Ntu rgb+d 120: A large-scale benchmark for 3d human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mauricio</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling-Yu</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2684" to="2701" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Disentangling and unifying graph convolutions for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal extension module for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuya</forename><surname>Obinata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuma</forename><surname>Yamamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="534" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ntu rgb+d: A large scale dataset for 3d human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Finegym: A hierarchical video dataset for fine-grained action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2616" to="2625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Two-stream adaptive graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12026" to="12035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Skeleton-based action recognition with multi-stream adaptive graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF</title>
		<meeting>the IEEE/CVF</meeting>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
