<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Few-Shot Object Detection by Attending to Per-Sample-Prototype</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hojun</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NAVER WEBTOON 2</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myunggi</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NAVER WEBTOON 2</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
							<email>nojunk@snu.ac.krmyunggi@webtoonscorp.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">NAVER WEBTOON 2</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Seoul National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Few-Shot Object Detection by Attending to Per-Sample-Prototype</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Few-shot object detection aims to detect instances of specific categories in a query image with only a handful of support samples. Although this takes less effort than obtaining enough annotated images for supervised object detection, it results in a far inferior performance compared to the conventional object detection methods. In this paper, we propose a meta-learning-based approach that considers the unique characteristics of each support sample. Rather than simply averaging the information of the support samples to generate a single prototype per category, our method can better utilize the information of each support sample by treating each support sample as an individual prototype. Specifically, we introduce two types of attention mechanisms for aggregating the query and support feature maps. The first is to refine the information of few-shot samples by extracting shared information between the support samples through attention. Second, each support sample is used as a class code to leverage the information by comparing similarities between each support feature and query features. Our proposed method is complementary to the previous methods, making it easy to plug and play for further improvement. We have evaluated our method on PASCAL VOC and COCO benchmarks, and the results verify the effectiveness of our method. In particular, the advantages of our method are maximized when there is more diversity among support data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Multi-object detection is a classical computer vision task of recognizing and localizing the instances of specific objects categories from a given scene. In virtue of abundant images with bounding box annotations, object detection has experienced an enormous advancement with numerous deep learning-based approaches <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref>. Notwithstanding its remarkable achievements, the methods still have diffi-culty in learning novel object categories when the number of labeled data samples is small <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b17">18]</ref>. Few-shot learning problems address such issues, which is common in realworld cases. However, learning few-shot samples by empirical risk minimization in a supervised manner easily overfits and may result in poor generalization <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b40">40]</ref>.</p><p>To alleviate this problem, several approaches have been studied, and meta-learning is one of the most successful ones in the few-shot classification scenario. In the few-shot setting, the model is given a small number of labeled support data for training, and at the time of inference, an input query image is classified as one of the support categories. Metric-based classification frameworks <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b34">34]</ref>, one of the popular meta-learning methods, firstly calculate the centroid of each support class called class prototype from the support data, and then classify the query by measuring the similarity of the prototype with the query.</p><p>Since a significant progress has been made in the literature of few-shot classification problems, the problem of few-shot learning for object detection (FSOD) has also been studied. One of the successful approaches for FSOD is to extend typical meta-learning approaches to FSOD. One of the key issues in this line of research is how to aggregate the class prototype with the query image <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b41">41]</ref>. Although there have been performance improvements of FSOD through previous methods based on meta-learning, previous aggregation methods have a couple of main problems.</p><p>First, a handful of support data may be noisy and this can cause unexpected side effects. For example, instances of different categories may be close to one another or instances of the same class may differ in shape and perspective, which causes some samples to be far from intra-class samples in the feature space ( <ref type="figure">Fig. 1 (a)</ref>). Therefore, if the information is not refined before averaging the support data, since it is a few-shot, the averaged prototype may be far from the centroid of the real distribution. Second, to our best knowledge, every method studied so far relies on classwise single representative by averaging the information of  <ref type="figure">Figure 1</ref>. Concept of our method: LEFT: (a) Toy example when support data have large diversity and are misclustered. RIGHT: (b) Feature vectors of collected K-shot support images of a person contain diverse information and just averaging this information for aggregation with the query may deteriorate the detection performance. (c) Instead of using just a single averaged prototype per class, we use one prototype for each support image which has been refined by attending to other support images.</p><p>the support data, which are compared with the query data ( <ref type="figure">Fig. 1 (b)</ref>). Instances in query images have large variations in size, perspective and even a possibility of occlusion. Furthermore, both the query and support images may have multiple instances with different categories close to each other. Therefore, rather than generating a single prototype per category that covers all the diversity and abolishing other information, it may be more advantageous to make better use of the information contained in the support data.</p><p>To resolve these problems, we propose a novel method ( <ref type="figure">Fig. 1 (c)</ref>) consisting of two modules that aggregate the query and support data. First, we propose a method to refine the support information through an attention mechanism among support data before aggregating the query and support data. Second, rather than averaging the information of the support image, we use each support image as a prototype, which we call per-sample prototype. Through this method, we can better aggregate the diverse information of support data with queries.</p><p>We have applied the proposed method to two different architectures <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b8">9]</ref> in different ways. Our method improves the average precision (AP) for new unseen classes on PAS-CAL VOC <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> and COCO <ref type="bibr" target="#b20">[21]</ref> benchmarks in both architectures. We also qualitatively verify that our method enhances the quality of the clusters available from support feature vectors of the same class via t-SNE <ref type="bibr" target="#b35">[35]</ref>.</p><p>Our contributions can be summarized as follows:</p><p>? We investigate that refinements of the support feature maps induce useful information for aggregation through an attention mechanism.</p><p>? We propose a method that aggregates query and support features without using one prototype per class, which allows fully leveraging information of support data.</p><p>? We show that our method can be applied to various types of architectures, and yields meaningful performance improvement on the PASCAL VOC and COCO benchmarks compared to our baselines <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b42">42]</ref>.</p><p>? Through t-SNE and clustering experiments, we demonstrate that intra-class support features are well clustered by our method, learning robust classifiable features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Object Detection It is the task of detecting instances of a specific category in an image. There have been many studies <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b21">22]</ref> on supervised learning with large annotated image datasets. Also, several variant tasks have been studied. For example, weakly supervised object detection <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b5">6]</ref> is the task of learning to detect only with weak annotations (e.g., image-level category) without bounding box annotation. Semi-supervised object detection <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b32">32]</ref> is a task using both labeled and unlabeled data, and few-shot object detection, which we deal with in this paper, aims to detect instances of novel categories with few samples. Object detectors are largely divided into single-stage and two-stage detectors. The single-stage detectors <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b19">20]</ref> predict the object's class and bounding box directly from the features from the feature extractor. The two-stage detectors <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3]</ref> detect objects in two steps: first, they generate classagnostic candidate boxes using RPN (Region Proposal Network). Then, the candidate boxes are classified and the corresponding bounding boxes are regressed. In FSOD, both single-stage <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14]</ref> and two-stage methods <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b42">42]</ref> have been studied. Following the majority trend, we devised our method that can be applied to two-stage detectors such as Faster R-CNN <ref type="bibr" target="#b27">[28]</ref>. Meta-Learning Briefly speaking, it is a research topic to learn how to learn. There are several approaches of metalearning, such as 1) gradient-based methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17]</ref> that learn to well-transfer the knowledge learned by several tasks to a new task and 2) model-based methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref> that aim to design structures that can generalize well, or utilize an external meta-learner or memory. 3) metric-based methods <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b44">44]</ref> that perform non-parametric learning by comparing query sample with support samples and predicting the category of test data by comparing with train-ing support samples. A typical strategy of applying metricbased learning to few-shot learning is to generate a single class prototype for each category by averaging features of support data belonging to the category <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b34">34]</ref>. If networks learn sampled mini-batches called episode through meta-learning, an episode will consist of sub-classes at each iteration. This episodic training strategy has been shown to generalize better on novel few-shot data because it naturally mimics the few-shot task. In this paper, instead of generating one prototype for each class, we propose an alternative method, generating per-sample prototypes, that makes better use of the information of the support data by treating each sample as a prototype.</p><p>Few-Shot Object Detection One of the most successful approaches in FSOD are meta-learning-based methods. Given a support data S composed of K samples of a specific category and a query image Q, the goal of FSOD methods based on meta-learning is to recognize and localize the instances of Q with the help of S. MetaDet <ref type="bibr" target="#b41">[41]</ref>, MetaYOLO <ref type="bibr" target="#b13">[14]</ref> and Meta R-CNN <ref type="bibr" target="#b43">[43]</ref> proposed methods for meta-learner to generate a prototype per category from the support data and aggregate these prototypes with the query features by channel-wise multiplication. FsDetView <ref type="bibr" target="#b42">[42]</ref> showed that when query features are aggregated with support features, it is more effective to concatenate three features together: the channel-wise multiplication feature map of query and support, the subtraction feature map of query and support, and the query feature map itself. FewX <ref type="bibr" target="#b8">[9]</ref> proposed a method of aggregating query features and support features before the region proposal process, unlike the previous methods that aggregate after the region proposal process. All of these methods generate a single prototype per category when performing aggregation, but we propose a novel method of aggregating each sample through the per-sample prototype.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>In the few-shot object detection scenario, we assume we have two sets of data sources, D b and D n . D b is a base dataset with abundant annotated instances of base classes C b , and D n is a novel dataset with few labeled instances of novel classes C n . Here, we assume there are no overlapping classes between C b and C n , i.e., C b ? C n = ?. Few-shot object detection aims to train a detector with limited data source D n to recognize and localize novel instances of categories C n with the help of knowledge learned from the base dataset D b . In this paper, as with the previous researches <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b39">39]</ref>, it is assumed that the novel dataset, D n , is composed of K annotated instances per category. Also, the number of novel categories is N , and we call this problem a K-shot, N -way few-shot object detection problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Overall Architecture</head><p>Our method aims to find the novel instances in the query image by leveraging meta-learning. In the meta-learning scenario, we concentrate on two aspects of the pipeline: how to extract informative representation from support set S and how to combine it with query features. To this end, we propose the Intra-Support Attention Module (ISAM) and the Query-Support Attention Module (QSAM), which can complement various frameworks in a plug-and-play manner and bring significant improvements in performance. <ref type="figure">Figure 2</ref> shows the overall architecture illustrating two baselines <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b42">42]</ref> we used for FSOD. As shown in the figure, there are two candidate locations to apply the proposed aggregation modules. For different baselines <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b42">[42]</ref>, we designed similar aggregation methods composed of ISAM and QSAM and applied them at different locations as shown by [A] and [B] in <ref type="figure">Fig. 2</ref>.</p><p>Specifically, the overall architecture is based on the twostage Faster R-CNN framework <ref type="bibr" target="#b27">[28]</ref>. Backbone network receives a query image and K samples from the support set of the same class and outputs K + 1 feature maps. The Region Proposal Network (RPN) proposes candidate boxes from the query feature map. Here, the input for the RPN depends on the baseline. In FewX <ref type="bibr" target="#b8">[9]</ref>, the aggregated query feature map is fed into RPN ([A] in <ref type="figure">Fig. 2</ref>). On the other hand, in FsDetView <ref type="bibr" target="#b42">[42]</ref>, the query feature map is fed directly into the RPN without aggregation. In both baselines, N roi query RoI features are aggregated with K support RoI features. Then, RoI Head outputs box offsets and class confidences.</p><p>Note that both baselines <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b42">42]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Aggregation Module</head><p>We propose a novel approach to aggregate the query and support features by treating each support feature as an individual prototype rather than generating a single prototype per category. <ref type="figure" target="#fig_2">Figure 3</ref> shows the aggregation methods according to the type of query feature map: (a) is for spatially boosting the prominence of the areas in the query feature map similar to the support RoI feature vectors, and (b) is to strengthen further the query RoI feature similar to support RoI feature vectors. Query Spatial Aggregation (a) is applied to <ref type="figure">Fig. 2</ref> [A], and Query RoI Aggregation (b) is applied to <ref type="figure">Fig. 2</ref>  <ref type="bibr">[B]</ref>. As shown in the figure, both of our aggregation methods are composed of two stages of Intra-Support Attention Module (ISAM) to refine the support features through the self-attention mechanism before aggrega-  tion and Query-Support Attention Module (QSAM) that aggregates the queries with the supports. Intra-Support Attention Module aims to refine each individual support feature vector with the help of other support feature vectors. Because some support feature vectors may be too far from other vectors due to the diverse nature of the support images, it can lead to performance degradation. For example, an upright instance of a person can represent the characteristics of the person class well, while an instance of a person doing a handstand cannot. Therefore, attention mechanism <ref type="bibr" target="#b36">[36]</ref> shown in Eq. (1) is utilized to enhance the shared information of the support features. Especially, following <ref type="bibr" target="#b36">[36]</ref>, we use multi-head attention:</p><formula xml:id="formula_0">Q attn = Attention(Q, K, V) = sof tmax( QK T ? d )V Q attn , Q ? R q?d , K, V ? R v?d<label>(1)</label></formula><p>where d is the length of a feature vector. q and v are the number of queries and keys respectively. Here, all of Q, K and V are assigned to the support feature vectors, i.e., the number of support samples q = v = K. Then, the support feature vectors can pay attention to one another so that the inherent characteristics of the data can be refined.</p><p>For implementation, ISAM is designed as the encoder of shallow Transformer <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b38">38]</ref> composed of multi-head attention network consisting of the process of Eq. (1) and multi-layer perceptron with layer normalization <ref type="bibr" target="#b0">[1]</ref>.</p><p>Query-Support Attention Module aggregates the query feature and support feature maps through attention mechanism shown in Eq.(1). Here, K and V are assigned to the support feature vectors, and Q is assigned to the query feature vectors. The query feature vectors are generated in one of two processes: by flattening the query feature map in (a), i.e., q=HW , or by concatenating query RoI feature vectors in (b), i.e., q=N roi . In other words, aggregation the query features and the support features are performed by dot-producting each query feature vector with all of the support feature vectors.</p><p>For implementation, QSAM is designed as the decoder of shallow Transformer which is also composed of multihead attention network containing multi-layer perceptron blocks with layer normalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training and Inference</head><p>Training Our framework is trained by two phases. First, the network is trained with abundant labeled base data D b with base class C b . At this phase, the trained classes are C b , i.e., C train = C b . Second, the network is finetuned with few-shot novel data D n of novel classes C n . At this phase, the training is done on a balanced dataset composed of K-shot instances per class for both base data and novel data, i.e., C train = C b ? C n . For both phases, the episodic training strategy is applied that each episode consists of Nway, K-shot support data and a query image. FewX <ref type="bibr" target="#b8">[9]</ref> are trained with 2-way, K-shot. Specifically, an episode consists of the following triplet: (q c1 , s c1 , s c2 ) where class c1 and c2 are different classes sampled from C train , and q c1 indicates the query data containing instances of c1-class. s c1 and s c2 indicate the 2-way, K-shot support data, i.e., |s c1 | = |s c2 | = K. And for FsDetView <ref type="bibr" target="#b42">[42]</ref>, an episode consists of a query image and all class of support data, i.e., |C train |-way, K-shot.</p><p>Objective function The loss function of RPN's foreground proposal and RoI Head's detection outputs are Eq.</p><p>(2) where L ?,loc is the bounding box regression loss calculated as the smooth L 1 loss, and L ?,cls is the classification loss calculated as the cross-entropy loss. Note that the output of FewX is binary classification whether the query RoI feature vectors match or not with the support RoI feature vectors, and FsDetView is multi-class classification with the softmax function. L meta is the cross-entropy loss used in FsDetView like Meta R-CNN <ref type="bibr" target="#b43">[43]</ref> for class features to be diverse for different classes. L = L rpn,loc + L rpn,cls + L det,loc + L det,cls + L meta <ref type="bibr" target="#b1">(2)</ref> Inference The few-shot samples used in finetuning are used as the support data at the inference time. Therefore, all the support data are passed into the backbone network and ISAM once, and the output support feature vectors of ISAM are stored for repeated use as multiple prototypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We evaluate our method on PASCAL VOC <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> and MS COCO <ref type="bibr" target="#b20">[21]</ref> benchmarks. We follow the experimental setup of previous works <ref type="bibr" target="#b43">[43,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b42">42]</ref>. For VOC experiments, our network is trained using PASCAL VOC 07+12 trainval dataset and tested on VOC 07 test dataset. The 20 classes are divided into 15 base classes and 5 novel classes and evaluated with three different splits. The number of shots is set to K ? {1, 2, 3, 5, 10}. For 10-shot and 30shot experiments on MS COCO, of the total 80 classes of COCO, the 20 classes overlapping with those of VOC are set as novel classes. As in TFA <ref type="bibr" target="#b39">[39]</ref>, it is assumed that Kshot base data can be used when finetuning with K-shot novel data. Because the performance can vary depending on the few-shot sample configuration, we distinguished between the experiments in fixed support samples and the experiments in which random sampling is performed multiple times.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation detail</head><p>The query images are resized to short size 600 and the maximum long side is set to be 1000. The support images are resized to 320x320 for FewX and 224x224 for Fs-DetView. We use Resnet-101 <ref type="bibr" target="#b10">[11]</ref> for the VOC experiment and Resnet-50 for the COCO experiment as the weightshared backbone network except the first convolution layer of FsDetView. Because input support data for FsDetView have 4 channels that consists of 3 rgb channels and 1 channel for the binary mask of ground truth bounding box, Fs-DetView has a 3-channel convolution layer for the query image and a 4-channel convolution layer for the support data. The backbone networks are pretrained on Imagenet-1k <ref type="bibr" target="#b28">[29]</ref>. The learnable parameters of batchnorm <ref type="bibr" target="#b11">[12]</ref> trained on imagenet 1k are frozen at both base training and finetuning for both baselines. Learning schedulings are the same for both FewX and FsDetView including the optimizer, learning rate, batch size and training/finetuning iterations. Unless otherwise noted, the numbers of support samples K during base training of FewX, FewX+Ours, FsDetView and FsDetView+Ours are set to 10, 10, 1 and 3 1 , respectively.</p><p>ISAM and QSAM are implemented by utilizing the encoder and the decoder of the Transformer <ref type="bibr" target="#b36">[36]</ref>, respectively. Both are set to have 2-heads, 2-layers with layer normalization, and ReLU is used as an activation function. The dropout <ref type="bibr" target="#b33">[33]</ref> rate of Transformer is 0.1. And the hidden dimensions of the Transformer are set to 256.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis for ISAM and QSAM</head><p>We applied our method to FsDetView <ref type="bibr" target="#b42">[42]</ref> and analyzed how the performance changes on PASCAL VOC by two experiments: ablation study of ISAM and QSAM <ref type="table" target="#tab_1">(Table 1)</ref>, and the effect of the number of per-class samples K during base training ( <ref type="table">Table 2</ref>). All experiments in <ref type="table" target="#tab_1">Table 1 and Table 2</ref> were conducted 30 times to calculate the means and standard deviations.</p><p>Ablation study <ref type="table" target="#tab_1">Table 1</ref> shows the average precision at IoU=0.50 (Intersection over Union) of the novel classes with or without ISAM and QSAM on PASCAL VOC07 test dataset. It can be seen that it is effective to refine support features by paying attention to other support features through ISAM. In addition, it can be seen that aggregation with the support features as they are through QSAM is more effective than generating a single class prototype by averaging support features. The table shows that both modules helped improve detection performance, and both processes work better as the number of shots K increased.</p><p>The number of shots K during base training <ref type="table">Table 2</ref> shows the AP50 results of the novel classes on PASCAL VOC07 test dataset according to the change of K during base training. Our performances are generally higher, but the more similar the K for base training and the K for finetuning, the higher the performance. When FsDetView is trained by base data with ours at K = 1, ISAM did not learn how to pay attention to support features, and QSAM did not learn how to aggregate multiple prototypes together. Even if ISAM and QSAM learn their roles when finetuning, the performances were lower than those of K = 3 or 10 when base training. In addition, the higher the number of shots during base training, the lower the standard deviations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Clustering of support feature vectors</head><p>t-SNE Our hypothesis is that collected natural images can be far from class prototypes. Hence, it is better that support features are refined into shared information through ISAM by paying attention to other support features. <ref type="figure" target="#fig_4">Figure 4</ref> is the t-SNE <ref type="bibr" target="#b35">[35]</ref> results of the novel classes on COCO 30-shot to verify the hypothesis. As in (a) and (b) of the figure, some features exist close to others despite the categories are different. In these cases, the data are noisy; for example, there are instances of several categories together in RGB images (a), (b). However, as can be seen in (c), ISAM makes clustering better for support features by paying attention to other support features. Some points are misclustered in all of (a), (b), and (c) when there are ambiguities in RGB images, such as multiple categories, occlusions, or partial appearances.</p><p>Distance from class centroid We evaluated quantitatively whether each support feature vector, which we plotted on t-SNE, is actually closest to the corresponding class mean (single prototype) calculated by 30-shots on the novel support data of COCO 30-shot. The accuracy was evaluated by measuring L1-distance with class means. The accuracies of Baseline, before ISAM and after ISAM are 75.2 %, 77.8% and 97.8%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with state-of-the-art</head><p>We applied our method to two baselines <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b42">42]</ref> and compared it with other methods on PASCAL VOC and COCO benchmarks. Note that the base models are trained by base data with K=10 for our method with FewX and K=3 for ours with FsDetView, as mentioned in implementation details (Sec. 4.2). Both models are finetuned from each base model and are evaluated on novel classes. <ref type="table">Table 3</ref> shows the AP50 results of the novel classes on PASCAL VOC07 test dataset. We evaluated Ours with FewX <ref type="bibr" target="#b8">[9]</ref> with the same few-shot samples as MetaYOLO <ref type="bibr" target="#b13">[14]</ref> and TFA <ref type="bibr" target="#b39">[39]</ref>, and there are significant performance improvements compared to the baseline. However, as shown in <ref type="table" target="#tab_1">Table 1</ref>, the variance of performance is large. Therefore, we evaluated ours with FsDetView <ref type="bibr" target="#b42">[42]</ref> by averaging 30 times of random samplings of few-shot samples. Likewise, significant performance improvements are found in FsDetView <ref type="bibr" target="#b42">[42]</ref>. <ref type="table">Table 4</ref> summarizes the results for novel classes on MS COCO dataset, and we report the standard   COCO metrics, average precision (AP) and average recall (AR). As shown in the table, our methods outperform the baselines in both cases of 10 shots and 30 shots. In addition, the box plot is visualized by repeating it 30 times in <ref type="figure" target="#fig_5">Fig.  5</ref>. As shown in the figure, if the networks learned the same shot, it is confirmed that the min value of ours is higher than the max value of the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PASCAL VOC</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MS COCO</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Future work</head><p>Aggregation with spatial information of support data As the support feature vectors were abstracted, spatial information of the vectors disappeared. Therefore, it is worth noting that the attention mechanism operates to reflect spatial information of support images. In other words, when ISAM refines the support data, it is helpful to pay attention to each other without the support feature maps being pooled by global average pooling. Similarly, when QSAM aggregates the query with the support, it is helpful to aggregate with spatial information of support feature maps rather than that of support feature vectors.</p><p>Toward class scalable detector We evaluate Ours with FewX trained only on the base data of COCO dataset without finetuning on the novel data. The novel 20-class AP is 7.1 (+0.9 point increase compared to baseline) even though the novel data were not finetuned. Because the current framework based on metric-based meta-learning has the form of P (box|query image, support images) . If the networks learn how to match given support images with the query image well, detection can be performed without finetuning the novel classes. If this characteristic is well utilized, it may be more advantageous to detect many unknown classes in the same domain or to perform it incrementally.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>There are studies on the Few-Shot Object Detection framework based on meta-learning that detects instances of support category in a query image. Based on this framework, we propose the Intra-Support Attention Module (ISAM) and Query-Support Attention Module (QSAM) applicable to various methods. ISAM performs an attention mechanism between support features of the same class to refine the information that may be noisy, and QSAM aggregates the query features and the support features by persample prototypes, not a single prototype per class for using unabridged information of support data. Better feature maps for detecting unseen novel classes in K-shot support data are generated through these two modules. We demonstrate the effectiveness of the proposed modules in that the support feature vectors are clustered when collected support samples are somewhat far from the prototype. And the performances are improved when the attention vectors were refined and aggregated as per-sample prototypes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>method backbone</head><p>Novel <ref type="table" target="#tab_1">Set 1  Novel set 2  Novel set 3  1  2  3  5 10  1  2  3  5 10  1  2  3  5</ref>   <ref type="table">Table 3</ref>. AP50 on VOC2007 test dataset. The first four rows are based on YOLOv2 <ref type="bibr" target="#b25">[26]</ref>, and the rest are based on Faster R-CNN <ref type="bibr" target="#b27">[28]</ref> with/without FPN <ref type="bibr" target="#b18">[19]</ref> or DCN <ref type="bibr" target="#b4">[5]</ref>. Methods with * marks are based on finetuning and the others are based on meta-learning. ? indicates the re-implemented version using the official code. marks mean multiple-run results. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>toy example : t-SNE Samples closer to other class centroids</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>generate a single prototype per class by averaging the support feature maps before aggregation at [A] for FewX and at [B] for FsDetView. Unlike the two baselines, our method performs aggregation by treating the support features as an individual prototype, i.e., per-sample prototype. The detailed per-sample aggregation procedures of ISAM and QSAM are introduced in Section 3.3 and Figure 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Two types of aggregation procedures (3-shot example) of the query feature maps and K support feature maps. (a) is for spatial attention to the query feature map and (b) is for aggregation of Nroi query feature maps and K support feature maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>all of (a), (b), (c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>t-SNE visualization for 30-shot support feature vectors of MS COCO novel classes. (b) and (c) are support feature vectors before and after our ISAM, respectively. For visualization, some samples are marked with different markers (+,X,*), and only some classes are plotted. Some patches shown on the rightmost column, which are considered as misclusterd in all of (a), (b), and (c), are circled.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Box plot of Ours with FsDetView on MS COCO</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Our overall Architecture based on Faster R-CNN<ref type="bibr" target="#b27">[28]</ref> to find instances of support category in the query image.[A] and [B] are aggregation procedure (Fig. 3) for the query feature and the support features. Baselines are FewX<ref type="bibr" target="#b8">[9]</ref> and FsDetView<ref type="bibr" target="#b42">[42]</ref>, where FewX has both [A] and [B] operations, and in FsDetView, query features are directly fed into RPN without aggregation of [A] operation.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>: Aggregation Operation</cell></row><row><cell></cell><cell>Backbone</cell><cell>(K, H roi , W roi , C)</cell><cell></cell><cell>( , , , ) : tensor shape</cell></row><row><cell></cell><cell>Network</cell><cell>Roi</cell><cell></cell><cell>: Path only in FewX</cell></row><row><cell></cell><cell></cell><cell>pooling</cell><cell></cell></row><row><cell>K-shot Support Images</cell><cell cols="2">Support Image GT box</cell><cell>RPN</cell><cell>RoI Head</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Roi</cell></row><row><cell></cell><cell></cell><cell>[A] QSA</cell><cell>pooling</cell><cell>[B] QRA</cell></row><row><cell></cell><cell>Backbone</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Network</cell><cell>(1, H q , W q , C)</cell><cell cols="2">(N roi , H roi , W roi , C)</cell></row><row><cell>Query Image</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Figure 2.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>? 6.0 35.9 ? 6.1 42.1 ? 4.3 48.7 ? 3.4 56.9 ? 2.9 FsDetView+ISAM 24.0 ? 6.1 35.6 ? 5.2 44.0 ? 4.6 50.0 ? 4.0 57.9 ? 3.1 FsDetView+QSAM 23.9 ? 6.9 35.9 ? 5.5 43.9 ? 4.9 50.5 ? 3.9 58.1 ? 3.0 FsDetView+ISAM+QSAM 24.3 ? 6.2 36.5 ? 5.3 44.9 ? 4.3 52.0 ? 3.8 59.2 ? 2.6 Ablation study on Novel set 1 of VOC07 test dataset. At base training, 3 support images per class are used.</figDesc><table><row><cell>method</cell><cell cols="2">K=1</cell><cell>Average precision at IoU=0.5 K=2 K=3 K=5</cell><cell>K=10</cell></row><row><cell cols="2">FsDetView 23.8 method base train K</cell><cell>K=1</cell><cell>Average precision at IoU=0.5 K=2 K=3 K=5</cell><cell>K=10</cell></row><row><cell>FsDetView</cell><cell>1</cell><cell cols="3">23.8 ? 6.5 35.3 ? 6.0 41.8 ? 4.8 48.3 ? 3.6 56.5 ? 2.9</cell></row><row><cell>FsDetView</cell><cell>3</cell><cell cols="3">23.8 ? 6.0 35.9 ? 6.1 42.1 ? 4.3 48.7 ? 3.4 56.9 ? 2.9</cell></row><row><cell>FsDetView</cell><cell>10</cell><cell cols="3">23.2 ? 5.7 34.6 ? 5.9 41.5 ? 4.5 48.8 ? 3.4 57.1 ? 2.9</cell></row><row><cell>FsDetView+ISAM+QSAM</cell><cell>1</cell><cell cols="3">24.7 ? 6.5 35.9 ? 5.4 42.2 ? 4.4 50.4 ? 4.0 57.2 ? 3.0</cell></row><row><cell>FsDetView+ISAM+QSAM</cell><cell>3</cell><cell cols="3">24.3 ? 6.2 36.5 ? 5.3 44.9 ? 4.3 52.0 ? 3.8 59.2 ? 2.6</cell></row><row><cell>FsDetView+ISAM+QSAM</cell><cell>10</cell><cell cols="3">24.1 ? 5.9 36.2 ? 5.1 44.1 ? 4.2 51.8 ? 3.7 59.8 ? 2.2</cell></row></table><note>Table 2. Ablation study with changes of K during base training on Novel set 1 of VOC07 test dataset.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>11.0 12.4 29.1 38.5 11.4 3.8 5.0 15.7 31.0 12.6 8.5 15.0 27.3 36.3 YOLOv2-ft [14] 6.6 10.7 12.5 24.8 38.6 12.5 4.2 11.6 16.1 33.9 13.0 15.9 15.0 32.2 38.4 MetaYOLO [14] 14.8 15.5 26.7 33.9 47.2 15.7 15.3 22.7 30.1 40.5 21.3 25.6 28.4 42.8 45.9 MetaDet [41] 17.1 19.1 28.9 35.0 48.8 18.2 20.6 25.9 30.6 41.5 20.1 22.3 27.9 41.9 42.9 MetaDet [41] VGG16 [30] 18.9 20.6 30.2 36.8 49.6 21.8 23.1 27.8 31.7 43.0 20.6 23.9 29.4 43.9 44.1 NP-RepMet [45] R101DCN-FPN 37.8 40.3 41.7 47.3 49.4 41.6 43.0 43.4 47.4 49.1 33.3 38.0 39.8 41.5 44.8 TFA w/ fc * [39] R101FPN 36.8 29.1 43.6 55.7 57.0 18.2 29.0 33.4 35.5 39.0 27.7 33.6 42.5 48.7 50.2 TFA w/ cos * [39] R101FPN 39.8 36.1 44.7 55.7 56.0 23.5 26.9 34.1 35.1 39.1 30.8 34.8 42.8 49.5 49.8 Meta R-CNN [43] R101 19.9 25.5 35.0 45.7 51.5 10.4 19.4 29.6 34.8 45.4 14.3 18.2 27.5 41.2 48.1 FewX ? R101 29.8 35.5 36.3 48.4 53.6 22.2 28.9 25.2 31.2 39.7 24.3 29.9 34.4 47.1 50.4 FewX+Ours R101 31.1 36.1 39.2 50.7 59.4 22.9 29.4 32.1 35.4 42.7 24.3 28.6 35.0 50.0 53.6 TFA w/ cos * [39] R101FPN 25.3 36.4 42.1 47.9 52.8 18.3 27.5 30.9 34.1 39.5 17.9 27.2 34.3 40.8 45.6 FsDetView [42] R101 24.2 35.3 42.2 49.1 57.4 21.6 24.6 31.9 37.0 45.7 21.2 30.0 37.2 43.8 49.6 FsDetView ? R101 23.8 35.3 41.8 48.3 56.5 19.4 26.4 30.3 36.6 44.6 21.7 31.3 34.2 40.6 47.9 FsDetView+Ours R101 24.3 36.5 44.9 52.0 59.2 20.5 27.5 33.1 40.9 47.1 22.4 33.0 37.8 43.9 51.5</figDesc><table><row><cell></cell><cell>10</cell></row><row><cell>LSTD [4]</cell><cell>8.2</cell></row><row><cell>Darknet19</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Red/Blue texts indicate the first/second best on multiple-run results.Table 4. AP and AR of novel claases on MS COCO minival. ? is re-implemented using official code. * marks method based on finetuning.</figDesc><table><row><cell>shots</cell><cell>method</cell><cell>backbone</cell><cell>image size</cell><cell cols="3">AP AP50:95 AP50 AP75</cell><cell>1</cell><cell>AR 10</cell><cell>100</cell></row><row><cell></cell><cell>MetaYOLO [14]</cell><cell cols="2">Darknet19 416x416</cell><cell>5.6</cell><cell>12.3</cell><cell>4.6</cell><cell cols="2">10.1 14.3 14.4</cell></row><row><cell></cell><cell>MetaDet [41]</cell><cell>VGG16</cell><cell>-</cell><cell>7.1</cell><cell>14.6</cell><cell>6.1</cell><cell cols="2">11.9 15.1 15.5</cell></row><row><cell></cell><cell>TFA w/ fc  *  [39]</cell><cell cols="2">R101FPN short800</cell><cell>10.0</cell><cell>19.2</cell><cell>9.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TFA w/ cos  *  [39]</cell><cell cols="2">R101FPN short800</cell><cell>10.0</cell><cell>19.1</cell><cell>9.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Meta R-CNN [43]</cell><cell>R50</cell><cell>short600</cell><cell>8.7</cell><cell>19.1</cell><cell>6.6</cell><cell cols="2">12.6 17.8 17.9</cell></row><row><cell>10</cell><cell>FewX  ? [9]</cell><cell>R50</cell><cell>short600</cell><cell>11.9</cell><cell>23.7</cell><cell>10.6</cell><cell cols="2">19.1 26.2 26.3</cell></row><row><cell></cell><cell>FewX+Ours</cell><cell>R50</cell><cell>short600</cell><cell>13.0</cell><cell>24.7</cell><cell>12.1</cell><cell cols="2">19.3 27.7 27.8</cell></row><row><cell></cell><cell>TFA w/ fc  *  [39]</cell><cell cols="2">R101FPN short800</cell><cell>9.1</cell><cell>17.3</cell><cell>8.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="3">TFA w/ cos  *  [39] R101FPN short800</cell><cell>9.1</cell><cell>17.1</cell><cell>8.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>FsDetView [42]</cell><cell>R50</cell><cell>short600</cell><cell>12.5</cell><cell>27.3</cell><cell>9.8</cell><cell cols="2">20.0 25.5 25.7</cell></row><row><cell></cell><cell>FsDetView  ?</cell><cell>R50</cell><cell>short600</cell><cell>10.6</cell><cell>25.5</cell><cell>6.3</cell><cell cols="2">18.1 23.8 23.9</cell></row><row><cell></cell><cell>FsDetView+Ours</cell><cell>R50</cell><cell>short600</cell><cell>13.4</cell><cell>30.6</cell><cell>9.1</cell><cell cols="2">20.7 26.7 26.8</cell></row><row><cell></cell><cell>MetaYOLO [14]</cell><cell cols="2">Darknet19 416x416</cell><cell>9.1</cell><cell>19.0</cell><cell>7.6</cell><cell cols="2">13.2 17.7 17.8</cell></row><row><cell></cell><cell>MetaDet [41]</cell><cell>VGG16</cell><cell>-</cell><cell>11.3</cell><cell>21.7</cell><cell>8.1</cell><cell cols="2">14.5 18.9 19.2</cell></row><row><cell></cell><cell>TFA w/ fc  *  [39]</cell><cell cols="2">R101FPN short800</cell><cell>13.4</cell><cell>24.7</cell><cell>13.2</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>TFA w/ cos  *  [39]</cell><cell cols="2">R101FPN short800</cell><cell>13.7</cell><cell>24.9</cell><cell>13.4</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>Meta R-CNN [43]</cell><cell>R50</cell><cell>short600</cell><cell>12.4</cell><cell>25.3</cell><cell>10.8</cell><cell cols="2">15.0 21.4 21.7</cell></row><row><cell>30</cell><cell>FewX ? [9]</cell><cell>R50</cell><cell>short600</cell><cell>13.8</cell><cell>25.8</cell><cell>13.5</cell><cell cols="2">20.8 30.8 31.0</cell></row><row><cell></cell><cell>FewX+Ours</cell><cell>R50</cell><cell>short600</cell><cell>15.3</cell><cell>29.3</cell><cell>14.5</cell><cell cols="2">21.2 31.7 32.1</cell></row><row><cell></cell><cell>TFA w/ fc  *  [39]</cell><cell cols="2">R101FPN short800</cell><cell>12.0</cell><cell>22.2</cell><cell>11.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="3">TFA w/ cos  *  [39] R101FPN short800</cell><cell>12.1</cell><cell>22.0</cell><cell>12.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell>FsDetView [42]</cell><cell>R50</cell><cell>short600</cell><cell>14.7</cell><cell>30.6</cell><cell>12.2</cell><cell cols="2">22.0 28.2 28.4</cell></row><row><cell></cell><cell>FsDetView  ?</cell><cell>R50</cell><cell>short600</cell><cell>14.3</cell><cell>31.5</cell><cell>10.6</cell><cell cols="2">21.9 28.7 28.8</cell></row><row><cell></cell><cell>FsDetView+Ours</cell><cell>R50</cell><cell>short600</cell><cell>17.1</cell><cell>35.2</cell><cell>14.7</cell><cell cols="2">24.8 31.2 32.0</cell></row></table><note>marks multiple-run results. Red/Blue texts indicate the first/second best on multiple-run results.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The K during base training of FewX and FsDetView without ours followed the official code.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Introduction to statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Boucheron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?bor</forename><surname>Lugosi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Summer School on Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="169" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6154" to="6162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lstd: A low-shot transfer detector for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuwen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Wildcat: Weakly supervised learning of deep convnets for image classification, pointwise localization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="642" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2007/workshop/index.html" />
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2012/workshop/index.html" />
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fewshot object detection with attention-rpn and multi-relation detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Consistency-based semi-supervised learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungeui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeesoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Few-shot object detection via feature reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8420" to="8429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Tell me what they&apos;re holding: Weakly-supervised object detection with transferable knowledge from human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daesik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyujeong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11246" to="11253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Few-shot learning with global class representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9715" to="9724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning to generalize: Meta-learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Overcoming classifier imbalance for long-tail object detection with balanced group softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10991" to="11000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Meta networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2554" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Meta-learning for semi-supervised fewshot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00676</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<imprint>
			<pubPlace>Michael Bernstein, Alexander C. Berg, and</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05175</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A simple semi-supervised learning framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04757</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Dropout: a simple way to prevent neural networks from overfitting. The journal of machine learning research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1199" to="1208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04080</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Learning deep transformer models for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingbo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><forename type="middle">F</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lidia</forename><forename type="middle">S</forename><surname>Chao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.01787</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Frustratingly simple few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06957</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Generalizing from a few examples: A survey on fewshot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><forename type="middle">M</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Metalearning to detect rare objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9925" to="9934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Few-shot object detection and viewpoint estimation for objects in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="192" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Meta r-cnn: Towards general solver for instance-level low-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anni</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9577" to="9586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Free lunch for fewshot learning: Distribution calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06395</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Restoring negative information in few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaojing</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqi</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11714</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
