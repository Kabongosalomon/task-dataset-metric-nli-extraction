<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Domain Generalization by Mutual-Information Regularization with Pre-trained Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbum</forename><surname>Cha</surname></persName>
							<email>junbum.cha@kakaobrain.com</email>
							<affiliation key="aff0">
								<orgName type="department">Kakao Brain</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungjae</forename><surname>Lee</surname></persName>
							<email>kyungjae.lee@ai.cau.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="institution">Chung-Ang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
							<email>sungrae.park@upstage.ai</email>
							<affiliation key="aff2">
								<orgName type="department">Upstage AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Chun</surname></persName>
							<email>sanghyuk.c@navercorp.com</email>
							<affiliation key="aff3">
								<orgName type="laboratory">NAVER AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Domain Generalization by Mutual-Information Regularization with Pre-trained Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Domain generalization (DG) aims to learn a generalized model to an unseen target domain using only limited source domains. Previous attempts to DG fail to learn domain-invariant representations only from the source domains due to the significant domain shifts between training and test domains. Instead, we re-formulate the DG objective using mutual information with the oracle model, a model generalized to any possible domain. We derive a tractable variational lower bound via approximating the oracle model by a pre-trained model, called Mutual Information Regularization with Oracle (MIRO). Our extensive experiments show that MIRO significantly improves the out-of-distribution performance. Furthermore, our scaling experiments show that the larger the scale of the pre-trained model, the greater the performance improvement of MIRO. Code is available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Emerging studies on the generalizability of deep neural networks have revealed that the existing models, which assume independent and identically distributed (i.i.d.) training and test distribution, are not robust to significant distribution shifts between training and test distribution, e.g., backgrounds <ref type="bibr" target="#b43">[62]</ref>, geographic distribution <ref type="bibr" target="#b40">[59]</ref>, demographic statistics <ref type="bibr" target="#b33">[52,</ref><ref type="bibr" target="#b48">67]</ref>, textures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr">24]</ref>, or day-to-night shifts <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">42]</ref>. Domain generalization (DG) aims to learn robust representations against distribution shifts from multiple source domains during training. The trained model is evaluated on an unseen domain to measure the robustness. The existing DG approaches have tried to learn invariant features across multiple domains <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr">23,</ref><ref type="bibr">33,</ref><ref type="bibr" target="#b36">55,</ref><ref type="bibr" target="#b53">72]</ref>. However, recent studies <ref type="bibr">[25,</ref><ref type="bibr">30]</ref> have shown that simple baselines without learning invariant features are comparable to or even outperform the existing DG methods on the diverse DG benchmarks with a fair evaluation protocol in realistic settings (e.g., using ResNet-50 instead of ). We presume that it is because training and test distributions differ too significantly to learn domain-invariant features by the training distribution only.</p><p>Instead of learning domain-invariant features, we let a model learn similar features to "oracle" representations, i.e., an optimal model generalized to any domain. In particular, we re-formulate the DG problem by maximizing the mutual information (MI) between the oracle model representations and the target model representations while preserving the training loss on source domains. However, the oracle model is not achievable in practice. Hence, we use a large pre-trained model (e.g., ImageNet pre-trained ResNet-50 <ref type="bibr">[26]</ref>) as an approximation. With this approximation, we derive a tractable variational lower bound of the proposed maximization problem, named Mutual Information Regularization with Oracle (MIRO). At a high level, our MIRO objective consists of two objectives: an original target task (i.e., an ERM objective) and a regularization term between the pre-trained model and the current target model. Note that the standard DomainBed benchmark <ref type="bibr">[25]</ref> uses the ImageNet pre-trained ResNet-50 as the initialization of a DG method, thus, we use the pre-trained ResNet as the initialization and the approximation of the oracle model at the same time.</p><p>While a naive fine-tuning approach of a large pre-trained model can harm the robustness against distribution shifts <ref type="bibr">[32,</ref><ref type="bibr" target="#b42">61]</ref>, our proposed algorithm remarkably improves the robustness against unseen domains during fine-tuning in a plug-and-play manner to any scale of the backbone model and datasets. In our experiment, we observe that the naive fine-tuning of a larger pre-trained model can fail to provide better performances, even though the larger pre-trained model is trained with more data and domains. For example, ERM with the ResNet pre-trained on ImageNet (trained with 1.3M images) shows 64.2% of averaged accuracy, while ERM with the ViT pre-trained on CLIP (trained with 400M image-caption pairs) shows 61.1%. On the other hand, we show that our method can significantly improve the average DG performances with backbone models at different scales, e.g., ImageNet pre-trained ResNet (64.2% ? 65.9%), 400M image-text pre-trained ViT (CLIP) <ref type="bibr" target="#b28">[47]</ref> (61.1% ? 73.7%) and Instagram 3.6B pre-trained RegNet (SWAG) <ref type="bibr" target="#b35">[54]</ref> (68.0% ? 74.1%). Especially, we observe that the pre-trained knowledge by larger pre-trained models, such as SWAG and CLIP, is more effective to learn domain generalized features than the ImageNet pre-trained model: MIRO with the ViT pre-trained on CLIP outperforms MIRO with the ResNet pre-trained on ImageNet in contrast to the naive fine-tuning. Furthermore, our feature-level regularization method is easily combined with the existing parameter space ensemble methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b42">61]</ref> (74.1% ? 77.3% average DG accuracy by combining with SWAD <ref type="bibr" target="#b12">[13]</ref> and pre-trained RegNet).</p><p>Our contribution is as follows: <ref type="bibr" target="#b0">(1)</ref> We re-formulate the DG objective by mutual information with the oracle model. Then, we approximate the oracle by a large pre-trained model to derive a tractable approximation of the target objective. We propose Mutual Information Regularization with Oracle (MIRO) to solve our objective. <ref type="bibr" target="#b1">(2)</ref> We analyze the pre-trained models in terms of the MI with the oracle model. Our analysis shows that naive fine-tuning of pre-trained models can harm the MI with the oracle, on the other hand, MIRO shows high MI with the oracle. (3) We compare MIRO with state-of-the-art DG methods on DomainBed. MIRO outperforms all methods in all settings, including varying optimizers and pre-trained models. We also provide extensive analysis to understand MIRO. For example, we observe that MIRO shows stronger DG performances with larger pre-trained models, such as SWAG <ref type="bibr" target="#b35">[54]</ref> or CLIP <ref type="bibr" target="#b28">[47]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>Domain generalization. Learning domain-invariant features from source domains has been a major branch in the DG field. The main idea is discarding biased knowledge to a specific domain while preserving invariant features over source domains, by minimizing feature divergences between the source domains [23, <ref type="bibr" target="#b17">36,</ref><ref type="bibr" target="#b19">38,</ref><ref type="bibr" target="#b22">41,</ref><ref type="bibr" target="#b24">43,</ref><ref type="bibr" target="#b36">55,</ref><ref type="bibr" target="#b51">70]</ref>, simulating domain shifts based on meta-learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr">19,</ref><ref type="bibr">33,</ref><ref type="bibr">35,</ref><ref type="bibr" target="#b50">69]</ref>, robust optimization <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr">31,</ref><ref type="bibr" target="#b32">51,</ref><ref type="bibr" target="#b34">53]</ref>, or augmenting source domain examples <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">44,</ref><ref type="bibr" target="#b26">45,</ref><ref type="bibr" target="#b30">49,</ref><ref type="bibr" target="#b47">66,</ref><ref type="bibr" target="#b52">71,</ref><ref type="bibr" target="#b53">72]</ref>. However, even if the model learns invariant representation to source domains, it can still be biased toward the source domains which causes limited performance on unseen target domains. That is, learning invariant representation across source domains is not enough to achieve the underlying objective of domain generalization <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. To compensate for the issue, this paper employs pre-trained models, which provide general representations across various domains including unseen target domains.</p><p>Exploiting pre-trained models. There have been numerous attempts to exploit pre-trained models in various fields. Transfer learning <ref type="bibr" target="#b18">[37,</ref><ref type="bibr" target="#b45">64]</ref> and knowledge distillation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b37">56]</ref> employ pre-trained models to improve in-domain performance when dataset or architecture shift occurs between pre-training and fine-tuning. Continual learning utilizes the pre-trained model to maintain old task performance when learning new tasks <ref type="bibr" target="#b21">[40]</ref>. Recently, several studies targeting the out-of-distribution generalization are emerging <ref type="bibr">[32,</ref><ref type="bibr" target="#b42">61]</ref>. Kumar et al . <ref type="bibr">[32]</ref> show that naive fine-tuning distorts the pre-trained features and propose a simple baseline, named LP-FT, to alleviate the distortion. WiSE-FT <ref type="bibr" target="#b42">[61]</ref> focuses on zero-shot models. It combines pre-trained and fine-tuned weights to preserve the generalizability of the pre-trained zero-shot models. In this paper, we propose a MI-based regularization method, MIRO, to exploit the generalizability of the pre-trained representation in the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this section, we first re-formulate the objective for the out-of-domain generalization by introducing an oracle model. Then, we derive a tractable variational bound of the objective by approximating the oracle model to the pre-trained model. The final form consists of the empirical risk and the mutual information (MI) regularization by querying the approximated oracle, named Mutual Information Regularization with Oracle (MIRO). We empirically validate our approximation by MI between the oracle model and large pre-trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Mutual information regularization with oracle</head><p>The main idea of the proposed method is to guide the learning process using oracle representations of training datasets. In general, the problem of domain generalization (DG) is to find a model that minimizes an expected loss of any domain by using training datasets from only partial domains, which are called source domains. Many existing methods minimize an empirical loss averaged over source domains. More specifically, suppose that training samples {S d } m d=1 are given in m domains and we consider a hypothesis set H for optimization. Then, many existing DG frameworks can be formulated as follows: To alleviate this issue, we re-formulate the DG problem by employing oracle representations of source domains. Here, we define an oracle model as a model that can be generalized to any possible domain, not only for the source domains. We define a model as a composition of a feature extractor f and a classifier g on the feature space where the whole classifier h can be written as h = f ? g.</p><formula xml:id="formula_0">h = arg min h?H m d=1 E S d (h),<label>(1)</label></formula><p>Then, let f * be a feature extractor of the oracle model. We first start from a strong assumption: we may assume that f * is accessible during the training phase. Then, we can obtain additional information from f * by querying the oracle representations of training samples in the source domains. By using the oracle representations, we can guide the learning process of a target model by maximizing MI between oracle representations and target ones. We formulate the proposed oracle-guided DG framework as follows:</p><formula xml:id="formula_1">max h I(Z f * ; Z f ) s.t. E S (h) ? E S (h) ? ?,<label>(2)</label></formula><p>where Z f * is a random feature extracted by f * and Z f is a random feature extracted by a target model f .</p><formula xml:id="formula_2">I(Z f * ; Z f ) is MI between Z f * and Z f , and E S (?) = m d=1 E S d (?)</formula><p>. The inequality constraint ensures the performance of the target model on the source domains. Maximizing the MI will inhibit the target model from overfitting domain-specific features in the limited source domains. Because we assume that the "oracle" is generalized well to any possible domain, the MI constraints (2) will be beneficial to learning robust representations.</p><p>Unfortunately, the oracle feature extractor f * is not accessible in practice. Instead, we approximate the oracle feature extractor by using a pre-trained model f 0 . Our assumption is that a model pre-trained on large-scale diverse datasets, such as ImageNet <ref type="bibr" target="#b31">[50]</ref>, contains information on diverse domains. In practice, we choose f 0 as the ImageNet pre-trained ResNet-50 <ref type="bibr">[26]</ref>, the standard initialization choice for evaluating DG algorithms <ref type="bibr">[25]</ref>. We also consider models trained by larger diverse datasets, such as CLIP <ref type="bibr" target="#b28">[47]</ref> (trained with 400M web crawled image-text pairs) and SWAG <ref type="bibr" target="#b35">[54]</ref> (trained with 3.6B noisy image-hashtag pairs crawled from Instagram). Although using CLIP and SWAG is not a fair comparison to the existing DG benchmark, here, we emphasize that naive fine-tuning of large pre-trained models leads to inferior generalizability to extreme distribution shifts at test time <ref type="bibr">[32,</ref><ref type="bibr" target="#b42">61]</ref>. In our experiments, we also observe a similar observation: naive fine-tuning of CLIP shows an inferior DG performance (61.1%) than ERM (64.2%).</p><p>Through the approximation of the oracle model, we derive a tractable variational bound of our objective <ref type="bibr" target="#b1">(2)</ref>. We assume a pre-trained model f 0 is located near f * in terms of distance equipped on the hypothesis set of the feature extractors and it can provide approximated representation of f * . Under this assumption, we can obtain a tractable objective by deriving an approximated lower bound of the MI. We first derive the variational lower bound of the MI as follows:</p><formula xml:id="formula_3">I(Z f * ; Z f ) =E Z f * ,Z f log q(Z f * | Z f ) p(Z f * ) + KL(p(Z f * | Z f )?q(Z f * | Z f )) ?E Z f * ,Z f [log q(Z f * | Z f )] + H(Z f * ),<label>(3)</label></formula><p>where q is the variational distribution with a mild regularity condition. More detailed derivation can be found in Barber and Agakov <ref type="bibr" target="#b6">[7]</ref>. Then, we approximate the expectation in Equation <ref type="formula">(</ref>3) by using f 0 .</p><formula xml:id="formula_4">I(Z f * ; Z f ) ? E Z f * ,Z f [log q(Z f * | Z f )] + H(Z f * ) ? E Z f 0 ,Z f log q(Z f 0 | Z f ) ? Cd 2,? (f * , f 0 ) + H(Z f * ),<label>(4)</label></formula><p>where C is a constant and d 2,</p><formula xml:id="formula_5">? (f * , f 0 ) := sup x ?f * (x) ? f 0 (x)? 2 . Note that d 2,</formula><p>? is a proper metric on the hypothesis set of feature extractor. The last inequality of Equation <ref type="formula" target="#formula_4">(4)</ref> is derived by using the first-order Taylor expansion and assuming the regularity condition of q (See Appendix A). We would like to note that the inequality is tight enough due to Taylor's theorem. In other words, equality condition of the last inequality of Equation (4) is d 2,? (f * , f 0 ) = 0. Hence, d 2,? (f * , f 0 ) represents the effect of the pre-trained model f 0 on the approximation of the lower bound. Intuitively speaking, the lower bound shows that the smaller d 2,? (f * , f 0 ) is, the tighter the gap between the true lower bound and approximated one is. In summary, the MI between Z f * and Z f can be maximized by maximizing the term</p><formula xml:id="formula_6">E Z f 0 ,Z f [log q(Z f 0 | Z f )].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Mutual Information Regularization with Oracle (MIRO)</head><p>Input: feature extractor f , classifier g, mean encoder ?, variance encoder ?, regularization coefficient ?, batch size N . Init: initialize f to pre-trained feature extractor f 0 . Output: learned feature extractor f and learned classifier g.</p><formula xml:id="formula_7">for sampled mini-batch (x, y) do z f = f (x) z f 0 = f 0 (x) L = 1 N N i CrossEntropy g(z i f ), y i + ? log ?(z i f ) + ?z i f 0 ? ?(z i f )? 2 ?(z i f ) ?1 update f, g, ?, ? to minimize L end</formula><p>Finally, to consider the constraint term, we introduce the Lagrangian method to Equation <ref type="formula" target="#formula_1">(2)</ref>, then we can derive an objective function from Equation <ref type="formula" target="#formula_4">(4)</ref>:</p><formula xml:id="formula_8">R(h) = E Z f 0 ,Z f [log q(Z f 0 | Z f )] ? ?E S (h),<label>(5)</label></formula><p>where ? indicates the Lagrangian multiplier. Note that the entropy of Z f * and d 2,? (f * , f 0 ) are omitted, since they are independent to our optimization target h = f ? g. In the implementation, we model the variational distribution as a Gaussian distribution with mean vector ?(Z f ) and covariance matrix ?(Z f ) and replace the multiplier ? with the regularization coefficient ?. Then, our final loss function becomes:</p><formula xml:id="formula_9">(MIRO) L(h) = E S (h) + ?E Z f 0 ,Z f log |?(Z f )| + ?Z f 0 ? ?(Z f )? 2 ?(Z f ) ?1 ,<label>(6)</label></formula><p>where ?x? A = ? x ? Ax and constants independent on h are omitted. Then, we optimize the loss function using a stochastic gradient method. The entire learning process is summarized in Algorithm 1. In the following sections, we empirically justify our approximation of f * and explain implementation details for the mean and variance encoders of the Gaussian distribution q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Mutual information analysis with the oracle model</head><p>Here, we empirically show how our approximation by pre-trained models is close to the oracle model and how our algorithm is effective to learn representations having high mutual information (MI) to the underlying oracle model. More specifically, we compare MI between the candidate models and the oracle model on the PACS dataset. Since the true oracle model is not achievable in practice, we train an oracle model by directly optimizing a model on the entire domains. We train two oracle models with ResNet-50 and RegNetY-16GF backbones, where the average validation accuracies across all domains are 97.2% and 98.4%, respectively. We estimate MI between models by mutual information neural estimation (MINE) <ref type="bibr" target="#b8">[9]</ref>. We describe the full details in Appendix B.3. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the empirical MI between the candidate models and the oracle model. In the figures, we first observe that the larger and more powerful pre-trained backbone ("Pre-trained" in <ref type="figure" target="#fig_0">Figure 1b</ref>) shows higher MI than the smaller backbone ("Pre-trained" in <ref type="figure" target="#fig_0">Figure 1a</ref>). Both pre-trained models consistently outperform "Random" in MI regardless of the backbone models. Our observations imply that a larger and stronger model is closer to the oracle model in terms of MI. Similarly, we observe that ERM+ always shows high MI than ERM?. However, interestingly, in <ref type="figure" target="#fig_0">Figure 1b</ref>, we observe that fine-tuning significantly harms MI of the pre-trained model ("Pre-trained" vs. "ERM+") when the pre-trained model becomes larger and more powerful. Our observation is aligned in the same line as the previous studies on fine-tuning of large models <ref type="bibr">[32,</ref><ref type="bibr" target="#b42">61]</ref>. Lastly, in both scenarios of ImageNet pre-trained ResNet <ref type="figure" target="#fig_0">(Figure 1a</ref>) and SWAG pre-trained RegNet <ref type="figure" target="#fig_0">(Figure 1b</ref>), our MIRO shows the highest MI with the oracle model. Note that MI with the oracle model may not be completely aligned with the DG performance, but in practice, we observed that the evaluation ranking of the candidates is the same as the MI ranking; MIRO scores the best, followed by ERM+ and ERM?. Detailed results are provided in Appendix C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Features and encoders design</head><p>Multi-scale features. One can only use the last-level features for our regularization. However, high-level features can include pre-training task-related information, often irrelevant to the target task. Instead, we use the intermedi-ate outputs by each model block, i.e., stem output, blocks 1, 2, 3, and 4 for ResNet <ref type="bibr">[26]</ref> and RegNet <ref type="bibr" target="#b29">[48]</ref>, and stem output, blocks 3, 6, 9, and 12 for ViT-B.</p><p>Design of the mean and variance encoders. The multi-level structure increases the feature size, resulting in a computational cost increase. We alleviate the issue by employing simple yet effective architectures, identity function for the mean encoder and a bias-only model with diagonal covariance for the variance encoder. We also tested more complicated architectures, but only computational cost was increased without performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment setups and implementation details</head><p>Evaluation protocols and datasets. We employ DomainBed evaluation protocols <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">25]</ref>   <ref type="bibr" target="#b12">[13]</ref>. We provide full details in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Main results</head><p>Comparison with domain generalization methods. We provide exhaustive out-of-domain performance comparisons on five DG benchmarks in <ref type="table" target="#tab_3">Table 1</ref>   The second part of <ref type="table" target="#tab_3">Table 1</ref> shows the performance with stochastic weight averaging densely (SWAD) <ref type="bibr" target="#b12">[13]</ref>, a state-of-the-art optimizer for DG by seeking flat minima. Since SWAD is an orthogonal direction to MIRO, we also evaluate the combination of MIRO and SWAD. As shown in the table, the combination of MIRO and SWAD achieves the best performance in all datasets, resulting in +0.8pp average improvement compared to the previous best results.</p><p>In the last part of <ref type="table" target="#tab_3">Table 1</ref>, we push the limits of the out-of-domain performance by employing a large-scale backbone, RegNetY-16GF pre-trained by SWAG <ref type="bibr" target="#b35">[54]</ref>; a weakly-supervised pre-trained model using 3.6 billion noisy Instagram images and hashtags. As shown in our previous study on MI with the oracle model, the pre-trained RegNet has higher MI than ImageNet pre-trained ResNet <ref type="figure" target="#fig_0">(Figure 1</ref>). In the experiments, we first observe that the improvement gap by MIRO becomes remarkably large compared to the ResNet pre-trained model (from +1.7pp to +6.1pp). We presume that this significantly large gap originated from the negative effect of naive fine-tuning as observed by previous works [32, 61] and our study ( <ref type="figure" target="#fig_0">Figure 1b</ref>). As shown in <ref type="figure" target="#fig_0">Figure 1b</ref>, MIRO keeps MI with the oracle model high, resulting in remarkable performance gains on large-scale models. We further explore the effect of the scalability of pre-trained models in the later section. Finally, by combining MIRO with RegNet backbone and SWAD, we achieve the best domain generalization results (77.3%) on our evaluation benchmark.</p><p>MIRO with various pre-trained models. In this subsection, we investigate the robustness of the proposed method to the choice of pre-trained models. In <ref type="table" target="#tab_4">Table 2</ref>, we explore the performance changes of MIRO by varying pre-training datasets, methods, and backbones. From the pre-training method perspective, we examine two image self-supervised pre-training methods (Barlow Twins <ref type="bibr" target="#b49">[68]</ref> and MoCo v3 <ref type="bibr" target="#b15">[16]</ref>), one image-language self-supervised pre-training method (CLIP <ref type="bibr" target="#b28">[47]</ref>), and one weakly-supervised pre-training method (SWAG <ref type="bibr" target="#b35">[54]</ref>), as well as ImageNet supervised pre-training baseline (ImageNet ERM Notably, performance improvements of MIRO are remarkable with large-scale pre-trained models, such as CLIP, CLIP-ViT, and SWAG. This is consistent with our observation in Section 3.2. Our method helps large-scale pre-trained models (in terms of the pre-training dataset size) not to be biased to the training source domains compared to naive fine-tuning. Especially, naive fine-tuning of CLIP-ViT (61.1%) shows worse out-of-domain performance than fine-tuning ImageNet pre-trained model (64.2%). In contrast, MIRO can leverage the pretrained knowledge from CLIP-ViT, resulting in superior performance (73.7%) compared with the ImageNet pre-trained model (65.9%). In our later analysis, we show that the knowledge of large-scale pre-trained models is more beneficial to domain generalization than the knowledge of ImageNet pre-trained models.</p><p>Comparison with methods exploiting pre-trained models. Other DG methods simply employ pre-trained models as weight initialization, while MIRO additionally exploits it in the training process. This is the first approach to exploit pre-trained models in domain generalization, but there are several studies in other fields for different purposes. <ref type="table" target="#tab_6">Table 3</ref> provides a comparison of the methods applicable to our DG settings. We exclude the methods that require additional information other than pre-trained models (e.g., pre-training datasets) or are restricted to a specific model. As shown in the table, MIRO outperforms the comparison methods with large margins. These results demonstrate the effectiveness of our method design for the out-of-domain generalization.  X-axis indicates the feature layer where the features z f are collected. In all datasets, the variances increase as the layer is closer to the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Analysis of MIRO</head><p>Loss function interpretation: ? distribution analysis. We can interpret the variance term of MIRO, ?(z f ) in Equation <ref type="formula" target="#formula_9">(6)</ref>, as control variables of the distance loss between pre-trained features z f 0 and current learning features z f . During the training phase, if the variance values become smaller then the model will preserve MI with the pre-trained model. On the contrary, when the model needs to learn new information, the variance will increase. We illustrate the learned variances in <ref type="figure" target="#fig_1">Figure 2</ref>. The figure shows that pre-trained information is preserved well in lower layers, while task-specific new information is learned in higher layers. This result is consistent with the interpretation that high layer features represent more task-specific semantic information than low layer features [20]; task shifts during fine-tuning make higher layer features learn more semantics than lower layers.</p><p>Case study on Camelyon17: large distribution shift between pre-training and fine-tuning. As shown in Equation <ref type="formula" target="#formula_4">(4)</ref>, the tightness of the lower bound is directly connected to the divergence between the representations of oracle and pre-trained models. Therefore, we investigate the case that there is a large shift between pre-trained and target datasets using the medical dataset <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">30]</ref>, Camelyon17. This dataset consists of whole-slide images of histological lymph node sections from the five hospitals, where each hospital corresponds to each domain. The task is to predict whether the image contains tumor tissue of breast cancer. There is a large gap between the pre-training distribution (ImageNet or Instagram-3.6B) and the fine-tuning distribution (Camelyon17). Detailed visual examples are provided in Appendix D.1. The results in <ref type="table" target="#tab_7">Table 4</ref> demonstrate MIRO leads the model to learn robust representations even in the large distribution shift setup between pre-training and fine-tuning. the performance difference of MIRO to ERM. ? is the intensity of the mutual information regularization. We compare three models: ResNet-50 pre-trained in ImageNet [26], RegNetY-16GF pre-trained by SWAG <ref type="bibr" target="#b35">[54]</ref>, and ViT-B pre-trained by CLIP <ref type="bibr" target="#b28">[47]</ref>.</p><p>Relationship between the pre-training scale and the intensity of the MI regularization. Our method has a control parameter ?, which controls the balance between the cross-entropy loss and the MI regularization loss. If ? becomes larger, it implies that the strength of MI regularization becomes stronger, while it weakens the strength of the ERM objective. Intuitively, if the pre-trained knowledge is informative enough to the target task, larger ? will improve the performances, while if the pre-trained knowledge is uninformative to the target task, then larger ? can harm the performances, because of the penalty on the ERM objective. We compare three pre-trained models (ImageNet pre-trained model, SWAG, and CLIP-ViT) by varying ?. <ref type="figure" target="#fig_2">Figure 3</ref> shows how the out-of-domain performance of MIRO with different pre-trained backbones changes by ?. The additional results on different datasets are given in Appendix D.2. First, we observe that the ImageNet pre-trained backbone has a negative correlation between the performance difference and ? in target domains. When distribution shifts significantly differ, such as cartoon and sketch domains, we can observe an apparent negative correlation. We presume that it is because the ImageNet samples barely contain non-photo images, such as art painting or sketch images. On the other hand, we observe that MIRO with SWAG and CLIP-ViT backbones make significant performance improvements by choosing larger ?. In other words, SWAG and CLIP-ViT pre-trained knowledge are helpful to learn robust features for various target domains compared to the ImageNet pretrained model. Furthermore, it implies that larger pre-trained models trained with massive diverse domain images show less sensitivity to the choice of ?, not only bringing remarkable performance improvements as shown in <ref type="table" target="#tab_4">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Traditional domain generalization (DG) approaches focus to learn a robust representation using multiple source domains. However, in the recent trends of scaling up pre-training, the use of a large-scale pre-trained model becomes more important than the use of DG algorithms for the real-world DG. In line with this trend, we propose Mutual Information Regularization with Oracle (MIRO) to robustly exploit the pre-trained model by approximating an oracle model. To do this, we first re-formulate the domain generalization objective by introducing a concept of an oracle model. Then, we derive a tractable variational bound of the objective by approximating the oracle model with the pre-trained model. Our experimental results demonstrate both the effectiveness and the potential of the proposed method. MIRO achieves state-of-the-art performance in the DomainBed benchmarks. Furthermore, when combining MIRO with largescale pre-trained backbones, such as CLIP <ref type="bibr" target="#b28">[47]</ref> or SWAG <ref type="bibr" target="#b35">[54]</ref>, the performance improvements remarkably increases. We hope that this study promotes a new research direction of exploiting pre-trained backbones to learn robust representations for domain generalization. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Derivation of Lower Bound</head><p>Assumption 1 The variational distribution q(?|z) satisfies the regularity condition such that, for any</p><formula xml:id="formula_10">P X|z ? {P ? X|z | E X|z [|X| 2 ] &lt; ?}, E X|z [(? x log q(x|z)| x=X ) ? ? x log q(x|z)| x=X ] &lt; ?,<label>(7)</label></formula><p>where E X|z is a conditional expectation of X given z.</p><p>Remark 1. Note that the Gaussian distribution used in our implementation satisfies the regularity condition. To check the regularity condition of Gaussian distribution, we first compute the gradient as follows,</p><formula xml:id="formula_11">? x log q(x|z)| x=X (8) = ? x C + 1 2 log |?(z)| + 1 2 (x ? ?(z)) ? ?(z) ?1 (x ? ?(z)) | x=X (9) = ?(z) ?1 (X ? ?(z)).<label>(10)</label></formula><p>Hence, we get,</p><formula xml:id="formula_12">E X|z [(? x log q(x|z)| x=X ) ? ? x log q(x|z)| x=X ]<label>(11)</label></formula><formula xml:id="formula_13">= E X|z (X ? ?(z)) ? ?(z) ?2 (X ? ?(z)) &lt; ?.<label>(12)</label></formula><p>since ?(z) and ?(z) are finite and E X|z [|X| 2 ] is bounded. Hence, the Gaussian distribution satisfies the regularity condition.</p><p>Under the assumption of q, we derive the lower bound.</p><p>Proof (Derivation of the Lower Bound). Based on the regularity condition, we derive the lower bound of the term,</p><formula xml:id="formula_14">E Z f * ,Z f [log q(Z f * | Z f )].</formula><p>Before starting the derivation, let us define d 2,? (f, g) := sup x ?f (x) ? g(x)? 2 . Then, the derivation starts from Taylor's theorem for a differentiable multivariate function. From Taylor's theorem, there exists a point c such that c = tx + (1 ? t)x 0 for some t ? [0, 1] and the following equality holds,</p><formula xml:id="formula_15">log q(x | y) = log q(x 0 | y) + ? x log q(x | y)| ? x=c (x ? x 0 ).<label>(13)</label></formula><p>Then, we can derive the following upper bound as follows,</p><formula xml:id="formula_16">log q(x | y) = log q(x 0 | y) + ? x log q(x | y)| ? x=c (x ? x 0 ) (14) ? log q(x 0 | y) + |? x log q(x | y)| ? x=c (x ? x 0 )| (15) ? log q(x 0 | y) + ?? x log q(x | y)| x=c ? 2 ?x ? x 0 ? 2<label>(16)</label></formula><p>By using this bound, we can derive the following lower bound,</p><formula xml:id="formula_17">E Z f * ,Z f [log q(Z f * | Z f )] = E X,X ? [log q(f * (X) | f (X ? ))]<label>(17)</label></formula><formula xml:id="formula_18">?E X,X ? log q(f 0 (X) | f (X ? )) ? E X,X ? ?? log q(c(X) | f (X ? ))? 2 f 0 (X) ? f * (X) 2 (18) ?E X,X ? log q(f 0 (X) | f (X ? )) ? E X,X ? [?? log q(c(X) | f (X ? ))? 2 ] d 2,? (f * , f 0 ) (19) ?E Z f 0 ,Z f log q(Z f 0 | Z f ) ? Cd 2,? (f * , f 0 ),<label>(20)</label></formula><p>where c(x) is the function between f 0 and f * , which selects the point satisfying Taylor's theorem, and C is a constant derived from the regularity condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Hyperparameter tuning</head><p>We split the hyperparameters (HPs) into two groups: algorithm-specific HPs and algorithm-agnostic HPs. The algorithm-agnostic HPs consist of batch size, learning rate, dropout, and weight decay, and MIRO has only one algorithm-specific HP, ?. To reduce the computational cost, we tune the algorithm-specific HPs and algorithm-agnostic HPs independently. We first search algorithm-specific HPs with default algorithm-agnostic HPs, then search algorithm-agnostic HPs with the tuned algorithm-specific HPs. That is, the ? is searched in [1.0, 0. Even though we use the efficient HP search protocol, it still requires heavy computational resources. Therefore, we tune ? only for the non-main experiments, including combination with SWAD, combination with various pre-trained backbones, and the case study on Camelyon17. Also, we use the batch size of 16 for SWAG <ref type="bibr" target="#b35">[54]</ref> due to the GPU memory limitation. Note that there is room for further performance improvement by intensive HP tuning and additional usage of GPU memory, considering the simplified HP search protocol and limited computational resources.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Implementation details</head><p>The variance encoder is initialized to estimate the variance of 0.1. It is chosen by observing the convergence point of the variance. Softplus function is employed to ensure non-negativity of the variance. Also, we empirically apply the 10 times larger learning rate for the mean and variance encoders than the feature extractor and the classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Mutual information estimation</head><p>In Section 3.2, we estimate the mutual information using Mutual Information Neural Estimator (MINE) <ref type="bibr" target="#b8">[9]</ref>. The mutual information is estimated by MINE as follows:</p><formula xml:id="formula_19">I(Z f * ; Z f ) = sup ??? E P Z f * Z f [T ? ] ? log E P Z f * ?P Z f e T ? .<label>(21)</label></formula><p>For the features Z f * and Z f , the features after global average pooling are uniformly collected by domains. The statistics network, T ? , consists of two hidden linear layers with 512 dimensions and ELU activation functions, following <ref type="bibr" target="#b8">[9]</ref>. In the case of fine-tuning, such as ERM?, ERM+, and MIRO, the models are trained as many as the number of target domains. Therefore, we estimate the mutual information for each model and report their average value. In general, domain generalization (DG) assumes that there are multiple source domains, source domain labels are available, and the same input has the same label between source and target domains. Here, we can make the variations on the problem settings by changing the assumption. Single-source DG does not assume the multiple source domains <ref type="bibr" target="#b12">[13,</ref><ref type="bibr">21]</ref>. Several studies try to solve DG problem without domain labels <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b22">41]</ref>. Heterogeneous DG deals with the label set shift, i.e., the same input can have different labels between source and target domains <ref type="bibr" target="#b20">[39,</ref><ref type="bibr" target="#b41">60]</ref>. In this task, it is assumed that a classifier is learnable in the target domain and the methods focus on the feature extractor. The proposed method exploits pre-trained models instead of assuming available multiple source domains or source domain labels, and focuses on the feature extractor instead of the classifier. Therefore, MIRO is directly applicable to single-source DG, DG without domain labels, and heterogeneous DG problems. On the other hand, we can consider a more specific type of distribution shift. In this case, we may need a different mutual information (MI) strategy. For example, we can employ class-conditional MI for class-conditional distribution shift (C-MIRO) by using I(Z f * ; Z f |Y ) instead of I(Z f * ; Z f ). In <ref type="table" target="#tab_9">Table 5</ref>, C-MIRO achieves comparable scores with MIRO and outperforms ERM even though the problem setting is not class-conditional. From the results, we believe that MIRO can be adapted to other distribution shifts by choosing the proper MI strategy. Our method assumes that knowledge of the oracle model helps domain generalization and it is transferable to the target model by maximizing mutual information (MI). These assumptions are quite intuitive, but there is no theoretical guarantee that MI with the oracle model is perfectly aligned with DG performance. Instead, we empirically observe that a high MI model shows better DG performance if the empirical loss constraint of Equation (2) holds; the pre-trained model itself has high MI but does not satisfy this constraint. <ref type="figure" target="#fig_0">Figure  1</ref> in the main text shows the rankings of MI for ERM?, ERM+, and MIRO are in order. <ref type="table" target="#tab_10">Table 6</ref> shows that the rankings are the same for accuracies: ERM? (51.6%), ERM+ (84.2%), and MIRO (85.4%) in ImageNet pre-trained ResNet and ERM? (51.5%), ERM+ (89.6%), and MIRO (97.4%) in Instagram-3.6B pre-trained RegNet, respectively. <ref type="figure" target="#fig_4">Figure 4</ref> shows a huge visual gap between pre-training (ImageNet) and finetuning (Camelyon17) datasets. The tasks are also different; ImageNet is an object recognition task and Camelyon17 is a binary classification of breast cancer. Despite the large gap between pre-training and fine-tuning distribution, the proposed method shows consistent performance improvement (See <ref type="table" target="#tab_7">Table 4</ref> in the main text). occurs between pre-training (ImageNet) and fine-tuning (Camelyon17). ImageNet is a multiclass objective recognition task and Camelyon17 is a binary classification task for reading whether the image contains tumor tissue. Instagram-3.6B examples are omitted since it is not publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Analysis and Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Variations on the assumptions of domain generalization</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 The relationship between mutual information and domain generalization performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Visual comparison between ImageNet and Camelyon17</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Relationship between the pre-training scale and the intensity of the mutual information regularization</head><p>In this section, we provide the extended results of <ref type="figure" target="#fig_2">Figure 3</ref> in the main text. <ref type="figure" target="#fig_5">Figure 5</ref> shows the additional comparison of three pre-trained backbones according to ? about OfficeHome, TerraIncognita, and DomainNet. The comparisons show similar trends with the results in PACS. ImageNet pre-trained backbone, such as ResNet-50 pre-trained in ImageNet [26], has a negative correlation between the performance difference and ? in some target domains. Large-scale pre-trained backbones, such as SWAG <ref type="bibr" target="#b35">[54]</ref> and CLIP <ref type="bibr" target="#b28">[47]</ref>, tend to consistently make significant performance improvements at high ? and become less sensitive to the choice of ?. the performance difference of MIRO to ERM. ? is the intensity of the mutual information regularization. We compare three models: ResNet-50 pre-trained in ImageNet [26], RegNetY-16GF pre-trained by SWAG <ref type="bibr" target="#b35">[54]</ref>, and ViT-B pre-trained by CLIP <ref type="bibr" target="#b28">[47]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Mutual information I (Z f * ; Z f ) with oracle model. The mutual information is estimated by MINE [9] in PACS. Oracle model is trained using all of the four domains. Random and Pre-trained indicate random and pre-trained model initialization, respectively. ERM-and ERM+ are trained from random and pre-trained model initialization, respectively. ? indicates models without fine-tuning. The experiments are repeated with two pre-trained models: ImageNet 1.3M pre-trained ResNet-50 and Instagram 3.6B pre-trained RegNetY-16GF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Distribution of ?(z f ). We plot the estimated variances, ?(z f ), for each layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Comparison of three pre-trained models according to ?. Y-axis indicates</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Acknowledgement</head><label></label><figDesc>This work was supported by IITP grant funded by the Korea government (MSIT) (No. 2021-0-01341, AI Graduate School Program, CAU). 18. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth 16x16 words: Transformers for image recognition at scale. In: International Conference on Learning Representations (2021) 11 19. Dou, Q., Castro, D.C., Kamnitsas, K., Glocker, B.: Domain generalization via model-agnostic learning of semantic features. Neural Information Processing System (2019) 3 20. Erhan, D., Bengio, Y., Courville, A., Vincent, P.: Visualizing higher-layer features of a deep network. University of Montreal 1341(3), 1 (2009) 12 21. Fan, X., Wang, Q., Ke, J., Yang, F., Gong, B., Zhou, M.: Adversarially adaptive normalization for single domain generalization. In: Computer Vision and Pattern Recognition (2021) 21 22. Fang, C., Xu, Y., Rockmore, D.N.: Unbiased metric learning: On the utilization of multiple datasets and web images for softening bias. In: International Conference on Computer Vision (2013) 8 23. Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F., Marchand, M., Lempitsky, V.: Domain-adversarial training of neural networks. Journal of Machine Learning Research 17(1), 2096-2030 (2016) 1, 3, 9 24. Geirhos, R., Rubisch, P., Michaelis, C., Bethge, M., Wichmann, F.A., Brendel, W.: Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. In: International Conference on Learning Representations (2019) 1 25. Gulrajani, I., Lopez-Paz, D.: In search of lost domain generalization. In: International Conference on Learning Representations (2021) 1, 2, 4, 5, 8, 9, 20 26. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: Computer Vision and Pattern Recognition (2016) 1, 2, 5, 8, 11, 13, 23, 24 27. Huang, Z., Wang, H., Xing, E.P., Huang, D.: Self-challenging improves crossdomain generalization. European Conference on Computer Vision (2020) 9 28. Kim, D., Yoo, Y., Park, S., Kim, J., Lee, J.: Selfreg: Self-supervised contrastive regularization for domain generalization. In: International Conference on Computer Vision (2021) 9 29. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: International Conference on Learning Representations (2015) 8 30. Koh, P.W., Sagawa, S., Marklund, H., Xie, S.M., Zhang, M., Balsubramani, A., Hu, W., Yasunaga, M., Phillips, R.L., Gao, I., et al.: Wilds: A benchmark of in-thewild distribution shifts. In: International Conference on Machine Learning (2021) 1, 13 31. Krueger, D., Caballero, E., Jacobsen, J.H., Zhang, A., Binas, J., Priol, R.L., Courville, A.: Out-of-distribution generalization via risk extrapolation (rex). arXiv preprint arXiv:2003.00688 (2020) 3, 9 32. Kumar, A., Raghunathan, A., Jones, R., Ma, T., Liang, P.: Fine-tuning can distort pretrained features and underperform out-of-distribution. In: International Conference on Learning Representations (2022) 2, 3, 5, 7, 10, 12 33. Li, D., Yang, Y., Song, Y.Z., Hospedales, T.: Learning to generalize: Meta-learning for domain generalization. In: AAAI Conference on Artificial Intelligence (2018) 1, 3, 9 34. Li, D., Yang, Y., Song, Y.Z., Hospedales, T.M.: Deeper, broader and artier domain generalization. In: International Conference on Computer Vision (2017) 8 35. Li, D., Zhang, J., Yang, Y., Liu, C., Song, Y.Z., Hospedales, T.M.: Episodic training for domain generalization. In: International Conference on Computer Vision (2019) 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>(a) ImageNet (pre-train) (b) Camelyon17 (fine-tuning) Example images of ImageNet and Camelyon17. Large distribution shift</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Comparison of three pre-trained models according to ?. Y-axis indicates</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>While CORAL achieves ?50% top-1 accuracy on four easy domains (59.2% for Clipart, 46.6% for Painting, 59.8% for Real, 50.1% for Sketches), it only shows 13.4% for QuickDraw and 19.7% for Infographics where the domains show the significant distribution shift comparing to others.</figDesc><table><row><cell>domain-</cell></row><row><cell>invariant representations using only partial domains when the target distribu-</cell></row><row><cell>tion differs significantly from the training distribution. For example, CORAL,</cell></row><row><cell>the state-of-the-art method, shows inconsistent out-of-domain accuracies across</cell></row><row><cell>domains in DomainNet [46].</cell></row></table><note>where d indicates an individual source domain and E S d is an empirical loss over the source domain d. Note that majority of existing DG methods can be inter- preted as the variant of Equation (1). For example, if we choose a simple cross- entropy loss for E S d , then Equation (1) becomes "ERM" baseline used in [25] 1 . Otherwise, E S d can be formulated as a regularized ERM, such as IRM [2] or CORAL [55]. However, the formulation (1) still suffers from learning</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>for a fair comparison. The five benchmark datasets are used: PACS [34] (4 domains, 7 classes, and 9, 991 images), VLCS [22] (4 domains, 5 classes, and 10, 729 images), OfficeHome [58] (4 domains, 65 classes, and 15, 588 images), TerraIncognita [8] (4 domains, 10 classes, and 24, 788 images), and DomainNet [46] (6 domains, 345 classes, and 586, 575 images). All performance scores are evaluated by leave-one-out cross-validation, where averaging all cases that use a single domain as the target (test) domain and the others as the source (training) domains. Every experiment is repeated three times. We leave 20% of source domain data for validation. We use training-domain validation for the model selection and the hyperparameter search following DomainBed [25]. Implementation details. We use ResNet-50 [26] pre-trained in the ImageNet [50] as default. The model is optimized using Adam [29] optimizer. A mini-batch contains all domains and 32 examples per domain. The regularization coefficient ? is tuned in [1.0, 0.1, 0.01, 0.001]. The other hyperparameters, such as batch size, learning rate, dropout rate, and weight decay, are tuned in the similar search space proposed in Cha et al .</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>. Compared to ERM, the proposed MI regularization significantly improves performance on every benchmark dataset, resulting in +1.7pp average improvement. Compared with the state-of-the-art methods, MIRO achieves the best performances in all benchmarks, except PACS. Especially, MIRO remarkably outperforms previous methods: +1.3pp in OfficeHome (mDSDI<ref type="bibr" target="#b10">[11]</ref>; 69.2% ? 70.5%) and +1.8pp in TerraIncognita (SagNet<ref type="bibr" target="#b25">[44]</ref>; 48.6% ? 50.4%). Considering the extensive experiment setup with 5 datasets and 22 target domains, the results demonstrate the effectiveness of MIRO to the diverse visual data types.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Comparison with domain generalization methods. Out-of-domain accuracies on five domain generalization benchmarks are shown. We highlight the best results in bold. The results marked by ?, ? are the reported numbers from Gulrajani and Lopez-Paz [25] and Cha et al . [13], respectively. The results of Fish, SelfReg, and mDSDI are the reported ones from each paper. Average accuracies and standard errors are reported from three trials. 2?0.2 79.0?0.3 69.2?0.4 48.1?1.4 42.8?0.1 65.1 MIRO 85.4?0.4 79.0?0.0 70.5?0.4 50.4?1.1 44.3?0.2 65.9 SWAD ? 88.3?0.1 78.9?0.1 71.3?0.1 51.0?0.1 46.8?0.0 67.3 MIRO + SWAD 88.4?0.1 79.6?0.2 72.4?0.1 52.9?0.2 47.0?0.0 68.1 Using RegNetY-16GF backbone with SWAG pre-training [54] ERM 89.6?0.4 78.6?0.3 71.9?0.6 51.4?1.8 48.5?0.6 68.0 MIRO 97.4?0.2 79.9?0.6 80.4?0.2 58.9?1.3 53.8?0.1 74.1 MIRO + SWAD 96.8?0.2 81.7?0.1 83.3?0.1 64.3?0.3 60.7?0.0 77.3</figDesc><table><row><cell>Algorithm</cell><cell>PACS</cell><cell>VLCS</cell><cell cols="2">OfficeHome TerraInc DomainNet Avg.</cell></row><row><cell>MMD  ? [36]</cell><cell cols="2">84.7?0.5 77.5?0.9</cell><cell>66.3?0.1</cell><cell>42.2?1.6 23.4?9.5 58.8</cell></row><row><cell>Mixstyle  ? [72]</cell><cell cols="2">85.2?0.3 77.9?0.5</cell><cell>60.4?0.3</cell><cell>44.0?0.7 34.0?0.1 60.3</cell></row><row><cell>GroupDRO  ? [51]</cell><cell cols="2">84.4?0.8 76.7?0.6</cell><cell>66.0?0.7</cell><cell>43.2?1.1 33.3?0.2 60.7</cell></row><row><cell>IRM  ? [2]</cell><cell cols="2">83.5?0.8 78.5?0.5</cell><cell>64.3?2.2</cell><cell>47.6?0.8 33.9?2.8 61.6</cell></row><row><cell>ARM  ? [69]</cell><cell cols="2">85.1?0.4 77.6?0.3</cell><cell>64.8?0.3</cell><cell>45.5?0.3 35.5?0.2 61.7</cell></row><row><cell>VREx  ? [31]</cell><cell cols="2">84.9?0.6 78.3?0.2</cell><cell>66.4?0.6</cell><cell>46.4?0.6 33.6?2.9 61.9</cell></row><row><cell>CDANN  ? [38]</cell><cell cols="2">82.6?0.9 77.5?0.1</cell><cell>65.8?1.3</cell><cell>45.8?1.6 38.3?0.3 62.0</cell></row><row><cell>DANN  ? [23]</cell><cell cols="2">83.6?0.4 78.6?0.4</cell><cell>65.9?0.6</cell><cell>46.7?0.5 38.3?0.1 62.6</cell></row><row><cell>RSC  ? [27]</cell><cell cols="2">85.2?0.9 77.1?0.5</cell><cell>65.5?0.9</cell><cell>46.6?1.0 38.9?0.5 62.7</cell></row><row><cell>MTL  ? [10]</cell><cell cols="2">84.6?0.5 77.2?0.4</cell><cell>66.4?0.5</cell><cell>45.6?1.2 40.6?0.1 62.9</cell></row><row><cell cols="3">Mixup  ? [60, 63, 65] 84.6?0.6 77.4?0.6</cell><cell>68.1?0.3</cell><cell>47.9?0.8 39.2?0.1 63.4</cell></row><row><cell>MLDG  ? [33]</cell><cell cols="2">84.9?1.0 77.2?0.4</cell><cell>66.8?0.6</cell><cell>47.7?0.9 41.2?0.1 63.6</cell></row><row><cell>Fish [53]</cell><cell cols="2">85.5?0.3 77.8?0.3</cell><cell>68.6?0.4</cell><cell>45.1?1.3 42.7?0.2 63.9</cell></row><row><cell>ERM  ? [57]</cell><cell cols="2">84.2?0.1 77.3?0.1</cell><cell>67.6?0.2</cell><cell>47.8?0.6 44.0?0.1 64.2</cell></row><row><cell>SagNet  ? [44]</cell><cell cols="2">86.3?0.2 77.8?0.5</cell><cell>68.1?0.1</cell><cell>48.6?1.0 40.3?0.1 64.2</cell></row><row><cell>SelfReg [28]</cell><cell cols="2">85.6?0.4 77.8?0.9</cell><cell>67.9?0.7</cell><cell>47.0?0.3 42.8?0.0 64.2</cell></row><row><cell>CORAL  ? [55]</cell><cell cols="2">86.2?0.3 78.8?0.6</cell><cell>68.7?0.3</cell><cell>47.6?1.0 41.5?0.1 64.5</cell></row><row><cell cols="2">mDSDI [11] 86.Combined with SWAD [13]</cell><cell></cell><cell></cell></row><row><cell>ERM + SWAD  ?</cell><cell cols="2">88.1?0.1 79.1?0.1</cell><cell>70.6?0.2</cell><cell>50.0?0.3 46.5?0.1 66.9</cell></row><row><cell>CORAL + ERM + SWAD</cell><cell cols="2">94.7?0.2 79.7?0.2</cell><cell>80.0?0.1</cell><cell>57.9?0.7 53.6?0.6 73.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Comparison with various pre-training datasets, methods, and backbones. We compare the performance changes according to the scale of the dataset, the method, and the backbone architecture of pre-training. ResNet-50 architecture is used as default. OH, TI, and DN indicate OfficeHome, TerraIncognita, and DomainNet, respectively. Every accuracy is averaged over three trials. Dataset (size) Pre-training Alg. PACS VLCS OH TI DN Avg. MIRO 95.6 82.2 82.5 54.3 54.0 73.7 (+12.6) Instagram (3.6B) SWAG (RegNet) ERM 89.6 78.6 71.9 51.4 48.5 68.0 MIRO 97.4 79.9 80.4 58.9 53.8 74.1 (+6.1)</figDesc><table><row><cell></cell><cell>ERM</cell><cell cols="2">ERM 84.2 77.3 67.6 47.8 44.0 MIRO 85.4 79.0 70.5 50.4 44.3 65.9 (+1.7) 64.2</cell></row><row><cell>ImageNet (1.3M)</cell><cell>Barlow Twins</cell><cell cols="2">ERM 78.7 77.3 57.6 36.9 41.7 MIRO 80.7 79.4 63.7 43.2 42.6 61.9 (+3.5) 58.4</cell></row><row><cell></cell><cell>MoCo v3</cell><cell cols="2">ERM 86.7 77.3 61.8 49.1 43.8 MIRO 86.3 78.5 66.8 48.4 44.7 65.0 (+1.3) 63.7</cell></row><row><cell>CLIP (400M)</cell><cell>CLIP (ResNet)</cell><cell cols="2">ERM 64.3 69.8 28.2 32.9 29.5 MIRO 76.6 78.9 59.5 49.0 42.0 61.2 (+16.3) 44.9</cell></row><row><cell></cell><cell>CLIP (ViT)</cell><cell>ERM 83.4 75.9 66.4 35.3 44.4</cell><cell>61.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>). From the pre-training scale perspective, we employ the ImageNet [50] dataset of 1.3 million examples, the CLIP dataset of 400 million examples, and the Instagram dataset of 3.6 billion examples. We use ResNet-50 [26] backbone architecture as default, but a bigger model is also used for the large-scale pre-training, such as ViT-B [18] for CLIP or RegNetY-16GF [48] for SWAG. As shown in the table, MIRO improves performances compared with the baseline ERM in all experiments. For the ImageNet pre-training, applying MIRO results in performance improvements of +1.7pp, +3.5pp, and +1.3pp for ERM (supervised learning), Barlow Twins, and MoCo v3, respectively. For the large-scale pre-training, such as CLIP and SWAG, MIRO brings larger performance improvements of +16.3pp, +12.6pp, and +6.1pp for CLIP, CLIP-ViT, and SWAG, respectively. These experiments demonstrate the robustness of the proposed method to the pre-training methods, datasets, and backbone architectures.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc>Comparison with methods exploiting pre-trained models. Out-ofdomain accuracies on five domain generalization benchmarks are shown. Average accuracies and standard errors are reported from three trials.</figDesc><table><row><cell>Algorithm</cell><cell>PACS</cell><cell>VLCS</cell><cell cols="4">OfficeHome TerraInc DomainNet Avg.</cell></row><row><cell>CRD [56]</cell><cell cols="2">82.3?1.0 76.6?0.9</cell><cell>67.6?0.4</cell><cell>44.0?1.9</cell><cell>42.1?0.1</cell><cell>62.5</cell></row><row><cell>VID [1]</cell><cell cols="2">84.9?0.3 76.2?0.2</cell><cell>64.6?0.5</cell><cell>48.3?1.3</cell><cell>42.5?0.1</cell><cell>63.3</cell></row><row><cell>LP-FT [32]</cell><cell cols="2">84.6?0.8 76.7?1.5</cell><cell>65.0?0.2</cell><cell>47.1?0.7</cell><cell>43.0?0.1</cell><cell>63.3</cell></row><row><cell>L 2 -SP [64]</cell><cell cols="2">83.6?0.3 78.8?0.4</cell><cell>65.0?0.3</cell><cell>47.9?2.1</cell><cell>42.5?0.2</cell><cell>63.6</cell></row><row><cell cols="3">DELTA [37] 83.1?1.1 77.7?0.4</cell><cell>68.5?0.3</cell><cell>45.7?0.9</cell><cell>42.8?0.1</cell><cell>63.6</cell></row><row><cell>LwF [40]</cell><cell cols="2">83.1?0.8 77.2?0.7</cell><cell>70.0?0.2</cell><cell>49.2?1.2</cell><cell>42.7?0.1</cell><cell>64.5</cell></row><row><cell>MIRO</cell><cell cols="2">85.4?0.4 79.0?0.0</cell><cell>70.5?0.4</cell><cell>50.4?1.1</cell><cell>44.3?0.2</cell><cell>65.9</cell></row><row><cell cols="2">stem blk1 blk2 blk3 blk4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Performance improvements in Camelyon17 medical dataset. Even in the large distribution shift setup between pre-training and target datasets, MIRO consistently outperforms ERM. Every accuracy is averaged over three trials.</figDesc><table><row><cell>Pretrain</cell><cell>Algorithm</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>Avg.</cell></row><row><cell>ImageNet ERM</cell><cell>ERM MIRO</cell><cell cols="6">97.1 94.7 95.7 96.4 90.7 97.5 94.5 95.6 96.7 93.7 95.6 (+0.7) 94.9</cell></row><row><cell>SWAG</cell><cell>ERM MIRO</cell><cell cols="6">97.0 94.1 95.3 96.0 89.5 97.4 95.5 96.5 96.1 90.9 95.3 (+0.9) 94.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>1, 0.01, 0.001] with the batch size of 32, the learning rate of 5e-5, no dropout, and no weight decay. Then, we search algorithm-agnostic HPs with the searched ? following Cha et al .<ref type="bibr" target="#b12">[13]</ref>. They propose reduced HP search space for efficiency compared to DomainBed[25]. The protocol searches the learning rate in [</figDesc><table><row><cell>1e-5,</cell></row><row><cell>3e-5, 5e-5], dropout in [0.0, 0.1, 0.5], and weight decay in [1e-4, 1e-6]. The batch</cell></row><row><cell>size per domain is fixed to 32. Since MIRO is a regularization method, we add</cell></row><row><cell>a case of no weight decay.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Performances of class-conditional MIRO.</figDesc><table><row><cell>Algorithm</cell><cell>PACS</cell><cell>VLCS</cell><cell cols="3">OfficeHome TerraInc Avg.</cell></row><row><cell>ERM</cell><cell cols="2">84.2?0.1 77.3?0.1</cell><cell>67.6?0.2</cell><cell>47.8?0.6</cell><cell>69.2</cell></row><row><cell>MIRO</cell><cell cols="2">85.4?0.4 79.0?0.0</cell><cell>70.5?0.4</cell><cell cols="2">50.4?1.1 71.3</cell></row><row><cell>C-MIRO</cell><cell cols="2">85.3?0.5 78.5?0.5</cell><cell>70.8?0.3</cell><cell>49.4?0.3</cell><cell>71.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Average accuracies of ERM?, ERM+, and MIRO in PACS. ERM? and ERM+ indicate ERM without and with pre-trained model, respectively.</figDesc><table><row><cell>Pre-trained model</cell><cell cols="3">ERM? ERM+ MIRO</cell></row><row><cell>ResNet-50 (ImageNet)</cell><cell>51.6</cell><cell>84.2</cell><cell>85.4</cell></row><row><cell>RegNet-16GF (Instagram-3.6B)</cell><cell>51.5</cell><cell>89.6</cell><cell>97.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that the terminology ERM can be unfair because other methods also minimize "empirical risk" but with different loss designs. We use the terminology "ERM" to indicate the cross-entropy baseline as suggested by Gulrajani and Lopez-Paz [25].</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variational information distillation for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Invariant risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning de-biased representations with biased representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bahng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Decaug: Out-of-distribution generalization via decomposed feature representation and semantic augmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H G</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Metareg: Towards domain generalization using meta-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From detection of individual metastases to classification of lymph node status at the patient level: the came-lyon17 challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Geessink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Manson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Dijk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Balkenhol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hermsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Paeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The im algorithm: a variational approach to information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Agakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recognition in terra incognita</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Domain generalization by marginal transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Deshmukh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Exploiting domain-specific features to enhance domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (2021) 1, 3</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Domain generalization by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>D&amp;apos;innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Swad: Domain generalization by seeking flat minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to balance specificity and invariance for in and out of domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Preserving domain private representation via mutual information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>De Silva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.03102</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Dark model adaptation: Semantic image segmentation from daytime to nighttime</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Domain generalization with adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Delta: Deep learning transfer using feature map with attention for convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain generalization via conditional invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Feature-critic networks for heterogeneous domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Domain generalization using a mixture of multiple latent domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsuura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mitzkus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rusak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bringmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.07484</idno>
		<title level="m">Benchmarking robustness in object detection: Autonomous driving when winter is coming</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Domain generalization via invariant feature representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reducing domain gap by reducing style bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Permuted adain: Reducing the bias towards global statistics in image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Nuriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
	<note>2021) 2, 5, 11, 13, 14</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Model-based domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Robey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hassani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>Distributionally robust neural networks</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Which shortcut cues will dnns choose? a study from the parameter-space perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Scimeca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Poli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Gradient matching for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Revisiting weakly supervised pre-training of visual perception models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adcock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De Freitas Reis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gedik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (2022) 2, 5, 9</title>
		<imprint>
			<date type="published" when="1920" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Contrastive representation distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<publisher>NY: Wiley</publisher>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep hashing network for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Does object recognition work for everyone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Heterogeneous domain generalization via domain mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust fine-tuning of zeroshot models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note>Recognition (2022) 2, 3, 5</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Noise or signal: The role of image backgrounds in object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Engstrom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ilyas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adversarial domain adaptation with domain mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Explicit inductive bias for transfer learning with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xuhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Davoine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.00677</idno>
		<title level="m">Improve unsupervised domain adaptation with mixup training</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Adversarial teacher-student representation learning for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">E</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">Y</forename><surname>Shiau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">C F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards fairer datasets: Filtering and balancing the distribution of the people subtree in the imagenet hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Qinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Fairness, Accountability, and Transparency</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Barlow twins: Self-supervised learning via redundancy reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zbontar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Adaptive risk minimization: Learning to adapt to domain shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Domain generalization via entropy regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning to generate novel domains for domain generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Domain generalization with mixstyle</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
