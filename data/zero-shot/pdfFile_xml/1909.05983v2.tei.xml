<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Content-Aware Unsupervised Deep Homography Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirong</forename><surname>Zhang</surname></persName>
							<email>zhangjirong@std.</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuan</forename><surname>Wang</surname></persName>
							<email>wangchuan@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuaicheng</forename><surname>Liu</surname></persName>
							<email>liushuaicheng@</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanpeng</forename><surname>Jia</surname></persName>
							<email>jialanpeng@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianjin</forename><surname>Ye</surname></persName>
							<email>yenianjin@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jue</forename><surname>Wang</surname></persName>
							<email>wangjue@megvii.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>sunjian@megvii.com</email>
							<affiliation key="aff1">
								<orgName type="department">Megvii Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Content-Aware Unsupervised Deep Homography Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>/JirongZhang/DeepHomography * Joint First Authors, ? Corresponding Author Accepted by ECCV 2020 ?</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Homography</term>
					<term>deep homography</term>
					<term>image alignment</term>
					<term>RANSAC</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Homography estimation is a basic image alignment method in many applications. It is usually conducted by extracting and matching sparse feature points, which are error-prone in low-light and low-texture images. On the other hand, previous deep homography approaches use either synthetic images for supervised learning or aerial images for unsupervised learning, both ignoring the importance of handling depth disparities and moving objects in real world applications. To overcome these problems, in this work we propose an unsupervised deep homography method with a new architecture design. In the spirit of the RANSAC procedure in traditional methods, we specifically learn an outlier mask to only select reliable regions for homography estimation. We calculate loss with respect to our learned deep features instead of directly comparing image content as did previously. To achieve the unsupervised training, we also formulate a novel triplet loss customized for our network. We verify our method by conducting comprehensive comparisons on a new dataset that covers a wide range of scenes with varying degrees of difficulties for the task. Experimental results reveal that our method outperforms the state-of-the-art including deep solutions and feature-based solutions.</p><p>Keywords: Homography; deep homography; image alignment; RANSAC ? This is the arXiv version of our ECCV 2020 paper (Oral, Top 2%, with 3/3 Strong Accepts), with more details revealed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Homography can align images taken from different perspectives if they approximately undergo a rotational motion or the scene is close to a planar surface <ref type="bibr" target="#b14">[13]</ref>. For scenes that satisfy the constraints, a homography can align them directly. For scenes that violate the constraints, e.g., a scene that consists of multiple planes or contains moving objects, homography usually serves as an <ref type="bibr">SIFT</ref>   <ref type="figure">Fig. 1</ref>: Our deep homography estimation on challenging cases, compared with one traditional feature-based, i.e. SIFT <ref type="bibr" target="#b24">[23]</ref> + RANSAC and one unsupervised DNN-based method <ref type="bibr" target="#b28">[27]</ref>. (a) An example with dominate moving foreground. (b) A low texture example. (c) A low light example. We mix the blue and green channels of the warped image and the red channel of the target image to obtain the visualization results as above, where the misaligned pixels appear as red or green ghosts. The same visualization method is applied for the rest of this paper.</p><p>initial alignment model before more advanced models such as mesh flow <ref type="bibr" target="#b21">[20]</ref> and optical flow <ref type="bibr" target="#b17">[16]</ref>. Most of the time, such a pre-alignment is crucial for the final quality. As a result, the homography has been widely applied in vision tasks such as multi-frame HDR imaging <ref type="bibr" target="#b11">[10]</ref>, multi-frame image super resolution <ref type="bibr" target="#b35">[34]</ref>, burst image denoising <ref type="bibr" target="#b23">[22]</ref>, video stabilization <ref type="bibr" target="#b22">[21]</ref>, image/video stitching <ref type="bibr" target="#b37">[36,</ref><ref type="bibr">12]</ref>, SLAM <ref type="bibr" target="#b27">[26,</ref><ref type="bibr" target="#b43">42]</ref>, augmented reality <ref type="bibr" target="#b31">[30]</ref> and camera calibration <ref type="bibr" target="#b41">[40]</ref>.</p><p>Homography estimation by traditional approaches generally requires matched image feature points such as SIFT <ref type="bibr" target="#b24">[23]</ref>. Specifically, after a set of feature correspondences are obtained, a homography matrix is estimated by Direct Linear Transformation (DLT) <ref type="bibr" target="#b14">[13]</ref> with RANSAC outlier rejection <ref type="bibr" target="#b10">[9]</ref>. Feature-based methods commonly could achieve good performance while they highly rely on the quality of image features. Estimation could be inaccurate due to insufficient number of matched points or poor distribution of the features, which is a common case due to the existence of textureless regions (e.g., blue sky and white wall), repetitive patterns or illumination variations. Moreover, the rejection of outlier points, e.g., point matches that located on the non-dominate planes or dynamic objects, is also important for high quality results. Consequently, feature-based homography estimation is usually a challenging task for these non-regular scenes.</p><p>Due to the development of deep neural networks (DNN) in recent years, DNN-based solutions to homography estimation are gradually proposed such as supervised <ref type="bibr" target="#b8">[7]</ref> and unsupervised <ref type="bibr" target="#b28">[27]</ref> ones. For the former solution, it requires homography as ground truth (GT) to supervise the training, so that only synthetic target images warped by the GT homography could be generated. Although the synthetic image pairs can be produced in arbitrary scale, they are far from real cases because real depth disparities are unavailable in the training data. As such, this method suffers from bad generalization to real images. To tackle this issue, Nguyen et al. proposed the latter unsupervised solution <ref type="bibr" target="#b28">[27]</ref>, which minimizes the photometric loss on real image pairs. However, this method has two main problems. One is that the loss calculated with respect to image intensity is less effective than that in the feature space, and the loss is calculated uniformly in the entire image ignoring the RANSAC-like process. As a result, this method cannot exclude the moving or non-planar objects to contribute the final loss, so as to potentially decrease the estimation accuracy. To avoid the above phenomenons, Nguyen et al. <ref type="bibr" target="#b28">[27]</ref> has to work on aerial images that are far away from the camera to minimize the influence of depth variations of parallax.</p><p>To tackle the aforementioned issues, we propose an unsupervised solution to homography estimation by a new architecture with content-awareness learning. It is designed specially for image pairs with a small baseline, as this case is commonly applicable for consecutive video frames, burst image capturing or photos captured by a dual-camera cellphone. In particular, to robustly optimize a homography, our network implicitly learns a deep feature for alignment and a content-aware mask to reject outlier regions simultaneously. The learned feature is used for loss calculation instead of using photometric loss as in <ref type="bibr" target="#b8">[7]</ref>, and learning a content-aware mask makes the network concentrate on the important and registrable regions. We further formulate a novel triplet loss to optimize the network so that the unsupervised learning could be achieved. Experimental results demonstrate the effectiveness of all the newly involved techniques for our network, and qualitative and quantitative evaluations also show that our network outperforms the state-of-the-art as shown in Figs. 1, 6 and 7. We also introduce a comprehensive image pair dataset, which contains 5 categories of scenes as well as human-labeled GT point correspondences for quantitative evaluation of its validation set ( <ref type="figure" target="#fig_5">Fig. 5</ref>). To summarize, our main contributions are:</p><p>-A novel network structure that enables content-aware robust homography estimation from two images with small baseline.</p><p>-A triplet loss designed for unsupervised training, so that an optimal homography matrix could be produced as an output, together with a deep feature map for alignment and a mask highlighting the alignment inliers being implicitly learned as intermediate results.</p><p>-A comprehensive dataset covers various scenes for unsupervised training of image alignment models, including but not limited to homography, mesh warps or optical flow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Traditional homography. A homography is a 3?3 matrix which compensates plane motions between two images. It consists of 8 degree of freedom (DOF), with each 2 for scale, translation, rotation and perspective <ref type="bibr" target="#b14">[13]</ref> respectively. To solve a homography, traditional approaches often detect and match image features, such as SIFT <ref type="bibr" target="#b24">[23]</ref>, SURF <ref type="bibr" target="#b5">[4]</ref>, ORB <ref type="bibr" target="#b30">[29]</ref>, LPM <ref type="bibr" target="#b26">[25]</ref>, GMS <ref type="bibr" target="#b6">[5]</ref>, SOSNet <ref type="bibr" target="#b33">[32]</ref>, LIFT <ref type="bibr" target="#b36">[35]</ref> and OAN <ref type="bibr" target="#b39">[38]</ref>. Two sets of correspondences were established between two images, following which robust estimation is adopted, such as the classic RANSAC <ref type="bibr" target="#b10">[9]</ref>, IRLS <ref type="bibr" target="#b16">[15]</ref> and MAGSAC <ref type="bibr" target="#b4">[3]</ref>, for the outlier rejection during the model estimation. A homogrpahy can also be solved directly without image features. The direct methods, such as seminal Lucas-Kanade algorithm <ref type="bibr" target="#b25">[24]</ref>, calculates sum of squared differences (SSD) between two images. The differences guide the shift of the images, yielding homography updates. A random initialized homography is optimized in this way iteratively <ref type="bibr" target="#b3">[2]</ref>. Moreover, the SSD can be replaced with enhanced correlation coefficient (ECC) for the robustness <ref type="bibr" target="#b9">[8]</ref>.</p><p>Deep homography. Following the success of various deep image alignment methods such as optical flow <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b17">16]</ref>, dense matching <ref type="bibr" target="#b29">[28]</ref>, learned descriptors <ref type="bibr" target="#b33">[32]</ref> and deep features <ref type="bibr" target="#b2">[1]</ref>, a deep homography solution was first proposed by <ref type="bibr" target="#b8">[7]</ref> in 2016. The network takes source and target images as input and produces 4 corner displacement vectors of source image, so as to yield the homography. It used GT homography to supervise the training. However, the training images with GT homography is generated without depth disparity. To overcome such issue, Nguyen et al. <ref type="bibr" target="#b28">[27]</ref> proposed an unsupervised approach that computed photometric loss between two images and adopted Spatial Transform Network (STN) <ref type="bibr" target="#b18">[17]</ref> for image warping. However, they calculated loss directly on the intensity and uniformly on the image plane. In contrast, we learn a content-aware mask. Notebaly, predicting mask for effective estimation has been attempted in other tasks, such as monocular depth estimation <ref type="bibr" target="#b42">[41,</ref><ref type="bibr">11]</ref>. Here, it is introduced for the unsupervised homography learning.</p><p>Image stitching. Image stitching methods <ref type="bibr" target="#b37">[36,</ref><ref type="bibr" target="#b20">19]</ref> are traditional methods that focus on stitching images under large baselines <ref type="bibr" target="#b38">[37]</ref> for the purpose of constructing the panorama <ref type="bibr" target="#b7">[6]</ref>. The stitched images were often captured with dramatic viewpoint differences. In this work, we focus on images with small baselines for the purpose of multi-frame applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Network Structure</head><p>Our method is built upon convolutional neural networks. It takes two grayscale image patches I a and I b as input, and produces a homography matrix H ab from I a to I b as output. The entire structure could be divided into three modules: a   <ref type="table">Table 1</ref>). To train the network in (a), we design a triplet loss composed of L n , L as defined in Eq. 4, 5 and 6.</p><p>feature extractor f (?), a mask predictor m(?) and a homography estimator h(?). f (?) and m(?) are fully convolutional networks which accepts input of arbitrary sizes, and the h(?) utilizes a backbone of ResNet-34 <ref type="bibr" target="#b15">[14]</ref> and produces 8 values. <ref type="figure" target="#fig_1">Fig. 2</ref>(a) illustrates the network structure.</p><p>Feature extractor. Unlike previous DNN based methods that directly utilizes the pixel intensity values as the feature, here our network automatically learns a deep feature from the input for robust feature alignment. To this end, we build a fully convolutional network (FCN) that takes an input of size H ? W ? 1, and produces a feature map of size H ? W ? C. For inputs I a and I b , the feature extractor shares weights and produces feature maps F a and F b , i.e.</p><formula xml:id="formula_0">F ? = f (I ? ), ? ? {a, b}<label>(1)</label></formula><p>The learned feature is more robust than pixel intensity when applied to loss calculation. Especially for the images with luminance variations, the learned feature is pretty robust when compared to the pixel intensity. See Sec. 4.3 and <ref type="figure" target="#fig_3">Fig. 3</ref> for a detailed verification of the effectiveness of this module.</p><p>Mask predictor. In non-planar scenes, especially those including moving objects, there exists no single homography that can align the two views. In traditional algorithm, RANSAC is widely applied to find the inliers for homography estimation, so as to solve the most approximate matrix for the scene alignment.   <ref type="table">Table 1</ref>: Layer configurations of feature extractor (a), mask predictor (b) and homography estimator (c). In (c), Layer 2 and 35 are max pool and global average pool separately.</p><p>Following the similar idea, we build a sub-network to automatically learn the positions of inliers. Specifically, a sub-network m(?) learns to produce an inlier probability map or mask, highlighting the content in the feature maps that contribute much for the homography estimation. The size of the mask is the same as the size of the feature maps F a and F b . With the masks, we further weight the features extracted by f before feeding them to the homography estimator, obtaining two weighted feature maps G a and G b as,</p><formula xml:id="formula_1">M ? = m(I ? ), G ? = F ? M ? , ? ? {a, b}<label>(2)</label></formula><p>As introduced later, the mask learned as above actually play two roles in the network, one works as an attention map, and the other works as a outlier rejecter. See the details in Sec. 3.2, 4.3 and <ref type="figure" target="#fig_4">Fig. 4</ref> for more discussion.</p><p>Homography estimator. Given the weighted feature maps G a and G b , we concatenate them to build a feature map [G a , G b ] of size H ? W ? 2C. Then it is fed to the homography estimator network and four 2D offset vectors (8 values) are produced. With the 4 offset vectors, it is straight-forward to obtain the homography matrix H ab with 8 DOF by solving a linear system. We use h(?) to represent the whole process, i.e.</p><formula xml:id="formula_2">H ab = h([G a , G b ])<label>(3)</label></formula><p>The backbone of h(?) follows a ResNet-34 structure. It contains 34 layers of strided convolutions followed by a global average pooling layer, which generates fixed size (8 in our case) of feature vectors regardless of the input feature dimensions. Please refer to <ref type="table">Table 1</ref> for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Triplet Loss for Robust Homography Estimation</head><p>With the homography matrix H ab estimated, we warp image I a to I a and then further extracts its feature map as F a . Intuitively, if the homography matrix H ab is accurate enough, F a should be well aligned with F b , causing a low l 1 loss between them. Considering in real scenes, a single homography matrix normally cannot satisfy the transformation between the two views, we also normalize the l 1 loss by M a and M b . Here M a is the warped version of M a . So the loss between the warped I a and I b is as follows,</p><formula xml:id="formula_3">w/o. f(?) Ours Input / target</formula><formula xml:id="formula_4">L n (I a , I b ) = i M a M b ? ||F a ? F b || 1 i M a M b<label>(4)</label></formula><p>where F a = f (I a ) and I a = W arp(I a , H ab ). Index i indicates pixel locations in the masks and feature maps. STN <ref type="bibr" target="#b18">[17]</ref> is used to achieve the warping operation. Directly minimizing Eq. 4 may easily cause trivial solutions, where the feature extractor only produces all zero maps, i.e. F a = F b = 0. In this case, the features learned indeed describe the fact that I a and I b are "well aligned", but it fails to reflect the fact that the original images I a and I b are mis-aligned. To this end, we involve another loss between F a and F b , i.e.</p><formula xml:id="formula_5">L(I a , I b ) = ||F a ? F b || 1<label>(5)</label></formula><p>and further maximize it when minimizing Eq. 4. This strategy avoids the trivial all-zero solutions, and enables the network to learn a discriminative feature map.</p><p>In practise, we swap the features of I a and I b and produce another homography matrix H ba . Following Eq. 4, we involve a loss L n (I b , I a ) between the warped I b and I a . We also add a constraint that enforces H ab and H ba to be inverse. So, the optimization procedure of the network is written as follows,  where ? and ? are balancing hyper-parameters, and I is a 3-order identity matrix. We set ? = 2.0 and ? = 0.01 in our experiments. We show the loss formulations in <ref type="figure" target="#fig_1">Fig. 2(b)</ref>, and validate its effectiveness by an ablation study detailed in Sec. 4.3, which shows that it decreases the error at least 50% in average.</p><formula xml:id="formula_6">min m,f,h L n (I a , I b ) + L n (I b , I a ) ? ?L(I a , I b ) + ?||H ab H ba ? I|| 2 2 (6) (a) (b) (c) (d) No</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Unsupervised Content-Awareness Learning</head><p>As mentioned above, our network contains a sub-network m(?) to predict an inlier probability mask. It is such designed that our network can be of contentawareness by the two-fold roles. First, we use the masks M a , M b to explicitly weight the features F a , F b , so that only highlighted features could be fully fed into homography estimator h(?). The masks actually serve as attention maps for the feature maps. Second, they are also implicitly involved into the normalized loss Eq. 4, working as a weighting item. By doing this, only those regions that are really fit for alignment would be taken into account. For those areas containing low texture or moving foreground, because they are non-distinguishable or misleading for alignment, they are naturally removed for homography estimation during optimizing the triplet loss as proposed. Such a content-awareness is achieved fully by an unsupervised learning scheme, without any GT mask data as supervision. To demonstrate the effectiveness of the mask as the two roles, we conduct an ablation study by disabling the effect of mask working as an at- tention map or as a loss weighting item. As seen in <ref type="table">Table 2</ref>(c), the accuracy has a significant decrease when mask is removed in either case.</p><p>We also illustrate several examples in <ref type="figure" target="#fig_4">Fig. 4</ref> to show the mask effectiveness. For example, in <ref type="figure" target="#fig_4">Fig. 4(a)(b)</ref> where the scenes contain large dynamic foregrounds, our network successfully rejects moving objects, even if the movements are inapparent as the fountain in (b), or the objects occupy a large space as in (a). These cases are very difficult for RANSAC to find robust inliers. <ref type="figure" target="#fig_4">Fig. 4(c)</ref> is a low-textured example, in which the sky and snow ground occupies almost the entire image. It is challenging for traditional methods because not enough feature matches can be provided. Our predicted mask concentrates on the horizon for the alignment. Last, <ref type="figure" target="#fig_4">Fig. 4(d)</ref> is a low light example, where only visible areas contain weights as seen. We also illustrate an example to show the two effects by the mask as separate roles in the bottom 2 rows of <ref type="figure" target="#fig_4">Fig. 4</ref>. Details about this ablation study are introduced later in Sec. 4.3.</p><p>We adopt a two-stage strategy to train our network. Specifically, we first train the network by disabling the attention map role of the mask, i.e. G ? = F ? , ? ? {a, b}. After about 60k iterations, we finetune the network by involving the attention map role of the mask as Eq. 2. We validate this training strategy by another ablation study detailed in Sec. 4.3, where we train the network totally from scratch. This two-stage training strategy reduces the error by 4.40% in average, as shown in Row 10 of Table 2(c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and Implementation Details</head><p>We propose our dataset for comprehensive homography evaluation considering there lacks dedicated dataset for this task. Our dataset contains 5 categories of totally 80k image pairs, including regular (RE), low-texture (LT), low-light (LL), small-foregrounds (SF), and large-foregrounds (LF) scenes, with each category ? 16k image pairs, as shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. For the test data, 4.2k image pairs are randomly chosen from all categories. For each pair, we manually marked 6 ? 8 equally distributed matching points for the purpose of quantitative comparisons, as illustrated in the rightmost column of <ref type="figure" target="#fig_5">Fig. 5</ref>. The category partition is based on the understanding and property of traditional homography registration. Experimental results demonstrate our method is robust over all categories as seen in <ref type="figure">Figs. 1, 6</ref>, 7 and the supplementary materials, which also contain a detailed introduction to each category.</p><p>Our network is trained with 120k iterations by an Adam optimizer <ref type="bibr" target="#b19">[18]</ref>, with parameters being set as l r = 1.0 ? 10 ?4 , ? 1 = 0.9, ? 2 = 0.999, ? = 1.0 ? 10 ?8 . The batch size is 64, and for every 12k iterations, the learning rate l r is reduced by 20%. Each iteration costs about 1.2s and it takes nearly 40 hours to complete the entire training. The implementation is based on PyTorch and the network training is performed on 4 NVIDIA RTX 2080 Ti. To augment the training data and avoid black boundaries appearing in the warped image, we randomly crop patches of size 315 ? 560 from the original image to form I a and I b . Code is available at https://github.com/JirongZhang/DeepHomography.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Comparisons with Existing Methods</head><p>Qualitative comparison. We first compare our method with the existing two deep homography methods, the supervised <ref type="bibr" target="#b8">[7]</ref> and the unsupervised <ref type="bibr" target="#b28">[27]</ref> approaches, as illustrated in <ref type="figure">Fig. 6. Fig. 6(a)</ref> shows an synthesized example with no disparities. In this case, the supervised solution <ref type="bibr" target="#b8">[7]</ref> performs well enough as ours. However, it fails in the case that real consecutive frames of the same footage are applied ( <ref type="figure">Fig. 6(b)</ref>), because it is unable to handle large disparities and moving objects of the scene. <ref type="figure">Fig. 6(c)</ref> shows an example that contains a dominate planar building surface, where all methods work well. However, if the image pair involves illumination variation caused by camera flash, the unsupervised method <ref type="bibr" target="#b28">[27]</ref> fails due to its alignment metric being pixel intensity value difference instead of semantic feature difference, as seen in <ref type="figure">Fig. 6(d)</ref>. <ref type="figure">Fig. 6</ref>(e) and (f) contain near-range objects and two dominate planes with moving objects at corners respectively, and <ref type="figure">Fig. 6(g)</ref> and (h) are low texture and low light examples separately. Similarly, in all of these scenarios, our method produces warped images with more pixels aligned, so as to obviously outperform the other two DNN-based methods.</p><p>We also compare our method with some feature-based solutions. Specially, we choose SIFT <ref type="bibr" target="#b24">[23]</ref>, ORB <ref type="bibr" target="#b30">[29]</ref>, LIFT <ref type="bibr" target="#b36">[35]</ref> and SOSNet <ref type="bibr" target="#b33">[32]</ref> as the feature descriptors and choose RANSAC <ref type="bibr" target="#b10">[9]</ref> and MAGSAC <ref type="bibr" target="#b4">[3]</ref> as the outlier rejection algorithms, obtaining 8 combinations. We show 3 examples in <ref type="figure" target="#fig_6">Fig. 7</ref>, where (a)(b) show the 8 combinations produce reasonable but low quality results, and (c) shows one that most of them fail thoroughly. Note that the failure cases caused by low texture or low light condition frequently appears in our dataset, and it may lead  <ref type="table">Table 2</ref>: Quantitative comparison between ours and all other methods including DNN-based (Row 3, 4) and feature-based (Row 5 ? 12) ones, in terms of errors (a) and robustness (b), as well as ablation studies on mask (Rows 2 ? 4), triplet loss (Row 5), feature extractor (Row 6), backbones (Rows 7 ? 9) and training strategy (Row 10) in (c). For (b), we calculate the inlier percentage when matched points are within 3 pixels. For each scene, we mark the best solution in red. For the scenes ours beats the others, we mark the 2nd best solution in blue.</p><p>to unstable results in real applications such as video stabilization or multi-frame image fusion. In comparison, our method is robust against these challenges.</p><p>Quantitative comparison. We demonstrate the performance of our method by comparing it with all of the other methods quantitatively. The comparison is based on our dataset and the average l 2 distances between the warped points and the human-labeled GT points are evaluated as the error metric. We report the errors for each category and the overall averaged error in <ref type="table">Table 2</ref>, where I 3?3 refers to a 3 ? 3 identity matrix as a "no-warping" homography for reference. As seen, our method outperforms the others for all categories, except for regular (RE) scenes if compared with feature-based methods. This result is reasonable because in RE scenes, rich texture delivers sufficient high quality features so that it is naturally friendly for the feature-based solutions. Even though, our error is only 5.85% higher than the best solution in this case, i.e. SIFT [23] + MAGSAC <ref type="bibr" target="#b4">[3]</ref>. For the rest scenes, our method consistently beats the others, especially for the low texture (LT) and low light (LL) scenes, where our error is lower than the 2nd best by 25.78% and 7.62% respectively. For the scenes containing small (SF) and large (LF) foreground, although the 2nd best method SOSNet <ref type="bibr" target="#b33">[32]</ref> + MAGSAC <ref type="bibr" target="#b4">[3]</ref> only loses to ours very slightly (0.57% and 2.82%), it cannot well handle the LT and LL scenes, where its errors are higher than the 2nd best by 100.78% and 109.05% separately. It is worth noting that the two solutions involving LIFT <ref type="bibr" target="#b36">[35]</ref> feature produce rather stable results for all scenes, but their average errors are higher than ours by at least 12.08%. As for the DNN-based solutions, the supervised method <ref type="bibr" target="#b8">[7]</ref> suffers severely from the generalization problem as demonstrated by its errors being higher than us by at least 142.37% for all scenes, and the unsupervised method <ref type="bibr" target="#b28">[27]</ref> also apparently fails in the LT scene, causing over 50% higher error than ours in this case.</p><p>To further evaluate the robustness, a threshold (3 pixels) is used to count the percentage of inliners. Matches that beyond the threshold are considered as outliers. <ref type="table">Table 2</ref>(b) shows the inlier percentage on different scene categories of various methods. As seen, for tough cases, our method achieves the highest robustness compared with other competitors while for regular cases, our performance is on par with the others, which draws similar conclusion as by <ref type="table">Table 2</ref>(a). Please see <ref type="table">Table 2</ref> and bar charts in Figs. 6 and 7 for the detailed comparisons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Studies</head><p>Content-aware mask. As mentioned in Sec. 3.3, the content-aware mask takes effects in two-folds, working as an attention for the feature map, or as a weighting map to reject the outliers. We verify its effectiveness by evaluating the performance in the case of disabling both or either effect and report the errors in Row 2, 3, 4 of <ref type="table">Table 2</ref>(c). Specifically, for Row 3 "Mask as attention only", Eq. 4 is modified as L n (I a , I b ) = L(I a , I b ) = ||F a ? F b || 1 . On the contrary, for Row 4 "Mask as RANSAC only", Eq. 2 is modified as G ? = F ? , ? ? {a, b}. As the errors indicate, for most scenes the mask takes effect increasingly by the two roles, except for the scenes LT and LF where disabling one role only may cause the worst result. We also illustrate one example in Row 3, 4 of <ref type="figure" target="#fig_4">Fig. 4</ref>, where in the case of "Mask as attention only" the mask learns to highlight the most attractive edges or texture regions without rejecting the other regions (Column 2). On the contrary, in the case of "Mask as RANSAC only", the mask learns to highlight only sparse texture regions (Column 3) as inliers for alignment. In contrast, our method balances the two effects and learn a comprehensive and informative weighting map as shown in Column 4.</p><p>Feature extractor. We also disable the feature extractor to verify its effectiveness, i.e. setting F ? = I ? , ? ? {a, b} so that the loss is evaluated on pixel intensity values instead. In this case, the network loses some robustness, especially if applied to images with luminance change, as <ref type="figure" target="#fig_3">Fig. 3</ref> shows. As seen, if f (?) is disabled, the masks would be abnormally sparse because the loss reflects only a small falsely "aligned" region, causing a wrong homography estimated. In contrast, our results are stable enough thanks to the luminance invariant property of learned features. The errors are listed in Row 6 of <ref type="table">Table 2</ref>(c).</p><p>Triplet loss. We further exam the effectiveness of our triplet loss by removing the term of Eq. 5 from Eq. 6. As shown in <ref type="table">Table 2</ref>(c) "w/o. triplet loss", the triplet loss decreases errors over 50%, especially beneficial in LT (118.42% lower error) and LL (70.10% lower error) scenes, demonstrating that it not only avoids the problem of obtaining trivial solutions, but also facilitates a better optimization.</p><p>Backbone. We also exam several popular backbones, including VGG <ref type="bibr" target="#b32">[31]</ref>, ResNet-18 <ref type="bibr" target="#b15">[14]</ref>, and ShuffleNet <ref type="bibr" target="#b40">[39]</ref> for h(?). As seen in Rows 7 ? 9 of <ref type="table">Table 2</ref>(c), the ResNet-18 achieves similar performance as ours (ResNet-34). The VGG back- <ref type="figure">Fig. 8</ref>: Failure cases. Odd and even columns show the results by SIFT+RANSAC and our method.</p><p>bone is slightly worse than ResNet-18 and ResNet-34. Interestingly, the lightweight backbone ShuffleNet achieves similar performance with other large ones, indicating the potential application to portable systems of our method.</p><p>Training strategy. As aforementioned, we use a two-stage strategy to train the network. To validate this strategy, we conduct an ablation study to train the network from scratch. As Row 10 and 11 of <ref type="table">Table 2</ref>(c) reveal, our training strategy brings a 4.40% lower error in average, demonstrating its usefulness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Failure Cases</head><p>Although our method achieves state-of-the-art performance in small baseline scenes compared with the existing methods, it still has its limitation of being applied to large baseline scenes. The reason behind may lie in the limited perception field of the network which is unable to perceive the alignment information between the two images. With this limitation, our method is unable to be applied to applications relying on large baseline alignment such as image stitching. We show two failure results in <ref type="figure">Fig. 8</ref> for large baseline scenes by our method, in comparison with those by SIFT+RANSAC. As seen, SIFT+RANSAC produces stable results for the scenes. We will leave the solution for the large baseline alignment as a future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We have presented a new architecture for unsupervised deep homography estimation with content-aware capability, for small baseline scenarios. Unlike traditional feature base methods that heavily rely on the quality of image features so as to be vulnerable to low-texture and low-light scenes, or previous DNN-based solutions that pay less attention to the depth disparity issue, our network learns a content-aware mask during the estimation to reject outliers, such that the network can concentrate on the regions that can be aligned by a homography. To achieve it, we have designed a novel triplet loss to enable unsupervised training of our network. Moreover, we present a comprehensive dataset for image alignment. The dataset is divided into 5 categories of scenes, which can be used for the future research of image alignment models, including but not limited to homography, mesh alignment and optical flow. Extensive experiments and ablation studies demonstrate the effectiveness of our network as well as the triplet loss design, and reveal the superiority of our method over the state-of-the-art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>The overall structure of our deep homography estimation network (a) and the triplet loss we design to train the network (b). In (a), two input patches I a and I b are fed into two branches consisting of feature extractor f (?) and mask predictor m(?) respectively, generating features F a , F b and masks M a , M b . Then the features and masks are fed into a homography estimator to produce 8 values of the homography matrix H ab . In h(?), convolution blocks in various colors differ in the number of channels (detailed in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>(a) Feature extractor f (?)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 :</head><label>3</label><figDesc>Ablation study on the effectiveness of our feature extractor, demonstrated by examples with illuminance change, displayed separately in the left and right two columns. For each example, the input and target GT images are in Row 1, followed by the results by disabling the feature extractor f (?) (Row 2) and by ours (Row 3), including the learned masks and the aligned results in odd and even columns. As seen, our results are obviously stable for such a case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 :</head><label>4</label><figDesc>Row 1 and 2: Our predicted masks for various of scenes. (a) and (b) contains large dynamic foreground. (c) contains few textures and (d) is an night example. Row 3 and 4: Ablation study on the content-aware mask. We disable both or either role of the mask for comparisons. Errors are shown at the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>A glace of our dataset. For left 6 columns, from top to bottom are the 5 categories of the dataset. The rightmost column shows two examples of human labeled point correspondences for quantitative evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Comparison with 8 feature-based solutions on 3 examples, shown in (a)(d), (b)(e) and (c)(f). For the first 2 examples, our method produces more accurate results, while for the last one but not the least, most of the feature-based solutions fail extremely, which happens frequently for the low texture or low light scenes. We also display the errors by all the methods in bar chart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Comparison with existing DNN-based approaches. Column 1 shows the input and GT target images, columns 2 to 4 are results by the supervised<ref type="bibr" target="#b8">[7]</ref>, the unsupervised<ref type="bibr" target="#b28">[27]</ref> and our method. The errors by all the DNN-based methods are displayed by a bar chart at the bottom.</figDesc><table><row><cell>Input / Target GT RE 3) Supervised [7] (a) (b) (c) (d) (e) (f) (g) (h) 1.5 2.5 3.5 4.5 5.5 6.5 7.5 8.5 1) 2) I3?3 4) Unsupervised [27] 5) SIFT [23] + RANSAC [9] 6) SIFT [23] + MAGSAC [3] 7) ORB [29] + RANSAC [9] 8) ORB [29] + MAGSAC [3] 9) LIFT [35] + RANSAC [9] 10) LIFT [35] + MAGSAC [3] 11) SOSNet [32] + RANSAC [9] 12) SOSNet [32] + MAGSAC [3] 13) Ours (b) Robustness: Inlier Percentage When Matched Points Are within 3 Pixels Superv ised Unsuperv ised LT LL SF LF Supervised Unsupervised Ours RE LT LL SF LF 7.88 (+360.82%) 8.07 (+215.23%) 7.41 (+252.86%) 8.11 (+360.80%) 4.29 (+142.37%) 7.12 (+316.37%) 7.53 (+194.14%) 6.86 (+226.67%) 7.83 (+344.89%) 4.46 (+151.98%) 1.88 (+9.94%) 3.21 (+25.39%) 2.27 (+8.10%) 1.93 (+9.66%) 1.97 (+11.30%) 1.72 (+0.58%) 2.56 (+0.00%) 4.97 (+136.67%) 1.82 (+3.41%) 1.84 (+3.95%) 1.71 (+0.00%) 3.15 (+23.05%) 4.91 (+133.81%) 1.88 (+6.82%) 1.79 (+1.13%) 1.85 (+8.19%) 3.76 (+46.88%) 2.56 (+21.90%) 2.00 (+13.64%) 2.29 (+29.38%) 2.02 (+18.13%) 5.18 (+102.34%) 2.78 (+32.38%) 1.92 (+9.09%) 2.25 (+27.12%) 1.76 (+2.92%) 3.04 (+18.75%) 2.14 (+1.90%) 1.82 (+3.41%) 1.92 (+8.47%) 1.73 (+1.17%) 2.92 (+14.06%) 2.10 (+0.00%) 1.79 (+1.70%) 1.79 (+1.13%) 1.72 (+0.58%) 3.70 (+44.53%) 4.58 (+118.09%) 1.84 (+4.54%) 1.83 (+3.39%) 1.73 (+1.17%) 5.14 (+100.78%) 4.39 (+109.05%) 1.76 (+0.00%) 1.77 (+0.00%) 1.81 (+5.85%) 1.90 (-25.78%) 1.94 (-7.62%) 1.75 (-0.57%) 1.72 (-2.82%) 1) RE LT LL SF LF 2) I3?3 12.75% (-85.35%) 37.83% (-54.13%) 36.68% (-55.32%) 48.46% (-42.43%) 64.30% (-25.15%) (c) Ablation Studies 1) RE LT LL SF LF 2) No mask involved 2.10 (+16.02%) 2.51 (+32.11%) 2.48 (+27.84%) 3.02 (+72.57%) 1.78 (+3.49%) 3) Mask as attention only 1.85 (+2.21%) 3.37 (+77.37%) 2.16 (+11.34%) 2.29 (+30.86%) 1.75 (+1.74%) 4) Mask as RANSAC only 1.85 (+2.21%) 2.16 (+13.68%) 2.17 (+11.86%) 2.04 (+16.57%) 2.16 (+25.58%) 5) w/o. Triple loss 2.16 (+19.34%) 4.15 (+118.42%) 3.30 (+70.10%) 2.49 (+42.29%) 2.09 (+21.51%) 6) w/o. Feature extractor 1.89 (+4.42%) 2.54 (+33.68%) 2.13 (+9.79%) 1.80 (+2.86%) 1.79 (+4.07%) 7) VGG [31] 1.91 (+5.52%) 2.89 (+52.11%) 2.05 (+5.67%) 2.14 (+22.29%) 1.88 (+9.30%) 8) ResNet-18 [14] 1.84 (+1.66%) 2.30 (+21.05%) 2.05 (+5.67%) 2.28 (+30.29%) 1.85 (+7.56%) 9) ShuffleNet-v2 [39] 2.05 (+13.26%) 2.85 (+50.00%) 2.61 (+34.54%) 2.72 (+55.43%) 1.99 (+15.70%) 10) Train from scratch 1.87 (+3.31%) 2.00 (+5.26%) 1.98 (+2.06%) 1.90 (+8.57%) 1.77 (+2.91%) Fig. 6: (a) Errors 11) Ours 1.81 1.90 1.94 1.75 1.72</cell><cell>Ours 7.15 (+245.41%) Avg Avg 6.76 (+226.57%) 2.25 (+8.70%) 2.58 (+24.64%) 3.20 (+54.59%) 2.49 (+20.29%) 2.83 (+36.71%) 2.14 (+3.38%) 2.07 (+0.00%) 2.73 (+31.88%) 2.99 (+44.44%) 1.82 (-12.08%) Avg 38.76% (-53.79%) Avg 2.38 (+30.77%) 2.27 (+24.73%) 2.07 (+13.74%) 2.84 (+56.04%) 2.03 (+11.54%) 2.17 (+19.23%) 2.06 (+13.19%) 2.44 (+34.07%) 1.90 (+4.40%) 1.82</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sosnet</surname></persName>
		</author>
		<idno>32]+RANSAC [9] 87.03% (-0.01%) 81.44% (-1.26%) 80.69% (-1.71%) 84.10% (-0.08%) 85.48% (-0.49%) 83.63% (-0.30%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sosnet</surname></persName>
		</author>
		<idno>32]+MAGSAC [3] 86.93% (-0.13%) 81.81% (-0.81%) 80.63% (-1.78%) 83.29% (-1.05%) 85.84% (-0.07%) 83.69%</idno>
		<imprint/>
	</monogr>
	<note>23%) 13) Ours 86.12% (-1.06%) 83.58% (+1.33%) 83.63% (+1.88%) 85.23% (+1.26%) 87.36% (+1.70%) 85.10% (+1.45%) References</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning to detect and match keypoints with deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Altwaijry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Lucas-kanade 20 years on: A unifying framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Magsac: marginalizing sample consensus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Barath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Noskova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Surf: Speeded up robust features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Gms: Grid-based motion statistics for fast, ultra-robust feature correspondence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recognising panoramas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03798</idno>
		<title level="m">Deep image homography estimation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Parametric image alignment using enhanced correlation coefficient maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">D</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Z</forename><surname>Psarakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-exposure imaging on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Gelfand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pulli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM Multimedia</title>
		<meeting>ACM Multimedia</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Digging into self-supervised monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Firman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint video stitching and stabilization from moving cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gabbouj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5491" to="5503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Multiple view geometry in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Cambridge university press</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust regression using iteratively reweighted leastsquares</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Welsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics-theory and Methods</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Spatial transformer networks</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Direct photometric alignment by mesh deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">F</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Meshflow: Minimum latency online video stabilization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="800" to="815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bundled camera paths for video stabilization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">78</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fast burst images denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Locality preserving matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">127</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Orb-slam: a versatile and accurate monocular slam system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on robotics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1147" to="1163" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised deep homography: A fast and robust homography estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Shivakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepmatching: Hierarchical deformable dense matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Orb: An efficient alternative to sift or surf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Markerless tracking using planar structures in the scene</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Symposium on Augmented Reality</title>
		<meeting>International Symposium on Augmented Reality</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="120" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Sosnet: Second order similarity regularization for local descriptor learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Heijnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Balntas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Handheld multi-frame super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wronski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Garcia-Dorado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ernst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krainin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Lift: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">As-projective-as-possible image stitching with moving dlt</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zaragoza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Parallax-tolerant image stitching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning two-view correspondences and geometry using order-aware network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">14</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A flexible new technique for camera calibration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1330" to="1334" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Coslam: Collaborative visual slam in dynamic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="354" to="366" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
