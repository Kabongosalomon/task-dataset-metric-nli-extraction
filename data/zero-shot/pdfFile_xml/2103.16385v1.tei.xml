<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Stacked Hourglass Networks for 3D Human Pose Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhan</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Osaka University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wataru</forename><surname>Takano</surname></persName>
							<email>takano@sigmath.es.osaka-u.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="institution">Osaka University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Stacked Hourglass Networks for 3D Human Pose Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:35+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we propose a novel graph convolutional network architecture, Graph Stacked Hourglass Networks, for 2D-to-3D human pose estimation tasks. The proposed architecture consists of repeated encoder-decoder, in which graph-structured features are processed across three different scales of human skeletal representations. This multiscale architecture enables the model to learn both local and global feature representations, which are critical for 3D human pose estimation. We also introduce a multi-level feature learning approach using different-depth intermediate features and show the performance improvements that result from exploiting multi-scale, multi-level feature representations. Extensive experiments are conducted to validate our approach, and the results show that our model outperforms the state-of-the-art.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, with the application of deep learning methods, the performance of 2D human pose estimation has been greatly improved. Recent works show that using such detected 2D joints positions, the 3D human pose can also be efficiently and accurately regressed <ref type="bibr" target="#b27">[27]</ref>. Due to the graph structure formed by the topology of the human skeleton, many attempts have been made to use the generalized form of CNN: Graph Convolutional Networks (GCN), to perform the regression task of 2D-to-3D human pose estimation <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23]</ref>. Since the graph convolution has a good feature extraction capability for the graph-structured data, the GCN-based approaches work well and some of them achieve the state-of-the-art results in the 2D-to-3D human pose estimation task.</p><p>However, the existing GCN-based approaches have the following limitations: First, the graph convolutions exploit all node information, which can be seen as that all features are processed at only "one scale". Thus it is difficult to extract features that can represent spatial local and global information and limits the representation capabilities. Second, most existing approaches use a straightforward archi-tecture of sequentially connecting the graph convolution layers ( <ref type="figure" target="#fig_3">Fig. 4 (Top)</ref>). Such a model architecture does not take advantage of the benefits of model depth, such as intermediate features at each depth, and therefore limits its performance.</p><p>The core issue mentioned above is that the features extracted by existing architecture are oversimple, which limits the expressiveness of the model. Features with greater representation capabilities, such as multi-scale and multilevel features, are commonly used in image-related tasks. For multi-scale features, they denote the information from small to large resolutions of the image features, thus bringing rich image understanding from local to global <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref>, while multi-level features denote the latent representations in different depths of the latent space, bringing important semantic information at all levels from shallow to deep. The introduction of the above multi-scale and multi-level features can enrich the performance of the model. However, because the upsampling and downsampling operations required for multi-scale features are defined on the image, and the graph has an irregular structure, such methods cannot be directly applied to the graph-structured data.</p><p>To address these issues, we propose a novel architecture for 2D-to-3D human pose estimation: Graph Stacked Hourglass Networks. Specifically, we do not focus on specific graph convolution operations, but rather consider how to integrate them in the architecture which gives the best performance improvement. Given the advantages of the 2D human pose estimation approaches, proposed architecture adopts the repeated encoder-decoder applicable to the graph-structured data for multi-scale feature extraction, as well as the intermediate features at each depth of the model for multi-level feature extraction. Such multi-scale and multi-level feature information makes the model more expressive and enables the model to achieve high-precision 3D human pose estimation.</p><p>Our work makes the following contributions. First, we propose Graph Hourglass modules suitable for extracting multi-scale human skeletal features, which includes novel pooling and unpooling operations considering human skeletal structure, called Skeletal Pool and Skeletal Unpool. Second, we introduce Graph Stacked Hourglass Networks (GraphSH) consisting of the proposed Graph Hourglass module , which incorporates multi-level feature representations at various depths of the architecture. Our architecture incorporates multi-scale, multi-level features and a priori knowledge of the human skeleton, achieving impressive performance improvement for 2D-to-3D human pose estimation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D Human Pose Estimation. Predicting the 3D human pose from images or videos has been an essential topic in computer vision for a long time. In the early days, handcrafted features, perspective relationships, and geometric constraints were used to predict 3D human pose <ref type="bibr" target="#b43">[42,</ref><ref type="bibr" target="#b33">32,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b13">14]</ref>. In recent years, with the development of deep learning, there has been an increase in using deep neural networks for image-to-3D human pose estimation <ref type="bibr" target="#b44">[43,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b35">34,</ref><ref type="bibr" target="#b42">41,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>Some methods regress 3D human pose directly from images. Tekin et al. <ref type="bibr" target="#b44">[43]</ref> propose a method that first train an autoencoder to learn the latent representation of the 3D human pose, then use CNN to regress the image with the latent representation, and finally connect the trained CNN with the decoder to achieve the prediction from image to 3D human pose. Pavlakos et al. <ref type="bibr" target="#b35">[34]</ref> exploit voxel to discretize representations of the space around the human body and use 3D heatmaps to estimate 3D human pose.</p><p>There are also methods that break the problem down into two steps: first predicting 2D human joints from the image, and then using the 2D joints information to predict 3D human pose. Our approach falls into this category. Martinez et al. <ref type="bibr" target="#b27">[27]</ref> propose a simple yet effective baseline for 3D hu-man pose estimation that uses only 2D joints information but get highly accurate results, showing the importance of 2D joints information for 3D human pose estimation. Since the human skeleton's topology can be viewed as a graph structure, there has been increasing use of Graph Convolutional Networks (GCN) for 2D-to-3D human pose estimation tasks <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Graph Convolutional Networks. Graph Convolutional Networks (GCN) are used to perform convolution operations on graph-structured data, such as human skeleton, thus enabling effective feature extraction. The early simple GCN is the 'vanilla' GCN proposed by Kipf and Welling <ref type="bibr" target="#b16">[17]</ref>, which consists of a simple graph convolution operation that performs the transformation and aggregation of graphstructured data, and it becomes the basic model for various graph convolution later on. The following GCNs are based on this model with some improvements and are applied to 2D-to-3D human pose estimation. Zhao et al. <ref type="bibr" target="#b49">[48]</ref> propose Semantic Graph Convolution (SemGConv), which has learnable adjacency matrix parameters, enabling the model to learn the semantic relationships between the human joints. In the two graph convolutions just introduced, each node information is transformed using the same weight matrix and then aggregated. Liu et al. <ref type="bibr" target="#b22">[23]</ref> point out that sharing the same weight by all nodes limits the representation capabilities of graph convolution, and propose a new method that, first transforming each node information using different weights and then aggregating them together, called Pre-Aggregation Graph Convolution (PreAggr). They also introduce an approach that decouples the self-connections in the graph and use separate weight to compute the selfinformation transformation.</p><p>The GCN using the above graph convolution for 3D hu- As the skeletal scale decrease, we increase the number of channels (64 ? 96 ? 128 in our experiments). Each graph convolution layer is followed by Batch Normalization <ref type="bibr" target="#b12">[13]</ref> and ReLU activation <ref type="bibr" target="#b30">[29]</ref>. Residual connections are used between features at each scale. Note that the inputs and outputs of the hourglass module maintain the same shape. man pose estimation, however, only use a straightforward overall architecture, as in <ref type="figure" target="#fig_3">Fig. 4</ref> (Top). Such a simple architecture prevents the model from using the multi-scale, multi-level features common in image-based tasks, limiting the performance of the model.</p><p>Multi-scale and Multi-level Learning. Multi-scale and multi-level features learning drives advances in a wide variety of image-based tasks <ref type="bibr" target="#b38">[37,</ref><ref type="bibr" target="#b23">24]</ref>. Multi-scale feature learning refers to integrating features in different resolutions to provide a better understanding within the spatial domain. Feature Pyramid Network (FPN) <ref type="bibr" target="#b21">[22]</ref> is a powerful multi-scale feature extractor and achieves encouraging results in object detection tasks. In the task of 2D human pose estimation from image, Hourglass structure for extracting multi-scale features allows the model to learn both local and global features, which are essential for human pose understanding. (e.g., spatial configuration relationships between human joints) <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b41">40]</ref>. Multi-level feature learning, on the other hand, represents the use of features at various depths of the network. Some methods use a skip layer to incorporate features from the intermediate layer of the network and then combine them into the output layer <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">25]</ref>. Zhao et al. <ref type="bibr" target="#b50">[49]</ref> incorporate multi-scale and multi-level features, combining image pyramids of different depths to extract higher feature representations.</p><p>These image-based methods take advantage of the fact that images can be easily scaled up and down and the richness of intermediate features, thus enabling multi-scale and multi-level feature extraction. However, due to the graph structure's irregularity, it is not trivial to scale it up or down like an image, so such approaches have not been applied much to tasks with graph-structured data.</p><p>In the next section, we present our proposed novel graph convolutional network architecture that integrates multiscale and multi-level features of the graph-structured data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Graph Stacked Hourglass Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Hourglass Module</head><p>Our approach is inspired by Stacked Hourglass Networks proposed by Newell et al. <ref type="bibr" target="#b32">[31]</ref> for estimating 2D human pose from images, which exploits repeated hourglass-like encoder-decoder architecture. We aim to extend such an hourglass structure to the graph for extracting multi-scale features of the graph-structured data.</p><p>Using deep neural networks for computer vision tasks, multi-scale features of the image are essential for image understanding. Since images have large amounts of information at high resolution, the model can extract much detailed information from them. Alternatively, while the image is at low resolution, the model can better extract globalized information. The hourglass structure, accompanied by downsampling and upsampling operations, enables the image features to go through all resolutions so that crucial information can be extracted at all scales. Previous works have shown that this hourglass structure has strong feature extraction capabilities, especially for tasks requiring both local and global information, such as human pose estimation <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b31">30]</ref>. Moreover, stacking such a structure enables repeated feature extraction and enhances model performance.</p><p>Our motivation is to extend such a structure with powerful multi-scale feature extraction capabilities to graphstructured data to achieve highly accurate 2D-to-3D human pose estimation. In the hourglass structure, downsampling and upsampling of the data are implemented by pooling and unpooling operations, respectively. Such operations are easy to define on the images because of their regularized structure, but there is no consistent way to define them on the graph. On this point, there are some previous works regarding pooling and unpooling operations on the graph <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>. However, these pooling and unpooling operations are defined on the more generalized arbitrary-shaped  <ref type="figure">Figure 3</ref>. The overall architecture of our network. The input 2D joints are fed into the hourglass module after the pre-processed graph convolution layer, and the outputs of the hourglass module are both processed as intermediate features, and also fed into the subsequent hourglass modules, except the last one. All the intermediate features are concatenated and entered into the SE block <ref type="bibr" target="#b11">[12]</ref>, then the final feature is passed through a 1x1 convolutional layer to output the final 3D pose prediction. The number of the convolution module indicates the number of channels of the output. Our network takes a 4-stacking approach. The input and output of the hourglass module keep the dimension of 64 channels. Note that the graph structure is maintained throughout the whole network. graph structure, while the human skeletal graph used in this study has a fixed structure as shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>. Here, we exploit this property to propose pooling and unpooling methods applicable to the human skeletal graph, called Skeletal Pooling and Skeletal Unpooling.</p><p>Skeletal pooling. According to the property of the human body structure, we group the human body nodes in pairs, where the corresponding two nodes' features are fused into one node in the lower-scale skeleton structure, using max pooling operation. With repeated pooling operation, we can obtain three different scales of skeletal structures containing 16, 8, and 4 nodes, respectively, and each of them corresponds to a different graph structure. For relatively lower-scale graph representations, we use more channels to encode information to prevent information degradation. The illustration of skeletal pooling and the three-scales skeleton graph structures are shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>.</p><p>Skeletal unpooling. We use unpooling operation to restore lower-scale skeletal structures to their original size, enabling them to fuse higher-scale skeletal information and pass it to subsequent processing. We adopt a very simple approach: since lower-scale nodes are generated by two grouped higher-scale nodes, in unpooling operation, we duplicate the feature representations of the lower-scale node and assign them to corresponding two nodes to recover the higher-scale skeletal representations. The illustration of skeletal unpooling is shown in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>.</p><p>Graph hourglass design. Using the above skeletal pooling and unpooling operations, we propose a novel graph hourglass module applicable to human skeleton representation.The detailed hourglass structure is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. For the same scale of the skeletal structure, we applied the residual connections <ref type="bibr" target="#b10">[11]</ref> to pass information and prevent the vanishing gradient problem. Note that our hourglass structure does not depend on the specific graph convolu-tion layer, so arbitrary graph convolution operation can be implemented on our model, such as the three introduced in Sect. 2.</p><p>Graph U-Nets proposed by Gao et al. <ref type="bibr" target="#b8">[9]</ref> is the closest work to our architecture, but differs in two ways. First, <ref type="bibr" target="#b8">[9]</ref> uses an input-related dynamic pooling operation so that different pooled skeletal structures are obtained depending on the input. Our method utilizes a priori knowledge of human skeletal structure, and this approach is more suitable for feature extraction of specific skeletal structures while ensuring the stability of pooling. Second, we use more channels at relatively low scales of skeleton representation to reduce information loss due to scale changes and make our architecture more expressive, while Graph U-Nets uses the same number of channels at all scales.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-scale and Multi-level Features</head><p>Features are extracted at multi-scales as the hourglass module processes the information across three skeletal structures. As multi-scale features that can represent information on the spatial aspect of the graph, we believe that multi-level features in terms of the depth of latent space can also bring valuable information to the final prediction. Specifically, we integrate the intermediate features at each depth level of the network for the final 3D human pose estimation. As shown in <ref type="figure">Fig. 3</ref>, we use the spatial 1x1 convolution to reduce the channels of intermediate features and concatenate them into an overall feature representation f cat . For the architecture that stacks n hourglass modules, the overall feature can be represented as:</p><formula xml:id="formula_0">f cat = Concat(f 1 , f 2 , ? ? ? , f n ) ? R K?C , f i ? R K? C n .</formula><p>(1) Here K is the number of joints, and C represents the number of input and output channels of the hourglass module (C = 64 in our experiments). Then, we follow the Squeezeand-Excitation block (SE block) <ref type="bibr" target="#b11">[12]</ref> to enhance important semantic information in multi-level features. SE block computes the channel-wise weights of the overall feature, and since the overall feature is concatenated by multi-level features along the channel axis, this block enables the model to extract the more semantically meaningful feature representations among each intermediate feature. Specifically, first we use global average pooling to transform the overall feature f cat ? R K?C into a channel-wise statistics z ? R C ; then we use z to calculate the channel-wise dependency s as follows:</p><formula xml:id="formula_1">s = Sigmoid(W 2 ReLU(W 1 z)).<label>(2)</label></formula><p>Here f cat = f cat s,</p><formula xml:id="formula_2">W 1 ? R C r ?C , W 2 ? R C? C r ,</formula><p>The output feature of SE blockf cat is then fed to the output layer for final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Network Architecture</head><p>As in previous works <ref type="bibr" target="#b32">[31,</ref><ref type="bibr" target="#b31">30]</ref>, our backbone network consists of the proposed graph hourglass module stacked.</p><p>The input 2D joint information is first mapped to the latent feature space via a pre-processing graph convolution layer. The features that through the hourglass module are fed into the 1x1 convolution layer to be transformed into intermediate features, and also passed to the next hourglass module, except the last one. All the intermediate features are concatenated into the final feature, and the SE block adjusts its channel-wise weights, and which is then fed into the output convolution layer and mapped to the output space. Our overall network structure is shown in <ref type="figure">Fig. 3</ref>.</p><p>In the following experiments, our model uses PreAggr <ref type="bibr" target="#b22">[23]</ref> as the graph convolution layer, with 4stacking hourglass approaches and latent space of 64 channels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we first describe the experimental setup for 2D-to-3D human pose estimation tasks. Next, we introduce the dataset used and its evaluation protocols. Then several ablation studies are conducted regarding the proposed architecture. Finally, we show our experimental results and comparisons with state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">2D-to-3D human pose estimation</head><p>Our goal is to predict 3D joint positions in the camera coordinate system with given 2D joint positions in the pixel coordinate system. Specifically, 2D joints consisting of K nodes are described by x ? R K?2 , the corresponding 3D joints are y ? R K?3 . The model aims to learn a mapping f * : R K?2 ? R K?3 that minimizes the errors over a dataset containing N poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-level Features</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-scale Features</head><formula xml:id="formula_4">f * = argmin f 1 N N i=1 L (f (x i ), y i ) .<label>(4)</label></formula><p>In this study, Mean Squared Error (MSE) is used as a loss function L for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Datasets and Evaluation Protocols</head><p>Datasets. The Human3.6M dataset <ref type="bibr" target="#b14">[15]</ref> is the most widely used dataset in the 3D human pose estimation tasks. It uses motion captures to obtain the 3D pose information of the subjects and 4 cameras with different orientations to record the corresponding video image information. The provided camera parameters allow us to obtain the ground truth of the corresponding 2D joint coordinates in each image frame. The dataset provides 3.6 million images by recording 11 professional actors performing 15 different actions, such as eating, walking, etc. In the following experiments, we mainly use the Human3.6M for training and testing. The MPI-INF-3DHP test set <ref type="bibr" target="#b28">[28]</ref> provides images in three different scenarios: studio with a green screen (GS), studio without green screen (noGS) and outdoor scene (Outdoor). We use this dataset to test the generalization capabilities of our proposed architecture.</p><p>Evaluation protocols. For the Human3.6M, There are two evaluation protocols used in previous works <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b6">7]</ref>. Protocol #1 uses the Mean Per Joint Position Error (MPJPE) in millimeter as evaluation metric, which calculates the Euclidean distance error between the predic-Method params MPJPE (mm) gPool/gUnpool <ref type="bibr" target="#b8">[9]</ref> 3.70M 40. tion and the ground truth after the origin (pelvis) alignment. Protocol #2 aligns the prediction with the ground truth by rigid transformation and then calculates the error.</p><p>In this study, we use Protocol #1 to evaluate our approach since performance under both protocols is usually consistent and Protocol #1 is more appropriate for our experimental setup. For the MPI-INF-3DHP test set, we follow previous works <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b6">7]</ref> and use 3D-PCK and AUC as evaluation metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>Our implementation follows the settings of previous works <ref type="bibr" target="#b27">[27,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b22">23]</ref>. As introduced in <ref type="bibr" target="#b36">[35]</ref>, we normalize the coordinates of the 2d and 3d joints and align the root joint (pelvis) to the origin.</p><p>In our experiments, we use a 4-stacked hourglass architecture. Previous works typically use 128 as the number of channels <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23]</ref>, but due to the relative complexity of our model architecture, we use 64 channels to keep the number of parameters at the same scale as previous works. We use Adam <ref type="bibr" target="#b15">[16]</ref> as the optimizer with an initial learning rate set to 0.0001 and decay by 0.92 per 20,000 iterations. We use a mini-batch size of 256. Since the learning of graph-structured data is very prone to overfitting, we apply Dropout <ref type="bibr" target="#b40">[39]</ref> with a dropout probability of 0.25 to all graph convolutional layers within the hourglass module. The entire training follows an end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>Pooling and Unpooling. Pooling and unpooling layers play an important role in the hourglass module.</p><p>Due to the graph structure's irregularity, there is no consistent way to pool graph-structured data, so we compare the impact of different pooling methods on model perfor-  <ref type="table">Table 3</ref>. Comparison of Sequential Residual blocks (SeqRes) ( <ref type="figure" target="#fig_3">Fig. 4 (Top)</ref>) and Graph Stacked Hourglass (GraphSH) architectures on 2d-to-3d human pose estimation errors. Three different graph convolution operations are used.</p><p>mance. Here we compare the performance of three graph pooling operations: gPool <ref type="bibr" target="#b8">[9]</ref>, SAGPool <ref type="bibr" target="#b17">[18]</ref>, and our proposed Skeletal Pool. The first two pooling methods take the same idea: calculate the scores of each node by some operation, and keep the part of the node with the higher score. Such pooling methods are initially designed for more general graph pooling situations, with the benefit that pooling operations can be defined for different graph-structured inputs. However, since the human skeleton used in this study has a fixed graph structure, it does not take advantage of the benefits of these pooling approaches. Moreover, since these pooling methods are input-dependent, different subgraphs are generated based on different inputs, which not only introduces computational complexity but also makes it difficult for the model to learn valuable features stably.</p><p>Compared to the above pooling methods that require computation, our pooling approach is more like methods that focus on geometric information of graph structure (e.g., edges, nodes), such as mesh sampling <ref type="bibr" target="#b37">[36]</ref> or edge contraction <ref type="bibr" target="#b9">[10]</ref> in mesh convolution. Specifically, We follow the node grouping method in <ref type="bibr" target="#b49">[48]</ref> to perform pooling operations on paired nodes. In their work, the features lose their graph structure after pooling. We extend their grouping concept by designing three subgraph structures of the human body consisting of 16, 8, and 4 nodes, respectively. Such a pooling approach not only exploits the topology of the human skeleton, which is more interpretable relative to other pooling approaches, but also, due to its simplicity, greatly reduces the computational complexity of the pooling layer and improves the speed of training and inference. Moreover, the two pooling methods mentioned above completely discard some nodes' information, resulting in some degree of information loss. Instead, our proposed Skeletal Pool refers to the concept of image pooling, which performs a maximum pooling operation between two nodes, thus can summarize the information of both nodes.</p><p>Regarding unpooling, in Graph U-Net <ref type="bibr" target="#b8">[9]</ref>, the gUnpool operation assigns zero vectors to nodes that are not selected at the time of pooling, which loses much valuable infor-   We conduct experiments based on three pool/unpool settings, and the results are shown in <ref type="table" target="#tab_1">Table 1</ref>. We also add a comparison that removes all the pool/unpool layers to validate the importance of the proposed skeletal pool/unpool approaches. Note that we use ground truth 2D joints as input in this and the following ablation experiments to eliminate the influence of the 2D human pose detector.</p><p>Multi-scale and Multi-level Features. Multi-scale feature extraction is achieved by pooling and unpooling in the hourglass module that transforms features across three different scales. For comparison, we remove all pooling and unpooling layers in our architecture, which means that the features are always processed at the highest scale. The results in <ref type="table" target="#tab_1">Table 1</ref>   <ref type="table">Table 2</ref>.</p><p>Stacked Hourglass Architecture. To verify that our architecture has better performance than the simple Sequential Residual blocks (denoted as SeqRes) in previous works <ref type="bibr" target="#b49">[48,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b6">7]</ref>, we use Vanilla Graph Convolution, Semantic Graph Convolution (SemGConv), and Pre-aggregation Graph Convolution (PreAggr) introduced in Sect. 2 as convolution layers in our GraphSH architecture, respectively, and compare them to the corresponding SeqRes models. To make the comparison fair, we reduce the number of channels in the convolution to 64, so that the overall number of parameters is at the same scale as the SeqRes model. Results are shown in <ref type="table">Table 3</ref>.</p><p>The results show that the model using our architecture performs better even though we use fewer channels than other GCN-based approaches. Our architecture does not rely on specific graph convolution layers, which suggests</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Prediction</head><p>Ground Truth Input Prediction Ground Truth <ref type="figure">Figure 5</ref>. Qualitative results of our method on Human3.6M <ref type="bibr" target="#b14">[15]</ref>.</p><p>that any graph convolution layers for 3D human pose estimation can be applied to our architecture and improve its performance compared to the SeqRes architecture. Moreover, our hourglass module can be seen as a wellintegrated, high-performance graph convolution module, indicating that our hourglass module is general and can be easily extended to other tasks using graph convolution, such as action recognition <ref type="bibr" target="#b47">[46,</ref><ref type="bibr" target="#b19">20]</ref>, motion prediction <ref type="bibr" target="#b20">[21]</ref>, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Comparison with the State-of-the-Art</head><p>We use two types of 2D joint detection data for evaluation: Cascaded Pyramid Network (CPN) <ref type="bibr" target="#b5">[6]</ref> detections and ground truth 2D keypoints, the results are shown in <ref type="table" target="#tab_4">Table 4 and Table 5</ref>, respectively. Among the other methods, some use temporal information <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">35,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b46">45]</ref>, some use additional data for training <ref type="bibr" target="#b42">[41,</ref><ref type="bibr" target="#b48">47,</ref><ref type="bibr" target="#b34">33]</ref>, and some use 3D pose scale in both training and testing <ref type="bibr" target="#b6">[7]</ref>. These results suggest that our approach outperforms the state-of-the-art. <ref type="table" target="#tab_5">Table 5</ref> show that when given precise 2D joint information, the performance improvement of our model is significant, outperforming other GCN-based methods by a large margin. Therefore, we believe that in combination with methods that refine detected 2D joint information, such as deep kinematic analysis <ref type="bibr" target="#b46">[45]</ref>, the performance of our method on noisy 2D joints can be enhanced even more. Compared to the second-place method <ref type="bibr" target="#b22">[23]</ref> (4.38M), our model uses fewer parameters (3.70M), showing that our architecture can extract more important features, which provides a better understanding of the human pose.</p><p>To evaluate the generalization capabilities of our ap-  <ref type="table">Table 6</ref>. Results on the MPI-INF-3DHP test set <ref type="bibr" target="#b28">[28]</ref>.</p><p>proach to domain shift, we apply the model trained on the Human3.6M to the MPI-INF-3DHP test set. Results are shown in <ref type="table">Table 6</ref>. Although we train the model using only the Human3.6M, our approach outperforms the others, indicating that our architecture has strong generalization capabilities to unseen datasets. The qualitative results of our method are shown in <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We present a novel architecture for 2D-to-3D human pose estimation, the Graph Stacked Hourglass Networks (GraphSH). With our unique skeletal pooling and skeletal unpooling scheme, together with the proposed architecture which has powerful multi-scale and multi-level feature extraction capabilities on graph-structured data, our method achieves accurate 2D-to-3D human pose estimation outperforming the state-of-the-art. As future work, we hope to introduce temporal multi-frame features in our architecture for further improvement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Description of the skeletal structure of the human body. (a) The human skeletal graph structure consisting of 16 joints used in this paper. (b) Illustration of Skeletal Pool. A pair of joints composed of the same color, mapped by the max pooling operation to corresponding joints in lower scale skeletal structure. Here we define three different scales of skeletal structures, containing 16, 8, and 4 joints, respectively. (c) Illustration of Skeletal Unpool. When reverting to a higher scale skeletal structure, we duplicate the features of the lower scale joints and assign them to the two corresponding joints in the higher scale skeletal structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Illustration of the Graph Hourglass Module. The skeletal scale is reduced by graph convolution and skeletal pooling, upsampled by skeletal unpooling after reaching the lowest scale, and finally restored to the original skeletal scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and r indicates reduction ratio (r = 8 in our experiments). Finally, the features of each channel are weighted by s as follows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Comparison of the model architecture. Top: Most used Sequential Residual blocks architecture (SeqRes) for GCN-based 2D-to-3D human pose estimation [23, 48, 7]. The rectangles represent the graph convolution layers. Bottom: Proposed Graph Stacked Hourglass Networks architecture. Multi-scale, multi-level features are indicated in the figure. Refer to Fig. 3 for detailed architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Ablation study of pool/unpool operation. Various methods of graph pool/unpool are used, and the method that remove the pool/unpool layer is also added for comparison.</figDesc><table><row><cell></cell><cell></cell><cell>6</cell></row><row><cell cols="2">SAGPool [18]/gUnpool [9] 3.70M</cell><cell>41.5</cell></row><row><cell>w/o Pool/Unpool</cell><cell>6.86M</cell><cell>38.2</cell></row><row><cell>Skeletal Pool/Unpool</cell><cell>3.70M</cell><cell>35.8</cell></row><row><cell>Method</cell><cell cols="2">params MPJPE (mm)</cell></row><row><cell>No ML-F</cell><cell>3.70M</cell><cell>38.0</cell></row><row><cell>ML-F w/o SE block</cell><cell>3.70M</cell><cell>37.1</cell></row><row><cell cols="2">ML-F w/ SE block (Ours) 3.70M</cell><cell>35.8</cell></row><row><cell cols="3">Table 2. Ablation study of multi-level features (ML-F). With al-</cell></row><row><cell cols="3">most the same number of parameters, our proposed approach using</cell></row><row><cell cols="3">multi-level features with SE block achieves the best results.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Direct. Discuss Eating Greet Phone Photo Pose Purch. Sitting SittingD. Smoke Wait WalkD. Walk WalkT. Avg. Lee et al. [19] ECCV'18 ( ?)</figDesc><table><row><cell>Protocol #1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40.2</cell><cell>49.2</cell><cell>47.8</cell><cell>52.6</cell><cell>50.1</cell><cell>75.0 50.2</cell><cell>43.0</cell><cell>55.8</cell><cell>73.9</cell><cell>54.1 55.6</cell><cell>58.2</cell><cell>43.3</cell><cell>43.3 52.8</cell></row><row><cell>Pavllo et al. [35] CVPR'19 ( ?)</cell><cell>45.2</cell><cell>46.7</cell><cell>43.3</cell><cell>45.6</cell><cell>48.1</cell><cell>55.1 44.6</cell><cell>44.3</cell><cell>57.3</cell><cell>65.8</cell><cell>47.1 44.0</cell><cell>49.0</cell><cell>32.8</cell><cell>33.9 46.8</cell></row><row><cell>Cai et al. [4] ICCV'19 ( ?)</cell><cell>44.6</cell><cell>47.4</cell><cell>45.6</cell><cell>48.8</cell><cell>50.8</cell><cell>59.0 47.2</cell><cell>43.9</cell><cell>57.9</cell><cell>61.9</cell><cell>49.7 46.6</cell><cell>51.3</cell><cell>37.1</cell><cell>39.4 48.8</cell></row><row><cell>Xu et al. [45] CVPR'20 ( ?)</cell><cell>37.4</cell><cell>43.5</cell><cell>42.7</cell><cell>42.7</cell><cell>46.6</cell><cell>59.7 41.3</cell><cell>45.1</cell><cell>52.7</cell><cell>60.2</cell><cell>45.8 43.1</cell><cell>47.7</cell><cell>33.7</cell><cell>37.1 45.6</cell></row><row><cell>Martinez et al. [27] ICCV'17</cell><cell>51.8</cell><cell>56.2</cell><cell>58.1</cell><cell>59.0</cell><cell>69.5</cell><cell>78.4 55.2</cell><cell>58.1</cell><cell>74.0</cell><cell>94.6</cell><cell>62.3 59.1</cell><cell>65.1</cell><cell>49.5</cell><cell>52.4 62.9</cell></row><row><cell>Tekin et al. [44] ICCV'17</cell><cell>54.2</cell><cell>61.4</cell><cell>60.2</cell><cell>61.2</cell><cell>79.4</cell><cell>78.3 63.1</cell><cell>81.6</cell><cell>70.1</cell><cell>107.3</cell><cell>69.3 70.3</cell><cell>74.3</cell><cell>51.8</cell><cell>63.2 69.7</cell></row><row><cell>Sun et al. [41] ICCV'17 (+)</cell><cell>52.8</cell><cell>54.8</cell><cell>54.2</cell><cell>54.3</cell><cell>61.8</cell><cell>67.2 53.1</cell><cell>53.6</cell><cell>71.7</cell><cell>86.7</cell><cell>61.5 53.4</cell><cell>61.6</cell><cell>47.1</cell><cell>53.4 59.1</cell></row><row><cell>Yang et al. [47] CVPR'18 (+)</cell><cell>51.5</cell><cell>58.9</cell><cell>50.4</cell><cell>57.0</cell><cell>62.1</cell><cell>65.4 49.8</cell><cell>52.7</cell><cell>69.2</cell><cell>85.2</cell><cell>57.4 58.4</cell><cell>43.6</cell><cell>60.1</cell><cell>47.7 58.6</cell></row><row><cell>Fang et al. [8] AAAI'18</cell><cell>50.1</cell><cell>54.3</cell><cell>57.0</cell><cell>57.1</cell><cell>66.6</cell><cell>73.3 53.4</cell><cell>55.7</cell><cell>72.8</cell><cell>88.6</cell><cell>60.3 57.7</cell><cell>62.7</cell><cell>47.5</cell><cell>50.6 60.4</cell></row><row><cell>Pavlakos et al. [33] CVPR'18 (+)</cell><cell>48.5</cell><cell>54.4</cell><cell>54.5</cell><cell>52.0</cell><cell>59.4</cell><cell>65.3 49.9</cell><cell>52.9</cell><cell>65.8</cell><cell>71.1</cell><cell>56.6 52.9</cell><cell>60.9</cell><cell>44.7</cell><cell>47.8 56.2</cell></row><row><cell>Zhao et al. [48] CVPR'19</cell><cell>48.2</cell><cell>60.8</cell><cell>51.8</cell><cell>64.0</cell><cell>64.6</cell><cell>53.6 51.1</cell><cell>67.4</cell><cell>88.7</cell><cell>57.7</cell><cell>73.2 65.6</cell><cell>48.9</cell><cell>64.8</cell><cell>51.9 60.8</cell></row><row><cell>Sharma et al. [38] ICCV'19</cell><cell>48.6</cell><cell>54.5</cell><cell>54.2</cell><cell>55.7</cell><cell>62.2</cell><cell>72.0 50.5</cell><cell>54.3</cell><cell>70.0</cell><cell>78.3</cell><cell>58.1 55.4</cell><cell>61.4</cell><cell>45.2</cell><cell>49.7 58.0</cell></row><row><cell>Ci et al. [7] ICCV'19 (+)( * )</cell><cell>46.8</cell><cell>52.3</cell><cell>44.7</cell><cell>50.4</cell><cell>52.9</cell><cell>68.9 49.6</cell><cell>46.4</cell><cell>60.2</cell><cell>78.9</cell><cell>51.2 50.0</cell><cell>54.8</cell><cell>40.4</cell><cell>43.3 52.7</cell></row><row><cell>Liu et al. [23] ECCV'20</cell><cell>46.3</cell><cell>52.2</cell><cell>47.3</cell><cell>50.7</cell><cell>55.5</cell><cell>67.1 49.2</cell><cell>46.0</cell><cell>60.4</cell><cell>71.1</cell><cell>51.5 50.1</cell><cell>54.5</cell><cell>40.3</cell><cell>43.7 52.4</cell></row><row><cell>Ours</cell><cell>45.2</cell><cell>49.9</cell><cell>47.5</cell><cell>50.9</cell><cell>54.9</cell><cell>66.1 48.5</cell><cell>46.3</cell><cell>59.7</cell><cell>71.5</cell><cell>51.4 48.6</cell><cell>53.9</cell><cell>39.9</cell><cell>44.1 51.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Quantitative evaluation results using MPJPE in millimeter on Human3.6M<ref type="bibr" target="#b14">[15]</ref> under Protocol #1, no rigid alignment or transform applied in post-processing. CPN<ref type="bibr" target="#b5">[6]</ref> detections 2D keypoints are used as input. (+) uses extra data from MPII<ref type="bibr" target="#b1">[2]</ref>. ( ?) uses temporal information. Best in bold.</figDesc><table><row><cell>Protocol #1</cell><cell cols="13">Direct. Discuss Eating Greet Phone Photo Pose Purch. Sitting SittingD. Smoke Wait WalkD. Walk WalkT. Avg.</cell></row><row><cell>Zhou et al. [50] ICCV'19 (+)</cell><cell>34.4</cell><cell>42.4</cell><cell>36.6</cell><cell>42.1</cell><cell>38.2</cell><cell>39.8 34.7</cell><cell>40.2</cell><cell>45.6</cell><cell>60.8</cell><cell>39.0 42.6</cell><cell>42.0</cell><cell>29.8</cell><cell>31.7 39.9</cell></row><row><cell>Ci et al. [7] ICCV'19 (+)( * )</cell><cell>36.3</cell><cell>38.8</cell><cell>29.7</cell><cell>37.8</cell><cell>34.6</cell><cell>42.5 39.8</cell><cell>32.5</cell><cell>36.2</cell><cell>39.5</cell><cell>34.4 38.4</cell><cell>38.2</cell><cell>31.3</cell><cell>34.2 36.3</cell></row><row><cell>Martinez et al. [27] ICCV'17</cell><cell>45.2</cell><cell>46.7</cell><cell>43.3</cell><cell>45.6</cell><cell>48.1</cell><cell>55.1 44.6</cell><cell>44.3</cell><cell>57.3</cell><cell>65.8</cell><cell>47.1 44.0</cell><cell>49.0</cell><cell>32.8</cell><cell>33.9 46.8</cell></row><row><cell>Zhao et al. [48] CVPR'19</cell><cell>37.8</cell><cell>49.4</cell><cell>37.6</cell><cell>40.9</cell><cell>45.1</cell><cell>41.4 40.1</cell><cell>48.3</cell><cell>50.1</cell><cell>42.2</cell><cell>53.5 44.3</cell><cell>40.5</cell><cell>47.3</cell><cell>39.0 43.8</cell></row><row><cell>Liu et al. [23] ECCV'20</cell><cell>36.8</cell><cell>40.3</cell><cell>33.0</cell><cell>36.3</cell><cell>37.5</cell><cell>45.0 39.7</cell><cell>34.9</cell><cell>40.3</cell><cell>47.7</cell><cell>37.4 38.5</cell><cell>38.6</cell><cell>29.6</cell><cell>32.0 37.8</cell></row><row><cell>Ours</cell><cell>35.8</cell><cell>38.1</cell><cell>31.0</cell><cell>35.3</cell><cell>35.8</cell><cell>43.2 37.3</cell><cell>31.7</cell><cell>38.4</cell><cell>45.5</cell><cell>35.4 36.7</cell><cell>36.8</cell><cell>27.9</cell><cell>30.7 35.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Quantitative evaluation results using MPJPE in millimeter on Human3.6M [15] under Protocol #1, no rigid alignment or transform applied in post-processing. Ground truth 2D keypoints are used as input. (+) uses extra data from MPII [2]. ( * ) uses pose scales in both training and testing. Best in bold.mation and results in more sparse features. Experimental results show that such an unpooling operation is not suitable for understanding structures with few nodes like the human skeleton. On the contrary, our proposed Skeletal Unpool operation copies and assigns the node features in the lower-scale graph representation to the corresponding two nodes in the higher-scale graph, passes them into the following graph convolutional layer, makes our model have better representation capability.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>show that multi-scale features derived from skeletal pooling and unpooling can improve model performance. For multi-level features, our architecture concate-nates each level's intermediate features and feeds them into the SE block to obtain the final overall feature. We compare three different settings: (1) No multi-level features are used. At this point the model simply connect all the hourglass modules sequentially, and the last hourglass modules is connected to the final output layer of 1x1 convolution. (2) Remove the SE block that calculates the weights of each intermediate feature. (3) Our proposed GraphSH architecture. Results are shown in</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recovering 3d human pose from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="58" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3686" to="3693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deepedge: A multiscale bifurcated deep network for top-down contour detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4380" to="4389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Jen</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadia</forename><forename type="middle">Magnenat</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="354" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimizing network structure for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning pose grammar to encode human body configuration for 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename><surname>Hao-Shu Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanlu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Surface simplification using quadric error metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Garland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">S</forename><surname>Heckbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th Annual Conference on Computer Graphics and Interactive Techniques</title>
		<meeting>the 24th Annual Conference on Computer Graphics and Interactive Techniques</meeting>
		<imprint>
			<publisher>ACM Press/Addison-Wesley Publishing Co</publisher>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kaiming He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR Workshop and Conference Proceedings</title>
		<editor>Francis R. Bach and David M. Blei</editor>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="448" to="456" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Latent structured models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2220" to="2227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catalin</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragos</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vlad</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Conference on Learning Representations, ICLR &apos;17</title>
		<meeting>the 5th International Conference on Learning Representations, ICLR &apos;17</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-attention graph pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inyeop</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Propagating lstm: 3d pose estimation based on joint interdependency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Actional-structural graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Densely connected gcn model for motion prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan-Ran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingteng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangde</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaosong</forename><surname>Sebastian Iulian Poiana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Animation and Virtual Worlds</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A Comprehensive Study of Weight Sharing in Graph Networks for 3D Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenkun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongqi</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiming</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">SSD: single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference, Amsterdam</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9905</biblScope>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Orinet: A fully convolutional network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxu</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rayat</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2659" to="2668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dushyant</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weipeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV</title>
		<imprint>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<title level="m">Fifth International Conference on</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference, Amsterdam</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<meeting><address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9912</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part VIII</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video Motion Capture from the Part Confidence Maps of Multi-Camera Images by Spatiotemporal Filtering Using the Human Skeletal Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takuya</forename><surname>Ohashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosuke</forename><surname>Ikegami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Yamamoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wataru</forename><surname>Takano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Konstantinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kostas</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semi-supervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generating 3D faces using convolutional mesh autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Bolkart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soubhik</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">11207</biblScope>
			<biblScope unit="page" from="725" to="741" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by generation and ordinal ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Varigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2325" to="2334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5693" to="5703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2621" to="2630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Action database for categorizing and inferring human poses from video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wataru</forename><surname>Takano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshihiko</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="116" to="125" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Structured prediction of 3d human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isinsu</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<editor>Richard C. Wilson, Edwin R. Hancock, and William A. P. Smith</editor>
		<meeting>the British Machine Vision Conference<address><addrLine>York, UK</addrLine></address></meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2016-09-19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Learning to fuse 2d and 3d image cues for monocular body pose estimation. ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Bugra Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Marquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Deep kinematics analysis for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenbo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiancheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaokang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Spatial temporal graph convolutional networks for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sijie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Semantic graph convolutional networks for 3d human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubbasir</forename><surname>Kapadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitris</forename><forename type="middle">N</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">M2det: A single-shot object detector based on multi-level feature pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongtao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibing</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third AAAI Conference on Artificial Intelligence,AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Hemlets pose: Learning part-centric heatmap triplets for accurate 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoguang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2344" to="2353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: A weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
