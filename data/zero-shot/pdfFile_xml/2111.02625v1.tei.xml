<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Qimera: Data-free Quantization with Synthetic Boundary Supporting Samples</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kanghyun</forename><surname>Choi</surname></persName>
							<email>kanghyun.choi@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deokki</forename><surname>Hong</surname></persName>
							<email>dk.hong@yonsei.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Artificial Intelligence</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noseong</forename><surname>Park</surname></persName>
							<email>noseong@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Artificial Intelligence</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngsok</forename><surname>Kim</surname></persName>
							<email>youngsok@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Artificial Intelligence</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinho</forename><surname>Lee</surname></persName>
							<email>leejinho@yonsei.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Artificial Intelligence</orgName>
								<orgName type="institution">Yonsei University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Qimera: Data-free Quantization with Synthetic Boundary Supporting Samples</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Accepted to NeurIPS 2021. Author&apos;s copy</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Model quantization is known as a promising method to compress deep neural networks, especially for inferences on lightweight mobile or edge devices. However, model quantization usually requires access to the original training data to maintain the accuracy of the full-precision models, which is often infeasible in real-world scenarios for security and privacy issues. A popular approach to perform quantization without access to the original data is to use synthetically generated samples, based on batch-normalization statistics or adversarial learning. However, the drawback of such approaches is that they primarily rely on random noise input to the generator to attain diversity of the synthetic samples. We find that this is often insufficient to capture the distribution of the original data, especially around the decision boundaries. To this end, we propose Qimera, a method that uses superposed latent embeddings to generate synthetic boundary supporting samples. For the superposed embeddings to better reflect the original distribution, we also propose using an additional disentanglement mapping layer and extracting information from the full-precision model. The experimental results show that Qimera achieves state-of-the-art performances for various settings on data-free quantization. Code is available at https://github.com/iamkanghyunchoi/qimera.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Among many neural network compression methodologies, quantization is considered a promising direction because it can be easily supported by accelerator hardwares <ref type="bibr" target="#b0">[1]</ref> than pruning <ref type="bibr" target="#b1">[2]</ref> and is more lightweight than knowledge distillation <ref type="bibr" target="#b2">[3]</ref>. However, quantization generally requires some form of adjustment (e.g., fine-tuning) using the original training data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref> to restore the accuracy drop due to the quantization errors. Unfortunately, access to the original training data is not always possible, especially for deployment in the field, for many reasons such as privacy and security. For example, the data could be medical images of patients, photos of confidential products, or pictures of military assets.</p><p>Therefore, data-free quantization is a natural direction to achieve a highly accurate quantized model without accessing any training data. Among many excellent prior studies <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>, generative methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref> have recently been drawing much attention due to their superior performance. Generative methods successfully generate synthetic samples that resemble the distribution of the original dataset and achieve high accuracy using information from the pretrained full-precision network, such as batch-normalization statistics <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b12">13]</ref> or intermediate features <ref type="bibr" target="#b13">[14]</ref>.</p><p>However, a significant gap still exists between data-free quantized models and quantized models finetuned with original data. What is missing from the current generative data-free quantization schemes? We hypothesize that the synthetic samples of conventional methods lack boundary supporting samples <ref type="bibr" target="#b15">[16]</ref>, which lie on or near the decision boundary of the full-precision model and directly affect the model performance. The generator designs are often based on conditional generative adversarial networks (CGANs) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> that take class embeddings representing class-specific latent features. Based on these embeddings as the centroid of each class distribution, generators rely on the input of random Gaussian noise vectors to gain diverse samples. However, one can easily deduce that random noises have difficulty reflecting the complex class boundaries. In addition, the weights and embeddings of the generators are trained with cross-entropy (CE) loss, further ensuring that these samples are well-separated from each other.</p><p>In this work, we propose Qimera, a method for data-free quantization employing superposed latent embeddings to create boundary supporting samples. First, we conduct a motivational experiment to confirm our hypothesis that samples near the boundary can improve the quantized model performance. Then, we propose a novel method based on inputting superposed latent embeddings into the generator to produce synthetic boundary supporting samples. In addition, we provide two auxiliary schemes for flattening the latent embedding space so that superposed embeddings could contain adequate features.</p><p>Qimera achieves significant performance improvement over the existing techniques. The experimental results indicate that Qimera sets new state-of-the-art performance for various datasets and model settings for the data-free quantization problem. Our contributions are summarized as the following:</p><p>? We identify that boundary supporting samples form an important missing piece of the current state-of-the-art data-free compression. ? We propose using superposed latent embeddings, which enables a generator to synthesize boundary supporting samples of the full-precision model. ? We propose disentanglement mapping and extracted embedding initialization that help train a better set of embeddings for the generator. ? We conduct an extensive set of experiments, showing that the proposed scheme outperforms the existing methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Data-free Compression</head><p>Early work on data-free compression has been led by knowledge distillation <ref type="bibr" target="#b2">[3]</ref>, which usually involves pseudo-data created from teacher network statistics <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>. Lopes et al. <ref type="bibr" target="#b18">[19]</ref> suggested generating pseudo-data from metadata collected from the teacher in the form of activation records. Nayak et al. <ref type="bibr" target="#b19">[20]</ref> proposed a similar scheme but with a zero-shot approach by modeling the output space of the teacher model as a Dirichlet distribution, which is taken from model weights. More recent studies have employed generator architectures similar to GAN <ref type="bibr" target="#b20">[21]</ref> to generate synthetic samples replacing the original data <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. In the absence of the original data for training, DAFL <ref type="bibr" target="#b21">[22]</ref> used the teacher model to replace the discriminator by encouraging the outputs to be close to a one-hot distribution and by maximizing the activation counts. KegNet [26] adopted a similar idea and used a low-rank decomposition to aid the compression. Adversarial belief matching <ref type="bibr" target="#b22">[23]</ref> and data-free adversarial distillation <ref type="bibr" target="#b23">[24]</ref> methods suggested adversarially training the generator, such that the generated samples become harder to train. One other variant is to modify samples directly using logit maximization as in DeepInversion <ref type="bibr" target="#b24">[25]</ref>. While this approach can generate images that appear natural to a human, it has the drawback of skyrocketing computational costs because each image must be modified using backpropagation.</p><p>Data-free quantization is similar to data-free knowledge distillation but is a more complex problem because quantization errors must be recovered. The quantized model has the same architecture as the full-precision model; thus, the early methods of post-training quantization were focused on how to convert the full-precision weights into quantized weights by limiting the range of activations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>, correcting biases <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, and equalizing the weights <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12]</ref>. ZeroQ <ref type="bibr" target="#b14">[15]</ref>   <ref type="bibr" target="#b12">[13]</ref> added a generator close to the ACGAN <ref type="bibr" target="#b17">[18]</ref> to generate better synthetic samples, and AutoReCon <ref type="bibr" target="#b26">[27]</ref> suggests a better performing generator found by neural architecture search. In addition, DSG <ref type="bibr" target="#b27">[28]</ref> further suggests relaxing the batch-normalization statistics alignment to generate more diverse samples. ZAQ <ref type="bibr" target="#b13">[14]</ref> adopted adversarial training of the generator on the quantization problem and introduced intermediate feature matching between the full-precision and quantized model. However, none of these considered aiming to synthesize boundary supporting samples of the full-precision model. Adversarial training of generators have a similar purpose, but it is fundamentally different from generating boundary supporting samples. In addition, adversarial training can be sensitive to hyperparameters and risks generating samples outside of the original distributions <ref type="bibr" target="#b28">[29]</ref>.</p><p>To the extent of our knowledge, this work is the first to propose using superposed latent embeddings to generate boundary supporting samples explicitly for data-free quantizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Boundary Supporting Samples</head><p>In the context of knowledge distillation, boundary supporting samples <ref type="bibr" target="#b15">[16]</ref> are defined as samples that lie near the decision boundary of the teacher models. As these samples contain classification information about the teacher models, they can help the student model correctly mimic the teacher's behavior. Heo et al. <ref type="bibr" target="#b15">[16]</ref> applied an adversarial attack <ref type="bibr" target="#b29">[30]</ref> to generate boundary supporting samples and successfully demonstrated that they improve knowledge distillation performance. In AMKD <ref type="bibr" target="#b30">[31]</ref>, triplet loss was used to aid the student in drawing a better boundary. Later, DeepDig <ref type="bibr" target="#b31">[32]</ref> devised a refined method for generating boundary supporting samples and analyzed their characteristics by defining new metrics. Although boundary supporting samples have been successful for many problems, such as domain adaptation <ref type="bibr" target="#b32">[33]</ref> and open-set recognition <ref type="bibr" target="#b33">[34]</ref>, they have not yet been considered for data-free compression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Motivational Experiment</head><p>To explain the discrepancy between the accuracy of the model fine-tuned with the original training data and the data-free quantized models, we hypothesized that the synthetic samples from generative data-free quantization methods lack samples near the decision boundary. To validate this hypothesis, we designed an experiment using the CIFAR-100 <ref type="bibr" target="#b34">[35]</ref> dataset with the ResNet-20 network <ref type="bibr" target="#b35">[36]</ref>.</p><p>First, we forwarded images in the dataset into the pre-trained full-precision ResNet-20. Among these, we selected 1500 samples (3% of the dataset, 15 samples per class) from samples where the highest confidence value was lower than 0.25, forming a group of 'confusing' samples. Then, we combined the confusing samples with synthetic samples generated from a previous study <ref type="bibr" target="#b12">[13]</ref> to fine-tune the quantized model with 4-bit weights and 4-bit activations. We also selected as a control group an equal number of random samples from the images classified as unconfusing and fine-tuned the quantized model using the same method.</p><p>The results are presented in <ref type="table">Table 1</ref>. The quantized model with synthetic + unconfusing real samples exhibited only 0.52%p increase in the accuracy from the baseline. In contrast, adding confusing samples provided 2.36%p improvement, filling almost half the gap towards the quantized model fine-tuned with the original data. These results indirectly validate that the synthetic samples suffer from a lack of confusing samples (i.e., boundary supporting samples). We aim to address the issue in this paper. As we indicate in Section 5, Qimera achieves a 1.71%p performance gain for the same model in a data-free setting, close to that of the addition of confusing synthetic samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full precision model boundary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quantized model boundary</head><p>Synthetic boundary supporting samples (a) Synthetic Images generated with Gaussian noise.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full precision model boundary</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Generating Boundary Supporting Samples with Superposed Latent Embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baseline Generative Data-free Quantization</head><p>Recent generative data-free quantization schemes <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> employ a GAN-like generator to create synthetic samples. In the absence of the original training samples, the generator G attempts to generate synthetic samples so that the quantized model Q can mimic the behavior of the full-precision model P . For example, in GDFQ <ref type="bibr" target="#b12">[13]</ref>, the loss function L GDF Q is</p><formula xml:id="formula_0">L GDF Q (G) = L P CE (G) + ?L P BN S (G),<label>(1)</label></formula><p>where the first term L CE guides the generator to output clearly classifiable samples, and the second term L BN S aligns the batch-normalization statistics of the synthetic samples with those of the batch-normalization layers in the full-precision model. In another previous work ZAQ <ref type="bibr" target="#b13">[14]</ref>,</p><formula xml:id="formula_1">L ZAQ (G) = L P,Q o (G) + ?L P,Q f (G) + ?L P a (G),<label>(2)</label></formula><p>where the first term L o separates the prediction outputs of P and Q, and the second term L f separates the feature maps produced by P and Q. These two losses let the generator be adversarially trained and allow it to determine samples where P cannot mimic Q adequately. Lastly, the third term L a maximizes the activation map values of P so that the created samples do not drift too far away from the original dataset.</p><p>The quantized model Q is usually jointly trained with G, such that</p><formula xml:id="formula_2">L GDF Q (Q) = L Q CE (Q) + ?L P KD (Q),<label>(3)</label></formula><p>for GDFQ, and</p><formula xml:id="formula_3">L ZAQ (Q) = ?L P,Q o (G) ? ?L P,Q f (G),<label>(4)</label></formula><p>for ZAQ, respectively, where L KD from Eq. 3 is the usual KD loss with Kullback-Leibler divergence.</p><p>While these two methods exhibit great performance, they both model the distribution of per-class embeddings in the latent input space as a Gaussian distribution, and generate diverse samples using random Gaussian noise inputs. However, based on the Gaussian distribution, it is difficult to correctly reflect the boundary between the two different classes, especially when all samples have one-hot labels. <ref type="figure" target="#fig_1">Figure 1</ref> visualizes such problems in a simplified two-dimensional space. With samples generated from gaussian noise <ref type="figure" target="#fig_1">(Figure 1a</ref>), the two per-class distributions are far away, and a void exists between the two classes (see <ref type="figure">Figure 3b</ref> for plots from experimental data). This can cause the decision boundaries to be formed at nonideal regions. One alternative is to use noise with higher variance, but the samples would overlap on a large region, resulting in too many samples with incorrect labels.</p><p>Furthermore, in the above approaches, the loss terms of Eqs. 1 and 2 such as L CE and L o encourage the class distributions to be separated from each other. While this is beneficial for generating clean, well-classified samples, it also prevents generating samples near the decision boundary, which is necessary for training a good model <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">31]</ref>. Therefore, in this paper, we focus on methods to generate synthetic samples near the boundary from the full-precision model P <ref type="figure" target="#fig_1">(Figure 1b</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Superposed Latent Embeddings</head><p>The overview of Qimera is presented in <ref type="figure" target="#fig_2">Figure 2</ref>. Often, generators use learned embeddings to create samples of multiple classes <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">37]</ref>. Given an input one-hot label y representing one of C classes and a random noise vector z, a generator G uses an embedding layer E ? R D?C to create a synthetic samplex:x</p><formula xml:id="formula_4">= G(z + E y ), z ? N (0, 1).<label>(5)</label></formula><p>To create boundary supporting samples, we superpose the class embeddings so that the generated samples have features lying near the decision boundaries of P . With two embeddings superposed, new synthetic samplex become?</p><formula xml:id="formula_5">x = G(z + (?E y1 + (1 ? ?)E y2 )), z ? N (0, 1), 0 ? ? ? 1.<label>(6)</label></formula><p>To avoid too many confusing samples from complicating the feature space, we also apply soft labels in the same manner as a regularizer:</p><formula xml:id="formula_6">y = ?y 1 + (1 ? ?)y 2 , 0 ? ? ? 1.<label>(7)</label></formula><p>Generalizing to K embeddings, Eqs. 6 and 7 become</p><formula xml:id="formula_7">(x ,? ) = G S(e) , K k ? k y k , S(e) = z + K k ? k e k , z ? N (0, 1),<label>(8)</label></formula><formula xml:id="formula_8">? i = Sof tmax(p i ) = exp(pi) / K k (exp(p k )), p i ? N (0, 1).<label>(9)</label></formula><p>where e ? R D?K has K embeddings from E as the column vectors (i.e., e = [E y0 , . . . , E y K?1 ]), and S : R D?K ? R D is a superposer function. Applying this to existing methods is straightforward and incurs only a small amount of computational overhead. Similar to knowledge distillation with boundary supporting samples <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">31]</ref>, the superposed embeddings are supposed to help transfer the decision boundary of the full precision (teacher) model to the quantized (student) model.</p><p>Although the superposed embedding scheme alone produces a substantial amount of performance gains, the generator embedding space is often not flat enough <ref type="bibr" target="#b37">[38]</ref>; therefore, linearly interpolating them can result in unnatural samples <ref type="bibr" target="#b38">[39]</ref>. To mitigate this, the embedding space used in Qimera should possess two characteristics. First, the embedding space should be as flat as possible so that the samples generated from Eq. 8 reflect the intermediate points in the feature space. Second, the individual embeddings should still be sufficiently distinct from each other, correctly representing the distance between each class distribution. In the remaining two subsections, we describe our strategies for enforcing the embeddings to contain the above characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Disentanglement Mapping</head><p>To perform superposing in a flatter manifold, we added a learnable mapping function M : R D ? R d before the embeddings are superposed, where D is the embedding dimension and d is the dimension of the target space. Thus,x from Eq. 8 becomesx = G S(m) , where m k = M (e k ). Although we do not add any specific loss that guides the output space of M to be flat, training to match the output of the full-precision model (i.e., L CE ) using the superposed? encourages M to map the input to a flatter space. In practice, we modeled M as a single-layer perceptron, which we call the disentanglement mapping layer. The experimental results from Section 5 demonstrate that the disentanglement mapping provides a considerable amount of performance gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Extracted Embedding Initialization</head><p>For the embeddings to be flat, we want the distributions of the embeddings fused with noise to be similar to the feature space. For this purpose, we utilize the fully connected layer of the full precision model P . Given f , the output features of the full-precision model before the last fully connected layer, we minimize C y dist P (f |y), P (g|y) , where C is the number of classes, g is the input of the generator, and dist is some distance metric. We do not have knowledge of the distribution of f ; thus, we modeled it as a Gaussian distribution, such that f |y ? N (? y , 1). Therefore, solving it against Eq. 5 simply yields E(y) = ? y . In practice, we use the corresponding column from the weight of the last fully connected layer of the full-precision model because its weights represent the centroids of the activations. If the weights of the last fully connected layer W = [w 1 , w 2 , ..., w C ], we set ? y = w y . Our experiments reveal that extracting the weights from the full-precision model and freezing them already works well (see Appendix B.2). However, using them as initializations and jointly training them empirically works better. We believe this is because fully connected layers have bias parameters in addition to weight parameters. Because we do not extract these biases into the embeddings, a slight tuning is needed by training them. This outcome aligns with the findings from the class prototype scheme used in self-supervised learning approaches <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b32">33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Implementation</head><p>Our method is evaluated on CIFAR-10, CIFAR-100 <ref type="bibr" target="#b34">[35]</ref> and ImageNet (ILSVRC2012 <ref type="bibr" target="#b40">[41]</ref>) datasets, which are well-known datasets for evaluating the performance of a model on the image classification task. CIFAR10/100 datasets consist of 50k training sets and 10k evaluation sets with 10 classes and 100 classes, respectively, and is used for small-scale experiments in our evaluation. ImageNet dataset is used for large-scale experiments. It has 1.2 million training sets and 50k evaluation sets. To keep the data-free environment, only the evaluation sets were used for test purposes in all experiments.</p><p>For the experiments, we chose ResNet-20 <ref type="bibr" target="#b35">[36]</ref> for CIFAR-10/100, and ResNet-18, ResNet-50, and MobileNetV2 <ref type="bibr" target="#b41">[42]</ref> for ImageNet. We implemented all our techniques using PyTorch <ref type="bibr" target="#b42">[43]</ref> and ran the experiments using RTX3090 GPUs. All the model implementations and pre-trained weights before quantization are from pytorchcv library <ref type="bibr" target="#b43">[44]</ref>. For quantization, we quantized all the layers and activations using n-bit linear quantization, described by <ref type="bibr" target="#b6">[7]</ref>, as below:</p><formula xml:id="formula_9">? = ? ? ? min interval(n) ? 2 n?1<label>(10)</label></formula><p>where ? is the full-precision value, ? is the quantized value, interval(n) is calculated as ?max??min 2 n ?1 . ? min and ? max are per-channel minimum and maximum value of ?.</p><p>To generate synthetic samples, we built a generator using the architecture of ACGAN <ref type="bibr" target="#b17">[18]</ref> and added a disentanglement mapping layer after class embeddings followed by a superposing layer. Among all batches, the superposing layer chooses between superposed embeddings and regular embeddings in p : 1 ? p ratio. The dimension of latent embedding and random noise was set to be the same with the channel of the last fully connected layer of the target network. For CIFAR, the intermediate embedding dimension after the disentanglement mapping layer was set as 64. The generator was trained using Adam <ref type="bibr" target="#b44">[45]</ref> with a learning rate of 0.001. For ImageNet, the intermediate embedding dimension was set to be 100. To maintain label information among all layers of the generator, we  <ref type="figure">Figure 3</ref>: PCA plots of the features before the last layer of the full-precision model. In the plots of Qimera-generated samples (c), the black dots represent synthetic boundary supporting samples generated with the superposed latent embeddings, which fill the space between the clusters. GDFQgenerated samples (b) form clusters that are smaller than that of the original data (a) and lack samples in the mid-ground. Best viewed in color.</p><p>apply conditional batch normalization <ref type="bibr" target="#b45">[46]</ref> rather than regular batch normalization layer, following SN-GAN <ref type="bibr" target="#b46">[47]</ref>. The optimizer and learning rate were the same as that of CIFAR.</p><p>To fine-tune the quantized model Q, we used SGD with Nesterov <ref type="bibr" target="#b47">[48]</ref> as an optimizer for Q with a learning rate of 0.0001 while momentum and weight decay terms as 0.9 and 0.0001 respectively. The generator G and the quantized model Q were jointly trained with 200 iterations for 400 epochs while decaying the learning rate by 0.1 per every 100 epochs. The batch size was 64 and 16 for CIFAR and ImageNet respectively. While Qimera can be applied almost all generator-based approaches, we chose to adopt baseline loss functions for training from GDFQ <ref type="bibr" target="#b12">[13]</ref>, because it was stable and showed better results for large scale experiments. Thus, loss functions L(G) and L(Q) are equal to Eq. 1 and Eq. 3 with ? = 0.1 and ? = 1.0, following the baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Visualizations of Qimera-generated Samples on Feature Space</head><p>To ensure that the superposed latent embedding creates boundary supporting samples, we conducted an experiment to compare generated synthetic samples on feature space visually. The experimental results are based on the generators trained with ResNet-20 for CIFAR-10 dataset. The features were extracted from the intermediate activation before the last fully connected layer of the full-precision model. For the Qimera-based generator, we set K=2 during the sample generations for clarity.</p><p>After extracting the features, we projected the features into a two-dimensional space using Principal Component Analysis (PCA) <ref type="bibr" target="#b48">[49]</ref>. The results are presented in <ref type="figure">Figure 3</ref>. Compared with the original training set data <ref type="figure">(Figure 3a)</ref>, the samples from the GDFQ-based generator <ref type="figure">(Figure 3b</ref>) show a lack of boundary supporting samples and the class distributions are confined to small regions around the centroid of each class. A generator trained with Qimera, however, exhibits different characteristics <ref type="figure">(Figure 3c</ref>). Samples that are generated from superposed latent embeddings are displayed as black dots. The black dots are mostly located on the sparse regions between the class clusters. This experimental result shows that our method, Qimera, successfully generates samples near the decision boundaries. In other words, superposed latent embeddings are not only superposed on embedding space but also in the feature space, serving as synthetic boundary supporting samples. <ref type="table" target="#tab_2">Table 2</ref> displays the classification accuracy on various datasets, target models, and bit-width settings. Note that nwma means n-bit quantization for weights and m-bit quantization for activations. As baselines, we selected ZeroQ <ref type="bibr" target="#b14">[15]</ref>, ZAQ <ref type="bibr" target="#b13">[14]</ref>, and GDFQ <ref type="bibr" target="#b12">[13]</ref> as the important previous works on generative data-free quantization. In addition, we implemented Mixup <ref type="bibr" target="#b49">[50]</ref> and Cutmix <ref type="bibr" target="#b50">[51]</ref> on top of GDFQ, which are data augmentation schemes that mix input images. To implement these schemes, we created synthetic images from GDFQ, and applied the augmentations to build training images and labels. All baseline results are from official code of the authors, where a small amount of   The results demonstrate that Qimera outperforms the baselines at almost all settings. Its performance improvement is especially large for low-bitwidth (4w4a) cases. For 5w5a setting, the gain is still significant, and its performance reaches near that of the full-precision model, which represents the upper bound. In addition, Qimera is not limited to small datasets having a low spatial dimension. The ImageNet results prove that Qimera performs beyond other baselines with considerable gaps, on a large-scale dataset with many classes. Interestingly, the result of GDFQ+Mixup and GDFQ+Cutmix did not produce much noticeable improvement except for GDFQ+Mixup in 4w4a ResNet -18 and -50. This implies that a mixture of generated synthetic samples of different classes in the sample space is not sufficient to represent boundary supporting samples. In summary, Qimera achieves superior accuracy on various environments regardless of dataset or model scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Quantization Results Comparison</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Qimera on Top of Various Algorithms</head><p>In this paper, we have applied GDFQ <ref type="bibr" target="#b12">[13]</ref> as a baseline. However, our design does not particularly depend on a certain method, and can be adopted by many schemes. To demonstrate this, we implemented Qimera on top of ZAQ <ref type="bibr" target="#b13">[14]</ref> and AutoReCon <ref type="bibr" target="#b26">[27]</ref>. The results are shown in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>AutoReCon <ref type="bibr" target="#b26">[27]</ref> strengthens the generator architecture using a neural architecture search, and therefore applying this is no different from that of the original Qimera implemented on top of GDFQ. However, original ZAQ does not use per-class sample generation. To attach the techniques from Qimera, we extended the generator with an embedding layer, and added a cross-entropy loss into the loss function as the following:</p><formula xml:id="formula_10">L(G) = L P,Q o (G) + ?L P,Q f (G) + ?L P a (G) + ?L P CE (G),<label>(11)</label></formula><formula xml:id="formula_11">L ( Q) = ?L P,Q o (G) ? ?L P,Q f (G) + ?L Q CE (G),<label>(12)</label></formula><p>where ? was set to 0.1, and L P CE (G), L Q CE (G) are calculated based on Eq. 8. For all cases, applying the techniques of Qimera improves the performance by a significant amount. Especially for Qimera + ZAQ, the performances obtained was better than those of our primary implementation Qimera + GDFQ. Unfortunately, it did not converge with ImageNet, and thus we did not consider this implementation as the primary version of Qimera.   <ref type="table">Table 5</ref>: Embedding Distance Ratio and Intrusion Score</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Further Experiments</head><p>We discuss some aspects of our method in this section, which are effect of each scheme introduced in Section 4 on the accuracy of Q, a sensitivity study upon various p and K settings, and a closer investigation into the effect of disentanglement mapping (DM) and extracted embedding initialization (EEI) on the embedding space. All experiments in this subsection are under 4w4a setting.</p><p>Ablation study. We conducted an ablation study by adding the proposed components one by one on the GDFQ baseline. The results are presented in <ref type="table" target="#tab_5">Table 4</ref>. As we can see, the superposed embedding (SE) alone brings a substantial amount of performance improvement for both Cifar-100 (1.16%p) and ImageNet (11.98%p). On Cifar-100, addition of EEI provides much additional gain of 0.68%p, while that of DM is marginal. ImageNet, on the other hand, DM provides 1.97%p additional gain, much larger than that of EEI. When both of DM and EEI are used together with SE, the additional improvement over the best among (+SE, DM) and (+SE, EEI) is relatively small. We believe this is because DM and EEI serve for the similar purposes. However, applying both techniques can achieve near-best performance regardless of the dataset.</p><p>Sensitivity Analysis. <ref type="table" target="#tab_5">Table 4</ref> also provides a sensitivity analysis for K (number of classes for superposed embeddings) and p (ratio of synthetic boundary supporting samples within the dataset). As shown in the table, both parameter clearly have an impact on the performance. For Cifar-100, the sweet spot values for K and p are both small, while those of the ImageNet were both large. We believe is because ImageNet has more classes. Because the number of class pairs (and the decision boundaries) is a quadratic function of number of classes, there needs more boundary supporting samples with more embeddings superposed to draw the correct border.</p><p>Investigation into the Embedding Space. To investigate how DM and EEI help shaping the embedding space friendly to SE, we conducted two additional experiments with ResNet-20 in <ref type="table">Table 5</ref>. First, setting K=2 (between two classes), we measured the ratio of the perceptual distance and Euclidean distance between the two embeddings in the classifier's feature space. The perceptual distance is measured by sweeping ? from Eqs. 6 and 7 from 0 to 1 by 0.01 and adding all the piecewise distances between points in the feature space (See <ref type="figure" target="#fig_6">Figure 5</ref> in Appendix). We want the distance ratio to be close to 1.0 for the embedding space distribution to be similar to that of the feature space. Second, we defined 'intrusion score', the sum of logits that are outside the chosen pair of classes. If the score is large, that means the synthetic boundary samples are regarded as samples of non-related classes. Therefore, lower scores are desired. For comparison, we have also included the Mixup as a naive method and measured both metrics. It shows that the DM and EEI are effective for in terms of both the distance ratio and the intrusion score, explaining how DM and EEI achieves better performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Does it Cause Invasion of Privacy?</head><p>The motivation for data-free quantization is that the private original data might not be available.</p><p>Using a generator to reconstruct samples that follow the distribution of original data might indicate the invasion of privacy. However, as displayed in <ref type="figure">Figure 4</ref>, at least with current technologies, there is no sign of privacy invasion. The generated data are far from being human interpretable, which was also the case for previous works <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref>. See Appendix ( <ref type="figure" target="#fig_7">Figure 6</ref>) for more generated images.</p><p>Geirhos et al. <ref type="bibr" target="#b51">[52]</ref> reveal an interesting property that CNNs are heavily biased by local textures, not global shapes. BagNet <ref type="bibr" target="#b52">[53]</ref> supports this claim by confirming that images with distorted shapes but preserved textures can still mostly be correctly classified by CNNs. In such regard, it is no wonder that the generated samples are non-interpretable, because there is only a few combinations that maintains the global shape out of all possibilities that preserve the textures. Nonetheless, not being observed does not guarantee the privacy protection. We believe it is a subject for further investigations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Limitations</head><p>Even though Qimera achieves a superior performance, one drawback of this approach is the fact that it utilizes embeddings of multiple classes to generate the boundary supporting samples. This restricts its application to classification tasks and its variants. For example, generation of datasets such as image segmentation or object detection do not take class embeddings as the input. However, generators for such data are not extensively studied yet, and we envision adding diversity to them would require inputting some form of labels or embeddings similar to paint-to-image <ref type="bibr" target="#b53">[54]</ref>, which would allow Qimera be easily applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we have proposed Qimera, a simple yet powerful approach for data-free quantization by generating synthetic boundary supporting samples with superposed latent embeddings. We show in our motivational experiment that current state-of-the-art generative data-free quantization can be greatly improved by a small set of boundary supporting samples. Then, we show that superposing latent embeddings can close much of the gap. Extensive experimental results on various environments shows that Qimera achieves state-of-the-art performance for many networks and datasets, especially for large-scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Code</head><p>The whole code is available at https://github.com/iamkanghyunchoi/qimera, including the training, evaluation, and visualization for all settings. This project code is licensed under the terms of the GNU General Public License v3.0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Baseline with Different Noise Level</head><p>Instead of the superposed embeddings, we tried the alternative of adjusting the variance of the noise inputs, as discussed in Section 4.1 of the main body. We have tested five values as standard deviation ? z : 0.25, 0.5, 1.0, 1.5, and 2.0. The results in <ref type="table" target="#tab_7">Table 6</ref> show that as we hypothesized, just by increasing the noise level did not provide much improvement on the performance of the baseline. Instead, we have noticed a slight improvement on GDFQ when the noise level is decreased, and we believe this is due to the generation of clearer samples. Nonetheless, the accuracy is far below that of the proposed Qimera.  To show that the weight from the last fully connected layer of the full-precision model is a good candidate for the initial embeddings, we have performed an experiment where the embeddings are frozen right after initialization. The results are presented in <ref type="table">Table 7</ref>. Qimera with frozen embeddings are not better than the primary Qimera method with trained embeddings. However, compared to the two baselines (ZAQ and GDFQ), they provides a comparable accuracy on Cifar-10 and better accuracies on Cifar-100 and ImageNet. Furthermore, the accuracy on Cifar-10 dataset is close to the upper bound for all techniques under comparison, and thus the differences are minimal.   In addition to our choice of hyperparameters presented in the main body, we have performed a further extensive sensitivity study on those parameters, which is displayed in <ref type="table" target="#tab_10">Table 9</ref>. All experiments are against 4w4a configuration, equal to the <ref type="table" target="#tab_3">Table 3</ref> (Section 5.4) in the main body. Regardless of the choice in p and K, the results are all better than the two baselines ZAQ and GDFQ. Furthermore, while they all provide a meaningfully good performance, the results show a clear trend: lower p, K for Cifar-10/100 and higher p, K for ImageNet as sweet spots. This result supports the use of Qimera in that these parameters are easily tunable, not something that must be exhaustively seAutoReConhed for optimal values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Comparison with DSG</head><p>Qimera is conceptually similar to DSG <ref type="bibr" target="#b27">[28]</ref> which tries to diversify the sample generation by relaxing the batch-norm stat alignments. However, Qimera is different from DSG because we explicitly try to   <ref type="table" target="#tab_12">Table 10</ref> shows the comparison of Qimera with DSG. We use the reported numbers for DSG, and perform a new set of experiments for Qimera to match the settings. We use the lowest-bit settings for each network evaluated in DSG. As displayed in the table, Qimera outperforms DSG in all settings, especially for 4w4a cases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Class-Pairwise Visualization</head><p>To look closely onto the visualization of the samples from Section 5.3 <ref type="figure">(Figure 3</ref>) in the main body, we have plotted them in a pair-wise manner. Even though 10 classes in total gives 45 possible pairs, we chose nine symbolically adjacent pairs in the figure. Although being symbolically adjacent does not have much meaning, we believe having nine pairs is enough for our purpose rather than showing all 45 possible pairs. The colors match that of the <ref type="figure">Figure 3</ref>, where the lightgreen dots represent the synthetic boundary supporting samples. Also, we have plotted the path between the centroids of the two clusters in black, by varying ? (the ratio of superposition) from 0 to 1 by 0.01 without any noise. Each 10th percentile is denoted as larger black dots. The results show that the samples and the path lie relatively in the middle of the two clusters. Please note that we have performed PCA plot for each pair to best show the distribution, so the position and orientation of the clusters do not exactly match those from <ref type="figure">Figure 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D More Generated Images</head><p>Lastly, <ref type="figure" target="#fig_7">Figure 6</ref> shows more samples generated from Qimera. <ref type="figure" target="#fig_7">Figure 6a</ref> displays the synthetic boundary supporting samples generated from Cifar-10 dataset, with K = 2 and ? = 0.5. Each row and column represents a class from Cifar-10. For example, the image at row 0 (airplane) and column 2 (bird) represents a sample generated from superposed embeddings of airplane and bird. Although still not very human-recognizable, we find that each sample in <ref type="figure" target="#fig_7">Figure 6a</ref> has some features adopted from each of the source classes in <ref type="figure">Figure 4d</ref>. <ref type="figure" target="#fig_7">Figure 6c</ref> shows the sample images created from ImageNet. Because there are too many classes within ImageNet (1000), we chose 10 classes from them, which are {0: 'tench, Tinca tinca', 100: 'black swan, Cygnus atratus', 200: 'Tibetan terrier, chrysanthemum dog', 300: 'tiger beetle', 400: 'academic gown, academic robe, judge's robe", 500: 'cliff dwelling', 600: 'hook, claw', 700: 'paper towel', 800: 'slot, one-armed bandit', 900: 'water tower'}, and the original samples from those classes are shown in <ref type="figure" target="#fig_7">Figure 6b</ref>. As in Cifar-10, the generated samples are far from human-recognizable, but each row is clearly distinguishable from the others. In addition, <ref type="figure" target="#fig_7">Figure 6d</ref> contains the synthetic boundary supporting samples from ImageNet, following the same rules from <ref type="figure" target="#fig_7">Figure 6a</ref>. Again, we see that each position in the sample matrix adopts features from the rows of the corresponding class pair in <ref type="figure" target="#fig_7">Figure 6c</ref>.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Synthetic boundary supporting samples with superposed latent embeddings (proposed).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Diagram of synthetic samples in the feature space of the full-precision network. The black curves represent the decision boundary of the full-precision model, which are considered ideal for the quantized model (orange dotted curves) to mimic. When synthetic images are generated with per-class embeddings and noises as in (a), their features do not support the decision boundary, whereas the proposed approach in (b) generates the boundary supporting samples, helping the quantized model to set the near-ideal decision boundary.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>An overview of the proposed method. Proposed components are denoted as colored shapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>(a) Original Cifar- 10 (Figure 4 :</head><label>104</label><figDesc>b) Samples from GDFQ (c) Samples from ZAQ (d) Samples from Qimera Synthetic samples generated for Cifar-10 dataset. Each row represents one of the 10 classes, except for ZAQ which generates samples without labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Visualization of the generated samples in the feature space. The lightgreen cloud represents the synthetic boundary supporting samples. The black dots represents the path between the two embeddings without any noise, where every 10th percentile is denoted as larger dots. The colors match that of theFigure 3of the main body, but the PCA dimension has been adjusted to best show each chosen class pair.(a) Synthetic boundary supporting samples from Cifar-10. (b) Original ImageNet samples from the selected 10 classes. (c) Synthetic samples from ImageNet.(d) Synthetic boundary supporting samples from ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Additional synthetic samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison on data-free quantization schemes</figDesc><table><row><cell>Dataset</cell><cell>Model (FP32 Acc.)</cell><cell>Bits</cell><cell>GDFQ</cell><cell>Qimera + GDFQ</cell><cell>ZAQ</cell><cell>Qimera + ZAQ</cell><cell>AutoReCon</cell><cell>Qimera + AutoReCon</cell></row><row><cell>Cifar-10</cell><cell>ResNet-20 (93.89)</cell><cell>4w4a 5w5a</cell><cell>90.25 93.38</cell><cell cols="3">91.26 (+1.01) 92.13 93.91 (+1.78) 93.46 (+0.08) 93.36 93.84 (+0.48)</cell><cell>88.55 92.88</cell><cell>91.16 (+2.61) 93.42 (+0.54)</cell></row><row><cell>Cifar-100</cell><cell>ResNet-20 (70.33)</cell><cell>4w4a 5w5a</cell><cell>63.39 66.12</cell><cell cols="3">65.10 (+1.71) 60.42 69.30 (+8.88) 69.02 (+2.90) 68.70 69.58 (+0.88)</cell><cell>62.76 68.40</cell><cell>65.33 (+2.57) 68.80 (+0.40)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Performance of Qimera implemented on top of GDFQ, ZAQ and AutoReCon modifications have been made on the GDFQ baseline for applying Mixup and Cutmix. We report top-1 accuracy for each experiment. The numbers inside the parentheses of Qimera results are improvements over the highest baseline.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation study and Sensitivity analysis</figDesc><table><row><cell>Method</cell><cell cols="2">Cifar-10</cell><cell cols="2">Cifar-100</cell></row><row><cell></cell><cell cols="4">Dist. Ratio Intrusion Dist. Ratio Intrusion</cell></row><row><cell>Mixup</cell><cell>2.44</cell><cell>0.800</cell><cell>3.14</cell><cell>0.400</cell></row><row><cell>SE Only</cell><cell>1.58</cell><cell>0.00260</cell><cell>1.64</cell><cell>0.053</cell></row><row><cell>SE+DM</cell><cell>1.67</cell><cell>0.00073</cell><cell>1.59</cell><cell>0.044</cell></row><row><cell>SE+DM+EEI</cell><cell>1.57</cell><cell>0.00013</cell><cell>1.52</cell><cell>0.029</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Experimental results on noise variance test B.2 Extracted Embedding Initialization without Training</figDesc><table><row><cell>Dataset</cell><cell>Model (FP32 Acc.)</cell><cell cols="2">Bits ZeroQ ZAQ</cell><cell cols="2">GDFQ Qimera</cell><cell>Extracted Init + Freeze</cell></row><row><cell>Cifar-10</cell><cell>ResNet-20 (93.89)</cell><cell>4w4a 79.30 5w5a 91.34</cell><cell cols="2">92.13  *  90.25 93.36 93.38  *</cell><cell>91.26 93.46</cell><cell>90.37 93.25</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Sensitivity Study on Number of DM LayersTo have a deeper look into the DM layers, we have conducted a sensitivity study on the number of DM layers inTable 8. In the table, all results are from 4w4a setting with p=0.4, K=2 for Cifar-10 and K=10 for Cifar-100. As displayed, we found that there are sometimes small improvements from using more DM layers above one, but a severe drop in performance has been observed for using too many layers (Cifar-100, 8 layers). 64.62 64.90 64.95 64.76 64.89 10 64.85 64.63 65.10 64.76 64.52 63.86 25 64.53 64.91 64.72 64.66 64.40 64.01 100 64.37 64.66 64.64 64.27 64.79 63.65 ImageNet (ResNet-50) 100 58.74 60.64 61.43 61.47 63.87 65.73 250 61.30 61.28 62.16 64.03 64.50 65.23 500 58.96 60.11 58.69 63.05 66.25 66.19 1000 58.65 59.62 61.20 58.86 65.12 64.24</figDesc><table><row><cell cols="3">B.4 More Sensitivity Study on Hyperparameters</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>K</cell><cell></cell><cell>p</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.10</cell><cell>0.25</cell><cell>0.40</cell><cell>0.55</cell><cell>0.70</cell><cell>0.85</cell></row><row><cell></cell><cell>2 64.18</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Cifar-100</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>(ResNet-20)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Further Sensitivity analysis</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Comparison with DSG generate boundary supporting samples, instead of relying on diversification. This would led to better performance as demonstrated in the motivational experiment of Section 3.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">In-datacenter performance analysis of a tensor processing unit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jouppi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium on Computer Architecture</title>
		<meeting>the International Symposium on Computer Architecture</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huizi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems Workshops</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Dorefa-net: Training low bitwidth convolutional neural networks with low bitwidth gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuchang</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06160</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards effective low-bitwidth convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Defensive quantization: When efficiency meets robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Quantization and training of neural networks for efficient integer-arithmetic-only inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Q-BERT: Hessian based ultra low precision quantization of BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Post-training 4-bit quantization of convolution networks for rapid-deployment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><surname>Banner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.05723</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Data-free quantization through weight equalization and bias correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Nagel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Data-free network quantization with adversarial knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoojin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Improving neural network quantization without retraining using outlier channel splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ritchie</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative low-bitwidth data free quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shoukai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Zero-shot adversarial quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15263</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ZeroQ: A novel zero shot quantization framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaohui</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Knowledge distillation with adversarial samples supporting decision boundary</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems Workshops</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Augustus</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Data-free knowledge distillation for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Raphael Gontijo Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thad</forename><surname>Fenu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Starner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07535</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Zero-shot knowledge distillation in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gaurav Kumar Nayak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Data-free learning of student networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Zero-shot knowledge transfer via adversarial belief matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Micaelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09768</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Data-free adversarial distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongfan</forename><surname>Fang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11006</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dreaming to distill: Data-free knowledge transfer via deepinversion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hongxu Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Knowledge extraction with no observable data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaemin</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">AutoReCon: Neural Architecture Search-based Reconstruction for Data-free Compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baozhou</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.12151</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Diversifying Sample Generation for Accurate Data-Free Quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangguo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021</meeting>
		<imprint>
			<biblScope unit="page" from="15658" to="15667" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mixup as locally linear out-of-manifold regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongyi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Intriguing properties of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adversarial metric knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihe</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Communication and Information Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Characterizing the decision boundary of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.11460</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Universal Domain Adaptation through Self Supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Open-set interference signal recognition using boundary samples: A hybrid approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Wireless Communications and Signal Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.utoronto.ca/~kriz/learning-features-2009-TR.pdf" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Latent space oddity: On the curvature of deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgios</forename><surname>Arvanitidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><forename type="middle">Kai</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?ren</forename><surname>Hauberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feature-based metrics for exploring the latent space of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Semi-supervised domain adaptation via minimax entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems Workshops</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Computer vision models on PyTorch</title>
		<ptr target="https://pypi.org/project/pytorchcv/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Modulating early visual processing by language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vries</forename><surname>Harm De</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Spectral Normalization for Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">A method for solving the convex programming problem with convergence rate O(1/k 2 )</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">E</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Dokl. Akad. Nauk SSSR</title>
		<imprint>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">LIII. On lines and planes of closest fit to systems of points in space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pearson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</title>
		<imprint>
			<date type="published" when="1901" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Internation Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization strategy to train strong classifiers with localizable features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Approximating CNNs with Bag-of-local-Features models works surprisingly well on ImageNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">SinGAN: Learning a generative model from a single natural image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamar</forename><forename type="middle">Rott</forename><surname>Shaham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Michaeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
