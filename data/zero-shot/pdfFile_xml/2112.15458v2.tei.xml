<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Accurate and Real-time 3D Pedestrian Detection Using an Efficient Attentive Pillar Network</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tho</forename><surname>Duy</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Data Science</orgName>
								<orgName type="institution">AI Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengcan</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Data Science</orgName>
								<orgName type="institution">AI Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Shi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Data Science</orgName>
								<orgName type="institution">AI Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Rezatofighi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Data Science</orgName>
								<orgName type="institution">AI Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Data Science</orgName>
								<orgName type="institution">AI Monash University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Accurate and Real-time 3D Pedestrian Detection Using an Efficient Attentive Pillar Network</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:39+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Deep Learning for Visual Perception</term>
					<term>Recogni- tion</term>
					<term>3D object detection</term>
					<term>LiDAR</term>
					<term>Real-time</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Efficiently and accurately detecting people from 3D point cloud data is of great importance in many robotic and autonomous driving applications. This fundamental perception task is still very challenging due to (i) significant deformations of human body pose and gesture over time and (ii) point cloud sparsity and scarcity for pedestrian class objects. Recent efficient 3D object detection approaches rely on pillar features to detect objects from point cloud data. However, these pillar features do not carry sufficient expressive representations to deal with all the aforementioned challenges in detecting people. To address this shortcoming, we first introduce a stackable Pillar Aware Attention (PAA) module for enhanced pillar features extraction while suppressing noises in the point clouds. By integrating multipoint-channel-pooling, point-wise, channel-wise, and task-aware attention into a simple module, the representation capabilities are boosted while requiring little additional computing resources. We also present Mini-BiFPN, a small yet effective feature network that creates bidirectional information flow and multi-level crossscale feature fusion to better integrate multi-resolution features. Our proposed framework, namely PiFeNet, has been evaluated on three popular large-scale datasets for 3D pedestrian Detection, i.e. KITTI, JRDB, and nuScenes achieving state-of-the-art (SOTA) performance on KITTI Bird-eye-view (BEV) and JRDB and very competitive performance on nuScenes. Our approach has inference speed of 26 frame-per-second (FPS), making it a real-time detector. The code for our PiFeNet is available at https:// github.com/ ldtho/ PiFeNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>F AST and reliable pedestrian detection in 3D world is a fundamental step in a perception system, conducive to many robotic and autonomous driving applications such as human-robot interactions, service robots, and autonomous navigation in crowded human environments. The recent 3D pillar-based object detectors, e.g. <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b48">[49]</ref> integrate pillar features, a down-sampled representation of point cloud, into their frameworks to detect objects reliably from point cloud data. These approaches have shown auspicious results in detecting rigid and large objects such as different types of vehicles. However, their effectiveness in detecting pedestrians is limited because of the two following challenges:</p><p>Challenge 1: Weak expressive capability of pillar features for pedestrian class. Pedestrians are not rigid bodies, hence they can take numerous poses (running, bowing, sitting, laying, etc.), as shown in <ref type="figure" target="#fig_0">Fig. 2A</ref>. The previous methods such as Pointpillars <ref type="bibr" target="#b17">[18]</ref> struggles distinguishing pedestrians from poles and trees due to 3D spatial information loss during quan- <ref type="figure">Fig. 1</ref>. The performance of our PiFeNet on KITTI <ref type="bibr" target="#b9">[10]</ref> official test split on both pedestrian Birds-eye-view and 3D detection versus inference speed in FPS. Our approach achieved state-of-the-art (SOTA) performance in detecting pedestrians. The comparison details are presented in Tab. I tisation process. This necessitates the development of more representative pillar features so that detectors can correctly classify pedestrians among other similarly-looking objects. To address this limitation, TANet <ref type="bibr" target="#b22">[23]</ref> has incorporated a triple attention module to enhance the pillar feature extraction. However, this approach only use max pooling to capture the context feature for detecting an object, suppressing a lot of information from the points inside the pillar, which are informative for better person detection and localisation.</p><p>To retain important information while generalising to various pedestrian representations, detectors require to adopt comprehensive attention mechanisms in pillar features. To this end, we propose a Pillar Aware Attention (PAA) module that performs attention on pillars utilising improved attention mechanisms. The PAA module includes multi-point-channel pooling, point-wise, channel-wise, and task-aware attention techniques to enhance pillar feature representation. Multipoint-channel pooling uses various pooling strategies to grasp context information from all points in a pillar. Point-and channel-wise attentions retain proper information during the feature extraction by suppressing redundant information. Taskaware attention switches the channels on or off based on their contribution to downstream tasks by selecting the most representative channels. We will show that PAA module is a key in boosting the performance of pedestrian detection.</p><p>Challenge 2: Fewer pillars enclosing pedestrians after quantization as a result of their small occupancy and point sparsity in 3D point cloud. In comparison with vehicles, human are relatively small to be easily captured by LiDAR sensors, thus occupying fewer pillars after the quantisation step. As demonstrated in <ref type="figure" target="#fig_0">Fig. 2B</ref>, distant pedestrians are really small and hard to detect from a LiDAR due to their size and the sparsity of the sensor's laser beams. This becomes more challenging in crowded and/or cluttered areas, where people tend to stand or move in dense groups (cf. <ref type="figure" target="#fig_5">Fig. 6</ref>). Considering this fact that a pedestrian can occupy a very small region in a point cloud data, distinguishing it from other background objects may require more than just extracting a naive pillar feature; For example, to enrich feature representations in both low-and high-level point clouds information, the multi-scale features of adjacent pillars can be also used.</p><p>Thus, we introduce Mini-BiFPN, a lightweight feature network inspired by <ref type="bibr" target="#b35">[36]</ref> that can bidirectionally connect and fuse multi-resolution features. Also, the learnable weights at fusion gates can be adjusted based on the contribution of features at multiple resolutions. The Mini-BiFPN is composed of three separable convolutional blocks with a minimal number of layers, making it lightweight and efficient.</p><p>Combining the aforementioned modules, we present PiFeNet, a novel end-to-end trainable 3D object detector capable of extracting important features in real-time, which benefits from the reduced inference latency of single-stage methods and the improved feature extraction mechanisms from attention. Our contributions are as follows:</p><p>1) We introduce a stackable Pillar Aware Attention Module, applying attention on pillars at the feature learning stage. 2) We propose Mini-BiFPN, a lightweight feature network that efficiently performs weighted multi-level feature fusion and provides bidirectional information flow. 3) We comprehensively evaluate our framework's performance and its generalisability using three large-scale object detection benchmarks, including KITTI <ref type="bibr" target="#b9">[10]</ref>, JRDB <ref type="bibr" target="#b24">[25]</ref> and Nuscenes <ref type="bibr" target="#b2">[3]</ref>. Comparing with the published counterparts, PiFeNet achieves SOTA performance on KITTI (BEV) and JRDB benchmarks and very competitive performance on nuScenes benchmark.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>3D object detection from LiDAR point cloud. In general, 3D detectors from LiDAR point cloud can be classified into two types: one-stage and two-stage. Two-stage detectors <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b44">[45]</ref> first generate a series of anchors by the region proposal network (RPN), and then predict proposals based on these anchors. Many two-stage works <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> focus on enhancing the RPN outputs, while others <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b44">[45]</ref> refine the representations of point clouds, through multi-modal feature fusion using RGB images and 3D point clouds. Although two-stage approaches provide more accurate predictions than one-stage detectors, they are significantly slower than one-stage counterparts due to the high computation overhead. One-stage approaches make a trade off between detection speed and accuracy. The recent progress on one-stage detectors <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b46">[47]</ref> have been shown a very competitive performance compared to the two-stage approaches. This type of architecture applies fully convolutional network on modified presentations of point cloud such as pillars <ref type="bibr" target="#b17">[18]</ref> and voxels <ref type="bibr" target="#b41">[42]</ref>, to directly regress proposals. For example, Voxelnet <ref type="bibr" target="#b47">[48]</ref> extracts features of voxels generated by PointNet <ref type="bibr" target="#b27">[28]</ref>. SECOND <ref type="bibr" target="#b41">[42]</ref> learns sparse voxel representations by using voxel-based 3D convolutional layers. PointPillars <ref type="bibr" target="#b17">[18]</ref> splits the input point cloud into pillars, assuming there is no overlapping object, and then uses pseudo-images as intermediate representations for efficient feature learning. SA-SSD <ref type="bibr" target="#b10">[11]</ref> introduced a detachable auxiliary network to transform convolutional feature maps to point-level representations, which makes the backbone network more aware for the structure of objects. 3D-SSD <ref type="bibr" target="#b42">[43]</ref> proposes a lightweight anchor-free single-stage architecture that can run at real-time speed by eliminating upsampling layers and the refining stage. CIA-SSD <ref type="bibr" target="#b0">[1]</ref> presents a fusion module with just one fusion gate to link low-and high-level features, while our Mini-BiFBN module makes use of four fusion gates to comprehensively fuse features across three different scales. Distance-variant IoU-weighted NMS is also proposed in this paper to generate smoother regressions and eliminate duplicate predictions. Most recently, SE-SSD <ref type="bibr" target="#b46">[47]</ref> proposes a shape-aware augmentation method to train a pair of teacher-student SSDs, which is encouraged to infer entire object forms. In spite of their great performance in detecting general object categories, these stateof-the-art 3D detectors, i.e. SA-SSD <ref type="bibr" target="#b10">[11]</ref>, 3D-SSD <ref type="bibr" target="#b42">[43]</ref>, and SE-SSD <ref type="bibr" target="#b46">[47]</ref>, do not perform well in detecting pedestrian class due to aforementioned challenges. TANet <ref type="bibr" target="#b22">[23]</ref> proposes a triple attention module, which can better extract discriminating features for pedestrians by attention mechanisms. However, this technique only considers the maximum values of the pillars' channels as encoded representations, and these maximum values do not reflect a rich representation of all the points inside that pillar, which is essential for precise object detection and localisation. In this paper, we address this weakness by proposing PAA module with multiple attention mechanisms and improved pooling strategies. Attention mechanisms in object detection. Attention mechanisms have been shown to be advantageous for a variety of computer vision applications, including image classification <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b12">[13]</ref>, segmentation <ref type="bibr" target="#b8">[9]</ref>, and object detection <ref type="bibr" target="#b34">[35]</ref>. Particularly in the detection of small objects, the technique is applied to guide the model's attention to very small details without incurring a considerable computational cost. Following the feature maps generated by convolutional layers, the attention masks are then computed to capture the spatial (Nonlocal Net <ref type="bibr" target="#b37">[38]</ref> and Criss-cross Net <ref type="bibr" target="#b14">[15]</ref>) and channel-related information (SE <ref type="bibr" target="#b12">[13]</ref> and AiA <ref type="bibr" target="#b6">[7]</ref>). Also, some techniques such as CBAM <ref type="bibr" target="#b39">[40]</ref> and CA <ref type="bibr" target="#b11">[12]</ref> combine the two for improved information fusion. In 3D vision tasks, Point-attention <ref type="bibr" target="#b7">[8]</ref> proposes a self-attention structure and skip-connection to capture long-range dependencies of points for point cloud segmentation. Point-Transformer <ref type="bibr" target="#b45">[46]</ref> learns the attention weights for the neighbour points, by using local geometric relationships between the center point and its surrounding.</p><p>Following the success of pillar-based approaches <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b22">[23]</ref>, PiFeNet is a novel end-to-end trainable 3D object detector capable of extracting important features of small objects in real-time, which benefits from the reduced inference latency of single-stage methods and the improved feature extraction mechanisms from attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OUR APPROACH</head><p>In this section, we introduce our end-to-end trainable PiFeNet for 3D object detection based on pillars, followed by the formulas of multi-point-aware pooling, channel-wise, pointwise, task-aware attention mechanisms in the PAA module, and the structure of the Mini-BiFPN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Pillar-Feature Network (PiFeNet)</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 3</ref>, PiFeNet firstly takes a point cloud as input and quantises it into pillars by a pillar generator module similar to <ref type="bibr" target="#b17">[18]</ref>. Next, we stack our proposed PAA modules to extract features of pillars with reduced information loss. Then the attention-weighted pillar features are scattered onto a pseudoimage, which is run through our Mini-BiFPN to fuse features at different resolutions. Finally, we use a simple detection head including classification and box regression branches to produce the detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. 3D object detection</head><p>By definition, 3D object detection is a task of detecting objects in 3D space, each object is defined by a 3D bounding box (c x , c y , c z , w, l, h, ?) where c x , c y , c z denote the x, y, and z centers of the box; w, l, h are the box's dimensions and ? is the heading angle. The input of the model is a set of points (point cloud), defined as T =</p><formula xml:id="formula_0">p i = [x i , y i , z i , r i ] ? R i=1,2,3,...,M where x i , y i , z i are the</formula><p>x, y, and z coordinate values of the points, respectively; r i is the points' reflection intensity that may be used as additional feature, and M is the total number of points. Assume the dimension of a point cloud T is defined as L * for length (x axis), W * for width (y-axis), and H * for height (z-axis). T is then equally divided into pillars <ref type="bibr" target="#b17">[18]</ref> (z-axis stacked voxels) along the x and y axes. Due to the sparsity and unequal distribution of the point clouds, each pillar has a varying amount of points. Let N be a pillar's maximum number of points, C be each point's number of channels, and P be the amount of pillars in the pillar grid</p><formula xml:id="formula_1">G where G = {p 1 , p 2 , . . . p j } j?P . Each pillar's dimensions are [p * W , p * L , p * H ],</formula><p>where p * H = H * since we do not divide the point cloud along z-axis. Let the number of output classes be N c and N a is the number of anchors per pillar. Our PiFeNet directly takes a raw point cloud as input and produces P ? N a ? N c prediction boxes and their classifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pillar Aware Attention (PAA) module</head><p>As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, PAA module begins with multi-pointchannel pooling followed by two branches of point-and channel-wise attention. The outputs are then integrated using element-wise multiplication before going through the taskaware attention sub-module. In ablation study, we will show that these three sub-modules can considerably improve the performance by enhancing feature representation.</p><p>Multi-point-channel pooling. To capture the context of all points and channels in a pillar, we present multi-point-channel pooling, which applies both max and average pooling to the point-and channel-wise dimensions in point clouds. For a pillar grid G ? R P ?N ?C , where N is the maximum number of points and C is the number of channels in a pillar, and P is the maximum amount of pillars {p 1 , p 2 , . . . p j } j?P in the grid, we perform average pooling and max pooling across N points in the pillar grid G to aggregate channel information, which results in two distinct channel context representations </p><formula xml:id="formula_2">A c/p = ? w 1 w 0 F mean c/p + w 1 w 0 F max c/p (1)</formula><p>where ? is the sigmoid function, the weights in the two fullyconnected layers are w 0 ? R N/r?N and w 1 ? R N ?N/r in case of point-wise attention, or w 0 ? R C/r?C and w 1 ? R C?C/r for channel-wise attention. The full attention matrix is then achieved by combining channel-wise attention A c and pointwise attention A p using element-wise multiplication M j = A p ? A c , where M j ? R N ?P ?C . By multiplying M j with the original pillar P j , we can obtain attention-weighted features in both channel-wise and point-wise dimensions.</p><p>After being processed by the point-wise and channel-wise attention sub-modules, the pillar features become more expressive and sensitive to all of the points in the point clouds and their channel features (x, y, z locations, centres).</p><p>Task-aware attention. The task-aware attention submodule is chained at the end to dynamically control the activation of each channel, the output pillar features are now reorganised into distinct activations in response to the needs of various downstream tasks. Similar to <ref type="bibr" target="#b5">[6]</ref>, given a feature map F c of a channel C k in the pillar P j and [? 1 , ? 1 , ? 2 , ? 2 ] T are the learnable parameters to monitor the activation functions. The activation at the channel C k is calculated as follows:</p><formula xml:id="formula_3">A k T (F c ) = max i?{1,2} ? i k ? F c + ? i k<label>(2)</label></formula><p>where ? i k and ? i k are the ? i and ? i at the k th channel and A k T (F c ) is the task-aware weighted pillar feature map of the channel C k .</p><p>To regulate the activation thresholds, a hyperfunction ?(?) = [? 1 , ? 1 , ? 2 , ? 2 ] is used where [? 1 , ? 1 , ? 2 , ? 2 ] is initialized with [1, 0, 0, 0]. ?(?) is calculated with using two fully connected layers similar to <ref type="bibr" target="#b12">[13]</ref>. The output is normalized by a shifted sigmoid layer f (x) = 2?(x) ? 1. Finally, the taskaware weighted features are concatenated or summed back to the original features.</p><p>Stackable PAA module. Our approach stacks two PAA modules to better leverage multi-level feature attention. The first module concatenates nine task-aware weighted features with the original ones; then the resulting 18 channels are increased to 64 at the end of the second PAA module and added to the input pillar grid. Eventually, a point-wise max pooling is performed to extract the strongest features, serving as inputs of Mini-BiFPN module, as shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Mini-BiFPN</head><p>We develop Mini-BiFPN detection head, a mini variant of BiFPN <ref type="bibr" target="#b35">[36]</ref> that can greatly improve the model's performance in 3D object detection with minimal efficiency trade-offs. The Mini-BiFPN firstly takes a pseudo-image, passes it through convolutional blocks B 1 , B 2 , B 3 to produce a list of features <ref type="figure" target="#fig_0">1, 2, 3)</ref>. Next the multi-scale features are aggregated by repeatedly applying top-down and bottomup bidirectional feature fusion, as shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. Trainable weights are added to adjust the fusion weights accordingly. Our Mini-BiFPN can be formalised as follows:</p><formula xml:id="formula_4">F in = F 1 in , F 2 in , F 3 in , with resolutions 1/2 i?1 of input pseudo-image where i ? (</formula><formula xml:id="formula_5">F 2 up = conv swish (w 1 ?F 2 in +w 2 ?upsample(F 3 in ) w 1 +w 2 +?<label>(3)</label></formula><formula xml:id="formula_6">F 2 out = conv swish (w 1 ?F 2 in +w 2 ?F 2 up +w 3 ?downsample(F 1 out ) w 1 +w 2 +w 3 +?<label>(4)</label></formula><p>where F 2 up is the fusion result of F 2 in , F 3 in . Then F 2 out is calculated using F 2 up , F 1 out , and F 2 in . w 1,..,i and w 1,..,i are  trainable parameters where i is the nu <ref type="bibr" target="#b28">[29]</ref>. F 1 out and F 3 out are computed similarly to F 2 out . The final feature representation is the concatenation of F 1 out , F 2 out , and F 3 out . It is then passed into a simple SSD <ref type="bibr" target="#b21">[22]</ref> detection head including classification and box regression branches for final predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>Our proposed PiFeNet is evaluated on KITTI <ref type="bibr" target="#b9">[10]</ref>, JRDB <ref type="bibr" target="#b24">[25]</ref> and Nuscenes <ref type="bibr" target="#b2">[3]</ref> object detection benchmarks. We choose CBGS-PP (with Pointpillars backbone) as a baseline for Nuscenes dataset and PointPillars for KITTI and JRDB.</p><p>KITTI Dataset: KITTI dataset has 7481 and 7518 samples for training and testing, respectively. The training set is further split into training and validating set with a ratio of 85:15. We conduct experiments on the "Pedestrian" category and use average precision (AP) with an (IoU) threshold 0.5 as an evaluation metric. The detection range is [0, 47.36], <ref type="bibr">[-19.84, 19</ref>.84], [-2.5, 0.5] metres in X and Y and Z coordinates, respectively. Pillar size is [0.16, 0.16, 4] metres in width, length, and height. Focal loss and L1 loss are used for box classification and regression. Also, the ground truths are mixed with sampled objects from other point clouds, together with random global rotation, scaling, and translation.</p><p>JRDB Dataset: JRDB is a novel human-centric dataset comprised of more than 1.8 million 3D cuboids surrounding all pedestrians in both indoor and outdoor settings. The dataset contains 54 sequences, 27 for training and 27 for testing. During training, we select 5 from 27 sequences for validation, as suggested in <ref type="bibr" target="#b24">[25]</ref>, and we use AP with an IoU value of 0.5 to monitor the training process. The detection range is [-39.68, <ref type="bibr" target="#b38">39</ref>.68] in X and Y, and [-2, 2] in Z LiDAR coordinate.</p><p>Random pedestrians are cut-and-pasted to the scene ensuring there are at least 30 people per input point cloud. Random rotation, flipping and scaling are also applied. The top 300 proposals after Non-max suppression (NMS) with IoU value of 0.5 are kept as final predictions.</p><p>Nuscene Dataset: We adopt CBGS <ref type="bibr" target="#b48">[49]</ref> augmentations strategies (global translation, scaling, and rotation) and multitask heads. The detection range is [-51.2, 51.2] metres for X and Y, and [-5, 3] for Z LiDAR coordinats, pillar size is set at [0.2, 0.2, 8] in width, length, and height respectively. We train the model for 20 epochs with batch size of 2. During testing, top 83 predictions with confidence scores greater than 0.1 are kept, and rotated NMS with IOU threshold of 0.02 is applied.</p><p>All the models are trained with AdamW <ref type="bibr" target="#b23">[24]</ref> optimiser and One Cycle policy <ref type="bibr" target="#b33">[34]</ref> with max LR at 0.003.  <ref type="bibr" target="#b0">1</ref> . This significant improvement of the performance is due to the enhanced expressiveness of the pillars, together with better communication between pillars at multiple scales, which are also shown experimentally by our ablation study in Sec. V. While achieving SOTA performance, PiFeNet's inference speed is 26 FPS (cf. Tab. V), making it one of the   efficient detectors on KITTI compared to many other voxeland pillar-based techniques. JRDB and nuScenes detection benchmark. We further evaluate our model on JRDB-3D detection benchmark. Compared to other published methods, our model is ranked first on JRDB 2019 2 in multiple AP thresholds (i.e. 0.3, 0.5, and 0.7), and is now the best one on the recently launched JRDB 2022 3 3D detection leaderboard. We also present in Tab. II the performance of our approach on nuScenes dataset compared to our chosen baselines Pointpillars++ <ref type="bibr" target="#b17">[18]</ref>, there is a significant improvement of 0.11 AP in pedestrian detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Comparison with</head><p>In <ref type="figure" target="#fig_6">Fig. 7</ref>, we present our qualitative findings which include both detection results ( <ref type="figure" target="#fig_6">Fig. 7A and 7C</ref>) and some typical failure cases ( <ref type="figure" target="#fig_6">Fig. 7B and 7D</ref>). <ref type="figure" target="#fig_6">Fig. 7C</ref> shows the robustness of our model when it can accurately detect all pedestrians in the picture, even at very long distances. Compared to Pointpillars <ref type="bibr" target="#b17">[18]</ref>, we successfully overcome the lack of expressiveness in the extracted pillar features, which usually leads to the model's confusion between narrow vertical features of trees/poles and pedestrians. Given the fact that we rarely see the model misclassify a tree/pole as a pedestrian, we did find an instance where the model was confused when encountering a stack of soda crates as in <ref type="figure" target="#fig_6">Fig. 7D</ref>. The second difficulty arises when pedestrians are too close together <ref type="figure" target="#fig_6">(Fig. 7B</ref>), preventing the model from accurately locating both items, and in most of the cases, we observe that one of the two pedestrians is ignored. However, as shown in <ref type="figure" target="#fig_5">Fig. 6A</ref>    achieve 66.73, 61.06, and 56.5 average precision (AP) on easy, moderate, and hard cases, reaching 61.43 overall mAP.</p><p>Attention modules analysis. We use both average and max pooling as a default setting, then we sequentially test each of the PAA module's components and their combinations. According to Tab. IV, channel-wise and task-aware attention alone does not significantly improve model performance. However, combining them results in a significant 2.2% mAP increase in detection performance. Also, integrating point-wise attention with either channel-wise or task-aware would slightly improve the performance. Therefore, while standalone pointwise attention (2.3% mAP increase) can significantly increase model performance, channel-wise and task-aware sub-modules must be employed together for better accuracy. The inference speed of each setting is also reported.</p><p>Pooling mechanisms analysis. In the last 3 rows of Tab. IV, we show that each pooling method improves the model's performance (point-, channel-, and task-aware sub-modules are included as default). The z-axis related channels may be used to extract the maximum height of points inside a pillar, while the x and y-axis related channels can be used to extract the mean position of points inside a pillar. While average pooling outperforms max pooling in detecting moderate and hard objects, max pooling is better in detecting easy ones. Concurrent usage of both pooling methods increases mAP to 64.2, showing that both pooling mechanisms are essential.</p><p>Mini-BiFPN module analysis. Tab. V presents the outcome of our ablation experiments on the contribution of the Mini-  BiFPN module. Since aggregating extra information from other stages in the feature network, the combination of crossscale connections and bidirectional information flow provided by our Mini-BiFPN module results in more accurate predictions. As in Tab. V, after switching from the baseline feature network to the Mini-BiFPN module, we achieve a significant improvement in all APs (7.05, 6.63, and 5.01 increase in easy, moderate, and hard APs) as well as 6.23 increase in mAP compared to the baseline. Thus, both PAA module and Mini-BiFPN are important components of our model.</p><p>Run-time Analysis At inference time, PiFeNet can reach an average speed of 26 FPS (39.16 ms latency), which breaks down into 5.84 ms for data pre-processing, 18.12 ms for PAA module, 0.60 ms for scattering the features to a pseudo-image, and 13.26 ms for Mini-BiFPN module, and 1.34 ms for postprocessing the predictions. The experiment is conducted on the KITTI dataset and a single NVIDIA V100. Following the previous works, we also compare the inference speed (in FPS) between PiFeNet and other published approaches. While this comparison cannot be easily benchmarked due to hardware/computation variations, it still demonstrates the efficiency of our detector for running in real-time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>Our PiFeNet has been presented, addressing the two main issues in 3D pedestrian detection. First, we introduced the Pillar Aware Attention Module, which combines multi-pointchannel-pooling, point-wise, channel-wise, and task-aware attention to better extract pillar features. Next is the Mini-BiFPN module, a lightweight feature network that leverages crossscale feature fusion and bidirectional connections, enriching information flow in the feature network. Our method achieves SOTA performance on both large scale benchmarks KITTI <ref type="bibr" target="#b9">[10]</ref> and JRDB <ref type="bibr" target="#b24">[25]</ref> and competitive results on Nuscenes <ref type="bibr" target="#b2">[3]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Point cloud density of pedestrian instances at increasing distances to the LiDAR sensor (B), and challenging postures (A) captured in the JRDB [25] dataset (point clouds are zoomed and coloured for better visibility). Blue bounding box indicates the instance of interest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>The PiFeNet's architecture. The input point cloud is first converted to pillars, and then go through the PAA modules for feature extraction and refinement. The most expressive features are chosen and converted into a pseudo-image, which serves as the Mini-BiFPN's input for generating the outputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>The Detailed Architecture of Pillar Aware Attention module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>??</head><label></label><figDesc>R P ?1?C . The same strategies are applied to the point-wise dimension to aggregate the point information F mean p R P ?N ?1 . Channel-and Point-wise attention. To encapsulate the global information, F mean c and F max c are passed through a shared multi-layer perceptron (MLP) network with two fully connected layers, an activation function, and a reduction ratio r (the same applied to F mean p and F max p ). The shared MLP's outputs are summed element-wise to generate the final attention score vector A c ? R P ?1?C and A p ? R P ?N ?1 :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Architecture of Mini-BiFPN module. The input pseudo-image goes through convolutional blocks, then the outputs are aggregated using bidirectional connections and concatenated. The final feature map is sent to a SSD<ref type="bibr" target="#b21">[22]</ref> detection head for final predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Qualitative results on JRDB 2022 official test set. Very crowded indoor (A) and outdoor (B) settings are visualised (2D panorama image at the top, 3D point cloud at the bottom). Prediction bounding boxes (with red cross showing heading direction) are shown in red. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Visualization on KITTI val set (A, B) and test set (C, D). Each visualization includes BEV (right), 3D point clouds (top), and 2D image (bottom). The ground truth boxes are green, the predicted boxes are red with yellow confident scores. B and D are typical val and test set failures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I COMPARISON</head><label>I</label><figDesc>WITH PREVIOUS APPROACHES FOR 3D OBJECT DETECTION TASK ON THE KITTI OFFICIAL test SPLIT FOR Pedestrian ON BIRDS-EYE-VIEW (BEV) AND 3D DETECTION TASKS. THE AP IS CALCULATED USING 40 RECALL POSITIONS, AND 3D MAP REPRESENTS THE MEAN AVERAGE PRECISION OF THE THREE DIFFICULTIES. "*" DENOTES RUN-TIME IS CITED FROM KITTI OFFICIAL WEBSITE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>State-of-the-arts KITTI detection benchmark. As shown in Tab. I, PiFeNet outperforms all the existing state-of-the-art approaches in BEV detection. Our model outperforms TANet [23] by 2.64 mAP in BEV and 2.22 mAP in 3D detection. It also surpasses the current strongest BEV pillar-based detector Frustum-PointPillars [26] by 2.06 mAP in BEV pedestrian detection</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II PIFENET</head><label>II</label><figDesc>PERFORMANCE ON OFFICIAL NUSCENES TEST SET COMPARED TO OTHER APPROACHES. BOLD VALUES INDICATE THE BEST PERFORMANCE.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>, PiFeNet can elegantly distinguish people from background objects in a very crowded 2 https://jrdb.erc.monash.edu/leaderboards/detection 3 https://jrdb.erc.monash.edu/leaderboards/detection22</figDesc><table><row><cell></cell><cell>Method</cell><cell>AP@0.3</cell><cell>AP@0.5</cell><cell>AP@0.7</cell></row><row><cell></cell><cell>F-Pointnet [27]</cell><cell>38.205</cell><cell>6.378</cell><cell>0.081</cell></row><row><cell>JRDB 2019</cell><cell>EPNet [14]</cell><cell>59.252</cell><cell>16.845</cell><cell>0.418</cell></row><row><cell></cell><cell>TANet++ [23]</cell><cell>63.922</cell><cell>27.991</cell><cell>1.842</cell></row><row><cell></cell><cell>PiFeNet (ours)</cell><cell>74.284</cell><cell>42.617</cell><cell>4.886</cell></row><row><cell>JRDB 2022</cell><cell>PointPillars* [18]</cell><cell>69.209</cell><cell>33.677</cell><cell>2.209</cell></row><row><cell></cell><cell>PiFeNet (ours)</cell><cell>70.724</cell><cell>39.838</cell><cell>4.59</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE III PIFENET</head><label>III</label><figDesc>PERFORMANCE ON OFFICIAL THE JRDB 2019 AND NEWLY LAUNCHED JRDB 2022 TEST SETS COMPARED TO OTHER APPROACHES. provide the results of extensive ablation experiments to evaluate the efficacy and contribution of each component in PiFeNet. The experiments are conducted using the official KITTI val split. We use Pointpillars<ref type="bibr" target="#b17">[18]</ref> as our baseline. The results of the ablation studies on point-wise attention, channelwise attention, task-aware attention, and pooling strategies are presented in the following sections. Because this method aims to improve the performance of 3D object detectors in classifying and localising pedestrians, we use the class "Pedestrian" as a benchmark to evaluate the model's attention modules. Our baseline without any attention mechanisms can</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="5">REPRODUCED RESULT</cell><cell></cell><cell></cell></row><row><cell cols="10">indoor scene, thanks to the improved representation and multi-</cell></row><row><cell cols="7">scale communication between pillars.</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">V. ABLATION STUDIES</cell><cell></cell><cell></cell></row><row><cell cols="2">Next, we Pooling</cell><cell></cell><cell>Modules</cell><cell></cell><cell></cell><cell></cell><cell>Pedestrian</cell><cell></cell><cell></cell></row><row><cell>Avg</cell><cell>Max</cell><cell>Point-</cell><cell>Channel-</cell><cell>Task-</cell><cell>Easy</cell><cell>Medium</cell><cell>Hard</cell><cell>mAP</cell><cell>FPS</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>66.7</cell><cell>61.1</cell><cell>56.5</cell><cell>61.4</cell><cell>41.5</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>69.6</cell><cell>63.4</cell><cell>58.1</cell><cell>63.7</cell><cell>36.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>67.3</cell><cell>62.1</cell><cell>56.4</cell><cell>61.9</cell><cell>35.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>68.2</cell><cell>61.4</cell><cell>55.9</cell><cell>61.8</cell><cell>31.2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>67.3</cell><cell>61.4</cell><cell>56.5</cell><cell>61.8</cell><cell>32.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>69.3</cell><cell>63.3</cell><cell>58.1</cell><cell>63.6</cell><cell>28.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>68.3</cell><cell>62.4</cell><cell>56.6</cell><cell>62.4</cell><cell>28.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>68.2</cell><cell>62.2</cell><cell>57.3</cell><cell>62.6</cell><cell>28.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>69.6</cell><cell>61.9</cell><cell>55.4</cell><cell>62.3</cell><cell>27.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>69.7</cell><cell>64.0</cell><cell>58.7</cell><cell>64.2</cell><cell>27.4</cell></row></table><note>*</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE IV ABLATION</head><label>IV</label><figDesc></figDesc><table /><note>STUDY ON PAA MODULE'S COMPONENTS.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE V CONTRIBUTION</head><label>V</label><figDesc>OF MODULES TO PIFENET</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">cvlibs.net/datasets/kitti/eval object.php?obj benchmark=bev</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Cia-ssd: Confident iou-aware single-stage object detector from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attention augmented convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3286" to="3295" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">nuscenes: A multimodal dataset for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">E</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="11621" to="11631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Object as hotspots: An anchor-free 3d object detection approach via firing of hotspots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="68" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Fast point r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
			<biblScope unit="page" from="9775" to="9784" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Dynamic relu</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="351" to="367" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention in attention networks for person retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Point attention network for semantic segmentation of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Gilani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">107446</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dual attention network for scene segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3146" to="3154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Structure aware single-stage 3d object detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11873" to="11882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Coordinate attention for efficient mobile network design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="13713" to="13722" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. pp</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Epnet: Enhancing point features with image semantics for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="35" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="603" to="612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint 3d proposal generation and object detection from view aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mozifian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harakeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Improving 3d object detection for pedestrians with virtual multi-view synthesis orientation estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Pon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Walsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Waslander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3459" to="3466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pointpillars: Fast encoders for object detection from point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12697" to="12705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Anchor-free 3d single stage detector with mask-guided attention for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.03634</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">From voxel to point: Iouguided 3d object detection for point cloud with voxel-to-point decoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.03648</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-task multisensor fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7345" to="7353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Tanet: Robust 3d object detection from point clouds with triple attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="11677" to="11684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Jrdb: A dataset and benchmark of egocentric robot visual perception of humans in built environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Martin-Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shenoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Frankel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Frustumpointpillars: A multi-stage approach for 3d object detection using rgb camera and lidar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paigwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sierra-Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Erkent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Laugier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2926" to="2933" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Frustum pointnets for 3d object detection from rgb-d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="918" to="927" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Pv-rcnn: Point-voxel feature set abstraction for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10529" to="10538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pointrcnn: 3d object proposal generation and detection from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="770" to="779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Point-gnn: Graph neural network for 3d object detection in a point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rajkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1711" to="1719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Super-convergence: Very fast training of neural networks using large learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Topin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications</title>
		<imprint>
			<biblScope unit="volume">11006</biblScope>
			<biblScope unit="page">1100612</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>International Society for Optics and Photonics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Bottleneck transformers for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="16519" to="16529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficientdet: Scalable and efficient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10781" to="10790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pointpainting: Sequential fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Helou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4604" to="4612" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<title level="m">Non-local neural networks. In: CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Frustum convnet: Sliding frustums to aggregate local point-wise features for amodal 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1742" to="1749" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Cbam: Convolutional block attention module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Pi-rcnn: An efficient multi-sensor 3d object detector with point-based attentive cont-conv fusion module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>AAAI</publisher>
			<biblScope unit="page" from="12460" to="12467" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Second: Sparsely embedded convolutional detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3337</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">3dssd: Point-based 3d single stage object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="11040" to="11048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Std: Sparse-to-dense 3d object detector for point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1951" to="1960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">3d-cvf: Generating joint camera and lidar features using cross-view spatial feature fusion for 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="720" to="736" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXVII 16</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09164</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Point transformer</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Se-ssd: Self-ensembling single-stage object detector from point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14494" to="14503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Voxelnet: End-to-end learning for point cloud based 3d object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4490" to="4499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.09492</idno>
		<title level="m">Class-balanced grouping and sampling for point cloud 3d object detection</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
