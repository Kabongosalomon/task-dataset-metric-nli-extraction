<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Word-level Deep Sign Language Recognition from Video: A New Large-scale Dataset and Methods Comparison</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
							<email>dongxu.li@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision (ACRV)</orgName>
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Rodriguez Opazo</surname></persName>
							<email>cristian.rodriguez@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision (ACRV)</orgName>
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
							<email>xin.yu@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision (ACRV)</orgName>
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
							<email>hongdong.li@anu.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Australian Centre for Robotic Vision (ACRV)</orgName>
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Word-level Deep Sign Language Recognition from Video: A New Large-scale Dataset and Methods Comparison</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision-based sign language recognition aims at helping deaf people to communicate with others. However, most existing sign language datasets are limited to a small number of words. Due to the limited vocabulary size, models learned from those datasets cannot be applied in practice. In this paper, we introduce a new large-scale Word-Level American Sign Language (WLASL) video dataset, containing more than 2000 words performed by over 100 signers. This dataset will be made publicly available to the research community. To our knowledge,it is by far the largest public ASL dataset to facilitate word-level sign recognition research.</p><p>Based on this new large-scale dataset, we are able to experiment with several deep learning methods for wordlevel sign recognition and evaluate their performances in large scale scenarios. Specifically we implement and compare two different models,i.e., (i) holistic visual appearance based approach, and (ii) 2D human pose based approach. Both models are valuable baselines that will benefit the community for method benchmarking. Moreover, we also propose a novel pose-based temporal graph convolution networks (Pose-TGCN) that model spatial and temporal dependencies in human pose trajectories simultaneously, which has further boosted the performance of the pose-based method. Our results show that pose-based and appearance-based models achieve comparable performances up to 62.63% at top-10 accuracy on 2,000 words/glosses, demonstrating the validity and challenges of our dataset. Our dataset and baseline deep models are available at https://dxli94.github.io/ WLASL/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Sign languages, as a primary communication tool for the deaf community, have their unique linguistic structures. Sign language interpretation methods aim at auto-Figure 1: ASL signs "read" (top row) and "dance" (bottom row) <ref type="bibr" target="#b13">[14]</ref> differ only in the orientations of the hands. matically translating sign languages using, for example, vision techniques. Such a process involves mainly two tasks, namely, word-level sign language recognition (or "isolated sign language recognition") and sentence-level sign language recognition (or "continuous sign language recognition"). In this paper, we target at word-level recognition task for American Sign Language (ASL) considering that it is widely adopted by deaf communities over 20 countries around the world <ref type="bibr" target="#b44">[45]</ref>.</p><p>Serving as a fundamental building block for understanding sign language sentences, the word-level sign recognition task itself is also very challenging:</p><p>? The meaning of signs mainly depends on the combination of body motions, manual movements and head poses, and subtle differences may translate into different meanings. As shown in <ref type="figure">Fig. 1</ref>, the signs for "dance" and "read" only differ in the orientations of hands.</p><p>? The vocabulary of signs in daily use is large and usually in the magnitude of thousands. In contrast, related tasks such as gesture recognition <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b0">1]</ref> and action recognition <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b11">12]</ref> only contains at most a few hundred categories. This greatly challenges the scalability of recognition methods.</p><p>? A word in sign language may have multiple coun-terparts in natural languages. For instance, the sign shown in <ref type="figure" target="#fig_2">Fig. 2 (a)</ref>, can be interpreted as "wish" or "hungry" depending on the context. In addition, nouns and verbs that are from the same lemma may have the same sign. These subtleties are not well captured in the existing small-scale datasets. In order to learn a practical ASL recognition model, the training data needs to contain a sufficient number of classes and training examples. Considering that existing word-level datasets do not provide a large-scale vocabulary of signs, we firstly collect large-scale word-level signs in ASL as well as their corresponding annotations. Furthermore, since we want to leverage the minimal hardware requirement for the sign recognition, only monocular RGB-based videos are collected from the Internet. By doing so, the trained sign recognition models do not rely on special equipment, such as depth cameras <ref type="bibr" target="#b32">[33]</ref> and colored gloves <ref type="bibr" target="#b50">[51]</ref>, and can be deployed in general cases. Moreover, when people communicate with each other, they usually sign in frontal views. Thus, we only collect videos with signers in near-frontal views to achieve a high-quality large-scale dataset. In addition, our dataset contains annotations for dialects that are commonly-used in ASL. In total, our proposed WLASL dataset consists 21,083 videos performed by 119 signers, and each video only contains one sign in ASL. Each sign is performed by at least 3 different signers. Thus, inter-signer variations in our dataset facilitates the generalization ability of the trained sign recognition models.</p><p>Based on WLASL, we are able to experiment with several deep learning methods for word-level sign recognition, based on (i) holistic visual appearance, and (ii) 2D humanpose. For appearance-based methods, we provide a baseline by re-training VGG backbone <ref type="bibr" target="#b56">[57]</ref> and GRU <ref type="bibr" target="#b16">[17]</ref> as a representative for convolutional recurrent networks. We also provide a 3D convolution networks baseline using finetuned I3D <ref type="bibr" target="#b11">[12]</ref>, which performs better than the VGG-GRU baseline. For pose-based methods, we firstly extract human poses from original videos and use them as input features. We provide a baseline using GRU to model the temporal movements of the poses. Giving that GRU captures explicitly only the temporal information in pose trajectories, it may not fully utilizes the spatial relationship between body keypoints. Motivated by this, we propose a novel pose-based model temporal graph convolutional network (TGCN) that captures the temporal and spatial dependencies in the pose trajectories simultaneously. Our results show that both pose-based approach and appearance-based approach achieve comparable classification performance on 2,000 words, reaching up to 62.63%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>In this section, we briefly review some existing publicly sign language datasets, and state-of-the-art sign language (a) The verb "Wish" (top) and the adjective "hungry" (bottom) correspond to the same sign.   recognition algorithms are also discuss to demonstrate the necessity of a large-scale ASL dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Sign Language Datasets</head><p>There are three publicly released word-level ASL datasets 1 , i.e. Purdue RVL-SLLL ASL Database <ref type="bibr" target="#b68">[69]</ref>, Boston ASLLVD <ref type="bibr" target="#b5">[6]</ref> and RWTH-BOSTON-50 <ref type="bibr" target="#b77">[78]</ref>.</p><p>Purdue RVL-SLLL ASL Database [69] contains 39 motion primitives with different hand-shapes that are commonly encountered in ASL. Each primitive is produced by 14 native signers. Note that, the primitives in <ref type="bibr" target="#b68">[69]</ref> are the elements constituting ASL signs but may not necessarily correspond to an English word. Boston ASLLVD <ref type="bibr" target="#b5">[6]</ref> has   <ref type="table" target="#tab_0">Table 1</ref>. All the previously mentioned datasets have their own properties and provide different attempts to tackle the wordlevel sign recognition task. However, they fail to capture the difficulties of the task due to insufficient amount of instance and signer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Sign Language Recognition Approaches</head><p>Existing word-level sign recognition models are mainly trained and evaluated on either private <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48]</ref> or small-scale datasets with less than one hundred words [?, <ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b69">70]</ref>. These sign recognition approaches mainly consists of three steps: the feature ex-traction, temporal-dependency modeling and classification. Previous works first employ different hand-crafted features to represent static hand poses, such as SIFT-based features <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b62">63]</ref>, HOG-based features <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20]</ref> and features in the frequency domain <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7]</ref>. Hidden Markov Models (HMM) <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b58">59]</ref> are then employed to model the temporal relationships in video sequences. Dynamic Time Warping (DTW) <ref type="bibr" target="#b40">[41]</ref> is also exploited to handle differences of sequence lengths and frame rates. Classification algorithms, such as Support Vector Machine (SVM) <ref type="bibr" target="#b46">[47]</ref>, are used to label the signs with the corresponding words.</p><p>Similar to action recognition, some recent works <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b34">35]</ref> use CNNs to extract the holistic features from image frames and then use the extracted features for classification. Several approaches <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36]</ref> first extract body keypoints and then concatenate their locations as a feature vector. The extracted features are then fed into a stacked GRU for recognizing signs. These methods demonstrate the effectiveness of using human poses in the word-level sign recognition task. Instead of encoding the spatial and temporal information separately, recent works also employ 3D CNNs <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b74">75]</ref> to capture spatial-temporal features together. However, these methods are only tested on small-scale datasets. Thus, the generalization ability of those methods remains unknown. Moreover, due to the lack of a standard word-level large-scale sign language dataset, the results of different methods evaluated on different small-scale datasets are not comparable and might not reflect the practical usefulness of models.</p><p>To overcome the above issues in sign recognition, we propose a large-scale word-level ASL dataset, coined WLASL database. Since our dataset consists of RGB-only videos, the algorithms trained on our dataset can be easily applied to real world cases with minimal equipment requirements. Moreover, we provide a set of baselines using state-of-the-art methods for sign recognition to facilitate the evaluation of future works. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Proposed WLASL Dataset</head><p>In this section, we introduce our proposed Word-Level American Sign Language dataset (WLASL). We first explain the data sources and the data collection process. Following with the description of our annotation process which combines automatic detection procedures with manual annotations to ensure the correctness between signs and their annotations. Finally, we provide statistics of our WLASL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dataset Collection</head><p>In order to construct a large-scale signer-independent ASL dataset, we resort to two main sources from Internet. First, there are multiple educational sign language websites, such as ASLU <ref type="bibr" target="#b1">[2]</ref> and ASL-LEX <ref type="bibr" target="#b13">[14]</ref>, and they provide lookup function for ASL signs. The mappings between glosses and signs from those websites are accurate since those videos have been checked by experts before uploaded. Another main source is ASL tutorial videos on YouTube. We select videos whose titles clearly describe the gloss of the sign. In total, we access 68,129 videos of 20,863 ASL glosses from 20 different websites. In each video, a signer performs only one sign (possibly multiple repetitions) in a nearly-frontal view with different backgrounds.</p><p>After collecting all the resources for the dataset, if the gloss annotations are composed of more than two words in English, we will remove those videos to ensure that the dataset contains words only. If the number of the videos for one gloss is less than seven, we also remove that gloss to guarantee that enough samples are split into the training and testing sets. Since most of the websites include daily used words, the small number of video samples for one gloss may imply those words are not frequently used. Therefore, removing those glosses with few video samples will not affect the usefulness of our dataset in practice. After this preliminary selection procedure, we have 34,404 video samples of 3,126 glosses for further annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Annotations</head><p>In addition to providing a gloss label for each video, some meta information, including temporal boundary, body bounding box, signer annotation and sign dialect/variation annotations, is also given in our dataset.</p><p>Temporal boundary: A temporal boundary is used to indicate the start and end frames of a sign. When the videos do not contain repetitions of signs, the boundaries are labelled as the first and last frames of the signs. Otherwise, we manually label the boundaries between the repetitions. For the videos containing repetitions, we only keep one sample of the repeated sign to ensure samples in which the same signer performs the same sign will not appear in both training and testing sets. Thus, we prevent learned models from overfiting to the testing set.</p><p>Body Bounding-box: In order to reduce side-effects caused by backgrounds and let models focus on the signers, we use YOLOv3 <ref type="bibr" target="#b49">[50]</ref> as a person detection tool to identify body bounding-boxes of signers in videos. Note that, the size of the bounding-box will change as a person signs, we use the largest bounding-box size to crop the person from the video.</p><p>Signer Diversity:</p><p>A good sign recognition model should be robust to inter-signer variations in the input data, e.g. signer appearance and signing paces, in order to generalize well to real-world scenarios. For example, as shown in <ref type="figure" target="#fig_2">Fig. 2c</ref>, the same sign is performed with slightly different hand positioning by two signers. From this perspective, sign datasets should have a diversity of signers. Therefore, we identify signers in our collected dataset and then provide the IDs of the signers as the meta information of the videos. To this end, we first employ the face detector and the face embedding provided by FaceNet <ref type="bibr" target="#b52">[53]</ref> to encode faces of the dataset, and then compare the Euclidean distances among the face embeddings. If the distance between two embeddings is lower than our pre-defined threshold (i.e., 0.9), we consider those two videos signed by the same person. After automatic labeling, we also manually check the identification results and correct the mislabelled ones.</p><p>Dialect Variation Annotation: Similar to natural languages, ASL signs also have dialect variations <ref type="bibr" target="#b44">[45]</ref> and those variations may contain different sign primitives, such as hand-shapes and motions. To avoid the situation where dialect variations only appear in testing dataset, we manually label the variations for each gloss. Our annotators receive training in advance to ensure that they understand the basic knowledge of ASL, in order to distinguish the differences from the signers variations and dialect variations. To speed up the annotation process and control the annotation quality, we design an interface which lets the annotators only compare signs from two videos displayed simultaneously. Then we count the number of dialects and assign labels for different dialects automatically. After the dialect annotation, we also give each video a dialect label. With the help of the dialect labels, we can guarantee the dialect signs in the testing set have corresponding training samples. We also discard the sign variations with less than five examples since there are not enough samples to be split into training, validation and testing sets. Furthermore, we notice that these variations are usually not commonly used in daily life.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Dataset Arrangement</head><p>After obtaining all the annotations for each video, we obtain videos with lengths ranging from 0.36 to 8.12 seconds, and the average length of all the videos is 2.41 seconds. The average intra-class standard deviation of the videos is 0.85 seconds.</p><p>We sort the glosses in a descending order in terms of the sample number of a gloss. To provide better understanding on the difficulties of the word-level sign recognition task and the scalability of sign recognition methods, we conduct experiments on the datasets with different vocabulary sizes. In particular, we select top-K glosses with K = {100, 300, 1000, 2000}, and organize them to four subsets, named WLASL100, WLASL300, WLASL1000 and WLASL2000, respectively.</p><p>In <ref type="table" target="#tab_1">Table 2</ref>, we present statistics of the four subsets of WLASL. As indicated by <ref type="table" target="#tab_1">Table 2</ref>, we acquire 21,083 video samples with a duration of around 14 hours for WLASL2000 in total, and each gloss in WLASL2000 has 10.5 samples on average, which is almost three times larger than the existing large-scale dataset Boston ASLLVD. We show example frames of our dataset in <ref type="figure" target="#fig_3">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method Comparison on WLASL</head><p>Signing, as a part of human actions, shares similarities with human action recognition and pose estimation. In this section, we first introduce some relevant works on action recognition and human pose estimation. Inspired by network architectures of action recognition, we employ imageappearance based and pose based baseline models for wordlevel sign recognition. By doing so, we not only investigate the usability of our collected dataset but also exam the sign recognition performance of deep models based on different modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image-appearance based Baselines</head><p>Early approaches employ handcrafted features to represent the spatial-temporal information from image frames and then ensemble them as a high-dimensional code for classification <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b66">67]</ref>.</p><p>Benefiting from the powerful feature extraction ability of deep neural networks, the works <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b64">65]</ref> exploit deep neural networks to generate a holistic representation for each input frame and then use the representations for recognition. To better establish the temporal relationship among the extracted visual features, Donahue et al. <ref type="bibr" target="#b21">[22]</ref> and Yue et al. <ref type="bibr" target="#b75">[76]</ref> employ use recurrent neural networks (e.g., LSTM). Some works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b9">10]</ref> also employ the joint locations as a guidance to extract local deep features around the joint regions.</p><p>Sign language recognition, especially word-level recognition, needs to focus on detailed differences between signs, such as the orientation of hands and movement direction of the arms, while the background context does not provide any clue for recognition. Motivated by the action recognition methods, we employ two image-based baselines to model the temporal and spatial information of videos in different manners.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">2D Convolution with Recurrent Neural Networks</head><p>2D Convolutional Neural Networks (CNN) are widely used to extract spatial features of input images while Recurrent Neural Networks (RNN) are employed to capture the longterm temporal dependencies among inputs. Thus, our first baseline is constructed by a CNN and a RNN to capture spatio-temporal features from input video frames. In particular, we use VGG16 <ref type="bibr" target="#b56">[57]</ref> pretrained on ImageNet to extract spatial features and then feed the extracted features to a stacked GRU <ref type="bibr" target="#b16">[17]</ref>. This baseline is referred to as 2D Conv RNN, and the network architecture is illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>.</p><p>To avoid overfiting the training set, the hidden sizes of GRU for the four subsets are set to 64, 96, 128 and 256 respectively, and the number of the stacked recurrent layers in GRU is set to 2. In the training phase, we randomly select at most 50 consecutive frames from each video. Cross-entropy losses is imposed on the output at all the time steps as well as the output feature from the average pooling of all the output features. In testing, we consider all the frames in the video and make predictions based on the average pooling of all the output features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">3D Convolutional Networks</head><p>3D convolutional networks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b29">30]</ref> are able to establish not only the holistic representation of each frame but also the temporal relationship between frames in a hierarchical fashion. Carreira et al. <ref type="bibr" target="#b12">[13]</ref> inflate 2D filters of the Inception network <ref type="bibr" target="#b60">[61]</ref> trained on ImageNet <ref type="bibr" target="#b51">[52]</ref>, thus obtaining well-initialized 3D filters. The inflated 3D filters are also fine-tuned on the Kinetics dataset <ref type="bibr" target="#b12">[13]</ref> to better capture the spatial-temporal information in a video.</p><p>In this paper, we employ the network architecture of I3D <ref type="bibr" target="#b12">[13]</ref> as our second image-appearance based baseline, and the network architecture is illustrated in <ref type="figure" target="#fig_4">Figure 4</ref>. As mentioned above, the original I3D network is trained on Ima-geNet <ref type="bibr" target="#b51">[52]</ref> and fine-tuned on Kinetics-400 <ref type="bibr" target="#b12">[13]</ref>. In order to model the temporal and spatial information of the sign language, such as focusing on the hand shapes and orientations as well as arm movements, we need to fine-tune the pre-trained I3D. In this way, the fine-tuned I3D can better capture the spatio-temporal information of signs. Since the class number varies in our WLASL subsets, only the last classification layer is modified in accordance with the class number.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Pose-based Baselines</head><p>Human pose estimation aims at localizing the keypoints or joints of human bodies from a single image or videos. Traditional approaches employ the probabilistic graphical model <ref type="bibr" target="#b72">[73]</ref> or pictorial structures <ref type="bibr" target="#b48">[49]</ref> to estimate singleperson poses. Recently, deep learning techniques have boosted the performance of pose estimation significantly. There are two mainstream approaches: regressing the keypoint positions <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b10">11]</ref>, and estimating keypoint heatmaps followed by a non-maximal suppression technique <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b71">72]</ref>. However, pose estimation only provides the locations of the body keypoints, while the spatial dependencies among the estimated keypoints are not explored.</p><p>Several works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b65">66]</ref> exploit human poses to recognize actions. The works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b65">66]</ref> represent the locations of body joints as a feature representation for recognition. These methods can obtain high recognition accuracy when the oracle annotations of the joint locations are provided. In order to exploit the pose information for SLR, the spatial and temporal relationships among all the keypoints require further investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Pose based Recurrent Neural Networks</head><p>Pose based approaches mainly utilize RNNs <ref type="bibr" target="#b43">[44]</ref> to model the pose sequences for analyzing human motions. Inspired by this idea, our first pose-based baseline employs RNN to model the temporal sequential information of the pose movements, and the representation output by RNN is used for the sign recognition.</p><p>In this work, we extract 55 body and hand 2D keypoints from a frame on WLASL using OpenPose <ref type="bibr" target="#b8">[9]</ref>. These keypoints include 13 upper-body joints and 21 joints for both left and right hands as defined in <ref type="bibr" target="#b8">[9]</ref>. Then, we concatenate all the 2D coordinates of each joint as the input feature and feed it to a stacked GRU of 2 layers. In the design of GRUs, we use the empirically optimized hidden sizes of 64, 64, 128 and 128 for the four subsets respectively. Similar to the training and testing protocols in Section 4.1.1, 50 consecutive frames are randomly chosen from the input video. Cross-entropy losses is employed for training. In testing, all the frames in a video are used for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Pose Based Temporal Graph Neural Networks</head><p>We introduce a novel pose-based approach to ISLR using Temporal Graph Convolution Networks (TGCN). Consider the input pose sequence X 1:N = [x 1 , x 2 , x 3 , ..., x N ] in N sequential frames, where x i ? R K represents the concatenated 2D keypoint coordinates in dimension K. We propose a new graph network based architecture that models the spatial and temporal dependencies of the pose sequence. Different from existing works on human pose estimation, which usually model motions using 2D joint angles, we encode temporal motion information as a holistic representation of the trajectories of body keypoints.</p><p>Motivated by the recent work on human pose forecasting <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b15">16]</ref>, we view a human body as a fully-connected graph with K vertices and represent the edges in the graph as a weighted adjacency matrix A ? R K?K . Although a human body is only partially connected, we construct the human body as fully-connected graph in order to learn the dependencies among joints via a graph network. In a deep graph convolutional network, the n-th graph layer is a function G n that takes as input features a matrix H n ? R K?F , where F is the feature dimension output by its previous layer. In the first layer, the networks takes as input the K ? 2N matrix coordinates of body keypoints. Given this formulation and a set of trainable weights W n ? R F ?F , a graph convolutional layer is expressed as:</p><formula xml:id="formula_0">H n+1 = G n (H n ) = ?(A n H n W n ),<label>(1)</label></formula><p>where A n is a trainable adjacency matrix for n-th layer and ?(?) denotes the activation function tanh(?). A residual graph convolutional block stacks two graph convolutional layers with a residual connection as shown in <ref type="figure" target="#fig_6">Fig. 5</ref>. Our  proposed TGCN stacks multiple residual graph convolutional blocks and takes the average pooling result along the temporal dimension as the feature representation of pose trajectories. Then a softmax layer followed by the average pooling layer is employed for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training and Testing Protocol</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Data Pre-processing and Augmentation</head><p>We resize the resolution of all original video frames such that the diagonal size of the person bounding-box is 256 pixels. For training VGG-GRU and I3D, we randomly crop a 224 ? 224 patch from an input frame and apply a horizontal flipping with a probability of 0.5. Note that, the same crop and flipping operations are applied to the entire video frames instead of in a frame-wise manner. Similar to <ref type="bibr" target="#b11">[12]</ref>, when training VGG-GRU, Pose-GRU and Pose-TGCN, for each video consecutive 50 frames are randomly selected and the models are asked to predict labels based on only partial observations of the input video. In doing so, we increase the discriminativeness of the learned model. For I3D, we follow its original training configuration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Implementation details</head><p>The models, i.e., VGG-GRU, Pose-GRU, Pose-TGCN and I3D are implemented in PyTorch. It is important to notice that we use the I3D pre-train weights provided by Carreira et al. <ref type="bibr" target="#b12">[13]</ref>.We train all the models with Adam optimizer <ref type="bibr" target="#b33">[34]</ref>. Note that, I3D was trained by stochastic gradient descent (SGD) in <ref type="bibr" target="#b11">[12]</ref>. However, I3D does not converge when using SGD to fine-tune it in our experiments. Thus, Adam is employed to fine-tune I3D. All the models are trained with 200 epochs on each subset. We terminate the training process when the validation accuracy stops increasing.</p><p>We split the samples of a gloss into the training, validation and testing sets following a ratio of 4:1:1. We also ensure each split has at least one sample per gloss. The split information will be released publicly as part of WLASL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Evaluation Metric</head><p>We evaluate the models using the mean scores of top-K classification accuracy with K = {1, 5, 10} over all the sign instances. As seen in <ref type="figure" target="#fig_2">Figure 2</ref>, different meanings have very similar sign gestures, and those gestures may cause errors in the classification results. However, some of the erroneous classification can be rectified by contextual information. Therefore, it is more reasonable to use top-K predicted labels for the word-level sign language recognition. <ref type="table" target="#tab_2">Table 3</ref> indicates that the performance of our baseline models based on poses and image-appearance. The results demonstrate that our pose-based TGCN further improves the classification accuracy in comparison to the pose-based sign recognition method Pose-GRU. This indicates that our proposed pose-TGCN captures both spatial and temporal relationships of the body keypoints since Pose-GRU mainly explores the temporal dependencies of the keypoints for classification. On the other hand, our fine-tuned I3D model achieves better performance compared to the other imageappearance based model VGG-GRU since I3D has larger network capacity and is pretrained on not only ImageNet but also Kinetics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Performance Evaluation of Baseline Networks</head><p>Although I3D is larger than our TGCN, Pose-TGCN can still achieve comparable results with I3D at top-5 and top-10 accuracy on the large-scale subset WLASL2000. This demonstrates that our TGCN effectively encodes human motion information. Since we use an off-the-shelf pose estimator <ref type="bibr" target="#b8">[9]</ref>, the erroneous estimation of poses may degrade the recognition performance. In contrast, image appearance-based baselines are trained in an end-to-end fashion for sign recognition and thus the errors residing in spatial features can be reduced during training. Therefore, training pose-based baselines in an end-to-end fashion could further improve the recognition performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Effect of Vocabulary Size</head><p>As seen in <ref type="table" target="#tab_2">Table 3</ref>, our baseline methods can achieve relatively high classification accuracy on small-size subsets. i.e., WLASL100 and WLASL300. However, the subset WLASL2000 is very close to the real-world word-level classification scenario due to its large vocabulary. Pose-GRU, pose-TGCN and I3D achieve similar performance on WLASL2000. This implies that the recognition performance on small vocabulary datasets does not reflect the model performance on large vocabulary datasets, and the large-scale sign language recognition is very challenging.</p><p>We also evaluate how the class number, i.e., vocabulary size, impacts on the model performance. There are two  factors mainly affecting the performance: (i) deep models themselves favor simple and easy tasks, and thus they perform better on smaller datasets. As indicated in <ref type="table" target="#tab_2">Table 3</ref>, the models trained on smaller vocabulary size sets perform better than larger ones (comparing along columns); (ii) the dataset itself has ambiguity. Some signs, as shown in <ref type="figure" target="#fig_2">Figure 2</ref>, are hard to recognize by even humans, and thus deep models will be also misled by those classes. As the number of classes increases, there will be more ambiguous signs.</p><p>In order to explain the impacts of the second factor, we dissect the models, i.e., I3D and Pose-TGCN, trained on WLASL2000. Here, we test our models on the WLASL100, WLASL300, WLASL1000 and WLASL2000. As seen in <ref type="table" target="#tab_3">Table 4</ref>, when the test class number is smaller, the models achieve higher accuracy (comparing along rows). The experiments imply that as the number of classes decreases, the number of ambiguous signs becomes smaller, thus making classification easier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Effect of Sample Numbers</head><p>As the class number in the dataset increases, training a deep model requires more samples. However, as illustrated in Table 1, although in our dataset each gloss contains more samples than other datasets, the number of training examples per class is still relatively small compared to some largescale generic activity recognition datasets <ref type="bibr" target="#b24">[25]</ref>. This brings some difficulties for the network training. Note that, the average training samples for each gloss in WLASL100 are twice large as those in WLASL2000. Therefore, models obtain better classification performance on the glosses with more samples, as indicated in <ref type="table" target="#tab_2">Table 3 and Table 4</ref>. Crowdsourcing via Amazon Mechanism Tucker (AMT) is a popular way to collect data. However, annotating ASL requires specific domian knowledge and makes crowdsourcing infeasible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a large-scale Word-Level ASL (WLASL) dataset covering a wide range of daily words and evaluated the performance of deep learning based methods on it. To the best of our knowledge, our dataset is the largest public ASL dataset in terms of the vocabulary size and the number of samples for each class. Since understanding sign language requires very specific domain knowledge, labelling a large amount of samples per class is unaffordable. After comparisons among deep sign recognition models on WLASL, we conclude that developing word-level sign language recognition algorithms on such a large-scale dataset requires more advanced learning algorithms, such as fewshot learning. In our future work, we also aim at utilizing word-level annotations to facilitate sentence-level and story-level machine sign translations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(b) The same sign represents different words "Rice" (top) and "soup" (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(c) Signers perform "Scream" with different hand positions and amplitude of hand movements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Ambiguity and variations of Signing. (a, b) shows linguistic ambiguity in ASL. (c) shows signing variations of different signers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Illustrations of the diversity of our dataset, which contains different backgrounds, illumination conditions and signers with different appearances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Illustrations of our baseline architectures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Residual Graph Convolution Block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Overview of word-level datasets in other languages.</figDesc><table><row><cell>Datasets</cell><cell cols="3">#Gloss #Videos #Signers</cell><cell>Type</cell><cell>Sign Language</cell></row><row><cell>LSA64 [51]</cell><cell>64</cell><cell>3,200</cell><cell>10</cell><cell>RGB</cell><cell>Argentinian</cell></row><row><cell>PSL Kinect 30 [34]</cell><cell>30</cell><cell>300</cell><cell>-</cell><cell>RGB, depth</cell><cell>Polish</cell></row><row><cell>PSL ToF [34]</cell><cell>84</cell><cell>1,680</cell><cell>-</cell><cell>RGB, depth</cell><cell>Polish</cell></row><row><cell>DEVISIGN [15]</cell><cell>2,000</cell><cell>24,000</cell><cell>8</cell><cell>RGB, depth</cell><cell>Chinese</cell></row><row><cell>GSL [24]</cell><cell>20</cell><cell>840</cell><cell>6</cell><cell>RGB</cell><cell>Greek</cell></row><row><cell>DGS Kinect [3]</cell><cell>40</cell><cell>3,000</cell><cell>15</cell><cell>RGB, depth</cell><cell>German</cell></row><row><cell>LSE-sign [27]</cell><cell>2,400</cell><cell>2,400</cell><cell>2</cell><cell>RGB</cell><cell>Spanish</cell></row><row><cell cols="6">2,742 words (i.e., glosses) with 9,794 examples (3.6 exam-</cell></row><row><cell cols="6">ples per gloss on average). Although the dataset has large</cell></row><row><cell cols="6">coverage of the vocabulary, more than 2,000 glosses have at</cell></row><row><cell cols="6">most three examples, which is unsuitable to train thousand-</cell></row><row><cell cols="6">way classifiers. RWTH-BOSTON-50 [78] contains 483</cell></row><row><cell cols="6">samples of 50 different glosses performed by 2 signers.</cell></row><row><cell cols="6">Moreover, RWTH-BOSTON-104 provides 200 continu-</cell></row><row><cell cols="6">ous sentences signed by 3 signers which in total cover 104</cell></row><row><cell cols="6">signs/words. RWTH-BOSTON-400, as a sentence-level</cell></row><row><cell cols="6">corpus, consists of 843 sentences including around 400</cell></row><row><cell cols="6">signs, and those sentences are performed by 5 signers. DE-</cell></row><row><cell cols="6">VISIGN is a large-scale word-level Chinese Sign Language</cell></row><row><cell cols="6">dataset, consists of 2,000 words and 24,000 examples per-</cell></row><row><cell cols="6">formed by 8 non-native signers in controlled lab environ-</cell></row><row><cell cols="6">ment. Word-level sign language datasets exist for other re-</cell></row><row><cell cols="6">gions, as summarized word-level sign language datasets in</cell></row><row><cell cols="2">other languages in</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparisons of our WLASL dataset with existing ASL datasets. Column "Mean" indicates the average number of video samples per gloss.</figDesc><table><row><cell>Datasets</cell><cell cols="5">#Gloss #Videos Mean #Signers Year</cell></row><row><cell>Purdue RVL-SLLL [69]</cell><cell>39</cell><cell>546</cell><cell>14</cell><cell>14</cell><cell>2006</cell></row><row><cell>RWTH-BOSTON-50 [78]</cell><cell>50</cell><cell>483</cell><cell>9.7</cell><cell>3</cell><cell>2005</cell></row><row><cell>Boston ASLLVD [6]</cell><cell>2,742</cell><cell>9,794</cell><cell>3.6</cell><cell>6</cell><cell>2008</cell></row><row><cell>WLASL100</cell><cell>100</cell><cell>2,038</cell><cell>20.4</cell><cell>97</cell><cell>2019</cell></row><row><cell>WLASL300</cell><cell>300</cell><cell>5,117</cell><cell>17.1</cell><cell>109</cell><cell>2019</cell></row><row><cell>WLASL1000</cell><cell>1,000</cell><cell>13,168</cell><cell>13.2</cell><cell>116</cell><cell>2019</cell></row><row><cell>WLASL2000</cell><cell>2,000</cell><cell>21,083</cell><cell>10.5</cell><cell>119</cell><cell>2019</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Top-1, top-5, top-10 accuracy (%) achieved by each model (by row) on the four WLASL subsets. top-5 top-10 top-1 top-5 top-10 top-1 top-5 top-10 top-1 top-5 top-10 Pose-GRU 46.51 76.74 85.66 33.68 64.37 76.05 30.01 58.42 70.15 22.54 49.81 61.38 Pose-TGCN 55.43 78.68 87.60 38.32 67.51 79.64 34.86 61.73 71.91 23.65 51.75 62.24 VGG-GRU 25.97 55.04 63.95 19.31 46.56 61.08 14.66 37.31 49.36 8.44 23.58 32.58 I3D 65.89 84.11 89.92 56.14 79.94 86.98 47.33 76.44 84.33 32.48 57.31 66.31</figDesc><table><row><cell>Method</cell><cell>WLASL100</cell><cell>WLASL300</cell><cell>WLASL1000</cell><cell>WLASL2000</cell></row><row><cell>top-1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Top-10 accuracy (%) of I3D (and Pose-TGCN when trained (row) and tested (column) on different WLASL subsets. 77.52 86.22 74.25 84.33 71.91 --WLASL2000 72.09 67.83 71.11 65.42 67.32 64.55 66.31 62.24</figDesc><table><row><cell></cell><cell cols="2">WLASL100</cell><cell cols="2">WLASL300</cell><cell cols="2">WLASL1000</cell><cell cols="2">WLASL2000</cell></row><row><cell></cell><cell>I3D</cell><cell>TGCN</cell><cell>I3D</cell><cell>TGCN</cell><cell>I3D</cell><cell>TGCN</cell><cell>I3D</cell><cell>TGCN</cell></row><row><cell>WLASL100</cell><cell cols="2">89.92 87.60</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>WLASL300</cell><cell cols="4">88.37 81.40 86.98 79.64</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">WLASL1000 85.27</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We notice that one paper<ref type="bibr" target="#b31">[32]</ref> aims at providing an ASL dataset containing 1,000 glosses. Since the dataset is not released at the time of preparing the paper, we cannot evaluate and compare with the dataset.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This research is supported in part by the Australia Research Council ARC Centre of Excellence for Robotics Vision (CE140100016), ARC-Discovery (DP 190102261) and ARC-LIEF (190100080). The authors gratefully acknowledge the GPU gift donated by NVIDIA Corporation. We thank all anonymous reviewers for their constructive comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<idno>Accessed: 2019-07-16. 1</idno>
		<ptr target="https://20bn.com/datasets/jester" />
		<title level="m">The 20bn-jester dataset-v1</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asl</forename><surname>University</surname></persName>
		</author>
		<idno>cessed: 2019-07-16. 4</idno>
		<ptr target="http://asluniversity.com/.Ac-" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dataset</forename><surname>Kinect Gesture</surname></persName>
		</author>
		<idno>2019-07-16. 3</idno>
		<ptr target="https://www.microsoft.com/en-us/download/details.aspx?id=52283" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video-based signer-independent arabic sign language recognition using hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Al-Rousan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Assaleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talaa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="990" to="999" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A low power, fully event-based gesture recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Amir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Melano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mckinstry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Di</forename><surname>Nolfo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andreopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Garreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mendoza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7243" to="7252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The american sign language lexicon video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Athitsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neidle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stefan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Thangali</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Indian sign language translator using gesture recognition algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Badhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kulkarni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Graphics, Vision and Information Security (CGVIS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="195" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning sign language by watching tv (using weakly aligned subtitles)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2961" to="2968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">OpenPose: realtime multi-person 2D pose estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08008</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Asl-lex: A lexical database of american sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">K</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Sehyr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Cohen-Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Emmorey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior research methods</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Devisign: Dataset and evaluation for 3d sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wanga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhoub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chena</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Beijing, Tech. Rep</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Action-agnostic human pose forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1423" to="1432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structured feature learning for pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4715" to="4723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1831" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sign language recognition using sub-units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E.-J</forename><surname>Ong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pugeault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2205" to="2231" />
			<date type="published" when="2012-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rpan: An end-to-end recurrent pose-attention network for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3725" to="3734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gslc: Creation and annotation of a greek sign language corpus for hci</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Efthimiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Fotinea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HCI</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Isolated sign language recognition using hidden markov models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grobel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Assan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="162" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lse-sign: A lexical database for spanish sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gutierrez-Sigut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Costello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carreiras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavior Research Methods</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="123" to="137" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sign language recognition using 3d convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE international conference on multimedia and expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3192" to="3199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="221" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Thumos challenge: Action recognition with a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Ms-asl: A large-scale data set and benchmark for understanding american sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R V</forename><surname>Joze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Koller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.01053</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recognition of hand gestures observed by depth cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kapuscinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oszust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wysocki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warchol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Advanced Robotic Systems</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Selfie sign language recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kishore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T K</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Intelligent Systems and Applications</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">63</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Neural sign language translation based on human keypoint estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">2683</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Sign language recognition with recurrent neural network using human keypoint detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-K</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">G</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Research in Adaptive and Convergent Systems</title>
		<meeting>the 2018 Conference on Research in Adaptive and Convergent Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="326" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Appearance based recognition of american sign language using gesture segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lokhande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">On space-time interest points. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="107" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning realistic human actions from movies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marsza?ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rozenfeld</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sign language recognition by combining statistical dtw and independent classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Lichtenauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Hendriks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Reinders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2040" to="2046" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Block-based histogram of optical flow for isolated sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="538" to="545" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automatic recognition of fingerspelled words in british sign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liwicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE computer society conference on computer vision and pattern recognition workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="50" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On human motion prediction using recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2891" to="2900" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The hidden treasure of Black ASL: Its history and structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Mccaskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bayley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hill</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Gallaudet University Press</publisher>
			<pubPlace>Washington, DC</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Scalable asl sign recognition using model-based machine learning and linguistically annotated corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dilsizian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Neidle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th Workshop on the Representation &amp; Processing of Sign Languages: Involving the Language Community, Language Resources and Evaluation Conference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Static hand gesture recognition for sign language alphabets using edge oriented histogram and multi class svm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Subashini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Applications</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gesture and sign language recognition with temporal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pigou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Van Herreweghe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dambre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV) Workshops</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<title level="m">Yolov3: An incremental improvement</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Lsa64: an argentinian sign language dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ronchetti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Quiroga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Estrebou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Lanzarini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosete</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">XXII Congreso Argentino de Ciencias de la Computaci?n</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th ACM international conference on Multimedia</title>
		<meeting>the 15th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="357" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Korean sign language recognition based on image and convolution neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-A</forename><surname>Jang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Image and Graphics Processing</title>
		<meeting>the 2nd International Conference on Image and Graphics Processing</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="52" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Real-time american sign language recognition using desk and wearable computer based video. IEEE Transactions on pattern analysis and machine intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weaver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1371" to="1375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Massachusetts Inst Of Tech Cambridge Dept Of Brain And Cognitive Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Starner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
	<note>Visual recognition of american sign language using hidden markov models</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="140" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Sift-based arabic sign language recognition system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tharwat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gaber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Hassanien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Shahin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Refaat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Afro-european conference for industrial advancement</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="359" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Deeppose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)<address><addrLine>Columbus, OH, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="24" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">An approach to posebased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kl?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng-Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Evaluation of local spatio-temporal features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Ullah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Purdue rvl-slll american sign language database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wilbur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Deep forest-based monocular visual sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1945</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Chinese sign language recognition based on video sequence appearance modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th IEEE Conference on Industrial Electronics and Applications</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">End-to-end learning of deformable mixture of parts and deep convolutional neural networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3073" to="3082" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Articulated pose estimation with flexible mixtures-of-parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Sift based approach on bangla sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yasir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alsadoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elchouemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE 8th International Workshop on Computational Intelligence and Applications (IWCIA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="35" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Recognizing american sign language gestures from within continuous videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huenerfauth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2064" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">American sign language recognition with the kinect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zafrulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Brashear</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Presti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on multimodal interfaces</title>
		<meeting>the 13th international conference on multimodal interfaces</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="279" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Combination of tangent distance and an image distortion model for appearance-based sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zahedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint Pattern Recognition Symposium</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
