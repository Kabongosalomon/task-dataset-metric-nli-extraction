<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CT-NET: CHANNEL TENSORIZATION NETWORK FOR VIDEO CLASSIFICATION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunchang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xianhang</forename><surname>Li</surname></persName>
							<email>xianhangli@knights.ucf.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
							<email>yl.wang@siat.ac.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
							<email>jun.wang@ucf.edu</email>
							<affiliation key="aff4">
								<orgName type="institution">University of Central Florida</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
							<email>yu.qiao@siat.ac.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institutes of Advanced Technology</orgName>
								<orgName type="institution">Guangdong-Hong Kong-Macao Joint Laboratory of Human-Machine Intelligence-Synergy Systems</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Chinese Academy of Sciences</orgName>
								<address>
									<postCode>518055</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">SIAT Branch</orgName>
								<orgName type="department" key="dep2">Shenzhen Institute of Artificial Intelligence and Robotics for Society</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CT-NET: CHANNEL TENSORIZATION NETWORK FOR VIDEO CLASSIFICATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T16:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3D convolution is powerful for video classification but often computationally expensive, recent studies mainly focus on decomposing it on spatial-temporal and/or channel dimensions. Unfortunately, most approaches fail to achieve a preferable balance between convolutional efficiency and feature-interaction sufficiency. For this reason, we propose a concise and novel Channel Tensorization Network (CT-Net), by treating the channel dimension of input feature as a multiplication of K sub-dimensions. On one hand, it naturally factorizes convolution in a multiple dimension way, leading to a light computation burden. On the other hand, it can effectively enhance feature interaction from different channels, and progressively enlarge the 3D receptive field of such interaction to boost classification accuracy. Furthermore, we equip our CT-Module with a Tensor Excitation (TE) mechanism. It can learn to exploit spatial, temporal and channel attention in a high-dimensional manner, to improve the cooperative power of all the feature dimensions in our CT-Module. Finally, we flexibly adapt ResNet as our CT-Net. Extensive experiments are conducted on several challenging video benchmarks, e.g., Kinetics-400, Something-Something V1 and V2. Our CT-Net outperforms a number of recent SOTA approaches, in terms of accuracy and/or efficiency. The codes and models will be available on https://github.com</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>3D convolution has been widely used to learn spatial-temporal representation for video classification <ref type="bibr" target="#b21">(Tran et al., 2015;</ref><ref type="bibr" target="#b0">Carreira &amp; Zisserman, 2017)</ref>. However, over parameterization often makes it computationally expensive and hard to train. To alleviate such difficulty, recent studies mainly focus on decomposing 3D convolution <ref type="bibr" target="#b22">(Tran et al., 2018;</ref>. One popular approach is spatial-temporal factorization <ref type="bibr" target="#b16">(Qiu et al., 2017;</ref><ref type="bibr" target="#b22">Tran et al., 2018;</ref><ref type="bibr" target="#b29">Xie et al., 2018)</ref>, which can reduce overfitting by replacing 3D convolution with 2D spatial convolution and 1D temporal convolution. But it still introduces unnecessary computation burden, since both spatial convolution and temporal convolution are performed over all the feature channels. To further decrease such computation cost, channel separation has been recently developed via operating 3D convolution in the depth-wise manner <ref type="bibr" target="#b23">(Tran et al., 2019)</ref>. However, it inevitably loses accuracy due to the lack of feature interaction between different channels. For compensation, it has to introduce point-wise convolution to preserve interaction with extra computation. So there is a natural question: How to construct effective 3D convolution to achieve a preferable trade-off between efficiency and accuracy for video classification? <ref type="figure">Figure 1</ref>: Simple illustration of channel tensorization (K = 2). We tensorize the channel dimension of input feature as a multiplication of K sub-dimensions. Via performing spatial/temporal tensor separable convolution along each sub-dimension, we can achieve a preferable balance between convolutional efficiency and feature-interaction sufficiency. Introduction shows more explanations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method 3D Convolution Convolutional Efficiency</head><p>Feature-Interaction Sufficiency (t ? h ? w) Spatial-temporal Channel Interact Manner Interact Field 1 C3D F ull : 3 ? 3 ? 3 ST C 3 3 <ref type="bibr" target="#b21">(Tran et al., 2015)</ref> R(2+1)D F ull : 1 ? 3 ? 3 SC 3 3 <ref type="bibr" target="#b22">(Tran et al., 2018)</ref> F ull : 3 ? 1 ? 1 T C CSN F ull : 1 ? 1 ? 1 C 3 3 <ref type="bibr" target="#b23">(Tran et al., 2019)</ref> DW : 3 ? 3 ? 3 ST Our CT-Net</p><formula xml:id="formula_0">C 1 : C 1 ? ? ? ? ? 1 ? (1 ? 3 ? 3 + 3 ? 1 ? 1) ST C 1 (2K + 1) 3 (C = C 1 ? ? ? ? ? C K ) C K : 1 ? ? ? ? ? C K ? (1 ? 3 ? 3 + 3 ? 1 ? 1) ST C K 1</formula><p>Interact Field means the receptive field for feature interaction. <ref type="table">Table 1</ref>: Two design principles to build effective video representation and efficient convolution.</p><p>This paper attempts to address this question by investigating two design principles.</p><p>(1) Convolutional Efficiency. As shown in <ref type="table">Table 1</ref>, current designs of spatial-temporal convolution mainly focus on decomposition from either spatial-temporal <ref type="bibr" target="#b22">(Tran et al., 2018)</ref> or channel dimension <ref type="bibr" target="#b23">(Tran et al., 2019)</ref>. To enhance convolutional efficiency, we consider decomposing convolution in a higher dimension with a novel representation of feature tensor.</p><p>(2) Feature-Interaction Sufficiency. <ref type="table">Table  1</ref> clearly shows that, for current decomposition approaches <ref type="bibr" target="#b22">(Tran et al., 2018;</ref>, feature interaction only contains one or two of spatial, temporal and channel dimensions at each sub-operation. Such a partial interaction manner would reduce classification accuracy. On one hand, it decreases the discriminative power of video representation, due to the lack of joint learning on all the dimensions. On the other hand, it restricts feature interaction in a limited receptive field, which ignores rich context from a larger 3D region. Hence, to boost classification accuracy, each sub-operation should achieve feature interaction on all the dimensions, and the receptive field of such interaction should be progressively enlarged as the number of sub-operations increases.</p><p>Based on these desirable principles, we design a novel and concise Channel Tensorization Module (CT-Module). Specifically, we propose to tensorize the channel dimension of input feature as a multiplication of K sub-dimensions, i.e., C = C 1 ? C 2 ? ? ? ? ? C K . Via performing spatial/temporal separable convolution along each sub-dimension, we can effectively achieve convolutional efficiency and feature-interaction sufficiency. For better understanding, we use the case of K = 2 as a simple illustration in <ref type="figure">Figure 1</ref>. First, we tensorize the input channel into C = C 1 ? C 2 . Naturally, we separate the convolution into distinct ones along each sub-dimension, e.g., for the 1 st sub-dimension, we apply our spatial-temporal tensor separable convolution with the size C 1 ?1?t?h?w, which allows us to achieve convolutional efficiency on all the spatial, temporal and channel dimensions. After that, we sequentially perform the tensor separable convolution sub-dimension by sub-dimension. As a result, we can progressively achieve feature interaction on all the channels, and enlarge the spatialtemporal receptive field. For example, after operating 1 st tensor separable convolution on the 1 st sub-dimension, C 1 channels interact, and 3D receptive field of such interaction is 3 ? 3 ? 3. Via further operating 2 nd tensor separable convolution on the 2 nd sub-dimension, all C 1 ? C 2 = C channels have feature interaction, and 3D receptive field of such interaction becomes 5 ? 5 ? 5. This clearly satisfies our principle of feature-interaction sufficiency.</p><p>We summarize our contributions in the following. First, we design a novel Channel Tensorized Module (CT-Module), which can achieve convolutional efficiency and feature-interaction sufficiency, via progressively performing spatial/temporal tensor separable convolution along each sub-dimension of the tensorized channel. Second, we equip CT-Module with a distinct Tensor Excitation (TE) mechanism, which can further activate the video features of each sub-operation by spatial, temporal and channel attention in a tensor-wise manner. Subsequently, we apply this full module in a residual block, and flexibly adopt 2D ResNet as our Channel Tensorized Network (CT-Net). In this case, we can gradually enhance feature interaction from a broader 3D receptive field, and learn the key spatial-temporal representation with light computation. Finally, we conduct extensive experiments on a number of popular and challenging benchmarks, e.g., Kinetics <ref type="bibr" target="#b0">(Carreira &amp; Zisserman, 2017)</ref>, Something-Something V1 and V2 <ref type="bibr" target="#b5">(Goyal et al., 2017b)</ref>. Our CT-Net outperforms the state-of-the-art methods in terms of classification accuracy and/or computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>2D CNN for video classification. 2D CNN is a straightforward but useful method for video classification <ref type="bibr" target="#b9">(Karpathy et al., 2014;</ref><ref type="bibr" target="#b17">Simonyan &amp; Zisserman, 2014;</ref><ref type="bibr" target="#b25">Wang et al., 2016;</ref><ref type="bibr" target="#b14">Liu et al., 2020;</ref><ref type="bibr" target="#b8">Jiang et al., 2019)</ref>. For example, Two-stream methods <ref type="bibr" target="#b17">(Simonyan &amp; Zisserman, 2014)</ref> learn video representations by fusing the features from RGB and optical flow respectively. Instead of sampling a single RGB frame, TSN <ref type="bibr" target="#b25">(Wang et al., 2016)</ref> proposes a sparse temporal sampling strategy to learn video representations. To further improve accuracy, TSM <ref type="bibr" target="#b13">(Lin et al., 2019)</ref> proposes a zeroparameter temporal shift module to exchange information with adjacent frames. However, these methods may lack the capacity of learning spatial-temporal interaction comprehensively, which often reduces their discriminative power to recognize complex human actions.</p><p>3D CNN for video classification. 3D CNN has been widely used to learn a rich spatial-temporal context better <ref type="bibr" target="#b21">(Tran et al., 2015;</ref><ref type="bibr" target="#b0">Carreira &amp; Zisserman, 2017;</ref><ref type="bibr" target="#b3">Feichtenhofer et al., 2019;</ref><ref type="bibr" target="#b20">Sudhakaran et al., 2020;</ref><ref type="bibr" target="#b2">Feichtenhofer, 2020)</ref>. However, it introduces a lot of parameters, which leads to a difficult optimization problem and large computational load. To resolve this issue, I3D <ref type="bibr" target="#b0">(Carreira &amp; Zisserman, 2017)</ref> inflates all the 2D convolution kernels pre-trained on ImageNet, which is helpful for optimizing. Other works also try to factorize 3D convolution kernel to reduce complexity, such as P3D <ref type="bibr" target="#b16">(Qiu et al., 2017)</ref> and R(2+1)D <ref type="bibr" target="#b22">(Tran et al., 2018)</ref>. Recently, CSN <ref type="bibr" target="#b23">(Tran et al., 2019)</ref> operates 3D convolution in the depth-wise manner. Nevertheless, all these methods still do not achieve a good trade-off between accuracy and efficiency. To tackle this challenge, we propose CT-Net which learns on spatial-temporal and channel dimensions jointly with lower computation than previous methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODS</head><p>In this section, we describe our Channel Tensorization Network (CT-Net) in detail. First, we formally introduce our CT-Module in a generic manner. Second, we design a Tensor Excitation (TE) mechanism to enhance CT-Module. Finally, we flexibly adapt ResNet as our CT-Net to achieve a preferable trade-off between accuracy and efficiency for video classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">CHANNEL TENSORIZATION MODULE</head><p>As discussed in the introduction, the previous approaches have problems in convolutional efficiency or feature-interaction sufficiency. To tackle such a problem, we introduce a generic Channel Tensorization Module (CT-Module), by treating the channel dimension of input feature as a multiplication of K sub-dimensions, i.e., C = C 1 ? C 2 ? ? ? ? ? C K . Naturally, this tensor representation allows to tensorize the kernel size of convolution TConv() as a multiplication of K sub-dimensions, too. To simplify the notation, the channel dimension of the output is omitted by default. The output X out can be calculated as follows:</p><formula xml:id="formula_1">X out = TConv X in , W C1?C2?????C K ?t?h?w</formula><p>(1) where X in and W denote the tensorized input and kernel respectively. However, such an operation requires large computation, so we introduce the tensor separable convolution to alleviate the issue.</p><p>Tensor Separable Convolution. We propose to factorize TConv() along K channel sub-dimensions. Specifically, we decompose TConv() as K tensor separable convolutions TSConv(), and apply TSConv() sub-dimension by sub-dimension as follows:</p><formula xml:id="formula_2">X k = TSConv X k?1 , W 1?????C k ?????1?t?h?w</formula><p>( <ref type="formula">2)</ref> where X 0 = X in and X out = X K . On one hand, the kernel size of the k th TSConv() is</p><formula xml:id="formula_3">(1 ? ? ? ? ? C k ? ? ? ? ? 1 ? t ? h ? w).</formula><p>It illustrates that only C k channels interact in the k th suboperation, which leads to convolution efficiency. On the other hand, as we stack the TSConv(), each convolution performs on the output features of the previous convolution. Therefore, the spatialtemporal receptive field is enlarged. Besides, interactions first occur in C 1 channels, second in C 1 ? C 2 channels and so on. Finally, C 1 ? C 2 ? ? ? ? ? C K = C channels can progressively interact. This clearly satisfies our principle of feature-interaction sufficiency.</p><p>Spatial-Temporal Tensor Separable Convolution. To further improve convolution efficiency, we factorize the 3D TSConv() into 2D spatial TSConv() and 1D temporal TSConv(). Thus, we can obtain the output features X S k and X T k as follows:</p><formula xml:id="formula_4">X S k = S-TSConv X k?1 , W 1?????C k ?????1?1?h?w (3) X T k = T-TSConv X k?1 , W 1?????C k ?????1?t?1?1<label>(4)</label></formula><p>where S-TSConv() and T-TSConv() represent spatial and temporal tensor separable convolution respectively. Finally, we attempt to aggregate spatial and temporal convolution. There are various connection types of spatial and temporal tensor separable convolution, e.g., parallel and serial types.</p><p>According to the results of the experiments in Section 4, we utilize the parallel method, which illustrates that we sum the spatial feature X S k and temporal feature X T k :</p><formula xml:id="formula_5">X k = X S k + X T k<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">TENSOR EXCITATION</head><p>Our CT-Module separates feature along spatial, temporal and channel dimensions. To make full use of their cooperative power to learn distinct video features, we design a concise Tensor Excitation (TE) mechanism for each dimension. First of all, we attempt to utilize the TE mechanism to enhance spatial and temporal features respectively. For the spatial feature X S k obtained by Equation 5, our corresponding spatial TE mechanism can be formulated as:</p><formula xml:id="formula_6">U k = X S k ? Sigmod(S-TSConv(T-Pool(X S k )))<label>(6)</label></formula><p>where T-Pool() represents global temporal pooling, i.e., T ? 1 ? 1 average pooling. By performing it on X S k , we obtain the feature with the size</p><formula xml:id="formula_7">(C 1 ? C 2 ? ? ? ? ? C K ? 1 ? H ? W )</formula><p>, which gathers spatial contexts along temporal dimension. Subsequently, the spatial tensor separable convolution S-TSConv() and the activate function Sigmod() are performed to generate the spatial attention heatmap. Finally, the element-wise multiplication ? broadcasts the spatial attention along the temporal dimension. Similarly, we perform the temporal TE mechanism for the temporal feature X T k :</p><formula xml:id="formula_8">V k = X T k ? Sigmod(T-TSConv(S-Pool(X T k )))<label>(7)</label></formula><p>where S-Pool() and T-TSConv() are global spatial pooling and temporal tensor separable convolution correspondingly. At last, after aggregating the spatial and temporal features by addition, i.e., R k = U k + V k , we perform a channel-wise TE mechanism as follows:</p><formula xml:id="formula_9">X k = R k ? Sigmod(PW-TSConv(S-Pool(R k )))<label>(8)</label></formula><p>We adopt point-wise tensor separable convolution PW-TSConv() to learn the weights for aggregating distinctive channels. The rest follows the previous design. Note that all tensor separable convolutions are performed on the same sub-dimension as the previous convolution, which is essentially different from the SE mechanism <ref type="bibr" target="#b7">(Hu et al., 2020)</ref>. Through the cooperation of the TE mechanism along three different dimensions, the spatial-temporal features can be significantly enhanced. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">CHANNEL TENSORIZATIONN NETWORK</head><p>We regard ResNet as an exemplar and build up our CT-Net from ResNet50 (or ResNet101). First, we design a simple CT-Block in <ref type="figure" target="#fig_0">Figure 2</ref>(a), which adapts the 3 ? 3 convolutional layer in Residual Block (ResBlock) into our CT-Module. It can achieve both convolutional efficiency and featureinteraction sufficiency. Second, we equip our simple CT-Block with the TE mechanism in <ref type="figure" target="#fig_0">Figure  2</ref>(b), forming a full CT-Block that can improve the cooperative power of all the feature dimensions. Besides, extra point-wise convolutions are added between different sub-operations, which are beneficial for more sufficient feature interaction. At last, we build up a novel CT-Net with CT-Block. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>(c), we replace one of every two ResBlocks with our CT-Block in every stage. Such a method guarantees a better balance between efficiency and accuracy in our experiments.</p><p>Discussion. In fact, the popular methods in video classification like C3D, R(2+1)D and CSN <ref type="bibr" target="#b21">(Tran et al., 2015;</ref> can be viewed as special cases of our CT-Net. We can generate different forms by adjusting three hyper-parameters: the number of sub-dimensions (K), the corresponding dimension size (C k ) and the spatial-temporal kernel size (Kernel k ). To degenerate into C3D, we can set K = 1 and Kernel 1 = 3 ? 3 ? 3. When K = 2, Kernel 1 = 1 ? 3 ? 3, Kernel 2 = 3 ? 1 ? 1 and C 1 = C 2 = C without channel tensorization, it becomes R(2+1)D. Unfortunately, because of lacking the decomposition of channels, C3D and R(2+1)D still have a large computational load.</p><formula xml:id="formula_10">When K = 2, C = C 1 ? C 2 = C ? 1, Kernel 1 = 1 ? 1 ? 1, Kernel 2 = 3 ? 3 ? 3, obviously it is equivalent to CSN.</formula><p>However, CSN has a limited receptive field of spatial-temporal interaction. In our CT-Net, we utilize channel tensorization and perform tensor separable convolution along each sub-dimension in turn. Such design can not only preserve interaction among spatial, temporal and channel dimensions but also enlarge the receptive field of feature interaction progressively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS AND RESULTS</head><p>Datasets and implementation details. We conduct experiments on three large video benchmarks: Kinetics-400 <ref type="bibr" target="#b0">(Carreira &amp; Zisserman, 2017)</ref>, Something-Something V1 and V2 <ref type="bibr" target="#b5">(Goyal et al., 2017b)</ref>. We choose ResNet50 and ResNet101 (He et al., 2016) pre-trained on ImageNet as the backbone and the parameters of CT-Module are randomly initialized. For training, we utilize the dense sampling strategy  for Kinetics and sparse sampling strategy <ref type="bibr" target="#b25">(Wang et al., 2016)</ref> for Something-Something. Random scaling and cropping are applied for data argumentation. Finally, we resize the cropped regions to 256 ? 256. For testing, we sample multiple clips per video (4 for Kinetics, 2 for others) for pursuing high accuracy, and average all scores for the final prediction. <ref type="bibr" target="#b21">(Tran et al., 2015)</ref> F ull : 3 ? 3 ? 3 59.9 46.1 75.0 R(2+1)D-Module <ref type="bibr" target="#b22">(Tran et al., 2018)</ref>  Kernel size GFLOPs Top-1 Top-5  We follow the same strategy in Non-local  to pre-process the frames and take 3 crops of 256 ? 256 as input. Because some multi-clip models in <ref type="table" target="#tab_5">Table 3</ref> and <ref type="table" target="#tab_7">Table 4</ref> sample crops of 256? 256, we simply multiply the GFLOPs reported in the corresponding papers by (256/224) 2 for a fair comparison. When considering efficiency, we use just 1 clip per video and the final crop is scaled to 256 ? 256 to ensure comparable GFLOPs. <ref type="table" target="#tab_2">Table 2</ref> shows our ablation studies on Something-Something V1, which is a challenging dataset that requires video architecture to have a robust spatial-temporal representation ability and is suitable to verify the effectiveness of our method. All models use ResNet50 as the backbone.</p><formula xml:id="formula_11">Method 3D Convolution (t ? h ? w) GFLOPs Top-1 Top-5 C3D-Module</formula><formula xml:id="formula_12">C 1 1 ? 1 ? 1 || 1 ? 1 ? 1 C 2 1 ? 3 ? 3 || 3 ? 1 ? 1 35.5 46.1 75.1 C 1 1 ? 1 ? 1 || 1 ? 1 ? 1 C 2 1 ? 5 ? 5 || 5 ? 1 ? 1 36.6 47.2 76.2 C 1 1 ? 3 ? 3 || 3 ? 1 ? 1 C 2 1 ? 3 ? 3 || 3 ? 1 ? 1 36.3 47.3 76.2 C 1 1 ? 3 ? 3 || 3 ? 1 ? 1 C 2 1 ? 5 ? 5 || 5 ? 1 ? 1 37.4 47.5 76.4 C 1 1 ? 5 ? 5 || 5 ? 1 ? 1 C 2 1 ? 5 ? 5 || 5 ? 1 ?</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">ABLATION STUDIES</head><p>Effectiveness of CT-Module. In <ref type="table" target="#tab_2">Table 2a</ref>, we replace the 3?3 convolutional layer in ResNet50 with different modules in recent methods <ref type="bibr" target="#b21">(Tran et al., 2015;</ref>. Compared with CSN-Module, our module achieves a better result with similar computation, which reflects the importance of sufficient feature interaction. Besides, it is slightly better than R(2+1)D-Module with much lower calculation, showing the necessity of efficient convolution. Such results demonstrate the effectiveness of our CT-Module. It illustrates our two design principles give preferable guidance for designing an efficient module for temporal modeling.  <ref type="bibr" target="#b14">(Liu et al., 2020)</ref> 2D R50 16 66 49.9 -62.1 -TEA <ref type="bibr" target="#b12">(Li et al., 2020b)</ref> 2D Res2Net50 16 70 51.9 80.3 --PEM+TDLoss <ref type="bibr" target="#b28">(Weng et al., 2020)</ref> 2D R50+TIM 16 66 50.9 -63.8 -PEM+TDLoss <ref type="bibr" target="#b28">(Weng et al., 2020)</ref> 2D <ref type="formula" target="#formula_5">R50+TIM</ref>   Number of sub-dimensions. Increasing the number of sub-dimensions saves a lot of computation, but the corresponding accuracy first increases and then decreases as shown in <ref type="table" target="#tab_2">Table 2b</ref>. Compared with the 1D method, the 4D method significantly reduces GFLOPs, achieving comparable accuracy.</p><p>As for the decrease of accuracy when K is too large, we argue that the number of channel in the shallow layer is small (64/128), thus there are too few channels in a single dimension, leading to insufficient feature-interaction. Since the 2D method obtains the best trade-off, we set K = 2 in all the following experiments.</p><p>Connection type of spatiotemporal convolution. The coupling 3 ? 3 ? 3 convolution can be decomposed into serial or parallel spatial/temporal convolution. <ref type="table" target="#tab_2">Table 2c</ref> reveals that factorizing the 3D kernel can boost results as expected. Besides, the parallel connection is better, thus we adopt parallel connection as the default.</p><p>Dimension size. As we set K = 2, it is essential to explore the impact of changing the dimension size C2. We can demonstrate that the computation is the lowest when C1 = C2 = ? C. Since C is not always a perfect square number, we adopt the rounded middle size ? C . <ref type="table" target="#tab_2">Table 2d</ref> shows that when C2 = ? C , the model not only requires the fewest computation cost but also achieves the best performance. Hence, we set C2 = ? C naturally.</p><p>Number and location of CT-Blocks. <ref type="table" target="#tab_2">Table 2e</ref> illustrates that simply replacing 1 block in stage5 can bring significant performance improvement (16.9% vs. 42.3%). As we replace more blocks from the bottom up, the GFLOPs continues to decrease. Moreover, the bottom blocks seem to be more beneficial to temporal modeling, since replacing the extra 3 blocks in stage2 and stage3 only improve the accuracy by 0.4% (46.9% vs. 47.3%). Since replacing 7 blocks achieves the highest accuracy, we replace 7 blocks by default.</p><p>Kernel sizes along different dimensions. In <ref type="table" target="#tab_2">Table 2f</ref>, we can observe that two concatenated 3 3 convolution kernels are slightly better than those with the same receptive field (1 3 +5 3 ). Furthermore, the larger kernel size can bring performance improvement but more calculation. It reveals that our CT-Module avoids the limited receptive field of feature interaction, and it can progressively enlarge the receptive field of such interaction on all the dimensions. Considering a better trade-off between accuracy and computation, we choose two concatenated 3 3 convolution kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Backbone #Frame GFLOPs Top-1 Top-5 R(2+1)D <ref type="bibr" target="#b22">(Tran et al., 2018)</ref> 2D R34 32?1?10 1520=152?10 72.0 91.4 TSN <ref type="bibr" target="#b25">(Wang et al., 2016)</ref> Inception 25?10?1 800=80?10 72.5 90.2 I3D <ref type="bibr" target="#b0">(Carreira &amp; Zisserman, 2017)</ref> Inception 64?N/A?N/A 108?N/A 71.1 89.3 TSM <ref type="bibr" target="#b13">(Lin et al., 2019)</ref> 2D R50 16?3?10 2580=86?30 74.7 -TEINet <ref type="bibr" target="#b14">(Liu et al., 2020)</ref> 2D R50 16?3?10 2580=86?30 76.2 92.5 bLVNet-TAM  bLR50 (16?2)?3?3 561=62.3?9 72.0 90.6 TEA <ref type="bibr" target="#b12">(Li et al., 2020b)</ref> 2D Res2Net50 16?3?10 2730=91?30 76.1 92.5 PEM+TDLoss <ref type="bibr" target="#b28">(Weng et al., 2020)</ref> 2D R50+TIM 16?3?10 2580=86?30 76.9 93.0 CorrNet  3D <ref type="formula" target="#formula_5">R50</ref>   Impact of different modules and different spatial resolution. In <ref type="table" target="#tab_2">Table 2g</ref>, our CT-Module can significantly boost its baseline (16.9% vs. 47.3%) and the TE mechanism can further improve the accuracy by 2.1% (48.0% vs. 50.1%). The extra point-wise convolution also boosts performance, which demonstrates that it is beneficial for sufficient feature interaction. Compared with the SE mechanisms, our TE mechanism focuses more on features in different sub-dimensions individually, thus effectively enhancing spatial-temporal features. In our experiments, to ensure GFLOPs is comparable with other methods, we crop the input to 256 ? 256 during testing. <ref type="table" target="#tab_2">Table 2h</ref> shows both training and testing with a larger spatial resolution of input bring clear performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">COMPARISONS WITH THE STATE-OF-THE-ARTS</head><p>Something-Something V1&amp;V2. We make a comprehensive comparison in <ref type="table" target="#tab_5">Table 3</ref>. Compared with NL I3D+GCN 32f , our CT-Net 8f gains 4.0% top-1 accuracy improvement with 49.1? fewer GFLOPs in Something-Something V1. Besides, our CT-Net 8f (51.7%) is better than the ir-CSN 32f (49.3%) which adopts ResNet-152 as the backbone. Moreover, our CT-Net 16f outperforms all the single-clip models in Something-Something V1&amp;V2 and even better than most of the multiclip models. It illustrates that our CT-Net is preferable to capture temporal contextual information efficiently. Surprisingly, with only 280 GFLOPs, our ensemble model CT-Net EN achieves 56.6%(67.8%) top-1 accuracy in Something-Something V1(V2), which outperforms all methods.</p><p>Kinetics-400. Kinetics-400 is a large-scale sence-related dataset, and the lightweight 2D models are usually inferior to the 3D models on it. <ref type="table" target="#tab_7">Table 4</ref> shows our CT-Net-R50 16f can surpass all existing lightweight models based on 2D backbone. Even compared with SlowFast-R50 40f , our CT-Net-R50 16f also achieves higher accuracy (77.3% vs. 76.4%). Note that our reproduced SlowFast-R50 performs worse than that in the paper <ref type="bibr" target="#b3">(Feichtenhofer et al., 2019)</ref>, which may result from the missing videos in Kinetics-400. As for the deeper model, compared with SlowFast-R101 80f , our CT-Net-R101 16f requires 3.7? fewer GFLOPs but gains comparable results (78.8% vs. 78.9%).</p><p>Besides, it achieves comparable top-1 accuracy with X3D-XL (78.8% vs. 79.1%) under a similar GFLOPs. However, X3D requires extensive model searching with an expensive GPU setting, while our CT-Net can be trained traditionally with feasible computation. We perform score fusion over CT-Net-R50 16f and CT-Net-R101 16f , which mimics two-steam fusion with two temporal rates. In this setting, our model is 2.4? faster than SlowFast-R101 80f and shows an 0.9% performance gain (79.8% vs. 78.9%) but only uses 32 frames. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">VISUALIZATION</head><p>We use Saliency Tubes <ref type="bibr" target="#b19">(Stergiou et al., 2019)</ref> to generate the visualization, for it can show the most discriminative features that the network locates. In <ref type="figure" target="#fig_1">Figure 3</ref>, we sample two videos from Something-Something V1 which requires complex temporal modeling. In the left example, our CT-Net focuses on a larger area around the towel, especially in the fourth and fifth frames, thus predicting that someone is twisting it. In contrast, R(2+1)D only concentrates on one side of the towel and gives the wrong judgment. The same situation can be seen in the right example. We argue that CT-Net can localize the action and object accurately thanks to the larger spatial-temporal receptive field. As for CSN, the regions of interest seem to be scattered, because it lacks sufficient spatial-temporal interaction, thus ignoring the rich context both in space and time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS</head><p>In this paper, we construct an efficient tensor separable convolution to learn the discriminative video representation. We view the channel dimension of the input feature as a multiplication of K subdimensions and stack spatial/temporal tensor separable convolution along each of K sub-dimensions. Moreover, CT-Module is cooperated with the Tensor Excitation mechanism to further improve performance. All experiments demonstrate that our concise and novel CT-Net obtains a preferable balance between accuracy and efficiency on large-scale video datasets. Our proposed principles are preferable guidance for designing an efficient module for temporal modeling. We use SGD with momentum 0.9 and cosine learning rate schedule <ref type="bibr" target="#b15">(Loshchilov &amp; Hutter, 2017)</ref> to train the entire network. The first 10 epochs are used for warm-up <ref type="bibr" target="#b4">(Goyal et al., 2017a)</ref> to overcome early optimization difficulty. For kinetics, the batch, total epochs, initial learning rate, dropout and weight decay are set to 64, 110, 0.01, 0.5 and 1e-4 respectively. All these hyper-parameters are set to 64, 45, 0.02, 0.3 and 5e-4 respectively for Something-Something.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ACKNOWLEDGEMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 TENSOR EXCITATION MECHANISM</head><p>The implementation of our Tensor Excitation is shown in <ref type="figure" target="#fig_2">Figure 4</ref>. Different from the SE module, we use tensor separable convolution in the TE mechanism. Moreover, when obtaining the spatial attention, we squeeze the temporal dimension and perform spatial tensor separable convolution, because temporal information is insignificant for spatial attention and vice versa. We add the Batch Normalization (BN) layer for better optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 RESULTS ON UCF101 AND HMDB51</head><p>To verify the generation ability of our CT-Net on smaller datasets, we conduct transfer learning experiments from Kinetics400 to UCF101 <ref type="bibr" target="#b18">(Soomro et al., 2012)</ref> and HMDB-51 <ref type="bibr" target="#b10">(Kuehne et al., 2011)</ref>. We test CT-Net with 16 input frames and evaluate it over three splits and report the averaged results. As shown in <ref type="table" target="#tab_10">Table 5</ref>, our CT-Net 16f achieves competitive performance when compared with the recent methods, which demonstrates the generation ability of our CT-Net. Method Backbone Pretrain UCF101 HMDB51 C3D <ref type="bibr" target="#b21">(Tran et al., 2015)</ref> 3D   <ref type="table" target="#tab_12">Table 6</ref> shows more results on Something-Something V1&amp;V2. We train CT-Net with a different number of input frames and then test these models with different sampling strategies. We average the prediction scores obtained from the previous models to evaluate the ensemble models. With more input frames, the corresponding accuracy becomes higher. As for the reason that CT-Net 24f is worse than CT-Net 16f , we argue that is because the model is hard to optimize with too many input frames. Sampling more clips or more crops also boosts performance. Moreover, our ensemble models gain the state-of-the-art top-1 accuracy of 56.6%(68.3%) on Something-Something V1(V2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 MORE ABLATION STUDIES ON MINI-KINETICS AND SOMETHING-SOMETHING V2</head><p>To verify the effectiveness of our module comprehensively, we also conduct experiments in Mini-Kinetics and Something-Something V2 and report the multi-clip accuracy and single-clip accuracy respectively. Mini-Kinetics covers 200 action classes and is a subset of Kinetics-400, while Something-Something V2 covers the same action classes as Something-Something V1 but contains more videos. As shown in <ref type="table" target="#tab_13">Table 7</ref>, the performance trend of different modules is similar to that shown in <ref type="table" target="#tab_2">Table 2a</ref>. Since Mini-Kinetics does not highly depend on temporal modeling, the gap becomes smaller but still demonstrates the effectiveness of our CT-Module.</p><p>A.6 ADAPTING DIFFERENT PRE-TRAINED IMAGENET ARCHITECTURES AS CT-NET In fact, by directly replacing the 3?3 convolution with our CT-Module, we can easily adapt different pre-trained ImageNet architectures as CT-Net. <ref type="table" target="#tab_14">Table 8</ref> shows that it is also sensible to use InceptionV3 as the backbone. We believe that through more elaborate design, our CT-Net based on different backbones can achieve comparable performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 VALIDATION PLOT</head><p>In <ref type="figure" target="#fig_3">Figure 5</ref>, we plot the accuracy vs per-clip GFLOPs on Kinetics-400. It reveals that our CT-Net achieves a better trade-off than most of the existing methods on Kinetics-400.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The pipelines of CT-Blocks and the overall architecture of CT-Net. We replace one of every two ResBlocks in ResNet with our CT-Block and the extra point-wise convolution in the last sub-dimension (k = K) is ignored. More details can be found in Section3.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Comparison of visualization. Videos are sampled from Something-Something V1. Compared with R(2+1)D and CSN, our CT-Net can localize the action and object better both in space and time thanks to the larger spatial-temporal receptive field.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The implementation of our Tensor Excitation (TE) mechanism.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Accuracy vs per-clip GFLOPs on Kinetics-400.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>F ull : 1 ? 3 ? 3 + F ull : 3 ? 1 ? 1 Effectiveness of CT-Module. CT-Module outperforms the recent modules for video modeling.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>45.8</cell><cell>47.0</cell><cell>76.1</cell></row><row><cell cols="4">CSN-Module (Tran et al., 2019)</cell><cell cols="4">F ull : 1 ? 1 ? 1 + DW : 3 ? 3 ? 3</cell><cell>35.6</cell><cell>46.8</cell><cell>75.7</cell></row><row><cell cols="3">Our CT-Module</cell><cell></cell><cell cols="4">C 1 , ? ? ? , C K : 1 ? 3 ? 3 + 3 ? 1 ? 1</cell><cell>36.3</cell><cell>47.3</cell><cell>76.2</cell></row><row><cell cols="4">(a) Number GFLOPs Top-1 Top-5</cell><cell>Type</cell><cell cols="2">Top-1 Top-5</cell><cell>C 2</cell><cell cols="3">GFLOPs Top-1 Top-5</cell></row><row><cell>1D</cell><cell>45.8</cell><cell>46.5</cell><cell>75.6</cell><cell>coupling</cell><cell>47.1</cell><cell>76.0</cell><cell>1</cell><cell>45.9</cell><cell>47.0</cell><cell>76.1</cell></row><row><cell>2D 3D 4D</cell><cell>36.3 35.7 35.6</cell><cell>47.3 47.1 46.5</cell><cell>76.2 75.8 75.3</cell><cell>serial parallel</cell><cell>47.2 47.3</cell><cell>76.1 76.2</cell><cell>4 16 ? C</cell><cell>37.6 36.4 36.3</cell><cell>46.8 47.2 47.3</cell><cell>76.0 76.0 76.2</cell></row><row><cell cols="4">(b) Number of sub-dimensions. The higher the dimension, the smaller the GFLOPs. 2D channel tensorization achieves the best trade-off.</cell><cell cols="3">(c) Connection type of spatiotemporal convolution. The parallel connection between spatial and temporal convolution is the best choice.</cell><cell cols="4">(d) Dimension size. C = C1 ? C2 and the best trade-off is achieved when adopting the rounded middle size ? C .</cell></row><row><cell>Number</cell><cell cols="4">GFLOPs Top-1 Top-5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+0 (TSN)</cell><cell></cell><cell>43.0</cell><cell>16.9</cell><cell>42.0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+1 stage5</cell><cell></cell><cell>41.9</cell><cell>42.3</cell><cell>71.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">+4 stage4-5</cell><cell>38.9</cell><cell>46.9</cell><cell>75.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">+6 stage3-5</cell><cell>37.1</cell><cell>47.2</cell><cell>76.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">+7 stage2-5</cell><cell>36.3</cell><cell>47.3</cell><cell>76.2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">+12 stage2-5</cell><cell>31.5</cell><cell>45.9</cell><cell>76.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>(e) Number and location of CT-Blocks. Simply replacing 1 block in stage5 can bring significant performance improvement. As we replace more blocks from the bottom up, the GFLOPs contin- ues to decrease. Replacing 7 blocks achieves the best trade-off between accuracy and GFLOPs.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation studies on Something-Something V1. All models use ResNet50 as the backbone</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Comparison with the state-of-the-art on Something-Something V1&amp;V2. Our CT- Net 16f outperforms all the single-clip models in Something-Something and even better than most of the multi-clip models. And our CT-Net EN outperforms all methods with very lower computation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 :</head><label>4</label><figDesc>Comparison with the state-of-the-art on Kinetics-400. It shows that CT-Net-R50 16f can surpass all existing lightweight models and even SlowFast-R50 40f . When fusing different models, our model is 2.4? faster than SlowFast-R101 80f and shows an 0.9% performance gain.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Comparison results on UCF101 and HMDB51.</figDesc><table /><note>A.4 MORE RESULTS ON SOMETHING-SOMETHING V1&amp;V2</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 6 :</head><label>6</label><figDesc>More results on Something-Something V1&amp;V2.</figDesc><table><row><cell>Method</cell><cell cols="2">Backbone GFLOPs</cell><cell cols="4">Mini-Kinetics Top-1 Top-5 Top-1 Top-5 SomethingV2</cell></row><row><cell>C3D-Module (Tran et al., 2015)</cell><cell>2D R50</cell><cell>59.9</cell><cell>77.5</cell><cell>93.0</cell><cell>59.1</cell><cell>85.5</cell></row><row><cell cols="2">R(2+1)D-Module (Tran et al., 2018) 2D R50</cell><cell>45.8</cell><cell>77.8</cell><cell>93.2</cell><cell>60.0</cell><cell>86.0</cell></row><row><cell>CSN-Module (Tran et al., 2019)</cell><cell>2D R50</cell><cell>35.6</cell><cell>77.6</cell><cell>93.2</cell><cell>59.5</cell><cell>86.0</cell></row><row><cell>Our CT-Module</cell><cell>2D R50</cell><cell>36.3</cell><cell>78.0</cell><cell>93.6</cell><cell>60.3</cell><cell>86.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 7 :</head><label>7</label><figDesc>More ablation studies on Mini-Kinetics and Something-Something V2.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="4">GFLOPs #Param.(M) Top-1 Top-5</cell></row><row><cell cols="3">Baseline (TSN) 2D ResNet-50 43.0</cell><cell>23.9</cell><cell>16.9</cell><cell>42.0</cell></row><row><cell>Our CT-Net</cell><cell cols="2">2D ResNet-50 37.3</cell><cell>21.0</cell><cell>50.1</cell><cell>78.8</cell></row><row><cell cols="2">Baseline (TSN) InceptionV3</cell><cell>45.8</cell><cell>22.1</cell><cell>18.3</cell><cell>43.9</cell></row><row><cell>Our CT-Net</cell><cell>InceptionV3</cell><cell>43.9</cell><cell>20.9</cell><cell>47.2</cell><cell>76.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 8 :</head><label>8</label><figDesc>Adapting different pre-trained ImageNet architectures as CT-Net.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">More is less: Learning efficient video representations by temporal aggregation module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pistoia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">X3d: Expanding architectures for efficient video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="200" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Slowfast networks for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6201" to="6210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Andrew Tulloch, Y. Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1 hour</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<idno>abs/1706.02677</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The &quot;something something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fr?nd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Thurau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5843" to="5851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kaiming He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Stm: Spatiotemporal and motion encoding for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengmeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Hmdb: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Smallbignet: Integrating core and contextual views for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1089" to="1098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Tea: Temporal excitation and aggregation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xintian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianguo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<idno>abs/2004.01398</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Tsm: Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7082" to="7092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Teinet: Towards an efficient architecture for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<idno>abs/1911.09435</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1212.0402</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Saliency tubes: Visual explanations for spatio-temporal convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><surname>Stergiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kapidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grigorios</forename><surname>Kalliatakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chrysoulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Veltkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1830" to="1834" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gate-shift networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><forename type="middle">S</forename><surname>Swathikiran Sudhakaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Escalera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1099" to="1108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lubomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Hong Xiu Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Video classification with channelseparated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5551" to="5560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Video modeling with correlation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feiszli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="349" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Videos as space-time region graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Temporal distinct representation learning for action recognition. ArXiv, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junwu</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jilin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Saining Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Eco: Efficient convolutional network for online video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mohammadreza Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brox</surname></persName>
		</author>
		<idno>abs/1804.09066</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
