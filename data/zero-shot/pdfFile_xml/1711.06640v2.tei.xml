<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Motifs: Scene Graph Parsing with Global Context</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
							<email>rowanz@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
							<email>sthomson@cs.cmu.edu</email>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Paul G. Allen School of Computer Science &amp; Engineering</orgName>
								<orgName type="institution">University of Washington</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Allen Institute for Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Motifs: Scene Graph Parsing with Global Context</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We investigate the problem of producing structured graph representations of visual scenes. Our work analyzes the role of motifs: regularly appearing substructures in scene graphs. We present new quantitative insights on such repeated structures in the Visual Genome dataset. Our analysis shows that object labels are highly predictive of relation labels but not vice-versa. We also find that there are recurring patterns even in larger subgraphs: more than 50% of graphs contain motifs involving at least two relations. Our analysis motivates a new baseline: given object detections, predict the most frequent relation between object pairs with the given labels, as seen in the training set. This baseline improves on the previous state-of-the-art by an average of 3.6% relative improvement across evaluation settings. We then introduce Stacked Motif Networks, a new architecture designed to capture higher order motifs in scene graphs that further improves over our strong baseline by an average 7.1% relative gain. Our code is available at github.com/rowanz/neural-motifs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We investigate scene graph parsing: the task of producing graph representations of real-world images that provide semantic summaries of objects and their relationships. For example, the graph in <ref type="figure">Figure 1</ref> encodes the existence of key objects such as people ("man" and "woman"), their possessions ("helmet" and "motorcycle", both possessed by the woman), and their activities (the woman is "riding" the "motorcycle"). Predicting such graph representations has been shown to improve natural language based image tasks <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b51">51]</ref> and has the potential to significantly expand the scope of applications for computer vision systems. Compared to object detection <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b34">34]</ref> , object interactions <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b2">3]</ref> and activity recognition <ref type="bibr" target="#b13">[13]</ref>, scene graph parsing poses unique challenges since it requires reasoning about the complex dependencies across all of these components.</p><p>Elements of visual scenes have strong structural regu-  <ref type="figure">Figure 1</ref>. A ground truth scene graph containing entities, such as woman, bike or helmet, that are localized in the image with bounding boxes, color coded above, and the relationships between those entities, such as riding, the relation between woman and motorcycle or has the relation between man and shirt.</p><p>larities. For instance, people tend to wear clothes, as can be seen in <ref type="figure">Figure 1</ref>. We examine these structural repetitions, or motifs, using the Visual Genome <ref type="bibr" target="#b22">[22]</ref> dataset, which provides annotated scene graphs for 100k images from COCO <ref type="bibr" target="#b28">[28]</ref>, consisting of over 1M instances of objects and 600k relations. Our analysis leads to two key findings. First, there are strong regularities in the local graph structure such that the distribution of the relations is highly skewed once the corresponding object categories are given, but not vice versa. Second, structural patterns exist even in larger subgraphs; we find that over half of images contain previously occurring graph motifs.</p><p>Based on our analysis, we introduce a simple yet powerful baseline: given object detections, predict the most frequent relation between object pairs with the given labels, as seen in the training set. The baseline improves over prior state-of-the-art by 1.4 mean recall points (3.6% relative), suggesting that an effective scene graph model must capture both the asymmetric dependence between objects and their relations, along with larger contextual patterns.</p><p>We introduce the Stacked Motif Network (MOTIFNET), a new neural network architecture that complements existing approaches to scene graph parsing. We posit that the key challenge in modeling scene graphs lies in devising an efficient mechanism to encode the global context that can directly inform the local predictors (i.e., objects and relations). While previous work has used graph-based inference to propagate information in both directions between objects and relations <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b24">24]</ref>, our analysis suggests strong independence assumptions in local predictors limit the quality of global predictions. Instead, our model predicts graph elements by staging bounding box predictions, object classifications, and relationships such that the global context encoding of all previous stages establishes rich context for predicting subsequent stages, as illustrated in <ref type="figure">Figure 5</ref>. We represent the global context via recurrent sequential architectures such as Long Short-term Memory Networks (LSTMs) <ref type="bibr" target="#b15">[15]</ref>.</p><p>Our model builds on Faster-RCNN <ref type="bibr" target="#b36">[36]</ref> for predicting bounding regions, fine tuned and adapted for Visual Genome. Global context across bounding regions is computed and propagated through bidirectional LSTMs, which is then used by another LSTM that labels each bounding region conditioned on the overall context and all previous labels. Another specialized layer of bidirectional LSTMs then computes and propagates information for predicting edges given bounding regions, their labels, and all other computed context. Finally, we classify all n 2 edges in the graph, combining globally contextualized representations of head, tail, and image representations using using low-rank outer products <ref type="bibr" target="#b19">[19]</ref>. The method can be trained end-to-end.</p><p>Experiments on Visual Genome demonstrate the efficacy of our approach. First, we update existing work by pretraining the detector on Visual Genome, setting a new state-of-the-art (improving on average across evaluation settings 14.0 absolute points). Our new simple baseline improves over previous work, using our updated detector, by a mean improvement of 1.4 points. Finally, experiments show Stacked Motif Networks is effective at modeling global context, with a mean improvement of 2.9 points (7.1% relative improvement) over our new strong baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Formal definition</head><p>A scene graph, G, is a structured representation of the semantic content of an image <ref type="bibr" target="#b17">[17]</ref>. It consists of:</p><p>? </p><formula xml:id="formula_0">(b i , o i ) ? B ? O, an end node (b j , o j ) ? B ? O, and a relationship label x i?j ? R,</formula><p>where R is the set of all predicate types, including the "background" predicate, BG, which indicates that there is no edge between the specified objects. See <ref type="figure">Figure 1</ref> for an example scene graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Scene graph analysis</head><p>In this section, we seek quantitative insights on the structural regularities of scene graphs. In particular, (a) how different types of relations correlate with different objects, and (b) how higher order graph structures recur over different scenes. These insights motivate both the new baselines we introduce in this work and our model that better integrates the global context, described in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Prevalent Relations in Visual Genome</head><p>To gain insight into the Visual Genome scene graphs, we first categorize objects and relations into high-level types. As shown in <ref type="table" target="#tab_0">Table 1</ref>, the predominant relations are geometric and possessive, with clothing and parts making up over one third of entity instances. Such relations are often obvious, e.g., houses tend to have windows. In contrast, semantic relations, which correspond to activities, are less frequent and less obvious. Although nearly half of relation types are semantic in nature, they comprise only 8.7% of relation instances. The relations "using" and "holding" account for 32.2% of all semantic relation instances. <ref type="figure">Figure 2</ref>. Types of edges between high-level categories in Visual Genome. Geometric, possessive and semantic edges cover 50.9%, 40.9%, and 8.7%, respectively, of edge instances in scene graphs. The majority of semantic edges occur between people and vehicles, artifacts and locations. Less than 2% of edges between clothes and people are semantic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semantic Geometric Possessive</head><p>Using our high-level types, we visualize the distribution of relation types between object types in <ref type="figure">Figure 2</ref>. Clothing and part entities are almost exclusively linked through possessive relations while furniture and building entities are almost exclusively linked through geometric relations. Geometric and spatial relationships between certain entities are interchangeable, for example, when a "part" is the head object, it tends to connect to other entities through a geometric relation (e.g. wheel on bike); when a "part" is the tail object, it tends to be connected with possessive relations (e.g. bike has wheel). Nearly all semantic relationship are headed by people, with the majority of edges relating to artifacts, vehicles, and locations. Such structural predictability and the prevalence of geometric and part-object relations suggest that common sense priors play an important role in generating accurate scene graphs.</p><p>In <ref type="figure">Figure 3</ref>, we examine how much information is gained by knowing the identity of different parts in a scene graphs. In particular, we consider how many guesses are required to determine the labels of head (h), edge (e) or tail (t) given labels of the other elements, only using label statistics computed on scene graphs. Higher curves imply that the element is highly determined given the other values. The graph shows that the local distribution of relationships has significant structure. In general, the identity of edges involved in a relationship is not highly informative of other elements of the structure while the identities of head or tail provide significant information, both to each other and to edge labels. Adding edge information to already given head or tail information provides minimal gain. Finally, the graph shows edge labels are highly determined given the identity of ob-  <ref type="figure">Figure 3</ref>. The likelihood of guessing, in the top-k, head, tail, or edge labels in a scene graph, given other graph components (i.e. without image features). Neither head nor tail labels are strongly determined by other labels, but given the identity of head and tail, edges (edge | head, tail) can be determined with 97% accuracy in under 5 guesses. Such strong biases make it critical to condition on objects when predicting edges.</p><p>ject pairs: the most frequent relation is correct 70% of the time, and the five most frequent relations for the pair contain the correct label 97% of the time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Larger Motifs</head><p>Scene graphs not only have local structure but have higher order structure as well. We conducted an analysis of repeated motifs in scene graphs by mining combinations of object-relation-object labels that have high pointwise mutual information with each other. Motifs were extracted iteratively: first we extracted motifs of two combinations, replaced all instances of that motif with an atomic symbol and mined new motifs given previously identified motifs. Combinations of graph elements were selected as motifs if both elements involved occurred at least 50 times in the Visual Genome training set and were at least 10 times more likely to occur together than apart. Motifs were mined until no new motifs were extracted. <ref type="figure">Figure 4</ref> contains example motifs we extracted on the right, and the prevalence of motifs of different lengths in images on the left. Many motifs correspond to either combinations of parts, or objects that are commonly grouped together. Over 50% of images in Visual Genome contain a motif involving at least two combinations of object-relation-object, and some images contain motifs involving as many as 16 elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Model</head><p>Here we present our novel model, Stacked Motif Network (MOTIFNET). MOTIFNET decomposes the probability of a graph G (made up of a set of bounding regions B,  <ref type="figure">Figure 4</ref>. On the left, the percent of images that have a graph motif found in Visual Genome using pointwise mutual information, composed of at least a certain length (the number of edges it contains). Over 50% of images have at least one motif involving two relationships. On the right, example motifs, where structures repeating many times is indicated with plate notation. For example, the second motif is length 8 and consists of 8 flower-in-vase relationships. Graph motifs commonly result from groups (e.g., several instances of "leaf on tree"), and correlation between parts (e.g., "elephant has head," "leg," "trunk," and "ear."). object labels O, and labeled relations R) into three factors:</p><formula xml:id="formula_1">Pr(G | I) = Pr(B | I) Pr(O | B, I) Pr(R | B, O, I). (1)</formula><p>Note that this factorization makes no independence assumptions. Importantly, predicted object labels may depend on one another, and predicted relation labels may depend on predicted object labels. The analyses in Section 3 make it clear that capturing these dependencies is crucial.</p><p>The bounding box model (Pr(B | I)) is a fairly standard object detection model, which we describe in Section 4.1. The object model (Pr(O | B, I); Section 4.2) conditions on a potentially large set of predicted bounding boxes, B. To do so, we linearize B into a sequence that an LSTM then processes to create a contextualized representation of each box. Likewise, when modeling relations (Pr(R | B, O, I); Section 4.3), we linearize the set of predicted labeled objects, O, and process them with another LSTM to create a representation of each object in context. <ref type="figure">Figure 5</ref> contains a visual summary of the entire model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Bounding Boxes</head><p>We use Faster R-CNN as an underlying detector <ref type="bibr" target="#b36">[36]</ref>. For each image I, the detector predicts a set of region proposals B = {b 1 , . . . , b n }. For each proposal b i ? B it also outputs a feature vector f i and a vector l i ? R |C| of (noncontextualized) object label probabilities. Note that because BG is a possible label, our model has not yet committed to any bounding boxes. See Section 5.1 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Objects</head><p>Context We construct a contextualized representation for object prediction based on the set of proposal regions B. Elements of B are first organized into a linear sequence, <ref type="bibr" target="#b0">1</ref> The object context, C, is then computed using a bidirectional LSTM <ref type="bibr" target="#b15">[15]</ref>:</p><formula xml:id="formula_2">[(b 1 , f 1 , l 1 ), . . . , (b n , f n , l n )].</formula><formula xml:id="formula_3">C = biLSTM([f i ; W 1 l i ] i=1,...,n ),<label>(2)</label></formula><formula xml:id="formula_4">C = [c 1 , .</formula><p>. . , c n ] contains the final LSTM layer's hidden states for each element in the linearization of B, and W 1 is a parameter matrix that maps the distribution of predicted classes, l 1 , to R 100 . The biLSTM allows all elements of B to contribute information about potential object identities.</p><p>Decoding The context C is used to sequentially decode labels for each proposal bounding region, conditioning on previously decoded labels. We use an LSTM to decode a category label for each contextualized representation in C:</p><formula xml:id="formula_5">h i = LSTM i ([c i ;? i?1 ]) (3) o i = argmax (W o h i ) ? R |C| (one-hot)<label>(4)</label></formula><p>We then discard the hidden states h i and use the object class commitments? i in the relation model (Section 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Relations</head><p>Context We construct a contextualized representation of bounding regions, B, and objects, O, using additional bidirectional LSTM layers:</p><formula xml:id="formula_6">D = biLSTM([c i ; W 2?i ] i=1,...,n ),<label>(5)</label></formula><p>where the edge context D = [d 1 , . . . , d n ] contains the states for each bounding region at the final layer, and W 2 is a parameter matrix mapping? i into R 100 .</p><p>Decoding There are a quadratic number of possible relations in a scene graph. For each possible edge, say between b i and b j , we compute the probability the edge will have label x i?j (including BG). The distribution uses global context, D, and a feature vector for the union of boxes 2 , f i,j :</p><formula xml:id="formula_7">g i,j = (W h d i ) ? (W t d j ) ? f i,j (6) Pr(x i?j | B, O) = softmax W r g i,j + w oi,oj . (7)</formula><p>W h and W t project the head and tail context into R 4096 . w oi,oj is a bias vector specific to the head and tail labels.   <ref type="figure">Figure 5</ref>. A diagram of a Stacked Motif Network (MOTIFNET). The model breaks scene graph parsing into stages predicting bounding regions, labels for regions, and then relationships. Between each stage, global context is computed using bidirectional LSTMs and is then used for subsequent stages. In the first stage, a detector proposes bounding regions and then contextual information among bounding regions is computed and propagated (object context). The global context is used to predict labels for bounding boxes. Given bounding boxes and labels, the model constructs a new representation (edge context) that gives global context for edge predictions. Finally, edges are assigned labels by combining contextualized head, tail, and union bounding region information with an outer product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Setup</head><p>In the following sections we explain (1) details of how we construct the detector, order bounding regions, and implement the final edge classifier (Section 5.1), (2) details of training (Section 5.2), and (3) evaluation (Section 5.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Model Details</head><p>Detectors Similar to prior work in scene graph parsing <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b25">25]</ref>, we use Faster RCNN with a VGG backbone as our underling object detector <ref type="bibr" target="#b36">[36,</ref><ref type="bibr" target="#b40">40]</ref>. Our detector is given images that are scaled and then zero-padded to be 592x592. We adjust the bounding box proposal scales and dimension ratios to account for different box shapes in Visual Genome, similar to YOLO-9000 <ref type="bibr" target="#b34">[34]</ref>. To control for detector performance in evaluating different scene graph models, we first pretrain the detector on Visual Genome objects. We optimize the detector using SGD with momentum on 3 Titan Xs, with a batch size of b = 18, and a learning rate of lr = 1.8 ? 10 ?2 that is divided by 10 after validation mAP plateaus. For each batch we sample 256 RoIs per image, of which 75% are background. The detector gets 20.0 mAP (at 50% IoU) on Visual Genome; the same model, but trained and evaluated on COCO, gets 47.7 mAP at 50% IoU. Following <ref type="bibr" target="#b47">[47]</ref>, we integrate the use the detector freezing the convolution layers and duplicating the fully connected lay-ers, resulting in separate branches for object/edge features.</p><p>Alternating Highway LSTMs To mitigate vanishing gradient problems as information flows upward, we add highway connections to all LSTMs <ref type="bibr" target="#b14">[14,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b58">58]</ref>. To additionally reduce the number of parameters, we follow <ref type="bibr" target="#b14">[14]</ref> and alternate the LSTM directions. Each alternating highway LSTM step can be written as the following wrapper around the conventional LSTM equations <ref type="bibr" target="#b15">[15]</ref>:</p><formula xml:id="formula_8">r i = ?(W g [h i?? , x i ] + b g ) (8) h i = r i ? LSTM(x i , h i?? ) + (1 ? r i ) ? W i x i ,<label>(9)</label></formula><p>where x i is the input, h i represents the hidden state, and ? is the direction: ? = 1 if the current layer is even, and ?1 otherwise. For MOTIFNET, we use 2 alternating highway LSTM layers for object context, and 4 for edge context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RoI Ordering for LSTMs</head><p>We consider several ways of ordering the bounding regions: (1) LEFTRIGHT (default): Our default option is to sort the regions left-to-right by the central x-coordinate: we expect this to encourage the model to predict edges between nearby objects, which is beneficial as objects appearing in relationships tend to be close together.</p><p>(2) CONFIDENCE: Another option is to order bounding regions based on the confidence of the maximum non-background prediction from the detector:</p><formula xml:id="formula_9">max j =BG l (j)</formula><p>i , as this lets the detector commit to "easy" regions, obtaining context for more difficult regions. <ref type="bibr" target="#b2">3</ref> (3) SIZE: Here, we sort the boxes in descending order by size, possibly predicting global scene information first. (4) RANDOM: Here, we randomly order the regions.</p><p>Predicate Visual Features To extract visual features for a predicate between boxes b i , b j , we resize the detector's features corresponding to the union box of b i , b j to 7x7x256. We model geometric relations using a 14x14x2 binary input with one channel per box. We apply two convolution layers to this and add the resulting 7x7x256 representation to the detector features. Last, we apply finetuned VGG fully connected layers to obtain a 4096 dimensional representation. <ref type="bibr" target="#b3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Training</head><p>We train MOTIFNET on ground truth boxes, with the objective to predict object labels and to predict edge labels given ground truth object labels. For an image, we include all annotated relationships (sampling if more than 64) and sample 3 negative relationships per positive. In cases with multiple edge labels per directed edge (5% of edges), we sample the predicates. Our loss is the sum of the cross entropy for predicates and cross entropy for objects predicted by the object context layer. We optimize using SGD with momentum on a single GPU, with lr = 6 ? 10 ?3 and b = 6.</p><p>Adapting to Detection Using the above protocol gets good results when evaluated on scene graph classification, but models that incorporate context underperform when suddenly introduced to non-gold proposal boxes at test time.</p><p>To alleviate this, we fine-tune using noisy box proposals from the detector. We use per-class non-maximal suppression (NMS) <ref type="bibr" target="#b38">[38]</ref> at 0.3 IoU to pass 64 proposals to the object context branch of our model. We also enforce NMS constraints during decoding given object context. We then sample relationships between proposals that intersect with ground truth boxes and use relationships involving these boxes to finetune the model until detection convergence.</p><p>We also observe that in detection our model gets swamped with many low-quality RoI pairs as possible relationships, which slows the model and makes training less stable. To alleviate this, we observe that nearly all annotated relationships are between overlapping boxes, <ref type="bibr" target="#b4">5</ref> and classify all relationships with non-overlapping boxes as BG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Evaluation</head><p>We train and evaluate our models on Visual Genome, using the publicly released preprocessed data and splits from <ref type="bibr" target="#b47">[47]</ref>, containing 150 object classes and 50 relation classes, but sample a development set from the training set of 5000 images. We follow three standard evaluation modes: (1) predicate classification (PREDCLS): given a ground truth set of boxes and labels, predict edge labels, (2) scene graph classification (SGCLS): given ground truth boxes, predict box labels and edge label and (3) scene graph detection (SGDET): predict boxes, box labels, and edge labels. The annotated graphs are known to be incomplete, thus systems are evaluated using recall@K metrics. <ref type="bibr" target="#b5">6</ref> In all three modes, recall is calculated for relations; a ground truth edge (b h , o h , x, b t , o t ) is counted as a "match" if there exist predicted boxes i, j such that b i and b j respectively have sufficient overlap with b h and b t , 7 and the objects and relation labels agree. We follow previous work in enforcing that for a given head and tail bounding box, the system must not output multiple edge labels <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b29">29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Frequency Baselines</head><p>To support our finding that object labels are highly predictive of edge labels, we additionally introduce several frequency baselines built off training set statistics. The first, FREQ, uses our pretrained detector to predict object labels for each RoI. To obtain predicate probabilities between boxes i and j, we look up the empirical distribution over relationships between objects o i and o j as computed in the training set. <ref type="bibr" target="#b7">8</ref> Intuitively, while this baseline does not look at the image to compute Pr(x i?j |o i , o j ), it displays the value of conditioning on object label predictions o. The second, FREQ-OVERLAP, requires that the two boxes intersect in order for the pair to count as a valid relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Results</head><p>We present our results in <ref type="table">Table 6</ref>. We compare MO-TIFNET to previous models not directly incorporating context (VRD <ref type="bibr" target="#b29">[29]</ref> and ASSOC EMBED <ref type="bibr" target="#b31">[31]</ref>), a stateof-the-art approach for incorporating graph context via message passing (MESSAGE PASSING) <ref type="bibr" target="#b47">[47]</ref>, and its reimplemenation using our detector, edge model, and NMS settings (MESSAGE PASSING+). Unfortunately, many scene graph models are evaluated on different versions of Visual Genome; see <ref type="table">Table 3</ref> in the supp for more analysis.</p><p>Our best frequency baseline, FREQ+OVERLAP, improves over prior state-of-the-art by 1.4 mean recall, pri- <ref type="bibr" target="#b5">6</ref> Past work has considered these evaluation modes at recall thresholds R@50 and R@100, but we also report results on R@20. <ref type="bibr" target="#b6">7</ref> As in prior work, we compute the intersection-over-union (IoU) between the boxes and use a threshold of 0.5. <ref type="bibr" target="#b7">8</ref> Since we consider an edge x i?j to have label BG if o has no edge to j, this gives us a valid probability distribution.  <ref type="bibr" target="#b47">[47]</ref> which ran VRD <ref type="bibr" target="#b29">[29]</ref> without language priors. All numbers in %. Since past work doesn't evaluate on R@20, we compute the mean by averaging performance on the 3 evaluation modes over R@50 and R@100. : results in <ref type="bibr" target="#b31">[31]</ref> are without scene graph constraints; we evaluated performance with constraints using saved predictions given to us by the authors (see <ref type="table">Table 3</ref> in supp). marily due to improvements in detection and predicate classification, where it outperforms MESSAGE PASSING+ by 5.5 and 6.5 mean points respectively. MOTIFNET improves even further, by 2.9 additional mean points over the baseline (a 7.1% relative gain).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene Graph Detection</head><p>Ablations To evaluate the effectiveness of our main model, MOTIFNET, we consider several ablations in Table <ref type="bibr" target="#b5">6</ref>. In MOTIFNET-NOCONTEXT, we predict objects based on the fixed detector, and feed non-contexualized embeddings of the head and tail label into Equation <ref type="bibr" target="#b5">6</ref>. Our results suggest that there is signal in the vision features for edge predictions, as MOTIFNET-NOCONTEXT improves over FREQ-OVERLAP. Incorporating context is also important: our full model MOTIFNET improves by 1.2 mean points, with largest gains at the lowest recall threshold of R@20. <ref type="bibr" target="#b8">9</ref> We additionally validate the impact of the ordering method used, as discussed in Section 5.1; the results vary less than 0.3 recall points, suggesting that MOTIFNET is robust to the RoI ordering scheme used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Qualitative Results</head><p>Qualitative examples of our approach, shown in <ref type="figure" target="#fig_4">Figure 6</ref>, suggest that MOTIFNET is able to induce graph motifs from detection context. Visual inspection of the results suggests that the method works even better than the quantitative results would imply, since many seemingly correct edges are predicted that do not exist in the ground truth.</p><p>There are two common failure cases of our model. The first, as exhibited by the middle left image in <ref type="figure" target="#fig_4">Figure 6</ref> of a skateboarder carrying a surfboard, stems from predicate <ref type="bibr" target="#b8">9</ref> The larger improvement at the lower thresholds suggests that our models mostly improve on relationship ordering rather than classification. Indeed, it is often unnecessary to order relationships at the higher thresholds: 51% of images have fewer than 50 candidates and 78% have less than 100. ambiguity ("wearing" vs "wears"). The second common failure case occurs when the detector fails, resulting in cascading failure to predict any edges to that object. For example, the failure to predict "house" in the lower left image resulted in five false negative relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Related Work</head><p>Context Many methods have been proposed for modeling semantic context in object recognition <ref type="bibr" target="#b6">[7]</ref>. Our approach is most closely related to work that models object co-occurrence using graphical models to combine many sources of contextual information <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b11">11,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b9">10]</ref>. While our approach is a type of graphical model, it is unique in that it stages incorporation of context allowing for meaningful global context from large conditioning sets.</p><p>Actions and relations have been a particularly fruitful source of context <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b50">50]</ref>, especially when combined with pose to create human-object interactions <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b2">3]</ref>. Recent work has shown that object layouts can provide sufficient context for captioning COCO images <ref type="bibr" target="#b52">[52,</ref><ref type="bibr" target="#b28">28]</ref>; our work suggests the same for parsing Visual Genome scene graphs. Much of the context we derive could be interpreted as commonsense priors, which have commonly been extracted using auxiliary means <ref type="bibr" target="#b59">[59,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b55">55]</ref>. Yet for scene graphs, we are able to directly extract such knowledge.</p><p>Structured Models Structured models in visual understanding have been explored for language grounding, where language determines the graph structures involved in prediction <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b16">16]</ref>. Our problem is different as we must reason over all possible graph structures. Deep sequential models have demonstrated strong performance for tasks such as captioning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b18">18]</ref> and visual question answering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b7">8]</ref>, including for problems not traditionally not thought of as sequential, such as multilabel classifi- cation <ref type="bibr" target="#b46">[46]</ref>. Indeed, graph linearization has worked surprisingly well for many problems in vision and language, such as generating image captions from object detections <ref type="bibr" target="#b52">[52]</ref>, language parsing <ref type="bibr" target="#b44">[44]</ref>, generating text from abstract meaning graphs <ref type="bibr" target="#b21">[21]</ref>. Our work leverages the ability of RNNs to memorize long sequences in order to capture graph motifs in Visual Genome. Finally, recent works incorporate recurrent models into detection and segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b35">35]</ref> and our methods contribute evidence that RNNs provide effective context for consecutive detection predictions.</p><p>Scene Graph Methods Several works have explored the role of priors by incorporating background language statistics <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b54">54]</ref> or by attempting to preprocess scene graphs <ref type="bibr" target="#b56">[56]</ref>. Instead, we allow our model to directly learn to use scene graph priors effectively. Furthermore, recent graph-propagation methods were applied but converge quickly and bottle neck through edges, significantly limiting information exchange <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">23]</ref>. On the other hand, our method allows global exchange of information about context through conditioning and avoids uninformative edge predictions until the end. Others have explored creating richer models between image regions, introducing new convolutional features and new objectives <ref type="bibr" target="#b31">[31,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b27">27]</ref>. Our work is complementary and instead focuses on the role of context. See the supplemental section for a comprehensive comparison to prior work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Conclusion</head><p>We presented an analysis of the Visual Genome dataset showing that motifs are prevalent, and hence important to model. Motivated by this analysis, we introduced strong baselines that improve over prior state-of-the-art models by modeling these intra-graph interactions, while mostly ignoring visual cues. We also introduced our model MO-TIFNET for capturing higher order structure and global interactions in scene graphs that achieves additional significant gains over our already strong baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental</head><p>Current work in scene graph parsing is largely inconsistent in terms of evaluation and experiments across papers are not completely comparable. In this supplementary material, we attempt to classify some of the differences and put the works together in the most comparable light.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Setup</head><p>In our paper, we compared against papers that (to the best of our knowledge) evaluated in the same way as <ref type="bibr" target="#b47">[47]</ref>. Variation in evaluation consists of two types:</p><p>? Custom data handling, such as creating paper-specific dataset splits, changing the data pre-processing, or using different label sets.</p><p>? Omitting graph constraints, namely, allowing a headtail pair to have multiple edge labels in system output. We hypothesize that omitting graph constraints should always lead to higher numbers, since the model is then allowed multiple guesses for challenging objects and relations. <ref type="table">Table 3</ref> provides a best effort comprehensive review against all prior work that we are aware of. Other works also introduce slight variations in the tasks that are evaluated: 10</p><p>? Predicate Detection (PREDDET). The model is given a list of labeled boxes, as in predicate classification, and a list of head-tail pairs that have edges in the ground truth (the model makes no edge predictions for head-tail pairs not in the ground truth).</p><p>? Phrase Detection (PHRDET). The model must produce a set of objects and edges, as in scene graph detection. An edge is counted as a match if the objects and predicate match the ground truth, with the IOU between the union-boxes of the prediction and the ground truth over 0.5 (in contrast to scene graph detection where each object box must independently overlap with the corresponding ground truth box).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Models considered</head><p>In <ref type="table">Table 3</ref>, we list the following additional methods:</p><p>? MSDN [25]: This model is an extension of the message passing idea from <ref type="bibr" target="#b47">[47]</ref>. In addition to using an RPN to propose boxes for objects, an additional RPN is used to propose regions for captioning. The caption generator is trained using an additional loss on the annotated regions from Visual Genome.</p><p>? MSDN-FREQ: To benchmark the performance on <ref type="bibr" target="#b25">[25]</ref>'s split (with more aggressive preprocessing than <ref type="bibr" target="#b9">10</ref> We use task names from <ref type="bibr" target="#b29">[29]</ref>, despite inconsistency in whether the underlying task actually involves classification or detection. <ref type="bibr" target="#b47">[47]</ref> and with small objects removed), we evaluated a version of our FREQ baseline in <ref type="bibr" target="#b25">[25]</ref>'s codebase. We took a checkpoint from the authors and replaced all edge predictions with predictions from the training set statistics from <ref type="bibr" target="#b25">[25]</ref>'s split.</p><p>? SCR <ref type="bibr" target="#b23">[23]</ref>: This model uses an RPN to generate triplet proposals. Messages are then passed between the head, tail, and predicate for each triplet.</p><p>? DR-NET <ref type="bibr" target="#b5">[6]</ref>: Similar to <ref type="bibr" target="#b47">[47]</ref>, this model uses an object detector to propose regions, and then messages are passed between relationship components using an approximation to CRF inference.</p><p>? VRL <ref type="bibr" target="#b27">[27]</ref>: This model constructs a scene graph incrementally. During training, a reinforcement learning loss is used to reward the model when it predicts correct components.</p><p>? VTE <ref type="bibr" target="#b57">[57]</ref>: This model learns subject, predicate, and object embeddings. A margin loss is used to reward the model for predicting correct triplets over incorrect ones.</p><p>? LKD <ref type="bibr" target="#b54">[54]</ref>: This model uses word vectors to regularize a CNN that predicts relationship triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary</head><p>The amount of variation in <ref type="table">Table 3</ref> requires extremely cautious interpretation. As expected, removing graph constraints significantly increases reported performance and both predicate detection and phrase detection are significantly less challenging than predicate classification and scene graph detection, respectively. On <ref type="bibr" target="#b25">[25]</ref>'s split, the MSDN-FREQ baseline outperforms MSDN on all evaluation settings, suggesting baseline is robust across alternative data settings. In total, the results suggest that our model and baselines are at least competitive with other approaches on different configurations of the task.  <ref type="table">Table 3</ref>. Results with and without scene graph constraints. Horizontal lines indicate different dataset preprocessing settings (the "other split" results, to the best of our knowledge, are reported on different splits). : <ref type="bibr" target="#b25">[25]</ref> authors acknowledge that their paper results aren't reproducible for SGCLS and PREDCLS; their current best reproducible numbers are one line below. MSDN-FREQ: Results from using node prediction from <ref type="bibr" target="#b25">[25]</ref> and edge prediction from FREQ.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative examples from our model in the Scene Graph Detection setting. Green boxes are predicted and overlap with the ground truth, orange boxes are ground truth with no match. Green edges are true positives predicted by our model at the R@20 setting, orange edges are false negatives, and blue edges are false positives. Only predicted boxes that overlap with the ground truth are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Object and relation types in Visual Genome, organized by super-type. Most, 25.2% of entities are parts and 90.9% of relations are geometric or possessive.</figDesc><table><row><cell>Type</cell><cell>Examples</cell><cell>Classes</cell><cell>Instances</cell></row><row><cell></cell><cell>Entities</cell><cell></cell><cell></cell></row><row><cell>Part</cell><cell>arm, tail, wheel</cell><cell>32</cell><cell>200k (25.2%)</cell></row><row><cell>Artifact</cell><cell>basket, fork, towel</cell><cell>34</cell><cell>126k (16.0%)</cell></row><row><cell>Person</cell><cell>boy, kid, woman</cell><cell>13</cell><cell>113k (14.3%)</cell></row><row><cell>Clothes</cell><cell>cap, jean, sneaker</cell><cell>16</cell><cell>91k (11.5%)</cell></row><row><cell>Vehicle</cell><cell>airplane, bike, truck,</cell><cell>12</cell><cell>44k (5.6%)</cell></row><row><cell>Flora</cell><cell>flower, plant, tree</cell><cell>3</cell><cell>44k (5.5%)</cell></row><row><cell>Location</cell><cell>beach, room, sidewalk</cell><cell>11</cell><cell>39k (4.9%)</cell></row><row><cell>Furniture</cell><cell>bed, desk, table</cell><cell>9</cell><cell>37k (4.7%)</cell></row><row><cell>Animal</cell><cell>bear, giraffe, zebra</cell><cell>11</cell><cell>30k (3.8%)</cell></row><row><cell>Structure</cell><cell>fence, post, sign</cell><cell>3</cell><cell>30k (3.8%)</cell></row><row><cell>Building</cell><cell>building, house</cell><cell>2</cell><cell>24k (3.1%)</cell></row><row><cell>Food</cell><cell>banana, orange, pizza</cell><cell>6</cell><cell>13k (1.6%)</cell></row><row><cell></cell><cell>Relations</cell><cell></cell><cell></cell></row><row><cell>Geometric</cell><cell>above, behind, under</cell><cell>15</cell><cell>228k (50.0%)</cell></row><row><cell>Possessive</cell><cell>has, part of, wearing</cell><cell>8</cell><cell>186k (40.9%)</cell></row><row><cell>Semantic</cell><cell>carrying, eating, using</cell><cell>24</cell><cell>39k (8.7%)</cell></row><row><cell>Misc</cell><cell>for, from, made of</cell><cell>3</cell><cell>2k (0.3%)</cell></row></table><note>a set B = {b 1 , . . . , b n } of bounding boxes, b i ? R 4 , ? a corresponding set O = {o 1 , . . . , o n } of objects, as- signing a class label o i ? C to each b i , and ? a set R = {r 1 , . . . , r m } of binary relationships be- tween those objects. Each relationship r k ? R is a triplet of a start node</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Results table, adapted from</figDesc><table><row><cell>Scene Graph Classification</cell><cell>Predicate Classification</cell><cell>Mean</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>11.8 14.1 27.9 35.0 ASSOC. EMBED [31] 8.1 8.2 21.8 22.6 54.2 55.5 9.7 11.3 26.5 30.0 68.0 75.2 MESSAGE PASSING [47] 3.4 4.2 21.7 24.4 44.8 53.0 MESSAGE PASSING+ 20.7 24.5 34.6 35.4 59.3 61.3 22.0 27.4 43.4 47.2 75.2 83.6 34.4 42.2 93.5 97.2 FREQ 23.5 27.6 32.4 34.0 59.9 64.1 25.3 30.9 40.5 43.7 71.3 81.2 37.2 45.0 88.3 90.1 FREQ-OVERLAP 26.2 30.1 32.3 32.9 60.6 62.2 28.6 34.4 39.0 43.4 75.7 82.9 41.6 49.9 94.6 96.9 MOTIFNET-NOCONTEXT 26.2 29.0 34.8 35.5 63.7 65.6 29.8 34.7 43.4 46.6 78.8 85.9 43.5 50.9 94.2 97.1 MOTIFNET 27.2 30.3 35.8 36.5 65.2 67.1 30.5 35.8 44.5 47.7 81.1 88.3 44.2 52.1 96.0 98.4</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Graph constraints</cell><cell></cell><cell></cell><cell cols="2">No graph constraints</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">SGDET</cell><cell>SGCLS PREDCLS</cell><cell cols="2">SGDET</cell><cell cols="3">SGCLS PREDCLS PHRDET</cell><cell cols="2">PREDDET</cell></row><row><cell></cell><cell>Model</cell><cell>R@50</cell><cell>R@100</cell><cell>R@50 R@100 R@50 R@100</cell><cell>R@50</cell><cell>R@100</cell><cell>R@50 R@100 R@50 R@100</cell><cell>R@50</cell><cell>R@100</cell><cell>R@50</cell><cell>R@100</cell></row><row><cell cols="5">[47]'s split 0.5 [25] split VRD [29], from [47] 0.3 MSDN [25] 10.7 14.2 24.3 26.5 67.0 71.0 MSDN 11.7 14.0 20.9 24.0 42.3 48.2 MSDN-FREQ 13.5 15.7 25.8 27.8 56.0 61.0 SCR[23]</cell><cell cols="2">10.67 13.81</cell><cell></cell><cell cols="2">16.58 21.54</cell><cell></cell></row><row><cell>other split</cell><cell>DR-NET[6] VRL[27] VTE[57] LKD[54]</cell><cell cols="2">12.57 13.34</cell><cell></cell><cell cols="2">20.79 23.76 5.52 6.04</cell><cell></cell><cell cols="4">23.95 27.57 88.26 91.26 14.36 16.09 9.46 10.45 92.31 95.68</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We consider several strategies to order the regions, see Section 5.1. 2 A union box is the convex hull of the union of two bounding boxes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">When sorting by confidence, the edge layer's regions are ordered by the maximum non-background object prediction as given byEquation 4.<ref type="bibr" target="#b3">4</ref> We remove the final ReLU to allow more interaction in Equation<ref type="bibr" target="#b5">6</ref>.<ref type="bibr" target="#b4">5</ref> A hypothetical model that perfectly classifies relationships, but only between boxes with nonzero IoU, gets 91% recall.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank the anonymous reviewers along with Ali </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">VQA: visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Annotating object instances with a polygon-rnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Castrejon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="4485" to="4493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hico: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mind&apos;s eye: A recurrent visual representation for image caption generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2422" to="2431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neil: Extracting visual knowledge from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1409" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Detecting visual relationships with deep relational networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An empirical study of context in object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1271" to="1278" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Building a large-scale multimodal knowledge base for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05670</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Every picture tells a story: Generating sentences from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hejrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rashtchian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="15" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Context based object categorization: A critical survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="712" to="722" />
		</imprint>
	</monogr>
	<note>Computer vision and image understanding</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Are you talking to a machine? dataset and methods for multilingual image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">E</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05612</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey on still image based human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep semantic role labeling: What works and whats next</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="1997-11-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05526</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3668" to="3678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hadamard Product for Low-rank Bilinear Pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 5th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">What are you talking about? text-to-image coreference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3558" to="3565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural amr: Sequence-to-sequence models for parsing and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Konstas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="146" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Vip-cnn: Visual phrase guided convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.07191</idno>
		<idno>arXiv: 1702.07191. 2</idno>
		<title level="m">ViP-CNN: Visual Phrase Guided Convolutional Neural Network</title>
		<imprint>
			<date type="published" when="2017-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scene graph generation from objects, phrases and region captions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">What, where and who? classifying events by scene and object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Deep Variation-structured Reinforcement Learning for Visual Relationship and Attribute Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03054</idno>
		<idno>arXiv: 1703.03054. 8</idno>
		<imprint>
			<date type="published" when="2017-03" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<meeting><address><addrLine>Zrich</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Actions in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2929" to="2936" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Pixels to graphs by associative embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-tosentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04870</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Galleguillos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Wiewiora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 11th international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Computer vision</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1612.08242</idno>
		<idno>arXiv: 1612.08242</idno>
		<title level="m">YOLO9000: Better, Faster, Stronger</title>
		<imprint>
			<date type="published" when="2016-12" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">End-to-end instance segmentation and counting with recurrent attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
			<affiliation>
				<orgName type="collaboration">cs</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:1506.01497</idno>
		<idno>arXiv: 1506.01497. 1</idno>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Image question answering: A visual semantic embedding model and a new dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.02074</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Edge and curve detection for visual scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Thurston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Comput</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="562" to="569" />
			<date type="published" when="1971-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Viske: Visual knowledge extraction and question answering by visual verification of relation phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1456" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Training very deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2377" to="2385" />
		</imprint>
	</monogr>
	<note>NIPS&apos;15</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Approaching the symbol grounding problem with probabilistic graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tellex</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dickerson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="64" to="76" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Grammar as a foreign language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Koo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2773" to="2781" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cnn-rnn: A unified framework for multi-label image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2285" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02426</idno>
		<idno>arXiv: 1701.02426</idno>
		<title level="m">Scene Graph Generation by Iterative Message Passing</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Modeling mutual context of object and human pose in human-object interaction activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Stating the obvious: Extracting visual common sense knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL</title>
		<meeting>NAACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Situation Recognition: Visual Semantic Role Labeling for Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Obj2text: Generating visually descriptive language from object layouts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Obj2text: Generating visually descriptive language from object layouts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.00278</idno>
		<title level="m">Visual madlibs: Fill in the blank image generation and question answering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Visual relationship detection with internal and external linguistic knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Zero-shot activity recognition with verb attribute induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno>2017. 7</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09239</idno>
		<idno>arXiv: 1606.09239. 8</idno>
		<title level="m">Learning Concept Taxonomies from Multimodal Data</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Visual translation embedding network for visual relation detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kyaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Highway long short-term memory rnns for distant speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="5755" to="5759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Reasoning about object affordances in a knowledge base representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="408" to="424" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
