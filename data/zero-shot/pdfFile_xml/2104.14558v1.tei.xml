<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research (FAIR)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a large-scale study on unsupervised spatiotemporal representation learning from videos. With a unified perspective on four recent image-based frameworks, we study a simple objective that can easily generalize all these methods to space-time. Our objective encourages temporallypersistent features in the same video, and in spite of its simplicity, it works surprisingly well across: (i) different unsupervised frameworks, (ii) pre-training datasets, (iii) downstream datasets, and (iv) backbone architectures. We draw a series of intriguing observations from this study, e.g., we discover that encouraging long-spanned persistency can be effective even if the timespan is 60 seconds. In addition to state-of-the-art results in multiple benchmarks, we report a few promising cases in which unsupervised pre-training can outperform its supervised counterpart. Code is made available at https://github.com/ facebookresearch/SlowFast.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A series of recent methods on unsupervised representation learning from images <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b8">9]</ref> are based on maximizing a similarity objective for different views of the same image under data augmentations <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b88">89]</ref>. In addition to the artificial augmentations on images, videos can provide natural augmentations of visual content under various changing factors, such as motion, deformation, occlusion, and illumination. This work aims to generalize these image-based methods <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b8">9]</ref> into space-time.</p><p>We study a simple objective that can be easily incorporated into these image-based methods. Our hypothesis is that the visual content is often temporally-persistent along a timespan in the video. This persistency may involve an action (e.g., a person dancing), an object (e.g., an individual person, who transitions from running to walking), and a scene (e.g., a room with people moving), covering short to long spans, with different levels of visual invariance (action, object, scene). Our objective simply encourages the visual representations in different clips of the same video  <ref type="figure">Figure 1</ref>. Learning to maximize the similarity between different temporal clips of the same video encourages feature persistency over time. A query clip (q) is matched to multiple key clips (k1, k2, . . .) that are temporally shifted. This method can be incorporated into several unsupervised learning frameworks (MoCo <ref type="bibr" target="#b35">[36]</ref>, SimCLR <ref type="bibr" target="#b11">[12]</ref>, BYOL <ref type="bibr" target="#b31">[32]</ref>, SwAV <ref type="bibr" target="#b8">[9]</ref>). The figure on the top shows that increasing the number (?) of temporal clips improves representation quality for all these frameworks.</p><p>to be similar. We empirically find that this objective works well across different unsupervised frameworks (MoCo <ref type="bibr" target="#b35">[36]</ref>, SimCLR <ref type="bibr" target="#b11">[12]</ref>, BYOL <ref type="bibr" target="#b31">[32]</ref>, SwAV <ref type="bibr" target="#b8">[9]</ref>), either with or without using dissimilar (negative) samples.</p><p>Our objective is a natural generalization of crops in images <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b88">89]</ref> to clips in videos. This allows us to make use of the recent unsupervised learning frameworks with minimal modifications. We aim to learn a high-level representation of the categorical semantics present in a video by enforcing persistency of the representation over space-time. We investigate factors such as the effective timespan, t, between positives, and number of temporal clips, ?, to find that longer timespans (up to a minute) and multiple samples are beneficial for downstream performance <ref type="figure">(Fig. 1)</ref>.</p><p>Our unsupervised training is performed on large-scale data, including Kinetics <ref type="bibr" target="#b46">[47]</ref> (240k videos) and three versions of million-scale Instagram sets. In addition to standard linear probing, we evaluate representation quality on multiple classification and detection downstream datasets, e.g., Charades <ref type="bibr" target="#b74">[75]</ref>, Something-Something <ref type="bibr" target="#b30">[31]</ref>, and AVA <ref type="bibr" target="#b32">[33]</ref>.</p><p>Our results suggest that unsupervised pre-training can achieve competitive performance in videos, and it can surpass the supervised pre-training counterparts in a few cases. Finally, our study also reveals room for improvement along multiple directions.</p><p>In summary, our large-scale study involves the following five aspects:</p><p>(i) Four unsupervised learning frameworks (MoCo <ref type="bibr" target="#b35">[36]</ref>, SimCLR <ref type="bibr" target="#b11">[12]</ref>, BYOL <ref type="bibr" target="#b31">[32]</ref>, SwAV <ref type="bibr" target="#b8">[9]</ref>) viewed from a unified perspective, and incorporated with a simple temporal persistency objective;</p><p>(ii) Three pre-training datasets, including the relatively well-controlled Kinetics <ref type="bibr" target="#b46">[47]</ref> and the relatively "in-the-wild" Instagram sets at million-scale;</p><p>(iii) Six downstream datasets/tasks for evaluating representation quality;</p><p>(iv) Ablation experiments on different factors, such as temporal samples, contrastive objective, momentum encoders, training duration, backbones, data augmentation, curated vs. uncurated, trimmed vs. untrimmed, etc.; and</p><p>(v) State-of-the-art results of unsupervised video representation learning on established benchmarks, UCF-101 <ref type="bibr" target="#b76">[77]</ref>, HMDB51 <ref type="bibr" target="#b49">[50]</ref> and Kinetics-400 <ref type="bibr" target="#b46">[47]</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unsupervised learning in images has been actively researched recently with approaches focusing on various pretext tasks related to color-or patch-based processing <ref type="bibr" target="#b66">[67,</ref><ref type="bibr" target="#b93">94,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b63">64]</ref>, instance discrimination with contrastive objectives <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b94">95,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b80">81]</ref> and ones that focus on positive pairs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Unsupervised learning in videos has followed a similar trajectory with earlier methods focusing on predictive tasks based on motion, color and spatiotemporal ordering <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b77">78,</ref><ref type="bibr" target="#b84">85,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b90">91,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b44">45]</ref>, and contrastive objectives with visual <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b91">92]</ref> and audio-visual input <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b68">69]</ref>.</p><p>Several recent ones <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b91">92,</ref><ref type="bibr" target="#b61">62]</ref> relate to image-based approaches <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b88">89]</ref>. With some of them using additional modalities of optical-flow <ref type="bibr" target="#b80">[81,</ref><ref type="bibr" target="#b34">35]</ref>, audio <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b61">62]</ref> and text <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b1">2]</ref> to transfer supervision from one modality to another.</p><p>In relation to these previous efforts, our work studies purely visual unsupervised learning from video and tries to compare the meta-methodologies on common ground.</p><p>Evaluation protocols and backbones in most imagebased approaches have converged to ResNet-50 <ref type="bibr" target="#b38">[39]</ref> encoders with ImageNet linear-classification protocol, and several smaller downstream tasks <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b8">9]</ref> for evaluation. In video understanding research, the field has not yet converged and is using different backbones with focus on finetuning performance on two relatively small datasets <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b49">50]</ref>.</p><p>We investigate this aspect by looking at different encoders and 6 different downstream benchmarks for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>The objective of this work is to study several recent unsupervised representation learning methodologies to train a spatiotemporal encoder f ? , exploring implementation details and comparing them on a common ground to measure their efficacy in video understanding. We focus on two contrastive approaches using positive and negative samples: SimCLR <ref type="bibr" target="#b11">[12]</ref> and MoCo <ref type="bibr" target="#b35">[36]</ref>, as well as two approaches that solely rely on positives, BYOL <ref type="bibr" target="#b31">[32]</ref> and SwAV <ref type="bibr" target="#b8">[9]</ref> (Sec. 3.2).</p><p>These approaches were originally presented for learning image representations, and they all share the objective of learning invariant features across different views (crops/augmentations) of the spatial image input. In this paper, this idea is extended to the temporal domain. Our core idea is to learn an encoder f ? that produces embeddings which are persistent in space-time, over multiple (?) temporally distant clips of the same video. This is related to Slow Feature Analysis <ref type="bibr" target="#b87">[88]</ref> where the objective is to minimize the representations' temporal derivative over the input. The general idea of learning temporally persistent features is not new and has been proposed in the past with similar motivation e.g., <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b28">29</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Persistent temporal feature learning</head><p>Our framework takes different augmented clips x of an unlabeled video and passes them through an encoder f ? with weights ? to obtain corresponding embeddings q = f ? (x). The encoder is spatiotemporal ConvNet, by default a ResNet-50 (R-50) <ref type="bibr" target="#b38">[39]</ref>, Slow-only pathway of SlowFast Networks <ref type="bibr" target="#b19">[20]</ref>, which is a 3D ResNet-50 <ref type="bibr" target="#b38">[39]</ref> without temporal pooling in convolutional feature maps, followed by an MLP projection head, that produces and output of dimension d.</p><p>The input clips are stacks of RGB frames of size 3 ? T ? S 2 for temporal ? spatial dimensions, which are sampled with temporal stride ? , i.e., the encoder processes only one out of ? frames of the raw video. Therefore, T ? ? define the timespan and resolution of the encoder.</p><p>Given a minibatch of B videos, our framework creates a set of ?B positive examples by sampling ? clips from the videos. The learning methodologies studied in this section maximize similarity of a "query" sample q with a set of positive "key" samples {k + } that are encoded versions of different clips of the same video as q is computed from. <ref type="figure">Fig. 1</ref> illustrates an example where ?=3 clips are used.</p><p>The next section describes how the contrastive and noncontrastive unsupervised representation learning methodologies are exemplified. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Unsupervised learning frameworks</head><p>Contrastive learning maximizes the similarity of a sample q with positive ones {k + } and minimizes similarity to negative ones {k ? }. The contrastive approaches in this paper use the InfoNCE <ref type="bibr" target="#b82">[83]</ref> objective,</p><formula xml:id="formula_0">Lq = ? log k?{k + } exp (sim(q, k)/?) k?{k + ,k ? } exp (sim(q, k)/?) ,<label>(1)</label></formula><p>with ? being a temperature hyper-parameter for scaling and {k + } are embedded clips of the same video as q. All the embeddings are 2 normalized and dot product (cosine) similarity is used to compare them sim(q, k) = q k/ q k .</p><p>SimCLR <ref type="bibr" target="#b11">[12]</ref> ( <ref type="figure" target="#fig_1">Fig. 2a</ref>) uses the embeddings of clips of other videos in the minibatch as negatives {k ? }.</p><p>MoCo <ref type="bibr" target="#b35">[36]</ref>  <ref type="figure" target="#fig_1">(Fig. 2b</ref>) is a method that uses an explicit momentum encoder which parameters, ? m , are a moving average ? m ? m? k + (1 ? m)? with m a momentum parameter. In eq. (1) MoCo uses this encoder to compute the positive embeddings {k + } from clips of the same video as q, and negative embeddings {k ? } are taken from a queue that stores embeddings of clips from previous iterations. There is no backpropagation into the momentum-encoder weights ? m . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BYOL</head><formula xml:id="formula_1">L q = D KL (q SK(k + )),<label>(3)</label></formula><p>where D KL is the The Kullback-Leibler divergence and gradients are not back-propagated through the SK operation. Compared to SimCLR and MoCo, in BYOL and SwAV, q and k are not typical "query" and "key" samples (but rather "source" and "target" samples); however, for consistency we use q, k terminology in notation for all methods.</p><p>Implementation specifics. We implement the methods with a symmetric loss, as in original SimCLR, BYOL and SwAV, where every input clip is used to produce a loss (and gradient) signal. For each of the ? ? 2 clips, we compute q, while all other ??1 clips of the same video are used as {k + } to evaluate sub-loss L q and the symmetric loss is the average over all ? sub-losses. Thus, for MoCo and BYOL, every input clip is processed by both encoders.</p><p>For MoCo and BYOL, our symmetric loss is aggregated sequentially which implies that memory consumption for ? &gt; 2 equals to a single clips' forward and backward pass, since these methods do not backpropagate through the momentum encoder. For SimCLR and SwAV the overall loss is evaluated in parallel across all clips and therefore memory consumption grows linearly with the number of clips used.</p><p>All details on implementation and pre-training are in ?B.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. Unless otherwise noted, we perform unsupervised pre-training on Kinetics-400 <ref type="bibr" target="#b46">[47]</ref> (K400) with ?240k training videos in 400 human action categories. data #videos t median tmean t std t min tmax Kinetics-400 (K400) <ref type="bibr">[</ref> To study learning from "in-the-wild" videos from the web, we pre-train the methods on Instagram videos: IG-Curated <ref type="bibr" target="#b23">[24]</ref>, a dataset with hashtags similar to K400 classes; IG-Uncurated which has videos taken randomly from Instagram; and IG-Uncurated-Short which is similar, but has constrained duration. Each dataset has 1M videos. <ref type="table" target="#tab_1">Table 1</ref> shows dataset statistics of all datasets used for unsupervised pre-training. Most of Kinetics videos are of 10 seconds in duration. IG-Curated is a dataset with Instagram videos that have an average duration t mean of 26.3 seconds and a standard deviation t std of 29.8 seconds. The maximum duration t max is 60s. IG-Uncurated contains videos taken randomly from Instagram, with larger deviation in length and maximum duration of 10 minutes (600s). IG-Uncurated-Short is a dataset consisting of random Instagram videos that have a duration between 10 and 16 seconds, to study the effect of a fixed duration and the assumption that short videos may hold more useful information for pre-training.</p><p>Evaluation protocols. For evaluation we use two protocols.</p><p>The first one is common to evaluate unsupervised image representations <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b11">12]</ref>. It validates the linear classifier performance based on frozen encoder features that are taken from the global average pooling layer. We report top-1 classification accuracy (%) on the K400 validation set.</p><p>The second protocol reports finetuning accuracy on the first split of the UCF101 dataset <ref type="bibr" target="#b76">[77]</ref> which contains 13k videos in 101 human action classes; this is a common procedure used to evaluate unsupervised video representations. Finally, we also report finetuning accuracy on AVA <ref type="bibr" target="#b32">[33]</ref>, Charades <ref type="bibr" target="#b74">[75]</ref>, Something-Something <ref type="bibr" target="#b30">[31]</ref> and HMDB51 <ref type="bibr" target="#b49">[50]</ref>.</p><p>Architecture. By default, we use a R-50 <ref type="bibr" target="#b38">[39]</ref> following the Slow pathway in <ref type="bibr" target="#b19">[20]</ref> with clips of T =8 frames sampled with stride ? =8 from 64 raw-frames of video. The supervised performance for training 200, 400, 800 epochs on K400 is 74.7%, 74.3% and 72.7%, respectively, and does not improve for training longer due to overfitting. Implementation details. We follow default settings in video classification <ref type="bibr" target="#b19">[20]</ref>. Specifics on the approaches, their training and evaluation and the impact of implementation on performance are provided in ?B and ?A.3, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Persistent temporal learning</head><p>Here, we investigate the impact of learning spatiotemporal vs. only spatial persistent features. <ref type="table" target="#tab_3">Table 2</ref> shows the accuracy of the four methods when trained for 200 epochs on K400, and evaluated on K400 (linear) and UCF101 (finetuned), i.e. our default setting.   Temporal augmentation. The first row in <ref type="table" target="#tab_3">Table 2</ref>, ?=1, uses two spatial crops at the same temporal instance, while the ?=2 row uses clips at different temporal locations as positives; therefore, learns persistent features in time. This difference has a large impact on performance, especially for SimCLR (60.5 ? 36.1) and SwAV (61.6 ? 38.6) performance degrades significantly when sampling positives from the same temporal instance (?=1).</p><p>More clips are beneficial. The remaining rows in <ref type="table" target="#tab_3">Table 2</ref> show that accuracy is further increasing with the number of temporal samples per video, e.g. at ?=4 the best accuracy is achieved with BYOL at 68.9% K400 and 93.8% UCF101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Negatives do not help but momentum encoders do.</head><p>When comparing the methods in <ref type="table" target="#tab_3">Table 2</ref>, we see that:</p><p>(i) There is no clear performance difference between contrastive/non-contrastive methods. This indicates that learning space-time persistence within a video is key for the methods, but learning in-persistence across videos is not.</p><p>(ii) There is a clear difference of ?4% on K400 between methods that employ momentum encoders (MoCo, BYOL), vs. these that do not (SimCLR, SwAV).</p><p>Increasing the number of clips per training iteration increases training cost, so it is reasonable to compare it to training more epochs. <ref type="table" target="#tab_4">Table 3</ref> is studying the base case ?=2 for various number of epochs (ep).</p><p>Overall, the results show that there is a clear gain for training longer which has been also observed in image-related tasks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b8">9]</ref>. BYOL performs the worst when training short durations. This might be related to hyper-parameter settings which we do not adjust for this experiment (the original implementation <ref type="bibr" target="#b31">[32]</ref> uses different hyper-parameters for different number of training epochs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Timespan between positives</head><p>All experiments with ??2 so far were using global temporal sampling of positives, which means that the clips can be sampled at unconstrained temporal locations from the input video. This might be counter-productive because if there is a long duration that has passed between a pair of positive clips they might no longer share the same semantic context for learning high-level features corresponding in time.  <ref type="table">Table 4</ref>. Maximum frame distance for positives. Method: BYOL, ? = 2. Training is surprisingly robust with increasing accuracy for increased distance between samples. Accuracy only (mildly) degrades when sampling positives that are more than 36 seconds apart when using uncurated (random) videos.</p><p>This experiment is concerned with the maximum distance between the positive training samples. We use BYOL pretraining on K400, IG-Curated-1M and IG-Uncurated-1M and report 400 linear readout accuracy in <ref type="table">Table 4</ref>. <ref type="table">Table 4a</ref> shows performance for increasing the maximum temporal distance between positives in K400 pre-training. It can be seen that using positives from the same time (t max =0) degrades perforance b ?5% but other than that performance is relatively robust up to global sampling of positive clips from the whole video (t max =10s). This is interesting as it seems that a long-temporal correspondence objectives does not hurt performance (but also does not boost it). <ref type="table">Table 4b</ref> shows performance for increasing the temporal distance between positive samples on IG-Curated-1M. This dataset has a maximum duration of 60 seconds; statistics are in <ref type="table" target="#tab_1">Table 1</ref>. <ref type="table">Table 4b</ref> reveals that increasing the maximum duration between positive pairs is beneficial for performance and unrestricted sampling of positives is the best with 64.1% top-1 accuracy for evaluation on K400. This is especially interesting, as it shows that even longer videos benefit from global sampling. There is no benefit from restricting the time window of positives, which can be interpreted as the objective of learning extremely-slow features <ref type="bibr" target="#b87">[88]</ref> that do not change over 60 seconds of video. Long-temporal-distance samples might also increase robustness of the model by providing "hard-positive" samples for learning. Note that here the videos are still sampled according to hashtags related to K400 classes <ref type="bibr" target="#b23">[24]</ref>; therefore, the conjecture might be biased.</p><p>Finally, we are looking at the IG-Uncurated-1M dataset which consists of a random sampling of 1M videos from Instagram. These videos can be between 0.5s and 10 minutes of duration. Most of the videos however are much shorter than 10 minutes, with a mean duration of 35.3 seconds and a standard deviation of 38.4 seconds <ref type="table" target="#tab_1">(Table 1)</ref>. For this data, <ref type="table">Table 4c</ref> shows the results of progressively increasing the maximum timespan between positive samples. It can be observed that increasing the maximum distance between positives up to 36 seconds is beneficial and beyond that performance decreases, but only slightly, even when performing global sampling of positives (the default).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Backbone architectures</head><p>So far all experiments were using a R-50, 8?8 Slow pathway <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b19">20]</ref>   <ref type="table">Table 5</ref>. Backbone comparison. The ResNet <ref type="bibr" target="#b38">[39]</ref> backbone (Slow pathway <ref type="bibr" target="#b19">[20]</ref>) is used with different depth (R-18, R-50, R-101), input frames T and stride ? . R2+1D <ref type="bibr" target="#b81">[82]</ref> and S3D-G <ref type="bibr" target="#b89">[90]</ref> are commonly used backbones for unsupervised video representation learning with downstream evaluation on UCF101. <ref type="table">Table 5</ref> compares different backbones for usage with MoCo in our default setting (?=2, 200 epoch pre-training on K400). From left to right, the table shows the input duration T , sampling-rate ? , FLOPs (at 224 2 spatial resolution) and parameters of these backbones, as well as the average duration for training one iteration of the MoCo algorithm (measured on a single machine with 8 V100 GPUs in PySlowFast <ref type="bibr" target="#b18">[19]</ref> and torchvision decoder), the supervised performance on K400 and UCF101 (finetuned from K400), as well as the downstream performance for K400 linear evaluation and UCF101 finetuning.</p><p>The first observation in <ref type="table">Table 5</ref> is that for the Slow architecture <ref type="bibr" target="#b19">[20]</ref>, using shallower (R-18) or deeper (R-101) networks can influence supervised and downstream performance in a sizable manner, with MoCo, K400 evaluation benefiting from more parameters. Doubling the input framerate (8?8 ? 16?4) boosts accuracy on UCF101.</p><p>The second observation is that R2+1D <ref type="bibr" target="#b81">[82]</ref> has a large gap on Kinetics (71.7% supervised vs. 57.2% unsupervised), while being remarkably strong on UCF101 (93.7%). This gap is also observed for S3D-G <ref type="bibr" target="#b89">[90]</ref>. The reason for this might be that UCF101 is a small dataset which is easy to overfit and can benefit from fewer parameters.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Uncurated data and video duration</head><p>In <ref type="table" target="#tab_8">Table 6</ref> we show the performance of all four methodologies on IG-Curated-1M (a), IG-Uncurated-1M (b) and IG-Uncurated-Short-1M (c) for pre-training with 50 and 200 epochs. We make the following observations:</p><p>(i) Among the methods MoCo performs the best with e.g. 69.0% vs. second-best 64.3% of SwAV on curated data (a).</p><p>(ii) MoCo and SwAV scale the best for training longer, gaining roughly 3-4% for 200ep vs. 50ep.</p><p>(iii) On uncurated data, MoCo and SwAV perform ?1% better on the unconstrained duration videos in <ref type="table" target="#tab_8">Table 6b</ref>.</p><p>(iv) BYOL and SimCLR show better performance on IG-Uncurated-Short (10-16s videos) in <ref type="table" target="#tab_8">Table 6c</ref>, seemingly benefiting from shorter videos, but there is no clear benefit from either longer or shorter duration among all methods.</p><p>(v) BYOL degrades performance for training longer which might be due to the requirement of different hyperparameters for different schedules (as noted in Sec. 4.1).</p><p>We will return to this point in ?A.1, where we show that increasing clips-size ? can overcome this issue in BYOL, along with further studies on the trade-off against training more epochs, and dataset scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Data augmentations</head><p>Importance of augmentations. Augmentations can have a major impact on visual unsupervised feature learning <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>. In <ref type="figure" target="#fig_3">Fig. 3</ref>, we ablate spatial cropping (S), temporal clipping (T) and radiometric color (C) augmentations from the four unsupervised learning methods (e.g. "T S C" are the baselines using all augmentations and removing "S C" equals ?=1 in <ref type="table" target="#tab_3">Table 2</ref>). We make three main observations:</p><p>(i) Among the methods, MoCo and BYOL perform most robust for using fewer augmentations; their advantage over SimCLR and SwAV might be related to the momentum encoder which can provide extra augmentation in training.  (ii) When minimizing the augmentations by resizing the shorter size of the video to the input size of 224 and only cropping along the long side of the video (Base in <ref type="figure" target="#fig_3">Fig. 3</ref>), MoCo still provides 42.2% K400 linear accuracy, over BYOLs' 32.4%, showing an advantage of the contrastive loss in a weak augmentation scenario.</p><p>(iii) Among the augmentations, learning temporal (T) persistency, has the largest impact on performance, except for MoCo which benefits more from color (C) (incl. grayscale) augmentations. Especially SimCLR and SwAV show significant drops in performance when removing T, i.e. when extracting positive clips from the same instance in time.</p><p>In the remainder of this section, we explore using stronger augmentations than the default ones in previous experiments. We perform the ablations with MoCo in the basic setting of ? = 2, 200 epochs K400 pre-training. Stronger color augmentation. In <ref type="table" target="#tab_9">Table 7</ref> color strength of 0.5 indicates the default one for MoCo <ref type="bibr" target="#b13">[14]</ref>, 0.75 and 1.0 increase the strength of randomly jittering brightness, contrast, saturation and hue proportionally. <ref type="table" target="#tab_9">Table 7</ref> shows that increasing it to 0.75 can improve K400/UCF101 accuracy. Increasing the random grayscale probability from 0.2 to 0.4 does not provide an improvement on either of the datasets. However, using a temporaldifference augmentation which randomly (with probability 0.2) first converts the frames to grayscale and then subtracts them across time, can increase K400 accuracy by 0.4%. Finally, using frame-rate jittering of ?50% of the original frame-rate does not improve K400 but UCF101 slightly. Spatial cropping. Our default implementation uses VGGstyle <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b38">39]</ref> cropping that randomly resizes the shorter spatial side of a video between [256, 320] pixels and takes a random 224 2 crop extended over time to extract a clip <ref type="bibr" target="#b19">[20]</ref>.</p><p>Since unsupervised learning might benefit from more aggressive cropping, we explore Inception-style <ref type="bibr" target="#b79">[80]</ref> cropping with aspect ratio augmentation that is commonly used in unsupervised learning from images <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b8">9]</ref>. This cropping procedure randomly resizes the input area between a minimum scale and a maximum scale and jitters aspect ratio between 3/4 to 4/3, before taking a 224 2 crop.</p><p>We do not change the cropping for downstream training, as this can drop accuracy significantly (by ?2% on K400).</p><p>In <ref type="table" target="#tab_10">Table 8</ref> we ablate this approach for MoCo (the augmentation in the downstream evaluators are unchanged).</p><p>The first ablation shows the comparison of default cropping <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b38">39]</ref> with a similar version that randomly crops a fraction between [0.49, 0.76] = [224 2 /320 2 , 224 2 /256 2 ] of the original area, instead of the short-side. The performance degrades by 1% on K400 linear evaluation. Randomly cropping based on area favors larger crops over the short-side resizing and we observe lower training error for this variant.</p><p>Next, adding aspect ratio augmentation can recover some of this performance (65.4%), and using a smaller minimum area of 0.2, with the maximum area of 0.76 leads to best performance of 66.8%. Using the default values for Inception <ref type="bibr" target="#b79">[80]</ref> training, [0.08, 1.00], appears to be too aggressive. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Alternative downstream tasks</head><p>The gap between K400 and UCF101 accuracy in Sec. 4.3 question if solely looking at typical evaluation of UCF101 (or the smaller HMDB51) is enough to identify and rank approaches for unsupervised learning in video. <ref type="table" target="#tab_1">Table 10</ref> studies several new downstream tasks for unsupervised representation learning in video. We use our MoCo, SimCLR, BYOL and SwAV models trained with ?=3 for 200 epochs on K400 and evaluate their performance by finetuning on Charades <ref type="bibr" target="#b74">[75]</ref>, AVA <ref type="bibr" target="#b32">[33]</ref>, or Something-Something <ref type="bibr" target="#b30">[31]</ref> (in addition to the K400 linear readout performance and UCF101 performance reported in <ref type="table" target="#tab_3">Table 2</ref>). Details on implementation are given in ?B.</p><p>The first two rows in <ref type="table" target="#tab_1">Table 10</ref> show the two main competitors for this evaluation: (i) training from scratch on the datasets and (ii) K400 pre-training. First, we observe that the supervised pre-trained backbones outperform the trainfrom-scratch counterpart significantly, as expected.</p><p>Downstream datasets. For K400 pre-training and linear evaluation, its supervised counterpart has an advantage between 12.7% and 6.4% top-1 accuracy among the methods.</p><p>On UCF101 unsupervised pre-training is only 1% lower than the supervised counterpart for BYOL (the strongest).</p><p>On AVA short-term action detection we observe that the BYOL pre-trained model is able to outperform the supervised counterpart by +1.2% mAP, when using the same, fixed region proposals <ref type="bibr" target="#b19">[20]</ref>. This result is significant, as e.g. switching from K400 to K600 (nearly double the size of K400) pre-training on AVA leads to a smaller gains in performance <ref type="bibr" target="#b19">[20]</ref>. Overall this is a surprising result as the tasks in K400 and AVA are similar <ref type="bibr" target="#b51">[52]</ref>, only that the temporal granularity of the actions in AVA is finer while their semantic granularity is coarser; e.g. "shoot" in AVA vs. "playing paintball" in Kinetics, which might be better captured by the BYOL objective which solely works on positive temporal samples of a video, without contrasting them to other videos ("shoot" might be a positive appearing in many different videos and contrasting them could be harmful to downstream performance). This line of thinking is supported with MoCo's (contrastive objective) performance that is 3.1% worse than BYOL on AVA. Similarly, SimCLR (contrastive) is worse than SwAV (non-contrastive) when benchmarked on AVA.</p><p>On Charades, long-term action classification, we observe the opposite. Here, the contrastive MoCo is clearly the best performer with 33.5% mAP (close to the supervised pre-training performance of 34.7% mAP), while the noncontrastive BYOL is 12.5% lower. Similarly, now SimCLR (contrastive) is better than SwAV (non-contrastive). Compared to AVA, Charades is a temporally less localized dataset containing activities that need to be recognized from a longer temporal range video, for which contrastive pre-training appears to be outperforming the non-contrastive variants. Pre-training sets: Kinetics vs. IG. Next, we experiment with pre-training on videos from the web. We first investigate IG-Curated-1M <ref type="bibr" target="#b23">[24]</ref>, which is a dataset that has been collected with hashtags that are similar to Kinetics labels. This data is a 1M subset of the original 65M introduced in <ref type="bibr" target="#b23">[24]</ref>. Using this data (penultimate row in <ref type="table" target="#tab_1">Table 10</ref>) can excel the performance of MoCo with K400 pre-training, which has a training set of 240K samples (roughly 4.2? smaller), and surprisingly even outperforms pre-training on K400 linear readout itself (69.9% vs. 67.3% accuracy).</p><p>Second, we ablate the effect of using uncurated videos, with IG-Uncurated-1M which are purely random videos taken from the web. On most downstream tasks performance shown in the last row of <ref type="table" target="#tab_1">Table 10</ref> is equal or only slightly lower than pre-training on K400. Specifically, MoCo changes by -1.3% on K400 (as expected), +0.1% on UCF, +0.2% on AVA, -2.2% on Charades and -1.2% on Something-Something v2. This is an encouraging result for unsupervised learning, as only ?4.2?the number of videos but random ones are required to match the performance of supervised K400 pre-training on the UCF101 and AVA.</p><p>Overall, our results indicate that unsupervised pretraining can be a new paradigm for all of these downstream tasks, for which supervised pre-training is the de-facto standard to achieve best performance. Further, the large difference in performance for pre-training methodologies and objectives (e.g. contrastive/non-contrastive) revealed in the light of these benchmarks signals large room for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Comparison to previous work</head><p>In a final experiment we take the best model from Table 9 and compare it with the state-of-the-art using the commonly used protocols on UCF101 and HMDB51 (across all 3 train/val splits) and K400. In <ref type="table" target="#tab_1">Table 11</ref> we show the results.</p><p>The strongest previous approaches are using multi-modal input, Vision "V", Audio "A", Text "T", to train a contrastive objective across modalities; XDC <ref type="bibr" target="#b2">[3]</ref> performs DeepCluster <ref type="bibr" target="#b7">[8]</ref> on (V+A), CVRL <ref type="bibr" target="#b70">[71]</ref>, GDT <ref type="bibr" target="#b67">[68]</ref> and MMV <ref type="bibr" target="#b1">[2]</ref> use an objective similar to SimCLR on (V), (V+A), and method pre-train backbone param T mod UCF HMDB K400 XDC <ref type="bibr" target="#b2">[3]</ref> K400 R(2+1)D- <ref type="bibr" target="#b17">18</ref>   <ref type="table" target="#tab_1">Table 11</ref>. Comparison with state-of-the-art. "param" indicates the number of parameters, T inference frames, in the backbone. "V" is Vision, "A" is Audio, "T" Text modality. ?BYOL is our best model trained with temporal persistency of ?=4. We report fine-tuning accuracy on UCF/HMDB and linear accuracy on K400.</p><p>(V+A+T), with the latter training on a Audioset (AS) <ref type="bibr" target="#b22">[23]</ref> and HowTo100M (HT) <ref type="bibr" target="#b58">[59]</ref>, and CoCLR <ref type="bibr" target="#b34">[35]</ref> can be seen as a variant of MoCo on rgb and optical-flow input. In comparisons, our best performing model ?BYOL, which is BYOL trained with temporal persistency over ?=4 clips, (cf <ref type="table" target="#tab_3">. Tables 2 &amp; 9</ref>), provides a substantial performance gain over the best published method <ref type="bibr" target="#b34">[35]</ref>: +5.7% and +12.1% top-1 accuracy on UCF101 and HMDB51 (using identical backbone and pre-training data).</p><p>On K400 linear evaluation with the same data and R-50, Slow pathway <ref type="bibr" target="#b19">[20]</ref> as backbone, our approach outperforms the concurrent CVRL <ref type="bibr" target="#b70">[71]</ref> by +5.4% accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper has studied four meta-methodologies for unsupervised learning from video. Our findings include that it is beneficial to sample positives with longer timespans between them, contrastive objectives are less influential than momentum encoders, and training duration, backbones, video augmentation and curation are all critical for good performance. Our resulting models which learn persistent features across augmented spacetime clips set a new state-of-the-art.</p><p>We observed that linear readout on Kinetics is a good indicator of the performance on other datasets and that unsupervised pre-training can compete with the supervised counterpart on several datasets, but there is room for improvement. We hope that our baselines will foster research and provide common ground for future comparisons.</p><p>This appendix provides additional material: ?A contains further results on "in-the-wild" data ( ?A.1), Kinetics-600 (K600) <ref type="bibr" target="#b9">[10]</ref> and Kinetics-700 (K700) <ref type="bibr" target="#b10">[11]</ref> data ( ?A.2) and on the effect of key implementation details ( ?A.3).</p><p>?B contains additional implementation details for: Unsupervised pre-training ( ?B.1), and downstream evaluation in Kinetics ( ?B.2), AVA ( ?B.3), Charades ( ?B.4), Something-Something V2 ( ?B.5), UCF101 ( ?B.6), HMDB51 ( ?B.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Scaling "in-the-wild" data</head><p>As a follow-up experiment  We also explore an experiment for increasing the clipsize in MoCo and training longer (as MoCo works stable for more epochs). <ref type="table" target="#tab_1">Table 13</ref> shows the results. It can be observed that increasing the number of clips from ?=2 to ?=3 can increase the results by 1.6%/0.9% K400 and 0.4%/1% on UCF101 for 100/200ep training. Going to ? = 4 brings further gain. In terms of efficiency, increasing ? is both more accurate and faster than increasing the number of epochs, e. Finally, we remark that the IG-Curated-1M is subsampled such that the hastags are uniformly distributed (roughly balanced). Therefore this dataset is matching K400 in terms of content and distribution. We revisit this point next by investigating the effect of scale, curation and balancing of the video data.</p><p>In this experiment, we increase the scale of the data from 128K to 1M distinct videos. We increase dataset size (number of videos) for IG-Curated <ref type="bibr" target="#b23">[24]</ref>, IG-Curated-Unbalanced <ref type="bibr" target="#b23">[24]</ref> (which has random class distribution), and   <ref type="figure" target="#fig_5">Fig. 4</ref> and reveals: (i) Comparing the curation axis: At 240K training samples, the four data sources provide 65.8%, 63.2%, 63.1%, 60.6% top-1 accuracy for K400, IG-Curated, IG-Curated-Unbalanced and IG-Uncurated, respectively. The decay from the heavily curated K400 to IG-Curated (2.6%) is similar to the one from IG-Curated to IG-Uncurated (2.5%), while the class balancing seems to have a minor effect on accuracy.</p><p>(ii) Comparing the scale axis: Doubling the data scale (number of videos) roughly linearly increases the accuracy across all datasets. With 1M uncurated videos the performance approaches 65.4% which is similar to the 65.8% produced by using K400 pre-training. The experiment indicates that it is possible to approach unsupervised Kinetics pretraining when using 4?more (1M vs. 240K in Kinetics), but random, videos when evaluating on Kinetics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Scaling Kinetics data</head><p>As referenced in Sec. 4 of the main paper, <ref type="table" target="#tab_1">Table 14</ref> shows a series of extra results for pre-training on the larger-scale Kinetics-600 (K600) <ref type="bibr" target="#b9">[10]</ref> and Kinetics-700 (K700) <ref type="bibr" target="#b10">[11]</ref> datasets, and is analyzed next: The first row of the table shows supervised training on the respective datasets, where UCF101 has two entries, one for training-from-scratch and one for using K400 as pre-training.</p><p>For the experiments we focus on our temporally persistent MoCo algorithm and, as in the main paper, evaluate Kinetics with the linear classification protocol and UCF101 by finetuning all weights. The first unsupervised row in <ref type="table" target="#tab_1">Table 14</ref> shows our best K400 pre-trained MoCo (?=4) model, achieving 69.0%, 70.0%, 54.2% and 93.6% on K400, K600, K700 and UCF101, respectively (this is the model with strong augmentations from <ref type="table" target="#tab_1">Table 10</ref> of the main paper).  <ref type="figure" target="#fig_5">(?=4)</ref>, is able to approach supervised pre-training on the popular UCF101 evaluation protocol, but there remains a gap for the linear protocol on K400, K600 and K700.</p><p>The next row shows MoCo trained on K600 with a temporal persistency objective across two clips, ?=2. This version is able to slightly outperform the K400 pre-trained variant on all datasets, except UCF101. Directly comparing this version with learning temporal persistency across ?=4 clips can significantly increase accuracy on all datasets by ?2%.</p><p>The final two rows of <ref type="table" target="#tab_1">Table 14</ref>, show the same two models when pre-trained on K700. Here, we see that going from K400 to K700 increases accuracy by 2.7%, 3.2% and 3.9%, 1.2% on K400, K600, K700 and UCF101, respectively.</p><p>Overall the experiments suggest clear benefits of using larger-scale datasets for unsupervised pre-training and room for improvement under the linear classification protocol, especially when evaluated on larger datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Key implementation specifics</head><p>While the full implementation details of all four metamethodologies are provided in ?B.1, we want to discuss the most impactful ones, which we found critical to achieve good performance in their realizations, throughout this section. Momentum annealing. BYOL is using an annealing of the rate at which parameters of the momentum encoder ? m , that are a moving average, with momentum m, of the trained encoder ?. During training BYOL starts with a momentum of m base =0.996 and increases it to 1 with a cosine annealing m = 1 ? (1 ? m base ) ? (cos(?k/K) + 1)/2 with k the current iteration and K the maximum number of training iterations <ref type="bibr" target="#b31">[32]</ref> (this is unrelated to the learning rate decay). By default MoCo, is using a fixed momentum of m = 0.999 during training. In <ref type="table" target="#tab_1">Table 15</ref>, we ablate the positive (or negative) effect of using momentum annealing with different starting rates m base for MoCo. We observe that not using any annealing (N/A) produces 64.5% accuracy and using momentum annealing can boost this performance by ?1%, while being relatively stable for different values of m base . Consequently, we are using momentum annealing with m base = 0.994 for all our MoCo experiments.  <ref type="figure">Figure 5</ref>. Key implementation specifics. BYOL, SimCLR, SwAV heavily rely on LARS, SyncBN, and BN in the MLP (MLP-BN), MoCo does not require these, but does not benefit of having them.</p><p>Normalization and optimization. Here, we present normalization specifics that we found critical to achieve good performance in the underlying implementation of the methods: SimCLR, BYOL and SwAV are using synchronized Batch-Normalization (BN) <ref type="bibr" target="#b41">[42]</ref> statistics (SyncBN) across 8 GPUs during training, batch-normalization after every MLP layer (MLP-BN), and a large-batch optimizer (LARS) <ref type="bibr" target="#b92">[93]</ref>. LARS adaptively scales the learning rate for each individual parameter by using the ratio between gradient and parameter magnitudes. MoCo is not using these components (None) by default. In <ref type="figure">Fig. 5</ref> we illustrate the results. It shows accuracy on K400 linear readout, if step-by-step adding these specifics to the methods. We make the following observations:</p><p>(i) Using None of the augmentations provides best performance for MoCo (its default) but significantly degrades BYOL, SimCLR and SwAV. Here, it is worth noting that BYOL provides decent accuracy of 32.9% without SyncBN, LARS and any BN in the MLP.</p><p>(ii) Adding LARS optimizer reduces performance in MoCo and BYOL, while having a boost of around 10% for both SimCLR and SwAV. It is interesting, that solely using a more advanced optimizer, which adapts the learning rates of the weights according to their gradient magnitudes, decreases performance in methods using a momentum encoder (MoCo, BYOL), but boosts it without (SimCLR, SwAV).</p><p>(iii) further adding SyncBN and MLP-BN increases BYOL performance dramatically; this related to recent studies <ref type="bibr" target="#b72">[73]</ref> which suggest that normalization is important to achieve good performance using BYOL.</p><p>(iv) While BYOL, SimCLR and SwAV do show further gains for adding SyncBN and MLP-BN, MoCo shows no significant change for using SyncBN, and degrades drastically in performance for using BN in the MLP-head.</p><p>Projection MLP. It has been shown that using a deeper projection MLP in pre-training can increase the accuracy of the resulting representations for image classification <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b12">13]</ref>. Here, we investigate the effect of more hidden layers for video classification, across all four meta architectures. The results are shown in <ref type="table" target="#tab_1">Table 16</ref> and discussed next.</p><p>(i) MoCo achieves a significant gain of 1.2% on K400 for using a 3-layer (2 hidden layers) MLP vs. using a 2layer MLP and there is no gain for using a 4 th layer. UCF performance appears stable to this modification. The gain is in line with results in image classification <ref type="bibr" target="#b13">[14]</ref>.</p><p>(ii) For BYOL, which has an additional Predictor MLP, with weights ? p (see <ref type="figure" target="#fig_1">Fig. 2c</ref>), we ablate two dimensions: increasing the projection depth, and the prediction depth. Our results show that using 3-layer projection vs. 2-layer does not affect performance on K400, and has a decay of -0.7% on UCF101. Increasing also the depth of the predictor from our default value of 2 to 3 layers will lead to a significant decrease of -2.2% and -2.5% on both K400 and UCF101.</p><p>(iii) SimCLR, shows similar behavior as MoCo: A consistent gain for using 3 projection layers (+1.5% on K400, +0.5% on UCF101), and no further gain for a 4-layer MLP.</p><p>(iv) SwAV shows continuing gains on K400 for adding more MLP layers, +1.3% for going from 2 to 3 and another +0.4% for 4-layer MLP; however, its UCF-101 performance is decaying with more projection layers.</p><p>Overall, <ref type="table" target="#tab_1">Table 16</ref> suggests that K400 linear evaluation accuracy gernally benefits from deeper projection heads, while the performance for fine-tuned UCF101 downstream performance is relatively unchanged and rather shows a decaying effect for deeper MLPs. When studying the training complexity for pre-training, which we measure as floating point operations (FLOPs) and Parameters for the full training architecture (encoders + MLPs), <ref type="table" target="#tab_1">Table 16</ref> shows that FLOPs are mostly unchanged by deeper MLPs (as they operate on feature maps of size 1?1?1), but parameters increase leading to large models especially for momentum encoder based approaches (MoCo and BYOL).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional Implementation Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Unsupervised pre-training</head><p>Training details. We use the initialization outlined in <ref type="bibr" target="#b37">[38]</ref>. The projection and prediction MLP weights are initialized with <ref type="bibr" target="#b26">[27]</ref>. We optimize with synchronized SGD training on 64 GPUs with a mini-batch size of 8 clips per GPU; therefore, the total mini-batch size is 512. We train with Batch Normalization (BN) <ref type="bibr" target="#b41">[42]</ref>, and the BN statistics are computed within each <ref type="bibr" target="#b7">8</ref>  synchronizing across 8 GPUs (SyncBN) for BYOL, SimCLR and SwAV. We adopt a half-period cosine schedule <ref type="bibr" target="#b55">[56]</ref> of learning rate decaying: the learning rate at the n-th iteration is ??0.5[cos( n nmax ?)+1], where n max is the maximum training iterations and the base learning rate ? is set for each method to ? MoCo = 0.4, and ? SimCLR = ? BYOL = ? SwAV = 4.8.</p><p>We apply (LARS) <ref type="bibr" target="#b92">[93]</ref> (except for bias and BN parameters <ref type="bibr" target="#b31">[32]</ref>), with trust coefficient of 0.001, for BYOL, Sim-CLR, and SwAV training. The SGD weight decay is 10 ?4 for MoCo and 10 ?6 for for BYOL, SimCLR and SwAV. The temperature parameter ? = 0.1 for MoCo, SimCLR and SwAV. The projection MLP output dimensions are d MoCo = d SimCLR = ? SwAV = 128, and d BYOL = 256, as in their original publications <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>MoCo details. We use a queue storing 65536 negatives and shuffling BN to avoid intra-batch communication among samples <ref type="bibr" target="#b35">[36]</ref>. We use a 3-layer (2 hidden layers, ablation in <ref type="table" target="#tab_8">Table 6</ref> of the main paper) projection MLP with hidden dimension 2048, ReLU activation <ref type="bibr" target="#b62">[63]</ref> and no BN. Other hyperparameters are as in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b13">14]</ref>. The momentum encoder weights ? m are updated with an annealed momentum m = 1 ? (1 ? m base ) ? (cos(?k/K) + 1)/2 with k the current iteration and K the maximum number of training iterations <ref type="bibr" target="#b31">[32]</ref>, starting with m base = 0.994. The corresponding ablation is in <ref type="table" target="#tab_4">Table 3</ref> of the main paper.</p><p>BYOL details. Our BYOL implementation uses a momentum annealing starting from m base = 0.996. We minimize the negative cosine similarity in equation <ref type="formula">(2)</ref> of the main paper multiplied by 2 which is equivalent to BYOL's MSE of 2 -normalized vectors <ref type="bibr" target="#b31">[32]</ref>. The projection and prediction MLPs have 2 layers (one hidden layer with dimension 4096) and use BN following the original publication <ref type="bibr" target="#b31">[32]</ref>.</p><p>SimCLR details. We follow the default implementation <ref type="bibr" target="#b11">[12]</ref>. We use a 3-layer projection MLP with a hidden dimension of 2048, ReLU and BN. The loss in equation <ref type="formula" target="#formula_0">(1)</ref> of the main paper is computed synchronized over the full batch size.</p><p>SwAV details. We follow the default implementation <ref type="bibr" target="#b8">[9]</ref>, using 3 Sinkhorn-Knopp iterations <ref type="bibr" target="#b14">[15]</ref> and freezing the prototypes for the first epoch. The Sinkhorn regularization parameter is set to 0.05. As in the default implementation <ref type="bibr" target="#b8">[9]</ref>, the matrix normalization statistics of the Sinkhorn-Knopp algorithm are computed synchronized over the full training batch. The projection MLP uses ReLU and BN and is identical to the one used in <ref type="bibr" target="#b8">[9]</ref>, only that we use a 3-layer MLP instead of 2 (ablations are in <ref type="table" target="#tab_8">Table 6</ref> of the main paper).</p><p>Encoder details. Our default encoder, f ? , is a R-50 Slow model <ref type="bibr" target="#b19">[20]</ref>, i.e. a ResNet-50 <ref type="bibr" target="#b38">[39]</ref> with a temporal dimension of size T and sample rate ? . We perform all ablations with default T ?? of 8?8. We show the architecture in <ref type="table" target="#tab_1">Table 17</ref>.</p><p>Augmentation details. We perform video decoding and data augmentation using PyTorch's torchvision package.</p><p>We obtain different clips from a video by the following procedure. For the temporal dimension, we randomly sample a clip (of T ?? frames) from the full-length video, and the input to the ResNet encoder are T frames subsampled from the raw clip with a stride of ? ; for the spatial dimension, we randomly crop 224?224 pixels from a video, or its horizontal flip, with a shorter side randomly sampled in [256, 320] pixels <ref type="bibr" target="#b19">[20]</ref> (VGG-style <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b38">39]</ref> spatial cropping, a comparison to Inception-style <ref type="bibr" target="#b79">[80]</ref> cropping, which we use for results in ?4.5, is given in <ref type="table" target="#tab_11">Table 9</ref> of the main paper).</p><p>To each clip, we apply a random horizontal flip, color distortion and Gaussian blur following the SimCLR and MoCo v2 implementation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>. For color augmentation we use the ColorJitter (probability 0.8) and RandomGrayscale (probability 0.2) method from torchvision.transforms module of PyTorch with the color strength parameter s: {brightness, contrast, saturation, hue} = {0.4s, 0.4s, 0.4s, 0.1s} By default s=0.5. Ablations are given in <ref type="table" target="#tab_10">Table 8</ref> of the main paper. For Gaussian blur we use a spatial kernel with standard-deviation ? [0.1, 2.0] applied with probability of 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Details: Kinetics Action Classification</head><p>Datasets. Kinetics-400 <ref type="bibr" target="#b46">[47]</ref> consists of ?240k training videos and 20k validation videos in 400 human action categories. Kinetics-600 <ref type="bibr" target="#b9">[10]</ref> has ?392k training videos and 30k validation videos in 600 classes. Kinetics-700 <ref type="bibr" target="#b10">[11]</ref> has ?523k training videos and 35k validation videos in 600 classes.</p><p>Linear classification protocol. We validate the methods by linear classification on frozen features, following the common protocol in image classification <ref type="bibr" target="#b35">[36]</ref>. After unsupervised pre-training on Kinetics, we freeze the features of the encoder and train a linear classifier on top of the last layer features (e.g. pool <ref type="bibr" target="#b4">5</ref> in <ref type="table" target="#tab_1">Table 17</ref>). For all ablations in the paper the classifier is trained for 60 epochs (using 100 epochs will increase accuracy by ?0.2%) using the same  Training augmentation. We use the default training augmentation <ref type="bibr" target="#b19">[20]</ref>. We randomly sample a clip (of T ?? frames) from the full-length video and randomly crop 224?224 pixels from a video, or its horizontal flip, with a shorter side randomly sampled in [256, 320] pixels.</p><p>Inference. Following common practice, in video classification <ref type="bibr" target="#b19">[20]</ref>, we report 30-view, top-1 classification accuracy on the Kinetics validation set. We uniformly sample 10 clips from a video along its temporal axis. For each clip, we scale the shorter spatial side to 256 pixels and take 3 crops of 256?256 to cover the spatial dimensions. We average the softmax scores for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Details: AVA Action Detection</head><p>Dataset. The AVA dataset <ref type="bibr" target="#b32">[33]</ref> has bounding box annotations for spatiotemporal localization of (possibly multiple) human actions. It has 211k training and 57k validation video segments. We follow the standard protocol reporting mean Average Precision (mAP) on 60 classes [33] on AVA v2.2.</p><p>Detection architecture. We exactly follow the detection architecture in <ref type="bibr" target="#b19">[20]</ref> to allow direct comparison of the pretrained models used as a backbone for the AVA task <ref type="bibr" target="#b32">[33]</ref>. The detector is similar to Faster R-CNN <ref type="bibr" target="#b71">[72]</ref> with minimal modifications adapted for video. Region-of-interest (RoI) features <ref type="bibr" target="#b25">[26]</ref> are extracted at the last feature map of res 5 (cf <ref type="table" target="#tab_1">. Table 17</ref>) by extending a 2D proposal at a frame into a 3D RoI by replicating it along the temporal axis, followed by application of frame-wise RoIAlign <ref type="bibr" target="#b36">[37]</ref> and temporal global average pooling. We set the spatial stride of res 5 to 1 (instead of 2), and use a dilation of 2 for its filters <ref type="bibr" target="#b19">[20]</ref>. This increases the spatial resolution of res 5 by 2?. The RoI features are then max-pooled and fed to a per-class sigmoid classifier for prediction.</p><p>Training. For direct comparison, the training procedure and hyper-parameters for AVA follow <ref type="bibr" target="#b19">[20]</ref> without modification. The network weights are initialized from the Kinetics models and we use step-wise learning rate decay, that is reduced by 10? after 16, 24 and 28 epochs. We train for 32 epochs on ?211k data, with linear warm-up <ref type="bibr" target="#b29">[30]</ref> for the first 5 epochs and use a weight decay of 10 ?7 , as in <ref type="bibr" target="#b19">[20]</ref>. For 8 GPU training, we use a batch-size of 64, a learning rate of 0.05 for the supervised pre-trained Kinetics models and 0.3 for the unsupervised ones, as this gives the best result for each of them.</p><p>The region proposal extraction also follows <ref type="bibr" target="#b19">[20]</ref> and is summarized here for completeness. Our region proposals are computed by an off-the-shelf person detector, i.e., that is not jointly trained with the action detection models. We adopt a person-detection model trained with Detectron <ref type="bibr" target="#b24">[25]</ref>. It is a Faster R-CNN with a ResNeXt-101-FPN backbone. It is pre-trained on ImageNet and the COCO human keypoint images <ref type="bibr" target="#b54">[55]</ref>. We fine-tune this detector on AVA for person (actor) detection. The person detector produces 93.9 AP@50 on the AVA validation set. Then, the region proposals for action detection are detected person boxes with a confidence of &gt; 0.8, which has a recall of 91.1% and a precision of 90.7% for the person class.</p><p>Inference. We perform inference on a single clip with 8 frames sampled with stride 8 centered at the frame that is to be evaluated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Details: Charades Action Classification</head><p>Dataset. Charades <ref type="bibr" target="#b74">[75]</ref> has ?9.8k training videos and 1.8k validation videos in 157 classes in a multi-label classification setting of longer activities spanning ?30 seconds on average. Performance is measured in mean Average Precision (mAP).</p><p>Training. For Charades, we fine-tune the Kinetics models, but extend their duration by 2? (T ?? = 16?8) to account for the long-term nature of the dataset. This increase accuracy of all models by ?3 mAP. Our training augmentation is the same as as in ?B.2. A per-class sigmoid output is used for mutli-class prediction. We train for 60 epochs using a batch size of 64 and a base learning rate of 0.2 (for 8 GPUs) with 10? step-wise decay at epoch 40 and 50, after warm-up in the first 5 epochs. We use weight decay of 10 -4 and dropout of 0.5. Other training details are analogous to Kinetics.</p><p>Inference. This is as for Kinetics ( ?B.2), but to infer the actions over a single video, we spatiotemporally max-pool prediction scores in testing <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5. Details: Something-Something V2 (SSv2)</head><p>Dataset. The Something-Something V2 dataset <ref type="bibr" target="#b30">[31]</ref> contains 169k training, and 25k validation videos. The videos show human-object interactions to be classified into 174 classes. We report top-1 accuracy on the validation set.</p><p>Training. We fine-tune the pre-trained Kinetics models. We train for 22 epochs using a batch size of 64 and a base learning rate of 0.12 (for 8 GPUs) with 10? step-wise decay at epoch 14 and 18. Weight decay is set to 10 ?6 and dropout 0.5. Our training augmentation is the same as in ?B.2, but as Something-Something V2 requires distinguishing between directions, we disable random flipping during training. We use segment-based input frame sampling <ref type="bibr" target="#b53">[54]</ref> that splits each video into segments, and from each of them, we sample one frame to form a clip.</p><p>Inference. We perform single center clip testing to form predictions over a single video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6. Details: UCF-101 Action Classification</head><p>Dataset. UCF101 <ref type="bibr" target="#b76">[77]</ref> has 13320 human action videos in 101 categories. Our ablations are performed on the first train/val split, and for the comparison to prior work we report the mean average accuracy over the three splits.</p><p>Training. We fine-tune the pre-trained Kinetics models and use the same augmentation as for Kinetics. We train for 200 epochs using a batch size of 64 and a base learning rate of 0.025 (for 8 GPUs) with 10? step-wise decay at epoch 60, 120 and 180. Weight decay is set to 0 and dropout to 0.8.</p><p>Inference. We use the same procedure as in Kinetics ( ?B.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7. Details: HMDB-51 Action Classification</head><p>Dataset. HMDB51 <ref type="bibr" target="#b49">[50]</ref> contains 6766 videos that have been annotated for 51 actions. Our evaluation follows the protocol for UCF101.</p><p>Training and Inference. Our settings are identical to the ones used for UCF101 and we expect further tuning of hyperparameters to increase its downstream performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Conceptual comparison of four unsupervised learning mechanisms applied to video. The inputs consist of ?=2 clips from B videos. Each clip is a stack of T frames with temporal stride ? and spatial resolution S 2 . Each method trains encoder weights ? by computing a positive loss component w.r.t. to the other clips of the same video. SimCLR (a) and MoCo (b) use a contrastive loss with negatives coming from different videos in the batch or a a queue. respectively. MoCo (b) and BYOL (c) use extra momentum encoders with weights ?m being moving averages of the trained ?. SwAV (d) uses a Sinkhorn-Knop (SK) transform to generate the positive targets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Ablating augmentations. We explore temporal (T), spatial (S), and color (C) augmentations to learn persistent features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Data scale and curation. We increase dataset size (number of videos) for IG-Curated, IG-Curated-Unbalanced, and IG-Uncurated. By using 4? the number of videos, IG-Uncurated approaches the heavily curated Kinetics (K400) pre-training on K400 linear evaluation protocol. The dotted line represents a linear trend. Method: MoCo, 200 epochs, ?=2. IG-Uncurated (which are random IG videos). The experiment with 200-epoch MoCo with ?=2, linear protocol downstream evaluation on K400 is shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Pre-training data statistics with timings in seconds.</figDesc><table><row><cell></cell><cell>47] 240K</cell><cell>10.0 9.3 1.7 1.0 10.0</cell></row><row><cell>IG-Curated [24]</cell><cell>1M</cell><cell>18.9 26.3 19.8 1.5 60.0</cell></row><row><cell>IG-Uncurated</cell><cell>1M</cell><cell>29.4 35.3 38.4 0.5 600.0</cell></row><row><cell>IG-Uncurated-Short</cell><cell>1M</cell><cell>13.0 13.1 1.6 10.0 15.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Number of temporal clips ?. Data: K400, 200 epochs. Learning temporally persistent features (? ? 2) is effective.</figDesc><table><row><cell cols="2">MoCo</cell><cell cols="2">BYOL</cell><cell cols="2">SimCLR</cell><cell cols="2">SwAV</cell></row><row><cell cols="8">ep K400 UCF101 K400 UCF101 K400 UCF101 K400 UCF101</cell></row><row><cell>50 52.6</cell><cell>84.6</cell><cell>30.2</cell><cell>78.5</cell><cell>45.7</cell><cell>79.7</cell><cell>55.9</cell><cell>81.4</cell></row><row><cell>100 60.5</cell><cell>89.5</cell><cell>47.6</cell><cell>88.6</cell><cell>57.3</cell><cell>85.6</cell><cell>59.4</cell><cell>85.5</cell></row><row><cell>200 65.8</cell><cell>91.0</cell><cell>65.8</cell><cell>92.7</cell><cell>60.5</cell><cell>88.9</cell><cell>61.6</cell><cell>87.3</cell></row><row><cell>400 67.4</cell><cell>92.5</cell><cell>66.9</cell><cell>92.8</cell><cell>62.0</cell><cell>87.9</cell><cell>62.9</cell><cell>88.3</cell></row><row><cell>800 67.4</cell><cell>93.2</cell><cell>66.2</cell><cell>93.6</cell><cell>61.8</cell><cell>88.4</cell><cell>63.2</cell><cell>89.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Training duration in epochs (ep): Dataset: K400, ?=2.</figDesc><table><row><cell>Training longer brings consistent gains for all methods up to 400</cell></row><row><cell>epochs and saturates for K400 but not for UCF101 at 800ep. SwAV</cell></row><row><cell>is the strongest performer for short training (50ep).</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>IG-Curated-1M, 50 epochs training.</figDesc><table><row><cell>tmax in seconds</cell><cell>0</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>8</cell><cell>10</cell></row><row><cell cols="8">K400 acc in % 60.6 65.2 65.7 65.8 65.8 65.6 65.8</cell></row><row><cell cols="6">(a) Dataset: K400, 200 epochs training.</cell><cell></cell><cell></cell></row><row><cell cols="2">tmax in seconds</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>60</cell><cell></cell></row><row><cell cols="2">K400 acc in %</cell><cell cols="5">62.7 63.1 63.1 63.9 64.1</cell><cell></cell></row><row><cell cols="2">(b) Dataset: tmax in seconds</cell><cell>12s</cell><cell>24</cell><cell>36</cell><cell>48</cell><cell>600</cell><cell></cell></row><row><cell cols="2">K400 acc in %</cell><cell cols="5">59.3 59.2 59.9 59.6 58.9</cell><cell></cell></row></table><note>(c) Dataset: IG-Uncurated-1M, 50 epochs training.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>as backbone. The next set of ablations studies different architectures for the spatiotemporal encoder.</figDesc><table><row><cell></cell><cell></cell><cell>training</cell><cell>sup.</cell><cell>MoCo (?=2)</cell></row><row><cell cols="5">backbone T ? ? FLOPs Param s/iter K400 K400 UCF101</cell></row><row><cell>R-50</cell><cell>8?8</cell><cell cols="3">41.7G 31.8M 1.6s 74.7 65.8</cell><cell>91.0</cell></row><row><cell>R-18</cell><cell>8?8</cell><cell cols="3">20.0G 20.2M 1.2s 68.9 56.2</cell><cell>87.1</cell></row><row><cell>R-101</cell><cell>8?8</cell><cell cols="3">93.3G 51.4M 2.1s 75.8 67.7</cell><cell>92.4</cell></row><row><cell>R-50</cell><cell cols="4">16?4 83.5G 31.8M 2.5s 76.1 67.6</cell><cell>93.3</cell></row><row><cell>R-50</cell><cell cols="4">32?2 167.0G 31.8M 4.6s 76.3 67.8</cell><cell>94.2</cell></row><row><cell cols="5">R2+1D-18 32?2 48.5G 15.4M 4.0s 71.7 57.2</cell><cell>93.7</cell></row><row><cell>S3D-G</cell><cell cols="4">32?2 36.0G 9.1M 4.1s 74.7 63.2</cell><cell>94.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc>Training on curated (a), uncurated (b) and short duration video (c) data from the web. Longer training degrades performance for BYOL, possibly due to suboptimal hyper-parameters. ?=2.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell>color</cell><cell>grayscale</cell><cell>temporal</cell><cell>fps</cell><cell cols="2">accuracy</cell></row><row><cell>strength</cell><cell>probability</cell><cell>difference</cell><cell>jitter</cell><cell>K400</cell><cell>UCF101</cell></row><row><cell>0.5</cell><cell>0.2</cell><cell></cell><cell></cell><cell>65.8</cell><cell>91.0</cell></row><row><cell>0.75</cell><cell>0.2</cell><cell></cell><cell></cell><cell>66.0</cell><cell>92.1</cell></row><row><cell>1.0</cell><cell>0.2</cell><cell></cell><cell></cell><cell>65.8</cell><cell>91.2</cell></row><row><cell>0.5</cell><cell>0.4</cell><cell></cell><cell></cell><cell>65.5</cell><cell>91.0</cell></row><row><cell>0.5</cell><cell>0.2</cell><cell></cell><cell></cell><cell>66.2</cell><cell>91.3</cell></row><row><cell>0.5</cell><cell>0.2</cell><cell></cell><cell></cell><cell>65.6</cell><cell>91.5</cell></row></table><note>. Radiometric augmentation. Method: MoCo, 200 epochs, ? = 2. Dataset: K400. Stronger color augmentation in K400 pre-training can especially benefit UCF101 (+1.3%).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>Cropping augmentation. Method: MoCo, 200 epochs, ? = 2. Dataset: K400. Stronger cropping and aspect ratio augmentation can be beneficial by +1.0% (K400) and 0.7% UCF101.</figDesc><table><row><cell>area</cell><cell></cell><cell>aspect</cell><cell cols="2">accuracy</cell></row><row><cell>[ min,</cell><cell>max]</cell><cell>ratio</cell><cell>K400</cell><cell>UCF101</cell></row><row><cell cols="2">default [76, 39, 20]</cell><cell></cell><cell>65.8</cell><cell>91.0</cell></row><row><cell>[0.49,</cell><cell>0.76]</cell><cell></cell><cell>64.8</cell><cell>91.7</cell></row><row><cell>[0.49,</cell><cell>0.76]</cell><cell></cell><cell>65.4</cell><cell>91.7</cell></row><row><cell>[0.20,</cell><cell>0.76]</cell><cell></cell><cell>66.8</cell><cell>91.8</cell></row><row><cell>[0.20,</cell><cell>0.50]</cell><cell></cell><cell>66.3</cell><cell>91.8</cell></row><row><cell>[0.20,</cell><cell>1.00]</cell><cell></cell><cell>66.6</cell><cell>91.7</cell></row><row><cell>[0.08,</cell><cell>0.50]</cell><cell></cell><cell>64.3</cell><cell>91.6</cell></row><row><cell>[0.08,</cell><cell>1.00]</cell><cell></cell><cell>65.3</cell><cell>91.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 .</head><label>9</label><figDesc>Stronger augmentations. Data: K400, 200 epochs. "aug+'' combines the best color and cropping augmentations fromTable 7andTable 8, respectively.Combined augmentations. We pull together the best color and cropping augmentations inTables 7 &amp; 8, and train MoCo and BYOL with ?=4 for 200ep on K400. The result shown as "aug+" inTable 9can increase performance on K400</figDesc><table><row><cell cols="2">MoCo (?=4)</cell><cell cols="2">BYOL (?=4)</cell></row><row><cell cols="4">aug+ K400 UCF101 K400 UCF101</cell></row><row><cell>67.8</cell><cell>93.5</cell><cell>68.9</cell><cell>93.8</cell></row><row><cell>69.0</cell><cell>93.6</cell><cell>69.8</cell><cell>93.9</cell></row></table><note>by ?1%. Training the linear classifier of BYOL (?=4) for 100ep instead of 60ep leads to our best accuracy of 70.0% on K400, which is 4.7% below the supervised R-50, Slow 8?8 accuracy of 74.7%.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 .</head><label>10</label><figDesc>Downstream benchmarks: We use linear evaluation on K400 and finetuning accuracy on the other datasets. 200 epochs. ?=3. Something-Something v2 (SSv2 in Table 10), all the methods perform strong, with BYOL pre-training showing the largest gain of +3% over supervised pre-training on Kinetics (55.8% vs. 52.8% top-1 accuracy).</figDesc><table><row><cell></cell><cell></cell><cell>linear protocol</cell><cell></cell><cell cols="2">finetuning accuracy</cell><cell></cell></row><row><cell>method</cell><cell>pre-train</cell><cell>K400</cell><cell>UCF101</cell><cell>AVA (mAP)</cell><cell>Charades (mAP)</cell><cell>SSv2</cell></row><row><cell>supervised</cell><cell>scratch</cell><cell>74.7</cell><cell>68.8</cell><cell>11.7</cell><cell>7.4</cell><cell>48.8</cell></row><row><cell>supervised</cell><cell>K400-240K</cell><cell>-</cell><cell>94.8</cell><cell>22.2</cell><cell>34.7</cell><cell>52.8</cell></row><row><cell>SimCLR</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>On</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12</head><label>12</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">compares training</cell></row><row><cell cols="4">BYOL longer (200ep) to increasing its clips-size ? but not</cell></row><row><cell cols="4">training longer (50ep). For both (a) curated and (b) random</cell></row><row><cell cols="4">data, this results in a significant gain of performance.</cell></row><row><cell cols="2">BYOL</cell><cell cols="2">BYOL</cell></row><row><cell cols="2">? ep K400 UCF101</cell><cell cols="2">? ep K400 UCF101</cell></row><row><cell>2 50 64.1</cell><cell>93.5</cell><cell>2 50 58.9</cell><cell>90.1</cell></row><row><cell>2 200 60.2</cell><cell>92.7</cell><cell>2 200 57.9</cell><cell>91.6</cell></row><row><cell>4 50 67.7</cell><cell>94.5</cell><cell>4 50 63.8</cell><cell>91.8</cell></row><row><cell cols="2">(a) IG-Curated-1M.</cell><cell cols="2">(b) IG-Uncurated-1M.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 .</head><label>12</label><figDesc>More epochs (ep) vs. more clips (?), Longer training degrades performance for BYOL, but increasing ? does not.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 13 .</head><label>13</label><figDesc>More epochs (ep) vs. more clips (?): Dataset: IG-Curated-1M, ?=2. Training longer is less effective than increasing the number of temporal clips per iteration (?).</figDesc><table><row><cell>g.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table 14 .</head><label>14</label><figDesc>Dataset scale: Configuration: backbone: R-50, Slow 8 ? 8, 200 epochs. Our approach, MoCo</figDesc><table><row><cell></cell><cell cols="3">pre-train</cell><cell></cell><cell></cell><cell></cell><cell>finetune</cell></row><row><cell>method</cell><cell>data</cell><cell cols="2">#videos</cell><cell>K400</cell><cell>K600</cell><cell>K700</cell><cell>UCF101</cell></row><row><cell>supervised</cell><cell>K400</cell><cell>scratch</cell><cell>240k</cell><cell>74.7</cell><cell>78.1 linear protocol</cell><cell>65.2</cell><cell>68.8 94.8</cell></row><row><cell>MoCo (?=4)</cell><cell>K400</cell><cell></cell><cell>240k</cell><cell>69.0</cell><cell>70.0</cell><cell>54.2</cell><cell>93.6</cell></row><row><cell>MoCo (?=2) MoCo (?=4)</cell><cell>K600</cell><cell></cell><cell>387k</cell><cell>69.6 71.5</cell><cell>70.7 72.8</cell><cell>55.1 57.7</cell><cell>92.7 94.5</cell></row><row><cell>MoCo (?=2) MoCo (?=4)</cell><cell>K700</cell><cell></cell><cell>522k</cell><cell>70.0 71.7</cell><cell>71.4 73.2</cell><cell>56.2 58.1</cell><cell>92.8 94.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table 15 .</head><label>15</label><figDesc>Momentum annealing for MoCo. Dataset: K400, 200 epochs, ?= 2. Using cosine-annealing of the momentum brings gains of ?1% accuracy. We use 0.994 as default for MoCo.</figDesc><table><row><cell>m base</cell><cell>N/A</cell><cell>0.988</cell><cell>0.990</cell><cell>0.992</cell><cell>0.994</cell><cell>0.996</cell></row><row><cell>acc.</cell><cell>64.5</cell><cell>65.5</cell><cell>65.5</cell><cell>65.6</cell><cell>65.8</cell><cell>65.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head>Table 17 .</head><label>17</label><figDesc>R-50, Slow pathway<ref type="bibr" target="#b19">[20]</ref>. The dimensions of kernels are denoted by {T ?S 2 , C} for temporal, spatial, and channel sizes. Strides are denoted as {temporal stride, spatial stride 2 }. Non-degenerate temporal filters are underlined. Residual blocks are in brackets. Temporal pooling is only performed at the last layer, collapsing spacetime dimensions. By default T ?? = 8?8. cosine schedule as for pre-training (Sec. B.1) with a base learning rate of ? = 4.0 (10?higher than in pre-training), linear warm-up in the first 8 epochs, and weight decay of 0.</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to see by moving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pulkit</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Selfsupervised multimodal versatile networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adri?</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalia</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS, 2020</title>
		<meeting>NeurIPS, 2020</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Self-supervised learning by cross-modal audio-video clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS, 2020</title>
		<meeting>NeurIPS, 2020</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Look, listen and learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Objects that sound</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning temporally persistent hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suzanna</forename><surname>Becker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SpeedNet: Learning the Speediness in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andras</forename><surname>Banki-Horvath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.01340</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">A short note about kinetics-600. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Noland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06987</idno>
		<title level="m">A short note on the kinetics-700 human action dataset</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sinkhorn distances: Lightspeed computation of optimal transport</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DynamoNet: Dynamic Action and Motion Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rainer</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with exemplar convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1734" to="1747" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Haoqi Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wan-Yen</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pyslowfast</surname></persName>
		</author>
		<ptr target="https://github.com/facebookresearch/slowfast,2020.5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">SlowFast Networks for Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Self-supervised video representation learning with odd-one-out networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hakan</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Geometry guided convolutional neural networks for self-supervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jort F Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aren</forename><surname>Freedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wade</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channing</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manoj</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Large-scale weaklysupervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Feiszli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueting</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Piotr Doll?r, and Kaiming He</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<idno>2018. 13</idno>
		<ptr target="https://github.com/facebookresearch/detectron" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep sparse rectifier neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Watching the world go by: Representation learning from unlabeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiana</forename><surname>Ehsani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dieter</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07990</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised learning of spatiotemporally coherent metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Tulloch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Yangqing Jia, and Kaiming He. Accurate, large minibatch SGD: training ImageNet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Something Something&quot; video database for learning and evaluating visual common sense</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghav</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Michalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanne</forename><surname>Westphal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heuna</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Haenel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Fruend</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yianilos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Mueller-Freitag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">AVA: A video dataset of spatiotemporally localized atomic visual actions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhui</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Pantofaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yeqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susanna</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Video representation learning by dense predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Large Scale Holistic Video Understanding, ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Selfsupervised co-training for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS, 2020</title>
		<meeting>NeurIPS, 2020</meeting>
		<imprint>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Ali Eslami, and A?ron van den Oord. Data-efficient image recognition with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Doersch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.09272</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning visual groups from co-occurrences in space and time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning image representations tied to ego-motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinesh</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristen</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Video representation learning by recognizing temporal transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Givi</forename><surname>Meishvili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.10730</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Invariant information clustering for unsupervised image classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9865" to="9874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chloe</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudheendra</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.06950</idno>
		<title level="m">The kinetics human action video dataset</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Selfsupervised video representation learning with space-time cubic puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyeon</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">In</forename><forename type="middle">So</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Cooperative learning of audio and video models from self-supervised synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning by sorting sequence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsin-Ying</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maneesh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meghana</forename><surname>Thotakuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Vostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00214</idno>
		<title level="m">The avakinetics localized human actions video dataset</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Learning spatiotemporal features via video and text pair discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.05691</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Temporal shift module for efficient video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">SGDR: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Shuffle and learn: Unsupervised learning using temporal order verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Deep learning from temporal coherence in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Audiovisual instance discrimination with cross-modal agreement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Morgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.12943</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="69" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Visually indicated sounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Owens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><forename type="middle">H</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><forename type="middle">H</forename><surname>Adelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2405" to="2413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning features by watching objects move</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepak</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Multi-modal self-supervision from generalized data transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><forename type="middle">M</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruth</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04298</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Evolving losses for unsupervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Aj Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, 2020</title>
		<meeting>CVPR, 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Demystifying contrastive self-supervised learning: Invariances, augmentations and dataset biases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13916</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.03800</idno>
		<title level="m">Spatiotemporal contrastive video representation learning</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NeurIPS</title>
		<meeting>NeurIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilal</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Piot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.10241</idno>
		<title level="m">Byol works even without batch statistics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Time-contrastive networks: Selfsupervised learning from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Intl. Conf. on Robotics and Automation</title>
		<meeting>Intl. Conf. on Robotics and Automation</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Hollywood in homes: Crowdsourcing data collection for activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Gunnar A Sigurdsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Contrastive bidirectional transformer for temporal representation learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Contrastive multiview coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV, 2020</title>
		<meeting>ECCV, 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Anticipating visual representations from unlabelled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Transitive invariance for self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Learning correspondence from the cycle-consistency of time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Slow feature analysis: Unsupervised learning of invariances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurenz</forename><surname>Wiskott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terrence</forename><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computation</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance-level discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR, volume abs/1805.01978</title>
		<meeting>CVPR, volume abs/1805.01978</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Rethinking spatiotemporal feature learning for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Self-supervised spatiotemporal learning via video clip order prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dejing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Video representation learning with visual tempo consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.15489</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Large batch training of convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Gitman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Ginsburg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03888</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Local aggregation for unsupervised learning of visual embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxu</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">Lin</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Yamins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
