<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rethinking Class Relations: Absolute-relative Supervised and Unsupervised Few-shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Systems Engineering Institute</orgName>
								<address>
									<country>AMS</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Data61/CSIRO</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songlei</forename><surname>Jian</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">National University of Defense Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Oxford</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rethinking Class Relations: Absolute-relative Supervised and Unsupervised Few-shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T12:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The majority of existing few-shot learning methods describe image relations with binary labels. However, such binary relations are insufficient to teach the network complicated real-world relations, due to the lack of decision smoothness. Furthermore, current few-shot learning models capture only the similarity via relation labels, but they are not exposed to class concepts associated with objects, which is likely detrimental to the classification performance due to underutilization of the available class labels. For instance, children learn the concept of tiger from a few of actual examples as well as from comparisons of tiger to other animals. Thus, we hypothesize that both similarity and class concept learning must be occurring simultaneously. With these observations at hand, we study the fundamental problem of simplistic class modeling in current few-shot learning methods. We rethink the relations between class concepts, and propose a novel Absolute-relative Learning paradigm to fully take advantage of label information to refine the image an relation representations in both supervised and unsupervised scenarios. Our proposed paradigm improves the performance of several state-of-the-art models on publicly available datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning, a popular learning paradigm in computer vision, has improved the performance on numerous computer vision tasks, such as category recognition, scene understanding and action recognition. However, deep models heavily rely on large amounts of labeled training data, costly data collection and labelling.</p><p>In contrast, humans enjoy the ability to learn and memorize new complex visual concepts from very few examples. Inspired by this observation, researchers have focused on the so-called Few-shot Learning (FSL), for which a network is * This work is accepted to CVPR'21.  <ref type="figure">Figure 1</ref>: Our few-shot learning paradigm. Absolute Learning (AL) refers to the strategy where a pipeline learns to predict absolute object information e.g., object or concept class. Relative Learning (RL) denotes similarity (relation) learning with the use of binary {0, 1} and/or soft [0; 1] similarity labels. Absolute-relative Learning (ArL) is a combination of AL and RL, which is akin to multi-task learning, and unary and pair-wise potentials in semantic segmentation. ArL is also conceptually closer to how humans learn from few examples. trained by the use of only few labeled training instances. Recently, deep networks based on relation-learning have gained the popularity <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36]</ref>. Such approaches often apply a form of metric learning adapted to the few-shot learning task. They learn object relations (similarity learning on query and support images) based on support classes, and can be evaluated on images containing novel classes.</p><p>However, there are two major problems in these relation learning pipelines, namely, (i) binary {0, 1} labels are used to express the similarity between pairs of images, which cannot capture the similarity nuisances in the real-world setting due to the hardness of such modeling, which leads to biases in the relation-based models, (ii) only pair-wise relation labels are used in these pipelines, so the models have no knowledge of the actual class concepts. In other words, these models are trained to learn the similarity between image pairs while they discard the explicit object classes despite they are accessible in the training stage.</p><p>We conjuncture that these two problems pose inconsistency between current few-shot learning approaches and human's cognitive processes. To this end, we propose the Absolute-relative Learning (ArL) which exposes few-shot learners to both similarity and class labels, and we employ semantic annotations to circumvent the issue with the somewhat rigid binary similarity labels {0, 1}.</p><p>Our ArL consists of two separate learning modules, namely, Absolute Learning (AL) and Relative Learning (RL). AL denotes the strategy in which we learn to predict the actual object categories or class concepts in addition to learning the class relations. In this way, the feature extracting network is exposed to additional object-or concept-related knowledge. RL refers to the similarity learning strategy for which (apart of binary {0, 1} labels) we employ semantic annotations to promote the realistic similarity between image pairs. We use attributes or word2vec to obtain the semantic relation labels and learn element-wise similarities e.g., if two objects have same colour, texture, etc. Such labels are further used as the supervisory cue in relation learning to capture the realistic soft relations between objects beyond the binary similarity.</p><p>By combing AL and RL which constitute on ArL, the relation network is simultaneously taught the class/object concepts together with more realistic class/object relations, thus naturally yielding an improved accuracy. Moreover, we use the predictions from the absolute and relative learners as interpretable features to promote the original relation learning via feedback connections.</p><p>Our approach is somewhat related to multi-modal learning which leverages multiple sources of data for training and testing. However, while multi-modal learning combines multiple streams of data on network inputs, our ArL models the semantic annotations in the label space, that is, we use them as the network output. We believe that using multiple abstractions of labels (relative vs. absolute) encourages the network to preserve more information about objects relevant to the few-shot learning task. Our strategy benefits from multi-task learning where two tasks learnt simultaneously help each other to outperform a naive fusion of two separate tasks. These tasks somewhat resemble unary and pair-wise potentials in semantic segmentation.</p><p>We note that obtaining the semantic information for novel classes (the testing step in few-shot learning) is not always easy or possible. Since our pipeline design is akin to multitask rather than multi-modal learning, our model does not require additional labeling at the testing stage. Therefore, it is a more realistic setting than that of existing approaches.</p><p>In addition to the classic supervised few-shot recognition, we extend our ArL to the unsupervised scenario. Different with approach <ref type="bibr" target="#b11">[12]</ref> that merely applies the self-supervised discriminator as an auxiliary task to improve the performance of supervised FSL, we develop an effective unsupervised FSL based on ArL. As there is no annotations for training samples, we rely on augmentation labelling (e.g., rotations, flips and colors) to perform Absolute-relative Learning. Below, we summarize our contributions:</p><p>i. We propose so-called Absolute-relative Learning which can be embedded into popular few-shot pipelines to exploit both similarity and object/concept labelling.</p><p>ii. We extend our approach to unsupervised FSL, and we show how to create self-supervised annotations for unsupervised Absolute-relative Learning.</p><p>iii. We investigate the influence of different types of similarity measures on attributes in Relative Learning to simulate realistic object relations.</p><p>iv. We investigate the influence of different Absolute Learning branches on the classification performance.</p><p>To the best of our knowledge, we are the first to perform an in-depth analysis of object and class relation modeling in the context of supervised and unsupervised few-shot learning given the Absolute-relative Learning paradigm via class, semantic and augmentation annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Below, we describe recent one-and few-shot learning algorithms followed by semantic-based approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Learning From Few Samples</head><p>For deep learning algorithms, the ability of 'learning from only a few examples is the desired characteristic to emulate in any brain-like system' <ref type="bibr" target="#b32">[33]</ref> is a desired operating principle which poses a challenge to typical CNNs designed for the large scale visual category recognition <ref type="bibr" target="#b33">[34]</ref>. One-and Few-shot Learning has been studied widely in computer vision in both shallow <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23]</ref> and deep learning scenarios <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b45">46]</ref>.</p><p>Early works <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref> propose one-shot learning methods motivated by the observation that humans can learn new concepts from very few examples. Siamese Network <ref type="bibr" target="#b18">[19]</ref> presents a two-streams convolutional neural network approach which generates image descriptors and learns the similarity between them. Matching Network <ref type="bibr" target="#b38">[39]</ref> introduces the concept of support set and L-way Z-shot learning protocols. It captures the similarity between one testing and several support images, thus casting the one-shot learning problem as set-to-set learning. Prototypical Networks <ref type="bibr" target="#b36">[37]</ref> learns a model that computes distances between a datapoint and prototype representations of each class. Model-Agnostic Meta-Learning (MAML) <ref type="bibr" target="#b9">[10]</ref> introduces a meta-learning model trained on a variety of different learning tasks. Relation Net <ref type="bibr" target="#b37">[38]</ref> is an efficient end-to-end network for learning the relationship between testing and support images. Conceptually, this model is similar to Matching Network <ref type="bibr" target="#b38">[39]</ref>. However, Relation Net leverages an additional deep neural network to learn similarity on top of the image descriptor generating network. Second-order Similarity Network (SoSN) <ref type="bibr" target="#b45">[46]</ref> is similar to Relation Net <ref type="bibr" target="#b37">[38]</ref>, which consists of the feature encoder and relation network. However, approach <ref type="bibr" target="#b37">[38]</ref> uses first-order representations for similarity learning. In contrast, SoSN investigates second-order representations to capture co-occurrences of features. Graph Neural Networks (GNN) have also been applied to few-shot learning in many recent works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b12">13]</ref> achieving promising results. Finally, noteworthy are domain adaptation and related approaches which can also operate in the small sample regime <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Learning from Semantic Labels</head><p>Semantic labels are used in various computer vision tasks e.g., object classification, face and emotion recognition, image retrieval, transfer learning, and especially in zero-shot learning. Metric learning often uses semantic information e.g., approach <ref type="bibr" target="#b4">[5]</ref> proposes an image retrieval system which uses semantics of images via probabilistic modeling. Approach <ref type="bibr" target="#b40">[41]</ref> presents a novel bi-relational graph model that comprises both the data graph and semantic label graph, and connects them by an additional bipartite graph built from label assignments. Approach <ref type="bibr" target="#b31">[32]</ref> proposes a classifier based on semantic annotations and provides the theoretical bound linking the error rate of the classifier and the number of instances required for training. Approach <ref type="bibr" target="#b14">[15]</ref> improves metric learning via the use of semantic labels with different types of semantic annotations.</p><p>Our relative learning is somewhat related to the idea using semantic information to learn metric. However, we use similarity measures to simulate realistic relation labels in supervised and unsupervised few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Multi-task Learning</head><p>Multi-task learning operates on a set of multiple related tasks. Approach <ref type="bibr" target="#b1">[2]</ref> treats the multi-task learning as a convex iterative problem. Approach <ref type="bibr" target="#b15">[16]</ref> considers the homoscedastic uncertainty of each task to weight multiple loss functions while HallNet <ref type="bibr" target="#b41">[42]</ref> learns old-fashioned descriptors as auxiliary tasks for action recognition.</p><p>In contrast, we focus on how to refine the backbone by learning from class concepts and relations to address the high-level few-shot learning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Background</head><p>The concept of few-shot learning and the standard pipeline for few-shot learning are described next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Relation Learning</head><p>Few-shot learning model typically consists of two parts: (i) feature encoder and (ii) relation module e.g., a similarity network or a classifier. Below we take the two-stage 'feature encoder-relation network' <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b45">46]</ref> as an example to elaborate on main aspects of few-shot learning pipelines.</p><p>A basic relation network <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b45">46]</ref> contains 2-4 convolutional blocks and 2 fully-connected layers. Let us define the feature encoding network as f : (R W?H ; R |F | ) R K?N , where W and H denote the width and height of an input image, K is the length of feature vectors (number of filters), N = N W ?N H is the total number of spatial locations in the last convolutional feature map. For simplicity, we denote an image descriptor by ? ? R K?N , where ? = f (X; F ) for an image X ? R W?H and F are the parameters-to-learn of the encoding network.</p><p>The relation network is denoted by r : (R K ; R |R| ) R. Typically, we write r(?; R), where ? ? R K , whereas R are the parameters-to-learn of the relation network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Supervised Few-shot Learning</head><p>For the supervised L-way Z-shot problem, we assume some support images {X s } s?W from set W and their corresponding image descriptors {? s } s?W which can be considered as a Z-shot descriptor. Moreover, we assume one query image X q with its image descriptor ? q . Both the Z-shot and the query descriptors belong to one of L classes in the subset C ? ? {c 1 , ..., c L } ? I C ? C. The L-way Z-shot learning step can be defined as learning similarity:</p><formula xml:id="formula_0">? sq = r ? {? s } s?W , ? * q , R ,<label>(1)</label></formula><p>where ? refers to similarity prediction of given support-query pair, r refers to the relation network, and R denotes network parameters that have to be learnt. ? is the relation operator on features of image pairs: we simply use concatenation. Following approaches <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b45">46]</ref>, the Mean Square Error (MSE) is employed as the objective function:</p><formula xml:id="formula_1">L= c?C ? c ?C ? r {? s } s?Wc , ? q?Q: (q)=c , R ??(c?c ) 2 , where ? s = f (X s ; F ) and ? q = f (X q ; F ).<label>(2)</label></formula><p>In the above equation, W c is a randomly chosen set of support image descriptors of class c ? C ? , Q is a randomly chosen set of L query image descriptors so that its consecutive elements belong to the consecutive classes in C ? ? {c 1 , ..., c L }. (q) corresponds to the label of q ? Q. Lastly, ? refers to the indicator function equal 1 if its argument is 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Unsupervised Few-shot Learning</head><p>There are no class annotations that can be directly used for relation learning in the unsupervised setting. However, the popular self-supervised contrastive learning captures selfobject relations by learning the similarity between different augmentations of the same image. Thus, we build our unsupervised few-shot learning pipeline based on contrastive learning. Given two image inputs X and Y, we apply random augmentations on these images e.g., rotation, flip, resized crop and color adjustment via operator Aug(?), which samples these transformations according to a uniform distribution. We obtain a set of M augmented images:</p><formula xml:id="formula_2">X i ? Aug(X),? i ? Aug(Y), i ? {1, ..., M }.<label>(3)</label></formula><p>We pass augmented images to the feature encoder f to get feature descriptors and obtain relation predictions ?, ? * ? R M?M from relation network r for augmented samples of X and Y, respectively, as well as relation predictions ? ? R M?M evaluated between augmented samples of X and Y:</p><formula xml:id="formula_3">? i = f (X i ; F ), ? * j = f (? j ; F ), i, j ? {1, ..., M }, (4) ? ij = r (? i , ? j ; R) , ? ij = r ? i , ? * j ; R , ? * ij = r ? * i , ? * j ; R .</formula><p>Lastly, we minimize the contrastive loss L urn w.r.t. F and R in order to push closer augmented samples generated from the same image (X and Y, resp.) and push away augmented samples generated from pairs images X and Y:</p><formula xml:id="formula_4">L urn = ? ? 1 2 F + ? * ? 1 2 F + ? 2 F .<label>(5)</label></formula><p>In practice, we sample a large number of image pairs X and Y with the goal of minimizing Eq. (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Approach</head><p>Below, we firstly explain the Relative Learning and Absolute Learning modules followed by the introduction of the Absolute-relative Learning pipeline. We note that all auxiliary information e.g., attributes and word2vec embeddings are used in the label space (not as extra inputs).</p><p>Given images X i and X j , we feed them into the feature encoder f to get image representations</p><formula xml:id="formula_5">? i = f (X i ; F ) and ? j = f (X j ; F ),</formula><p>where F are the parameters of feature encoder. Subsequently, we perform our proposed Relative Learning and Absolute Learning on ? i and ? j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Relative Learning</head><p>In conventional few-shot learning, binary class labels are employed to train the CNNs in order to model the relations between pairs of images. However, labeling such pairs as similar/dissimilar (i.e., {0, 1}) cannot fully reflect the actual relations between objects.</p><p>In this paper, we take a deeper look at how to represent relations in the few-shot learning scenario. To better exploit class relations in the label space, we employ semantic annotations e.g., attributes and word2vec. Based on these semantic annotations, we investigate how semantic relation labels influence the final few-shot learning performance. <ref type="figure" target="#fig_0">Figure 2</ref> (bottom right corner) shows that the classic relation learning can be viewed as an intersection (or relation) operation over the original class labels. Thus, we apply intersection on the semantic annotations to obtain the relative semantic information for the relative supervision, which can contribute to obtaining more realistic image relations in the label space. Let us denote the class labels and attributes of image X i as c i , a i . Given two samples X i and X j with their class labels c i , c j (and one-hot vectors c i , c j ) and attributes a i , a j , we obtain the binary relation label? ij which represents if the two images are from the same class. We also have semantic relation label? ij which represents attributes shared between X i and X j . Semantic annotations often contain continuous rather than binary values. Thus, we use the RBF function with the p p norm. Specifically, we obtain:</p><formula xml:id="formula_6">c ij = c i ?c j = ?(c i ?c j ) and? ij = e ?||ai?aj || p p ,<label>(6)</label></formula><p>If we train the network only with? ij , it becomes the basic few-shot learning. However, the simultaneous use of? ij and a ij for similarity learning should yield smoother similarity decision boundaries.</p><p>To learn from multi-modal relative supervisions, we apply a two-stage learner consisting of a shared part g and respective parts r. Let us denote the class and semantic relative learners as r c and r a . To make relative predictions, we firstly apply the relation operator ? over ? i and ? j (concatenation along the channel mode), and feed such a relation descriptor into g (4 blocks of Conv-BN-ReLU-MaxPool) to obtain the refined pair-wise representation ? ij :</p><formula xml:id="formula_7">? ij = g(?(? i , ? j ); G).<label>(7)</label></formula><p>Subsequently, we feed ? ij into learners r c and r a to get class-and semantics-wise relation predictions? * ij and? * ij :</p><formula xml:id="formula_8">c * ij = r c (? ij ; R c ) and? * ij = r a (? ij ; R s ),<label>(8)</label></formula><p>where R c and R s refer to the parameters of r c and r a , respectively. The objectives for class-and semantic-wise relative learners are:</p><formula xml:id="formula_9">L relc = i j (r c (? ij ; R c ) ?? ij ) 2 ,<label>(9)</label></formula><formula xml:id="formula_10">L rels = i j (r a (? ij ; R s ) ?? ij ) 2 .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Absolute Learning</head><p>In contrast to Relative Learning which applies the relative labels to learn similarity, Absolute Learning refers to the strategy in which the network learns predefined object annotations e.g., class labels, attributes, etc. The motivation behind the Absolute Learning is that current few-shot learning pipelines use the relation labels as supervision which prevents the network from capturing objects concepts. In other words, the network knows if the two objects are similar (or not) but it does not know what these objects are.</p><p>Branches for Absolute Learning are shown in <ref type="figure" target="#fig_0">Figure 2</ref>. In this paper, we apply an additional network branch following the feature encoder to learn the absolute object annotations. Firstly, consider the class prediction as an example. Once we obtain the image representation ? i given image X i , we feed it into the class absolute learner h c with parameter H c .:</p><formula xml:id="formula_11">c * i = h c (? i ; H c ).<label>(11)</label></formula><p>Subsequently, we apply the cross-entropy loss to train the class absolute learner (l c is the target class integer):</p><formula xml:id="formula_12">L absc = ? N i log( exp(c * i [l c i ]) j exp(c * i [j])</formula><p>).</p><p>For the semantic absolute learner, we use the MSE loss by feeding ? i into h a :</p><formula xml:id="formula_14">a * i = h a (? i ; H s ),<label>(13)</label></formula><formula xml:id="formula_15">L abss = 1 N N i ||a i ? a * i || 2 2 .<label>(14)</label></formula><p>The Absolute Learning module may appear somewhat similar to self-supervised learning applied to few-shot learning. However, we use discriminators to classify different types of object annotations while the typical self-supervision recognises the patterns of image transformations. We believe our strategy helps refine the feature encoder to capture both the notion of similarity as well as concrete object concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Absolute-relative Learning</head><p>For our Absolute-relative Learning (ArL), we simultaneously train the relation network with relative object similarity labels, and introduce an auxiliary task which learns specific object labels. The pipeline of ArL is shown in <ref type="figure" target="#fig_0">Figure 2</ref> which highlights that the ArL model uses the auxiliary semantic soft labels to train the relation network to capture more realistic image relations while employing auxiliary predictor branches to infer different types of object information, thus refining the feature representations and the feature encoder.</p><p>In addition to merging the absolute/relative learners, we introduce several connections from the outputs of absolute and relative learners wired to relative learners to promote the original relation learner, which does not require absolute labels or semantic labels at the testing time. In contrast, multi-modal learning needs all modalities in the testing step. <ref type="figure">Figure 3</ref> shows the Absolute-relative Learning pipeline (unsupervised setting). As the supervised ArL, the unsupervised ArL pipeline consists of absolute and relative learners. However, the annotations used during the training phase are self-supervised augmentation keys, not class labels.</p><p>Let l denote the number of layers in g. We apply ? over the intermediate descriptor ?</p><formula xml:id="formula_16">(l?1) ij</formula><p>, which is the (l-1)-th layer  <ref type="figure">Figure 3</ref>: The proposed pipeline for Absolute-relative Learning (unsupervised setting). In contrast to supervised ArL that uses class and semantic annotations in absolute and relative learners, we apply a random augmentation sequence to augment unlabeled datapoints, and we store the augmentation keys as instance annotations.</p><p>of g, and absolute predictions c * i , c * j , a * i , a * j . We call this operation the absolute feedback:</p><formula xml:id="formula_17">? (l?1) ij = ? ? (l?1) ij , c * i ), c * j ), a * i , a * j .<label>(15)</label></formula><p>We use? ij from the last layer of g to train the semantic relative learner r a :</p><formula xml:id="formula_18">? ij =? (l) ij = g (l) ? (l?1) ij ,<label>(16)</label></formula><formula xml:id="formula_19">L rels = i j r a ? ij ; R s ?? ij 2 ,? ij = e ?||ai?aj | p p .</formula><p>Let? * ij denote the outputs of semantic relative learner. Then we apply the relative feedback by combining? ij and a * ij to promote the training of class relative learner r c :</p><formula xml:id="formula_20">? * ij = ?(? ij ,? * ij ),<label>(17)</label></formula><formula xml:id="formula_21">L relc = i j r c ? * ij ; R c ?? ij 2 ,? ij = ?(c i ?c j ).</formula><p>We minimize the following objective for ArL:</p><p>min L relc + ?L rels + ?L absc + ?L abss .</p><p>where (?, ?, ?) ? [0.001; 1] 3 are hyper-parameters that control the impact of each learner and are estimated with 20 steps of the HyperOpt package [4] on a given validation set. Nullifying ?, ? or ? disables corresponding losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>Below, we demonstrate the usefulness of our approach by evaluating it on the miniImagenet <ref type="bibr" target="#b38">[39]</ref>, fine-grained CUB-200-2011 <ref type="bibr" target="#b39">[40]</ref> and Flower102 <ref type="bibr" target="#b30">[31]</ref> datasets. <ref type="figure" target="#fig_0">Figure  2</ref> presents our ArL with the two-stage relation learning pipeline but ArL applies to any few-shot learning models with any type of base learners (e.g., nearest neighbour discrimination, relation module, multi-class linear classifier, etc). The core objective of ArL is to improve the representation quality. Thus, we employ the classic baseline models, i.e., Prototypical Net (PN) <ref type="bibr" target="#b36">[37]</ref>, Relation Net <ref type="bibr" target="#b37">[38]</ref>, SoSN <ref type="bibr" target="#b45">[46]</ref>, MetaOptNet <ref type="bibr" target="#b23">[24]</ref>, etc, as our baseline models to evaluate our Relative Learning, Absolute Learning and the Absolute-relative Learning in both supervised and unsupervised settings. The Adam solver is used for model training. We set the initial learning rate to be 0.001 and decay it by 0.5 every 50000 iterations. We evaluate ArL on RelationNet (RN) <ref type="bibr" target="#b37">[38]</ref>, Prototypical Net (PN) <ref type="bibr" target="#b36">[37]</ref>, Secondorder Similarity Network (SoSN) <ref type="bibr" target="#b45">[46]</ref> and MetaOptNet <ref type="bibr" target="#b23">[24]</ref>. For augmentations in the unsupervised setting, we randomly apply resized crop (scale 0.6-1.0, ratio 0.75-1.33), horizon-tal+vertical flips, rotations (0-360 ? ), and color jitter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>Below, we describe our setup, standard and fine-grained datasets with semantic annotations and evaluation protocols. miniImagenet <ref type="bibr" target="#b38">[39]</ref> consists of 60000 RGB images from 100 classes, each class containing 600 samples. We follow the standard protocol <ref type="bibr" target="#b38">[39]</ref> and use 80/20 classes for training/testing, and images of size 84?84 for fair comparisons with other methods. For semantic annotations, we manually annotate 31 attributes for each class. We also leverage word2vec extracted from GloVe as the class embedding.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Caltech-UCSD-Birds 200-2011 (CUB-200-2011) [40] has</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Performance Analysis</head><p>Absolute-relative Learning (ArL). <ref type="table" target="#tab_1">Table 1</ref> shows that Absolute-relative Learning (ArL) effectively improves the performance on all datasets. On miniImagenet, SoSN+ArL improve the 1-and 5-shot performance by 3.6% and 4.1%, MetaOptNet+ArL improves the performance by 2.6% and  <ref type="figure">Figure 4</ref>: The validation of p given the semantic similarity measure function e ?||a i ?a i || p p on selected datasets.  Attribute absolute predictions -I1</p><p>Attribute absolute predictions -I2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attribute absolute predictions -I3</head><p>Attribute absolute predictions -I4</p><p>Attribute relative predictions (I1 -I2)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Attribute relative predictions (I2 -I3)</head><p>Attribute relative predictions (I3 -I4)    <ref type="table" target="#tab_2">Table 2</ref>, SoSN+ArL improves the 1-and 5-shot accuracy by 1.4% and 1.6%. For unsupervised learning, ArL with SoSN brings 3.5% and 4.4% gain on miniImagenet, 1.0% and 5.7% gain on CUB-200-2011, 7.9% and 8.1% gain on Flower102 for 1-and 5-shot learning, respectively. Our unsupervised U-SoSN+ArL often outperforms recent supervised methods on fine-grained classification datasets. Visualization. Below, we visualize absolute and relative semantic predictions to explain how such an information can be used. As shown in <ref type="figure">Fig. 5</ref>, we randomly select 4 images from two classes, among which I 1 and I 2 belong to one class, and I 3 and I 4 belong to another class. <ref type="figure">Figure 5</ref> shows that the semantic absolute predictions of images from the same class have more consistent distributions, the relative predictions over same-class image pairs have high responses to the same subset of bins. Predictions over images from disjoint classes result in smaller intersection of corresponding peaks. Ablations on absolute and relative learners. <ref type="figure">Figure 4</ref> shows results w.r.t. p from Eq. 6. <ref type="table">Table 5</ref> shows how different absolute and relative learners affect few-shot learning results on miniImagenet. For example, for the SoSN baseline, the attribute-based absolute and relative learners work the best among all absolute and relative learning modules. Relative Learning (RL).    <ref type="figure">Figure 6</ref>: Ablations on ?, ?, ? for SoSN <ref type="bibr" target="#b45">[46]</ref> and RN <ref type="bibr" target="#b37">[38]</ref> in the supervised setting. These evaluations are just an illustration as we tune parameters on the validation splits via the HyperOpt package.</p><p>indicate that the performance of few-shot similarity learning can be improved by employing the semantic relation labels at the training stage. For instance, SoSN with attribute soft label (att.) achieves 0.6% and 1.7% gain for 1-and 5-shot protocols, compared with the baseline (SoSN) in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>The results on CUB-200-2011 and Flower102 from <ref type="table" target="#tab_2">Table 2</ref> indicate similar gains. Absolute Learning (AL). <ref type="table">Table 5</ref> shows that different absolute learning modules help improve the performance on miniImagenet. SoSN with the attribute predictor (SoSN-AL) achieves the best performance of 55.61% on 1-shot and 71.03% (5-shot). <ref type="table">Table 5</ref> shows that applying multiple absolute learning modules does not always further improve the accuracy. The attribute-based predictor (att.) also works the best among all variants on CUB-200-2011 and Flower102. For instance, SoSN with the attribute-based predictor achieves 2.1% and 3.3% improvements on CUB-200-2011, and 2.4% and 2.1% improvement on Flower102 for 1-and 5-shot protocols, respectively. We note that the class predictor (cls.) does not work well on the fine-grained classification datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this paper, we have demonstrated that binary labels commonly used in few-shot learning cannot capture complex class relations well, leading to inferior results. Thus, we have introduced semantic annotations to aid the modeling of more realistic class relations during network training. Moreover, we have proposed a novel Absolute-relative Learning (ArL) paradigm which combines the similarity learning with the concept learning, and we extend ArL to unsupervised FSL. This surprisingly simple strategy appears to work well on all datasets in both supervised and unsupervised settings, and it perhaps resembles a bit more closely the human learning processes. In contrast to multi-modal learning, we only use semantic annotations as labels in training, and do not use them during testing. Our proposed approach achieves the state-of-the-art performance on all few-shot learning protocols. Below we supplement additional results in the unsupervised setting on two popular datasets, tiered-Imagenet and OpenMIC. tiered-Imagenet consists of 608 classes from ImageNet. We follow the protocol that uses 351 base classes, 96 validation classes and 160 novel test classes. Open MIC is the Open Museum Identification Challenge (Open MIC) <ref type="bibr" target="#b20">[21]</ref>, a recent dataset with photos of various museum exhibits, e.g. paintings, timepieces, sculptures, glassware, relics, science exhibits, natural history pieces, ceramics, pottery, tools and indigenous crafts, captured from 10 museum spaces according to which this dataset is divided into 10 subproblems. In total, it has 866 diverse classes and 1-20 images per class. Following the setup in SoSN, we combine (shn+hon+clv), (clk+gls+scl), (sci+nat) and (shx+rlc) into subproblems p1,..., p4, and form 12 possible pairs in which subproblem x is used for training and y for testing (x?y). Results on tiered-Imagenet. <ref type="table" target="#tab_10">Table 4</ref> shows that our proposed unsupervised few-shot learning strategy achieves strong results of 42.31% and 57.21% accuracy for 1-and 5-shot learning protocols. Though it does not outperform the recent supervised works, the performance of many prior works is not provided for this recent dataset. In general, we believe that our ArL approach boosts unsupervised learning and our unsupervised learning yields reasonable accuracy given no training labels being used in this process at all. Results on Open MIC. This dataset has very limited <ref type="bibr" target="#b2">(3)</ref><ref type="bibr" target="#b3">(4)</ref><ref type="bibr" target="#b4">(5)</ref><ref type="bibr" target="#b5">(6)</ref><ref type="bibr" target="#b6">(7)</ref><ref type="bibr" target="#b7">(8)</ref><ref type="bibr" target="#b8">(9)</ref><ref type="bibr" target="#b9">(10)</ref><ref type="bibr" target="#b10">(11)</ref><ref type="bibr" target="#b11">(12)</ref><ref type="bibr" target="#b12">(13)</ref><ref type="bibr" target="#b13">(14)</ref><ref type="bibr" target="#b14">(15)</ref> images for both base and novel classes, which highlights its difference to miniImagenet and tiered-Imagenet whose base classes consist of hundreds of images. <ref type="table" target="#tab_11">Table 6</ref> shows that our unsupervised variant of Second-order Similarity Network, U-SoSN with 224 ? 224 res. images outperforms the supervised SoSN on all evaluation protocols. Even without <ref type="table">Table 5</ref>: Ablation studies re. the impact of absolute and relative learning modules given miniImagenet dataset (5-way acc. with Conv-4 backbone given). We denote the same/different class relation as (bin.), attribute-based labels (relative and absolute) as (att.), word2wec embedding (relative and absolute) as (w2v.) and absolute class labeling as (cls.) RL and AL are Absolute and Relative Learners.</p><p>high-resolution training images, our U-SoSN outperforms the supervised SoSN on many data splits. This observation demonstrates that our unsupervised relation learning is beneficial and practical in case of very limited numbers of training images where the few-shot learning task is closer to the retrieval setting (in Open MIC, images of each exhibit constitute on one class). Most importantly, combining ArL with unsupervised SoSN boosts results further by up to 4%.</p><p>B. Ablation study on absolute and relative learners. <ref type="table">Table 5</ref> (miniImagenet as example) illustrates that the semantic relation learner enhanced performance on Relation Net <ref type="bibr" target="#b37">[38]</ref>, SoSN <ref type="bibr" target="#b45">[46]</ref> and SalNet <ref type="bibr" target="#b46">[47]</ref>. The results in the table indicate that the performance of few-shot similarity learning can be improved by employing the semantic relation labels at the training stage. For instance, SoSN with attribute soft label (att.) achieves 0.6% and 1.7% improvements for 1-and 5-shot compared to the baseline (SoSN). <ref type="table">Table 5</ref> also demonstrates the ablation studies for absolute learning. It can be seen from the table that the attribute predictor works the best among all options except for SoSN, and applying multiple Absolute Learning modules does not further improve the accuracy. We expect that attributes are a clean form of labels in contrast to word2vec and very complementary to class labels cls. For augmentations, we randomly apply resized crop (scale 0.6-1.0, ratio 0.75-1.33), horizontal+vertical flips, rotations (0-360 ? ), and color jitter. Annotated per class attribute vectors (miniImagenet) have 31 attributes (5 environments, 10 colors, 7 shapes, 9 materials). For augmentation keys, taking rotation as example, we set a 4-bit degree to annotate random rotations, '0001' refers to rotations with 0 ? 90 ? , '0010' refers to rotations with 90 ? 180 ? .</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The proposed pipeline for our Absolute-relative Learning (supervised setting). It consists of three blocks, namely (i) feature encoder to extract the image representations, (ii) Absolute Learning module to enhance the feature quality with auxiliary supervision, (iii) Relative Learning module to learn image relations based on multi-modal relation supervisions. With our Absolute-relative Learning, we want to both learn if the two objects share the same label and how similar they are semantically e.g., in terms of shared visual attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>910557280900008 0.955278640450004 0.9 0.877525512860841 0.968377223398316 0.735424868893541 0.936754446796632 0.9 0.815609110854142 0.85857864376269 0.677509690068058 0.625834261322606 0.552786404500042 0.787867965644036 0.603767744876821 0.832667994693185 0.564110105645933 0.755051025721682 0.929289321881345 6010355 .332250084976216 0.58712839152382 0.63 0.40301515190165 0.398918615653508 0.650502525316942 0.457512886940357 0.623318841949277 0.551507575950825 0.548243616279248 0.16551894327301 0.649039230775036 0.181479990742884 0.0508467053153008 0.535239567856843 0.2936380923364 0.546637683898553 0.093782217350893 0.56444558288274 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 Figure 5 :</head><label>15</label><figDesc>Visualization of semantic absolute and relative predictions which shows how their bins relate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Compare Absolute-relative Learning How similar?</head><label></label><figDesc>Relations? e. g., colour, shape Relations? e. g., colour, shape Learn to predict various class annotations when simulating the object relations.Learn to simulate realistic object relations with all available class annotations.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Absolute Learning</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Predict</cell><cell>re d ye llo w g re e n fu rr y sm o o th sp o t e ye s le g s</cell></row><row><cell>lation twork</cell><cell>0</cell><cell>Binary MSE 1/0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Compare</cell><cell>Relative Learning How similar?</cell></row><row><cell></cell><cell cols="2">Relative MSE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">Soft Semantic Label</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Predict</cell><cell></cell></row><row><cell>lation twork</cell><cell>0</cell><cell>0.6</cell><cell>0.8 1 0 0.2 0.4 0.6</cell><cell>0</cell><cell>1</cell><cell>2</cell><cell>3</cell><cell>4</cell><cell>5</cell><cell>re d ye llo w g re e n fu rr y sm o o th sp o t e ye s le g s</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Absolute Labels:</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Relative Labels:</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Binary MSE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>lation twork</cell><cell>0</cell><cell>1/0</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>fo. Predictor</cell><cell cols="2">re d ye llo w g re e n fu rr y sm o o th sp o t e ye s le g s</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">re d ye llo w g re e n fu rr y sm o o th sp o t e ye s le g s</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>s.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="9">Relative Learning Modules</cell><cell></cell></row></table><note>ss, AND how similar the two objects are. ss, AND what the two objects are.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Evaluations on the miniImagenet dataset (5-way acc. given) for the ArL in supervised and unsupervised settings. ('U-' refers to the unsupervised FSL.)</figDesc><table><row><cell>Model</cell><cell>Backbone</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell></cell><cell cols="3">Supervised Few-shot Learning</cell></row><row><cell>Matching Nets [39]</cell><cell>-</cell><cell>43.56 ? 0.84</cell><cell>55.31 ? 0.73</cell></row><row><cell>Meta Nets [30]</cell><cell>-</cell><cell>49.21 ? 0.96</cell><cell>-</cell></row><row><cell>PN [37]</cell><cell>Conv-4-64</cell><cell>49.42 ? 0.78</cell><cell>68.20 ? 0.66</cell></row><row><cell>MAML [10]</cell><cell>Conv-4-64</cell><cell>48.70 ? 1.84</cell><cell>63.11 ? 0.92</cell></row><row><cell>RN [38]</cell><cell>Conv-4-64</cell><cell>51.36 ? 0.82</cell><cell>66.12 ? 0.70</cell></row><row><cell>SoSN [46]</cell><cell>Conv-4-64</cell><cell>53.73 ? 0.83</cell><cell>68.58 ? 0.70</cell></row><row><cell>SoSN [46]</cell><cell>ResNet-12</cell><cell>59.01 ? 0.83</cell><cell>75.49 ? 0.68</cell></row><row><cell>MAML++ [1]</cell><cell>Conv-4-64</cell><cell>52.15 ? 0.26</cell><cell>68.32 ? 0.44</cell></row><row><cell>MetaOptNet [24]</cell><cell>ResNet-12</cell><cell>62.64 ? 0.61</cell><cell>78.63 ? 0.46</cell></row><row><cell>PN + ArL</cell><cell>Conv-4-64</cell><cell>53.93 ? 0.65</cell><cell>69.68 ? 0.45</cell></row><row><cell>RN + ArL</cell><cell>Conv-4-64</cell><cell>53.79 ? 0.68</cell><cell>68.86 ? 0.43</cell></row><row><cell>SoSN + ArL</cell><cell>Conv-4-64</cell><cell>57.48 ? 0.65</cell><cell>72.64 ? 0.45</cell></row><row><cell>SoSN + ArL</cell><cell>ResNet-12</cell><cell>61.36 ? 0.67</cell><cell>78.95 ? 0.42</cell></row><row><cell>MetaOptNet + ArL</cell><cell>ResNet-12</cell><cell>65.21 ? 0.58</cell><cell>80.41 ? 0.49</cell></row><row><cell></cell><cell cols="3">Unsupervised Few-shot Learning</cell></row><row><cell>Pixel (Cosine)</cell><cell>-</cell><cell>23.00</cell><cell>26.60</cell></row><row><cell>BiGAN (knn) [7]</cell><cell>-</cell><cell>25.56</cell><cell>31.10</cell></row><row><cell>BiGAN (cluster matching) [7]</cell><cell>-</cell><cell>24.63</cell><cell>29.49</cell></row><row><cell>DeepCluster (knn) [6]</cell><cell>-</cell><cell>28.90</cell><cell>42.25</cell></row><row><cell>DeepCluster (cluster matching) [6]</cell><cell>-</cell><cell>22.20</cell><cell>23.50</cell></row><row><cell>UMTRA [17]</cell><cell>Conv-4-64</cell><cell>39.91</cell><cell>50.70</cell></row><row><cell>CACTUs [14]</cell><cell>Conv-4-64</cell><cell>39.94</cell><cell>54.01</cell></row><row><cell>U-RN</cell><cell>Conv-4-64</cell><cell>35.14 ? 0.91</cell><cell>44.10 ? 0.88</cell></row><row><cell>U-PN</cell><cell>Conv-4-64</cell><cell>35.85 ? 0.85</cell><cell>48.01 ? 0.82</cell></row><row><cell>U-SoSN</cell><cell>Conv-4-64</cell><cell>37.94 ? 0.87</cell><cell>50.95 ? 0.81</cell></row><row><cell>U-RN + ArL</cell><cell>Conv-4-64</cell><cell>36.37 ? 0.92</cell><cell>46.97 ? 0.86</cell></row><row><cell>U-PN + ArL</cell><cell>Conv-4-64</cell><cell>38.76 ? 0.84</cell><cell>51.08 ? 0.84</cell></row><row><cell>U-SoSN + ArL</cell><cell>Conv-4-64</cell><cell>41.13 ? 0.84</cell><cell>55.39 ? 0.79</cell></row><row><cell>U-SoSN + ArL</cell><cell>ResNet-12</cell><cell>41.08 ? 0.83</cell><cell>57.01 ? 0.79</cell></row><row><cell>11788 images of 200 bird species. 100/50/50 classes are</cell><cell></cell><cell></cell><cell></cell></row><row><cell>randomly selected for meta-training, meta-validation and</cell><cell></cell><cell></cell><cell></cell></row><row><cell>meta-testing. 312 attributes are provided for each class.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Evaluations on the CUB-200-2011 and Flower102. (5way acc. given).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Flower102 [31] is a fine-grained category recognition</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>dataset that contains 102 classes of various flowers. Each</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>class consists of 40-258 images. We randomly select 60</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>meta-train classes, 20 meta-validation classes and 22 meta-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>test classes. 1024 attributes are provided for each class.</cell></row><row><cell></cell><cell cols="2">CUB-200-2011</cell><cell cols="2">Flower102</cell></row><row><cell>Model</cell><cell>1-shot</cell><cell>5-shot</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell></cell><cell cols="4">Supervised Few-shot Learning</cell></row><row><cell>PN [37]</cell><cell>37.42</cell><cell>51.57</cell><cell>62.81</cell><cell>82.11</cell></row><row><cell>RN [38]</cell><cell>40.56</cell><cell>53.91</cell><cell>68.26</cell><cell>80.94</cell></row><row><cell>SoSN [46]</cell><cell>46.72</cell><cell>60.34</cell><cell>71.90</cell><cell>84.87</cell></row><row><cell>RN + ArL</cell><cell>44.53</cell><cell>58.76</cell><cell>71.12</cell><cell>83.49</cell></row><row><cell>SoSN -RL(cls.) [46]</cell><cell>46.72</cell><cell>60.34</cell><cell>71.90</cell><cell>84.87</cell></row><row><cell>SoSN -RL(att.)</cell><cell>49.24</cell><cell>64.04</cell><cell>74.96</cell><cell>87.21</cell></row><row><cell>SoSN -AL(cls.)</cell><cell>46.88</cell><cell>60.90</cell><cell>72.97</cell><cell>85.35</cell></row><row><cell>SoSN -AL(att.)</cell><cell>48.85</cell><cell>63.64</cell><cell>74.31</cell><cell>86.97</cell></row><row><cell>SoSN + ArL</cell><cell>50.62</cell><cell>65.87</cell><cell>76.21</cell><cell>88.36</cell></row><row><cell></cell><cell cols="4">Unsupervised Few-shot Learning</cell></row><row><cell>BiGAN(knn)[7]</cell><cell>28.02</cell><cell>30.17</cell><cell>44.68</cell><cell>59.12</cell></row><row><cell>U-RN</cell><cell>29.36</cell><cell>36.36</cell><cell>55.54</cell><cell>68.86</cell></row><row><cell>U-PN</cell><cell>29.87</cell><cell>37.13</cell><cell>55.36</cell><cell>68.49</cell></row><row><cell>U-SoSN</cell><cell>36.89</cell><cell>45.81</cell><cell>61.26</cell><cell>75.98</cell></row><row><cell>U-RN + ArL</cell><cell>31.27</cell><cell>38.41</cell><cell>57.19</cell><cell>70.23</cell></row><row><cell>U-PN + ArL</cell><cell>31.58</cell><cell>39.95</cell><cell>57.61</cell><cell>70.31</cell></row><row><cell>U-SoSN + ArL</cell><cell>37.93</cell><cell>51.55</cell><cell>69.14</cell><cell>84.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study of the impact of different annotations (e.g., class labels, attributes) on ArL.</figDesc><table><row><cell></cell><cell cols="2">Rel. Learn.</cell><cell cols="2">Abs. Learn.</cell><cell cols="2">Top-1 Acc.</cell></row><row><cell>Baseline</cell><cell>cls.</cell><cell>att.</cell><cell>cls.</cell><cell>att.</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>51.36</cell><cell>65.32</cell></row><row><cell>RN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>52.38 51.41</cell><cell>66.74 66.01</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>52.35</cell><cell>66.53</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>53.73</cell><cell>68.58</cell></row><row><cell>SoSN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>55.56 55.12</cell><cell>70.97 70.91</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>55.31</cell><cell>71.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 (Table 1</head><label>51</label><figDesc>miniImagenet) illustrates the performance enhanced by the semantic-based relation on Relation Net, SoSN and SalNet. Results in the table</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>-1-1</cell></row><row><cell></cell><cell>0</cell><cell>55.25</cell><cell></cell></row><row><cell cols="2">0.0001</cell><cell>56.21</cell><cell></cell></row><row><cell cols="2">0.001</cell><cell>57.11</cell><cell></cell></row><row><cell cols="2">0.01</cell><cell>55.01</cell><cell></cell></row><row><cell cols="2">0.1</cell><cell>53.02</cell><cell></cell></row><row><cell></cell><cell>58</cell><cell></cell><cell></cell></row><row><cell>Top-1 acc.</cell><cell>55 56 57</cell><cell></cell><cell></cell></row><row><cell></cell><cell>54</cell><cell></cell><cell></cell></row><row><cell></cell><cell>53</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0</cell><cell>? 0.0001 0.001</cell><cell>0.01</cell><cell>0.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell></cell><cell>Lrel-c</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>53.73</cell><cell>53.73</cell><cell>53.73</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.001</cell><cell>53.89</cell><cell>53.95</cell><cell>54.68</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.01</cell><cell>54.45</cell><cell>55.12</cell><cell>55.31</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0.1</cell><cell>55.51</cell><cell>55.03</cell><cell>54.67</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>1</cell><cell>55.01</cell><cell>54.87</cell><cell>54.91</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>56</cell><cell></cell><cell></cell><cell></cell><cell>53</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>55</cell><cell></cell><cell></cell><cell></cell><cell>52</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>54</cell><cell></cell><cell></cell><cell>Lrel-s</cell><cell>51</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Labs-s</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>53</cell><cell></cell><cell></cell><cell>Labs-c</cell><cell>50</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>0</cell><cell>0.001</cell><cell>0.01</cell><cell>0.1</cell><cell>1</cell><cell>0</cell><cell>0.001</cell><cell>0.01</cell><cell>0.1</cell><cell>1</cell></row><row><cell></cell><cell></cell><cell>?, ?, ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?, ?, ?</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 2 - 1</head><label>21</label><figDesc></figDesc><table><row><cell></cell><cell>Lrel-c</cell><cell>?</cell><cell>?</cell></row><row><cell></cell><cell>0</cell><cell>51.36</cell><cell>51.36</cell><cell>51.36</cell></row><row><cell></cell><cell>0.001</cell><cell>51.91</cell><cell>50.89</cell><cell>51.87</cell></row><row><cell></cell><cell>0.01</cell><cell>51.78</cell><cell>51.41</cell><cell>52.01</cell></row><row><cell></cell><cell>0.1</cell><cell>52.36</cell><cell>51.01</cell><cell>52.35</cell></row><row><cell></cell><cell>1</cell><cell>52.38</cell><cell>51.13</cell><cell>52.02</cell></row><row><cell>SoSN</cell><cell>RN</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Lrel-s</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Labs-s</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Labs-c</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 4 :</head><label>4</label><figDesc>Top-1 accuracy on the novel test classes of the tiered-Imagenet dataset (5-way acc. given). Note that 'U-' variants do not use class labels during learning at all.</figDesc><table><row><cell>Model</cell><cell>1-shot</cell><cell>5-shot</cell></row><row><cell>MAML</cell><cell>51.67 ? 1.81</cell><cell>70.30 ? 0.08</cell></row><row><cell>Prototypical Net</cell><cell>53.31 ? 0.89</cell><cell>72.69 ? 0.74</cell></row><row><cell>Relation Net</cell><cell>54.48 ? 0.93</cell><cell>71.32 ? 0.78</cell></row><row><cell>SoSN</cell><cell>58.62 ? 0.92</cell><cell>75.19 ? 0.79</cell></row><row><cell>Pixel (Cosine)</cell><cell>27.13 ? 0.94</cell><cell>32.35 ? 0.76</cell></row><row><cell>BiGAN(knn)</cell><cell>29.65 ? 0.92</cell><cell>34.08 ? 0.75</cell></row><row><cell>U-RN</cell><cell>37.23 ? 0.94</cell><cell>49.54 ? 0.83</cell></row><row><cell>U-PN</cell><cell>38.83 ? 0.92</cell><cell>50.64 ? 0.81</cell></row><row><cell>U-SoSN</cell><cell>42.07 ? 0.92</cell><cell>56.21 ? 0.76</cell></row><row><cell>U-SoSN + ArL</cell><cell cols="2">43.68 ? 0.91 58.56 ? 0.74</cell></row><row><cell cols="2">Appendices</cell><cell></cell></row><row><cell cols="3">A. Additional results in unsupervised setting.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Evaluations on the Open MIC dataset (Protocol I) (given 5-way 1-shot learning accuracies). Note that the ' U-' variants do not use class labels during learning at all.</figDesc><table><row><cell>Model</cell><cell cols="7">p1? p2 p1? p3 p1? p4 p2? p1 p2? p3 p2? p4 p3? p1</cell><cell>p3? p2</cell><cell>p3? p4</cell><cell>p4? p1</cell><cell>p4? p2</cell><cell>p4? p3</cell></row><row><cell>Relation Net</cell><cell>71.1</cell><cell>53.6</cell><cell>63.5</cell><cell>47.2</cell><cell>50.6</cell><cell>68.5</cell><cell>48.5</cell><cell>49.7</cell><cell>68.4</cell><cell>45.5</cell><cell>70.3</cell><cell>50.8</cell></row><row><cell>SoSN</cell><cell>81.4</cell><cell>65.2</cell><cell>75.1</cell><cell>60.3</cell><cell>62.1</cell><cell>77.7</cell><cell>61.5</cell><cell>82.0</cell><cell>78.0</cell><cell>59.0</cell><cell>80.8</cell><cell>62.5</cell></row><row><cell>Pixle (Cosine)</cell><cell>56.8</cell><cell>40.4</cell><cell>57.5</cell><cell>33.3</cell><cell>35.1</cell><cell>46.1</cell><cell>32.3</cell><cell>44.6</cell><cell>45.9</cell><cell>33.5</cell><cell>50.1</cell><cell>34.6</cell></row><row><cell>BiGAN(knn)</cell><cell>59.9</cell><cell>43.2</cell><cell>60.3</cell><cell>37.1</cell><cell>38.6</cell><cell>50.2</cell><cell>37.6</cell><cell>48.2</cell><cell>47.5</cell><cell>38.1</cell><cell>55.0</cell><cell>37.8</cell></row><row><cell>U-RN</cell><cell>70.3</cell><cell>50.3</cell><cell>64.1</cell><cell>42.9</cell><cell>48.2</cell><cell>61.1</cell><cell>53.2</cell><cell>59.1</cell><cell>55.7</cell><cell>48.5</cell><cell>68.3</cell><cell>45.2</cell></row><row><cell>U-PN</cell><cell>70.1</cell><cell>49.7</cell><cell>64.4</cell><cell>43.3</cell><cell>47.9</cell><cell>60.8</cell><cell>52.8</cell><cell>59.4</cell><cell>56.2</cell><cell>49.1</cell><cell>68.8</cell><cell>44.9</cell></row><row><cell>U-SoSN</cell><cell>78.6</cell><cell>58.8</cell><cell>74.3</cell><cell>61.1</cell><cell>57.9</cell><cell>72.4</cell><cell>62.3</cell><cell>75.6</cell><cell>73.7</cell><cell>58.5</cell><cell>76.5</cell><cell>54.6</cell></row><row><cell>U-SoSN + ArL</cell><cell>80.2</cell><cell>59.7</cell><cell>76.1</cell><cell>62.8</cell><cell>59.6</cell><cell>74.4</cell><cell>64.2</cell><cell>78.4</cell><cell>75.2</cell><cell>60.1</cell><cell>79.2</cell><cell>57.3</cell></row><row><cell cols="5">C. Remaining experimental details.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work is in part supported by the Equipment Research and Development Fund (no. ZXD2020C2316), NSF Youth Science Fund (no. 62002371), the ANU VC's Travel Grant and CECS Dean's Travel Grant (H. Zhang's stay at the University of Oxford).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">How to train your maml</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antreas</forename><surname>Antoniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amos</forename><surname>Storkey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09502</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multi-task feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Argyriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theodoros</forename><surname>Evgeniou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Cross-generalization: Learning novel classes from a single example by feature replacement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Bart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shimon</forename><surname>Ullman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="672" to="679" />
		</imprint>
		<respStmt>
			<orgName>CVPR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Hyperopt: a python library for model selection and hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brent</forename><surname>Komer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Eliasmith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Yamins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David D</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Science &amp; Discovery</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14008</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Semantic based image retrieval: a probabilistic approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Bradshaw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighth ACM international conference on Multimedia</title>
		<meeting>the eighth ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="167" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep clustering for unsupervised learning of visual features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="132" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">One-shot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="594" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Object classification from a single example utilizing class relevance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="449" to="456" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modelagnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Few-shot learning with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.04043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Boosting few-shot visual learning with self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05186</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generating classification weights with gnn denoising autoencoders for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unsupervised learning via meta-learning. ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<idno>abs/1810.02334</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Metric learning from probabilistic labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengdi</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenglin</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuling</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1541" to="1550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Multi-task learning using uncertainty to weigh losses for scene geometry and semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised meta-learning for few-shot image and video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siavash</forename><surname>Khodadadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ladislau</forename><surname>B?l?ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>abs/1811.11819</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Edge-labeling graph neural network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongmin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><forename type="middle">D</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Domain adaptation by mixture of alignments of second-or higher-order scatter tensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Tas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Museum exhibit identification challenge for the supervised domain adaptation and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Tas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Power normalizations in fine-grained image, few-shot image and graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brenden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CogSci</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Word-level deep sign language recognition from video: A new large-scale dataset and methods comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1459" to="1469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Tspnet: Hierarchical feature learning via temporal semantic pyramid for sign language translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Swift</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanna</forename><surname>Suominen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12034" to="12045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Transferring cross-domain knowledge for video sign language recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongxu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6205" to="6214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rapid natural scene categorization in the near absence of attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufin</forename><surname>Fei Fei Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christof</forename><surname>Vanrullen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="9596" to="9601" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning from one example through shared densities on transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Matsakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="464" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Meta networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsendsuren</forename><surname>Munkhdalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="2554" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<meeting>the Indian Conference on Computer Vision, Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2008-12" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning on probabilistic labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond Chi-Wing</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillp S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 SIAM International Conference on Data Mining</title>
		<meeting>the 2014 SIAM International Conference on Data Mining<address><addrLine>SIAM</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="307" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Neural Information Processing: Research and Development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lipo</forename><surname>Jagath Chandana Rajapakse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Springer-Verlag Berlin and Heidelberg GmbH &amp; Co. KG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adaptive subspaces for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehrtash</forename><surname>Harandi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Learning to compare: Relation network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Flood</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hospedales</surname></persName>
		</author>
		<idno>CoRR:1711.06025</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>- port CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Re</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Image annotation using bi-relational graph of images and semantic labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="793" to="800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hallucinating IDT descriptors and I3D optical flow features for action recognition with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Huynh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8697" to="8707" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Deepemd: Few-shot image classification with differentiable earth mover&apos;s distance and structured classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Model selection for generalized zero-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="198" to="204" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Zero-shot kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Power normalizing second-order similarity network for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fewshot learning via saliency-guided hallucination of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Fewshot action recognition with permutation-invariant attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torr</forename><surname>Phs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koniusz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV 2020)</title>
		<meeting>the European Conference on Computer Vision (ECCV 2020)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">12350</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Laplacian regularized few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Imtiaz Masud Ziko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Dolz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ismail</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben Ayed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
