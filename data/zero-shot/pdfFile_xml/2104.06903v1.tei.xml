<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Harmonious Semantic Line Detection via Maximal Weight Clique Selection Seong-Gyun Jeong 42dot.ai</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongkwon</forename><surname>Jin</surname></persName>
							<email>dongkwonjin@mcl.korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonhui</forename><surname>Park</surname></persName>
							<email>whpark@mcl.korea.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
							<email>changsukim@korea.ac.kr</email>
							<affiliation key="aff1">
								<orgName type="institution">Korea University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Harmonious Semantic Line Detection via Maximal Weight Clique Selection Seong-Gyun Jeong 42dot.ai</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A novel algorithm to detect an optimal set of semantic lines is proposed in this work. We develop two networks: selection network (S-Net) and harmonization network (H-Net). First, S-Net computes the probabilities and offsets of line candidates. Second, we filter out irrelevant lines through a selection-and-removal process. Third, we construct a complete graph, whose edge weights are computed by H-Net. Finally, we determine a maximal weight clique representing an optimal set of semantic lines. Moreover, to assess the overall harmony of detected lines, we propose a novel metric, called HIoU. Experimental results demonstrate that the proposed algorithm can detect harmonious semantic lines effectively and efficiently. Our codes are available at https://github.com/dongkwonjin/Semantic-Line-MWCS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>A semantic line <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b18">19]</ref> is defined as a meaningful line, separating different semantic regions in a scene, which is approximated by an end-to-end straight line. A group of semantic lines in an image can be regarded as optimal, when they convey the composition of the image harmoniously, as shown in <ref type="figure">Figure 1</ref>(e). Thus, in an optimal set, the lines should harmonize with one another.</p><p>Semantic lines provide important visual cues in highlevel image understanding <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36]</ref>. In photography, semantic lines, such as horizontal, vertical, and symmetric ones, are essential composition components. Harmony of such lines are closely related to subjective quality of a photograph <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref>. In autonomous driving systems <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13]</ref>, boundaries of road lanes and sidewalks should be detected reliably to control vehicle maneuvers, which can be also described by semantic lines. Moreover, dominant parallel lines intersect at vanishing points <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b35">36]</ref> under perspective projection, conveying depth impression. They are also semantic lines <ref type="bibr" target="#b15">[16]</ref>. However, it is challenging to detect semantic lines, which are often unobvious and implied by complex boundaries of semantic regions.  <ref type="figure">Figure 1</ref>: In each scene, straight lines approximating region boundaries are shown in (a). Among them, three subsets of lines are shown in (b), (c), and (d), which are insufficient, over-segmenting, and sub-optimal for describing the composition of the scene, respectively. In contrast, an optimal set of semantic lines in (e) convey the composition of the scene harmoniously.</p><p>Many techniques have been developed to detect line segments in a scene by exploiting hand-crafted features <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref> or deep features <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37]</ref>. However, they may extract redundant short line segments or focus on identifying obvious line structures in man-made environments. Recently, several attempts have been made to detect semantic lines <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>. Horizon lines, which are a specific type of semantic lines, have been estimated by CNN-based methods <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b31">32]</ref>. In <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>, semantic line detectors have been proposed. They have two stages: line detection and refinement. In the detection stage, deep line features are extracted to classify line candidates, but implied lines may be undetected or the computational cost for extracting discriminative features can be too high. In the refinement stage, redundant lines are removed through non-maximum suppression (NMS) or pairwise comparison. Although these techniques provide promising results, they may fail to consider the harmony between detected lines and thus may yield sub-optimal results, as shown in <ref type="figure">Figure 1</ref> In this paper, a novel algorithm to detect an optimal set arXiv:2104.06903v1 [cs.CV] 14 Apr 2021 of harmonious semantic lines is proposed based on maximal weight clique selection (MWCS). We formulate the detection as finding a maximal weight clique in a complete graph <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>. To this end, we design two networks: selection network (S-Net) and harmonization network (H-Net). Given an image and a set of line candidates, S-Net first computes the classification probability and regression offsets of each candidate. Second, we filter out irrelevant lines by performing a selection-and-removal process. Third, we construct a complete graph, in which the node set contains the selected lines. H-Net computes its edge weights. Finally, we determine a maximal weight clique representing harmonious semantic lines. Experimental results demonstrate that the proposed algorithm can detect harmonious semantic lines accurately and efficiently.</p><p>This work has the following major contributions:</p><p>? We formulate the semantic line detection as finding an maximal weight clique in a complete graph.</p><p>? We develop two networks, S-Net and H-Net, to construct the complete graph.</p><p>? We introduce a novel metric, called HIoU, to assess the overall harmony of semantic lines, which is more reasonable than the existing metrics in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>? The proposed algorithm yields competitive semantic line detection performance to the state-of-the-art DRM technique <ref type="bibr" target="#b15">[16]</ref>, while reducing the computational complexity by a factor of 1 20 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Line segment detection</head><p>Line segments give important visual cues for image semantics. In line segment detection <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b27">28]</ref>, many short segments are detected using low-level features, such as image gradients. This approach, however, may not discriminate meaningful lines from noisy ones. To utilize higherlevel features, deep learning methods have been proposed <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37]</ref>. In <ref type="bibr" target="#b14">[15]</ref>, a line heat map and junctions were predicted by networks. Then, a wireframe was obtained by connecting the junctions based on the heat map. In <ref type="bibr" target="#b36">[37]</ref>, a line candidate was generated by connecting two junctions and then was classified into either a salient one or not. In <ref type="bibr" target="#b30">[31]</ref>, attraction field maps were computed by a network to deal with local ambiguity and class imbalance in line segment detection. In <ref type="bibr" target="#b21">[22]</ref>, a network was trained with a Hough transform block to combine local information with global line priors. These methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37]</ref> focus on detecting obvious lines in man-made environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Semantic line detection</head><p>Semantic lines, located near the boundaries of semantic regions, represent the layout and composition of images. Several methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref> have been developed to detect implied but semantically meaningful lines. In <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>, horizon lines were detected by CNNs, which were refined by exploiting vanishing points or using soft labels of line parameters. In <ref type="bibr" target="#b18">[19]</ref>, Lee et al. proposed the first semantic line detector. They devised a line pooling layer to extract local features along each line candidate. Those features were fed into classification and regression layers to detect semantic lines. Then, an NMS scheme was performed to remove redundant lines, based on the edge detector <ref type="bibr" target="#b29">[30]</ref>. In <ref type="bibr" target="#b15">[16]</ref>, Jin et al. extracted more discriminative line features by designing a region pooling layer and the mirror attention module. Then, they selected the most semantic lines and removed redundant lines alternately through pairwise ranking and matching. In <ref type="bibr" target="#b9">[10]</ref>, Han et al. transformed line features into a Hough parametric space to facilitate parallel processing of multiple line candidates. Then, they trained a network to predict a line probability map, which was used to determine semantic lines by computing the centroids of connected components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Road lane detection</head><p>In autonomous driving systems, it is important to reliably detect the boundaries of road lanes, sidewalks, or crosswalks. Early methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b34">35]</ref> used hand-crafted low-level features to extract lanes. Recently, to cope with complicated road scenes, attempts have been made to detect road lanes using deep semantic segmentation frameworks <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. In <ref type="bibr" target="#b23">[24]</ref>, Pan et al. proposed a network to learn spatial relationship of lanes through message passing between convolution layers. In <ref type="bibr" target="#b13">[14]</ref>, a network was designed to generate attention maps at different layers, which were used to refine the output of deeper ones. In <ref type="bibr" target="#b12">[13]</ref>, the inter-region affinity graph was constructed to transfer structural relationship between lanes from teacher to student networks. In <ref type="bibr" target="#b24">[25]</ref>, to achieve a high processing speed, a network was developed to identify the location of each lane on a predefined set of rows only. <ref type="figure" target="#fig_2">Figure 2</ref> is an overview of the proposed algorithm, which contains S-Net and H-Net. First, given an image and a set of line candidates, S-Net computes the line probability and the regression offsets of each candidate. Second, irrelevant candidates are filtered out through a selection-and-removal process. Third, a complete graph, whose node set consists of the selected lines, is constructed and its edge weights are computed by H-Net. Finally, a maximal weight clique, representing harmonious semantic lines, is determined.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Algorithm</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem formulation</head><p>Semantic lines in an image can be regarded as optimal if they convey the composition of the image harmoniously. In other words, in an optimal set, every pair of semantic lines should harmonize with each other. As in <ref type="figure" target="#fig_4">Figure 3</ref>(b), a pair of semantic lines should direct visual attention to meaningful regions. In contrast, in <ref type="figure" target="#fig_4">Figure 3</ref>(c), two lines are redundant or inharmonious. Based on this observation, we formulate the semantic line detection as finding a maximal weight clique in a complete graph <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b7">8]</ref>. In the complete graph, detected lines form the node set, and each edge weight represents how harmonious the associated two lines are. Thus, by finding a maximal weight clique, we find an optimal set of harmonious semantic lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Node selection: filtering line candidates</head><p>It is computationally infeasible to construct a complete graph for all line candidates. Therefore, we select reliable nodes only by filtering line candidates.  Line candidate generation: A line candidate, which is an end-to-end straight line in an image, can be parameterized by polar coordinates in the Hough space <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22]</ref>. Let l = (?, ?) denote a line, where ? is its distance from the center of the image and ? is its angle from the x-axis. Then, we generate N line candidates, denoted by l n = (? n , ? n ), 1 ? n ? N , by quantizing ? and ? uniformly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>S-Net:</head><p>For each line candidate, we compute its classification probability and regression offsets. To this end, we develop S-Net based on the conventional line detectors <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>. <ref type="figure" target="#fig_5">Figure 4</ref>(a) shows the architecture of S-Net. From an image, S-Net extracts a convolutional feature map X = [X 1 , X 2 , . . . , X C ] ? R H?W ?C , where H, W , and C denote the feature height, the feature width, and the number of channels. Then, the line feature map Y = [Y 1 , Y 2 , . . . , Y C ] ? R N ?C is obtained by averaging the features of pixels along l n ;</p><formula xml:id="formula_0">Y c (n) = 1 |ln| p?ln X c (p)<label>(1)</label></formula><p>for 1 ? n ? N and 1 ? c ? C, where |l n | denotes the number of pixels along l n . We then obtain the probability vector P and the line offset matrix O by</p><formula xml:id="formula_1">P = ?(f 1 (Y )) and O = f 2 (Y )<label>(2)</label></formula><p>where f 1 and f 2 are fully-connected layers of sizes C ? 1 and C ? 2 for classification and regression, respectively, and ?(?) is the sigmoid activation function. For the nth line candidate l n = (? n , ? n ), P n indicates the probability that it is semantic, and O n = ?l n = (?? n , ?? n ) is the offset vector for line refinement in Section 3.4. The architecture and training process of S-Net are described in detail in the supplemental document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Selection and removal:</head><p>In the conventional algorithms <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>, to detect semantic lines, only the line candidates with probabilities higher than a threshold are selected and then post-processed (e.g. non-maximum suppression). However, this may cause false negatives, which have low probabilities because of being implicit but are semantic nonetheless. To reduce such false negatives, instead of thresholding, we perform the selection-and-removal process in <ref type="figure" target="#fig_2">Figure 2</ref>(b). We select the most reliable line l i by</p><formula xml:id="formula_2">i = arg max i P i<label>(3)</label></formula><p>and then remove overlapping lines with the selected one. Specifically, we remove 24 lines within the 5 ? 5 grid centered at l i in the Hough space <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b21">22]</ref>. We perform this process K times to compose the node set of K selected lines. <ref type="figure">Figure 5</ref> We extract the regional feature vector r i of R i by</p><formula xml:id="formula_3">r i = 1 |Ri| p?Ri X(p).<label>(4)</label></formula><p>We compute the softmax probability a i of the area |R i | to scale the regional feature vectors, and then concatenate the scaled vectors into R = [a 1 r 1 , a 2 r 2 , a 3 r 3 , a 4 r 4 ]</p><p>of size C ? 4. If M = 3, we fill in the rightmost vector with zeros. Then, R is fed into a fully connected layer to yield the IRC feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H-Net:</head><p>We develop H-Net using the IRC module. It takes an image and a pair of lines, indexed by i and j, to yield the harmony score h ij ranging from 0 to 1. <ref type="figure" target="#fig_5">Figure 4(b)</ref> shows the H-Net architecture. The convolution layers of VGG16 <ref type="bibr" target="#b25">[26]</ref> are used as the feature extractor, which is followed by three parallel branches of the IRC module and line pooling layers. We employ the line pooling layers to perform the pooling in (1) for lines i and j, respectively. We use two types of regression layers: one for yielding the IRC score of the two lines (Reg1), and the other for computing unary reliability of each line (Reg2). Finally, we compute the harmony score h ij by multiplying the IRC score with the average of the unary reliability levels. We configure the training data for H-Net as follows. It is assumed that every pair of ground-truth semantic lines in an image harmonize with each other. Thus, we declare such pairs as positive, while the others as negative. In other words, a line pair (i, j) is positive only if both lines i and j are semantic. Then, the harmony scoreh ij is annotated as 1 or 0 depending on whether the pair (i, j) is positive or not. However, this strict definition of a positive pair causes a class imbalance: there are too few positive pairs. Thus, we disturb the line locations of each positive pair and annotate the corresponding harmony scoreh ij to be proportional to e ?(d 2 i +d 2 j ) , where d i and d j denote the disturbances of lines i and j. Also, the loss function for training H-Net is defined as H = (h ij ?h ij ) 2 , whereh ij is the ground-truth harmony score and h ij is its estimate. The supplemental document describes the training process and architecture of H-Net in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Graph optimization: finding harmonious lines</head><p>Graph construction: We construct a complete graph G = (V, E), in which the node set V = {v 1 , v 2 , . . . , v K } represents the K lines selected using S-Net in Section 3.2. Every pair of lines are connected by an edge in the edge set <ref type="figure">Figure 5</ref>(f) visualizes a complete weighted graph.</p><formula xml:id="formula_5">E = {(v i , v j ) : i = j}. Each edge is assigned a weight w(v i , v j ) = h ij by H-Net in Section 3.3.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MWCS:</head><p>As mentioned earlier, a set of semantic lines is optimal, if any two lines in the set are harmonious with each other. Thus, finding such an optimal set is equivalent to finding a clique of nodes <ref type="bibr" target="#b7">[8]</ref>, which are mutually connected and have a maximal sum of weights (i.e. harmony scores).</p><p>Let ? denote a clique, represented by the index set of member nodes. Then, we define the harmonization energy E harmony (?) of clique ? as</p><formula xml:id="formula_6">E harmony (?) = i?? j??,j&gt;i w(v i , v j )<label>(6)</label></formula><p>which is the sum of all edge weights in ?. Finding the clique that maximizes this energy is NP-hard <ref type="bibr" target="#b5">[6]</ref>. However, in this <ref type="figure">Figure 5</ref>: Illustration of the proposed algorithm: (a) input image, (b) selected lines through the selection-and-removal process, (c) semantic lines, (d) probabilities of line candidates in the Hough space, (e) node set corresponding to the selected lines, (f) complete graph, (g) maximal clique. In (f), an edge is depicted in green or red depending on whether its weight is above the threshold ? in <ref type="bibr" target="#b7">(8)</ref> or not. As the weight approaches zero, the transparency increases.</p><formula xml:id="formula_7">(a) (b) (c) (d) (e) (f) (g)</formula><p>work, K is set to be a small number. The default K is 8. There are about 2 K possible cliques, which are also manageable. Thus, exhaustive search is adopted to find a maximal weight clique. First, we generate the set of possible cliques ? = {? t } in the graph G, where each clique ? t consists of more than two nodes. Then, we select the maximal weight clique ? that maximizes the harmonization energy:</p><formula xml:id="formula_8">? = arg max ?t?? E harmony (? t )<label>(7)</label></formula><p>subject to a constraint</p><formula xml:id="formula_9">min i,j?? w(v i , v j ) &gt; ?<label>(8)</label></formula><p>where ? is a threshold. If there is no clique satisfying the constraint, we select the maximal single-node clique ? = {i } by i = arg max i h ii .</p><p>The self-harmony score h ii is obtained by applying the same line as duplicated input to H-Net.</p><p>After obtaining the set of harmonious semantic lines, we refine each line by l vi + ?l vi <ref type="bibr" target="#b9">(10)</ref> where ?l vi denotes the offset vector, generated by the regression layer of S-Net. <ref type="figure">Figure 5</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Metrics</head><p>Conventional metrics: There are two existing metrics to assess semantic line detection results: mIoU <ref type="bibr" target="#b18">[19]</ref> and EAscore <ref type="bibr" target="#b9">[10]</ref>. In the mIoU metric, a detected line is regarded as correct if its mIoU score with the ground-truth semantic line is greater than a threshold ? as illustrated in <ref type="figure">Figure 6(a)</ref>.</p><p>In the EA-score, a detected line is regarded as correct if its similarity with the ground-truth is greater than the threshold as shown in <ref type="figure">Figure 6(b)</ref>. The similarity is composed of two factors S d and S ? , which are based on the Euclidean distance between the midpoints of the lines and the angular distance of the lines, respectively. In both metrics, the precision and the recall are computed by <ref type="bibr" target="#b10">11)</ref> where N l is the number of correctly detected semantic lines, N e is the number of false positives, and N m is the number of false negatives. Then, the F-measure is computed by</p><formula xml:id="formula_11">Precision = N l N l +Ne , Recall = N l N l +Nm<label>(</label></formula><formula xml:id="formula_12">F-measure = 2?Precision?Recall Precision+Recall .<label>(12)</label></formula><p>The area under curve (AUC) performances of the precision, recall, F-measure curves are measured in the entire range of the threshold ? , which are denoted by AUC P, AUC R, and AUC F, respectively <ref type="bibr" target="#b18">[19]</ref>. However, these metrics measure only the positional accuracy of each detected line. They do not consider how harmonious multiple detected lines are with one another in a scene. Hence, they may yield misleading scores, as exemplified in <ref type="figure">Figure 7</ref>.</p><p>HIoU metric: We propose the harmony-based intersectionover-union (HIoU) metric to assess the overall harmony of detected lines. Detected lines tend to convey harmonious impression about the composition of an image, when their mIoU = IoU , ? + IoU , ? 2 (a) mIoU metric ? ? HIoU = IoU( 1 , 1 ) + IoU( 2 , 1 ) + ? + IoU( 3 , 5 ) 9</p><p>(c) HIoU metric Ground-truth Detection result <ref type="figure">Figure 6</ref>: Illustration of two existing metrics of mIoU <ref type="bibr" target="#b18">[19]</ref> and EA-score <ref type="bibr" target="#b9">[10]</ref> and the proposed HIoU metric.  <ref type="figure">Figure 7</ref>: Comparison of mIoU <ref type="bibr" target="#b18">[19]</ref>, EA-score <ref type="bibr" target="#b9">[10]</ref>, and the proposed HIoU metric: There are two detection results for the same ground-truth. In result I, the position of each detected line is different from the ground-truth, but the detected lines convey the composition of the image relatively well. In result II, two detected lines match the ground-truth exactly, but they are not harmonious with the remaining one. As a group, they are inferior to result I. Since mIoU and EA-score consider only the accuracy of each individual line, they do not tell the difference between these two results and provide only marginally different scores. In contrast, HIoU quantifies the superiority of result I correctly.</p><p>division of the image is consistent with the division by the ground-truth. Suppose that the set of detected lines and the set of ground-truth lines divide the image into regions S = {s 1 , s 2 , . . . , s N } and T = {t 1 , t 2 , . . . , t M }, respectively. Then, we define HIoU as</p><formula xml:id="formula_13">HIoU = N i=1 max k IoU(si,t k )+ M j=1 max k IoU(tj ,s k ) N +M .<label>(13)</label></formula><p>In other words, for each s i , we find the matching t k and measure their IoU. Similarly, for each t j , we find its IoU with the matching s k . Then, the average of these bidirectional matching IoU's becomes the HIoU score.  <ref type="figure">Figure 7</ref> shows that HIoU assesses detected lines more reasonably than the existing metrics do, by considering the harmony among the detected lines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparative assessment</head><p>We compare semantic line detection results of the proposed algorithm with those of the conventional SLNet <ref type="bibr" target="#b18">[19]</ref>, DHT <ref type="bibr" target="#b9">[10]</ref>, and DRM <ref type="bibr" target="#b15">[16]</ref>.</p><p>Comparison on SEL: <ref type="figure" target="#fig_10">Figure 8</ref> compares the precision, recall, and F-measure curves of the proposed algorithm and the conventional algorithms on the SEL dataset. <ref type="table" target="#tab_1">Table 1</ref> reports the AUC performances of these curves. The proposed algorithm provides a poorer recall but a better precision than the conventional algorithms. F-measure is the harmonic mean of recall and precision. Note that the proposed algorithm outperforms all conventional algorithms in terms of F-measure and HIoU.</p><p>Comparison on SEL Hard: <ref type="table" target="#tab_1">Table 1</ref> also compares the results on SEL Hard. For this comparison as well, we use the same algorithms that are trained using the training images in the SEL dataset. As mentioned previously, SEL Hard images are much more complicated than SEL images. Also, many of SEL images contain only one semantic line. Thus, Ground-truth Proposed DRM <ref type="bibr" target="#b15">[16]</ref> DHT <ref type="bibr" target="#b9">[10]</ref> SLNet <ref type="bibr" target="#b18">[19]</ref>   it is challenging to use only SEL images to learn the harmony between lines in more complicated SEL Hard images. Nevertheless, the proposed algorithm yields competitive results to DRM, which performs the best but demands a too high computational cost. Note that the proposed algorithm is about 20 times faster than DRM. Moreover, the proposed algorithm outperforms DRM in terms of AUC P. <ref type="figure" target="#fig_11">Figure 9</ref> compares detection results on the SEL and SEL Hard datasets. The conventional algorithms detect redundant lines near object boundaries or fail to detect implied semantic lines. In contrast, the proposed algorithm detects implied as well as obvious semantic lines more reliably, while ensuring the harmony between detected lines. <ref type="table" target="#tab_2">Table 2</ref> compares the performances on the SL5K dataset. Zhao et al. <ref type="bibr" target="#b32">[33]</ref> report the performances of their algorithm in the EA-score metric only, and their training codes or model parameters are not available. Thus, we compare the results in the EA-score metric only, as done in <ref type="bibr" target="#b32">[33]</ref> . We see that the proposed algorithm outperforms Zhao et al. by significant margins 9.1, 6.9, and 8.0 in terms of precision, recall, and F-measure, respectively. Also, the proposed algorithm yields the HIoU score of 74.1. <ref type="figure" target="#fig_12">Figure 10</ref> shows some detection results. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison on SL5K:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison on CULane:</head><p>We compare the proposed algorithm with the conventional road lane detectors <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b24">25]</ref> on the 'no lane' category in CULane, in which lanes are implicit or invisible. Conventional techniques are based on the segmentation framework and the ground-truth is also given as a binary mask for each lane. Thus, for comparison, we declare the most overlapping line with the segmentation mask of each lane as a semantic line. The experimental settings are described in detail in the supplemental document. <ref type="figure" target="#fig_13">Figure 11</ref> shows some ground-truth semantic lines and compares their detection results. Although the lines are extremely unobvious, the proposed algorithm detects them more reliably than the conventional detectors. <ref type="table" target="#tab_3">Table 3</ref> compares the AUC and HIoU scores. Note that, unlike the conventional detectors, the proposed algorithm does not use the information of the maximum number of lanes in a scene. The conventional algorithms poorly recall implied or invisible lanes. The proposed algorithm is slightly less pre-Ground-truth Proposed SAD <ref type="bibr" target="#b13">[14]</ref> UFS <ref type="bibr" target="#b24">[25]</ref>    cise, but provides significantly higher recall and F-measure scores than the conventional detectors. Also, the proposed algorithm yields a better HIoU score than the conventional detectors, by exploiting the harmonious property of road lanes, such as parallelness and equal width between adjacent lanes.</p><p>Running time analysis: <ref type="table" target="#tab_1">Table 1</ref> also compares the running times. We use a PC with Intel Core i5-8500 CPU and NVIDIA RTX 2080 ti GPU. Note that SLNet and DRM require a lot of time to extract discriminative line features. Especially, DRM is the slowest method at 1.05 fps, because its mirror attention module and iterative ranking-and-matching process are too demanding. The proposed algorithm and DHT are much faster. Although DHT is the fastest, its recall performance is not competitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation studies</head><p>We conduct ablation studies to analyze the efficacy of the proposed S-Net, H-Net, and MWCS process on the SEL dataset. <ref type="table" target="#tab_4">Table 4</ref> compares several ablated methods. Method I uses S-Net only to detect semantic lines, in which the selection-and-removal process is performed iteratively until the maximum probability becomes lower than 0.5. Method II uses H-Net and the MWCS process as well, but H-Net is trained without employing the IRC module. In Method III, line offsets are not used to refine detection results. Method I is significantly inferior to the other methods, indicating that both H-Net and MWCS are essential for detecting harmonious semantic lines. Also, by comparing II with IV, we see that the inter-region correlation feature is effective for estimating the harmony between two lines. Also, from III with IV, note that the performance is improved by refining detected lines using regression offsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We proposed a novel semantic line detector. First, we developed S-Net to compute the line probabilities and offsets of line candidates. Second, we filtered out irrelevant lines through a selection-and-removal process. Third, we constructed a complete graph, whose edge weights were computed by H-Net. Finally, we determined a maximal weight clique representing a group of harmonious semantic lines. Also, to assess the overall harmony of detected lines, we proposed a novel metric called HIoU. It was experimentally demonstrated that the proposed algorithm can detect harmonious semantic lines effectively and efficiently.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Overview of the proposed algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>In a line set, a pair of lines can be harmonious and draw visual attention to meaningful regions as in (b). In contrast, they can be redundant or inharmonious as in (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Architecture of S-Net and H-Net: (a) S-Net takes an image and line candidates and extracts the line feature map. The classification probabilities and regression offsets of the line candidates are then computed by two fully connected layers. (b) H-Net takes an image and a line pair (i, j) to extract local and inter-region features. Two types of regression layers are used to compute the harmony score h ij .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(b) and (e) show such selected lines on the image and Hough spaces, respectively.3.3. Edge weighting: harmony score estimation Inter-region correlation: To tell positive pairs in Figure 3(b) from negative pairs in Figure 3(c), we design the inter-region-correlation (IRC) module that analyzes the regions separated by a pair of lines. Let R i , 1 ? i ? M , denote the regions separated by two lines. There can be three or four regions, i.e. M = 3 or 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(c) and (g) show the set of harmonious semantic lines on the image and Hough spaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Figure 6(c) illustrates how to compute an HIoU score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 :</head><label>8</label><figDesc>Comparison of the precision, recall, and Fmeasure curves in terms of the threshold ? on the SEL dataset. The mIoU metric is used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 9 :</head><label>9</label><figDesc>Comparison of semantic line detection results. The left three images are from SEL, and the others from SEL Hard.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Detection results of the proposed algorithm on the SL5K dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Comparison of semantic line detection results on the CULane dataset ('no lane' category).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>It is the first semantic line dataset, containing 1,750 outdoor images, which are split into 1,575 training and 175 testing images. Each semantic line is annotated by the coordinates of two end points on an image boundary.</figDesc><table><row><cell>CULane [24]: It is a dataset for road lane detection, con-</cell></row><row><cell>taining 88,000 training images. Its 34,680 test images are</cell></row><row><cell>classified into 9 categories. For each image, the pixel-wise</cell></row><row><cell>mask for up to 4 road lanes is provided. The proposed algo-</cell></row><row><cell>rithm is tested on 3,911 test images in the 'no lane' category,</cell></row><row><cell>in which each lane is highly implied or even invisible.</cell></row><row><cell>4. Experimental Results</cell></row><row><cell>4.1. Datasets</cell></row><row><cell>SEL [19]: SEL Hard [16]: It is a more challenging dataset for testing</cell></row><row><cell>semantic line detectors. It contains 300 test images, selected</cell></row><row><cell>from the ADE20K segmentation dataset [34]. Its semantic</cell></row><row><cell>lines are less obvious and more severely occluded in more</cell></row><row><cell>cluttered scenes.</cell></row></table><note>SL5K [33]: It is a rich and diverse dataset in terms of the number of lines and scene categories. It is composed of 4,000 training and 1,000 testing images.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of the AUC and HIoU scores (%) on the SEL and SEL Hard datasets. The processing speeds in frames per second (fps) are also compared. For the AUC scores, the mIoU metric is used.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">SEL</cell><cell></cell><cell></cell><cell cols="2">SEL Hard</cell><cell></cell><cell>fps</cell></row><row><cell></cell><cell>AUC P</cell><cell>AUC R</cell><cell>AUC F</cell><cell>HIoU</cell><cell>AUC P</cell><cell>AUC R</cell><cell>AUC F</cell><cell>HIoU</cell><cell></cell></row><row><cell>SLNet [19]</cell><cell>80.72</cell><cell>84.50</cell><cell>82.57</cell><cell>77.87</cell><cell>74.22</cell><cell>70.68</cell><cell>72.41</cell><cell>59.71</cell><cell>7.35</cell></row><row><cell>DHT [10]</cell><cell>89.27</cell><cell>78.53</cell><cell>83.56</cell><cell>79.62</cell><cell>83.55</cell><cell>67.98</cell><cell>75.09</cell><cell>63.39</cell><cell>30.30</cell></row><row><cell>DRM [16]</cell><cell>85.44</cell><cell>87.16</cell><cell>86.29</cell><cell>80.23</cell><cell>87.19</cell><cell>77.69</cell><cell>82.17</cell><cell>68.83</cell><cell>1.05</cell></row><row><cell>Proposed</cell><cell>89.61</cell><cell>84.21</cell><cell>86.83</cell><cell>81.03</cell><cell>87.60</cell><cell>72.56</cell><cell>79.38</cell><cell>65.99</cell><cell>21.74</cell></row><row><cell>Ground-truth</cell><cell cols="2">Detection result ?</cell><cell>Detection result ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>mIoU</cell><cell>0.90</cell><cell>?</cell><cell>0.88</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>EA-score</cell><cell>0.79</cell><cell>?</cell><cell>0.81</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HIoU</cell><cell>0.68</cell><cell>&gt;</cell><cell>0.59</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the EA-scores (Precision, Recall, F-measure) on the SL5K dataset.</figDesc><table><row><cell></cell><cell cols="4">Precision Recall F-measure HIoU</cell></row><row><cell>Zhao et al. [33]</cell><cell>70.3</cell><cell>74.5</cell><cell>72.3</cell><cell>-</cell></row><row><cell>Proposed</cell><cell>79.4</cell><cell>81.4</cell><cell>80.3</cell><cell>74.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison of the AUC and HIoU scores (%) on the 'no lane' category in the CULane dataset.</figDesc><table><row><cell></cell><cell cols="3">AUC P AUC R AUC F</cell><cell>HIoU</cell></row><row><cell>UFS [25]</cell><cell>93.00</cell><cell>83.47</cell><cell>87.98</cell><cell>72.68</cell></row><row><cell>SAD [14]</cell><cell>93.64</cell><cell>84.20</cell><cell>88.67</cell><cell>74.77</cell></row><row><cell>Proposed</cell><cell>92.43</cell><cell>91.66</cell><cell>92.04</cell><cell>76.46</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Ablation studies for the S-Net, H-Net, and MWCS process on the SEL dataset.</figDesc><table><row><cell></cell><cell cols="2">AUC F HIoU</cell></row><row><cell>I. S-Net</cell><cell>77.75</cell><cell>69.03</cell></row><row><cell>II. S-Net+H-Net+MWCS(w/o IRC)</cell><cell>84.66</cell><cell>79.14</cell></row><row><cell>III. S-Net+H-Net+MWCS(w/o offset)</cell><cell>86.60</cell><cell>80.33</cell></row><row><cell>IV. S-Net+H-Net+MWCS</cell><cell>86.83</cell><cell>81.03</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the National Research Foundation of Korea (NRF) through the Korea Government (MSIT) under grant NRF-2018R1A2B3003896 and in part by the 42dot Inc.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">EDLines: A real-time line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cuneyt</forename><surname>Akinlar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cihan</forename><surname>Topal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recog. Lett</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1633" to="1642" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real time detection of lane markers in urban streets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Aly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Chromatic Graph Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Meaningful alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agn?s</forename><surname>Desolneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lionel</forename><surname>Moisan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="23" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Soft labels for ordinal regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Marathe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generalized network design problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinne</forename><surname>Feremans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martine</forename><surname>Labb?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilbert</forename><surname>Laporte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The Photographer&apos;s Eye: Composition and Design for Better Digital Photos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Freeman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Focal Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Graph Theory and Its Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Yellen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>CRC Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Robust road boundary estimation for intelligent vehicles in challenging scenarios based on a semantic graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunzhao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Yamabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seiichi</forename><surname>Mita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Hough transform for semantic line detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Color-based road detection in urban traffic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinghua</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="309" to="318" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recent progresss in road and lane detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aharon</forename><surname>Bar Hillel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Levi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Raz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Vis. Appl</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="727" to="745" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Inter-region affinity distillation for road marking segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tak-Wai</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning lightweight lane detection CNNs by self attention distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuenan</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to parse wireframes in images of man-made environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Semantic line detection using mirror attention and comparative ranking and matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongkwon</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Tae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A probabilistic Hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nahum</forename><surname>Kiryati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Eldar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfred</forename><forename type="middle">M</forename><surname>Bruckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recog</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="303" to="316" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Photography: The Art of Composition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bert</forename><surname>Krages</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Simon and Schuster</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Semantic line detection and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Tae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Ul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Photographic composition classification and dominant geometric element detection for outdoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Tae</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han-Ul</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Su</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Vis. Commun. Image Represent</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="91" to="105" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tae-Hee Lee, Hyun Seok Hong, Seung-Hoon Han, and In So Kweon. VPGNet: Vanishing point guided network for lane and road marking detection and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae</forename><forename type="middle">Shin</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghak</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Bailo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namil</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep Hough-transform line priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yancong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Silvia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Robust detection of lines using the progressive probabilistic Hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Galambos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Kittler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Image Understand</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="119" to="137" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Spatial As Deep: Spatial CNN for traffic scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingang</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ultra fast structureaware deep lane detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zequn</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huanyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Sem-LSD: A learningbased semantic line segment detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xushen</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.06591</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">LSD: A fast line segment detector with a false detection control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Von</forename><surname>Rafael Grompone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremie</forename><surname>Gioi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Michel</forename><surname>Jakubowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Randall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="722" to="732" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Horizon lines in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menghua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. BMVC</title>
		<meeting>BMVC</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning attraction field representation for robust line segment detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fudong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gui-Song</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Detecting vanishing points using global image context in a nonmanhattan world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menghua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Deep Hough transform for semantic line detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang-Bin Zhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04676</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Scene parsing through ADE2020K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CVPR</title>
		<meeting>IEEE CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A novel lane detection based on geometrical model and gabor filter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhua</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqiang</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huiyan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Intelligent Vehicles Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="59" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">NeurVPS: Neural vanishing point scanning via conic convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">End-to-end wireframe parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE ICCV</title>
		<meeting>IEEE ICCV</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
