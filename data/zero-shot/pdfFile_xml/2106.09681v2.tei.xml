<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">XCiT: Cross-Covariance Image Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Facebook</forename><surname>Ai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inria</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sorbonne University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">XCiT: Cross-Covariance Image Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Following tremendous success in natural language processing, transformers have recently shown much promise for computer vision. The self-attention operation underlying transformers yields global interactions between all tokens, i.e. words or image patches, and enables flexible modelling of image data beyond the local interactions of convolutions. This flexibility, however, comes with a quadratic complexity in time and memory, hindering application to long sequences and high-resolution images. We propose a "transposed" version of self-attention that operates across feature channels rather than tokens, where the interactions are based on the cross-covariance matrix between keys and queries. The resulting cross-covariance attention (XCA) has linear complexity in the number of tokens, and allows efficient processing of high-resolution images. Our cross-covariance image transformer (XCiT) -built upon XCA -combines the accuracy of conventional transformers with the scalability of convolutional architectures. We validate the effectiveness and generality of XCiT by reporting excellent results on multiple vision benchmarks, including (self-supervised) image classification on ImageNet-1k, object detection and instance segmentation on COCO, and semantic segmentation on ADE20k.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformers architectures <ref type="bibr" target="#b68">[69]</ref> have provided quantitative and qualitative breakthroughs in speech and natural language processing (NLP). Recently, Dosovitskiy et al. <ref type="bibr" target="#b21">[22]</ref> established transformers as a viable architecture for learning visual representations, reporting competitive results for image classification while relying on large-scale pre-training. Touvron et al. <ref type="bibr" target="#b64">[65]</ref> have shown on par or better accuracy/throughput compared to strong convolutional baselines such as EfficientNets <ref type="bibr" target="#b57">[58]</ref> when training transformers on ImageNet-1k using extensive data augmentation and improved training schemes. Promising results have been obtained for other vision tasks, including image retrieval <ref type="bibr" target="#b22">[23]</ref>, object detection and semantic segmentation <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b82">83]</ref>, as well as video understanding <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>One major drawback of transformers is the time and memory complexity of the core selfattention operation, that increases quadratically with the number of input tokens, or similarly number of patches in computer vision. For w?h images, this translates to a complexity of O(w 2 h 2 ), which is prohibitive for most tasks involving high-resolution images, such as object detection and segmentation. Various strategies have been proposed to alleviate this complexity, for instance using approximate forms of self-attention <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b80">81]</ref>, or pyramidal architectures which progressively downsample the feature maps <ref type="bibr" target="#b70">[71]</ref>. However, none of the existing solutions are fully satisfactory, as they either trade complexity for accuracy, or their complexity remains excessive for processing very large images.</p><p>We replace the self-attention, as originally introduced by Vaswani et al. <ref type="bibr" target="#b68">[69]</ref>, with a "transposed" attention that we denote as "cross-covariance attention" (XCA). Cross-covariance attention substitutes the explicit full pairwise interaction between tokens by self-attention among features, where the attention map is derived from the cross-covariance matrix computed over the key and query projections of the token features. Importantly, XCA has a linear complexity in the number of patches. To construct our Cross-Covariance Image Transformers (XCiT), we combine XCA with local patch interaction modules that rely on efficient depthwise convolutions and point-wise feedforward networks commonly used in transformers, see <ref type="figure" target="#fig_8">Figure 1</ref>. XCA can be regarded as a form of a dynamic 1?1 convolution, which multiplies all tokens with the same data-dependent weight matrix. We find that the performance of our XCA layer can be further improved by applying it on blocks of channels, rather than directly mixing all channels together. This "block-diagonal" shape of XCA further reduces the computational complexity with a factor linear in the number of blocks.           Given its linear complexity in the number of tokens, XCiT can efficiently process images with more than thousand pixels in each dimension. Notably, our experiments show that XCiT does not compromise the accuracy and achieves similar results to DeiT <ref type="bibr" target="#b64">[65]</ref> and CaiT <ref type="bibr" target="#b67">[68]</ref> in comparable settings. Moreover, for dense prediction tasks such as object detection and image segmentation, our models outperform popular ResNet <ref type="bibr" target="#b27">[28]</ref> backbones as well as the recent transformer-based models <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b80">81]</ref>. Finally, we also successfully apply XCiT to the self-supervised feature learning using DINO <ref type="bibr" target="#b11">[12]</ref>, and demonstrate improved performance compared to a DeiT-based backbone <ref type="bibr" target="#b64">[65]</ref>.</p><formula xml:id="formula_0">W + z Y z l 1 J f j O X O f 8 U G K Y x f D k 1 B j A k 6 b z R O I 0 s S 5 R c f B V w x C t H M G o Q q b m d F N C S 5 v v b 3 8 K w I z Y u U L x v 7 m + v N 5 + v N 3 u b a V m</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q I H w f q h Z i T w B N v 3 Z s 3 U v 3 / E t O F K v o d 5 y I Y B m U g + 5 p S A h U a r h X v l E g 4 I T C k R 8 e u k s v</head><formula xml:id="formula_1">O o V 0 W v E A Z 2 A v E 7 N Y a A n C R Y s D F U c D g l E l Q Q Y x 0 J N n h c D 2 E Y P 7 F 7 E j + 1 W 2 / n A I M K 0 Q Y 2 h x p i f z R L E q z 5 Z A r V E s b u G R n M Z f b l e f H b 5 C D u Y O</formula><formula xml:id="formula_2">b i t i J d N Y = " &gt; A A A D U n i c t V J L b x M x E P Y m P E o o k M K R i 0 V S K Z V Q m i 2 v X p C K e k G K V D V</formula><formula xml:id="formula_3">V w q + D v w V a K G V n A b N z 2 S a s S L h K T B J t R 7 7 Z Z t U g W C S 2 w Y p N M 8 p m 9 M Z H z u Y U l d q Y s q V s H j X M V M c Z c q d F H D J / h l h a K L 1 I g m d 5 7 J l v W l b k n + z j Q u I D i d G p H k B P G V V o a i Q G D K 8 3 C 8 8 F Y o z k A s H K F P C 9 Y p Z T B V l</formula><formula xml:id="formula_4">W K + z i U G W 3 + s = " &gt; A A A E B H i c v V N L b 9 N A E H Y T H s W 8 W r j B Z U U T K R F 9 J O V 5 Q W o V K a J q F C X Q R 6 R s G q 3 X m 2 S V</formula><formula xml:id="formula_5">b G r D X U C d G c S M r Q N g C T 6 T m g S q + x X X X d c k t Z V t Q h Q C d o R w L T h O b + V m e n a g s / t Y K b j P l r T a V P i f Z R m 8 G p 0 l N U a T b b N q B F Z k y 3 l Q 7 c s i c U n a 7 5 n I y V t J y 2 g F v m M o w A g Z o y a S x b K 2 / P c v Y a f A + J N H k V / Y W H S y u 1 9 V q 2 0 F W j P j d W n P n q D J d + Y l / R K L C i q C D G 9 O v Z Y I k G T g V L X B w Z F h I 6 J W P W t 6 Y k t s o g z n 7 i B J U t 4 q O R 0 v a R g D L 0 f E Z M A m N</formula><formula xml:id="formula_6">k = " &gt; A A A D m X i c v V J b b 9 M w F E 5 b L i P c O p B 4 2 Y t F U 6 k V 0 C X j K g T S p g q 0 0 W l q G d s q 1 V 3 k u G 5 r N Y k z + 2 R a Z e</formula><formula xml:id="formula_7">E i l D I f k A U C 3 n M T o B D y P q J Z C Q K Q n Y W z N q 5 / e y S S c V F / A P m C R t G Z B L z M a c E D O R v l n 7 W H R w R m F I S 6 r 2 s 0 X n Z a 6 L P C A O 7 A n 0 s x h C R q w y H b A w N n E x J D C L S W K Y h G 7 x 2 E x j q N + b O 9 F t z 9 T r n G E S C t r G 6 k K B H / i z L s O S T K T Q d j O 2 V M p j H x S s I 9 P f s X B 9 h 4 B F T 6 C h b e P b M Z x E y J a A 7 W Z F 4 G w N J n b 9 g b w E 6 q 3 l 9 v W i 8 3 8 7 + F 5 X r J d d o m b g l s Z F / k Z l W q R R K v a L i k k h O Y s p Q o 9 9 u I g L A 4 v x v f L T t + q E w a V G X A J 2 i g x i Y J D Q 3 o c Z h 9 6 B p O y 8 c v 1 p z W 2 4 h 6 K b i L Z W a t Z S u X / 2 F R 4 K m k a l B Q 6 L U w C v Y E g m c h i y z c a p Y Q u i M T N j A q D E x D Q 9 1 s V k Z q h t k h M Z C m h M D K t D r E Z p E S s 2 j w H j m x N W 6 L Q f / Z R u k M P 4 w 1 D x O U k O e L g q N 0 x C B Q P m a o h G X j E I 4 N w q h k p t e E Z 2 S f B p m m W 0 z B G + d 8 k 3 l d K f l v W t 5 v Z</formula><formula xml:id="formula_8">h / 5 h Y q p 2 V t Z q m 7 X s o O t C f S 6 s W f P T d V Y + Y 0 / S y D e N o Y J o P a h n 0 y E K O B U s s X G k W U j o l I z Z w I g B M a G G c b Y J C S o b x E M j q c w X A M r Q i x 4 x 8 b W e + a 6 x T B P W V 3 U p e J N u E M H o z T D O 2 s c C m g c a R c K 0 E q V r h T y u G A U x M w K h i p t c E Z 2 Q d H p m + W z T h P r V k q 8 L h 9 u b 9 V e b 9 d 7 2 2 s 7 6 v B 1 L 1 h P r q V W x 6 t Z r a 8 d 6 Z 3 W t A 4 s W P h a + F r 4 X f h Q / F b 8 V f x Z / 5 a a F h b n P Y + v S W S z + B h 0 u Z P w = &lt; / l a t e x i t &gt; K 2 R N ?d k , Q 2 R N ?d q &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F o A H g 4 H M S R c R a y f j k 7 h i / q D 5 h A w = " &gt; A A A D w H i c v V J b a 9 s w F H a T X T r v 0 n Z 7 3 I t Y U 0 j Y 2 i b d l U G h W y C s t I R k W 9 t A l I Z j W a l F Z M u R j t t</formula><formula xml:id="formula_9">v 4 Z B z w E a c E D D R c y 3 0 r O t g n M K F E x H t J 6 W C r W 0 a f E A Z 2 C f G h H I F P L h M s 2 A h K O J y Q A K Q f Y x U J 1 n 9 T C 2 E Q v z V 7 E r 8 z W / f g F I M M 0 T b W Z w p i b z h N E q z 4 e A J l B 2 P 7 W h r M g / m f 6 8 Z f k t O 4 j Y H 7 T K N 2 k j G 7 5 s h c J g T i g 2 Q e e B s D i Z w / Y D c D n e t x h 3 F W e K + R / K 9 W r q a 8 0 Z b x W z T m D c 8 S x 2 4 o q X W l I c + J 4 i S g D O 0 B s C C d A y r 1 G n t l u 3 j I x K h C / q I n R F + Q g C M G i I h q 2 b a L L W n S o g 4 B O k H 7 A T B F a E Z t d f Z N A O e 1 U a T J m F d p S n V B l I f a D C 6 k m q J S s 9 k 2 h B a Z M d W W y r e L r p B 0 W v E 4 G c v A x D Q V 2 E U e h B E g k F M W a B O t l d V v Y v Y a / A i J 1 H k L X Y G d A 2 S a R n c N M 1 X N M e Q u + g f F y D J c 3 a x V a / O F b h v 1 h b F p L V Z n u P o d e 5 J G v h G J C q J 1 v z 4 f H 1 H A q W C J j S P N Q k K n Z M</formula><p>Overall, we summarize our contributions as follows:</p><p>? We introduce cross-covariance attention (XCA), which provides a "transposed" alternative to conventional self-attention, attending over channels instead of tokens. Its complexity is linear in the number of tokens, allowing for efficient processing of high-resolution images, see <ref type="figure" target="#fig_13">Figure 2</ref>.</p><p>? XCA attends to a fixed number of channels, irrespective of the number of tokens. As a result, our models are significantly more robust to changes in image resolution at test time, and are therefore more amenable to process variable-size images.</p><p>? For image classification, we demonstrate that our models are on par with state-of-theart vision transformers for multiple model sizes using a simple columnar architecture, i.e., in which we keep the resolution constant across layers. In particular, our XCiT-L24 model achieves 86.0% top-1 accuracy on ImageNet, outperforming its CaiT-M24 <ref type="bibr" target="#b67">[68]</ref> and NFNet-F2 <ref type="bibr" target="#b9">[10]</ref> counterparts with comparable numbers of parameters.</p><p>? For dense prediction tasks with high-resolution images, our models outperform ResNet and multiple transformer-based backbones. On the COCO benchmark, we achieve a strong performance of 48.5% and 43.7% mAP for object detection and instance segmentation respectively. Moreover, we report 48.4% mIoU for semantic segmentation on the ADE20k benchmark, outperforming the state-of-the-art Swin Transformer <ref type="bibr" target="#b43">[44]</ref> backbones across all comparable model sizes.</p><p>? Finally, our XCiT model is highly effective in self-supervised learning setups, achieving 80.9% top-1 accuracy on ImageNet-1k using DINO <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Deep vision transformers. Training deep vision transformers can be challenging due to instabilities and optimization issues. Touvron et al. <ref type="bibr" target="#b67">[68]</ref> successfully train models with up to 48 layers using LayerScale, which weighs contributions of residual blocks across layers and improves optimization. Additionally, the authors introduce class attention layers which decouple the learning of patch features and the feature aggregation stage for classification.</p><p>Spatial structure in vision transformers. Yuan et al. <ref type="bibr" target="#b78">[79]</ref> propose applying a soft split for patch projection with overlapping patches which is applied repeatedly across model layers, reducing the number of patches progressively. Han et al. <ref type="bibr" target="#b26">[27]</ref> introduce a transformer module for intra-patch structure, exploiting pixel-level information and integrating with an interpatch transformer to attain higher representation power. d'Ascoli et al. <ref type="bibr" target="#b18">[19]</ref> consider the initialization of self-attention blocks as a convolutional operator, and demonstrate that such initialization improves the performance of vision transformers in low-data regimes. Graham et al. <ref type="bibr" target="#b25">[26]</ref> introduce LeViT, which adopts a multi-stage architecture with progressively reduced feature resolution similar to popular convolutional architectures, allowing for models with high inference speed while retaining a strong performance. Moreover, the authors adopt a convolution-based module for extracting patch descriptors. Yuan et al. <ref type="bibr" target="#b77">[78]</ref> improve both the performance and the convergence speed of vision transformers by replacing the linear patch projection with convolutional layers and max-pooling, as well as modifying the feed-forward networks in each transformer layer to incorporate depth-wise convolutions.</p><p>Efficient attention. Numerous methods for efficient self-attention have been proposed in the literature to address the quadratic complexity of self-attention in the number of input tokens. These include restricting the span of the self-attention to local windows <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b49">50]</ref>, strided patterns <ref type="bibr" target="#b13">[14]</ref>, axial patterns <ref type="bibr" target="#b29">[30]</ref>, or an adaptive computation across layers <ref type="bibr" target="#b56">[57]</ref>. Other methods provide an approximation of the self-attention matrix which can be achieved by a projection across the token dimension <ref type="bibr" target="#b69">[70]</ref>, or through a factorization of the softmax-attention kernel <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b76">77]</ref>, which avoids explicit computation of the attention matrix. While conceptually different, our XCA performs similar computations without being sensitive to the choice of the kernel. Similarly, Lee-Thorp et al. <ref type="bibr" target="#b40">[41]</ref> achieve faster training by substituting self-attention with unparametrized Fourier Transform. Other efficient attention methods rely on local attention and adding a small number of global tokens, thus allowing interaction among all tokens only by hopping through the global tokens <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b79">80]</ref>. Data-dependent layers. Our XCiT layer can be regarded as a "dynamic" 1?1 convolution, which multiplies all token features with the same data-dependent weight matrix, derived from the key and query cross-covariance matrix. In the context of convolutional networks, Dynamic Filter Networks <ref type="bibr" target="#b8">[9]</ref> explore a related idea, using a filter generating subnetwork to produce convolutional filters based on features in previous layers. Squeeze-and-Excitation networks <ref type="bibr" target="#b31">[32]</ref> use data dependent 1?1 convolutions in convolutional architectures. Spatially average-pooled features are fed to a 2-layer MLP which produces per channel scaling parameters. Closer in spirit to our work, Lambda layers propose a way to ensure global interaction in ResNet models <ref type="bibr" target="#b3">[4]</ref>. Their "content-based lambda function" is computing a similar term as our cross-covariance attention, but differing in how the softmax and 2 normalizations are applied. Moreover, Lambda layers also include specific position-based lambda functions, and LambdaNetworks are based on ResNets while XCiT follows the ViT architecture. Recently data-independent analogues of self-attention have also been found to be an effective alternative to convolutional and self-attention layers for vision tasks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b66">67]</ref>. These methods treat entries in the attention map as learnable parameters, rather than deriving the attention map dynamically from queries and keys, but their complexity remains quadratic in the number of tokens. Zhao et al. <ref type="bibr" target="#b81">[82]</ref> consider alternative attention forms in computer vision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we first recall the self-attention mechanism, and the connection between the Gram and covariance matrices, which motivated our work. We then propose our crosscovariance attention operation (XCA) -which operates along the feature dimension instead of token dimension in conventional transformers -and combine it with local patch interaction and feedforward layers to construct our Cross-Covariance Image Transformer (XCiT). See <ref type="figure" target="#fig_8">Figure 1</ref> for an overview.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head><p>Token self-attention. Self-attention, as introduced by Vaswani et al. <ref type="bibr" target="#b68">[69]</ref>, operates on an input matrix X ? R N ?d , where N is the number of tokens, each of dimensionality d. The input DeiT-S Cait-S12 ViL (Longformer) ViL (Performer) Swin-T PVT Small ResNet50 XCiT-S12/16 XCiT-S12/8  DeiT-S ResNet-50 XCiT-S12/16 XCiT-S12/8 <ref type="figure">Figure 3</ref>: Performance when changing the resolution at test-time for models with a similar number of parameters. All networks were trained at resolution 224, w/o distillation. XCiT is more tolerant to changes of resolution than the Gram-based DeiT and benefit more from the "FixRes" effect <ref type="bibr" target="#b63">[64]</ref> when inference is performed at a larger resolution than at train-time.</p><p>X is linearly projected to queries, keys and values, using the weight matrices</p><formula xml:id="formula_10">W q ? R d?dq , W k ? R d?d k and W v ? R d?dv , such that Q=XW q , K=XW k and V =XW v , where d q = d k .</formula><p>Keys and values are used to compute an attention map A(K, Q) = Softmax(QK / ? d k ), and the output of the self-attention operation is defined as the weighted sum of N token features in V with the weights corresponding to the attention map: Attention(Q, K, V ) = A(K, Q)V . The computational complexity of self-attention scales quadratically in N , due to pairwise interactions between all N elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relationship between Gram and covariance matrices.</head><p>To motivate our cross-covariance attention operation, we recall the relation between Gram and covariance matrices. The unnormalised d ? d covariance matrix is obtained as C=X X. The N ?N Gram matrix contains all pairwise innerproducts: G=XX . The non-zero part of the eigenspectrum of the Gram and covariance matrix are equivalent, and the eigenvectors of C and G can be computed in terms of each other. If V are the eigenvectors of G, then the eigenvectors of C are given by U =XV . To minimise the computational cost, the eigendecomposition of either the Gram or covariance matrix can be obtained in terms of the decomposition of the other, depending on which of the two matrices is the smallest. <ref type="bibr" target="#b0">1</ref> We draw upon this strong connection between the Gram and covariance matrices to consider if it is possible to avoid the quadratic cost to compute the N ?N attention matrix, which is computed from the analogue of the N ?N Gram matrix QK =XW q W k X . Below we consider how we can use the d k ?d q cross-covariance matrix, K Q=W k X XW q , which can be computed in linear time in the number of elements N , to define an attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Cross-covariance attention</head><p>We propose a cross-covariance based self-attention function that operates along the feature dimension, rather than along the token dimension as in token self-attention. Using the definitions of queries, keys and values from above, the cross-covariance attention function is defined as:</p><formula xml:id="formula_11">XC-Attention(Q, K, V ) = V A XC (K, Q), A XC (K, Q) = Softmax K Q /? ,<label>(1)</label></formula><p>where each output token embedding is a convex combination of the d v features of its corresponding token embedding in V . The attention weights A are computed based on the cross-covariance matrix. <ref type="table" target="#tab_6">Table 1</ref>: XCiT models. Design choices include model depth, patch embeddings dimensionality d, and the number of heads h used in XCA. By default our models are trained and tested at resolution 224 with patch sizes of 16?16. We also train with distillation using a convolutional teacher (denoted ?) as proposed by Touvron et al. <ref type="bibr" target="#b64">[65]</ref>. Finally, we report performance of our strongest models obtained with 8?8 patch size, fine-tuned (?) and tested at resolution 384?384 (column @384/8), using distillation with a teacher that was also fine-tuned @384. power of the operation by removing a degree of freedom. Therefore, we introduce a learnable temperature parameter ? which scales the inner products before the Softmax, allowing for sharper or more uniform distribution of attention weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Block-diagonal cross-covariance attention.</head><p>Instead of allowing all features to interact among each other, we divide them into a h groups, or "heads", in a similar fashion as multi-head token self-attention. We apply the cross-covariance attention separately per head where for each head, we learn separate weight matrices to project X to queries, keys and values, and collect the corresponding weight matrices in the tensors W q ? R h?d?dq , W k ? R h?d?d k and W v ? R h?d?dv , where we set d k =d q =d v =d/h. Restricting the attention within heads has two advantages: (i) the complexity of aggregating the values with the attention weights is reduced by a factor h; (ii) more importantly, we empirically observe that the block-diagonal version is easier to optimize, and typically leads to improved results. This observation is in line with observations made for Group Normalization <ref type="bibr" target="#b72">[73]</ref>, which normalizes groups of channels separately based on their statistics, and achieves favorable results for computer vision tasks compared to Layer Normalization <ref type="bibr" target="#b2">[3]</ref>, which combines all channels in a single group.  <ref type="figure">h+N d)</ref>. Therefore, our model scales much better to cases where the number of tokens N is large, and the feature dimension d is relatively small, as is typically the case, in particularly when splitting the features into h heads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Cross-covariance image transformers</head><p>To construct our cross-covariance image transformers (XCiT), we adopt a columnar architecture which maintains the same spatial resolution across layers, similarly to <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b67">68]</ref>. We combine our cross-covariance attention (XCA) block with the following additional modules, each one being preceded by a LayerNorm <ref type="bibr" target="#b2">[3]</ref>. See <ref type="figure" target="#fig_8">Figure 1</ref> for an overview. Since in this section we specifically design the model for computer vision tasks, tokens correspond to image patches in this context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Local patch interaction.</head><p>In the XCA block communication between patches is only implicit through the shared statistics. To enable explicit communication across patches we add a simple Local Patch Interaction (LPI) block after each XCA block. LPI consists of two depth-wise 3?3 convolutional layers with Batch Normalization and GELU non-linearity in between. Due to its depth-wise structure, the LPI block has a negligible overhead in terms of parameters, as well as a very limited overhead in terms of throughput and memory usage during inference.</p><p>Feed-forward network. As is common in transformer models, we add a point-wise feedforward network (FFN), which has a single hidden layer with 4d hidden units. While interaction between features is confined within groups in the XCA block, and no feature interaction takes place in the LPI block, the FFN allows for interaction across all features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global aggregation with class attention.</head><p>When training our models for image classification, we utilize the class attention layers as proposed by Touvron et al. <ref type="bibr" target="#b67">[68]</ref>. These layers aggregate the patch embeddings of the last XCiT layer through writing to a CLS token by one-way attention between the CLS tokens and the patch embeddings. The class attention is also applied per head, i.e. feature group.  Handling images of varying resolution. In contrast to the attention map involved in token self-attention, in our case the covariance blocks are of fixed size independent of the input image resolution. The softmax always operates over the same number of elements, which may explain why our models behave better when dealing with images of varying resolutions (see <ref type="figure">Figure 3</ref>). In XCiT we include additive sinusoidal positional encoding <ref type="bibr" target="#b68">[69]</ref> with the input tokens. We generate them in 64 dimensions from the 2d patch coordinates and then linearly project to the transformer working dimension d. This choice is orthogonal to the use of learned positional encoding, as in ViT <ref type="bibr" target="#b21">[22]</ref>. However, it is more flexible since there is no need to interpolate or fine-tune the network when changing the image size. <ref type="table" target="#tab_6">Table 1</ref> we list different variants of our model which we use in our experiments, with different choices for model width and depth. For the patch encoding layer, unless mentioned otherwise, we adopt the alternative used by Graham et al. <ref type="bibr" target="#b25">[26]</ref> with convolutional patch projection layers. We also experimented with a linear patch projection as described in <ref type="bibr" target="#b21">[22]</ref>, see our ablation in <ref type="table" target="#tab_6">Table 4</ref>. Our default patch size is 16?16, as in other vision transformer models including ViT <ref type="bibr" target="#b21">[22]</ref>, DeiT <ref type="bibr" target="#b64">[65]</ref> and CaiT <ref type="bibr" target="#b67">[68]</ref>. We also experiment with smaller 8?8 patches, which has been observed to improve performance <ref type="bibr" target="#b11">[12]</ref>. Note that this is efficient with XCiT as its complexity scales linearly which the number of patches, while ViT, DeiT and CaiT scale quadratically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model configurations. In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental evaluation</head><p>In this section we demonstrate the effectiveness and versatility of XCiT on multiple computer vision benchmarks, and present ablations providing insight on the importance of its different components. In the supplementary material we provide additional analysis, including the impact on performance of image resolution in Section A.1 and of multiple approximate attention baselines in Section A.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Image classification</head><p>We use ImageNet-1k <ref type="bibr" target="#b19">[20]</ref> to train and evaluate our models for image classification. It consists of 1.28M training images and 50k validation images, labeled across 1,000 semantic categories. Our training setup follows the DeiT recipe <ref type="bibr" target="#b64">[65]</ref>. We train our model for 400 epochs with the AdamW optimizer <ref type="bibr" target="#b44">[45]</ref> using a cosine learning rate decay. In order to enhance the training of larger models, we utilize LayerScale <ref type="bibr" target="#b67">[68]</ref> and adjust the stochastic depth <ref type="bibr" target="#b32">[33]</ref> for each of our  <ref type="table" target="#tab_6">Table 4</ref>: Ablations of various architectural design choices on the task of ImageNet-1k classification using the XCiT-S12 model. Our baseline model uses the convolutional projection adopted from LeVit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Ablation</head><p>ImNet top-1 acc.</p><p>XCiT-S12/16 Baseline 82.0 XCiT-S12/8 <ref type="bibr">83.4</ref> XCiT-S12/16 Linear patch proj. 81.1 XCiT-S12/8 83.1</p><p>XCiT-S12/16 w/o LPI layer 80.8 w/o XCA layer 75.9</p><p>XCiT-S12/16 w/o 2 -normal. failed w/o learned temp. ? 81.8 models accordingly (see the supplementary material for details). Following <ref type="bibr" target="#b67">[68]</ref>, images are cropped with crop ratio of 1.0 for evaluation. In addition to the ImageNet-1k validation set, we report results for ImageNet-V2 <ref type="bibr" target="#b54">[55]</ref> which has a distinct test set. Our implementation is based on the Timm library <ref type="bibr" target="#b71">[72]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on ImageNet.</head><p>We present a family of seven models in <ref type="table" target="#tab_6">Table 1</ref> with different operating points in terms of parameters and FLOPs. We observe that the performance of the XCiT models benefits from increased capacity both in depth and width. Additionally, consistent with <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b67">68]</ref> we find that using hard distillation with a convolutional teacher improves the performance. Because of its linear complexity in the number of tokens, it is feasible to train XCiT at 384?384 resolution with small 8?8 patches, i.e. 2304 tokens, which provides a strong boost in performance across all configurations. We compare to the state-of-the-art convolutional and transformer-based architectures <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b67">68]</ref> in <ref type="table" target="#tab_1">Table 2</ref>. By varying the input image resolution and/or patch size, our models provide competitive or superior performance across model sizes and FLOP budgets. First, the models operating on 224?224 and 16?16 (e.g. XCiT-S12/16) enjoy high accuracy at relatively few FLOPs compared to their counterparts with comparable parameter count and FLOPs. Second, our models with 16 ? 16 and 384 ? 384 resolution images (e.g. XCiT-S12/16?) yield an improved accuracy at the expense of higher FLOPs, and provide superior or on-par performance compared to state-of-the-art models with comparable computational requirements. Finally, XCiT linear complexity allows us to scale to process 384?384 images with 8 ? 8 patch sizes (e.g. XCiT-S12/8?), achieving the highest accuracy across the board, albeit at a relatively high FLOPs count. <ref type="figure" target="#fig_16">Figure 4</ref> we show the class attention map obtained in the feature aggregation stage. Each head focuses on different semantically coherent regions in the image (e.g. faces or umbrellas). Furthermore, heads tend to focus on similar patterns across images (e.g. bird head or human face), but adapts by focusing on other salient regions when such patterns are absent. <ref type="figure">Figure 3</ref> we report the accuracy of XCiT-S12, DeiT-S and ResNet-50 trained on 224?224 images and evaluated at different image resolutions. While DeiT outperforms ResNet-50 when train and test resolutions are similar, it suffers from a larger drop in performance as the image resolution deviates farther from the training resolution. XCiT displays a substantially increased accuracy when train and test resolutions are similar, while also being robust to resolution changes, in particular for the model with 8?8 patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Class attention visualization. In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness to resolution changes. In</head><p>Self-supervised learning. We train XCiT in a self-supervised manner using DINO <ref type="bibr" target="#b11">[12]</ref> on ImageNet-1k. In <ref type="table" target="#tab_2">Table 3</ref> we report performance using the linear and k-NN protocols as in <ref type="bibr" target="#b11">[12]</ref>. Across model sizes XCiT obtains excellent accuracy with both protocols, substantially improving DINO with ResNet-50 or ViT architectures, as well as over those reported for Swin-Transformer trained with MoBY <ref type="bibr" target="#b75">[76]</ref>. Comparing the larger models to ViT, we also observed improved performance for XCiT achieving a strong 80.3% accuracy. For fair comparison, all reported models have been trained for 300 epochs. Further improved performance of small models is reported by Caron et al. <ref type="bibr" target="#b11">[12]</ref> when training for 800 epochs, which we expect to carryover to XCiT based on the results presented here. <ref type="table" target="#tab_6">Table 4</ref> we provide ablation experiments to analyse the impact of different design choices for our XCiT-S12 model. First, we observe the positive effect of using the convolutional patch projection as compared to using linear patch projection, for both 8?8 and 16?16 patches. Second, while removing the LPI layer reduces the accuracy by only 1.2% (from 82.0 to 80.8), removing the XCA layer results in a large drop of 6.1%, underlining the effectiveness of XCA. We noticed that the inclusion of two convolutional componentsconvolutional patch projection and LPI -not only brings improvements in accuracy, but also accelerates training. Third, although we were able to ensure proper convergence without <ref type="table" target="#tab_6">Table 5</ref>: COCO object detection and instance segmentation performance on the mini-val set. All backbones are pretrained on ImageNet-1k, use Mask R-CNN model <ref type="bibr" target="#b28">[29]</ref> and are trained with the same 3x schedule.  <ref type="table" target="#tab_6">Table 6</ref>: ADE20k semantic segmentation performance using Semantic FPN <ref type="bibr" target="#b37">[38]</ref> and UperNet <ref type="bibr" target="#b73">[74]</ref> (in comparable settings). We do not include comparisons with other state-of-the-art models that are pre-trained on larger datasets <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b82">83]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis and ablations. In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Object detection and instance segmentation</head><p>Our XCiT models can efficiently process high-resolution images (see <ref type="figure" target="#fig_13">Figure 2</ref>). Additionally, XCiT has a better adaptability to varying image resolutions compared to ViT models (see <ref type="figure">Figure 3</ref>). These two properties make XCiT a good fit for dense prediction tasks including detection and segmentation.</p><p>We evalutate XCiT for object detection and instance segmentation using the COCO benchmark <ref type="bibr" target="#b41">[42]</ref> which consists of 118k training and 5k validation images including bounding boxes and mask labels for 80 categories. We integrate XCiT as backbone in the Mask R-CNN <ref type="bibr" target="#b28">[29]</ref> detector with FPN <ref type="bibr" target="#b42">[43]</ref>. Since the XCiT architecture is inherently columnar, we make it FPNcompatible by extracting features from different layers (e.g., <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12]</ref> for XCiT-S12). All features have a constant stride of 8 or 16 based on the patch size, and the feature resolutions are adjusted to have strides of <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32]</ref>, similar to ResNet-FPN backbones, where the downsampling is achieved by max pooling and the upsampling is obtained using a single transposed convolution layer (see suppl. mat. for details). The model is trained for 36 epochs (3x schedule) using the AdamW optimizer with learning rate of 10 ?4 , 0.05 weight decay and 16 batch size. We adopt the multiscale training and augmentation strategy of DETR <ref type="bibr" target="#b10">[11]</ref>. Our implementation is based on the mmdetection library <ref type="bibr" target="#b12">[13]</ref>. <ref type="table" target="#tab_6">Table 5</ref> we report object detection and instance segmentation results of four variants of XCiT using 16?16 and 8?8 patches. We compare to ResNets <ref type="bibr" target="#b27">[28]</ref> and concurrent efficient vision transformers <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b80">81]</ref>. All models are trained using the 3x schedule after ImageNet-1k pre-training. Note that other results with higher absolute numbers have been achieved when pre-training on larger datasets <ref type="bibr" target="#b43">[44]</ref> or with longer schedules <ref type="bibr" target="#b3">[4]</ref>, and are therefore not directly comparable to the reported results. First, across all model sizes XCiT outperforms the convolutional ResNet <ref type="bibr" target="#b27">[28]</ref> and ResNeXt <ref type="bibr" target="#b74">[75]</ref> by a large margin with either patch size. Second, we observe a similar increase in accuracy compared to PVT <ref type="bibr" target="#b70">[71]</ref> and ViL <ref type="bibr" target="#b80">[81]</ref> backbones. Finally, XCiT provides a competitive performance with Swin [44] 2 . For relatively small models, XCiT-S12/8 outperforms its Swin-T counterpart with a decent margin. On the other hand, Swin-S provides slightly stronger results compared to XCiT-S24/8. Utilizing smaller 8?8 patches leads to a consistent gain across all models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results on COCO. In</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Semantic segmentation</head><p>We further show transferability of our models with semantic segmentation experiments on the ADE20k dataset <ref type="bibr" target="#b83">[84]</ref>, which consists of 20k training and 5k validation images with labels over 150 semantic categories. We integrate our backbones in two segmentation methods:</p><p>Semantic FPN <ref type="bibr" target="#b37">[38]</ref> and UperNet <ref type="bibr" target="#b73">[74]</ref>. We train for 80k and 160k iterations for Semantic FPN and UperNet respectively. Following <ref type="bibr" target="#b43">[44]</ref>, the models are trained using batch size 16 and an AdamW optimizer with learning rate of 6 ? 10 ?5 and 0.01 weight decay. We apply the same method of extracting FPN features as explained in Section 4.2. We report the performance using the standard single scale protocol (without multi-scale and flipping). Our implementation is based on the mmsegmentation library <ref type="bibr" target="#b16">[17]</ref>.</p><p>Results on ADE20k. We present the semantic segmentation performance using XCiT backbones in <ref type="table" target="#tab_6">Table 6</ref>. First, for Semantic FPN <ref type="bibr" target="#b37">[38]</ref>, XCiT provides a superior performance compared to ResNet, ResNeXt and PVT backbones using either option of patch size. Second, compared to Swin Transformers using the same UperNet decoder <ref type="bibr" target="#b73">[74]</ref>, XCiT with 8?8 patches consistently achieves a higher mIoU for different models. XCiT with 16?16 patches provides a strong performance especially for smaller models where XCiT-S12/16 outperforms Swin-T.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Contributions. We present an alternative to token self-attention which operates on the feature dimension, eliminating the need for expensive computation of quadratic attention maps. We build our XCiT models with the cross-covariance attention as its core component and demonstrate the effectiveness and generality of our models on various computer vision tasks. In particular, it exhibits a strong image classification performance on par with state-ofthe-art transformer models while similarly robust to changing image resolutions as convnets.</p><p>XCiT is effective as a backbone for dense prediction tasks, providing excellent performance on object detection, instance and semantic segmentation. Finally, we showed that XCiT can be a strong backbone for self-supervised learning, matching the state-of-the-art results with less compute. XCiT is a generic architecture that can readily be deployed in other research domains where self-attention has shown success.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XCiT: Cross-Covariance Image Transformers Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Preliminary study on Vision Transformers (ViT)</head><p>In this appendix we report the results associated with our preliminary study on highresolution transformers. Most of the experiments were carried out on the ViT architecture <ref type="bibr" target="#b21">[22]</ref> with DeiT training <ref type="bibr" target="#b64">[65]</ref>, and intended to analyze different aspects of transformers when considering images with varying resolution or high-resolution images specifically. <ref type="bibr" target="#b76">77</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Impact of resolution versus patch size</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Approximate attention models in ViT with DeiT training</head><p>In <ref type="table" target="#tab_6">Table A</ref>.1, we report the results that we obtain by replacing the Multi-headed Self-attention operation with efficient variants <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b70">71]</ref> in the DeiT-S backbone. First, we can notice that for all efficient self-attention choices there is a clear drop in performance compared to the Deit-S baseline. The spatial reduction attention (SRA) proposed in PVT <ref type="bibr" target="#b70">[71]</ref> has a significantly weaker performance compared to the full-attention with a quadratic complexity that is more efficient than full-attention by only a constant factor R 2 . Linformer <ref type="bibr" target="#b69">[70]</ref> provides a better accuracy compared to SRA, however, it is also clearly weaker than full-attention. Moreover, Linformer does not have the flexibility of processing variable length sequences which limits its application in many computer vision tasks. Efficient attention <ref type="bibr" target="#b55">[56]</ref> provides a better trade-off than the aforementioned methods, with improved accuracy and linear complexity. However, it has a 3.6% drop in performance compared to full-attention. Finally, axial attention <ref type="bibr" target="#b29">[30]</ref> provides the strongest performance among the efficient attention variants we studied with a 1.5% drop in accuracy compared to the baseline. We observe a saving in memory usage, but a drop in speed due to the separate row and column attention operations. Our observations are consistent with <ref type="bibr" target="#b21">[22]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Training and testing with varying resolution</head><p>As discussed in the main manuscript, for several tasks it is important that the network is able to handle images of varying resolutions. This is the case, for instance, for image segmentation, image detection, or image retrieval where the object of interest may have very different sizes.</p><p>We present an analysis of train/test resolution trade-off in <ref type="table" target="#tab_6">Table A.</ref>2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional details of training and our architecture B.1 Sinusoidal Positional Encoding</head><p>We adopt a sinusoidal positional encoding as proposed by Vaswani et al. <ref type="bibr" target="#b68">[69]</ref> and adapted to the 2D case by Carion et al. <ref type="bibr" target="#b10">[11]</ref>. However we depart from this method in that we first produce this encoding in an intermediate 64-d space before projecting it to the working space of the transformers. More precisely, in our implementation each of the x and y coordinates is encoded using 32 dimensions corresponding to cosine and sine functions with different frequencies (16 frequency for each function). The encoding of both coordinates are eventually concatenated to obtain a 64 dimension 2D positional encoding. Finally, the 64 dimension positional encoding is linearly projected to the working dimension of the model d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Obtaining Feature Pyramid for Dense Prediction</head><p>For state-of-the-art detection and segmentation models, FPN is an important component which provides features of multiple scales. We adapt XCiT to be compatible with FPN detection and segmentation methods through a simple re-scaling of the features extracted from different layers. In particular, for models with 12 layers, we extract features from the 4 th , 6 th , 8 th and 12 th layers respectively. As for models with 24 layers, we extract features from 8 th , 12 th , 16 th and 24 th layers. Concerning the re-scaling of the features, the 4 feature levels are downsized by a ratio of 4, 8, 16 and 32 compared to the input image size. Feature downsizing is performed with max pooling and upsampling is achieved using a single layer of transposed convolutions with kernel size k = 2 and stride s = 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Hyper-parameters: LayerScale initialization and Stochastic Depth droprate</head><p>We list the stochastic depth d r and LayerScale initialization hyperparameters used by each of our models in <ref type="table" target="#tab_6">Table B</ref>.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 More XCiT models</head><p>We present additional results for our XCiT models in <ref type="table" target="#tab_6">Table D.</ref>1. We include performance of 384?384 images using a 16?16 patch size as well as results for images with 224?224 resolution using patch size of 8?8. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Transfer Learning</head><p>In order to further demonstrate the flexibility and generality of our models, we report transfer learning experiments in <ref type="table" target="#tab_6">Table D</ref>.2 for models that have been pre-trained using ImageNet-1k and finetuned for other datasets including CIFAR-10, CIFAR-100 <ref type="bibr" target="#b39">[40]</ref>, Flowers-102 <ref type="bibr" target="#b46">[47]</ref>, Stanford Cars <ref type="bibr" target="#b38">[39]</ref> and iNaturalist <ref type="bibr" target="#b30">[31]</ref>. We observe that the XCiT models provide competitive performance when compared to strong baselines like ViT-B, ViT-L, DeiT-B and EfficientNet-B7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Image Retrieval</head><p>Context of this study. Vision-based retrieval tasks such as landmark or particular object retrieval have been dominated in the last years by methods extracting features from highresolution images. Traditionally, the image description was obtained as the aggregation of local descriptors, like in VLAD <ref type="bibr" target="#b35">[36]</ref>. Most of the modern methods now rely on convolutional III neural networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b60">61]</ref>. In a recent paper, El-Nouby et al. <ref type="bibr" target="#b22">[23]</ref> show promising results with vision transformers, however they also underline the inherent scalability limitation associated with the fact that ViT models do not scale well with image resolution. Therefore, it cannot compete with convolutional neural networks whose performance readily improve with higher resolution images. Our XCiT models do not suffer from this limitation: our models scale linearly with the number of pixels, like convnets, and therefore makes it possible to use off-the-shelf methods initially developed for retrieval with high-resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3.1 Datasets and evaluation measure</head><p>In each benchmark, a set of query images is searched in a database of images and the performance is measured as the mean average precision. The Holidays <ref type="bibr" target="#b34">[35]</ref> dataset contains images of 500 different objects or scenes. We use the version of the dataset where the orientation of images (portrait or landscape) has been corrected. Oxford <ref type="bibr" target="#b48">[49]</ref> is a dataset of building images, which corresponds to famous landmark in Oxford. A similar dataset has been produced for famous monuments in Paris and referred to as Paris6k <ref type="bibr" target="#b15">[16]</ref>. We use the revisited version of the Oxford benchmark <ref type="bibr" target="#b50">[51]</ref>, which breaks down the evaluation into easy, medium and hard categories. We report results on the "medium" and "hard" settings, as we observed that the ordering of techniques does not change under the easy measures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3.2 Image representation: global and local description with XCiT</head><p>We consider three existing methods to extract an image vector representations from the pretrained XCiT models. Note that to the best of our knowledge, for the first time we extract local features from the output layer of a transformer layer, and treat them as patches fed to traditional state-of-the-art methods based on matching local descriptors or CNN. CLS token. Similar to El-Nouby et al. <ref type="bibr" target="#b22">[23]</ref> with ViT, we use the final vector as the image descriptor. In this context, the introduction of class-attention layers can be regarded as a way to learn the aggregation method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VLAD.</head><p>We treat the patches before the class-attention layers as individual local descriptors, and aggregate them into a higher-dimensional vector by employing the Vector of locally aggregated Descriptors <ref type="bibr" target="#b35">[36]</ref>.</p><p>AMSK. We also apply the aggregated selective match kernel from Tolias et al. <ref type="bibr" target="#b59">[60]</ref>. This method was originally introduced for local descriptors, but got adapted to convolutional networks. To the best of our knowledge this is the state of the art on several benchmarks <ref type="bibr" target="#b61">[62]</ref>.</p><p>For all these methods, we use the models presented in our main paper, starting from the version fine-tuned at resolution 384?384. By default the resolution is 768. This is comparable to the choice adopted in the literature for ResNet (e.g., 800 in the work by Berman et al. <ref type="bibr" target="#b5">[6]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3.3 Experimental setting: Image retrieval with models pretrained on Imagenet1k only</head><p>We only consider models pre-trained on Imagenet-1k. Note that the literature reports significant improvement when learning or fine-tuning networks <ref type="bibr" target="#b51">[52,</ref><ref type="bibr" target="#b61">62]</ref> on specialized datasets (e.g., of buildings for Oxford5k and Paris6k). We consider only XCiT-S12 models, since they have a number of parameters comparable to that of ResNet-50. We report the results in <ref type="table" target="#tab_6">Table D</ref>.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scaling resolution.</head><p>As expected increasing the resolution with XCiT improves the performance steadily up to resolution 768. This shows that our models are very tolerant to resolution changes considering that they have been fine-tuned at resolution 384. The performance starts to saturates at resolution 1024, which led us to keep 784 as the pivot resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-supervision.</head><p>The networks XCiT pre-trained with self-supervision achieve a comparatively better performance than their supervised counterpart on Holidays, however, we have the opposite observation for ROxford. <ref type="table" target="#tab_6">Table D</ref>.4: Instance retrieval experiments. The default resolution is 768. The default class token size is 128 dimensions. The "local descriptor" representation extracted from the activations is in 128 dimensions. To our knowledge the state of the art with ResNet-50 on Holidays with Imagenet pre-training only is the Multigrain method <ref type="bibr" target="#b5">[6]</ref>, which achieves mAP=92.5%. Here we compare against this method under the same training setting, i.e., off-the-shelf network pre-trained on Imagenet1k only and with the same training procedure and resolution. We refer the reader to Tolias et al. <ref type="bibr" target="#b61">[62]</ref> for the state of the art on ROxford, which involves some training on the target domain with images depicted building and fine-tuning at the target resolution.</p><p>Base model parameters ROxford5k (mAP) Holidays (mAP)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Medium Hard</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XCiT-class token</head><p>XCiT-S12/ <ref type="bibr" target="#b15">16</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XCiT-VLAD</head><p>XCiT-S12/16 k=256 36.6 11.6 89.9 XCiT-S12/16 k=1024 40.0 13.0 90.7</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>XCiT-ASMK</head><p>XCiT-S12/8 k=1024 36 Impact of Image description. We adopt the class-token as the descriptor, and in our experiments we verified that this aggregation method is better than average and GeM pooling <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b51">52]</ref>. In <ref type="table" target="#tab_6">Table D</ref>.4 one can see there is a large benefit in employing a patch based method along with our XCiT transformers: XCiT-VLAD performs significantly better than the CLS token, likely thanks to the higher dimensionality. This is further magnified with AMSK, where we obtain results approaching the absolute state of the art on Holidays, despite a suboptimal training setting for image retrieval. This is interesting since our method has not been fine-tuned for retrieval tasks and we have not been adapted in any significant way beyond applying off-the-shelf this aggregation technique. A direct comparison with ResNet-50 shows that our XCiT method obtains competitive results in this comparable setting, slightly below the ResNet-50 on ROxford but significantly better on Holidays.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Runtime and Memory Usage</head><p>We present the peak memory usage as well as the throughput of multiple models including full-attention and efficient vision transformers in <ref type="table" target="#tab_6">Table D</ref>.5. Additionally, in <ref type="figure">Figure D</ref>.1 we plot the processing speed represented as millisecond per image as a function of image resolution for various models. We can observe that XCiT provides a strong trade-off, possessing the best scalability in terms of peak memory, even when compared to ResNet-50. Additionally, the processing time scales linearly with respect to resolution, with only ResNet-50 providing a better trade-off on that front.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Queries and Keys magnitude visualizations</head><p>Our XCA operation relies on the cross-covariance matrix of the queriesQ and keysK which are 2 normalized across the patch dimension. Therefore, each element in the d ? d matrix represents a cosine similarity whose value is strongly influenced by the magnitude of each patch. In <ref type="figure">Figure D.</ref>2 we visualize the magnitude of patch embeddings in the queries and keys matrices. We observe that patch embeddings with higher magnitude corresponds to more salient regions in the image, providing a very cheap visualization and interpretation of which regions in the image contribute more in the cross-covariance attention. DeiT-S Cait-S12 ViL (Longformer) ViL (Performer) Swin-T PVT Small ResNet50 XCiT-S12/16</p><p>XCiT-S12/8   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Code: https://github.com/facebookresearch/xcit &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 8 / r b F h a o A y t 8 0 V M 2 u V I E w 0 d o O 7 8 = " &gt; A A A D l H i c v V J b a 9 s w F H b s X b r s 0 m S D v e x F L C k k s K V O d + k Y G 6 S E w U p K S b a l D U S p k R U l F r E t V z o u D c J / a D 9 n b / s 3 k 5 0 M m n T P O y D 5 6 D v X 7 / j 4 S c g V u O 7 v k u 3 c u X v v / s 6 D 8 s N H j 5 / s V q p P z 5 R I J W V D K k I h R z 5 R L O Q x G w K H k I 0 S y U j k h + z c X 3 R z + / k V k 4 q L + A c s E z a J y D z m M 0 4 J G M i r l n 7 u 1 X F E I K A k 1 E d Z o / d q 0 E S f E Q Z 2 D f q 7 m E F E r j M c s h k 0 c B K Q G E S k s U x D N n 7 j J j D R b 8 2 d 6 X f m G v Q u M I g E 7 W N 1 K U F P v U W W Y c n n A T T r G J c 3 y m A e F y / f 1 9 + y C 3 2 K g U d M o d N s 5 T k w n 1 V I Q E D 3 s i L x P g a S 1 v + C g x V Y 3 8 z r 6 V X j o 2 7 2 v 6 j c L L l F y 8 S t i U 2 9 y 8 y 0S q V Q 6 j U V V 0 R y E l O G G q N u E x E A F u d / 4 2 O 5 f C J M V t Q n Q A N 0 H A O T h O Y W 1 D jp H z e 9 S s 1 t u Y W g 2 0 p 7 r d S s t f S 9 y i 8 8 F T S N T H o a E q X G 7 Y I o k c B p y L I y T h V L C F 2 Q O R s b N S a m 1 4 k u l i p D e w a Z o p m Q 5 s S A C v R m h C a R U s v I N 5 4 5 Z 7 V t y 8 F / 2 c Y p z D 5 M N I + T 1 P C m q 0 K z N E Q g U L 6 h a M o l o x A u j U K o 5 K Z X R A O S T 8 L s c d k M o b 1 N + b Z y d t B q v 2 + 1 B w e 1 j r s e x 4 7 1 w n p p N a y 2 d W h 1 r K 9 W 3 x p a 1 K 7 a h 3 b H P n K e O 5 + c r v N l 5 W q X 1 j H P r A 1 x T v 8 A Z J Q l D g = = &lt; / l a t e x i t &gt; Local Patch Interaction (LPI) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K P Z v d n R o 7 k w a d B 5 M h b d N D P u + n J k = " &gt; A A A D m X i c v V J b b 9 M w F E 5 b L i P c O p B 4 2 Y t F U 6 k V 0 C X j K g T S p g q 0 0 W l q G d s q 1 V 3 k u G 5 r N Y k z + 2 R a Z e U 3 8 V 9 4 4 9 / g p E V a O 5 4 5 k p 3 j 7 1 y / k x M k I V f g u r 9 L 5 c q t 2 3 f u b t y z 7 z 9 4 + O h x d f P J q R K p p O y E i l D I f k A U C 3 n M T o B D y P q J Z C Q K Q n Y W z N q 5 / e y S S c V F / A P m C R t G Z B L z M a c E D O R v l n 7 W H R w R m F I S 6 r 2 s 0 X n Z a 6 L P C A O 7 A n 0 s x h C R q w y H b A w N n E x J D C L S W K Y h G 7 x 2 E x j q N + b O 9 F t z 9 T r n G E S C t r G 6 k K B H / i z L s O S T K T Q d j O 2 V M p j H x S s I 9 P f s X B 9 h 4 B F T 6 C h b e P b M Z x E y J a A 7 W Z F 4 G w N J n b 9 g b w E 6 q 3 l 9 v W i 8 3 8 7 + F 5 X r J d d o m b gl s Z F / k Z l W q R R K v a L i k k h O Y s p Q o 9 9 u I g L A 4 v x v f L T t + q E w a V G X A J 2 i g x i Y J D Q 3 o c Z h 9 6 B p O y 8 c v 1 p z W 2 4 h 6 K b i L Z W a t Z S u X / 2 F R 4 K m k a l B Q 6 L U w C v Y E g m c h i y z c a p Y Q u i M T N j A q D E x D Q 9 1 s V k Z q h t k h M Z C m h M D K t D r E Z p E S s 2 j w H j m x N W 6 L Q f / Z R u k M P 4 w 1 D x O U k O e L g q N 0 x C B Q P m a o h G X j E I 4 N w q h k p t e E Z 2 S f B p m m W 0 z B G + d 8 k 3 l d K f l v W t 5 v Z 3 a r r s c x 4 a 1 Z T 2 3 G p Z n v b d 2 r X 2 r a 5 1 Y t P y s / K n 8 p f y 1 s l XZ q + x X v i 1 c y 6 V l z F N r R S r H f w A f u C X i &lt; / l a t e x i t &gt; + &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K P Z v d n R o 7 k w a d B 5 M h b d N D P u + n J k = " &gt; A A A D m X i c v V J b b 9 M w F E 5 b L i P c O p B 4 2 Y t F U 6 k V 0 C X j K g T S p g q 0 0 W l q G d s q 1 V 3 k u G 5 r N Y k z + 2 R a Z eU 3 8 V 9 4 4 9 / g p E V a O 5 4 5 k p 3 j 7 1 y / k x M k I V f g u r 9 L 5 c q t 2 3 f u b t y z 7 z 9 4 + O h x d f P J q R K p p O y E i l D I f k A U C 3 n M T o B D y P q J Z C Q K Q n Y W z N q 5 / e y S S c V F / A P m C R t G Z B L z M a c E D O R v l n 7 W H R w R m F I S 6 r 2 s 0 X n Z a 6 L P C A O 7 A n 0 s x h C R q w y H b A w N n E x J D C L S W K Y h G 7 x 2 E x j q N + b O 9 F t z 9 T r n G E S C t r G 6 k K B H / i z L s O S T K T Q d j O 2 V M p j H x S s I 9 P f s X B 9 h 4 B F T 6 C h b e P b M Z x E y J a A 7 W Z F 4 G w N J n b 9 g b w E 6 q 3 l 9 v W i 8 3 8 7 + F 5 X r J d d o m b g l s Z F / k Z l W q R R K v a L i k k h O Y s p Q o 9 9 u I g L A 4 v x v f L T t + q E w a V G X A J 2 i g x i Y J D Q 3 o c Z h 9 6 B p O y 8 c v 1 p z W 2 4 h 6 K b i L Z W a t Z S u X / 2 F R 4 K m k a l B Q 6 L U w C v Y E g m c h i y z c a p Y Q u i M T N j A q D E x D Q 9 1 s V k Z q h t k h M Z C m h M D K t D r E Z p E S s 2 j w H j m x N W 6 L Q f / Z R u k M P 4 w 1 D x O U k O e L g q N 0 x C B Q P m a o h G X j E I 4 N w q h k p t e E Z 2 S f B p m m W 0 z B G + d 8 k 3 l d K f l v W t 5 v Z 3 a r r s c x 4 a 1 Z T 2 3 G p Z n v b d 2 r X 2 r a 5 1 Y t P y s / K n 8 p f y 1 s l X Z q + x X v i 1 c y 6 V l z F N r R S r H f w A f u C X i &lt; / l a t e x i t &gt; + &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " h 7 p d r 1 A Q w d j b t y G 3 A 2 9 c i W Z x Z z E = " &gt; A A A D t X i c v V J b b 9 o w F E 5 h l y 6 7 l G 6 P e 7 E G l U B b K b B L p 0 m T O l V C q 1 o h 2 N Y W C V N 0 4 j h g k c S p f d I W W f m F e 9 v b / s 0 c Y F K h e 9 6 R 7 B x / 5 / 7 l e E k o N D Y a v z c K x X v 3 H z z c f O Q + f v L 0 2 V Z p + / m Z l q l i / J T J U K q + B 5 q H I u a n K D D k / U R x i L y Q n 3 v T w 9 x + f s W V F j L + g b O E D y M Y x y I Q D N B C o + 2 N n z s V G g F O G I T m S 1 Y 9 f t O r k c + E I r 9 B 8 1 0 G G M F N R k M e Y J U m E 4 h R R o a q N O S D t 4 0 E h + a d v T P z 3 l 6 9 4 w u K M i F 7 V F 8 q N P 5 o m m V U i f E E a x V K 3 Z U y V M T z l + e Z b 9 m F 6 V A U E d e k k y 0 8 e / a z C J k A m u N s n n i P I q S V v 2 B v A V Z W 8 4 7 M o v H + Y f a / R r l d c m 0 s G 7 c c z B 9 d Z r Z V p q T W u 0 x e g R I Q M 0 6 q / c M a A U Q e 5 3 / j k + v u n E i b l n Q B 2 Y Q c x c g V s N x E q i f d o 5 q t / L r i t j n 3 d 9 t S X Y P y S Y f j t V R T U m 2 3 O 7 V R q d y o N + Z C 7 i r N p V J 2 l t I d l X 5 R X 7 I 0 s v V Z C F o P m n M m Q K F g I c 9 c m m q e A J v C m A + s G o M d Z m j m W 5 e R H Y v 4 J J D K n h j J H L 0 d Y S D S e h Z 5 1 j M n R a / b c v B f t k G K w c e h E X G S W m L Y o l C Q h g Q l y V e Y + E J x h u H M K s C U s L 0 S N o G c K b v o r i W h u T 7 y X e W s V W 9 + q D d 7 r f J B Y 0 n H p v P S e e V U n a a z 7 x w 4 X 5 2 u c + q w Q q v Q L 0 D B K + 4 X h 0 W / G C x c C x v L m B f O i h T l H 4 J S L / Q = &lt; / l a t e x i t &gt; Feed-Forward Network (FFN) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F o A H g 4 H M S R c R a y f j k 7 h i / q D 5 h A w = " &gt; A A A D w H i c v V J b a 9 s w F H a T X T r v 0 n Z 7 3 I t Y U 0 j Y 2 i b d l U G h W y C s t I R k W 9 t A l I Z j W a l F Z M u R j t t m w n 9 y D 4 P 9 m 8 l J B r 3 s e Q c k H 3 3 n / v k E q R Q G 6 / X f S 6 X y n b v 3 7 i 8 / 8 B 8 + e v x k Z X X t 6 b F R m W b 8 i C m p d C 8 A w 6 V I + B E K l L y X a g 5 x I P l J M G 4 W 9 p N z r o 1 Q y X e c p n w Q w 1 k i R o I B O m i 4 t v R r o 0 J j w I i B t J / y 6 s G r b o 3 s E o r 8 E u 0 3 N c I Y L n M q + Q i r N I 0 g Q R V b q j P J + 6 / r K Q 7 s G 3 f n 9 q 2 7 u g e n F F V K t q m Z a L T h c J z n V I u z C G s V S v 1 r Z a h I Z q 8 g s F / z U 9 u m K G J u S D u f e 3 b d Z x 4 S A d q D f J Z 4 m y J k l b 9 g d w 5 W r u c d 2 n n j v W b + v 0 a 5 W v L G W C 5 u M V g 4 n O S u V a a V M Z t M n Y M W k D B O q r 1 m j Q A i T 4 q / 8 d H 3 N w 6 V S 0 s 6 g C w i + w l y D a w w k e p h Z 7 / m K r 9 0 a V q c h 5 s t p S 9 A h 6 T N 8 U L p M a m 2 W u 2 a f w h T r t t K x 8 P V 9 f p W f S b k t t J Y K O v e Q j r D 1 Z 8 0 V C y L X S t M g j H 9 x o w U 0 C i Y 5 L l P M 8 N T Y G M 4 4 3 2 n J u D m G t j Z A u Z k w y E h G S n t T o J k h l 6 N s B A b M 4 0 D 5 1 n w Y 2 7 a C v B f t n 6 G o w 8 D K 5 I 0 c x y x e a F R J g k q U m w z C Y X m D O X U K c C 0 c L 0 S F k F B m t t 5 3 5 H Q u D n y b e V 4 Z 6 v x b q v R 3 V n f q y / o W P a e e y + 8 q t f w 3 n t 7 3 h e v 4 x 1 5 r L R b Y i V Z i s u f y 1 F Z l S d z 1 9 L S I u a Z d 0 3 K P / 4 A C 4 4 0 K g = = &lt; / l a t e x i t &gt; LayerNorm &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L e Z 0 3 n P O m m T Y M N u o j o W c y q x F V T k = " &gt; A A A D 6 3 i c v V N b b 9 M w F M 5 S L i P c N n j k x W K t 1 A q 6 t e M q J K R B p Y p p U 9 U C 2 y r V X e U 4 b m M 1 s T P 7 Z F t l 5 S / w w g M I 8 c o f 4 o 1 / g 5 M W t A v P W I p z c i 7 f O d 9 n x 0 8 i r q H R + L X k l q 5 c v X Z 9 + Y Z 3 8 9 b t O 3 d X V u / t a 5 k q y v a o j K T q + 0 S z i A u 2 B x w i 1 k 8 U I 7 E f s Q N / 2 s r j B 8 d M a S 7 F R 5 g l b B i T i e B j T g l Y 1 2 j V X a q U c U w g p C Q y b 7 L q z u N e D b 1 G G N g p m A 9 y D D E 5 z X D E x l D F S U g E y N h g l U Z s 8 K S R w N A 8 t X t m n t m t t 3 O I Q S Z o A + s j B S Y Y T b M M K z 4 J o V b G 2 D v X B n N R f P m + e Z 8 d m g 4 G H j O N O t k 8 s 2 d f 8 5 K Q g N n J C u A N D C Q t / 3 H 2 5 s 7 y e d y R m Q / e b 2 X / i 8 r Z l h d o 2 b o F s W B 0 l N l R q Z J a 1 6 k 8 J o o T Q R m q 9 l s 1 R A C Y y E / j l e d V d q W F R V 0 C N E T b A p g i N A + h 6 m 5 3 u 2 Y 7 P 7 I w b c a C e l u q E 6 I C 1 G F w I t U U V d v t j k 3 Y J T O m O l L F X u V t J O m 0 H n A y k c J i 9 l v 1 v 5 0 8 L p I U E M g p E 3 q 0 s t Z Y b x Q L X T a a C 2 P N W a z u a O U n D i R N Y 4 t F I 6 L 1 o F n o R x R w G r H M w 6 l m C a F T M m E D a w p i J R i a 4 q 5 m q G I 9 A R p L Z R 8 B q P C e r T A k 1 n o</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>M h x 7 L z w H n o V J 2 m 8 8 L Z c t 4 5 X W f P o W 7 o f n K / u F 9 L c e l z 6 V v p + z z V X V r U 3 H f O r d K P 3 + C F R D A = &lt; / l a t e x i t &gt; input tokens &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " e r u e b N S a e V 0 4 b S I v V 8 O y x 0 Q M g A k = " &gt; A A A D 9 n i c v V N L b 9 N A E H Y d H s U U a O H I Z U U S K R E k T c p T S E h F k S K q R F E C t I 2 U T a P 1 e p O s Y u + 6 u + O 2 k e U f w o U D C H H l t 3 D j 3 7 C 2 A + q D M y t 5 P f 5 m 5 v t m Z r 1 u 6 H M N j c a v N b t w 7 f q N m + u 3 n N s b d + 7 e 2 9 y 6 f 6 B l p C j b p 9 K X a u g S z X w u 2 D 5 w 8 N k w V I w E r s 8 O 3 U U r 9 R + e M K W 5 F B 9 h G b J x Q G a C T z k l Y K D J l r 1 R L u G A w J w S P 3 6 b V D p P B l X 0 B m F g Z x B / k F M I y F m C f T a F C g 7 n R I A M Y q w i n 4 2 e N k I Y x 8 / M n s T P z T b o H G G Q I d r G + l h B 7 E 0 W S Y I V n 8 2 h W s L Y u S C D u c i + X D d + n x z F P Q w 8 Y B r 1 k j x y Y F 5 5 y p x A 3 E k y 4 m 0 M J C r 9 A Q c 5 W L r I O 4 n z w o e t 5 H + 1 c l 7 y U l s m b 9 W Y N z l O T K l U S a 1 r V J 4 Q x Y m g D F W G r S o i A E y k p / H a c c p d a W h R n w C d o z 0 B T B G a u l C l 2 9 + r G u X H h q b N m F d r S 3 V K l I d 6 D E 6 l W q B K u 9 0 z A V 2 y Z K o n V e C U X V / S R c 3 j Z C a F 4 R y 2 a n + V n D I X Y Q Q I 5 I I J 7 Z S 6 e Z 2 l y W a x U W 9 k C 1 0 1 m i u j a K 1 W f 7 L 5 E 3 u S R o G h p T 7 R e t T M R k k U c O q z x M G R Z i G h C z J j I 2 M K Y l T G c f b b J q h s E A 9 N p T K P A J S h 5 z N i E m i 9 D F w T m U 5 V X / a l 4 L 9 8 o w i m r 8 Z x 1 i I T N B e a R r 5 p F 6 V 3 A H l c M Q r + 0 h i E K m 5 q R X R O 0 l G b m + K Y I T Q v t 3 z V O N i p N 1 / U m 4 O d 4 m 5 j N Y 5 1 6 6 H 1 y K p Y T e u l t W u 9 s / r W v k V t b X + y v 9 h f C 2 e F z 4 V v h e 9 5 q L 2 2 y n l g X V i F H 7 8 B n 7 p H / w = = &lt; / l a t e x i t &gt; L? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 1 C E K z s C F O n o R X A G 5 s 9 z r 9 j / 7 S a Y = " &gt; A A A E A n i c v V N L b 9 N A E H Y T H s W 8 W r g g c V n R R E o E a Z P y F B J S U a S I q l G U A G 0 j Z d N o v d 4 k q 6 x 3 3 d 1 x 2 8 i y u P B X u H A A I a 7 8 C m 7 8 G 9 Z 2 Q H 1 w Z i W v x 9 / M f N / M r N c L B T d Q r / 9 a K h Q v X b 5 y d f m a e / 3 G z V u 3 V 1 b v 7 B k V a c p 2 q R J K 9 z 1 i m O C S 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>A B M 6 i T 5 J E 9 + 8 p T p g T i n S Q j 3 s B A o t I f s J e D p b O 8 o z g v v N 9 M / l c r p y X P t W X z F o 3 5 o 8 P E l k q 1 M q Z G 1 R H R n E j K U K X f r C I C w G R 6 G i 9 d t 9 x W l h Z 1 C d A p 2 p b A N K G p C 1 X a 3 e 2 q V X 5 o a V q M + b W W 0 s d E + 6 j D 4 F j p G a q 0 W h 0 b 0 C Z z p j t K B 2 7 Z E 4 r O a j 4 n E y U t Z 7 9 Z + 6 v k l r k M I 0 C g Z k w a S 9 v O C y 2 5 / S Z / j 0 R K M l p Z q 6 / X s 4 U u G o 2 F s e Y s V n e 0 8 h P 7 i k a B l a C C G D N o Z G M l G j g V L H F x Z F h I 6 I x M 2 M C a k l j B Y Z z 9 w g k q W 8 R H Y 6 X t I w F l 6 O m M m A T G z A P P R q Y T N u d 9 K f g v 3 y C C 8 Y t h n L X L J M 2 F x p G w r a P 0 P i C f a 0 Z B z K 1 B q O a 2 V k S n J B 2 7 v T W u H U L j f M s X j b 3 N 9 c a z 9 U Z v c 2 2 r v h j H s n P f e e B U n I b z 3 N l y 3 j h d Z 9 e h h Q + F T 4 U v h a / F j 8 X P x W / F 7 3 l o Y W m R c 9 c 5 s 4 o / f g P 7 c U w 7 &lt; / l a t e x i t &gt; XCiT layer &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 z V y O U g 3 F w Z M g v s I 1 c B I 4 D v t O P Q = " &gt; A A A C b H i c b V F N b x M x E P V u + S j h o 2 n h A K q Q L J K i V K r C b q G l F 6 Q i L k i 9 N I K 0 l e I Q e R 1 v Y s U f W 3 s W N b L 2 x D / k x k / g w m / A 2 e R Q W k b y z N O b N / L 4 O S u k c J A k v 6 J 4 7 c 7 d e / f X H z Q e P n r 8 Z K O 5 u X X m T G k Z 7 z M j j b 3 I q O N S a N 4 H A Z J f F J Z T l U l + n s 0 + L f r n 3 7 l 1 w u i v M C / 4 U N G J F r l g F A I 1 a v 5 o E 0 V h y q j 0 H 6 v O y V 5 v F 3 / A B P g V + C 8 m B 0 W v K i J 5 D h 1 S T K k G o z y x p e S D t 0 k B Q / 8 u 5 M o f h N Q 7 + U b A F P g N c Z c W / H g 0 q y p i x W Q K u 2 1 C G j v t X i g 1 W A q v 6 d q j Z i v p J n X g 2 y B d g R Z a x e m o + Z O M D S s V 1 8 A k d W 6 Q 1 t t Q C 4 J J X j V I 6 X h B 2 Y x O + C B A T R V 3 Q 1 + b V e G d w I x x b m w 4 G n D N X p / w V D k 3 V 1 l Q L p x x N 3 s L 8 n + 9 Q Q n 5 0 d A L X Z T A N V t e l J c S g 8 E L 5 / F Y W M 5 A z g O g z I q w K 2 Z T a i m D 8 D + N Y E J 6 8 8 m 3 w d l + N z 3 s p r 3 9 1 v H R y o 5 1 t I 1 e o Q 5 K 0 X t 0 j D 6 j U 9 R H D P 2 O N q L n 0 Y v o T / w s 3 o 5 f L q V x t J p 5 i v 6 J + P V f m R q 6 E Q = = &lt; / l a t e x i t &gt; A(K, Q) t e x i t s h a 1 _ b a s e 6 4 = " z U 2 d N x L F y I w F v h 8 H / I j C u h U N V e s = " &gt; A A A C 3 3 i c t V J L b x M x E P Y u r 7 I 8 G u D I x S K J l E o o 7 J Z X L 0 h F v S D 1 0 g j S R o r D y u t 4 E y t e e 7 F n U S N r L 1 w 4 g B B X / h Y 3 / g h n n E 0 O o e X K S B 5 / + u Y b z X j G W S m F h T j + F Y R X r l 6 7 f m P n Z n T r 9 p 2 7 u 6 1 7 9 0 + t r g z j Q 6 a l N q O M W i 6 F 4 k M Q I P m o N J w W m e R n 2 e J o F T / 7 y I 0 V W r 2 D Z c k n B Z 0 p k Q t G w V N p 6 3 e 3 Q w o K c 0 a l e 1 3 3 j h 8 P 9 v A r T I C f g 3 u r c y j o e U 0 k z 6 F H y j l V o A t H T C X 5 + G l c w s Q 9 8 7 5 2 z 7 0 b H L 8 n o E v 8 h N g P B t w 0 X d Q 1 M W I 2 h 7 0 O I V G 3 M / B X A 9 b C L V 0 n 2 u 4 h d e v q o 6 P 6 / / W T t t p x P 2 4 M X w b J B r T R x k 7 S 1 k 8 y 1 a w q u A I m q b X j p K l H D Q g m e R 2 R y v K S s g W d 8 b G H i h b c T l y z n x p 3 P T P F u T b + K M A N u 5 3 h a G H t s s i 8 c j U H e z G 2 I v 8 V G 1 e Q H 0 y c U G U F X L F 1 o b y S G D R e L R t P h e E M 5 N I D y o z w v W I 2 p 4 Y y 8 F 8 i 8 k N I L j 7 5 M j j d 7 y c v + s l g v 3 1 4 s B n H D n q I H q E e S t B L d I j e o B M 0 R C w g w a f g S / A 1 p O H n 8 F v 4 f S 0 N g 0 3 O A / S X h T / + A E Z Z 5 5 A = &lt; / l a t e x i t &gt; A XC (K, Q) t e x i t s h a 1 _ b a s e 6 4 = " 5 r j i cX q A d n E d J F d x F 5 d 1 q K W f Y L Y = " &gt; A A A C b H i c b V H L b h M x F P V M e Z T wa H g s Q B W S R S Y o l a o w 0 x b o B q m I D V I 3 j S B t p T h E H s e T W P F j a t 9 B j a x Z 8 Y f s + A Q 2 f A P O Y 1 F a r u R 7 j 8 4 9 V 7 4 + z k s p H K T p r y j e u H X 7 z t 3 N e 4 3 7 D x 4 + 2 m o + f n L q T G U Z 7 z M j j T 3 P q e N S a N 4 H A Z K f l 5 Z T l U t + l s 8 + L f p n 3 7 l 1 w u i v M C / 5 U N G J F o V g F A I 1 a v 5 o J 0 R R m D I q / c e 6 c 7 z b 2 8 E f M A F + C f 6 L K U D R y 5 p I X k C H l F O q w S h P b C X 5 Y D 8 t Y e g P Q q 7 9 2 5 B 6 x 9 8 I m B K / I e 7 C g h + P Z n V N r J h M Y S c h p J H 0 Q g 6 1 n a x 0 V 2 T J q N l K u + k y 8 E 2 Q r U E L r e N k 1 P x J x o Z V i m t g k j o 3 y J b L U A u C S V 4 3 S O V 4 S d m M T v g g Q E 0 V d 0 O / N K v G 7 c C M c W F s O B r w k r 0 6 4 a l y b q 7 y o F w Y 4 6 7 3 F u T / e o M K i s O h F 7 q s g G u 2 u q i o J A a D F 8 7 j s b C c g Z w H Q J k V Y V f M p t R S B u F / G s G E 7 P q T b 4 L T v W 7 2 r p v 1 9 l p H h 2 s 7 N t E 2 e o U 6 K E P v 0 R H 6 j E 5 Q H z H 0 O 9 q K n k c v o j / x s 3 g 7 f r m S x t F 6 5 i n 6 J + L X f w G C C b o R &lt; / l a t e x i t &gt; Q &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " I r u 0 2 V I U g Z o 2 s F V 5 B D 2 x O m n M F V k = " &gt; A A A C b H i c b V H L b h M x F P V M e Z T w a H g s Q B W S R S Y o l a o w 0 x b o B q m I D V I 3 j S B t p T h E H s e T W P F j a t 9 B j a x Z 8 Y f s + A Q 2 f A P O Y 1 F a r u R 7 j 8 4 9 V 7 4 + z k s p H K T p r y j e u H X 7 z t 3 N e 4 3 7 D x 4 + 2 m o + f n L q T G U Z 7 z M j j T 3 P q e N S a N 4 H A Z K f l 5 Z T l U t + l s 8 + L f p n 3 7 l 1 w u i v M C / 5 U N G J F o V g F A I 1 a v 5 o J 0 R R m D I q / c e 6 c 7 z b 2 8 E f M A F + C f 6 L K U D R y 5 p I X k C H l F O q w S h P b C X 5 Y D 8 t Y e g P Q q 7 9 2 5 B 6 x 9 8 I m B K / I e 7 C g h + P Z n V N r J h M Y S c h p N F O e q E E k K x 0 V 2 T J q N l K u + k y 8 E 2 Q r U E L r e N k 1 P x J x o Z V i m t g k j o 3 y J b L U A u C S V 4 3 S O V 4 S d m M T v g g Q E 0 V d 0 O / N K v G 7 c C M c W F s O B r w k r 0 6 4 a l y b q 7 y o F w Y 4 6 7 3 F u T / e o M K i s O h F 7 q s g G u 2 u q i o J A a D F 8 7 j s b C c g Z w H Q J k V Y V f M p t R S B u F / G s G E 7 P q T b 4 L T v W 7 2 r p v 1 9 l p H h 2 s 7 N t E 2 e o U 6 K E P v 0 R H 6 j E 5 Q H z H 0 O 9 q K n k c v o j / x s 3 g 7 f r m S x t F 6 5 i n 6 J + L X f w G B H r o R &lt; / l a t e x i t &gt; K &gt; / p d k &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " m z z Z z E a 2 n 4 o + H N r l O N K 9 h Y k g q H E = " &gt; A A A C 7 3 i c t V J L b x M x E P Y u F E p 4 N I U j F 4 t s p F R C 6 W 5 5 9 Y J U 1 A t S L o 0 g b a Q 4 r B z H m 7 X i X W / t W d T I 2 j / B h Q M I c e X v c O P f 4 N 3 k 0 A d X R v L 4 8 z c z m s 9 j z w o p D I T h H 8 + / d X v r z t 3 t e 6 3 7 D x 4 + 2 m n v P j 4 1 q t S M j 5 i S S o 9 n 1 H A p c j 4 C A Z K P C 8 1 p N p P 8 b L Y 8 r u N n n 7 k 2 Q u U f Y V X w a U Y X u U g E o + C o e N f b 6 g Y k o 5 A y K u 2 7 q j d 4 P t z D b z E B f g H 2 g 0 o g o x c V k T y B H i l S m o P K L N G l 5 J M X Y Q F T + 9 L 5 y r 5 y b j j 4 R E A V e J + Y c w 1 2 H i + r i m i x S G E v I K T V D Y Z u c y A g K Q U 7 q J r s f Q K 0 D O r g 1 e M l S b F d i x k f V / 9 P X t z u h P 2 w M X w T R B v Q Q R s 7 i d u / y V y x M u M 5 M E m N m U R N P 6 p B M M m r F i k N L y h b 0 g W f O J j T j J u p b d 6 r w l 3 H z H G i t F s 5 4 I a 9 X G F p Z s w q m 7 n M e g 7 m e q w m / x W b l J A c T q 3 I i x J 4 z t a N k l J i U L h + f D w X m j O Q K w c o 0 8 J p x S y l m j J w X 6 T l h h B d v / J N c H r Q j 1 7 3 o + F B 5 + h w M 4 5 t 9 B Q 9 Q z 0 U o T f o C L 1 H J 2 i E m C e 9 L 9 4 3 7 7 t / 7 n / 1 f / g / 1 6 m + t 6 l 5 g q 6 Y / + s v F v n r x g = = &lt; / l a t e x i t &gt;K &gt; /? &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 2 F 7 + u x w f 9 N h M K z t O T e 2 4 2 W T C H 2 g = " &gt; A A A C 8 H i c t V J L b x M x E P Y u U M r y a A p H L h Z J p F R C 6 W 5 5 9 Y J U 1 A t S L l 1 B 2 k h x W H k d b 9 a K 9 4 E 9 i x p Z + y u 4 c A C h X v t z u P F v c H Z z o C 1 X R v L 4 0 z c z m s 8 z j k s p N P j + b 8 e 9 d f v O 1 t 3 t e 9 7 9 B w 8 f 7 X R 2 H 5 / q o l K M j 1 k h C z W J q e Z S 5 H w M A i S f l I r T L J b 8 L F 4 e r + N n X 7 j S o s g / w q r k s 4 w u c p E I R s F S 0 a 6 z 1 e + R j E L K q D T v 6 s H o e b i H 3 2 I C / B z M h y K B j J 7 X R P I E B q R M a Q 5 F Z o i q J J + + 8 E u Y m Z f W 1 + a V d e H o E 4 G i x P t E f 1 Z g 5 t G y r o k S i x T 2 e o R 4 / V 5 o r w a Q l I I Z 1 U 3 6 P g F a 9 b y W C 1 u u 5 1 3 R F J l W z e S 4 / n / 6 o k 7 X H / q N 4 Z s g 2 I A u 2 t h J 1 P l F 5 g W r M p 4 D k 1 T r a d D 0 o w o E k 7 z 2 S K V 5 S d m S L v j U w p x m X M 9 M s 7 A a 9 y 0 z x 0 m h 7 M k B N + z f F Y Z m W q + y 2 G a u 5 6 C v x 9 b k v 2 L T C p L D m R F 5 W Q H P W d s o q S S G A q + 3 j + d C c Q Z y Z Q F l S l i t m K V U U Q b 2 j 3 h 2 C M H 1 J 9 8 E p w f D 4 P U w C A + 6 R 4 e b c W y j p + g Z G q A A v U F H 6 D 0 6 Q W P E n M z 5 6 n x 3 f r j K / e b + d C / a V N f Z 1 D x B V 8 y 9 / A P g y + y M &lt; / l a t e x i t &gt;Q &gt; &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " T g W p T 0 w M M I g 4 l L u z y V h n Y o t J x z k = " &gt; A A A D U n i c t V J L b x M x E P Y m P E o o N I U j F 4 s k U i q h N F u g 7 Q W p q B e k S F U D p I 0 U p y u v 4 8 1 a 8 T 5 q z 6 J G l n 8 j E u L C D + H C A X A 2 Q Z Q U j o z k 8 e d v Z j w P T Z h L o a H b / e J V q r d u 3 7 m 7 c a 9 2 f / P B w 6 3 6 9 q M z n R W K 8 Q H L Z K a G I d V c i p Q P Q I D k w 1 x x m o S S n 4 e z 4 4 X 9 / A N X W m T p e 5 j n f J z Q a S o i w S g 4 K t j 2 4 l a T J B R i R q V 5 b d u 9 Z / 0 d / A o T 4 F d g 3 m U R J P T K E s k j a J M 8 p i l k i S G q k H z 0 v J v D 2 L x w 2 p q X T v V 7 F w S y H O 8 S f a n A T I K Z t U S J a Q w 7 T U J q 1 7 M Q k Z a v M D R v 7 Y U 5 I S A S r v G J X T i 2 m n 1 3 l Y D E F E z P l v / u E q B F 8 x f Z X 5 L l + / e / g V n W P T y 2 / 7 G T f 6 Z c a 8 v F r R q b B J e 2 G d Q b 3 U 6 3 F H w T + C v Q Q C s 5 D e q f y C R j R c J T Y J J q P f L L O q k C w S S 3 N V J o n l M 2 o 1 M + c j C l L t X Y l C t h c c s x E x x l y p 0 U c M l e j z A 0 0 X q e h M 5 z U b J e t y 3 I v 9 l G B U S H Y y P S v A C e s m W i q J A Y M r z Y L z w R i j O Q c w c o U 8 L V i l l M F W X g t r D m h u C v t 3 w T n O 1 1 / P 2 O 3 9 9 r H B 2 u x r G B n q C n q I 1 8 d I C O 0 B t 0 i g a I e R + 9 r 9 5 3 7 0 f l c + V b 1 a t W l 6 4 V b x X z G P 0 h 1 c 2 f J Z E S d Q = = &lt; / l a t e x i t &gt; A 2 R N ?N &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " L A i C o X 4 t 7 S b G e C 9 g I 1 m</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>A 2 k h x u v I 6 3 q w V 7 6 P 2 L G p k + T c i I S 7 8 E C 4 c A G c 3 S C S F I y N 5 / P m b t z V h L o W G X u + r V 6 v f u H n r 9 t a d x t 3 t e / c f N H c e n u m s U I w P W S Y z N Q q p 5 l K k f A g C J B / l i t M k l P w 8 n B 8 v 7 e c f u d I i S z / A I u e T h M 5 S E Q l G w V H B j h f v t k l C I W Z U m j e 2 0 3 8 6 2 M O v M Q F + B e Z 9 F k F C r y y R P I I O y W O a Q p Y Y o g r J x 8 9 6 O U z M c 6 e t e e H U o H 9 B I M v x P t G X C s w 0 m F t L l J j F s N c m p L F W h o i 0 f I W h e W c v z A k B k X C N T 2 z l O X B X F R J T M H 1 b J t 4 n Q I v 2 b 3 J Q k e 3 1 v I G p G h 8 d 2 / 8 4 y r 8 q b k z l w l Z z T Y N L 2 w 6 a r V 6 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>4 L a w 4 T 7 B 3 x z 5 O j g 7 6 P o v u / 7 g o H V 0 u P q O L f Q Y P U E d 5 K N X 6 A i 9 R a d o i J j 3 y f v m / f B + 1 r 7 U v t e 9 e r 1 y r X m r m E d o T e r b v w D 5 k h J 1 &lt; / l a t e x i t &gt; A XC 2 R d k ?dq &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " y e U A D H G I y i 0 0 g A s u</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>9 a 6 7</head><label>7</label><figDesc>O 2 4 b W T 5 w 4 a 9 w 4 Q B C X P k R 3 P g 3 r O 2 A + u D M S l 6 P v 5 n 5 Z r 5 Z r x c K b q B W + 7 V Q K F 6 7 f u P m 4 i 3 3 9 p 2 7 9 + 4 v L T 8 4 M C r S l O 1 T J Z T u e c Q w w S X b B w 6 C 9 U L N S O A J d u h N G 6 n / 8 I R p w 5 X c g 1 n I B g E Z S z 7 i l I C F h s u F R + U S D g h M K B H x d l L Z X e 1 W 0 R u E g Z 1 B / F 6 N I C B n C R Z s B B U c T o g E F c R Y R 4 L 1 n 9 V C G M T P 7 Z 7 E L + z W 3 T 3 C o E K 0 g c 2 x h t g f T p M E a z 6 e Q L W E s X u h D O Y y + / K 8 + F 1 y F L c x 8 I A Z 1 E 7 y y K 5 9 5 S k T A v F u k h F v Y C B R 6 Q / Y z c H S R d 5 h n D f e a y T / S 8 r 5 k p d k 2 b y 5 M H 9 4 n J T c h l</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>m g W c j 0 x m b y 7 4 U / J e v H 8 H o 9 S D O p D N J 8 0 K j S N g x o P R G I J 9 r R k H M r E G o 5 r Z X R C c k n b y 9 N 6 4 d Q v 2 y 5 K v G w e Z 6 / e V 6 v b u 5 s r U 6 H 8 e i 8 9 h 5 4 l S c u v P K 2 X L e O h 1 n 3 6 G F D 4 V P h S + F r 8 W P x c / F b 8 X v e W h h Y Z 7 z 0 L m w i j 9 + A 2 6 1 S 4 4 = &lt; / l a t e x i t &gt; Cross-Covariance Attention (XCA) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " K P Z v d n R o 7 k w a d B 5 M h b d N D P u + n J</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>U 3 8</head><label>8</label><figDesc>V 9 4 4 9 / g p E V a O 5 4 5 k p 3 j 7 1 y / k x M k I V f g u r 9 L 5 c q t 2 3 f u b t y z 7 z 9 4 + O h x d f P J q R K p p O y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>3 a r r s c x 4 a 1 Z</head><label>1</label><figDesc>T 2 3 G p Z n v b d 2 r X 2 r a 5 1 Y t P y s / K n 8 p f y 1 s l X Z q + x X v i 1 c y 6 V l z F N r R S r H f w A f u C X i &lt; / l a t e x i t &gt; + &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " P N I a u B E 2 M L C A 7 j 7 X k F w T z T j b G / 0 = " &gt; A A A E S X i c v V N L b 9 N A E H a T A s U 8 2 s K R y 4 o m U i L 6 S M r z g t Q q U k S V K E q g j 0 j Z 1 F q v N 8 k q a 6 + 7 O 2 4 b W f 5 7 X L h x 4 z 9 w 4 Q B C n F j b Q f Q l j q z k 3 f E 3 M 9 + 8 N G 4 o u I Z a 7 c t C o b h 4 6 / a d p b v 2 v f s P H i 6 v r D 4 6 1 D J S l B 1 Q K a T q u 0 Q z w Q N 2 A B w E 6 4 e K E d 8 V 7 M i d N l L 9 0 S l T m s t g H 2 Y h G / p k H P A R p w Q M 5 K w W n H I J + w Q m l I h 4 N 6 m 0 1 n t V 9 B Z h Y O c Q f 5 A j 8 M l 5 g g U b Q Q W H E x K A 9 G O s I s E G z 2 s h D O M X 5 k 7 i l + b q t Y 4 x y B B t Y X 2 i I P a c a Z J g x c c T q J Y w t i + F w T z I / l w 3 f p 8 c x x 0 M 3 G c a d Z L c s m e e 3 G V C I G 4 l G f E W B h K V / o C 9 H C x d 5 n X i P P F + I / l f p V w M e a U s 4 z c v z H N O E p N q Q 0 m t N x r y l C h O A s r Q L g A L 0 k G g S r + x W 7 X t c l s a W t Q l Q C d o L w C m C M 3 1 7 e 5 e 1 U R + Z m i a j H k b T a n O i P J Q h 8 G Z V F N U a T Y 7 x q B N Z k x 1 p P L t s i s k n W 5 4 n I x l Y D h N A L v M g z A C B H L K A m 3 Y 2 n l + h r P f 4 P t I p M 7 r 6 C 9 c a i F T E 7 p p V m l T S s a 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>m w n 9 y D 4 P 9 m 8 l J B r 3 s e Q c k H 3 3 n / v k E q R Q G 6 / X f S 6 X y n b v 3 7 i 8 / 8 B 8 + e v x k Z X X t 6 b F R m W b 8 i C m p d C 8 A w 6 V I + B E K l L y X a g 5 x I P l J M G 4 W 9 p N z r o 1 Q y X e c p n w Q w 1 k i R o I B O m i 4 t v R r o 0 J j w I i B t J / y 6 s G r b o 3 s E o r 8 E u 0 3 N c I Y L n M q + Q i r N I 0 g Q R V b q j P J + 6 / r K Q 7 s G 3 f n 9 q 2 7 u g e n F F V K t q m Z a L T h c J z n V I u z C G s V S v 1 r Z a h I Z q 8 g s F / z U 9 u m K G J u S D u f e 3 b d Z x 4 S A d q D f J Z 4 m y J k l b 9 g d w 5 W r u c d 2 n n j v W b + v 0 a 5 W v L G W C 5 u M V g 4 n O S u V a a V M Z t M n Y M W k D B O q r 1 m j Q A i T 4 q / 8 d H 3 N w 6 V S 0 s 6 g C w i + w l y D a w w k e p h Z 7 / m K r 9 0 a V q c h 5 s t p S 9 A h 6 T N 8 U L p M a m 2 W u 2 a f w h T r t t K x 8 P V 9 f p W f S b k t t J Y K O v e Q j r D 1 Z 8 0 V C y L X S t M g j H 9 x o w U 0 C i Y 5 L l P M 8 N T Y G M 4 4 3 2 n J u D m G t j Z A u Z k w y E h G S n t T o J k h l 6 N s B A b M 4 0 D 5 1 n w Y 2 7 a C v B f t n 6 G o w 8 D K 5 I 0 c x y x e a F R J g k q U m w z C Y X m D O X U K c C 0 c L 0 S F k F B m t t 5 3 5 H Q u D n y b e V 4 Z 6 v x b q v R 3 V n f q y / o W P a e e y + 8 q t f w 3 n t 7 3 h e v 4 x 1 5 r L R b Y i V Z i s u f y 1 F Z l S d z 1 9 L S I u a Z d 0 3 K P / 4 A C 4 4 0 K g = = &lt; / l a t e x i t &gt; LayerNorm &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " F o A H g 4 H M S R c R a y f j k 7 h i / q D 5 h A w = " &gt; A A A D w H i c v V J b a 9 s w F H a T X T r v 0 n Z 7 3 I t Y U 0 j Y 2 i b d l U G h W y C s t I R k W 9 t A l I Z j W a l F Z M u R j t t m w n 9 y D 4 P 9 m 8 l J B r 3 s e Q c k H 3 3 n / v k E q R Q G 6 / X f S 6 X y n b v 3 7 i 8 / 8 B 8 + e v x k Z X X t 6 b F R m W b 8 i C m p d C 8 A w 6 V I + B E K l L y X a g 5 x I P l J M G 4 W 9 p N z r o 1 Q y X e c p n w Q w 1 k i R o I B O m i 4 t v R r o 0 J j w I i B t J / y 6 s G r b o 3 s E o r 8 E u 0 3 N c I Y L n M q + Q i r N I 0 g Q R V b q j P J + 6 / r K Q 7 s G 3 f n 9 q 2 7 u g e n F F V K t q m Z a L T h c J z n V I u z C G s V S v 1 r Z a h I Z q 8 g s F / z U 9 u m K G J u S D u f e 3 b d Z x 4 S A d q D f J Z 4 m y J k l b 9 g d w 5 W r u c d 2 n n j v W b + v 0 a 5 W v L G W C 5 u M V g 4 n O S u V a a V M Z t M n Y M W k D B O q r 1 m j Q A i T 4 q / 8 d H 3 N w 6 V S 0 s 6 g C w i + w l y D a w w k e p h Z 7 / m K r 9 0 a V q c h 5 s t p S 9 A h 6 T N 8 U L p M a m 2 W u 2 a f w h T r t t K x 8 P V 9 f p W f S b k t t J Y K O v e Q j r D 1 Z 8 0 V C y L X S t M g j H 9 x o w U 0 C i Y 5 L l P M 8 N T Y G M 4 4 3 2 n J u D m G t j Z A u Z k w y E h G S n t T o J k h l 6 N s B A b M 4 0 D 5 1 n w Y 2 7 a C v B f t n 6 G o w 8 D K 5 I 0 c x y x e a F R J g k q U m w z C Y X m D O X U K c C 0 c L 0 S F k F B m t t 5 3 5 H Q u D n y b e V 4 Z 6 v x b q v R 3 V n f q y / o W P a e e y + 8 q t f w 3 n t 7 3 h e v 4 x 1 5 r L R b Y i V Z i s u f y 1 F Z l S d z 1 9 L S I u a Z d 0 3 K P / 4 A C 4 4 0 K g = = &lt; / l a t e x i t &gt; LayerNorm &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " j t d b b q 1 u m a F i w 3 s S 5 i d I F z H K L a g = " &gt; A A A E a n i c v V N b a 9 s w F H a T b O u 8 d W s 7 2 B h 9 E W s C C W 3 S p L u + D F o C Y S U h J O s t E K d B l p V E R J Z c 6 b h t M I b 9 x r 3 t F + x l P 2 J y n L H e 2 O M E l o 4 / f e f 2 H e Q G n G m o V n 8 s Z b K 5 B w 8 f L T + 2 n z x d e f Z 8 d W 3 9 R M t Q E X p M J J e q 5 2 J N O R P 0 G B h w 2 g s U x b 7 L 6 a k 7 r S f 3 p x d U a S b F E c w C O v D x W L A R I x g M N F z L f C v k H R / D h G A e 7 c f F 5 n a 3 h D 4 j B + g V R I d y B D 6 + i h 1 O R 1 B 0 g g k W I P 3 I U S G n / b f V A A b R O 7 P H 0 X u z d Z t n D s g A 7 T j 6 X E H k D a d x 7 C g 2 n k A p 7 z j 2 j T Q O E / M / 1 4 2 + x m d R 2 w H m U 4 3 a c c r s m i N 1 m W C I m v E 8 8 I 4 D O M z / A b s p m L 8 Z d x i l h f f q 8 f 9 q 5 X r K W 2 0 Z v 0 V j 3 v A 8 N q X W l d S 6 X J c X W D E s C E X 7 A F Q k g 0 D F X n 2 / Z B 9 S P i r j v + A J 1 p d Y M E Q B Y V 4 p 2 X a h J U 1 a 1 M F A J u h A A F W Y p N R W 5 6 B k K t s y a R q U e u W G V J d Y e a h N 4 V K q K S o 2 G m 1 D a O E Z V W 2 p f L v g c k m m Z Y / h s R Q m p i n A L j A R h I B A T q n Q J l o r r d / E 7 N X Z E e K J 8 z a 6 B u e b y D S N 7 h t m o l r e k L v o H x Q j y 3 B 1 s 1 q p z h e 6 a 9 Q W x q a 1 W J 3 h 6 n f H k y T 0 j U i E Y 6 3 7 t f n 4 s A J G O I 1 t J 9 Q 0 w G S K x 7 R v T I F N q k E 0 f y o x K h j E Q y O p z C c A z d H r H h H 2 t Z 7 5 r m E m B e v b d w l 4 3 1 0 / h N G n Q T T X j w q S J h q F 3 G i J k n e H P K Y o A T 4 z B i a K m V o R m e B k f O Z 1 2 k a E 2 u 2 W 7 x o n u 5 X a h 0 q t u 7 u 5 t 7 2 Q Y 9 n a s N 5 Y R a t m f b T 2 r C 9 W x z q 2 S O Z n d i X 7 M v s q + y u 3 n n u d 2 0 i p m a W F z w v r x s r l f w P J 7 G t m &lt; / l a t e x i t &gt; Self-attention (Vaswani et al.) &lt; l a t e x i t s h a 1 _ b a s e 6 4 = " 9 e d z z R L 4 u M W n I O w + v c y P E h 0 6 C + k = " &gt; A A A E a n i c v V N b a 9 s w F H a T b O u 8 d W s 7 2 B h 9 E a s D C W v S p L u + D F o C Y a U h J O t l g S g N s q w k I r L l S s d t g z H s N + 5 t v 2 A v + x G T 4 4 z 1 x h 4 n s H T 8 6 T u 3 7 y A 3 F F x D r f Z j K Z c v 3 L v / Y P m h / e j x y p O n q 2 v r J 1 p G i r J j K o V U P Z d o J n j A j o G D Y L 1 Q M e K 7 g n 1 1 p 4 3 0 / u s 5 U 5 r L 4 A h m I R</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 1 :</head><label>1</label><figDesc>z 6 x g y I S T W I 5 0 8 l Q U W D e G g k l f k C Q H P 0 q k d M f K 1 n v m u Y a c H 6 5 l 0 K 3 n X X j 2 D 0 c R D P 9 W M B z R K N I m G 0 R O m 7 Q x 5 X j I K Y G Y N Q x U 2 t i E 5 I O j 7 z O m 0 j Q v 1 m y 7 e N k 5 1 q / X 2 1 3 t 3 Z 3 N 1 a y L F s b V i v r J J V t z 5 Y u 9 Z n q 2 M d W z T 3 M 7 + S f 5 5 / k f 9 V W C + 8 L G x k 1 N z S w u e Z d W 0 V n N / Q h m t m &lt; / l a t e x i t &gt;Cross-Covariance Attention (XCA) Our XCiT layer consists of three main blocks, each preceded by LayerNorm and followed by a residual connection: (i) the core cross-covariance attention (XCA) operation, (ii) the local patch interaction (LPI) module, and (iii) a feed-forward network (FFN). By transposing the query-key interaction, the computational complexity of XCA is linear in the number of data elements N , rather than quadratic as in conventional self-attention.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Transformers for high-resolution images. Several works adopt visual transformers to highresolution image tasks beyond image classification, such as object detection and image segmentation. Wang et al.<ref type="bibr" target="#b70">[71]</ref> design a model with a pyramidal architecture and address complexity by gradually reducing the spatial resolution of keys and values. Similarly, for video recognition Fan et al.<ref type="bibr" target="#b23">[24]</ref> utilize pooling to reduce the resolution across the spatial and temporal dimensions to allow for an efficient computation of the attention matrix. Zhang et al.<ref type="bibr" target="#b80">[81]</ref> adopt global tokens and local attention to reduce the model complexity, while Liu et al.<ref type="bibr" target="#b43">[44]</ref> provide an efficient method for local attention with shifted windows. In addition, Zheng et al.<ref type="bibr" target="#b82">[83]</ref> and Ranftl et al.<ref type="bibr" target="#b53">[54]</ref> study problems like semantic segmentation and monocular depth estimation with the quadratic self-attention operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 2 :</head><label>2</label><figDesc>Inference memory usage of vision transformer variants. Our XCiT models scale linearly in the number of tokens, which makes it possible to scale to much larger image sizes, even in comparison to approaches employing approximate self-attention or a pyramidal design. All measurements are performed with a batch size of 64 on a single V100-32GB GPU.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Figure 4shows that each head learns to focus on semantically coherent parts of the image, while being flexible to change what type of features it attends to based on the image content.Complexity analysis. The usual token self-attention with h heads has a time complexity of O(N 2 d) and memory complexity of O(hN 2 +N d). Due to the quadratic complexity, it is problematic to scale token self-attention to images with a large number of tokens. Our cross-covariance attention overcomes this drawback as its computational cost of O(N d 2 /h) scales linearly with the number of tokens, as does the memory complexity of O(d 2 /</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 4 :</head><label>4</label><figDesc>Visualization of the attention map between the CLS token and individual patches in the classattention stage. For each column, each row represents the attention map w.r.t. one head, corresponding to the image in the first raw. Each head seems salient to semantically coherent regions. Heads are sensitive to similar features within the same or across images (e.g. people or bird faces). They trigger on different concepts when such features are missing (e.g., cockpit for race cars).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure A. 1 :</head><label>1</label><figDesc>Impact of input resolution on accuracy for DeiT-S. We consider different image resolutions, and either (1) increase the patch size while keeping the number of tokens fixed; or (2) keep the patch size fixed and use more tokens. Larger input images are beneficial if the number of tokens increases. The impact of a change of a resolution for a constant number of patches (of varying size) is almost neutral. As one can observe, the main driver of performance is the number of patches. The patch size has a limited impact on the accuracy, except when considering very small ones. We have observed and confirmed similar trends with XCiT models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure D. 1 :</head><label>1</label><figDesc>We present the millisecond per image during inference of multiple models. Our XCiT-S12/16 model provides a speed up for images with higher resolution compared to existing vision transformers, especially the ones with quadratic complexity like DeiT and CaiT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Figure D. 2 :</head><label>2</label><figDesc>Visualization of the queriesQ and keysK norm across the feature dimension. We empirically observe that magnitude of patch embeddings in the queries and keys correlates with the saliency of their corresponding region in the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>ImageNet classification. Number of parameters, FLOPs, image resolution, and top-1 accuracy on ImageNet-1k and ImageNet-V2. Training strategies vary across models, transformer-based models and the reported RegNet mostly follow recipes from DeiT<ref type="bibr" target="#b64">[65]</ref>.</figDesc><table><row><cell>Model</cell><cell cols="4">#params FLOPs Res. ImNet V2</cell></row><row><cell cols="2">EfficientNet-B5 RA [18] 30M</cell><cell>9.9B 456</cell><cell>83.7</cell><cell>_</cell></row><row><cell>RegNetY-4GF [53]</cell><cell>21M</cell><cell>4.0B 224</cell><cell cols="2">80.0 72.4</cell></row><row><cell>DeiT-S? [65]</cell><cell>22M</cell><cell>4.6B 224</cell><cell cols="2">81.2 68.5</cell></row><row><cell>Swin-T [44]</cell><cell>29M</cell><cell>4.5B 224</cell><cell>81.3</cell><cell>_</cell></row><row><cell>CaiT-XS24? ? [68] XCiT-S12/16?</cell><cell>26M 26M</cell><cell>19.3B 384 4.8B 224</cell><cell cols="2">84.1 74.1 83.3 72.5</cell></row><row><cell>XCiT-S12/16? ? XCiT-S12/8? ?</cell><cell>26M 26M</cell><cell>14.3B 384 55.6B 384</cell><cell cols="2">84.7 74.1 85.1 74.8</cell></row><row><cell cols="2">EfficientNet-B7 RA [18] 66M</cell><cell>37.0B 600</cell><cell>84.7</cell><cell>_</cell></row><row><cell>NFNet-F0 [10]</cell><cell>72M</cell><cell>12.4B 256</cell><cell cols="2">83.6 72.6</cell></row><row><cell>RegNetY-8GF [53]</cell><cell>39M</cell><cell>8.0B 224</cell><cell cols="2">81.7 72.4</cell></row><row><cell>TNT-B [79]</cell><cell>66M</cell><cell>14.1B 224</cell><cell>82.8</cell><cell>_</cell></row><row><cell>Swin-S [44]</cell><cell>50M</cell><cell>8.7B 224</cell><cell>83.0</cell><cell>_</cell></row><row><cell>CaiT-S24? ? [68] XCiT-S24/16?</cell><cell>47M 48M</cell><cell>32.2B 384 9.1B 224</cell><cell cols="2">85.1 75.4 83.9 73.3</cell></row><row><cell>XCiT-S24/16? ? XCiT-S24/8? ?</cell><cell cols="2">48M 48M 105.9B 384 26.9B 384</cell><cell cols="2">85.1 74.6 85.6 75.7</cell></row><row><cell cols="2">Fix-EfficientNet-B8 [66] 87M</cell><cell>89.5B 800</cell><cell cols="2">85.7 75.9</cell></row><row><cell>RegNetY-16GF [53]</cell><cell>84M</cell><cell>16.0B 224</cell><cell cols="2">82.9 72.4</cell></row><row><cell>Swin-B? [44]</cell><cell>88M</cell><cell>47.0B 384</cell><cell>84.2</cell><cell>_</cell></row><row><cell>DeiT-B? ? [65] CaiT-S48? ? [68] XCiT-M24/16?</cell><cell>87M 89M 84M</cell><cell>55.5B 384 63.8B 384 16.2B 224</cell><cell cols="2">85.2 75.2 85.3 76.2 84.3 73.6</cell></row><row><cell>XCiT-M24/16? ? XCiT-M24/8? ?</cell><cell cols="2">84M 84M 187.9B 384 47.7B 384</cell><cell cols="2">85.4 75.1 85.8 76.1</cell></row><row><cell>NFNet-F2 [10]</cell><cell>194M</cell><cell>62.6B 352</cell><cell cols="2">85.1 74.3</cell></row><row><cell>NFNet-F3 [10]</cell><cell cols="2">255M 114.8B 416</cell><cell cols="2">85.7 75.2</cell></row><row><cell>CaiT-M24? ? [68] XCiT-L24/16?</cell><cell cols="2">186M 116.1B 384 189M 36.1B 224</cell><cell cols="2">85.8 76.1 84.9 74.6</cell></row><row><cell>XCiT-L24/16? ? XCiT-L24/8? ?</cell><cell cols="2">189M 106.0B 384 189M 417.8B 384</cell><cell cols="2">85.8 75.8 86.0 76.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Self-supervised learning. Top-1 acc. on ImageNet-1k. Wwe report with a crop-ratio 0.875 for consistency with DINO. For the last row it is set to 1.0 (improves from 80.7% to 80.9%). All models are trained for 300 epochs.</figDesc><table><row><cell cols="2">SSL Method Model</cell><cell>#params</cell><cell cols="2">FLOPs Linear k-NN</cell></row><row><cell>MoBY [76]</cell><cell>Swin-T [44]</cell><cell>29M</cell><cell>4.5B 75.0</cell><cell>-</cell></row><row><cell>DINO [12]</cell><cell>ResNet-50 [28]</cell><cell>23M</cell><cell>4.1B 74.5</cell><cell>65.6</cell></row><row><cell>DINO [12]</cell><cell>ViT-S/16 [22]</cell><cell>22M</cell><cell>4.6B 76.1</cell><cell>72.8</cell></row><row><cell>DINO [12]</cell><cell>ViT-S/8 [22]</cell><cell>22M</cell><cell>22.4B 79.2</cell><cell>77.2</cell></row><row><cell>DINO [12]</cell><cell>XCiT-S12/16</cell><cell>26M</cell><cell>4.9B 77.8</cell><cell>76.0</cell></row><row><cell>DINO [12]</cell><cell>XCiT-S12/8</cell><cell>26M</cell><cell>18.9B 79.2</cell><cell>77.1</cell></row><row><cell>DINO [12]</cell><cell>ViT-B/16 [22]</cell><cell>87M</cell><cell>17.5B 78.2</cell><cell>76.1</cell></row><row><cell>DINO [12]</cell><cell>ViT-B/8 [22]</cell><cell>87M</cell><cell>78.2B 80.1</cell><cell>77.4</cell></row><row><cell>DINO [12]</cell><cell>XCiT-M24/16</cell><cell>84M</cell><cell>16.2B 78.8</cell><cell>76.4</cell></row><row><cell>DINO [12]</cell><cell>XCiT-M24/8</cell><cell>84M</cell><cell>64.0B 80.3</cell><cell>77.9</cell></row><row><cell>DINO [12]</cell><cell>XCiT-M24/8?384</cell><cell>84M</cell><cell>188.0B 80.9</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Backbone #params AP b AP b 50 AP b 75 AP m AP m 50 AP m 75</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table A</head><label>A</label><figDesc></figDesc><table><row><cell cols="3">.1: ImageNet Top-1 accuracy of efficient self-attention variants (after 300 epochs of training).</cell></row><row><cell>Model</cell><cell cols="2">Complexity Top-1</cell></row><row><cell>DeiT-S [65]</cell><cell>O(N 2 )</cell><cell>79.9</cell></row><row><cell>SRA (Average Pool) [71]</cell><cell>O(N 2 /R 2 )</cell><cell>73.5</cell></row><row><cell>SRA (Convolutional) [71] Linformer (k= ? n) [70]</cell><cell>O(N 2 /R 2 ) O(kN )</cell><cell>74.0 75.7</cell></row><row><cell>Efficient Transformer [56] Axial [30]</cell><cell>O(N ) O(N ? N )</cell><cell>76.3 78.4</cell></row><row><cell>I</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table A</head><label>A</label><figDesc></figDesc><table><row><cell cols="3">.2: Trade-off between train and test resolutions for DeiT. MS refers to multi-scale training, where the models</cell></row><row><cell cols="2">have seen images from different resolutions at training time.</cell></row><row><cell cols="2">Test / Train 160 224 256 288 320</cell><cell>MS</cell></row><row><cell>160</cell><cell cols="2">77.2 75.9 73.3 68.2 59.6 76.3</cell></row><row><cell>224</cell><cell cols="2">78.0 79.9 79.9 79.0 77.9 79.6</cell></row><row><cell>256</cell><cell cols="2">77.3 80.4 80.7 80.2 79.9 80.6</cell></row><row><cell>288</cell><cell cols="2">76.3 80.4 81.0 81.2 80.8 81.0</cell></row><row><cell>320</cell><cell cols="2">75.0 80.1 80.9 81.3 81.5 81.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table B .</head><label>B</label><figDesc>We provide a PyTorch-style pseudo code of the Cross-covariance attention operation. The pseudo code resembles the Timm library<ref type="bibr" target="#b71">[72]</ref> implementation of token self-attention. We II show that XCA only requires few modifications, namely the 2 normalization, setting the learnable temperature parameters and a transpose operation of the keys, queries and values. Pseudocode of XCA in a PyTorch-like style.</figDesc><table><row><cell cols="2">Algorithm 1 # self.qkv: nn.Linear(dim, dim * 3, bias=qkv_bias)</cell><cell></cell><cell></cell></row><row><cell cols="3"># self.temp: nn.Parameter(torch.ones(num_headss, 1, 1))</cell><cell></cell></row><row><cell>def forward(self, x):</cell><cell></cell><cell></cell><cell></cell></row><row><cell>B, N, C = x.shape</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)</cell></row><row><cell>qkv = qkv.permute(2, 0, 3, 1, 4)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">q, k, v = qkv[0], qkv[1], qkv[2] # split into query, key and value</cell></row><row><cell>q = q.transpose(-2, -1)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">k = k.transpose(-2, -1) # Transpose to shape (B, h, C, N)</cell><cell></cell></row><row><cell>v = v.transpose(-2, -1)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">q = F.normalize(q, dim=-1, p=2) # L2 Normalization across the token dimension</cell></row><row><cell>k = F.normalize(k, dim=-1, p=2)</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">attn = (k @ q.transpose(-2, -1)) # Computing the block diagonal cross-covariance matrix</cell></row><row><cell cols="4">attn = attn * self.temp # Adjusting the activations scale with temperature parameter</cell></row><row><cell cols="2">attn = attn.softmax(dim=-1) # d x d attention map</cell><cell></cell><cell></cell></row><row><cell cols="3">x = attn @ v # Apply attention to mix channels per token</cell><cell></cell></row><row><cell cols="2">x = x.permute(0, 3, 1, 2).reshape(B, N, C)</cell><cell></cell><cell></cell></row><row><cell>x = self.proj(x)</cell><cell></cell><cell></cell><cell></cell></row><row><cell>return x</cell><cell></cell><cell></cell><cell></cell></row><row><cell>D Additional results</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Layer-</cell></row><row><cell>Scale initialization .</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Patch size</cell><cell>d r</cell><cell></cell></row><row><cell>XCiT-N12</cell><cell>8 &amp; 16</cell><cell>0.0</cell><cell>1.0</cell></row><row><cell>XCiT-T12</cell><cell>8 &amp; 16</cell><cell>0.0</cell><cell>1.0</cell></row><row><cell>XCiT-T24</cell><cell>8 &amp; 16</cell><cell cols="2">0.05 10 ?5</cell></row><row><cell>XCiT-S12</cell><cell>8 &amp; 16</cell><cell>0.05</cell><cell>1.0</cell></row><row><cell>XCiT-S24 XCiT-M24 XCiT-L24 XCiT-L24</cell><cell>8 &amp; 16 8 &amp; 16 16 8</cell><cell cols="2">0.1 10 ?5 0.15 10 ?5 0.25 10 ?5 0.3 10 ?5</cell></row><row><cell>C Pseudo-code</cell><cell></cell><cell></cell><cell></cell></row></table><note>1: Hyperparameters used for training our models, including the Stochastic depth drop rate dr and</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table D .</head><label>D</label><figDesc></figDesc><table><row><cell>Models</cell><cell>Depth</cell><cell>d</cell><cell cols="2">#Blocks params</cell><cell cols="2">P = (16 ? 16)</cell><cell></cell><cell cols="2">P = (8 ? 8)</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="6">GFLOPs @224 @224? @384? GFLOPs @224 @224? @384?</cell></row><row><cell>XCiT-N12</cell><cell>12</cell><cell>128</cell><cell>4</cell><cell>3M</cell><cell>0.5 69.9</cell><cell>72.2</cell><cell>75.4</cell><cell>2.1 73.8</cell><cell>76.3</cell><cell>77.8</cell></row><row><cell>XCiT-T12</cell><cell>12</cell><cell>192</cell><cell>4</cell><cell>7M</cell><cell>1.2 77.1</cell><cell>78.6</cell><cell>80.9</cell><cell>4.8 79.7</cell><cell>81.2</cell><cell>82.4</cell></row><row><cell>XCiT-T24</cell><cell>24</cell><cell>192</cell><cell>4</cell><cell>12M</cell><cell>2.3 79.4</cell><cell>80.4</cell><cell>82.6</cell><cell>9.2 81.9</cell><cell>82.6</cell><cell>83.7</cell></row><row><cell>XCiT-S12</cell><cell>12</cell><cell>384</cell><cell>8</cell><cell>26M</cell><cell>4.8 82.0</cell><cell>83.3</cell><cell>84.7</cell><cell>18.9 83.4</cell><cell>84.2</cell><cell>85.1</cell></row><row><cell>XCiT-S-24</cell><cell>24</cell><cell>384</cell><cell>8</cell><cell>48M</cell><cell>9.1 82.6</cell><cell>83.9</cell><cell>85.1</cell><cell>36.0 83.9</cell><cell>84.9</cell><cell>85.6</cell></row><row><cell>XCiT-M24</cell><cell>24</cell><cell>512</cell><cell>8</cell><cell>84M</cell><cell>16.2 82.7</cell><cell>84.3</cell><cell>85.4</cell><cell>63.9 83.7</cell><cell>85.1</cell><cell>85.8</cell></row><row><cell>XCiT-L24</cell><cell>24</cell><cell>768</cell><cell>16</cell><cell>189M</cell><cell>36.1 82.9</cell><cell>84.9</cell><cell>85.8</cell><cell>142.2 84.4</cell><cell>85.4</cell><cell>86.0</cell></row></table><note>1: ImageNet-1k top-1 accuracy of XCiT for additional combinations of image and patch sizes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table D .2: Evaluation on transfer learning.</head><label>D</label><figDesc>ArchitectureCIFAR 10 CIFAR 100 Flowers102 Cars iNat 18 iNat 19</figDesc><table><row><cell>EfficientNet-B7 [58]</cell><cell>98.9</cell><cell>91.7</cell><cell>98.8</cell><cell>94.7</cell><cell>_</cell><cell>_</cell></row><row><cell>ViT-B/16 [22]</cell><cell>98.1</cell><cell>87.1</cell><cell>89.5</cell><cell>_</cell><cell>_</cell><cell>_</cell></row><row><cell>ViT-L/16 [22]</cell><cell>97.9</cell><cell>86.4</cell><cell>89.7</cell><cell>_</cell><cell>_</cell><cell>_</cell></row><row><cell>Deit-B/16 [65] ?</cell><cell>99.1</cell><cell>91.3</cell><cell>98.8</cell><cell>92.9</cell><cell>73.7</cell><cell>78.4</cell></row><row><cell>XCiT-S24/16 ?</cell><cell>99.1</cell><cell>91.2</cell><cell>97.4</cell><cell>92.8</cell><cell>68.8</cell><cell>76.1</cell></row><row><cell>XCiT-M24/16 ?</cell><cell>99.1</cell><cell>91.4</cell><cell>98.2</cell><cell>93.4</cell><cell>72.6</cell><cell>78.1</cell></row><row><cell>XCiT-L24/16 ?</cell><cell>99.1</cell><cell>91.3</cell><cell>98.3</cell><cell>93.7</cell><cell>75.6</cell><cell>79.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table D .3: The basic statistics on the image retrieval datasets.</head><label>D</label><figDesc></figDesc><table><row><cell></cell><cell cols="3">number of images nb of instances</cell></row><row><cell>Dataset</cell><cell cols="2">database queries</cell><cell></cell></row><row><cell>Holidays</cell><cell>1491</cell><cell>500</cell><cell>500</cell></row><row><cell>R-Oxford</cell><cell>4993</cell><cell>70</cell><cell>26</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table D .</head><label>D</label><figDesc>our XCiT small model compared to other models of comparable size that include token self-attention. All models tested using batch size of 64 on a V100 GPU with 32GB memory.</figDesc><table><row><cell>Model</cell><cell cols="2">#params ImNet</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Image Resolution</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>(?10 6 )</cell><cell>Top-1</cell><cell>224 2</cell><cell></cell><cell>384 2</cell><cell></cell><cell>512 2</cell><cell></cell><cell></cell><cell>1024 2</cell></row><row><cell></cell><cell></cell><cell>@224</cell><cell cols="8">im/sec mem (MB) im/sec mem (MB) im/sec mem (MB) im/sec mem (MB)</cell></row><row><cell>ResNet-50</cell><cell>25</cell><cell>79.0</cell><cell>1171</cell><cell>772</cell><cell>434</cell><cell>2078</cell><cell>245</cell><cell>3618</cell><cell>61</cell><cell>14178</cell></row><row><cell>DeiT-S</cell><cell>22</cell><cell>79.9</cell><cell>974</cell><cell>433</cell><cell>263</cell><cell>1580</cell><cell>116</cell><cell>4020</cell><cell>N/A</cell><cell>OOM</cell></row><row><cell>CaiT-S12</cell><cell>26</cell><cell>80.8</cell><cell>671</cell><cell>577</cell><cell>108</cell><cell>2581</cell><cell>38</cell><cell>7117</cell><cell>N/A</cell><cell>OOM</cell></row><row><cell>PVT-Small</cell><cell>25</cell><cell>79.8</cell><cell>777</cell><cell>1266</cell><cell>256</cell><cell>3142</cell><cell>134</cell><cell>5354</cell><cell>N/A</cell><cell>OOM</cell></row><row><cell>Swin-T</cell><cell>29</cell><cell>81.3</cell><cell>704</cell><cell>1386</cell><cell>220</cell><cell>3890</cell><cell>120</cell><cell>6873</cell><cell>29</cell><cell>26915</cell></row><row><cell>XCiT-S12/16</cell><cell>26</cell><cell>82.0</cell><cell>781</cell><cell>731</cell><cell>266</cell><cell>1372</cell><cell>151</cell><cell>2128</cell><cell>37</cell><cell>7312</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Q</cell><cell></cell><cell></cell><cell>K</cell><cell></cell><cell></cell><cell></cell></row></table><note>5: Inference throughput and peak GPU memory usage for</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">-Normalization and temperature scaling. In addition to building our attention operation on the cross-covariance matrix, we make a second modification compared to token selfattention. We restrict the magnitude of the query and key matrices by 2 -normalising them, such that each column of length N of the normalised matricesQ andK has unit norm, and every element in d?d cross-covariance matrixK Q is in the range [?1, 1]. We observed that controlling the norm strongly enhances the stability of training, especially when trained with a variable numbers of tokens. However, restricting the norm reduces the representational<ref type="bibr" target="#b0">1</ref> For C to represent the covariance, X should be centered, i.e. X1=0. For the relation between C and G, however, centering is not required.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We use report the results provided by the authors in their open-sourced code https://github.com/ SwinTransformer/Swin-Transformer-Object-Detection</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Etc: Encoding long and structured inputs in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaclav</forename><surname>Cvicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Vivit: A video vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lambdanetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08602</idno>
		<title level="m">Modeling long-range interactions without attention</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Longformer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">The long-document transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">MultiGrain: a unified image embedding for classes and instances</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Berman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.05509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Is space-time attention all you need for video understanding?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic filter networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>De Brabandere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.14794</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Total recall: Automatic query expansion with a generative feature model for object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">MMSegmentation: Openmmlab semantic segmentation toolbox and benchmark</title>
		<ptr target="https://github.com/open-mmlab/mmsegmentation" />
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Randaugment: Practical automated data augmentation with a reduced search space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10697</idno>
		<title level="m">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">RepMLP: Re-parameterizing convolutions into fully-connected layers for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guiguang</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01883</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Training vision transformers for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05644</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Multiscale vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11227</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end learning of deep visual representations for image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?r?me</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Levit: a vision transformer in convnet&apos;s clothing for faster inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.01136</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00112</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oisin</forename><forename type="middle">Mac</forename><surname>Grant Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Shepard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><forename type="middle">J</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06642</idno>
		<title level="m">The iNaturalist species classification and detection dataset</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perceiver</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03206</idno>
		<title level="m">General perception with iterative attention</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Hamming embedding and weak geometric consistency for large scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Aggregating local image descriptors into compact codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Transformers are RNNs: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Nikolaos Pappas, and Fran?ois Fleuret</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Panoptic feature pyramid networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>CIFAR</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lee-Thorp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Eckstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03824</idno>
		<title level="m">Fnet: Mixing tokens with fourier transforms</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Decoupled weight decay regularization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Melas-Kyriazi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.02723</idno>
		<title level="m">Do you even need attention? a stack of feed-forward layers does surprisingly well on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<meeting>the Indian Conference on Computer Vision, Graphics and Image Processing</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Blockwise self-attention for long document understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02972</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Revisiting oxford and paris: Large-scale image retrieval benchmarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><surname>Iscen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Giorgos Tolias, Yannis Avrithis, and Ond?ej Chum</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fine-tuning CNN image retrieval with no human annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Designing network design spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilija</forename><surname>Radosavovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raj</forename><forename type="middle">Prateek</forename><surname>Kosaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Kaiming He, and Piotr Doll?r</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Vision transformers for dense prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ren?</forename><surname>Ranftl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Bochkovskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.13413</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Do imagenet classifiers generalize to imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishaal</forename><surname>Shankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Efficient attention: Attention with linear complexities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Adaptive attention span in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07799</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Yfcc100m: The new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bart</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerald</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Image search with selective match kernels: aggregation across single and multiple images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Avrithis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Particular object retrieval with integral max-pooling of cnn activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Sicre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning and aggregating deep local descriptors for instance-level recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Jenicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ond?ej</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mlp-Mixer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.01601</idno>
		<title level="m">An all-MLP architecture for vision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Fixing the train-test resolution discrepancy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>J?gou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers and distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.08237</idno>
		<title level="m">Matthijs Douze, and Herv? J?gou. Fixing the train-test resolution discrepancy: Fixefficientnet</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alaaeldin</forename><surname>El-Nouby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Resmlp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03404</idno>
		<title level="m">Feedforward networks for image classification with data-efficient training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Alexandre Sablayrolles, Gabriel Synnaeve, and Herv? J?gou. Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Group normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Unified perceptual parsing for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Selfsupervised learning with swin transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenda</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuliang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.04553</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<title level="m" type="main">Nystr?mformer: A nystr?m-based algorithm for approximating self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyang</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanpeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudrasis</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glenn</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03902</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Incorporating convolution designs into visual transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaopeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aojun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11816</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token ViT: Training vision transformers from scratch on ImageNet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.14062</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<title level="m" type="main">Multi-scale vision longformer: A new vision transformer for high-resolution image encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15358</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Exploring self-attention for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<monogr>
		<title level="m" type="main">Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixiao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiatian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zekun</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15840</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Scene parsing through ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
