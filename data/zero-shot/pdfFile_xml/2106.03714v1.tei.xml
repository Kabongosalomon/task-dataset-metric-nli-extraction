<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Refiner: Refining Self-attention for Vision Transformers</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
							<email>zhoudaquan21@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
							<email>shi.yujun@u.nus.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
							<email>weihaoyu6@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Refiner: Refining Self-attention for Vision Transformers</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T11:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision Transformers (ViTs) have shown competitive accuracy in image classification tasks compared with CNNs. Yet, they generally require much more data for model pre-training. Most of recent works thus are dedicated to designing more complex architectures or training methods to address the data-efficiency issue of ViTs. However, few of them explore improving the self-attention mechanism, a key factor distinguishing ViTs from CNNs. Different from existing works, we introduce a conceptually simple scheme, called refiner, to directly refine the selfattention maps of ViTs. Specifically, refiner explores attention expansion that projects the multi-head attention maps to a higher-dimensional space to promote their diversity. Further, refiner applies convolutions to augment local patterns of the attention maps, which we show is equivalent to a distributed local attention-features are aggregated locally with learnable kernels and then globally aggregated with self-attention. Extensive experiments demonstrate that refiner works surprisingly well. Significantly, it enables ViTs to achieve 86% top-1 classification accuracy on ImageNet with only 81M parameters. Code is publicly available at https://github.com/zhoudaquan/Refiner_ViT.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent progress on image classification largely attributes to the development of vision transformers (ViTs) <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref>. Unlike convolutional neural networks (CNNs) that rely on convolutions (e.g., using 3 ? 3 kernels) to process features locally <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b64">65,</ref><ref type="bibr" target="#b57">58]</ref>, ViTs take advantage of the self-attention (SA) mechanism <ref type="bibr" target="#b50">[51]</ref> to establish the global-range relation among the image patch features (a.k.a. tokens) and aggregate them across all the spatial locations. Such global information aggregation nature of SA substantially increases ViT's expressiveness and has been proven to be more flexible than convolutions <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref> on learning image representations. However, it also leads ViTs to need extraordinarily larger amount of data for pre-training (e.g., JFT-300M) or much longer training time and stronger data augmentations than its CNN counterparts to achieve similar performance <ref type="bibr" target="#b47">[48]</ref>.</p><p>Most of recent works thus design more complex architectures or training methods to improve dataefficiency of ViTs <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b44">45]</ref>. However, few of them pay attention to the SA component of ViTs. Within a SA block in ViTs, each token is updated by aggregating the features from all tokens according to the attention maps, as shown in <ref type="figure" target="#fig_1">Fig. (1)</ref> <ref type="bibr">(b)</ref>. In this way, the tokens of a layer are able to sufficiently exchange information with each other and thus offer great expressiveness. However, this can also cause different tokens to become more and more similar, especially as the model goes deeper (more information aggregation). This phenomenon, also known as over-smoothing, has been identified by some recent studies <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b17">18]</ref> to largely degrade the performance of the ViTs.  The token-wise attention maps from vanilla self-attention of ViTs tend to be uniform, and thus they aggregate all the patch embeddings densely and generate overly-similar tokens. (c) Differently, our proposed refiner augments the attention maps into diverse ones with enhanced local patterns, such that they aggregate the token features more selectively and the resulting tokens are distinguishable from each other.</p><p>In this work, we explore to address the above limitation by directly refining their self-attention maps. Specifically, inspired by the recent works <ref type="bibr" target="#b48">[49]</ref> demonstrating that increasing the number of SA heads can effectively improve the model performance, we investigate how to promote the diversity of the attention maps using a similar strategy. However, given a transformer model with fixed embedding dimension, directly increasing the number of heads will reduce the number of embeddings allocated to each head, making the computed attention map less comprehensive and accurate as shown in Tab. 9 in <ref type="bibr" target="#b48">[49]</ref>. To address this dilemma, we explore attention expansion that linearly projects the multi-head attention maps to a higher-dimensional space spanned by a larger number of attention maps. As such, the number of attention heads (used for computing the attention maps) are implicitly increased without reducing the embedding dimension per head, enabling the model to enjoy both benefits from more SA heads and high embedding dimension.</p><p>Additionally, we argue that ignoring the local relationship among the tokens is another main cause of the above mentioned over-smoothing issue of global SA. The locality (local receptive fields) and spatial invariance (weight sharing) have been proven to be the key of the success of CNNs across many computer vision tasks <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b0">1]</ref>. Therefore, we explore how to leverage the convolution to augment the attention mechanism of ViTs. Some recent works have studied injecting convolutional locality into ViTs, e.g., integrating both convolution and global self-attention into a hybrid model <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b44">45]</ref>. However, they mostly consider directly applying convolution to the token features, keeping the convolutional block and the SA block separately. Differently, we explore introducing convolution to the attention maps directly to augment their spatial-context and local patterns, thus increasing their diversity. Such an approach, as we explain later, can be understood as a distributed local attention mechanism that combines both strengths of self-attention (global-range modeling) and convolution (reinforcing local patterns).</p><p>Based on the above two strategies, we introduce the refiner scheme to directly enhance self-attention maps within ViTs. Though being conceptually simple, it works surprisingly well. As shown in <ref type="figure" target="#fig_1">Fig. 1(c)</ref>, the attention map with refiner has preciser focus on the target object, compared to the vanilla one. By directly applying refiner to the SA module, the performance of the ViT-Base <ref type="bibr" target="#b18">[19]</ref> is improved by 1.7% on ImageNet under the same training recipes with negligible memory overhead (less than 1M). We have experimentally verified that this improvement is consistent when using more complex training recipes <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref> and the results are shown in Appendix. Surprisingly, refiner is also applicable for natural language processing tasks. On GLUE <ref type="bibr" target="#b51">[52]</ref>, it increases the average score by 1% over the state-of-the-art BERT <ref type="bibr" target="#b15">[16]</ref>.</p><p>In summary, our contributions are: First, we tackle the important low data-efficiency issue of ViTs from a new angle, i.e., directly refining the self-attention maps. We investigate a simple yet novel refiner scheme and find it works surprisingly well, even outperforming the state-of-the-art methods using sophisticated training recipes or leveraging extra models. Secondly, the introduced refiner makes several interesting modifications on the self-attention mechanism that would be inspiring for future works. Its attention expansion gives a new viewpoint for addressing the common dilemma on the trade-off between the number of attention heads and the embedding dimensions. Besides, different form the common practice that applies convolutions to features, it proves that convolutions can effectively augment attention maps as well and improve the model performance significantly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Transformers for vision tasks Transformers <ref type="bibr" target="#b50">[51]</ref> were originally developed for solving natural language processing tasks and achieved remarkable success <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref>. Different from other deep neural network architectures like CNNs and RNNs <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40]</ref>, transformers rely on the self-attention mechanism to perform the global interactions among features instead of relying on certain inductive biases (e.g., locality and weight sharing from CNNs). This grants transformers stronger capacity in learning from large-scale data. Those properties of transformers motivate researchers to explore its application on computer vision tasks, including image enhancement <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b58">59]</ref>, image classification <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b65">66]</ref>, object detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b63">64]</ref>, segmentation <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b63">64]</ref>, video processing <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b66">67]</ref> and 3D point cloud data processing <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b21">22]</ref>. Among them, Vision Transformer (ViT) <ref type="bibr" target="#b18">[19]</ref> is one of the early pure-transformer based models that achieve state-of-the-art performance on ImageNet. However, due to the larger model capacity, ViTs need extraordinarily large-scale datasets (e.g., ImageNet-22K and JFT-300M) for pre-training to achieve comparable performance with CNNs at similar model size. Some follow-up models thus try to resolve the low data-efficiency limitation either by modifying the model architecture <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49]</ref> or adopting new training techniques like knowledge distillation <ref type="bibr" target="#b47">[48]</ref>.</p><p>Injecting locality into ViTs Recent works find that injecting the convolution locality into transformer blocks can boost performance of ViTs. As initially proposed in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b44">45]</ref>, the hybrid ViT uses pre-trained CNN models to extract the patch embedding from the input images and then deploys multiple transformer blocks for feature processing. <ref type="bibr" target="#b31">[32]</ref> replaces the feed forward block in the transformer block with an inverted residual one <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b25">26]</ref> and <ref type="bibr" target="#b36">[37]</ref> propose to build two concurrent branches: one with convolution and the other one with transformer. However, they all leverage convolutions to strengthen the features. Besides, the interaction between the global information aggregation at the SA block and the local information extraction in the convolution block is not well explored. In this paper, we focus on enhancing the attention maps and explore how the global context and the local context can be combined in a novel way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Refiner</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries on ViT</head><p>The vision transformer (ViT) first tokenizes an input image by regularly partitioning it into a sequence of n small patches. Each patch is then projected into an embedding vector x i via a linear projection layer, with an additional learnable position embedding.</p><p>Then ViT applies multiple multi-head self-attention (MHSA) and feedforward layers to process the patch embeddings to model their long-range dependencies and evolve the token embedding features. Suppose the input tensor is X ? R din?n , the MHSA applies linear transformation with parameters W K , W Q , W V to embed them into the key K = W K X ? R d?n , query Q = W Q X ? R d?n and value V = W V X ? R d?n respectively. Suppose there are H self-attention heads. These embeddings are uniformly split into H segments Q h , K h , V h ? R d/H?n . Then the MHSA module computes the head-specific attention matrix (map) 1 A and aggregate the token value features as follows:</p><formula xml:id="formula_0">Attention(X, h) = A h V h with A h = Softmax Q h K h d/H , h = 1, . . . , H.<label>(1)</label></formula><p>Limitations of ViTs The ViT purely relies on the global self-attention to establish the relationship among all the tokens without deploying any spatial structure priors (or inductive biases). Though benefiting in learning flexible features from large-scale samples, such global-range attention may lead to overly-smoothed attention maps and token features as pointed out by <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b20">21]</ref>. We hypothesize this may slow the feature evolving speed of ViTs, i.e., the features change slowly when traversing the model blocks, and make the model perform inferior to the ones with faster feature evolving speed.</p><p>To verify this, we present a pilot experiment to compare the feature learning speed of ViTs with similar-size CNNs (ResNet) <ref type="bibr" target="#b24">[25]</ref> that incorporate strong locality regularization and DeiT <ref type="bibr" target="#b47">[48]</ref> which is a better-performing transformer model trained with sophisticated scheme.  <ref type="figure">Figure 2</ref>: The features of ViT evolves slower than ResNet <ref type="bibr" target="#b24">[25]</ref> and DeiT <ref type="bibr" target="#b47">[48]</ref> across the model blocks.</p><p>In <ref type="figure">Fig. 2</ref>, we plot the CKA similarity <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Attention Expansion</head><p>The MHSA of transformers captures different aspects of the input features <ref type="bibr" target="#b50">[51]</ref> by projecting them into different subspaces and computing self-attention therein. Thus, increasing the number of attention heads within MHSA, which is shown to be effective at improving the model performance <ref type="bibr" target="#b48">[49]</ref>, can potentially increase diversity among the attention maps. However, as mentioned above, if naively adding more SA heads, for a model with fixed hidden dimension, it is difficult to trade-off the benefit of having more SA heads and the harm of reducing the embedding dimension per head.</p><p>To resolve this issue, instead of directly expanding the attention heads, we investigate expanding the number of self-attention maps via linear transformation. Concretely, we use a linear projection</p><formula xml:id="formula_1">W A ? R H ?H with H &gt; H to project the attention maps A = [A 1 ; . . . ; A H ] to a new set of attention maps? = [? 1 , . . . ,? H ] with? h = H i=1 W A (h, i) ? A i , h = 1, .</formula><p>. . , H . Then we use the new attention maps? to aggregate features as in Eqn. (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The above attention expansion implicitly increases the number of SA heads but does not sacrifice the embedding dimensions. To understand how it works, recall each of the original SA map A h is computed by</head><formula xml:id="formula_2">A h = Q h K h .</formula><p>Here we omit the softmax and normalizing factor for illustration simplicity. We use w i to denote W A (h, i). Thus, the up-projected attention map is computed b?</p><formula xml:id="formula_3">A h = H i=1 w i ? Q i K i = [w 1 ? Q 1 , . . . , w H ? Q H ] K.</formula><p>Different from the vanilla MHSA that divides Q, K into H sub-matrices for SA computation, the attention expansion reweighs the Q features with learnable scalars and uses the complete reweighed Q to compute the SA maps. Getting rid of the feature division, the attention expansion effectively solves the above issue. When only one weight scalar is non-zero, it degenerates to the vanilla MHSA. The attention expansion is similar to the talking-heads attention <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b65">66]</ref> which are proven effective in improving attention diversity but differently, it expands the number of attention maps while talking-heads does not change the number. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Distributed Local Attention</head><p>Our second strategy is to investigate how convolutions can help enhance self-attention maps since convolutions are good at strengthening local patterns. Different from some earlier trials on introducing convolutions to process features in ViTs <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b36">37]</ref>, we explore directly applying convolutions to the self-attention maps.</p><p>Given the pre-computed self-attention matrix A h from head h, we apply a learnable convolution kernel w ? R k?k of size k ? k to modulate it as follows:</p><formula xml:id="formula_4">A h * i,j = k a,b=1 w a,b ? A h i? k 2 +a,j? k 2 +b .</formula><p>Then the standard SA-based feature aggregation as Eqn. <ref type="formula" target="#formula_0">(1)</ref> is conducted with this new attention matrix? i = n j=1? * i,j ? v j . Though being conceptually simple, the above operation establishes an interesting synergy between the global context aggregation (with self-attention) and local context modeling (with convolution). To see this, consider applying 1D convolution w of length k to obtain the convolution-augmented SA matrix, with which the feature aggregation becomes</p><formula xml:id="formula_5">v i = n j=1 A h * i,j ? v j = n j=1 k a=1 w a ? A h i,j? k 2 +a ? v j = n j=1 k a=1 w a ? A h i,j? k 2 +a ? v j = n j=1 k a=1 A h i,j ? w a ? v j?a+ k 2 .</formula><p>The above clearly shows that the feature aggregation based on the convolution-processed attention matrix is equivalent to: (1) applying the convolution w, with a location-specific reweighing scalar A h i,j , to aggregate features locally at first and (2) summing over the locally aggregated features. Therefore, we name such an operation as distributed local attention (DLA).</p><p>We empirically find DLA works pretty well in the experiments. We conjecture that DLA effectively combines strengths of the two popular feature aggregation schemes. Convolutional schemes are strong at capturing local patterns but inefficient in global-range modeling. Moreover, stacking multiple convolutions may suffer the increasingly center-biased receptive field <ref type="bibr" target="#b34">[35]</ref>, leading the model to ignore the features at the image boundary. In contrast, self-attention based feature aggregation can effectively model the global-range relation among features but suffer the over-smoothing issue. DLA can effectively overcome their limitations and better model the local and global context jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Refiner and Refined-ViT</head><p>The attention expansion and distributed local attention can be easily implemented via 1 ? 1 and 3 ? 3 convolutions. <ref type="figure" target="#fig_2">Fig. 3</ref> illustrates the refiner architecture. Different from the vanilla self-attention, refiner expands the attention maps at first and then applies head-wise convolution to process the maps. Refiner additionally introduces the attention reduction for reducing the model computation cost and maintaining the model embedding dimension. Specifically, similar to attention expansion, the attention reduction applies linear projection to reduce the number of attention maps from H to the original H, after the distributed local attention. Refiner can serve as a drop-in module to directly replace the vanilla self-attention in each ViT transformer block, giving the Refined-ViT model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experiment Setup</head><p>We mainly evaluate Refiner for the image classification task. Besides, we also conduct experiments on natural language processing tasks to investigate its generalizability to NLP transformer models. Computer vision We evaluate the effectiveness of the refiner on ImageNet <ref type="bibr" target="#b14">[15]</ref>. For a fair comparison with other methods, we first replace the SA module with refiner on ViT-Base <ref type="bibr" target="#b18">[19]</ref> model as it is the most frequently used one <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b48">49]</ref>. When comparing with other state-of-the-art methods, we modify the number of transformer blocks and the embedding dimensions to increase the efficiency in the same manner as <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b65">66]</ref>. Natural language processing We evaluate our model on the General Language Understanding Evaluation (GLUE) benchmark <ref type="bibr" target="#b51">[52]</ref>. GLUE benchmark includes various tasks which are formatted as single sentence or sentence pair classification. See Appendix for more details of all tasks. We measure accuracy for MNLI, QNLI, QQP, RTE, SST, Spearman correlation for STS and Matthews correlation for CoLA. The GLUE score is the average score for all the 8 tasks.</p><p>Training setup All the experiments are conducted upon PyTorch <ref type="bibr" target="#b35">[36]</ref> and the timm <ref type="bibr" target="#b55">[56]</ref> library. The models are trained on ImageNet-1k from scratch without auxiliary dataset. For the ablation experiments, we follow the standard training schedule and train our models on the ImageNet dataset for 300 epochs. When compared to state-of-the-art (SOTA) models, we use the advanced training recipes as proposed in <ref type="bibr" target="#b47">[48]</ref>. Detailed training hyper-parameters are listed in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Analysis</head><p>Refiner introduces attention expansion and convolutional local kernels to augment the self-attention maps of ViTs. Here we individually investigate their effectiveness through ablative studies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of attention expansion</head><p>We adopt 1 ? 1 convolution to adjust the number of attention maps. In this ablative study, we vary the expansion ratio r = H /H from 1 to 6. From the results in Tab. 1, we observe that along with increased expansion ratio (and more attention maps), the model performance monotonically increases from 82.3% to 83.0%. This clearly demonstrates the effectiveness of attention expansion in improving the model performance. Note when the expansion ratio equals to 1, the attention expansion degenerates to the talking-heads attention <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b65">66]</ref>. Compared with this strong baseline, increasing the ratio to 2 brings 0.5% top-1 accuracy improvement. Interestingly, using larger expansion ratio speeds up the convergence of model training. Using an expansion ratio of 6 saves the number of training epochs by nearly 13% (261 vs. 300 epochs). This   reflects that attention expansion can help the model learn the discriminative token features faster. However, when the expansion ratio is larger than 3, the benefits in the model accuracy attenuates. This motivates us to explore the distributed local attention for further performance enhancement.</p><p>Effect of attention reduction Refiner deploys another 1 ? 1 convolution to reduce the number of attention maps from H to the original number H, after the attention expansion and distributed local attention, in order to reduce the computation overhead due to expansion. We conduct experiments to study whether reduction will hurt model performance. As observed from Tab. 3, attention reduction drops the accuracy very marginally, implying the attention maps have been sufficiently augmented in the higher-dimension space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of distributed local attention</head><p>We then evaluate whether the distributed local attention (DLA) works consistently well across a broad spectrum of model architectures. We evaluate its effectiveness for various ViTs with 12 to 32 SA blocks, without the attention expansion. Following the common practice <ref type="bibr" target="#b18">[19]</ref>, the hidden dimension is 768 for the ViT-12B and 384 for all the other ViTs. We set the local attention window size as k = 3 and use the DLA to replace all the self-attention block within ViT. From the results summarized in Tab. 2, DLA can consistently boost the top-1 accuracy of various ViTs by 1.2% to 1.7% with negligible model size increase. Such significant performance boost clearly demonstrates its effectively and the benefits brought by its ability in jointly modeling the local and global context of input features. The combined effects of attention expansion and DLA are shown in Tab. 7. Effect of the local attention kernels Refiner directly applies the 3?3 convolution onto the attention maps A. We compare this natural choice with another two feasible choices for the local kernels. The first one is to apply spatially adjacent convolution on the reshaped attention maps such that the aggregated tokens by the local kernels are spatially adjacent in the original input. Specifically, we reshape each row of A into a ? n ? ? n matrix 2 which together form a ? n ? ? n ? n tensor and apply 3 ? 3 convolution. The second one is to apply the combination of row and column 1D convolution. From the results in Tab. 4, we find the spatially adjacent convolution will slightly hurt the performance while only adopting the 1D convolution leads to 1.3% accuracy drop. Directly applying convolution to the attention maps (as what refiner does) <ref type="bibr" target="#b1">2</ref> Recall n is the number of tokens and the input image is divided into ? n ? ? n patches.</p><p>gives the best performance, demonstrating the effectiveness of augmenting the local patterns of the attention maps.</p><p>Refiner augments attention maps and accelerates feature evolving We visualize the attention maps output from different self-attention blocks without and with the refiner in <ref type="figure" target="#fig_4">Fig. 5</ref>. Clearly, the attention maps after Refiner become not so uniform and present stronger local patterns. We also plot the feature evolving speed in <ref type="figure" target="#fig_3">Fig. 4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with SOTA</head><p>We compare refined-ViT with state-of-the-art models in Tab. 7. For small-sized model, with 224?224 test resolution, Refined-ViT-S (with only 25M parameters) achieves 83.6% accuracy on ImageNet, which outperforms the strong baseline DeiT-S by 3.7%. For medium-sized model with 384? 384 test resolution, the model Refined-ViT-M achieves the accuracy of 85.6%, which outperforms the latest SOTA CaiT-S36 by 0.2% with 13% less of parameters (55M vs. 68M) and outperforms LV-ViT-M by 1.6%. Note that CaiT uses knowledge distillation based method to improve their model, requiring much more computations. For large-sized model finetuned with input resolution of 448? 448, refined-ViT achieves 86%, better than CaiT-M36 with 70% less of parameters (81M vs. 271M). This is the new state-of-the-art results on ImageNet with less than 100M parameters. Notably, as shown in Tab. 7, our models significantly outperform the recent models that introduce locality into the token features across all the model sizes. This clearly demonstrates: first, refining the attention maps (and thus the feature aggregation mechanism) is more effective than augmenting the features for ViTs; secondly, jointly modeling the local and global context (from DLA in the refiner) is better than using the convolution and self-attention separately and deploying them into different blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Receptive field calibration</head><p>In this subsection, we present and investigate a simple approach that we find can steadily improve classification performance of ViT. The approach is a generic one that also works well for CNNs.</p><p>As pointed out in <ref type="bibr" target="#b49">[50]</ref>, pre-processing the training images and testing images separately could lead to a distribution shift between the training and testing regimes. This issue has been named by <ref type="bibr" target="#b49">[50]</ref> as the mismatch between the region of classification (RoC), which could degrade performance of the classification model. A common remedy solution is to apply random cropping to get a centrally cropped image patch for classification, with a certain cropping ratio (less than 1), at the testing phase. However, as the receptive field of a deep neural network at the deep layers is typically larger than the image size, we argue that it is not necessary to crop a patch from the testing images.</p><p>Instead, we propose to pad the testing image such that all the image features can be captured by the model. Different from directly feeding the image into the model, padding the image with zeros can align the informative region of an image to the center of the receptive filed of the model, similar to random cropping, while getting rid of information loss. Thus it can further improve the model accuracy. We name this padding method as receptive field calibration (RFC). RFC can be easily implemented by setting the cropping ratio to be larger than 1, which we call it RFC ratio, during the testing phase.</p><p>To investigate its effectiveness, among all the models whose pre-trained weights are publicly available, we apply RFC on the top 2 accurate CNN models and ViTs respectively. The results are summarized in Tab. 5. It is observed that RFC can easily further improve the performance of current SOTA model by 0.11% without fine-tuning. It is worth noting that the selected models are pre-trained with large auxiliary datasets such as ImageNet-22k and JFT-300M and the performance is nearly saturated. The testing image resolutions vary from 384 to 800. Under a variety of model configurations, RFC consistently improve the accuracy. With RFC, our proposed Refined-ViT achieves a new SOTA on the ImageNet (among models with less than 100M parameters).</p><p>It should be noted that in the above experiments <ref type="figure" target="#fig_2">(Sec. 4.2 and 4.3)</ref>, including comparing Refined-ViT with SOTAs, we did not apply RFC for fair comparisons. Here we would like to share such RFC technique as an interesting finding from our recent experiments. RFC is a generic and convenient technique to improve model's classification performance. We believe it is inspiring for rethinking the common random cropping techniques. More importantly, it motivates future studies to dig into the effect of the model's receptive field in classification performance and explore how to calibrate it w.r.t. the input images to gain better performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Applied to NLP tasks</head><p>We also evaluate the performance of Refiner-ViT models for natural language processing tasks on the GLUE benchmark, to investigate whether Refiner also improves other transformer-based models. We use the BERT-small <ref type="bibr" target="#b10">[11]</ref> as the baseline model and replace the self-attention module with refiner, using the same pre-training dataset and recipes. From the results in Tab. 6, Refiner boosts the model performance across all the tasks significantly and increases the average score by 1%, demonstrating Refiner is well generalizable to transformer-based NLP models to improve their attentions and final performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we introduced the refiner, a simple scheme that augments the self-attention of ViTs by attention expansion and distributed local attention. We find it works surprisingly well for improving performance of vision transformers (ViT). Furthermore, it also improves the performance of NLP transformers (BERT) by a large margin. Though refiner is limited to improving diversity of the selfattention maps, we believe its working mechanism is also inspiring for future works on understanding and further improving the self-attention mechanism.</p><p>Some interesting questions arise from the empirical observations but we leave them open for future studies. First, traditionally all the elements within the attention maps are normalized to be between 0 and 1. However, the attention expansion does not impose such a constraint while giving quite good results. It is worthy future studies on the effects of such subtraction attention (with negative attention weights) for feature aggregation. Second, the local distributed attention indeed imposes a locality inductive bias which is proven to be effective in enhancing learning efficiency of ViTs from medium-sized datasets (ImageNet). How to automatically learn the suitable inductive biases would be another interesting direction and some recent works have started to explore <ref type="bibr" target="#b13">[14]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A More implementation details A.1 Training hyper-parameters</head><p>For all the ablation experiments, we use the standard training recipe <ref type="bibr" target="#b55">[56]</ref> that is used for reproducing the ViT baselines. When comparing with other state-of-the-art (SOTA) methods, we adopt the advanced training recipe as proposed in <ref type="bibr" target="#b26">[27]</ref>. Their detailed configurations are shown in Tab. 8. We train all models with 8 NVIDIA Telsa-V100 GPUs. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Fine-tuning with larger image resolutions</head><p>On ImageNet, all models are trained from scratch with image resolution of 224 ? 224. When compare with other SOTA models, we fine-tune the models with larger image resolutions using the same fine-tuning procedures as adopted in <ref type="bibr" target="#b47">[48]</ref>. As larger image resolutions have more positional embeddings, we interpolate the positional encodings in the same manner as the one proposed in <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Model configurations of Refiner-ViT</head><p>As discussed in <ref type="bibr" target="#b59">[60]</ref>, a deep and narrow neural network typically is more efficient than a wide and shallow one. To verify this, we conduct a pair of experiments using these two architecture configurations respectively and the results are shown in Tab. 9. It is clearly observed that with comparable classification accuracy, a deep and narrow network takes 71% less number of parameters. As shown in <ref type="figure" target="#fig_3">Fig. 2 and Fig. 4</ref> in the main paper, we compare the similarity between the intermediate token features at the output of each block and the final layer, normalized by the similarity between the first layer's output features and the final output features as shown in Eqn. <ref type="formula" target="#formula_6">(2)</ref>: where f k denotes the token features at layer k, f in denotes the token features at the first transformer block and f out denotes the features at the final output of the model. The metric CKA is calculated with linear kernel function as proposed in <ref type="bibr" target="#b27">[28]</ref>. The batch size is set to be 32. The similarity score at the output of each block is an average of ten runs with randomly sampled image batches.</p><formula xml:id="formula_6">F k = CKA(f k , f out ) CKA(f in , f out ) ,<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More experiments B.1 Performance improvements of refiner under different training techniques</head><p>As shown in DeiT <ref type="bibr" target="#b47">[48]</ref>, more complex augmentations and fine-tuned training recipe could improve the performance of ViTs significantly without architecture changes. We run a set of experiments to show that the improvements brought by refiner is orthogonal to the training recipes to some extent. We select two training recipes for comparison. The first one uses the searched learning rate schedule and data augmentation policy as proposed in <ref type="bibr" target="#b47">[48]</ref>. The second one uses the more complicated patch processing as proposed in <ref type="bibr" target="#b59">[60]</ref>. As shown in Tab. 11, under different training settings, the improvements from the refiner are all significant on ImageNet. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Impacts of the attention heads</head><p>As discussed in <ref type="bibr" target="#b48">[49]</ref>, increasing the number of attention heads will reduce the embedding dimension per head and thus degrade the network performance. Differently, refiner expands the number of heads with a linear projection function. With the implementation of the linear projection, refiner will keep each head's embedding dimension unchanged. As shown in Tab. 12, the performance keeps increasing with Refined-ViT.  kernels achieve much better accuracy than the baseline method (kernel size of 1) and the refiner with kernel size 3 achieves that highest accuracy. This further verify that attention reinforcement within the local area could improve the performance of ViTs. Refiner enables the attention maps to capture some critical spatial relation among the tokens. To see this, we compute the attention maps with Refiner and let the following a number of blocks to reuse such pre-computed attention maps. The results are given in Tab. <ref type="bibr" target="#b12">13</ref>. Interestingly we find that the attention maps after Refiner can be shared (reused) by several following SA blocks. Let the directly adjacent SA block reuse the refined attention map from the previous block will maintain the model overall performance. Sharing the attention maps with the following two SA blocks only drop the accuracy by 0.2%. In stark contrast, the attention maps from the vanilla SA are completely not shareable, leading the model performance to drop severely. This demonstrates Refiner is effective at extracting useful patterns and this finding is also in accordance with the one from a recent NLP study <ref type="bibr" target="#b46">[47]</ref>. Therefore, across the experiments with Refiner, we shared the attention maps for 1 adjacent transformer block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Refiner makes the attention maps shareable</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Dataset description for NLP experiments</head><p>GLUE benchmark <ref type="bibr" target="#b51">[52]</ref> is a collection of nine natural language understanding tasks where the labels of the testing set is hidden and the researchers need to submit their predictions to the evaluation server 3 to obtain results on testing sets. In this work, we only present results of single-task setting for fair comparison. The nine tasks included in GLUE benchmark are described in details as below.</p><p>MNLI The Multi-Genre Natural Language Inference Corpus <ref type="bibr" target="#b56">[57]</ref> is a dataset of sentence pairs with textual entailment annotations. Given a premise sentence and a hypothesis sentence, the task is to predict their relationships including ENTENTAILMENT, CONTRADICTION and NEUTRAL. The data is collected from ten distinct genres including both written and spoken English.</p><p>QNLI Question Natural Language Inference is a binary sentence pair classification task, converted from The Stanford Question Answering Dataset <ref type="bibr" target="#b38">[39]</ref> (a question-answering dataset). Each sample of QNLI contains a context sentence and a question. The task is to determine whether the context sentence contains the answer to the question.</p><p>QQP The Quora Question Pairs dataset <ref type="bibr" target="#b9">[10]</ref> is a collection of question pairs from Quora (a community question-answering website). The task is to determine whether the two questions in a pair are semantically equivalent.</p><p>RTE Similar to MNLI, the Recognizing Textual Entailment (RTE) dataset is also a binary classification task, i.e., entailment and not entailment. It is from a series of annual textual entailment challenges including RTE1 <ref type="bibr" target="#b11">[12]</ref>, RTE2 <ref type="bibr" target="#b22">[23]</ref>, RTE3 <ref type="bibr" target="#b19">[20]</ref>, and RTE5 <ref type="bibr" target="#b1">[2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SST-2</head><p>The Stanford Sentiment Treebank <ref type="bibr" target="#b42">[43]</ref> consists of sentences from movie reviews and human annotations of their sentiment. GLUE uses the two-way (POSITIVE/NEGATIVE) class split.</p><p>MRPC The Microsoft Research Paraphrase Corpus <ref type="bibr" target="#b16">[17]</ref> contains data from online news that consists of sentence pairs with human annotations regarding whether the sentences in the pair are semantically equivalent.</p><p>CoLA The Corpus of Linguistic Acceptability <ref type="bibr" target="#b54">[55]</ref> is a binary single-sentence classification dataset containing the examples annotated with whether it is a grammatical English sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SST-B</head><p>The Semantic Textual Similarity Benchmark <ref type="bibr" target="#b6">[7]</ref> is a collection of human-annotated sentence pairs with a similarity score ranging from 1 to 5. The target is to to predict the scores with a given sentence.</p><p>WNLI Winograd NLI <ref type="bibr" target="#b29">[30]</ref> is a small natural language inference dataset. However, as highligthed on the GLUE web page 4 , there are issues with the construction of it. Thus following the practice of previous works (GPT <ref type="bibr" target="#b37">[38]</ref> and BERT <ref type="bibr" target="#b28">[29]</ref> etc.), we exclude this dataset for a fair comparison.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Illustration on our motivation. (a) The input image is regularly partitioned into patches for patch embedding. (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) Architecture design of refiner. Different from the vanilla self-attention block, the refiner applies linear attention expansion to attention maps output from the softmax operation to increase their number. Then head-wise spatial convolution is applied to augment these expanded attention maps. Finally another linear projection is deployed to reduce the number of attention maps to the original one. Note that r = H /H is the expansion ratio. (b) Modified transformer block with refiner as a drop-in component.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Refiner accelerates feature evolving compared with CNNs, the vanilla ViT and the Deit trained with a more complex scheme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Compared with the attention matrices A from the vanilla SA (top), for deeper blocks, refiner (bottom) strengthens the local patterns of their attention maps, making them less uniform and better model local context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Top-1 classification accuracy of Refined-ViT with different local kernel sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Effect of the expansion ratio in attention expansion, which varies from 1 to 6. The model is Refined-ViT with 16 blocks, 12 attention heads and 384 embedding dimension.</figDesc><table><row><cell cols="4">Expan. Ratio Params Converge (#Epoch) Top-1 (%)</cell></row><row><cell>1</cell><cell>25M</cell><cell>300</cell><cell>82.3</cell></row><row><cell>2</cell><cell>25M</cell><cell>300</cell><cell>82.8</cell></row><row><cell>3</cell><cell>25M</cell><cell>273</cell><cell>82.8</cell></row><row><cell>4</cell><cell>25M</cell><cell>270</cell><cell>82.9</cell></row><row><cell>6</cell><cell>25M</cell><cell>261</cell><cell>83.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Impacts of convolution on attention maps. We directly apply the 3 ? 3 convolution on the attention maps from the multi-head self-attention of ViTs with respect to various architectures. We can observe clear improvement for all ViT variants when adding the proposed DLA.</figDesc><table><row><cell cols="6">Model #Blocks Hidden dim #Heads Params Top-1 (%)</cell></row><row><cell>VIT</cell><cell>12</cell><cell>768</cell><cell>12</cell><cell>86M</cell><cell>79.5</cell></row><row><cell>+ DLA</cell><cell>12</cell><cell>768</cell><cell>12</cell><cell>86M</cell><cell>81.2</cell></row><row><cell>VIT</cell><cell>16</cell><cell>384</cell><cell>12</cell><cell>24M</cell><cell>78.9</cell></row><row><cell>+ DLA</cell><cell>16</cell><cell>384</cell><cell>12</cell><cell>24M</cell><cell>80.3</cell></row><row><cell>VIT</cell><cell>24</cell><cell>384</cell><cell>12</cell><cell>36M</cell><cell>79.3</cell></row><row><cell>+ DLA</cell><cell>24</cell><cell>384</cell><cell>12</cell><cell>36M</cell><cell>80.9</cell></row><row><cell>VIT</cell><cell>32</cell><cell>384</cell><cell>12</cell><cell>48M</cell><cell>79.2</cell></row><row><cell>+ DLA</cell><cell>32</cell><cell>384</cell><cell>12</cell><cell>48M</cell><cell>81.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Effect of attention reduction onRefined-ViT-16B with 384 hidden dimension.</figDesc><table><row><cell>Model</cell><cell>Attn. map Top-1 (%)</cell></row><row><cell cols="2">Refined-ViT-16B w/o reduction 82.99</cell></row><row><cell cols="2">Refined-ViT-16B w/ reduction 82.95</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Evaluation on how the spatial span within DLA affects the model performance. We compare the model performance with three different constraints on the local kernels.</figDesc><table><row><cell>Model</cell><cell cols="2">Constraints Top-1 (%)</cell></row><row><cell>Refined-ViT-16B</cell><cell>None</cell><cell>83.0</cell></row><row><cell>Refined-ViT-16B</cell><cell>Spatial</cell><cell>82.7</cell></row><row><cell cols="2">Refined-ViT-16B Row+Col</cell><cell>81.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>, in terms of the CKA similarity change w.r.t. the first block features and Refined-ViT evolves the features much faster than ViTs.</figDesc><table><row><cell>SA</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Refiner</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Block 1</cell><cell>Block 4</cell><cell>Block 11</cell><cell>Block 18</cell><cell>Block 23</cell><cell>Block 29</cell><cell>Block 30</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>RFC can improve both CNN and ViT-based SOTA models on ImageNet, outperforming the strategy of random cropping. We apply RFC on the best performing pre-trained models available online. RFC improves the top-1 accuracy consistently across a wide spectrum of models and configurations, and establishes the new SOTA on ImageNet.</figDesc><table><row><cell>Model</cell><cell cols="4">Rand Crop. Ratio Top-1 (%) RFC Ratio Top-1 (%)</cell></row><row><cell>EfficientNet-l2-ns-800 [46]</cell><cell>0.960</cell><cell>88.35</cell><cell>1.120</cell><cell>88.46</cell></row><row><cell>EfficientNet-b7-ns-475 [46]</cell><cell>0.936</cell><cell>88.23</cell><cell>1.020</cell><cell>88.33</cell></row><row><cell>Swin-large-patch4-w12-384 [34]</cell><cell>1.000</cell><cell>87.15</cell><cell>1.100</cell><cell>87.18</cell></row><row><cell>CaiT-m48-448 [21]</cell><cell>1.000</cell><cell>86.48</cell><cell>1.130</cell><cell>86.56</cell></row><row><cell>Refined-ViT-448</cell><cell>1.000</cell><cell>85.94</cell><cell>1.130</cell><cell>85.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Comparison of BERT-small w/o and w/ refiner on the GLUE development set.</figDesc><table><row><cell>Model</cell><cell cols="8">Params MNLI QNLI QQP RTE SST MRPC CoLA STS-B Avg.</cell></row><row><cell>BERT-small [11]</cell><cell>14M</cell><cell>75.8</cell><cell>83.7</cell><cell>86.8 57.4 88.4</cell><cell>83.8</cell><cell>41.6</cell><cell>83.6</cell><cell>75.1</cell></row><row><cell>+ Refiner</cell><cell>14M</cell><cell>78.1</cell><cell>86.4</cell><cell>88.0 57.6 88.8</cell><cell>84.1</cell><cell>42.2</cell><cell>84.0</cell><cell>76.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Top-1 accuracy comparison with other methods on ImageNet<ref type="bibr" target="#b14">[15]</ref> and ImageNet Real<ref type="bibr" target="#b2">[3]</ref>. All models are trained without external data. With the same computation and parameter constraint, Refined-ViT consistently outperforms other SOTA CNN-based and ViT-based models. The results of CNNs and ViT are adopted from<ref type="bibr" target="#b48">[49]</ref>.</figDesc><table><row><cell>Network</cell><cell cols="6">Params FLOPs Train size Test size Top-1(%) Real Top-1 (%)</cell></row><row><cell></cell><cell></cell><cell></cell><cell>CNNs</cell><cell></cell><cell></cell><cell></cell></row><row><cell>EfficientNet-B5 [46]</cell><cell>30M</cell><cell>9.9B</cell><cell>456</cell><cell>456</cell><cell>83.6</cell><cell>88.3</cell></row><row><cell>EfficientNet-B7 [46]</cell><cell>66M</cell><cell>37.0B</cell><cell>600</cell><cell>600</cell><cell>84.3</cell><cell>_</cell></row><row><cell>Fix-EfficientNet-B8 [46, 50]</cell><cell>87M</cell><cell>89.5B</cell><cell>672</cell><cell>800</cell><cell>85.7</cell><cell>90.0</cell></row><row><cell>NFNet-F0 [4]</cell><cell>72M</cell><cell>12.4B</cell><cell>192</cell><cell>256</cell><cell>83.6</cell><cell>88.1</cell></row><row><cell>NFNet-F1 [4]</cell><cell>133M</cell><cell>35.5B</cell><cell>224</cell><cell>320</cell><cell>84.7</cell><cell>88.9</cell></row><row><cell></cell><cell></cell><cell cols="2">Transformers</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ViT-B/16 [19]</cell><cell>86M</cell><cell>55.4B</cell><cell>224</cell><cell>384</cell><cell>77.9</cell><cell>83.6</cell></row><row><cell>ViT-L/16 [19]</cell><cell>307M</cell><cell>190.7B</cell><cell>224</cell><cell>384</cell><cell>76.5</cell><cell>82.2</cell></row><row><cell>T2T-ViT-14 [60]</cell><cell>22M</cell><cell>5.2B</cell><cell>224</cell><cell>224</cell><cell>81.5</cell><cell>_</cell></row><row><cell>T2T-ViT-14?384 [60]</cell><cell>22M</cell><cell>17.1B</cell><cell>224</cell><cell>384</cell><cell>83.3</cell><cell>_</cell></row><row><cell>CrossViT [8]</cell><cell>45M</cell><cell>56.6B</cell><cell>224</cell><cell>480</cell><cell>84.1</cell><cell>_</cell></row><row><cell>Swin-B [34]</cell><cell>88M</cell><cell>47.0B</cell><cell>224</cell><cell>384</cell><cell>84.2</cell><cell>_</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Default training hyper-parameters for our experiments.</figDesc><table><row><cell>H-param.</cell><cell>Standard</cell><cell>Advanced</cell></row><row><cell>Epoch</cell><cell>300</cell><cell>300</cell></row><row><cell>Batch size</cell><cell>256</cell><cell>512</cell></row><row><cell>LR LR decay</cell><cell>5e-3 ? batch_size 256 cosine</cell><cell>5e-3? batch_size 512 cosine</cell></row><row><cell>Weight decay</cell><cell>0.05</cell><cell>0.05</cell></row><row><cell>Warmup epochs</cell><cell>5</cell><cell>5</cell></row><row><cell>Dropout</cell><cell>0</cell><cell>0</cell></row><row><cell>Stoch. Depth MixUp</cell><cell>0.1 0.2</cell><cell>0.1 ? #Blocks 12 0.8</cell></row><row><cell>CutMix</cell><cell>0</cell><cell>1.0</cell></row><row><cell>Erasing prob.</cell><cell>0.25</cell><cell>0.25</cell></row><row><cell>RandAug</cell><cell>9/0.5</cell><cell>9/0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Comparison of the deep-narrow and shallow-wide architecture configurations. Thus, we design the architecture of Refiner-ViT based on the same deep and narrow network architecture. The detailed configurations and the comparison with the ViT-base architecture are shown in Tab. 10.</figDesc><table><row><cell>Model</cell><cell cols="5">#Blocks Hidden dim #Heads Params Top-1 (%)</cell></row><row><cell>Refined-ViT</cell><cell>12</cell><cell>768</cell><cell>12</cell><cell>86M</cell><cell>83.1</cell></row><row><cell>Refined-ViT</cell><cell>16</cell><cell>384</cell><cell>12</cell><cell>25M</cell><cell>83.0</cell></row><row><cell cols="5">A.4 Feature evolving speed and feature similarity with CKA</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 10 :</head><label>10</label><figDesc>Model architecture configurations.</figDesc><table /><note>Model #Blocks Hidden dim #Head #Params Training Resolution</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 11 :</head><label>11</label><figDesc>Performance gain from refiner under different training recipes.</figDesc><table><row><cell>Training techniques</cell><cell cols="3">#Param. Original Top-1 (%) + Refined-ViT</cell></row><row><cell>Baseline (ViT-Base)</cell><cell>86M</cell><cell>79.3</cell><cell>81.2 (+1.9)</cell></row><row><cell>+ DeiT-Base ([48])</cell><cell>86M</cell><cell>81.5</cell><cell>82.3 (+0.8)</cell></row><row><cell cols="2">+ More convs for patch embedding 86M</cell><cell>82.2</cell><cell>83.1 (+0.9)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 12 :</head><label>12</label><figDesc>Ablation on the impact of the number of the heads for ViT-Small with 16 blocks and 384 embedding dimension. We report top-1 accuracy on ImageNet. The accuracy for ViT-Small with different head numbers are adopted from<ref type="bibr" target="#b48">[49]</ref>. Impacts of the kernel size For all the experiments in the main paper, the convolution kernel is set to be 3 ? 3 by default. InFig. 6, we show the classification results on ImageNet with different kernel size. It is observed that all</figDesc><table><row><cell cols="3"># Heads ViT-Small Refined-ViT</cell></row><row><cell>8</cell><cell>79.9</cell><cell>82.5</cell></row><row><cell>12</cell><cell>80.0</cell><cell>82.6</cell></row><row><cell>16</cell><cell>80.0</cell><cell>82.8</cell></row><row><cell>B.3</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 13 :</head><label>13</label><figDesc>Ablation on the number of shared attention maps on ViT-16B with 384 embedding dimension.</figDesc><table><row><cell cols="3">#Shared Blocks SA Refiner</cell></row><row><cell>0</cell><cell>82</cell><cell>83.0</cell></row><row><cell>1</cell><cell>77</cell><cell>83.0</cell></row><row><cell>2</cell><cell>70</cell><cell>82.8</cell></row><row><cell>3</cell><cell cols="2">NAN 82.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We interchangeably use attention matrix and attention map to call A when the context is clear. Specifically, the attention matrix means the original A while the attention map means reshaped A to the input size.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://gluebenchmark.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://gluebenchmark.com/faq</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lambdanetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.08602</idno>
		<title level="m">Modeling long-range interactions without attention</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The fifth pascal recognizing textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Bentivogli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<editor>TAC</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolesnikov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07159</idno>
		<title level="m">Xiaohua Zhai, and A?ron van den Oord. Are we done with imagenet? arXiv preprint</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">High-performance large-scale image recognition without normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simonyan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06171</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">End-to-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.12872</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I?igo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Crossvit: Cross-attention multi-scale vision transformer for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14899</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenhua</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.00364</idno>
		<title level="m">Pre-trained image processing transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Quora question pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leqi</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Electra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10555</idno>
		<title level="m">Pre-training text encoders as discriminators rather than generators</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Ido Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Glickman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Magnini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning Challenges Workshop</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page" from="177" to="190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Up-detr: Unsupervised pre-training for object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junying</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09094</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>St?phane D&amp;apos;ascoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Leavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulio</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Biroli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sagun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10697</idno>
		<title level="m">Convit: Improving vision transformers with soft convolutional inductive biases</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><forename type="middle">Toutanova</forename><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatically constructing a corpus of sentential paraphrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brockett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Workshop on Paraphrasing (IWP2005)</title>
		<meeting>the Third International Workshop on Paraphrasing (IWP2005)</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Attention is not all you need: Pure attention loses rank doubly exponentially with depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihe</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.03404</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The third pascal recognizing textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-PASCAL workshop on textual entailment and paraphrasing</title>
		<meeting>the ACL-PASCAL workshop on textual entailment and paraphrasing</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Improve vision transformers training by suppressing over-smoothing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengyue</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12753</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Xiong</forename><surname>Meng-Hao Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng-Ning</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tai-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Ralph R Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09688</idno>
		<title level="m">Pct: Point cloud transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The second pascal recognising textual entailment challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>R Bar Haim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Dagan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Ferro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Idan</forename><surname>Magnini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szpektor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second PASCAL Challenges Workshop on Recognising Textual Entailment</title>
		<meeting>the Second PASCAL Challenges Workshop on Recognising Textual Entailment</meeting>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Transformer in transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enhua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunjing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhe</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00112</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Token labeling: Training a 85</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10858</idno>
	</analytic>
	<monogr>
		<title level="m">4% top-1 accuracy vision transformer with 56m parameters on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Similarity of neural network representations revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3519" to="3529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Piyush Sharma, and Radu Soricut. ALBERT: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11942</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The winograd schema challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hector</forename><surname>Levesque</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernest</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leora</forename><surname>Morgenstern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Scale-aware fast r-cnn for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengmei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tingfa</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="985" to="996" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05707</idno>
		<title level="m">Localvit: Bringing locality to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Recurrent neural network for text classification with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanjing</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05101</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.14030</idno>
		<title level="m">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Understanding the effective receptive field in deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04128</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8026" to="8037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Conformer: Local features coupling global representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiliang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanzhi</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingxi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaowei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03889</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">SQuAD: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-11" />
			<biblScope unit="page" from="2383" to="2392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Long short-term memory recurrent neural network architectures for large scale acoustic modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hasim</forename><surname>Sak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?oise</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Beaufays</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Mobilenetv2: Inverted residuals and linear bottlenecks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Zhmoginov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4510" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.02436</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Talking-heads attention. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.6418</idno>
		<title level="m">Learning invariant representations with local transformations</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11605</idno>
		<title level="m">Jonathon Shlens, Pieter Abbeel, and Ashish Vaswani. Bottleneck transformers for visual recognition</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Efficientnet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.11946</idno>
		<title level="m">Rethinking model scaling for convolutional neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Synthesizer: Rethinking self-attention in transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00743</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Sablayrolles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.12877</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.17239</idno>
		<title level="m">Alexandre Sablayrolles, Gabriel Synnaeve, and Herv? J?gou. Going deeper with image transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.06423</idno>
		<title level="m">Matthijs Douze, and Herv? J?gou. Fixing the train-test resolution discrepancy</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.07461</idno>
		<title level="m">Glue: A multi-task benchmark and analysis platform for natural language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Residual attention network for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengqing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">End-to-end video instance segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoliang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoshan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaxia</forename><surname>Xia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.14503</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Neural network acceptability judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Warstadt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="625" to="641" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Pytorch image models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<ptr target="https://github.com/rwightman/pytorch-image-models" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1112" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1492" to="1500" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Learning texture transformer network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baining</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5791" to="5800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.11986</idno>
		<title level="m">Tokens-to-token vit: Training vision transformers from scratch on imagenet</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Revisiting knowledge distillation via label smoothing regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">H</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3903" to="3911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Learning joint spatial-temporal transformations for video inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanhong</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="528" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.09164</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Point transformer. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">End-to-end object detection with adaptive clustering transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09315</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Neural epitome search for architecture-agnostic network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qibin</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochen</forename><surname>Lian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11886</idno>
		<title level="m">Qibin Hou, and Jiashi Feng. Deepvit: Towards deeper vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">End-to-end dense video captioning with masked transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8739" to="8748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Deformable detr: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
