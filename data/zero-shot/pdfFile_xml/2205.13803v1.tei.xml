<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bongard-HOI: Benchmarking Few-Shot Visual Reasoning for Human-Object Interactions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
							<email>h.jiang@northeastern.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojian</forename><surname>Ma</surname></persName>
							<email>xiaojian.ma@ucla.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">UCLA</orgName>
								<address>
									<addrLine>3 NVIDIA 4</addrLine>
									<settlement>Austin 5 Caltech</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weili</forename><surname>Nie</surname></persName>
							<email>wnie@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
							<email>zhidingy@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
							<email>yukez@cs.utexas.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
						</author>
						<title level="a" type="main">Bongard-HOI: Benchmarking Few-Shot Visual Reasoning for Human-Object Interactions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A significant gap remains between today's visual pattern recognition models and human-level visual cognition especially when it comes to few-shot learning and compositional reasoning of novel concepts. We introduce Bongard-HOI, a new visual reasoning benchmark that focuses on compositional learning of human-object interactions (HOIs) from natural images. It is inspired by two desirable characteristics from the classical Bongard problems (BPs): 1) few-shot concept learning, and 2) context-dependent reasoning. We carefully curate the few-shot instances with hard negatives, where positive and negative images only disagree on action labels, making mere recognition of object categories insufficient to complete our benchmarks. We also design multiple test sets to systematically study the generalization of visual learning models, where we vary the overlap of the HOI concepts between the training and test sets of fewshot instances, from partial to no overlaps. Bongard-HOI presents a substantial challenge to today's visual recognition models. The state-of-the-art HOI detection model achieves only 62% accuracy on few-shot binary prediction while even amateur human testers on MTurk have 91% accuracy. With the Bongard-HOI benchmark, we hope to further advance research efforts in visual reasoning, especially in holistic perception-reasoning systems and better representation learning. Code is available. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, great strides have been made on visual recognition benchmarks, such as ImageNet <ref type="bibr" target="#b7">[8]</ref> and COCO <ref type="bibr" target="#b33">[34]</ref>. Nonetheless, there remains a considerable gap between machine-level pattern recognition and human-level cognitive reasoning. Current image understanding models typically require a large amount of training data yet struggle * First two authors contributed equally. <ref type="bibr" target="#b0">1</ref>  to generalize beyond the visual concepts seen during training. In contrast, humans can reason about new visual concepts in a compositional manner from just a few examples <ref type="bibr" target="#b21">[22]</ref>. To march towards human-level visual cognition, we need to depart from conventional benchmarks on closed-vocabulary recognition tasks and aim to systematically examine compositional and few-shot learning of novel visual concepts. While existing benchmarks such as miniImageNet <ref type="bibr" target="#b46">[47]</ref>, Meta-Dataset <ref type="bibr" target="#b44">[45]</ref>, and ORBIT <ref type="bibr" target="#b44">[45]</ref> have been dedicated to studying few-shot visual learning, they focus on recognizing object categories instead of the compositional structures of visual concepts, e.g., visual relationships. A par-actions with dogs actions with oranges From top to bottom, left to right: washing, walking, and feeding dogs; eating, squeezing, and peeling oranges. To differentiate these images, we need compositional understanding on both the actions and the objects. We exploit this to select hard negatives in Bongard-HOI: negative images contain the same object as the positives, but the actions are different. allel line of research aims at building benchmarks for abstract reasoning by taking inspiration from cognitive science such as RPM (Raven-style Progressive Matrices) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b43">44]</ref> and Bongard-LOGO <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33]</ref>. In these benchmarks, a model has to learn concept induction rules from a few examples and the concepts are context-dependent in each task. However, they use simple synthetic images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33]</ref> or focus on basic object-level properties, such as shapes and categories <ref type="bibr" target="#b43">[44]</ref>. Our new benchmark: In this paper, we introduce Bongard-HOI, a new benchmark for compositional visual reasoning with natural images. It studies human-object interactions (HOIs) as the visual concepts, requiring explicit compositional reasoning of object-level concepts. Our Bongard-HOI benchmark inherits two important characteristics of the classic Bongard problems (BPs) <ref type="bibr" target="#b2">[3]</ref>: 1) few-shot binary prediction, where a visual concept needs to be induced from just six positive and six negative examples and 2) context-dependent reasoning, where the label of an image may be interpreted differently under different contexts.</p><p>Furthermore, Bongard-HOI upgrades the original BPs from synthetic graphics to natural images. Our benchmark contains rich visual stimuli featuring large intra-class variance, cluttered background, diverse scene layouts, etc. In Bongard-HOI, a single few-shot binary prediction instance, referred to as BP, contains a set of six positive images and a set of six negative images, along with query images (see <ref type="figure" target="#fig_0">Fig. 1</ref> for examples). The task is making binary predictions on the query images.</p><p>We construct the few-shot instances in Bongard-HOI on top of the HAKE dataset <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. To encourage the explicit reasoning of visual relationships, we use hard negatives to construct few-shot instances. The hard negatives consist of negatives that contain objects from the same categories as those contained in the positive sets but with different action labels. <ref type="figure" target="#fig_1">Fig. 2</ref> presents some examples of these images. Since both positive and negative examples contain object instances from the same categories, mere recognition of object categories is insufficient to complete the tasks. Rather, reasoning about visual relationships between person and objects is required to solve these few-shot binary prediction problems. The existence of such hard negatives distinguishes our benchmark from existing visual abstract reasoning counterparts <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b43">44]</ref>. Comparisons with different benchmarks can be found in <ref type="table">Table 1</ref>.</p><p>We carefully curate the annotations in HAKE when constructing the few-shot instances. Recall the visual concept contained in the positive images should not appear in any of the negative ones. Thus, we have to carefully select the images in both sets. We employ high-quality annotators from the Amazon Mechanical Turk platform to curate the test set to further remove ambiguously and wrongly labeled few-shot instances. In this process, 2.5% of the few-shot instances in the test set are discarded. We end up with 23K and 15K few-shot instances in disjoint training and test sets, respectively.</p><p>An important goal of the Bongard-HOI benchmark is to systematically study the generalization of machine learning models for real-world visual relationship reasoning. To this end, we introduce four separate test sets to investigate different types of generalization, depending on whether the action and object classes are seen in the training set. <ref type="figure">Fig. 3</ref> illustrates their design. This way, we have full control of the overlap between the concepts (i.e., HOIs) between training and test of few-shot instances. It enables us to carefully examine the generalization of visual learning models. Ideally, a learning model should be able to generalize beyond the concepts it has seen during training. Even for unseen HOI concepts, the model should be able to learn how to induce the underlying visual relationship from just a few examples. Establishing baselines: In our experiments, we first examine the state-of-the-art HOI detection models' performance on this new task, we trained an oracle model with HOITrans <ref type="bibr" target="#b53">[54]</ref> on all the HOI categories, including those in the test sets of our Bongard-HOI benchmark, and output binary prediction on the query image via a majority vote based on HOI detections. Its accuracy is only 62.46% (with a chance performance of 50%), demonstrating the challenge of our visual reasoning tasks. We then evaluate state-of-theart few-shot learning approaches, including non-episodic and meta-learning methods. We show that the current learning models struggle to solve the Bongard-HOI problems. Compared to amateur human testers' 91.42% overall accuracy, who have access to a few examples of visual relationships before working on solving our problems, the state-of-the-art few-shot learning model <ref type="bibr" target="#b5">[6]</ref> only has 55.82% accuracy.</p><p>The results above lead to this question: why do they per- form so poorly? To this end, we offer a detailed analysis of the results and propose several conjectures. The first one is a lack of holistic perception and reasoning systems, since models that have only good pattern recognition performances, e.g. HOITrans, are likely to fail on our benchmarks. Moreover, we believe there is a need for additional representation learning, e.g. pre-training, since currently we only train on binary labels of few-shot instances. Nonetheless, we believe much effort is still needed to further investigate the challenges brought by our benchmark.</p><p>To sum up, this paper makes the following contributions: ? We introduce Bongard-HOI, a new benchmark for fewshot visual reasoning with human-object interactions, aiming at combining the best of few-shot learning, compositional reasoning, and challenging real-world scenes.</p><p>? We carefully curate Bonagrd-HOI with hard negatives, making mere recognition of object categories insufficient to complete our tasks. We also introduce multiple test sets to systematically study different types of generalization.</p><p>? We analyze state-of-the-art few-shot learning and HOI detection methods. However, experimental results show their inability on achieving good results on Bongard-HOI. Our conjectures suggest future research in models with holistic perception-reasoning systems and better representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Bongard-HOI Benchmark</head><p>For a few-shot binary prediction instance in Bonagrd-HOI, it has a set of positive examples P, a set of negative samples N , and a query image I q . Images in P depict a certain visual concept (e.g., ride bicycle in <ref type="figure" target="#fig_0">Fig. 1</ref>), while images in N do not. In each task, there are only six images in both P and N . As a result, a human tester or machine learning model needs to induce the underlying concept from just a few examples. Given the query image I q , a binary prediction needs to be made: whether the certain visual concept depicted in P is available in I q or not. Later, we will detail how to construct these few-shot instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Constructing Bongard Problems</head><p>Few-shot instances in Bongard-HOI are constructed with natural images. We choose to use visual relationships as underlying visual concepts. In our early experiments, we also studied visual attributes to construct few-shot instances, for example, color and shape of bird parts <ref type="bibr" target="#b47">[48]</ref>, facial attributes <ref type="bibr" target="#b27">[28]</ref>. But such visual attributes annotations either require too much domain knowledge for human annotators or are too noisy to curate. Another option we investigated is scene graph <ref type="bibr" target="#b18">[19]</ref>, which is a combination of both visual relationships and visual attributes. However, there could be too many convoluted visual concepts in a single image, resulting in ambiguous few-shot instances.</p><p>In this paper, we construct few-shot instances on top of the HAKE dataset <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> focusing on human-object interactions. It provides unified annotations following the annotation protocol in HICO <ref type="bibr" target="#b3">[4]</ref> for a set of datasets widely used for HOI detection, including HICO <ref type="bibr" target="#b3">[4]</ref>, V-COCO <ref type="bibr" target="#b9">[10]</ref>, Open-Images <ref type="bibr" target="#b19">[20]</ref>, HCVRD <ref type="bibr" target="#b51">[52]</ref>, and PIC <ref type="bibr" target="#b25">[26]</ref>. HAKE has 80 object categories, which are consistent with the vocabulary defined in the standard COCO dataset <ref type="bibr" target="#b26">[27]</ref>. It also has 117 action labels, leading to 600 human-object relationships 2 .</p><p>Denote a concept c = ?s, a, o? as a visual relationship triplet, where s, a, o are the class labels of subject, action, and object, respectively. In this paper, s is always person. We start with selecting a set of positive images I c = {I 1 , . . . } from HAKE that depict such a relationship. We also need negative images, where the visual concept c is <ref type="table">Table 1</ref>. An overview of different benchmark datasets covering HOI detection, few-shot learning, and abstract visual reasoning. In the first row, the abbreviation ctx denotes context; generalization types indicates if a benchmark includes multiple test splits to examine different types of generalization. * We consider the concept of object counts as compositional while others such as object attributes and categories not <ref type="bibr" target="#b43">[44]</ref> not contained by them. In specific, we collect another set of images Ic with conceptc = ?s,?, o?, where? ? = a, meaning that we select hard negatives. As a result, images from both I c and Ic contain the same categories of objects and the only differences are the action labels, making it impossible to trivially distinguish positive images from the negatives by doing visual recognition of object categories only. Rather, detailed visual reasoning about the interactions of human and objects are desired. <ref type="figure" target="#fig_1">Fig. 2</ref> illustrates the difficulties introduced by the hard negatives. Finally, as an entire image may contain multiple HOI instances, we use image regions (crops) around each HOI instance instead of the original image to ensure only a single HOI instance is presented in a single image. Next, we need to sample few-shot instances from the positive images I c and the negatives Ic. We randomly sample images to form P, N , and a query image I q . Two parameters control the sampling process: M , the number of images in P and N (M = 6 in Bongard-HOI), and the overlap threshold ? , indicating the maximum number of overlapped images between two few-shot instances. We want to sample as many few-shot instances as possible, but we also need to avoid significant image overlap between few-shot instances, which limits the diversity of the data. We end up setting ? = 3 and ? = 2 for training and test sets, respectively. More details can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Data Curation</head><p>Although the HAKE dataset <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> has provided highquality annotations, we found that curations are still needed to construct few-shot instances. Recall, to sample negative images, we assume a particular action is not depicted in them. In HAKE, an image region may have multiple action labels. Naively relying on the provided annotations is problematic as the action labels are either not manually exclusive or not exhaustively annotated. We hire high-quality testers on the Amazon Mechanical Turk (MTurk) platform, who maintain a good job approval record, to curate existing HOI annotations.</p><p>We discuss the data curation process in detail and show visual examples in detail in the supplementary material.</p><p>After the aforementioned data curations, each image region is assigned to a single action label, describing the most salient visual relationship. With the curated annotations, action labels between a person and objects of a certain category are mutually exclusive so that we can significantly reduce the ambiguity when constructing few-shot instances. Finally, we hire high-quality testers on the MTurk platform to further remove the ambiguous few-shot instances in the test set. Every single few-shot instance is assigned to three independent testers. We compare their responses with the ground-truth labels and discard about 2.5% few-shot instances where none of the three testers correctly classifies the query images. In the end, we report the accuracy of human testers on those left unambiguous few-shot instances as a human study to examine human-level performance on our Bongard-HOI benchmark, where the average accuracy is 91.42%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Generalization Tests</head><p>Transferring the knowledge that an agent has seen and learned is a hallmark of visual intelligence, which is a longstand goal for the entire AI community. It is also a core focus of the Bongard-HOI benchmark. Following <ref type="bibr" target="#b1">[2]</ref>, we provide multiple test splits to investigate different types of generalization, aiming at a systematic understanding of how the tested models generalize on our benchmark. Specifically, the visual concept we consider in Bongard-HOI is an HOI triplet ?s, a, o? and we have two variables of freedom: action a and object o. Therefore, by controlling whether an action or object is seen during training, we can study generalization to unseen actions, unseen objects, or a combination of two. We end up introducing four separate test sets, as shown in <ref type="figure">Fig. 3</ref>. We provide detailed statistics on our training and test sets in the supplementary material.</p><p>Ideally, after learning from examples of sit_on bed, a machine learning model can quickly grasp the concept sit_on bench. More importantly, such a model should learn how to learn from just a few examples, so that they can still induce the correct concept (visual relationship) in the most challenging cases, where both actions and objects are not seen during training (e.g., shear sheep).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Possible Models for Bongard-HOI</head><p>There are many possible ways of tackling Bongard-HOI, such as few-shot learning, conventional HOI detection, etc.</p><p>We are particularly interested in investigating few-shot learning methods, as our benchmark requires the learner to identify the visual concept with very few samples (positive and negative images in P and N , respectively). To further improve the few-shot learning methods, we consider encoding the images with Relation Network <ref type="bibr" target="#b40">[41]</ref>, aiming at better compositionality in the learned representations. Finally, we introduce an oracle model to testify whether Bongard-HOI can be trivially solved using state-of-the-art HOI detection models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Few-shot Learning in Bongard-HOI</head><p>We start with a formal definition of the few-shot learning problem in Bongard-HOI. Specifically, each task includes multiple few-shot instance with N = 2 classes and 2M samples, i.e., the model learns from a training set</p><formula xml:id="formula_0">S = P ?N = {(I P 1 , 1), . . . , (I P M , 1), (I N 1 , 0), . . . , (I N M , 0)</formula><p>} and is evaluated on a query image (I q , y q ). Each example (I, y) includes an image I ? R H?W ?3 and a class label y ? {0, 1}, indicating whether I contains the visual concepts depicted in P. In Bonagrd-HOI, we set M = 6 as our default parameter and therefore each few-shot instance is "2-way, 6-shot". Following <ref type="bibr" target="#b44">[45]</ref>, we propose to solve these few-shot prediction instances with the following two families of approaches:</p><p>Non-episodic methods. In these methods, a simple classifier is trained to map all the images in a few-shot instance (including images in P, N , and the query image) to the class of the query. The classifier can be parameterized as a neural network over some learned image embeddings, i.e. representations produced by convolutional neural networks (CNNs). In other words, we view each few-shot instance as a single training sample (</p><formula xml:id="formula_1">2M +1 i=1 I i , y q )</formula><p>rather than a few-shot instance with multiple training samples (I, y). Our experiments cover two different ways to encode the images: CNN and Wide Relational Network (WReN) <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Meta-learning methods. These methods adopt the episodic learning setting, i.e., they learn to train a classifier using 2M samples from S and evaluate their trained classifier on the query (I q , y q ). In general, their objective (also called meta-objective) is to minimize the prediction error on the query. Different meta-learning methods have their own ways to build the classifier and optimize the meta-objective. In our experiments, we consider the following state-of-the-art meth- ods: 1) ProtoNet <ref type="bibr" target="#b42">[43]</ref>, a metric-based method; 2) MetaOpt-Net <ref type="bibr" target="#b22">[23]</ref> and ANIL <ref type="bibr" target="#b34">[35]</ref>, two optimization-based approaches. Moreover, we also use a strong baseline meta-learning model, Meta-Baseline <ref type="bibr" target="#b5">[6]</ref>, which reports competitive results in many few-shot prediction tasks. We refer readers to the related papers for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Image Encoding with Relational Network</head><p>As mentioned above, representation learning of the input images can be crucial to the success of few-shot learning methods on Bongard-HOI. As our benchmark demands learning compostional concepts (HOIs), simply feeding an image into a Convolutional Neural Network (CNN) is not optimal. To this end, we propose to use the Relational Network <ref type="bibr" target="#b40">[41]</ref>, which shows promising compositional reasoning accuracy on a Visual Question Answering (VQA) benchmark <ref type="bibr" target="#b15">[16]</ref>, to explicitly encode the compositionality of visual relationships. In specific, the feature representations of the image I is computed as</p><formula xml:id="formula_2">RN(I) = f ? ? i,j g ? (concat(h ? (o i , I), h ? (o j , I))) ,</formula><p>where o i and o j are two detected objects of the image I, provided by ground truth object annotations or a pre-trained object detector like Faster R-CNN <ref type="bibr" target="#b37">[38]</ref>. h ? denotes the RoI Pooled features of o i from a ResNet backbone <ref type="bibr" target="#b11">[12]</ref> followed by a MLP (multi-layer perceptron) <ref type="bibr" target="#b37">[38]</ref>, which is parameterized by ?. g ? and f ? are two additional MLPs.</p><p>A challenge we are facing is the unseen object categories in the test sets. Since the object detector has to be pre-trained on a dataset without the unseen object categories, it is likely to fail on our test set where images could contain objects belonging to these categories. To tackle this issue, we train a binary class-agnostic (objectness) detection model instead to get o i and o j . Class-agnostic object detections are shown in <ref type="figure" target="#fig_2">Fig. 4</ref>. As we can see, all objects of interest have been successfully detected. But at the same time, there are a lot of other distracting ones, such as the bench and the wagon in the left image of <ref type="figure" target="#fig_2">Fig. 4</ref>. This is a unique challenge of dealing with visual reasoning over real-world images. We devote discussions to it in the experiment section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Oracle</head><p>One may wonder if our Bongard-HOI benchmark could be trivially solved using the state-of-the-art HOI detection model. To address this concern, we develop an oracle model resorting to the HOITrans <ref type="bibr" target="#b53">[54]</ref>, which is based on the Transformer model <ref type="bibr" target="#b45">[46]</ref> and reports state-of-the-art accuracy on the HICO <ref type="bibr" target="#b3">[4]</ref> and V-COCO <ref type="bibr" target="#b9">[10]</ref> benchmarks. In specific, let's denote the HOI detections in the P and N as D P and D N , respectively. D P contains the detections from all of the images in the P, defined as</p><formula xml:id="formula_3">D P = {c P i } N P i=1 , where c P i</formula><p>is a HOI triplet introduced in Section 2.1. N P is the total number of detections. Note that there may be multiple or no detections for a single image. Similarly, D N is defined as</p><formula xml:id="formula_4">D N = {c N i } N N i=1 .</formula><p>According to the property of Bongard-HOI, the visual concept c P should only appear in the P, not in the N . We, therefore, compute c P as</p><formula xml:id="formula_5">c P = majority_vote(D P ? D N ),</formula><p>where ? is the set operator for set subtraction. Here we first exclude the HOIs detected in N from D P , then the majority of the remaining HOIs will be viewed as the visual concept c P . Given the detections</p><formula xml:id="formula_6">D q = {c q i } Nq i=1</formula><p>for the query image I q , our prediction y becomes y = 1, if c P ? D q , 0, otherwise. Discussions of how to deal with the corner cases, e.g., majority_vote returns more than 1 concept, D q is empty, etc, are provided in the supplementary material. We illustrate how this model works in <ref type="figure">Fig. 5</ref>, where we show HOI detections in each image.</p><p>We call it our oracle model as it has privileged information, i.e., the entire HOI action &amp; object vocabulary, including those held-out ones in the test set. As we shall we in Section 4, such an oracle model still struggles on our Bongard-HOI benchmark, achieving only 62.46% accuracy on average, which is far below the human-level performance of 91.42%. It suggests that our Bongard-HOI benchmark is not trivial to solve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We benchmark the models introduced in Section 3 on Bongard-HOI to test their performance on human-level fewshot visual reasoning. We use a ResNet50 <ref type="bibr" target="#b11">[12]</ref> as an encoder for the input images. We consider different pre-training strategies: 1) no pre-training at all (scratch), 2) pre-trained on the ImageNet dataset with manual labels <ref type="bibr" target="#b7">[8]</ref>, and 3) latest self-supervised approach <ref type="bibr" target="#b4">[5]</ref> pre-trained on ImageNet but without manual labels. We train an Faster R-CNN <ref type="bibr" target="#b37">[38]</ref> class-agnostic objectness detection model on the COCO dataset <ref type="bibr" target="#b33">[34]</ref> using a ResNet101 <ref type="bibr" target="#b11">[12]</ref> pre-trained on Ima-geNet <ref type="bibr" target="#b7">[8]</ref> as the backbone. We use the RoIPool operation <ref type="bibr" target="#b37">[38]</ref> to get feature representations for each bounding P N Query images:</p><p>Predictions: positive negative <ref type="figure">Figure 5</ref>. Illustration of our oracle model. We first generate some detections for all the images using HOITrans <ref type="bibr" target="#b53">[54]</ref>. Note that some images may not have any detection at all. According to the detections in the P and N , the common concept is eat donut.</p><p>As a result, in the bottom row, the first query image is considered to be positive as its HOI detections contain eat donat. The second query image is negative. Zoom in for the best view.</p><p>box. We also use ground-truth bounding boxes provided in HAKE <ref type="bibr" target="#b23">[24]</ref> as input to diagnose the effectiveness of the visual perception. In addition to RoIPooled region features, we also concatenate each bounding box's normalized coordinates (center and spatial dimensions) as spatial information to the Relational Network encoder introduced in Section 3.1.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Quantitative Results</head><p>The quantitative results of different models on our Bongard-HOI benchmark can be found in <ref type="table">Table 2</ref>. We make the following observations: First of all, despite the overall difficulties brought by our benchmarks, most models perform worse on the challenging test splits, where actions and/or object categories are completely unseen during training. This observation aligns well with our hypothesis, i.e. existing machine learning approaches can be limited in terms of generalizing beyond training concepts. It also echos the findings in Bongard-LOGO <ref type="bibr" target="#b32">[33]</ref>, a dataset studying a similar problem with synthetic images. Second, meta-learning approaches generally tend to perform better than non-episodic counterparts, which can be on par with or even worse than random guesses (50% chance). We hypothesize the reason to be the focus on learning to learn in these methods, which is essentially required to solve the few-shot instances in the Bongard-HOI benchmark, especially for the challenging test splits with novel categories. Similar observations have also been made in Bongard-LOGO. Moreover, some metalearning models are distracted by bounding boxes provided <ref type="table">Table 2</ref>. Quantitative results on the Bongard-HOI benchmark. All the models use a ResNet50 as the image encoder. For the input of bounding boxes (bbox), we consider two options: from an object detection model (det) and ground-truth annotations (gt). For the ResNet50 encoder, we experiment with different pre-training strategies: no pre-training at all (scratch), pre-trained on the ImageNet dataset with manual labels (IN), and state-of-the-art self-supervised approach MoCoV2 <ref type="bibr" target="#b4">[5]</ref>. (* denotes that we are unable to get meaningful results; # indicates that the trained model has a run-time error during the inference stage since the condition of the QP solver can not be satisfied by an object detection model. We will discuss this issue in the next section. Surprisingly, the oracle model (HOITrans) also struggles on our tests with an averaged accuracy of 62.46%, albeit being trained with direct HOI supervision and all action&amp;object categories. It suggests a clear gap between the existing HOI detection datasets, e.g. HAKE <ref type="bibr" target="#b23">[24]</ref> and Bongard-HOI, where the latter one requires capabilities beyond perception, e.g. HOI recognition. Rather, a model might also need contextdependent reasoning, learning-to-learn from very few examples, etc., to perform well on our benchmarks.</p><p>Finally, machine learning models still largely fall behind amateur human testers (e.g., 55.82% of Meta-Baseline vs 91.42%). While we only give human testers a couple of examples about visual relationships before they start working on solving Bongard-HOI, they can quickly learn how to induce visual relationships from just a few examples, reporting an average 91.42% accuracy on our Bongard-HOI benchmark. Particularly, there are no significant differences for the different subsets of the test set. We hope our findings will foster more research efforts on closing this gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussions</head><p>We need holistic perception and reasoning. Our work suggests that the significant challenges in current visual reasoning systems lie in both the reliability of perception and the intricacy of the reasoning task itself. Models that have only good pattern recognition performances are likely to fail on our benchmarks. Rather, an ideal learner needs to integrate visual perception in natural scenes and detailed cognitive reasoning as a whole. This marks our key motivation to propose Bongard-HOI as the first step towards studying these two problems holistically. Pre-training improves performances. Intuitively, models for Bongard-HOI might need additional representation learning, e.g. pre-training, since currently we only train on binary labels of few-shot instances. We can see from <ref type="table">Table 2</ref> that pre-training is very helpful. Compared to no pre-training, using either manual labels or self-supervision leads to a performance boost. In particular, the self-supervised pretraining <ref type="bibr" target="#b4">[5]</ref> does not use any manual labels for supervision. Yet it can produce better results than learning from scratch. Visual perception matters in Bongard-HOI. Finally, an imperfect perception could still be a major obstacle here. Different from Bongard-LOGO <ref type="bibr" target="#b32">[33]</ref> which uses synthetic shapes instead, Bongard-HOI studies visual reasoning on natural scenes, which often contain rich visual stimuli, issuing such as large intra-class variance and cluttered background also present challenges to reliable visual perception on which reasoning is based. In our case, bounding boxes produced by an object detection model can be inevitably noisy. Some meta-learning models, including ProtoNet <ref type="bibr" target="#b42">[43]</ref>, have difficulties inducing the true visual relationships. For MetaOptNet <ref type="bibr" target="#b22">[23]</ref>, although we can finish training, we constantly encounter run-time errors where the condition of the QP solver is not satisfied during the inference stage. Instead, when taking clean ground-truth bounding boxes as input, all of these approaches produce better accuracy. Note that using ground-truth bounding boxes only serves as an oracle, which does not indicate the models' authentic performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Related Work</head><p>Visual relationship detection benchmarks. Various benchmarks are also dedicated for visual relationship recognition and detection, particularly for human-centric relationships (i.e., HOI). In the seminal work of Visual Genome <ref type="bibr" target="#b18">[19]</ref>, scene graph annotations, including relationships of different objects, are provided. A subset of the annotations is used in VRD <ref type="bibr" target="#b28">[29]</ref> to focus on visual relationship detection. In a recent effort, large-scale visual relationships are provided in the Open Images dataset <ref type="bibr" target="#b19">[20]</ref>. HOI, is of particular interest to understand the interactions of humans and other objects. A lot of HOI benchmarks, such as HICO <ref type="bibr" target="#b3">[4]</ref>, COCOa <ref type="bibr" target="#b38">[39]</ref>, vCOCO <ref type="bibr" target="#b9">[10]</ref>, and HOI-COCO <ref type="bibr" target="#b13">[14]</ref>, are built on top of the object categories provided in the COCO dataset <ref type="bibr" target="#b26">[27]</ref>. The MECCANO <ref type="bibr" target="#b35">[36]</ref> dataset focuses on human-object interactions in egocentric settings and industrial scenarios. Ambiguous-HOI <ref type="bibr" target="#b24">[25]</ref> is part of the HAKE project <ref type="bibr" target="#b23">[24]</ref>, where the focus is human activity understanding with a largescale knowledge base and visual reasoning.</p><p>Although our Bongard-HOI benchmark is built on top of the dataset HAKE <ref type="bibr" target="#b23">[24]</ref>, it differs from the existing visual relationship and HOI benchmarks, since we focus on humanlevel cognitive reasoning instead of recognition. To solve Bongard-HOI, one might not need to explicitly name the underlying visual relationship but does need to induce the HOI from a few images and perform context-dependent reasoning. Our results also suggest that Bongard-HOI cannot be trivially solved by the state-of-the-art models on these datasets, e.g. HOITrans <ref type="bibr" target="#b53">[54]</ref>.</p><p>Few-shot and meta learning models. Few-shot learning aims at learning from a limited number of training samples <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b17">18]</ref>. With the goal of extracting the generic knowledge across tasks and generalizing to a new task using task-specific information, meta-learning (or learningto-learn) <ref type="bibr" target="#b12">[13]</ref> becomes one of the leading approaches to deal with the few-shot learning problems. In general, meta-learning methods are divided into three categories: 1) memory-based methods, such as MANN <ref type="bibr" target="#b39">[40]</ref> and SNAIL <ref type="bibr" target="#b31">[32]</ref>, 2) metric-based methods, such as Matching Networks <ref type="bibr" target="#b46">[47]</ref> and ProtoNet <ref type="bibr" target="#b42">[43]</ref>, and 3) optimization-based methods, such as MetaOptNet <ref type="bibr" target="#b22">[23]</ref> and ANIL <ref type="bibr" target="#b34">[35]</ref>.</p><p>These meta-learning methods have been evaluated on several commonly used few-shot learning benchmarks, including miniImageNet <ref type="bibr" target="#b46">[47]</ref> and tieredImageNet <ref type="bibr" target="#b36">[37]</ref>. Although state-of-the-art meta-learning algorithms have achieved excellent performance on these standard few-shot image classification benchmarks, whether these approaches can gen-eralize to tasks where the concepts to learn (in a few-shot manner) are compositional, e.g. visual relationships rather than simple object categories is unknown <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b16">17]</ref>. In other words, existing benchmarks fail to account for the challenging problem of generalizing to new compositional concepts in few-shot learning. Therefore, with a focus on the more challenging visual concepts of visual relationships, we propose Bongard-HOI to serve as a new benchmark for the fewshot learning methods. We believe that our benchmark can foster the development of new few-shot learning, especially meta-learning algorithms to achieve better performances on learning and generalizing with compositional concepts.</p><p>Abstract visual reasoning benchmarks. Inspired by cognitive studies, several benchmarks have been built for abstract reasoning, highlighting cognitive abstract reasoning. Notable examples include compositional question answering <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b29">30]</ref>, physical reasoning <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b50">51]</ref>, math problems <ref type="bibr" target="#b41">[42]</ref>, and general artificial intelligence <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b49">50]</ref>. The most relevant to our benchmark are RPM <ref type="bibr" target="#b1">[2]</ref>, its variant with natural images <ref type="bibr" target="#b43">[44]</ref>, and Bongard problems with synthetic shapes <ref type="bibr" target="#b32">[33]</ref> and physical problems <ref type="bibr" target="#b48">[49]</ref>. While most of them consider synthetic images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49]</ref>, our Bongard-HOI benchmark studies cognitive reasoning on natural images, which impose unique challenges due to the difficulty of visual perception. Moreover, we use human-object interaction as the underlying concepts to construct few-shot instances, which require explicit compositional concept learning in a few-shot manner, compared to the object categories and shapes <ref type="bibr" target="#b43">[44]</ref>. Moreover, the existence of hard negatives in the few-shot instances makes our benchmark more challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we introduced the Bongard-HOI benchmark focusing on the few-shot learning and the generalization with compositional concepts in real-world visual relationship reasoning. Drawing inspirations from the classic Bongard problems <ref type="bibr" target="#b2">[3]</ref>, we constructed few-shot instances using the visual relationships between humans and objects as the underlying concepts. Our benchmark is built on top of an existing HOI dataset, HAKE <ref type="bibr" target="#b23">[24]</ref>, where we carefully curated the provided annotations to construct the few-shot instances. We benchmarked state-of-the-art few-shot learning methods, including both non-episodic and meta-learning approaches. Our findings suggested that current machine learning models still struggle to generalize beyond concepts that they have seen during the training process. Moreover, natural images in our benchmark contain rich stimuli, imposing great challenges to the machine learning models in the real-world visual relationship reasoning tasks. By building the Bongard-HOI benchmark, we hope to foster research efforts in real-world visual relationship reasoning, especially in holistic perception-reasoning systems and better representation learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Limitation Statement</head><p>We re-use the images collected by the HAKE <ref type="bibr" target="#b3">[4]</ref> creators, including the ones for HICO <ref type="bibr" target="#b3">[4]</ref>, V-COCO <ref type="bibr" target="#b9">[10]</ref>, OpenImages <ref type="bibr" target="#b19">[20]</ref>, HCVRD <ref type="bibr" target="#b51">[52]</ref>, and PIC <ref type="bibr" target="#b25">[26]</ref>, which were crawled from the web. Except the images, in this paper, no identity related information were collected nor used when constructing the dataset and benchmarking other approaches. It is possible, however, that some person may be identified via facial recognition techniques. We will provide contact information of the benchmark maintainer and commit to processing request of removing some certain images from the dataset. In addition, similar to other human-centric dataset, the images we use are from just a small portion of the population, which may contain biases toward some certain races, gender, ethnic groups, etc. We are unable to measure the bias as we do not have any identity-related data. We encourage researchers to investigate such issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More details on the Bongard-HOI Benchmark</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Constructing Bongard Problems</head><p>Given positive images I c that depict a certain relationship c = ?s, a, o? and and negative images Ic that does not, we need to sample few-shot instances from them. We randomly sample images to form P, N , and a query image I q . Two parameters control the sampling process: M , the number of images in P and N (M = 6 in Bongard-HOI), and the overlap threshold ? , indicating the maximum number of overlapped images between two few-shot instances. We want to sample as many few-shot instances as possible, but we also need to avoid significant image overlap between few-shot instances, which limits the diversity of the data. The sampling process in summarized in Algorithm 1. We set ? = 3 and ? = 2 for training and test sets, respectively. </p><formula xml:id="formula_7">t = overlap(P i , N i , I i q , Q); if t &lt; ? then Q = Q ? (P i , N i , I i q );</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Data Curation</head><p>Although the HAKE dataset <ref type="bibr" target="#b23">[24]</ref> has provided highquality annotations, we found that curations are still needed to construct the Bongard problems (few-shot instances) for our Bongard-HOI benchmark. Recall, to sample negative images, we assume a particular action is not depicted in them. In HAKE, an image region may have multiple action labels. Naively relying on the provided annotations is problematic as the action labels are either not manually exclusive or not exhaustively annotated. We show different cases of data curations in <ref type="figure" target="#fig_4">Fig. 6</ref> and discuss them in details as follows. Similar actions. Although some action labels may convey different semantic meanings, for some certain object categories, they look visually similar and indistinguishable. As shown in <ref type="figure" target="#fig_4">Fig. 6(a)</ref>, scratch cat and pet cat are hard to differentiate visually. If we simply use images of scratch cat as negatives to construct few-shot instances for pet cat, such few-shot instances are ambiguous, as it violates the basic assumption that the visual concept depicted in the Set A is not available in the Set B. We therefore simply merge such similar action labels to reduce the visual ambiguity. Hierarchical actions. Action labels are inherently hierarchical. For example, as shown in <ref type="figure" target="#fig_4">Fig. 6(b)</ref>, eat carrot very likely also means hold carrot visually. There are two problems to construct few-shot instances with multiple hierarchical action labels associated with the same image region. First of all, as we previously explained, using images of eat carrot as negatives for hold carrot may cause ambiguity. More importantly, there is the visual specificity issue. People tend to focus on capturing the most salient actions in an image, which are usually the parent actions (eat carrot in this case). In our preliminary experiments, images of eat carrot were used as positives for hold carrot to construct few-shot instances. We found that it caused a lot of confusion for human testers. To this end, we merge such hierarchical action labels for the same region, keeping the parent action labels only. Hard-to-see objects. In some cases, the person or the objects in image regions are hard to see. For example, in <ref type="figure" target="#fig_4">Fig. 6(c)</ref>, the person with the action label stand_on boat is hard to see clearly. On the one hand, it causes significant challenges for a visual perception system (e.g., <ref type="bibr" target="#b10">[11]</ref>) to accurately localize the meaningful objects. At the same time, it also imposes difficulty for annotators to accurately annotate the image region. We simply discard all image regions with hard-to-see objects.</p><p>Extrapolating actions. Actions are continuous. As a result, annotators tend to extrapolate the action label given a single image, instead of describing the current state the action. For example, as we can see in the top row of <ref type="figure" target="#fig_4">Fig. 6(d)</ref>, the eat action is about to happen. Yet, the action is different from a normal hold banana without any indication of eat. To distinguish different scenarios, we introduce hold_not_about_to_eat banana, hold_and_about_to_eat banana, and eat banana. In this way, all the actions are mutually exclusive. We can sample image regions for form few-shot instances without worrying about causing ambiguity. Inaccurate or confusing actions. In some rare cases, the annotations in HAKE are inaccurate or confusing, as shown in <ref type="figure" target="#fig_4">Fig. 6(e)</ref>. We modify the action labels if such a image region depicts a clear action label. Otherwise we discard such regions to avoid introducing ambiguity to sampled fewshot instances. MTurk data curation. After performing the aforementioned data curations, each image region is assigned to a single action label, describing the most salient content. Such action labels are mutually exclusive so that we can significantly reduce the ambiguity when constructing few-shot instances. Finally, we hire high-quality testers on the Amazon Mechanical Turk (MTurk) platform, who maintain a good job approval record, to curate the testing set to further remove the ambiguous few-shot instances. Every single BP is assigned to three independent testers. We compare their responses with the ground-truth labels and disard about 2.5% few-shot instances where none of the three testers correctly classifies the query images. We provide more details of the MTurk curations in Section D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Dataset statistics</head><p>Our Bongard-HOI benchmark provides disjoint training, validation, and testing sets. In specific, there are 118 concepts (visual relationships) and <ref type="bibr" target="#b20">21</ref>  responding to 167 and 166 visual concepts. Detailed distribution of concepts and few-shot instances among different generalization types are provided in <ref type="table" target="#tab_4">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Illustration about the Context-Dependent Reasoning Property</head><p>Two Bongard problems (few-shot instancess) are shown in <ref type="figure">Fig. 7</ref>. For the same query image, among different context (i.e., positive and negative examples), it receives different classification labels. This context-dependent reasoning property distinguishes our Bongard-HOI benchmark from other few-shot learning ones, where an image always has a fixed label.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More details on the oracle model</head><p>We first review how our oracle model works. Denoting the HOI detections in the P and N as D P and D N , respectively. D P contains the detections from all of the images in the P, defined as D P = {c P i } N P i=1 , where c P i is a HOI triplet. N P is the total number of detections. Note that there may be multiple or no detections for a single image. Similarly, D P is defined as</p><formula xml:id="formula_8">D P = {c P i } N P i=1 .</formula><p>According to the property of Bongard-HOI, the visual concept c P should only appear in the P, not in the N . We, therefore, compute c P as</p><formula xml:id="formula_9">c P = majority_vote(D P ? D N ),</formula><p>where ? is the set operator for set subtraction. Given the detections D q = {c q i } Nq i=1 for the query image I q , our prediction y becomes</p><formula xml:id="formula_10">y = 1, if c P ? D q , 0, otherwise.</formula><p>We now discuss some possible corner cases where the main paper does not cover.</p><p>What if majority_vote return multiple concepts? In this case, we simply enumerate each of them when making predictions for y. The predicted y will be 1 as long as at least one returned concepts present in D q ; otherwise it will be 0.</p><p>What if D P , D N or D q is empty? In case when D P is empty, we view this example as an failure case for our oracle model, as it does not induce the right concept as expected. On the contrary, it's totally fine that D N , meaning that no detection need to be removed from D P . Finally, how we handle the case when D q is empty depends on the true label y ? . If y ? is 1, then we view this example as an failure case. But we will make the prediction an automatic success if y ? is 0, since our oracle model finds there is no ground truth concept presenting in the query, which should be the right prediction.</p><p>We show successful cases of our oracle model in <ref type="figure">Fig. 8</ref>, <ref type="figure" target="#fig_0">Fig. 9, Fig. 10, Fig. 11</ref>. A failure case is shown in <ref type="figure" target="#fig_0">Fig. 12</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Details on MTurk Data Curation</head><p>User interface. The user interface of data curation on the Amazon Mechanical Turkp (MTurk) platform is shown in <ref type="figure" target="#fig_0">Fig. 13</ref>. In the top part, we show images depicting a common visual relationship between human and objects in the left P N Query images:</p><p>Predictions: positive negative <ref type="figure" target="#fig_0">Figure 11</ref>. Illustration of our oracle model. The concept in P is hold clock.</p><p>(i.e., positive examples P in our Bongard problem). In the right, images that do not contain the visual relationship are shown (i.e., negative examples N ). In the bottom part, given a query image, a tester needs to decide whether it depicts the particular visual relationship or not. Each MTurk job contains two few-shot instances, where a tester can freely switch between two pages. They can only submit the job once both two tasks are finished. We do not tell the testers what objects to focus on to induce the common visual relationship. It is intended to be similar to what a machine learning model does, which needs to do object detection first. Simple examples given to testers. To ensure testers who see the form of few-shot instances for the first time can successfully finish the job, we provide some examples of different visual relationships and encourage them to take a look at these examples before starting working on a job. Such examples are shown in <ref type="figure" target="#fig_0">Fig. 14.</ref> MTurk job setting. We provided more details about the job setting below.</p><p>? Region. We restrict the regions of testers to be in the US and Germany.</p><p>? Approval rate. Each MTurker tester maintains a job approval rate based on their performance on previous jobs. We invite only MTurk testers whose job approval rate is P N Query images:</p><p>Predictions: negative (wrong) negative <ref type="figure" target="#fig_0">Figure 12</ref>. A failure of our oracle model. The concept in P is eat cake. The HOITrans model <ref type="bibr" target="#b52">[53]</ref> incorrectly recognizes the first query image as hold cake (which should be eat cake). As a result, it makes a wrong prediction for the first query image. equal to or greater than 98%.</p><p>? Number of approved jobs. Setting a qualification for the job approval rate only is not sufficient to hire highquality testers since newly registered novel testers have a job approval rate of 100%. Therefore, we also set a qualification such that only testers who have more than 500 jobs approved previously are invited.</p><p>? Invited annotators. Through a couple of small-scale preliminary studies, we identified 35 reliable annotators on MTurk. For the large-scale data curation, we invited them to participate only.</p><p>? Reward setting. We provide $0.15 for each job with an additional $0.15 bonus if consistently high-quality annotations are made. According to our experiences of finishing the job, it roughly corresponds to about $30 per hour.</p><p>? Number of testers for each job. We hire three independent testers for each job and aggregate their annotations. In specific, we only keep the few-shot instances where at least one of the three testers correctly classified the query image according to the ground-truth annotations. Otherwise, it suggests that a BP is either ambiguous or too difficult. We discard 2.5% of the few-shot instances that we submitted to MTurk.</p><p>? Job life time. A job will not be available after 7 days if it is not claimed by any tester. But we found that all of the jobs were finished within such a limit. <ref type="figure" target="#fig_0">Figure 13</ref>. The user interface (UI) of MTurk data curation. <ref type="figure" target="#fig_0">Figure 14</ref>. Examples of different visual relationships given to MTurk testers. For each example, we tell what the visual relationship is so that the testers can better understand the scope of the job.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>https://github.com/nvlabs/Bongard-HOI Illustration of a few-shot learning instance from our Bongard-HOI benchmark. The positive images in the top left part follow the visual relationship of riding a bike between the person and objects while such a relationship does not exist in the negative examples. Note that an actual problem in Bongard-HOI contains 6 images of positive examples, 6 negative examples, and 1 query image, which is different from the illustration here.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Examples of different actions with the same object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Class-agnostic (objectness) detections. We show the detections from our class-agonostic detector (in green) and groundtruth human and object boxes (in red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 :</head><label>1</label><figDesc>Sample few-shot instances for a visual concept c Input: Positive images I c , negative images Ic, number of images in a few-shot instance M , overlap threshold ? . Output: Sampled few-shot instances Q. Q = ?; while True do P i , N i , I i q = sample_instance(I c , Ic, M ); if sample fails then break;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Samples of annotations where curations are needed. For each image region, its annotated action labels are show on its top and bounding boxes corresponding to the person and object are shown for visualization purpose. From left to right: (a) similar actions, (b) hierarchical annotations, (c) hard-to-see objects, (d) extrapolating annotations, and (e) inaccurate or confusing annotations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .Figure 8 .Figure 9 .Figure 10 .</head><label>78910</label><figDesc>Illustration of the context-dependent reasoning property of the Bongard problems (few-shot instances) in our Bongard-HOI benchmark. Two instances are shown here with their underlying visual concepts (relationships) displayed on top with red color. The same query image receives two different labels (negative in the top and positive in the bottom) among different context (i.e., positive and negative examples). Illustration of our oracle model. The concept in P is wash car. Illustration of our oracle model. The concept in P is ride car. Illustration of our oracle model. The concept in P is teach person.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>,956 few-shot instances in the training set. There are 17,184 and 13,941 few-shot instances in the validation and testing set, respectively, cor-</figDesc><table><row><cell></cell><cell cols="2">seen object unseen object</cell></row><row><cell>seen action</cell><cell>99 / 5008</cell><cell>36 / 5002</cell></row><row><cell>unseen action</cell><cell>20 / 3402</cell><cell>12 / 3775</cell></row><row><cell></cell><cell>(a) validation set</cell><cell></cell></row><row><cell></cell><cell cols="2">seen object unseen object</cell></row><row><cell>seen action</cell><cell>102 / 4476</cell><cell>27 / 4562</cell></row><row><cell>unseen action</cell><cell>21 / 3291</cell><cell>16 / 1612</cell></row><row><cell></cell><cell>(b) test set</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Number of concepts and few-shot instances in the validation and test sets. Depending on whether an action and object is seen during the training, we divide the validation and test sets into four categories, where we can study the systematic generalization of machine learning models. For each category, we show number of concepts (combinations of action and object) and number of few-shot instances.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Some combinations of objects and actions are infeasible.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Phyre: A new benchmark for physical reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Bakhtin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Gustafson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5083" to="5094" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Measuring abstract reasoning in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="511" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">The recognition problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bongard</forename><surname>Mikhail Moiseevich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
		</imprint>
		<respStmt>
			<orgName>Foreign Technology Div Wright-Patterson AFB Ohio</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">HICO: A benchmark for recognizing human-object interactions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wei</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improved baselines with momentum contrastive learning. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A new meta-baseline for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinbo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04390</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">On the measure of intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.01547</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A bayesian approach to unsupervised oneshot learning of object categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fe-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Ninth IEEE International Conference on Computer Vision</title>
		<meeting>Ninth IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1134" to="1141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04474</idno>
		<title level="m">Visual semantic role labeling</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="386" to="397" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to learn using gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Younger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Peter R Conwell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Detecting human-object interaction via fabricated compositional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Baosheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual compositional learning for human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="584" to="600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compositional learning for human object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keizo</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="234" to="251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML deep learning workshop</title>
		<meeting><address><addrLine>Lille</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The open images dataset v4: Unified image classification, object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alina</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jasper</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shahab</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
	<note>and visual relationship detection at scale. IJCV, 2020. 3</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">One shot learning of simple visual concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brenden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
		<editor>Laura A. Carlson, Christoph H?lscher, and Thomas F. Shipley</editor>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human-level concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Meta-learning with differentiable convex optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinash</forename><surname>Ravichandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="10657" to="10665" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">HAKE: human activity knowledge engine. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoshu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1904" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Detailed 2d-3d joint representation for human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong-Lu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiefeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PPDM: parallel point detection and matching for real-time human-object interaction detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Relvit: Concept-guided vision transformer for visual relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weili</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaowei</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Orbit: A real-world few-shot dataset for teachable object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Massiceti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luisa</forename><surname>Zintgraf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Bronskill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lida</forename><surname>Theodorou</surname></persName>
		</author>
		<idno>ICCV, 2021. 4</idno>
		<editor>Matthew Tobias Harris, Ed Cutrell, Cecily Morrison, Katja Hofmann, and Simone Stumpf</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">A simple neural attentive meta-learner. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bongard-logo: A new benchmark for human-level concept learning and reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weili</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><forename type="middle">B</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Coco attributes: Attributes for people, animals, and objects. European Conference on Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Genevieve</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Rapid learning or feature reuse? towards understanding the effectiveness of maml. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aniruddh</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maithra</forename><surname>Raghu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The meccano dataset: Understanding human-object interactions from egocentric videos in an industrial-like domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Ragusa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salvatore</forename><surname>Livatino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1569" to="1578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Meta-learning for semi-supervised few-shot classification. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sachin</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Faster R-CNN: towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Describing common human visual actions in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matteo</forename><surname>Ruggero Ronchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Meta-learning with memoryaugmented neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Bartunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1842" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Saxton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01557</idno>
		<title level="m">Analysing mathematical reasoning abilities of neural models</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Prototypical Networks for Few-shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Chunhua Shen, and Anton van den Hengel. V-prom: A benchmark for visual reasoning using visual progressive matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiewei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingqiao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Meta-dataset: A dataset of datasets for learning to learn from few examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eleni</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tyler</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Utku</forename><surname>Evci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Gelada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>ICLR, 2020. 1, 4, 5</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds-200-2011 Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno>- port CNS-TR-2011-001</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Re</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Physical bongard problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Weitnauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helge</forename><surname>Ritter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ifip international conference on artificial intelligence applications and innovations</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="157" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sirui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiyu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11344</idno>
		<title level="m">Halma: Humanlike abstraction learning meets affordance in rapid problem solving</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">CLEVRER: collision events for video representation and reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<idno>ICLR, 2020. 8</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">HCVRD: A benchmark for large-scale human-centered visual relationship detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">D</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<editor>Sheila A. McIlraith and Kilian Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2018" />
			<publisher>AAAI</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">End-to-end human object interaction detection with hoi transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">End-to-end human object interaction detection with hoi transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boxun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
