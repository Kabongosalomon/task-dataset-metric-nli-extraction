<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Video Instance Segmentation with a Propose-Reduce Paradigm</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaijia</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 2 SmartMore</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizheng</forename><surname>Wu</surname></persName>
							<email>rzwu@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 2 SmartMore</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
							<email>sliu@smartmore.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 2 SmartMore</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangbo</forename><surname>Lu</surname></persName>
							<email>jiangbo@smartmore.com</email>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 2 SmartMore</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The Chinese University of Hong</orgName>
								<address>
									<addrLine>Kong 2 SmartMore</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Video Instance Segmentation with a Propose-Reduce Paradigm</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video instance segmentation (VIS) aims to segment and associate all instances of predefined classes for each frame in videos. Prior methods usually obtain segmentation for a frame or clip first, and merge the incomplete results by tracking or matching. These methods may cause error accumulation in the merging step. Contrarily, we propose a new paradigm -Propose-Reduce, to generate complete sequences for input videos by a single step. We further build a sequence propagation head on the existing image-level instance segmentation network for long-term propagation. To ensure robustness and high recall of our proposed framework, multiple sequences are proposed where redundant sequences of the same instance are reduced. We achieve stateof-the-art performance on two representative benchmark datasets -we obtain 47.6% in terms of AP on YouTube-VIS validation set and 70.4% for J&amp;F on DAVIS-UVOS validation set. Code is available at https://github.com/ dvlab-research/ProposeReduce.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video instance segmentation (VIS), proposed in <ref type="bibr" target="#b52">[53]</ref>, is a task to segment all instances of the predefined classes in each frame. Segmented instances are linked in the entire video. It is important in the field of video understanding, and can be applied to video editing, autonomous driving, etc. Unlike image-level instance segmentation, VIS requires not only detection and segmentation of each frame, but also tracking of objects in the video, which make it a very challenging task.</p><p>Recently, several approaches were proposed for this task <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b28">29]</ref>. Based on the patterns of generating instance sequences, existing frameworks can be roughly categorized into two paradigms: 'Track-by-Detect' <ref type="figure" target="#fig_0">(Fig. 1(a)</ref>) and 'Clip-Match' <ref type="figure" target="#fig_0">(Fig. 1(b)</ref>). The 'Track-by-Detect' paradigm detects and segments instances for each individual frame, followed by obtaining instance sequences * Equal Contribution. with frame-by-frame tracking <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b6">7]</ref>. Differently, 'Clip-Match' adopts the divide-and-conquer strategy. It divides an entire video into multiple short overlapped clips, and obtains VIS results for each clip and generates instance sequences with clip-by-clip matching <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>. Both of the paradigms need two independent steps to generate a complete sequence. They both generate multiple incomplete sequences (i.e., frames or clips) from a video, and merge (or complete) them by tracking/matching at the second stage. Intuitively, these paradigms are vulnerable to error accumulation in the process of merging sequences, especially when occlusion or fast motion exists.</p><p>To avoid error accumulation brought by merging incomplete sequences, one intuitive solution is to generate a complete instance sequence for an entire video with only one step. As shown in <ref type="figure" target="#fig_0">Fig. 1(c)</ref>, starting from any key frame of a video, we can obtain instance sequences by propagating the instance segmentation results from this frame to all oth- ers. However, the propagation quality from different starting key frames varies a lot (as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>). One key frame may only contain part of the instances in a video, inappropriate for the whole sequence.</p><p>For robust propagation and high recall to cover enough instances, we propose a new paradigm, named Propose-Reduce ( <ref type="figure" target="#fig_0">Fig. 1(d)</ref>). It first produces sequence proposals from multiple key frames and reduces the redundant sequence proposals of the same instances. This paradigm not only discards the step of merging incomplete sequences, but also achieves robust results considering multiple key frames.</p><p>The idea behind Propose-Reduce is proved effective in the task of image-level object detection. Methods for this task can be classified as one-stage <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b24">25]</ref> and twostage <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b20">21]</ref> detection frameworks. Compared with the one-stage frameworks, the two-stage ones first generate a large number of candidate proposals by the region proposal network (RPN) <ref type="bibr" target="#b36">[37]</ref> to ensure high recall, and then reduce abundant proposals by non-maximum suppression (NMS) as post-processing. The great performance of the two-stage detection frameworks shows the potential of our Propose-Reduce in the video domain.</p><p>Based on the above analysis, to propagate instance segmentation from each key frame to all other frames, we design an additional module for long-term propagation, since a frame to be propagated can be far from the key frame. We propose attaching a sequence propagation head (Seq-Prop head) upon a widely-used image-level instance segmentation network: Mask R-CNN <ref type="bibr" target="#b13">[14]</ref>. It enables sharing backbone features for multiple heads of different functions of classification head, bounding box head, mask head and sequence propagation head. With the sharing backbone, our propagation module is light-weighted.</p><p>Besides, we adopt a memory propagation strategy on every key frame to enable long-term propagation. After obtaining sequence proposals from all key frames, we imple-ment a variant of NMS to reduce redundant proposals at the sequence level. With the above design, our overall framework is neat and can be trained in an end-to-end fashion. The overall contributions are summarized below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image-Level Instance Segmentation Image-level instance segmentation is a classical computer vision task with many solutions <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b34">35]</ref> proposed. They can be mainly divided into top-down <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18]</ref>, bottom-up <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b29">30]</ref>, and direct segmentation methods <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b50">51]</ref>. Among these methods, top-down structure is popular for high performance. It first utilizes detectors to detect objects and then segments them based on detected bounding boxes. One representative method is Mask R-CNN <ref type="bibr" target="#b13">[14]</ref>. It is built on a two-stage detector <ref type="bibr" target="#b36">[37]</ref>, adds a mask head for segmentation upon the detector, and keeps the original classification head as well as the bounding box head in the detector. We design our framework based on Mask R-CNN from image to video domain. We introduce an extra sequence propagation head as well as a new paradigm for both spatial and temporal processing in the video instance segmentation task. Our method is simple and surprisingly effective.</p><p>Video Instance Segmentation Video instance segmentation (VIS) was introduced in <ref type="bibr" target="#b52">[53]</ref>, which requires classification, segmentation and tracking on instances simultaneously in a video. existing work <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b18">19]</ref> can be divided into two types based on the way of sequence generation.</p><p>One straight-forward paradigm is 'Track-by-Detect' <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b28">29]</ref> with two parts: detection and tracking. In the detection part, instances are located with existing image-level instance segmentation methods <ref type="bibr" target="#b13">[14]</ref> in a frame-by-frame manner. The detected instances are associated among different frames in the tracking part. Another paradigm is summarized as 'Clip-Match' based methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. It divides an entire video into multiple short clips and completes  <ref type="figure">Figure 3</ref>. The Propose-Reduce paradigm consists of two stages. In the Sequence Proposal Generation stage, a sequence set S k is generated by first detecting O instances at the k th key frame. We assume frame t selected as the k th key frame for convenience. Instance set S k (t) at frame t are then propagated to the whole video with memory K-Propagation (Sec. 3.2.2). K ? O sequences {S o k } are gathered to form a redundant set S, which is reduced to the final sequence set S in the Sequence Proposal Reduction stage. Different texture in circles differentiates among instances. the VIS task in a clip-by-clip manner via propagation <ref type="bibr" target="#b1">[2]</ref> or spatial-temporal embedding <ref type="bibr" target="#b0">[1]</ref>. Neighboring clips are merged with matching (e.g., bipartite graph matching).</p><p>Semi-supervised Video Object Segmentation Semisupervised video object segmentation (VOS) <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34]</ref> refers to the problem of segmenting specified objects in videos given the annotated first frame. Research <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b25">26]</ref> was extensive. The most related methods to our framework are propagation-based, which propagate segmentation masks from the annotated first frame to the rest of the videos. Early research work <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b49">50]</ref> propagates in a frame-by-frame pipeline, which is fragile and easily fails in distant frames due to occlusion and fast motion.</p><p>Recent memory-based method STM <ref type="bibr" target="#b30">[31]</ref> resolves the problem in long-term propagation. Several methods <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b38">39]</ref> were proposed to improve the performance of STM. In this paper, the propagation strategy of our Seq-Prop head is inspired by STM. Compared with STM that adopts two separate backbones for extracting features, we make the Seq-Prop head light-weighted by sharing the same feature backbone with other heads in Mask R-CNN.</p><p>Unsupervised Video Object Segmentation Compared with semi-supervised VOS, no annotated frame is given in unsupervised VOS (UVOS) <ref type="bibr" target="#b5">[6]</ref>. UVOS can be regarded as a variant of VIS, while VIS segments objects with pre-defined classes. UVOS is to segment class-agnostic salient objects. Recent work detects salient objects <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b55">56]</ref>. Besides, topological structures <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b43">44]</ref> were proposed to obtain object segmentation in temporal domain. For example, RNN <ref type="bibr" target="#b41">[42]</ref> structure or graph convolution network <ref type="bibr" target="#b43">[44]</ref> can be utilized. Similar to VIS, detect-by-track <ref type="bibr" target="#b28">[29]</ref> and clipmatch based <ref type="bibr" target="#b0">[1]</ref> methods can be applied to UVOS, which first generate instance segmentation on a single frame (or clip) and track (or match) objects across frames or clips. Our paradigm can also be applied with the classification head modified from multi-class classification to two-class one (i.e., foreground and background).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>We propose the paradigm Propose-Reduce for the task of video instance segmentation. As shown in <ref type="figure">Fig. 3</ref>, the paradigm consists of two stages. Redundant sequence proposals are generated in the first stage (Sec. 3.1). We obtain instance segmentation on K selected key frames (Sec. 3.1.1). The segmentation results are then propagated to the whole video (Sec. 3.1.2) with our proposed Seq Mask R-CNN framework (Sec. 3.1.3). To reduce the redundancy in sequence proposals, in the second stage (Sec. 3.2), a se-</p><formula xml:id="formula_0">M ? {S k (t)}; for j = t ? 1; j ? 0; j ? j ? 1 do S k (j) ? Propagate(M, I j ); // (Sec. 3.1.3) M ? M ? S k (j); end S k ? (S k (0), S k (1), ..., S k (T ? 1)); k ? k + 1; end S ? S 0 ? S 1 ? ... ? S K?1 ; // Gather return S;</formula><p>quence reduction method is applied to all sequences for final sequence set as output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Sequence Proposals Generation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Key Frames Selection</head><p>To generate sequence proposals, we first select K key frames to obtain their image-level instance segmentation masks. Specifically, for a T -frame video, the K key frames {I g(0) , I g(1) , ..., I g(K?1) } are selected at fixed intervals evenly, given by</p><formula xml:id="formula_1">g(k) = max{?T /K?, 1} ? k, k = 0, ..., K ? 1. (1)</formula><p>The number of key frames plays an important role in our design. As described in Sec. 1, when selecting only one key frame, it degrades to paradigm (c) in <ref type="figure" target="#fig_0">Fig. 1</ref>, where the final results are highly dependent of the instance segmentation quality in the selected key frames. However, when many key frames are selected, the computational cost of detection and propagation would increase. We accordingly choose a small number of key frames in our experiments.</p><p>For each key frame, we generate its instance segmentation results by multiple heads (i.e., bbox, classification and mask head) in Seq Mask R-CNN. For non-key frames, we only extract the backbone and FPN <ref type="bibr" target="#b20">[21]</ref> features of these frames for the following propagation step. It saves computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Memory K-Propagation</head><p>The instance masks on K selected key frames are propagated bi-directionally to obtain K sets of mask sequences, i.e., {S 0 , S 1 , ..., S K?1 }, as illustrated in Alg. 1. After all propagation finished, we gather K sequence sets from different key frames into one set</p><formula xml:id="formula_2">S = S 0 ? S 1 ? ... ? S K?1 .</formula><p>As shown in Alg. 1, we maintain a memory M to alleviate error accumulation in long-term propagation <ref type="bibr" target="#b30">[31]</ref>. It stores the encoded feature of previously segmented frames and propagates mask information to the current frame. The operations on M (e.g., read and update) are similar to that of STM <ref type="bibr" target="#b30">[31]</ref>. The difference is that the memory in <ref type="bibr" target="#b30">[31]</ref> is utilized to propagate from the annotated first frame to the end of a video, while our work propagates the estimated mask from a key frame to the beginning and end of the video for K times.</p><p>Directly applying STM to our paradigm requires another two backbones to extract features for memory and query. Instead, we design an additional propagation module that can be seamlessly inserted into the image-level instance segmentation frameworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Sequence Mask R-CNN</head><p>We incorporate a propogation head (Seq-Prop head) on the top of Mask R-CNN for memory K-Propagation, which is called Sequence Mask R-CNN (Seq Mask R-CNN).</p><p>Architecture The architecture of Seq Mask R-CNN is shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. It is based on outputting instance segmentation results for a single image, to which we add an extra propagation head that propagates instance masks to other frames. <ref type="figure" target="#fig_4">Fig. 4</ref> illustrates an example that takes two frames as input. We refer to them as the guidance frame (the t th frame) and the query frame (the (t + ?) th frame). For the guidance frame, we take the estimated mask M g and largest FPN <ref type="bibr" target="#b20">[21]</ref> feature P g 2 as input for encoding feature F g . For a query frame, we take its P q 2 feature as input to obtain feature F q . With the two encoded feature maps, we utilize a nonlocal operation (NL) <ref type="bibr" target="#b45">[46]</ref> to propagate mask information from the guidance frame to the query one and obtain the propagated feature F g?q . Finally, F g?q and the largest backbone feature C q 2 from the query frame are utilized for decoding and generating the query mask M q . The FPN feature P q 2 and the backbone feature C q 2 are utilized in encoding and decoding respectively, since they contain the richest semantic and detailed information on multiple instances.</p><p>Training In the training stage, we randomly select two frames as input for memory efficiency, i.e., one guidance and one query frame. In one epoch, the query frame in a pair of frames is selected once per frame per video. The guidance frame is randomly sampled from the same video.  We adopt Seq-Prop head on Mask R-CNN for propagating instance masks from the guidance frame at time t to a query frame at time t + ?. P g 2 , P q 2 are the largest FPN <ref type="bibr" target="#b20">[21]</ref> feature of input images, and C q 2 is the largest backbone feature. NL is a non-local operation <ref type="bibr" target="#b45">[46]</ref>. '?2' denotes 2 consecutive residual blocks. '?' and '?' denote matrix multiplication and summation, respectively. Detailed architectures are illustrated in the supplementary files.</p><p>Besides, in order to make the Seq-Prop head learn to propagate from the imperfect segmented masks, we utilize the estimated instance masks instead of the ground-truth ones as the guidance input for training. It makes the head more robust at the inference stage.</p><p>To train our overall framework, we adopt a multi-task loss L = L cls + L box + L mask + L prop . The classification loss L cls , bounding-box loss L box , and mask loss L mask are the same as those in Mask R-CNN <ref type="bibr" target="#b13">[14]</ref>. As for the propagation loss L prop utilized to train Seq-Prop head, we adopt a scale-balanced soft IoU loss <ref type="bibr" target="#b19">[20]</ref>, since the Seq-Prop head propagates multi-scale instances masks at the same time.</p><p>Inference During the stage of inference, the guidance frame input is replaced by a memory pool (illustrated in Alg. 1), which stores the encoded features from the frames that have been propagated. Specifically, for each iteration of propagation, memory is updated by appending the encoded feature of the current frame, which increases model's robustness to occluded instances <ref type="bibr" target="#b30">[31]</ref>.</p><p>By sharing backbone features with the other three heads in Seq Mask R-CNN, our propagation head discards two heavy encoders for memory and query in STM <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sequence Proposals Reduction</head><p>Redundant sequence proposals exist after the first stage where sequences of the same instance may be generated for multiple times from different key frames. To reduce redundancy, inspired by NMS <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38]</ref> that is widely used in image-level instance segmentation in post-processing, we adopt a variant of NMS for sequences reduction. To apply it to sequences, three key elements in NMS need to be defined, i.e., the input sequence sets, sequence score and sequences IoU.</p><p>Input Sequence Set In the stage of sequence proposal generation, we obtain K sets of sequence proposals {S 0 , S 1 , ..., S K?1 } gathered as S. For each sequence set S k , we have its corresponding mask M (S k ) and classification score C(S k ). We set the maximum instance number in a key frame as O. Then S k can be represented as a set of instance sequences </p><formula xml:id="formula_3">S = {S o k }, where k ? [0, K-1], o ? [0, O-1]</formula><p>, which consists of a maximum of K ? O instance sequences. Since the instance number per key frame is less than O in most cases, many sequences in S are empty and the sequence number is much less than K ? O. Our target is to reduce the redundant sequence set S into a final sequence set S.</p><p>Sequence Score The score for each instance sequence C(S o k ) reflects its priority to be selected. To represent the priority of an instance sequence, we consider all frames in this sequence. For each instance, on any frame I t , we obtain its classification score C(S o k (t)) ? [0, 1] |C| from the classification head of Seq Mask R-CNN, where |C| indicates the number of instance classes. We average the scores among all frames and take the max score among |C| classes as the score of this instance sequence. The score for each sequence C(S o k ) is defined as</p><formula xml:id="formula_4">C(S o k ) = max |C| 1 T T ?1 t=0 C(S o k (t)) .<label>(2)</label></formula><p>Sequences IoU Intersection-over-union (IoU) between two sequences measures their overlap. We calculate the mask IoU instead of the bounding-box IoU to measure the overlap more precisely. We denote the masks of two sequences as M (S o k ) and M (S? k ), where M (S o k ) indicates the mask sequence of the o th instance from the k th key frame, and M (S? k ) is similarly defined. Then the IoU between two sequences, i.e., IoU (S o k and S? k ), is computed as</p><formula xml:id="formula_5">IoU (S o k , S? k ) = T ?1 t=0 |M (S o k (t)) ? M (S? k (t))| T ?1 t=0 |M (S o k (t)) ? M (S? k (t))| ,<label>(3)</label></formula><p>where M (S o k (t)) and M (S? k (t)) are the masks of the t th frame from the two sequences S o k and S? k , respectively. With the defined sequence set, sequence score and sequences IoU, we directly apply the traditional NMS algorithm to the sequence set to reduce the redundant sequences. More details of this algorithm is included in our supplementary files. The sequence set S after NMS is our final result for the task of VIS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>YouTube-VIS <ref type="bibr" target="#b52">[53]</ref> YouTube-VIS dataset is currently the largest dataset for video instance segmentation task. It contains 2,238 training videos and 302 validation videos, with 40 categories involved. Validation scores are evaluated on online benchmark. Similar to image instance segmentation <ref type="bibr" target="#b21">[22]</ref> , the benchmark adopts Average Precision (AP) and Average Recall (AR) metrics to evaluate the sequence accuracy averaged over the category set. DAVIS-UVOS <ref type="bibr" target="#b5">[6]</ref> DAVIS-UVOS dataset is proposed for unsupervised video object segmentation for salient generic objects. It contains 60 training videos and 30 validation videos with high-quality annotations. This task can be viewed as a special case of video instance segmentation with 2 categories (foreground and background). In the evaluation stage, it considers no more than 20 predicted sequences in a video and measures the average between J scores (the mean IoU between the estimated mask and ground-truth) and F scores (the F-measure of the estimated mask boundaries) via bipartite graph matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation Details</head><p>The training data in the above datasets is not sufficient, resulting in over-fitting. To alleviate this issue, we adopt 80K training images in the COCO <ref type="bibr" target="#b21">[22]</ref> dataset (image instance segmentation) for compensation (also adopted in <ref type="bibr" target="#b0">[1]</ref>). For each image in COCO, we augment it with ?30 ? rotation to generate a three-frame pseudo video. For the training on YouTube-VIS, we only select images with overlapping categories in COCO. For DAVIS-UVOS, we select all images from COCO and treat all annotated instances as one category, i.e., foreground.</p><p>Our training consists of two stages, i.e., main-training stage and finetuning stage. In the main-training stage, we first train our model on the mixed dataset including COCO and the video dataset (i.e., YouTube-VIS, DAVIS-UVOS) for 4 epochs with 640 ? 320 input size. In the finetuning stage, for YouTube-VIS dataset, the model is trained on this dataset with the same input size, while the model finetuned on DAVIS-UVOS dataset takes 854 ? 480 size as input. We finetune the models for 5 epochs for both datasets.</p><p>All models are trained with 6 NVIDIA Titan X GPUs, implemented by PyTorch. The training time takes about 2-4 days for each dataset. We set K as 6 for YouTube-VIS and 4 for DAVIS-UNVOS (see <ref type="bibr">Sec. 4.4)</ref>. More details are included in our supplementary files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Main Results</head><p>YouTube-VIS The quantitative results on YouTube-VIS are included in <ref type="table">Table 1</ref>. We list the backbone <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b2">3]</ref> used in different methods for fair comparison. MaskProp, the SOTA method, takes a strong backbone (i.e., STSN [3]-ResNeXt-101) to extract spatial-temporal features, and a stronger detection head (i.e., HTC <ref type="bibr" target="#b7">[8]</ref>) to refine detection results iteratively. In contrast, our best model only uses ResNeXt-101 to extracting spatial representation features and the vanilla head in Mask R-CNN for detection. Our model already outperforms MaskProp by 1% in terms of AP and 3.4% in terms of AR@10.</p><p>The large improvement of recall stems from the sampling strategy on multiple key frames. Note that MaskProp adopts a post-process that refines masks to gain 1.9% improvement, while Seq Mask R-CNN does not adopt this post-process. The previous best method that only extracts spatial features is EnsembleVIS, which combines four separate networks into a complex system, including detection <ref type="bibr" target="#b13">[14]</ref>, classification <ref type="bibr" target="#b51">[52]</ref>, re-identification <ref type="bibr" target="#b28">[29]</ref>, and segmentation <ref type="bibr" target="#b8">[9]</ref>. Our single-model method surpasses Ensemble-VIS by 2.8% in AP and 4.3% in AR@10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DAVIS-UVOS</head><p>We also evaluate our approach on DAVIS-UVOS dataset, as shown in <ref type="table" target="#tab_1">Table 2</ref>. The SOTA method UnOVOST combines multiple models (e.g., Mask R-CNN <ref type="bibr" target="#b13">[14]</ref>, PWC-Net <ref type="bibr" target="#b40">[41]</ref> and ReID Net <ref type="bibr" target="#b48">[49]</ref>) into a complex system. Our single-model method with ResNet-101 backbone achieves a comparable performance. With a stronger backbone (i.e., ResNeXt-101), our method outperforms Un-OVOST in both J and F scores. Compared with SOTA single-model method STEm-Seg, our method with the same backbone (i.e., ResNet-101) surpasses it by 3.6% in J &amp;F.</p><p>Visualization We further present the comparison with previous paradigms ( <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b0">1]</ref>) in long-term occlusion scenarios, as shown in <ref type="figure" target="#fig_6">Fig. 5</ref>. Salient objects (bear/surfboard) are occluded by trees/waves in multiple frames. Track-by-Detect <ref type="bibr" target="#b6">[7]</ref> fails to re-identify the same instance with distorted appearance. Clip-Match <ref type="bibr" target="#b0">[1]</ref> treats them as two instances due to being out of the matching scopes. In contrast, our paradigm generates complete sequences via long-term propagation. More visual results are shown in <ref type="figure">Fig. 9</ref>. Our method fails to propagate masks with highly consistentlyoccluded instances of the same category (i.e., person).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Experiments</head><p>All the ablation experiments are conducted with the ResNeXt-101 <ref type="bibr" target="#b51">[52]</ref> backbone on YouTube-VIS and DAVIS-UVOS validation sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training Stage</head><p>We conduct experiments to study the effect of different training stages (Sec. 4.2), as shown in <ref type="table" target="#tab_3">Table 3</ref>. With the finetuning stage only, the large drop of performance indicates that insufficient video data leads to over-fitting. Adopting the main-training stage only alleviates over-fitting. It is still hard to reach the performance by    <ref type="figure">Figure 6</ref>. Visual results on DAVIS-UVOS and YouTube-VOS. Frames are sampled at challenging moments (e.g., fast motion). We also show in the last row a failure case of overlapped same-category instances, where the arm of one occluded person is segmented to the other. Category 'Instance' in DAVIS-UVOS denotes the salient generic object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Seq.</head><p>YouTube DAVIS</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Backbone</head><p>Reduce AP AR@100 J F pears after the category assignment. Sequences assigned to the same category conflict with each other in the final evaluation. These redundant sequences can be filtered by applying the same sequence reduction techniques (Sec. 3.2) for each category. Ablation results on Tab. 5 demonstrate that such a category-aware reduction post-processing stably improves the accuracy on different backbones. Key Frames In our inference paradigm, the number of key frames (hyper-parameter K) plays an important role in controlling the trade-off between accuracy and efficiency. As shown in <ref type="figure" target="#fig_7">Fig. 7</ref>, our model performs poorly when K = 1 since one sequence proposal is sensitive to segmentation quality at the key frame (see also <ref type="figure" target="#fig_1">Fig. 2</ref>) and may miss some instances. When sampling more key frames, accuracy increases dramatically as sufficient sampling improves robustness and recall for the same instance. With more key frames sampled, accuracy fluctuates, probably because the quality of instance sequences varies among different key frames and the estimated sequence score (Eq. (2)) in NMS cannot effectively reflect the priority of the sequences. Note that the accuracy in YouTube-VIS gradually drops as K ? 8, since its evaluation metric (Sec. 4.1) is sensitive to false positives from residual redundant sequences after NMS. We set K to 6 and 4 for YouTube-VIS and DAVIS-UVOS by default.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have proposed a new paradigm to tackle the task of video instance segmentation, which requires no tracking/matching part to avoid error accumulation and ensures high recall. Following the paradigm, we design our framework named Seq Mask R-CNN, which incorporates a newly-designed propagation head on Mask R-CNN. The extensive experiments verify the effectiveness of our framework. Besides, this work provides a new perspective on the VIS task, which motivates future work on extending imagelevel methods to video domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Overview</head><p>We provide additional details in this supplementary file. Sec. B describes the details of the sequence proposals reduction. In Sec. C, we describe more details regarding the Seq-Prop head. In Sec. D, we clarify more details of the implementation. More discussions are presented in Sec. E. More visual results are shown in Sec. F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Sequence Proposals Reduction</head><p>With the defined input sequence set S, sequence score (Eq. (1)) and sequences IoU (Eq. (2)) described in our main paper (Sec. 3.2), we apply the traditional NMS algorithm on the sequence set to reduce the redundant sequences. The algorithm for sequence proposal reduction is illustrated in Alg. 2. The IoU threshold ? is set to 0.5 in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Seq-Prop Head</head><p>Architecture The detailed architecture of the Seq-Prop head is shown in <ref type="figure" target="#fig_9">Fig. 8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soft-Agg</head><p>The soft aggregation <ref type="bibr" target="#b49">[50]</ref> of estimated mask M q (p) is defined as</p><formula xml:id="formula_6">M o q (p) =M o q (p)/(1 ?M o q (p)) O i=0M i q (p)/(1 ?M i q (p)) ,<label>(4)</label></formula><formula xml:id="formula_7">whereM 0 q (p) = O i=1</formula><p>(1 ?M i q (p)) denotes the background prediction.</p><p>Training Loss WithM q and M q denoting the groundtruth and predicted masks, the scale-balanced soft IoU loss <ref type="bibr" target="#b19">[20]</ref> is defined as</p><formula xml:id="formula_8">L(Mq, Mq) = 1 ? 1 O O o=1 p min(M o q (p), M o q (p)) p max(M o q (p), M o q (p))) ,<label>(5)</label></formula><p>where M o q (p) denotes the value of the o th instance in query mask M q at pixel p and so asM o q (p).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation Details</head><p>Training We follow the training setup as in <ref type="bibr" target="#b13">[14]</ref>. We train our model for 4 epochs in the main-training stage and 5 epochs for the finetuning stage. In the main-training stage, we adopt the SGD optimizer with an initial learning rate of 5e-3. The learning rate decays by a factor of 10 at the 3 and 4 epochs. In the finetuning stage, the learning rate is fixed at 5e-5. The batch size is set to the maximum possible magnitude for different backbones. Our model is initialized with the pre-trained weight of Mask R-CNN <ref type="bibr" target="#b13">[14]</ref> on COCO, while the additional propagation head is initialized randomly. Inference During the testing stage, RPN generates 200 proposals for each key frame. For a key frame, the detected instances are sorted by score and the top 10 (i.e., O) ones with scores higher than 0.2 are used for generating sequence proposals. The memory pool is updated every 5 frames in the Seq-Prop head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Discussion</head><p>Comparison with MaskProp Since MaskProp <ref type="bibr" target="#b1">[2]</ref> does not release codes or pre-computed results, a qualitative comparison is infeasible. Nevertheless, Tab. 1 can give some hints about the difference between MaskProp and our method. In Tab. 1, when the AP is close (47.6 vs. 46.6), our method has a better AR@10 than MaskProp (56.0 vs. 52.6). It indicates our method has more true positives in the top-10 scoring instances. However, with higher recall, they have similar AP. This suggests that MaskProp has the better scoring (according to the rules of mAP), which may be because of the stronger backbone employed (i.e., STSN-ResNeXt-101). When using the same backbone (ResNeXt-101), our method is better than MaskProp by 3.3% in terms of AP.</p><p>Distant Frame Pairs During the inference stage, the Seq-Prop head propagates segmented masks (in key frames) to other frames. It is worth investigating the effectiveness of propagating to distant frames. To this end, we group se-  quence proposals into three types with different initial quality (i.e., IoU) at the starting key frame <ref type="figure" target="#fig_0">(Fig. 10</ref>). The IoU of propagated masks drops by around 20% with high-quality initial masks. As the initial mask quality lowers, the IoU at distant frames drops less and even rises. This may be due to the reason that the propagation head learned shape information for objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Visual Results</head><p>We provide more visual results in <ref type="figure">Fig. 9</ref>. The last row is a failure case, where the deer is misclassified as a 'fox'.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Four paradigms of generating instance sequences in VIS. (a) Track-by-Detect links detected instances via frame-by-frame tracking. (b) Clip-Match matches overlapped sub-sequences between video-clips. (c) An alternative propagates detected instances from one key frame to the rest of a video. (d) Our proposed paradigm, named Propose-Reduce, generates instance sequence proposals based on multiple key frames and reduces redundant sequences of the same instances.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Effect of propagation from different key frames. (a) Four ordered frames where severe occlusion occurs in the first frame. (b) Propagating from the first frame leads to inaccurate segmentation results caused by error accumulation. (c) Propagating from the third frame produces satisfactory segmentation masks, due to reasonable segmentation results in the key frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Framework of Seq Mask R-CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>{S o k }, where o ? [0, O-1]. Correspondingly, their masks and scores are defined as the set of {M (S o k )} and {C(S o k )}, where M (S o k ) ? {0, 1} T ?H?W and the score of each sequence C(S o k ) is defined later. Accordingly, we obtain the input sequence set as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Visual comparison of different paradigms on challenging scenarios. Frames are sampled before and after occlusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Key frame analysis on YouTube-VIS and DAVIS-UVOS validation set. '#key frames' denotes the number of selected key frames in a video, where K = ? means all frames are key frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Algorithm 2 :</head><label>2</label><figDesc>Sequence Proposals Reduction Input: Input sequence set S = {S o k }; Its classification score C(S) = {C(S o k )}; Its mask sequence set M (S) = {M (S o k )}; where k = 0, 1, ..., K ? 1 ando = 0, 1, ..., O ? 1. IoU threshold ?. Output: Final sequence set S S ? {}; while S ? = ? do (k ? , o ? ) ? argmax C(S); V ? S o ? k ? ; S ? S ? V ; S ? S ? V ; for S o k in S do if IoU(M (V ), M (S o k )) ? ? then S ? S ? S o k ; C(S) ? C(S) ? C(S o k ); M (S) ? M (S) ? M (S o k ); end end end return S;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Architectures of the (a) Seq-Prop head, including the (b) NLBlock<ref type="bibr" target="#b45">[46]</ref> and the (c) ResBlock<ref type="bibr" target="#b14">[15]</ref>. O, T , H and W indicate instance number, frame number, height and width respectively. '(? O)' denotes expanding the tensor along the specific dimension. The 'Soft-Agg' operation refers to Eq. (4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 .Figure 10 .</head><label>910</label><figDesc>Visual results in various scenarios on DAVIS-UVOS and YouTube-VIS validation set. Category 'Instance' in DAVIS-UVOS denotes the salient generic object. The last row is a failure case. Zoom in for details. IoU (i.e., J -Mean) drops regarding propagated distance on DAVIS with ResNeXt-101. '[a, b)' indicates the group of sequence proposals where the IoU between the initial mask and corresponding ground-truth is in the range of [a%, b%).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>Paradigm</cell><cell>Method</cell><cell></cell><cell></cell><cell>Backbone</cell><cell cols="5">HR-Ref AP AP@50 AP@75 AR@1 AR@10</cell></row><row><cell></cell><cell cols="2">MaskTrack [53]</cell><cell></cell><cell>ResNet-50</cell><cell></cell><cell>30.3</cell><cell>51.1</cell><cell>32.6</cell><cell>31.0</cell><cell>35.5</cell></row><row><cell>Track-by-Detect</cell><cell cols="2">SipMask [7]</cell><cell></cell><cell>ResNet-50</cell><cell></cell><cell>33.7</cell><cell>54.1</cell><cell>35.8</cell><cell>35.4</cell><cell>40.1</cell></row><row><cell></cell><cell cols="2">EnsembleVIS [28]</cell><cell cols="2">ResNeXt-101*</cell><cell>?</cell><cell>44.8</cell><cell>-</cell><cell>48.9</cell><cell>42.7</cell><cell>51.7</cell></row><row><cell></cell><cell cols="2">STEm-Seg [1]</cell><cell></cell><cell>ResNet-50</cell><cell></cell><cell>30.6</cell><cell>50.7</cell><cell>33.5</cell><cell>31.6</cell><cell>37.1</cell></row><row><cell></cell><cell cols="2">STEm-Seg [1]</cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell>34.6</cell><cell>55.8</cell><cell>37.9</cell><cell>34.4</cell><cell>41.6</cell></row><row><cell>Clip-Match</cell><cell cols="2">MaskProp [2]</cell><cell cols="2">ResNeXt-101</cell><cell>?</cell><cell>44.3</cell><cell>-</cell><cell>48.3</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">MaskProp [2]</cell><cell cols="2">STSN [3]-ResNeXt-101</cell><cell></cell><cell>44.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="2">MaskProp [2]</cell><cell cols="2">STSN [3]-ResNeXt-101</cell><cell>?</cell><cell>46.6</cell><cell>-</cell><cell>51.2</cell><cell>44.0</cell><cell>52.6</cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell></cell><cell>ResNet-50</cell><cell></cell><cell>40.4</cell><cell>63.0</cell><cell>43.8</cell><cell>41.1</cell><cell>49.7</cell></row><row><cell>Propose-Reduce</cell><cell>Ours</cell><cell></cell><cell></cell><cell>ResNet-101</cell><cell></cell><cell>43.8</cell><cell>65.5</cell><cell>47.4</cell><cell>43.0</cell><cell>53.2</cell></row><row><cell></cell><cell>Ours</cell><cell></cell><cell cols="2">ResNeXt-101</cell><cell></cell><cell>47.6</cell><cell>71.6</cell><cell>51.8</cell><cell>46.3</cell><cell>56.0</cell></row><row><cell cols="10">Table 1. Quantitative results of video instance segmentation in YouTube-VIS validation set. 'HR-Ref' indicates post-processing that resizes</cell></row><row><cell cols="10">cropped masks to a large resolution and refines details with extra convolutional layers. *: EnsembleVIS adopts multiple models and their</cell></row><row><cell cols="2">largest backbone is ResNeXt-101.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">J &amp;F J -Mean F-Mean</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RVOS [42]</cell><cell>ResNet-101</cell><cell>41.2</cell><cell>36.8</cell><cell>45.7</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>STEm-Seg [1]</cell><cell>ResNet-101</cell><cell>64.7</cell><cell>61.5</cell><cell>67.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">UnOVOST [29] ResNet-101*</cell><cell>67.9</cell><cell>66.4</cell><cell>69.3</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>ResNet-101</cell><cell>68.3</cell><cell>65.0</cell><cell>71.6</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>ResNeXt-101</cell><cell>70.4</cell><cell>67.0</cell><cell>73.8</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>. Quantitative results of unsupervised video object segmen- tation on DAVIS-UVOS validation set.*: UnOVOST combines multiple models and their largest backbone is ResNet-101.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Training data analysis on YouTube-VIS and DAVIS-UVOS validation set. 'Both' denotes two-stage training, including main-training and finetuning. We report AP for YouTube-VIS and J &amp;F for DAVIS-UVOS.</figDesc><table><row><cell>two-stage training, since there exist a domain gap between</cell></row><row><cell>image (i.e., COCO) and video datasets (i.e., YouTube-VIS,</cell></row><row><cell>DAVIS-UVOS).</cell></row><row><cell>Sequence Reduction Tab. 4 reports the ablation results</cell></row><row><cell>with and without sequence reduction. Its effects on</cell></row><row><cell>YouTube-VIS and DAVIS-UVOS are different for their eva-</cell></row><row><cell>lution metric. The evaluation metric in YouTube-VIS is sen-</cell></row><row><cell>sitive to false positive. Sequence reduction significantly in-</cell></row><row><cell>creases AP at the cost of a slight decrease of AR. For</cell></row><row><cell>DAVIS that does not penalize false positives, sequence re-</cell></row><row><cell>duction stably increases all the metrics.</cell></row><row><cell>Category-Aware Reduction In the evaluation of</cell></row><row><cell>category-aware metrics (e.g., AP), new redundancy ap-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablations of Category-Aware Reduction (CA. Reduce) on different backbones. We reports AP for YouTube-VIS.</figDesc><table><row><cell>ResNet-101</cell><cell>?</cell><cell>19.3 43.8</cell><cell>55.1 53.2</cell><cell>62.4 69.5 65.0 71.6</cell></row><row><cell>ResNeXt-101</cell><cell>?</cell><cell>20.7 47.6</cell><cell>58.1 56.0</cell><cell>64.9 70.9 67.0 73.8</cell></row><row><cell cols="5">Table 4. Sequence reduction analysis on YouTube-VIS and</cell></row><row><cell cols="2">DAVIS-UVOS validation set.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">CA. Reduce ResNet-50 ResNet-101 ResNeXt-101</cell></row><row><cell></cell><cell>40.4</cell><cell></cell><cell>43.8</cell><cell>47.6</cell></row><row><cell>?</cell><cell>41.5</cell><cell></cell><cell>45.1</cell><cell>48.3</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Stem-seg: Spatio-temporal embeddings for instance segmentation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Athar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sabarinath</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aljo?a</forename><surname>O?ep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Classifying, segmenting, and tracking object instances in video with mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Object detection in video with spatiotemporal sampling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Yolact: Real-time instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bolya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fanyi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><forename type="middle">Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Oneshot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Leal-Taix?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Sergi Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alberto</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevis-Kokitsi</forename><surname>Montes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00737</idno>
		<title level="m">The 2019 davis challenge on vos: Unsupervised multi-object segmentation</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Sipmask: Spatial information preservation for fast image and video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiale</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisham</forename><surname>Rao Muhammad Anwer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Cholakkal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanwei</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hybrid task cascade for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Encoder-decoder with atrous separable convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Liang-Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tensormask: A foundation for dense object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning what to learn for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bhat</forename><surname>Goutam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><forename type="middle">J?remo</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Felsberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Piotr Doll?r, and Ross Girshick. Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Mask scoring r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaojin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongchao</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Lucid data dreaming for multiple object tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eddy</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09554</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Centermask: Real-time anchor-free instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngwan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongyoul</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Rogerio Feris, and Linglin He. Video instance segmentation tracking with a modified vae architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Ching</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Hung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Agss-vos: Attention guided single-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaijia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Sgn: Sequential grouping networks for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Path aggregation network for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Yang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Video object segmentation with episodic graph memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinkai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Danelljan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from unlabeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Video instance segmentation 2019: A winning approach for combined detection, segmentation, classification and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCVW</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unovost: Unsupervised offline video object segmentation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Luiten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Idil Esen Zulfikar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Video object segmentation using space-time memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergi</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00675</idno>
		<title level="m">The 2017 davis challenge on video object segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Pointins: Point-based instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingcong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06148</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Non-maximum suppression for object detection by passing messages between windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Rasmus Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Guillaumin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Kernelized memory network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongje</forename><surname>Seong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junhyuk</forename><surname>Hyun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Euntai</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pyramid dilated deeper convlstm for video salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kin-Man</forename><surname>Lam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Pwc-net: Cnns for optical flow using pyramid, warping, and cost volume</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Rvos: Endto-end recurrent network for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carles</forename><surname>Ventura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miriam</forename><surname>Bellver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreu</forename><surname>Girbau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amaia</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier Giro-I</forename><surname>Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Feelvos: Fast end-to-end embedding learning for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Voigtlaender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Zero-shot video object segmentation via attentive graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiankai</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICCV</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning unsupervised video object segmentation through visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenguan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmei</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanyuan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Solo: Segmenting objects by locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Memory selection network for video propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaijia</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fast video object segmentation by referenceguided mask propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joon-Young</forename><surname>Seoung Wug Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalyan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seon Joo</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Polarmask: Single shot instance segmentation with polar representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peize</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoge</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuebo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Video instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Efficient video object segmentation via network modulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuehan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianchao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aggelos K</forename><surname>Katsaggelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A transductive approach for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhuo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Matnet: Motion-attentive transition network for zero-shot video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbing</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="issue">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
