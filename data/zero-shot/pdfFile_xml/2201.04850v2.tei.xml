<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bridging Video-text Retrieval with Multiple Choice Questions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Ge</surname></persName>
							<email>yuyingge@hku.hkyixiaoge</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ARC Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xihui</forename><surname>Liu</surname></persName>
							<email>xihui.liu@berkeley.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Content Understanding Center</orgName>
								<address>
									<addrLine>3 Tencent PCG 5 UC</addrLine>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">ARC Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohu</forename><surname>Qie</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Tencent PCG</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
							<email>pluo@cs.hku.hk</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Bridging Video-text Retrieval with Multiple Choice Questions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>? VideoFormer "A girl in shorts and a hat is [?] on the [?]" TextFormer "dancing"? "laying" ? "talking" ? "green grass" ? "stage" ? "beach" ? BridgeFormer , 2 1 Noun question Verb question Verb answer Noun answer MCQ Remove for retrieval Figure 1. Overview of our novel pretext task, Multiple Choice Questions (MCQ), for video-text pre-training. MCQ is performed using a newly-proposed parametric module BridgeFormer, which associates all-level local features (intermediate tokens) from VideoFormer and TextFormer to answer multiple choice questions in the form of contrastive learning. Given that nouns and verbs carry informative local objects and object motions, we construct a noun question (in yellow) and a verb question (in red) by erasing the corresponding phrase from the sentence. The BridgeFormer is trained to select the correct erased phrase via visual reasoning with intermediate tokens from VideoFormer, given the questions' intermediate tokens from TextFormer. The noun and verb questions promote VideoFormer to capture detailed spatial content and temporal information. The semantic associations between video-text intermediate tokens are also enhanced via the proxy task of questions and answers. Note that BridgeFormer is removed for downstream retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Pre-training a model to learn transferable video-text representation for retrieval has attracted a lot of attention in recent years. Previous dominant works mainly adopt two separate encoders for efficient retrieval, but ignore local associations between videos and texts. Another line of research uses a joint encoder to interact video with texts, but results in low efficiency since each text-video pair needs to be fed into the model. In this work, we enable finegrained video-text interactions while maintaining high efficiency for retrieval via a novel pretext task, dubbed as Multiple Choice Questions (MCQ), where a parametric module BridgeFormer is trained to answer the "questions" constructed by the text features via resorting to the video features. Specifically, we exploit the rich semantics of text (i.e., nouns and verbs) to build questions, with which the video encoder can be trained to capture more regional content and temporal dynamics. In the form of questions and answers, the semantic associations between local video-text features can be properly established. BridgeFormer is able to be removed for downstream retrieval, rendering an efficient and flexible model with only two encoders. Our method outperforms state-of-the-art methods on the popular text-to-video retrieval task in five datasets with different experimental setups (i.e., zero-shot and fine-tune), including HowTo100M (one million videos). We further conduct zeroshot action recognition, which can be cast as video-to-text retrieval, and our approach also significantly surpasses its counterparts. As an additional benefit, our method achieves competitive results with much shorter pre-training videos on single-modality downstream tasks, e.g., action recognition with linear evaluation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Pre-training a model to learn transferable representations for video-text retrieval requires the understanding of video concepts, text semantics, and the relationships be-tween videos and texts. Existing works for video-text pretraining can be divided into two main categories. "Dualencoder" methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b51">50]</ref> (see <ref type="figure">Fig. 2</ref> (a)) adopt two separate encoders to contrast video-level and sentence-level representations respectively, ignoring the detailed local information within each modality and the associations between modalities. "Joint-encoder" methods <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b52">51</ref>] (see <ref type="figure">Fig. 2 (b)</ref>) concatenate texts and videos as inputs to a joint encoder for the interactions between local features of videos and texts, sacrificing the retrieval efficiency (every text-video pair needs to be fed into the encoder during inference) for the benefits of finegrained feature learning.</p><p>To enable fine-grained video-text interactions and at the same time maintaining high retrieval efficiency, we introduce a novel parametric pretext task for video-text pre-training, namely, Multiple Choice Questions (MCQ), which properly bridges texts with videos in all their feature levels. A new module in vitro, termed BridgeFormer, makes it possible, as illustrated in <ref type="figure">Fig. 1</ref>. Based on the backbone of a "dual-encoder" framework, BridgeFormer is trained to answer the "questions" generated by the text features via visual reasoning with the video features. MCQ enhances local feature learning within each modality as well as the fine-grained semantic associations cross modalities, and the BridgeFormer can be readily removed when transferring to downstream tasks without the loss of representation discriminativeness.</p><p>Specifically, we construct the "questions" by erasing a content phrase from the raw text, and the correct "answer" should be the erased phrase itself. Motivated by the observation that noun and verb phrases in a text carry rich semantic information <ref type="bibr" target="#b51">[50]</ref>, which can reflect the local objects and object motions in the video respectively, we randomly choose nouns or verbs as our content phrases. Bridge-Former is then trained to select the correct answer from multiple choices (all the erased content phrases in a batch) in the form of contrastive learning by resorting to the local features from the video encoder. Such a proxy training objective enforces the video encoder to capture accurate spatial content (to answer nouns) and temporal dynamics (to answer verbs), promoting the discriminativeness of the local features and the semantic associations between the local video patches and the text phrases.</p><p>BridgeFormer connects local features of videos and texts in all feature levels (low-, mid-, and high-level), i.e., taking each stage's features from the video and text encoders as input. The regularization will be directly imposed on the video and text features, which is different from the video-text feature aggregation by the conventional "jointencoder". Therefore, the proxy BridgeFormer only serves for the pre-training step and can be seamlessly removed for downstream retrieval, rendering a flexible and efficient  <ref type="figure">Figure 2</ref>. Comparison between existing paradigms and ours for video-text pre-training. Previous dominant methods either (a) adopt two separate encoders to contrast video-level and sentencelevel representations, ignoring local associations between videos and texts, or (b) use a joint encoder to interact fine-grained features of videos and texts through concatenating them as inputs, resulting in low efficiency for retrieval. (c) We propose a novel pretext task that uses a BridgeFormer to promote local feature learning and fine-grained video-text associations. For downstream retrieval task, the proxy BridgeFormer is removed.</p><p>model like the conventional "dual-encoder" methods, i.e., the similarity between video and text representations can be directly measured via dot product.</p><p>Our contributions are three-fold. (1) We introduce a novel pretext task, Multiple Choice Questions (MCQ), for video-text pre-training to receive the benefits of both "dualencoder" and "joint-encoder" methods, i.e., enhancing finegrained semantic associations between video and text features at the same time preserving high retrieval efficiency.</p><p>(2) We propose a parametric module, dubbed as Bridge-Former, to realize the pretext task of MCQ, with which the video encoder is trained to be more aware of regional objects and temporal dynamics, and the associations between local video-text features are established. Since the Bridge-Former will be removed on downstream tasks, we do not increase any additional parameters or computational overhead for retrieval compared to vanilla backbones. (3) Extensive results on text-to-video retrieval with different setups ( i.e., zero-shot and fine-tune) on five datasets, including the large-scale HowTo100M <ref type="bibr" target="#b29">[29]</ref> (1 million videos), demonstrate the large superiority of our method (see <ref type="figure">Fig. 3</ref> (a)). Furthermore, we evaluate zero-shot action recognition, which can be cast as a video-to-text retrieval task. Our method significantly surpasses its competitive counterparts by a large margin, as demonstrated in <ref type="figure">Fig. 3 (a)</ref>. As a bonus, we find our method also benefits single-modality (a) (b) <ref type="figure">Figure 3</ref>.</p><p>(a) Comparison between recent video-text pretraining methods for zero-shot text-to-video retrieval on MSR-VTT (R@1), HowTo100M (R@50) and zero-shot action recognition (video-to-text retrieval) on HMDB51 (top-1) and UCF101 (top-1). (b) Video length for pre-training and the top-1 accuracy of action recognition with linear evaluation, where "-X" denotes the modality used for pre-training besides videos, i.e., optical flow (OF), motion vector (MV), audio (A), and text (T). video representations as shown in <ref type="figure">Fig. 3 (b)</ref>, where the top-1 accuracy of action recognition with linear evaluation is reported. Despite those considerably longer videos being used in state-of-the-art pre-training methods (e.g., 11? longer in MMV <ref type="bibr" target="#b1">[2]</ref> than ours), our method still compares favorably with them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Pre-training for video-text retrieval. Dominant pretraining methods for video-text retrieval can be classified into two categories. Methods in the first category <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b28">28,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b47">46,</ref><ref type="bibr" target="#b51">50]</ref> adopt two individual encoders to embed video features and text features, and project them into the same latent space. Contrastive objectives <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b30">30]</ref> are used here to distinguish paired video-text data with unpaired data. This kind of methods is more favored by largescale retrieval applications due to its high efficiency. However, simply imposing the regularizations on the final features ([CLS] tokens) from two modalities leads to the insufficient interaction between local video-text representations. Methods in the second category <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b43">42,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b52">51]</ref> ensemble texts and videos as inputs to a joint encoder for the cross-modality fusion, followed by a binary classifier which is trained to predict whether videos and texts are aligned or not. Despite they can build local associations between video-text tokens, each pair of video and text candidates needs to be fed into the model for similarity calculation during inference, resulting in extremely low efficiency. In contrast, our method gains the benefits of the above two kinds of methods, i.e., achieving fine-grained video-text interactions while remaining high retrieval efficiency.</p><p>The pretext task of masked word prediction. Previous cross-modality pre-training work <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b52">51]</ref> use the pretext of masked word prediction (MWP), which randomly masks a proportion of words in the sentence and regularize the network to predict the masked words from a fixed vocabulary under the condition of visual inputs. Our introduced MCQ pretext task differs from MWP in two ways: (1) Predicting words in MWP imposes the regularizations on low-level word tokens, which may harm the interacted representation learning since the network also needs to serve as a text decoder. In contrast, contrasting answers with content phrases in our MCQ focuses on high-level semantics, showing significantly better results than MWP (will be discussed in experiments). (2) MCQ erases noun and verb phrases to construct informative questions, which reflects salient semantic information in visual features, while MWP randomly masks words (e.g., function words without content).</p><p>Video question answering (VQA). Works on video question answering (VQA) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b38">37,</ref><ref type="bibr" target="#b50">49</ref>] aims to answer questions about videos through training a model with question and answer pairs, which cannot be directly applied for pre-training as they are deliberately optimized for increasing VQA accuracy. By contrast, our work aims to learn downstream-agnostic generic features for video-text retrieval, where a new pretext task, multiple choice questions, is proposed to enhance the semantic associations between video and text. Our paper is the first to use the form of VQA as a pre-training pretext task, with two key innovations: the MCQ loss and the BridgeFormer module. BridgeFormer smoothly bridges the final objective of learning well-aligned video and text features with the regularization of a VQA pretext task.</p><p>Video-text retrieval with nouns and verbs. Works <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b45">44,</ref><ref type="bibr" target="#b49">48,</ref><ref type="bibr" target="#b53">52]</ref> solved video-text retrieval by focusing on verbs and nouns of texts, which are specially designed for retrieval with verbs and nouns as the refined text representations to directly align with videos. By contrast, we exploit the rich semantics of nouns and verbs in the text to build questions for improving text and video encoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We adopt the "dual-encoder" structure for video-text pretraining to realize highly efficient retrieval, and propose a new pretext task, Multiple Choice Questions (MCQ), with a parametric module BridgeFormer, to enhance fine-grained semantic associations between videos and texts. In this section, we first revisit the dual-encoder in Sec. 3.1. We then introduce the pretext task MCQ in Sec. 3.2 and the pretraining objectives in Sec. 3.3. At last, we describe the architecture of three components including a VideoFormer, a TextFormer, and a BridgeFormer in Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Dual-encoder for Video-text Pre-training: a revisit</head><p>As shown in <ref type="figure" target="#fig_1">Fig. 4</ref>, we adopt a dual-encoder structure, which consists a VideoFormer for learning video representations from raw video frame pixels, and a TextFormer for encoding text representations from natural languages. Given a video and its corresponding text description (e.g., "A girl in shorts and a hat is dancing on the green grass"), we first embed their respective representations from Video-Former and TextFormer, which are projected to a common embedding space as f v and f t via two separate linear layers. The similarity between the video and the text is calculated via the dot product between f v and f t . A contrastive objective <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b30">30]</ref> is utilized to maximize the similarity between f v and f t of positive pairs while minimizing the similarity between f v and f t of negative pairs (A video and its corresponding text description is regarded as a positive pair, and otherwise as a negative pair). The independent dual encoder pathways require only the dot product between video and text representations for similarity calculation in retrieval, which ensures the high efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multiple Choice Questions</head><p>As shown in <ref type="figure">Fig. 1</ref>, the pretext task MCQ is performed using a parametric module BridgeFormer, which associates all-level intermediate tokens from VideoFormer and TextFormer to answer multiple choice questions. Given observed that noun and verb phrases in a text carry rich semantic information, which can reflect the local objects and object motions in the video respectively, we randomly erase a noun or verb phrase to construct noun or verb questions. BridgeFormer is then trained to select the correct answer from multiple choices (all the erased phrases in a batch) by resorting to the local tokens of VideoFormer in the form of contrastive learning. The pretext task MCQ involves the objectives of answering noun questions and verb questions.</p><p>Answer Noun Question. Given a video and its corresponding text description (e.g., "A girl in shorts and a hat is dancing on the green grass"), we randomly erase a noun phrase (e.g., "green grass") as a noun question (e.g., "A girl in shorts and a hat is dancing on the [?]"). As shown in <ref type="figure" target="#fig_1">Fig. 4</ref>  tokens {z} v as the key and value to obtain the noun answer representations through cross-modality attention. The erased noun phrase is fed into TextFormer for noun representations. Similarly, the noun answer representations and the noun representations are projected into a common embedding space as f noun a and f noun via two separate linear layers, and their similarity is calculated via dot product. We adopt a contrastive objective to maximize the similarity between f noun a and f noun , when f noun is the representations of the correct noun phrase, and minimize the similarity between f noun a and f noun , when f noun is the representations of other (wrong) noun phrases. Training BridgeFormer to select the correct noun phrase by resorting to video tokens enforces VideoFormer to capture accurate spatial content.</p><p>Answer Verb Question. Similarly, we randomly erase a verb phrase (e.g., "dancing") of the text description as a verb question (e.g., "A girl in shorts and a hat is [?] on the green grass"). As shown in <ref type="figure" target="#fig_1">Fig. 4</ref>, BridgeFormer takes verb question text tokens {z} verb q from TextFormer as the query, and video tokens {z} v as the key and value to obtain the verb answer representations. The erased verb phrase is fed into TextFormer for verb representations. The verb answer representations and the verb representations are projected into a common embedding space as f verb a and f verb . A contrastive objective is adopted to maximize the similarity between f verb a and f verb , when f verb is the representations of the correct verb phrase, and minimize the similarity between f verb a and f verb , when f verb is the representations of other verb phrases. Training BridgeFormer to choose the correct verb phrase through seeking help from video tokens forces VideoFormer to capture detailed temporal dynamics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pre-training Objectives</head><p>We adopt the Noise-Contrastive Estimation (NCE) <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b30">30]</ref> as the contrastive objective and combine three objectives to optimize the entire model in an end-to-end manner as follows,</p><formula xml:id="formula_0">L = Lvanilla + Lnoun + Lverb<label>(1)</label></formula><p>where L vanilla is the NCE loss between video representations f v and text representations f t , L noun is the NCE loss between noun answer representations f noun a and noun representations f noun , L verb is the NCE loss between verb answer representations f verb a and verb representations f verb . We formulate NCE loss as below,</p><formula xml:id="formula_1">NCE(xi, yi) = ?log exp(x T i yi/? ) B j=1 exp(x T i yj/? )<label>(2)</label></formula><p>where B is the number of the batch size and the temperature hyper-parameter ? is empirically set to 0.05 per <ref type="bibr" target="#b5">[6]</ref>. VideoBlock. The input video token sequence {z} 0 v is fed into VideoFormer, which consists of a stack of VideoBlocks as shown in <ref type="figure" target="#fig_4">Fig. 5</ref>, adopting the structure of ViT <ref type="bibr" target="#b11">[12]</ref>. We make a minor modification to the original ViT to allow for the input of video frames with variable length. Specifically, given z l?1 v ? R (1+M ?N )?D from previous VideoBlock, we perform multi-head attention (MSA) <ref type="bibr" target="#b11">[12]</ref> for the [CLS] token through attending to all (1 + M ? N ) patches across time and space for temporal and spatial self-attention.   <ref type="bibr" target="#b11">[12]</ref>, normalization (norm) and multi-layer perception <ref type="bibr" target="#b10">[11]</ref> (MLP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">TextFormer</head><p>Input. TextFormer takes three kinds of nature languages as inputs, including a complete text description, noun or verb questions with a noun or verb phrase erased, and the erased noun or verb phrase. A [CLS] token is concatenated to the beginning of the input for final text representations.</p><p>TextBlock. We adopt a multi-layer bidirectional transformer encoder <ref type="bibr" target="#b39">[38]</ref> as TextFormer, which consists of a stack of TextBlocks as shown in <ref type="figure" target="#fig_4">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">BridgeFormer</head><p>Input. BridgeFormer takes noun question or verb question tokens from TextFormer as the query, and video tokens from VideoFormer as the key and value to obtain the answer representations with cross-modality attention.</p><p>BridgeBlock. BridgeFormer is built upon a vision transformer with a stack of BridgeBlocks as shown in <ref type="figure" target="#fig_4">Fig. 5</ref>. Specifically, given noun question or verb question text tokens {z} l?1 q ? R L?D from TextBlock as the query, and video tokens {z} l?1 v ? R M ?(N ?D) (without the [CLS] token) from VideoBlock as the key and value, BridgeBlockl obtains the interacted tokens {z} l qv through performing multi-head attention, which calculates the cross-modality attention between the question text tokens and video patch tokens within each frame. The interacted tokens {z} l qv added with the output {z} l?1 a from the previous Bridge-Block further go through the attention block for temporal and spatial self-attention as shown in <ref type="figure" target="#fig_4">Fig. 5</ref> to obtain the answer tokens {z} l a . The answer representations are extracted from the [CLS] token of the final block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Pre-training Datasets</head><p>Following the recent work <ref type="bibr" target="#b5">[6]</ref>, we jointly pre-train our model on an image dataset Google Conceptual Captions (CC3M) <ref type="bibr" target="#b40">[39]</ref> with 3.3M image-text pairs, and a video dataset WebVid-2M [6] with 2.5M video-text pairs. We do not pre-train our model on the large-scale video-text dataset HowTo100M <ref type="bibr" target="#b29">[29]</ref> with 136M video-text pairs considering the enormous computation cost. Instead, we use HowTo100M as a large-scale zero-shot text-to-video retrieval benchmark for evaluation, which is in line with realworld applications.  <ref type="bibr" target="#b5">[6]</ref>. (e). HowTo100M [29] contains 1.22M videos with 136M descriptions. All sentence descriptions for a video are concatenated as a single query. To our knowledge, it is the first time that downstream textto-video retrieval is evaluated on the large-scale dataset, i.e., HowTo100M. Two setting are explored for evaluation, including zero-shot and fine-tune.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Downstream Tasks</head><p>Action Recognition. (a). HMDB51 <ref type="bibr" target="#b21">[21]</ref>, which contains 6,766 videos with 51 categories. (b). UCF101 <ref type="bibr" target="#b41">[40]</ref>, which contains 13,320 videos with 101 action classes. Three setting are explored for evaluation, including linear, where parameters of the learned video encoder are frozen and only a linear classifier is optimized, fine-tune, where the video encoder is fine-tuned with the linear classifier, and zero-shot, which performs video-to-text retrieval through using the names of the action classes as the text description.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>Videos are resized to 224 ? 224 as input. We divide a video into M equal segments, and randomly sample a single frame from each segment for training while uniformly sample a frame from each segment for testing. VideoFormer contains 12 blocks with patch size P = 16, and sequence dimension D = 768. It is initialized with ViT <ref type="bibr" target="#b11">[12]</ref> weights trained on ImageNet-21k following <ref type="bibr" target="#b5">[6]</ref>. TextFormer adopts the architecture of DistilBERT <ref type="bibr" target="#b39">[38]</ref> pre-trained on English Wikipedia and Toronto Book Corpus. The dimension of the common feature space is set to 256. The temperature hyper-parameter of the contrastive objective is set to 0.05. The above implementation details follow the recent work <ref type="bibr" target="#b5">[6]</ref> for fair comparison. BridgeFormer contains 12 blocks. We first pre-train our model on the image dataset CC3M and video dataset WebVid-2M using 1 frame for 10 epochs with the batch size of 2048 and the learning rate of 1 ? 10 ?4 . We then pre-train our model on the video dataset WebVid-2M using 4 frames for 4 epochs with the batch size of 800 and the learning rate of 3 ? 10 ?5 . Pre-training takes a total of 25 hours. For downstream tasks, 4 frames for text-to-video retrieval and 16 frames for action recognition are uniformly sampled following the setting of previous work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Main Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Text-to-Video Retrieval</head><p>Table. 1 lists the results on MST-VTT <ref type="bibr" target="#b48">[47]</ref>. First of all, our method outperforms all previous work by a large margin. The significantly higher performance of our model under the zero-shot evaluation demonstrates the stronger generalization ability of our pre-trained model. Fine-tuning our pre-trained model on the training set of MSR-VTT also surpasses its counterparts overwhelmingly, showing its advantage in using task-specific data for optimization. Second, while previous work mostly pre-train on HowTo100M <ref type="bibr" target="#b29">[29]</ref> with the magnitude exceedingly large than our pre-training dataset CC3M <ref type="bibr" target="#b40">[39]</ref> and WebVid-2M <ref type="bibr" target="#b5">[6]</ref> (20x larger in the number of video-text pairs), our method still achieves the highest performance with much lower computation cost (i.e. VATT [1] takes 3 days using 256 TPUs while ours takes 25 hours using 40 A100.) Third, previous work rely on preextracted features from "expert" models as the input of the video encoder (i.e. SupportSet <ref type="bibr" target="#b31">[31]</ref> uses features from a 34layer, R(2+1)-D model <ref type="bibr" target="#b44">[43]</ref> pre-trained on IG65M <ref type="bibr" target="#b14">[14]</ref> as the input), while our model takes raw video frame pixels as inputs and achieves significant performance gain. Finally, compared with previous work <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b46">45,</ref><ref type="bibr" target="#b52">51]</ref> that adopt a joint encoder to concatenate videos and texts as inputs and thus every text-video combination needs to be imputed to the model for retrieval, our model only contains a video and a text encoder for downstream retrieval, which requires only the dot product between the video and text representations, thus greatly improves efficiency. We further show text-to-video retrieval results on MSVD <ref type="bibr" target="#b8">[9]</ref>, DiDeMo <ref type="bibr" target="#b4">[5]</ref> and LSMDC in <ref type="table">Table.</ref> 2. We can observe that our model achieves the best performance on these three datasets with both zero-shot and fine-tuning evaluation.</p><p>Besides evaluating text-to-video retrieval on a relatively small number of videos following previous work (e.g. 1K videos in MSR-VTT test set), we evaluate our model on the large-scale HowTo100M with 1 million videos, which is a more challenging and realistic scenario. <ref type="table">Table.</ref> 3 shows that our pre-trained model surpasses SOTA Frozen <ref type="bibr" target="#b5">[6]</ref>, ranging from 10K videos to 1M videos. Since our method and Frozen both adopt two encoders (built on ViT <ref type="bibr" target="#b11">[12]</ref> and Dis-tilBERT <ref type="bibr" target="#b39">[38]</ref>) for retrieval and are pre-trained on the same datasets, the superior performance of ours proves the effectiveness of our pretext task MCQ in learning powerful representations for text-to-video retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Action Recognition</head><p>We conduct zero-shot action recognition on HMDB51 <ref type="bibr" target="#b21">[21]</ref> and UCF101 <ref type="bibr" target="#b41">[40]</ref>, which can be treated as video-to-text retrieval and it is not evaluated in recent methods. As shown in <ref type="table">Table.</ref> 4, our model significantly surpasses its competitive counterparts. The top-1 accuracy of our model averaged on three splits improves 16.3% and 9.9% on HMDB51, 25.3%  and 7.2% on UCF101 than the recently proposed ClipBert and Frozen, which shows the great advantage of our model in learning joint representations between videos and languages that enable zero-shot action recognition. We further evaluate the single-modality video representations of our model via action recognition with linear and fully fine-tuning evaluation as shown in <ref type="table">Table.</ref> 5, where the representations from VideoFormer are extracted as the input of a trainable linear classifier. Our method achieves higher accuracy than some previous work that pre-train their model on datasets with considerably longer video time (e.g. 14? longer in XDC <ref type="bibr" target="#b2">[3]</ref>, 10? longer in MIL-NCE <ref type="bibr" target="#b28">[28]</ref> and VATT <ref type="bibr" target="#b0">[1]</ref>), showing the effectiveness of our method in learning transferable video representations for action recognition. Despite MMV <ref type="bibr" target="#b1">[2]</ref> performs better than our method when pre-training on datasets 11? longer than ours with multiple modalities including audio and text besides video, its performance lags far behind ours when only audio and video or text and video are used. We can conclude that our method utilizes the language modality more efficiently to learn stronger video representations with fewer video hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">CLIP-based Pre-training</head><p>Because of the prominent success of the CLIP <ref type="bibr" target="#b34">[34]</ref> (Contrastive Language-Image Pre-training) in learning imagetext representations, which is pre-trained on 400 million image-text pairs, some recent work <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b33">33]</ref> utilize the pretrained CLIP for text-to-video retrieval. We also initialize <ref type="table">Table 5</ref>. Experiments of action recognition on HMDB51 and UCF101 with linear evaluation (Lin) and fully fine-tuning evaluation (Full). The evaluation metric is top-1 accuracy. "Mod" denotes the modality used for pre-training besides videos, i.e., optical flow (OF), motion vector (MV), audio (A), text (T). "Len" denotes the video length for pre-training in kilo hours. our model from CLIP weights to pre-train a model following the setting of CLIP4Clip <ref type="bibr" target="#b26">[26]</ref>. Specifically, we use the pre-trained CLIP (ViT-B/32) as the backbone of Vide-oFormer and TextFormer, and randomly initialize Bridge-Former. The comparisons between our method and other CLIP-initialized methods are shown in <ref type="table">Table.</ref> 6. We can observe that our CLIP-based pre-trained model achieves higher performance for text-to-video retrieval on three datasets with under both the zero-shot and fine-tune evaluation. Our pretext task MCQ also benefits CLIP-based videotext pre-training for downstream text-to-video retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Studies</head><p>In this section, we discuss the effectiveness of our design on the pretext task MCQ through evaluating different models for zero-shot text-to-video retrieval on MSR-VTT, and zero-shot action recognition on HMDB51 and UCF101.</p><p>Is MCQ effective? Yes. As shown in <ref type="table">Table.</ref> 7, pre-training a model without MCQ pretext task drops performance significantly, where only two separate encoders are adopted to contrast video-level and sentence-level features.</p><p>Does it help to answer noun and verb questions? Yes. As shown in <ref type="table">Table.</ref> 7, training the BridgeFormer through answering noun questions only or verb questions only both harm performance. Randomly erasing words to construct questions also achieves worse results.</p><p>Do videos help to answer questions? Yes. As shown in Table. 8, when the noun-question and verb-question select answers only through calculating the similarity between question representations and phrase representations from text  Training the BridgeFormer to predict the answer in the form of word tokens (similar to existing masked work prediction (MWP)) rather than select the correct answer in a batch of phrases in our MCQ actually hurts performance as shown in <ref type="table">Table.</ref> 7, which is even lower than the baseline (w/o MCQ).</p><p>All-level features vs. highest-level features for Bridge-Former. When BridgeFormer takes the highest-level features from the text and video encoders as inputs (a cascading structure) instead of all-level features (a parallel structure), we observe the performance drops as shown in Table. 7 due to the lack of regularization on intermediate features. Even so, using only the highest-level features can also slightly outperform our baseline (w/o MCQ), indicating the effectiveness of our MCQ pretext task. Actually, such a cascading structure is similar to those used in previous works <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b43">42]</ref> where two separate encoders followed by a cross transformer are adopted. However, the cross transformer in these works cannot be easily removed in the same way as our BridgeFormer for downstream retrieval, e.g., evident 6.7% decreases were observed in <ref type="bibr" target="#b25">[25]</ref> in terms of R@1 on text-to-video retrieval, further indicating the flexibility and feasibility of our novel MCQ.   <ref type="figure">Figure 6</ref>. The visualization of the cross-modality attention between the text tokens of noun questions (as query) and video tokens (as key and value) from BridgeFormer. In the second column, the noun phrase marked in blue (Q1) is erased as the question, and in the third column, the noun phrase marked in green (Q2) is erased as the question. BridgeFormer attends to video patches with specific object information to answer noun questions.</p><p>Comparison of Video Encoder with Frozen. Frozen <ref type="bibr" target="#b5">[6]</ref> also adopts ViT <ref type="bibr" target="#b11">[12]</ref> as the video encoder, and adds temporal attention blocks based on the spatial attention blocks of ViT to encode videos with variable-length sequences. As shown in <ref type="table">Table.</ref> 10, compared with Frozen, our Vide-oFormer decreases 28 million parameters. Furthermore, the model without the pretext task MCQ indeed takes the same pre-training approach as Frozen except for the video encoder, and achieves better results for zero-shot text-to-video retrieval on MSR-VTT <ref type="bibr" target="#b48">[47]</ref>, which proves the efficiency and effectiveness of our VideoFormer. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Visualization</head><p>In our method, the pretext task MCQ is performed using a parametric module BridgeFormer, to answer multiple choice questions. We construct questions through erasing the content phrases (i.e. noun and verb phrases) of the text, and BridgeFormer is trained to select the correct answer from multiple choices by resorting to the local tokens of VideoFormer. Specifically, given question text tokens from TextFormer as the query, and video tokens from VideoFormer as the key and value, BridgeFormer performs cross-modality attention between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.1">Answering Noun Questions</head><p>We first visualize the cross-modality attention between noun question tokens and video tokens in <ref type="figure">Fig. 6</ref>. In the second column, the noun phrase marked in blue (Q1) is erased as the question, and in the third column, the noun phrase marked in green (Q2) is erased as the question. In <ref type="figure">Fig. 6</ref> (a), when "an old couple" is erased as the question (Q1), BridgeFormer focuses on video tokens that depict the appearance characteristics of the persons, and when "a plate of bread" is erased (Q2), it focuses on object video tokens on the table. In <ref type="figure">Fig. 6 (d)</ref>, when "football" is erased (Q1), BridgeFormer focuses on the object video tokens that can be associated with "play", and when the location phrase "countryside lawn" is erased (Q2), it pays more attention to the video tokens in the background to infer the answer. Bridge-Former attends to video patches with specific object information to answer questions, which also shows that Video-Former extracts accurate spatial content from videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.2">Answering Verb Questions</head><p>We further visualize the cross-modality attention between verb question tokens and video tokens in <ref type="figure">Fig. 7</ref>. Three frames are sampled from a video and the verb phrase marked in blue is erased as the question. In <ref type="figure">Fig. 7 (a)</ref>, when the verb "cutting" is erased, BridgeFormer focuses on the motion of the spoon on the pizza, and in <ref type="figure">Fig. 7 (b)</ref>, when the verb "drinking" is erased, it follows the movement of the hand holding a cup of water. BridgeFormer focuses on object motions of video tokens to answer verb questions, which also shows that VideoFormer captures temporal dynamics of videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we introduce a novel pretext task, Multiple Choice Questions (MCQ) for video-text pre-training, which strengthens fine-grained semantic associations between local video and text features, and at the same time preserves high efficiency for retrieval. A parametric module Bridge-Former is trained to answer questions constructed by text features via resorting to video features, and can be readily removed for downstream tasks. Extensive evaluations on the text-to-video retrieval and zero-shot action recognition clearly show the great superiority of our method.</p><p>Limitation. (1) Off-the-shelf NLP models can not extract completely accurate noun and verb phrases for us to construct questions. (2) The text descriptions and corresponding videos may be actually misaligned in existing video-text datasets, leading to noisy supervision.</p><p>Negative Social Impacts. Since we do not filter out possible inappropriate videos (e.g., of blood and violence) in the pre-training dataset, our model can be used to search terrible videos for spreading. Utilizing the pre-trained model to filter out those videos and re-training a model can help.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Our pre-training pipeline, which (1) contrasts video representations fv with text representations ft, (2) trains Bridge-Former to select the correct noun answer by contrasting noun answer representations fnoun a with noun representations fnoun, (3) trains BridgeFormer to choose the correct verb answer by contrasting verb answer representations f verb a with verb representations f verb . Note that BridgeFormer receives all-level tokens as the input, but we only draw one pathway here for brevity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Architecture 3.4.1 VideoFormer Input. VideoFormer takes a video V ? R M ?3?H?W as input containing variable M frames of resolution H ? W . The input video is first divided into M ? N patches of size P ? P , where N = HW/P 2 . The video patches v ? R M ?3?N ?P ?P are fed into a linear projection head with a convolutional layer and are flattened into a sequence of tokens z v ? R M ?N ?D , where D is the number of embedding dimensions. Following BERT [11], a learnable [CLS] token is concatenated to the beginning of the token sequence, which is used to produce the final video representations. Learnable spatial positional embeddings E pos ? R (N +1)?D are added to each video token as the final input token sequence z 0 v ? R (1+M ?N )?D and all patches in the same spatial location in different frames are given the same spatial positional embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>For the rest (M ? N ) patch tokens, MSA is performed within each of M frames with N + 1 tokens (N patch tokens and 1 [CLS] token) for spatial self-attention. The video representations are obtained from the [CLS] token of the final VideoBlock.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Linear Projection "[CLS] A girl in shorts and a hat is [?] on the green grass" The architecture of TextFormer, VideoFormer and BridgeFormer, which contain a stack of TextBlocks, VideoBlocks and BridgeBlocks respectively. Tokens from all-level VideoBlock and TextBlock are fed into the corresponding BridgeBlock to perform cross-modal attention and then are added to the output tokens of the previous BridgeBlock (if any). Each block performs a series of operations such as multi-head attention</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>" 3 "Figure 7 .</head><label>37</label><figDesc>A hand is cutting/[?] (Q) the pizza on the wooden table." A man standing on the lake shore is drinking/[?] (Q) hot tea." The visualization of the cross-modality attention between the text tokens of verb questions (as query) and video tokens (as key and value) from BridgeFormer. Three frames sampled from a video are shown and the verb phrase marked in blue (Q) is erased as the question. BridgeFormer focuses on object motions of video tokens to answer verb questions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>, the noun question is fed into TextFormer for intermediate text tokens {z} noun q . The intermediate video tokens are extracted from VideoFormer as {z} v . BridgeFormer takes the noun question tokens {z} noun q as the query, and video</figDesc><table><row><cell></cell><cell>e _</cell><cell></cell></row><row><cell></cell><cell>_</cell><cell></cell></row><row><cell></cell><cell>BridgeFormer</cell><cell></cell></row><row><cell></cell><cell>{ } ou _</cell><cell>{ }</cell><cell>,</cell></row><row><cell></cell><cell>TextFormer</cell><cell cols="2">VideoFormer</cell></row><row><cell>Text,</cell><cell>"[CLS] A girl in shorts</cell><cell></cell></row><row><cell>Verb?</cell><cell>and a hat is [?]/dancing</cell><cell></cell></row><row><cell>Noun</cell><cell>on the [?]/green grass"</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Text-to-Video Retrieval. (a). MSR-VTT<ref type="bibr" target="#b48">[47]</ref> contains 10K YouTube videos with 200K descriptions, which is split into 9K videos for training and 1K videos for test. (b). MSVD [9] consists of 1,970 videos from YouTube with 80K descriptions, which is split into 1200, 100 and 670 videos for training, validation and testing. (c). LSMDC [35] consists of 118,081 video clips from 202 movies. The validation set and the test set contain 7,408 and 1,000 videos. (d). DiDeMo [5] contains 10K Flickr videos with 40K sentences, where the test set contains 1,000 videos. We concatenate all sentence descriptions for a video as a single query following</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Experiments of text-to-video retrieval on MSR-VTT test set with 1K videos, where higher R@k and lower MedR (Median Rank) indicate better performance. Video Encoder Input: 3D features from the architectures (Raw Videos means training on raw video frame pixels without using pre-extracted features). # Pairs PT: the number of video-text pairs for pre-training. We show results with zero-shot evaluation (top) and fine-tuning evaluation (bottom).</figDesc><table><row><cell></cell><cell>Method</cell><cell cols="2">Year Video Encoder Input</cell><cell></cell><cell>PT Dataset</cell><cell cols="4">#Pairs PT R@1 R@5 R@10 MedR</cell></row><row><cell></cell><cell>ActBERT [51]</cell><cell>2020</cell><cell>ResNet-3D</cell><cell></cell><cell>HowTo100M</cell><cell>120M</cell><cell>8.6</cell><cell>23.4</cell><cell>33.1</cell><cell>36.0</cell></row><row><cell></cell><cell>MMV [2]</cell><cell>2020</cell><cell>Raw Videos</cell><cell cols="2">HowTo100M, AudioSet</cell><cell>138M</cell><cell>9.3</cell><cell>23.0</cell><cell>31.1</cell><cell>38.0</cell></row><row><cell cols="3">MIL-NCE [28] 2020</cell><cell>Raw Videos</cell><cell></cell><cell>HowTo100M</cell><cell>120M</cell><cell>9.9</cell><cell>24.0</cell><cell>32.4</cell><cell>29.6</cell></row><row><cell></cell><cell>VATT [1]</cell><cell>2021</cell><cell>Raw Videos</cell><cell cols="2">HowTo100M, AudioSet</cell><cell>138M</cell><cell>-</cell><cell>-</cell><cell>29.7</cell><cell>49.0</cell></row><row><cell></cell><cell>NoiseEst [4]</cell><cell>2021</cell><cell>ResNeXt-101</cell><cell></cell><cell>HowTo100M</cell><cell>110M</cell><cell>8.0</cell><cell>21.3</cell><cell>29.3</cell><cell>33.0</cell></row><row><cell></cell><cell>TACo [50]</cell><cell>2021</cell><cell>I3D, S3D</cell><cell></cell><cell>HowTo100M</cell><cell>120M</cell><cell>9.8</cell><cell>25.0</cell><cell>33.4</cell><cell>29.0</cell></row><row><cell cols="3">VideoCLIP [46] 2021</cell><cell>S3D</cell><cell></cell><cell>HowTo100M</cell><cell>110M</cell><cell>10.4</cell><cell>22.2</cell><cell>30.0</cell><cell>-</cell></row><row><cell></cell><cell>MCN [8]</cell><cell>2021</cell><cell>ResNeXt-101</cell><cell></cell><cell>HowTo100M</cell><cell>120M</cell><cell>10.5</cell><cell>25.2</cell><cell>33.8</cell><cell>-</cell></row><row><cell cols="3">SupportSet [31] 2021</cell><cell>R(2+1)D-34</cell><cell></cell><cell>HowTo100M</cell><cell>120M</cell><cell>12.7</cell><cell>27.5</cell><cell>36.2</cell><cell>24.0</cell></row><row><cell></cell><cell>Frozen [6]</cell><cell>2021</cell><cell>Raw Videos</cell><cell></cell><cell>CC3M, WebVid-2M</cell><cell>5.5M</cell><cell>18.7</cell><cell>39.5</cell><cell>51.6</cell><cell>10.0</cell></row><row><cell></cell><cell>AVLnet [36]</cell><cell>2021</cell><cell>ResNeXt-101</cell><cell></cell><cell>HowTo100M</cell><cell>120M</cell><cell>19.6</cell><cell>40.8</cell><cell>50.7</cell><cell>9.0</cell></row><row><cell></cell><cell>Ours</cell><cell>2021</cell><cell>Raw Videos</cell><cell></cell><cell>CC3M, WebVid-2M</cell><cell>5.5M</cell><cell>26.0</cell><cell>46.4</cell><cell>56.4</cell><cell>7.0</cell></row><row><cell></cell><cell>ActBERT [51]</cell><cell>2020</cell><cell>ResNet-3D</cell><cell></cell><cell>HowTo100M</cell><cell>120M</cell><cell>16.3</cell><cell>42.8</cell><cell>56.9</cell><cell>10.0</cell></row><row><cell></cell><cell>UniVL [25]</cell><cell>2020</cell><cell>S3D</cell><cell></cell><cell>HowTo100M</cell><cell>110M</cell><cell>21.2</cell><cell>49.6</cell><cell>63.1</cell><cell>6.0</cell></row><row><cell></cell><cell>MMT [13]</cell><cell>2020</cell><cell>S3D</cell><cell></cell><cell>HowTo100M</cell><cell>120M</cell><cell>26.6</cell><cell>57.1</cell><cell>69.6</cell><cell>4.0</cell></row><row><cell></cell><cell>HERO [23]</cell><cell>2021</cell><cell>SlowFast</cell><cell></cell><cell>TV and HowTo100M</cell><cell>120M</cell><cell>16.8</cell><cell>43.4</cell><cell>57.7</cell><cell>-</cell></row><row><cell></cell><cell>NoiseEst [4]</cell><cell>2021</cell><cell>ResNeXt-101</cell><cell></cell><cell>HowTo100M</cell><cell>110M</cell><cell>17.4</cell><cell>41.6</cell><cell>53.6</cell><cell>8.0</cell></row><row><cell></cell><cell>ClipBert [22]</cell><cell>2021</cell><cell>Raw Videos</cell><cell></cell><cell>COCO, VisGenome</cell><cell>5.6M</cell><cell>22.0</cell><cell>46.8</cell><cell>59.9</cell><cell>6.0</cell></row><row><cell></cell><cell>AVLnet [36]</cell><cell>2021</cell><cell>ResNeXt-101</cell><cell></cell><cell>HowTo100M</cell><cell>120M</cell><cell>27.1</cell><cell>55.6</cell><cell>66.6</cell><cell>4.0</cell></row><row><cell></cell><cell>VLM [45]</cell><cell>2021</cell><cell>S3D</cell><cell></cell><cell>HowTo100M</cell><cell>110M</cell><cell>28.1</cell><cell>55.5</cell><cell>67.4</cell><cell>4.0</cell></row><row><cell></cell><cell>TACo [50]</cell><cell>2021</cell><cell>I3D, S3D</cell><cell></cell><cell>HowTo100M</cell><cell>120M</cell><cell>28.4</cell><cell>57.8</cell><cell>71.2</cell><cell>4.0</cell></row><row><cell cols="3">SupportSet [31] 2021</cell><cell>R(2+1)D-34</cell><cell></cell><cell>HowTo100M</cell><cell>120M</cell><cell>30.1</cell><cell>58.5</cell><cell>69.3</cell><cell>3.0</cell></row><row><cell cols="3">VideoCLIP [46] 2021</cell><cell>S3D</cell><cell></cell><cell>HowTo100M</cell><cell>110M</cell><cell>30.9</cell><cell>55.4</cell><cell>66.8</cell><cell>-</cell></row><row><cell></cell><cell>Frozen [6]</cell><cell>2021</cell><cell>Raw Videos</cell><cell></cell><cell>CC3M, WebVid-2M</cell><cell>5.5M</cell><cell>31.0</cell><cell>59.5</cell><cell>70.5</cell><cell>3.0</cell></row><row><cell></cell><cell>Ours</cell><cell>2021</cell><cell>Raw Videos</cell><cell></cell><cell>CC3M, WebVid-2M</cell><cell>5.5M</cell><cell>37.6</cell><cell>64.8</cell><cell>75.1</cell><cell>3.0</cell></row><row><cell cols="3">(a) MSVD test set with 670 videos.</cell><cell cols="4">(b) LSMDC test set with 1K videos.</cell><cell></cell><cell cols="2">(c) DiDeMo test set with 1K videos.</cell></row><row><cell>Method</cell><cell cols="2">R@1 R@5 R@10 MedR</cell><cell cols="4">Method R@1 R@5 R@10 MedR</cell><cell></cell><cell>Method</cell><cell>R@1 R@5 R@10 MedR</cell></row><row><cell cols="3">NoiseEst [4] 13.7 35.7 47.7 12.0</cell><cell cols="4">AVLnet [36] 1.4 5.9 9.4 273.5</cell><cell cols="3">VideoCLIP [46] 16.6 46.9</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">SupportSet [31] 21.4 46.2 57.7 6.0</cell><cell cols="4">NoiseEst [4] 4.2 11.6 17.1 119.0</cell><cell></cell><cell>Frozen [6]</cell><cell>21.1 46.0 56.2 7.0</cell></row><row><cell>Frozen [6]</cell><cell cols="2">33.7 64.7 76.3 3.0</cell><cell cols="4">Frozen [6] 9.3 22.0 30.1 51.0</cell><cell></cell><cell>Ours</cell><cell>25.6 50.6 61.1 5.0</cell></row><row><cell>Ours</cell><cell cols="2">43.6 74.9 84.9 2.0</cell><cell cols="2">Ours</cell><cell cols="2">12.2 25.9 32.2 42.0</cell><cell></cell><cell>HERO [23]</cell><cell>2.1</cell><cell>-</cell><cell>11.4</cell><cell>-</cell></row><row><cell cols="3">NoiseEst [4] 20.3 49.0 63.3 6.0</cell><cell cols="4">NoiseEst [4] 6.4 19.8 28.4 39.0</cell><cell></cell><cell>CE [24]</cell><cell>16.1 41.1 82.7 8.3</cell></row><row><cell cols="3">SupportSet [31] 28.4 60.0 72.9 4.0</cell><cell cols="4">MMT [13] 12.9 29.9 40.1 19.3</cell><cell cols="3">ClipBert [22] 20.4 48.0 60.8 6.0</cell></row><row><cell>Frozen [6]</cell><cell cols="2">45.6 79.8 88.2 2.0</cell><cell cols="4">Frozen [6] 15.0 30.8 39.8 20.0</cell><cell></cell><cell>Frozen [6]</cell><cell>31.0 59.8 72.4 3.0</cell></row><row><cell>Ours</cell><cell cols="2">52.0 82.8 90.0 1.0</cell><cell cols="2">Ours</cell><cell cols="2">17.9 35.4 44.5 15.0</cell><cell></cell><cell>Ours</cell><cell>37.0 62.2 73.9 3.0</cell></row></table><note>Table 2. Experiments of text-to-video retrieval on different datasets, where higher R@k and lower MedR (Median Rank) indicate better performance. We show results with zero-shot evaluation (top) and fine-tuning evaluation (bottom).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Experiments of zero-shot text-to-video retrieval on the large-scale HowTo100M, where higher R@k and lower MedR indicate better performance. "Video Num" denotes the number of sampled videos for evaluation, where 1M denotes the whole set.</figDesc><table><row><cell>Video Num</cell><cell>Method</cell><cell cols="3">R@50 R@200 R@500</cell><cell>MedR</cell></row><row><cell></cell><cell>ClipBert [22]</cell><cell>15.8</cell><cell>33.6</cell><cell>49.8</cell><cell>506.0</cell></row><row><cell>10K</cell><cell>Frozen [6]</cell><cell>28.0</cell><cell>46.6</cell><cell>61.5</cell><cell>244.0</cell></row><row><cell></cell><cell>Ours</cell><cell>31.6</cell><cell>50.9</cell><cell>65.2</cell><cell>189.0</cell></row><row><cell>50K</cell><cell>Frozen [6] Ours</cell><cell>13.4 15.9</cell><cell>25.0 28.6</cell><cell>36.2 40.2</cell><cell>1247.0 965.0</cell></row><row><cell>0.1M</cell><cell>Frozen [6] Ours</cell><cell>9.4 11.5</cell><cell>18.5 21.7</cell><cell>27.5 31.2</cell><cell>2519.0 1907.0</cell></row><row><cell>0.5M</cell><cell>Frozen [6] Ours</cell><cell>4.0 5.0</cell><cell>8.5 10.3</cell><cell>13.4 15.9</cell><cell>12501.0 9449.0</cell></row><row><cell>1M</cell><cell>Frozen [6] Ours</cell><cell>2.6 3.4</cell><cell>5.9 7.3</cell><cell>9.5 11.6</cell><cell>24597.0 18612.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table><row><cell cols="9">Experiments of zero-shot action recognition (video-to-</cell></row><row><cell cols="9">text retrieval) on HMDB51 and UCF101, in terms of top-1 ac-</cell></row><row><cell cols="9">curacy. "S" denotes different test splits and "Mean" reports the</cell></row><row><cell cols="4">results averaged on three splits.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="2">HMDB51</cell><cell></cell><cell></cell><cell cols="2">UCF101</cell></row><row><cell></cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>Mean</cell><cell>S1</cell><cell>S2</cell><cell>S3</cell><cell>Mean</cell></row><row><cell cols="5">ClipBert [22] 20.0 22.0 22.3 21.4</cell><cell cols="4">27.5 27.0 28.8 27.8</cell></row><row><cell>Frozen [6]</cell><cell cols="4">27.5 28.3 27.7 27.8</cell><cell cols="4">45.4 44.7 47.7 45.9</cell></row><row><cell>Ours</cell><cell cols="4">38.0 36.1 39.1 37.7</cell><cell cols="4">51.1 54.3 53.8 53.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Text-to-video retrieval results of models initialized from CLIP<ref type="bibr" target="#b34">[34]</ref> weights on different datasets under zero-shot and fine-tune evaluation, where higher R@k and lower MdR (Median Rank) and MnR (Mean Rank) indicate better performance.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">MSR-VTT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MSVD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>LSMDC</cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell cols="15">R@1 R@5 R@10 MdR MnR R@1 R@5 R@10 MdR MnR R@1 R@5 R@10 MdR MnR</cell></row><row><cell cols="2">CLIP-straight [33]</cell><cell>31.2</cell><cell>53.7</cell><cell>64.2</cell><cell>4.0</cell><cell>-</cell><cell>37.0</cell><cell>64.1</cell><cell>73.8</cell><cell>3.0</cell><cell>-</cell><cell>11.3</cell><cell>22.7</cell><cell>29.2</cell><cell>56.5</cell><cell>-</cell></row><row><cell cols="2">CLIP4Clip [26]</cell><cell>32.0</cell><cell>57.0</cell><cell>66.9</cell><cell>4.0</cell><cell>34.0</cell><cell>38.5</cell><cell>66.9</cell><cell>76.8</cell><cell>2.0</cell><cell>17.8</cell><cell>15.1</cell><cell>28.5</cell><cell>36.4</cell><cell cols="2">28.0 117.0</cell></row><row><cell>Ours</cell><cell></cell><cell>33.2</cell><cell>58.0</cell><cell>68.6</cell><cell>4.0</cell><cell>25.7</cell><cell>48.4</cell><cell>76.4</cell><cell>85.8</cell><cell>2.0</cell><cell>7.4</cell><cell>15.5</cell><cell>30.7</cell><cell>38.7</cell><cell>22.0</cell><cell>97.9</cell></row><row><cell cols="2">CLIP4Clip [26]</cell><cell>43.1</cell><cell>70.4</cell><cell>80.8</cell><cell>2.0</cell><cell>16.2</cell><cell>46.2</cell><cell>76.1</cell><cell>84.6</cell><cell>2.0</cell><cell>10.0</cell><cell>20.7</cell><cell>38.9</cell><cell>47.2</cell><cell>13.0</cell><cell>65.3</cell></row><row><cell>Ours</cell><cell></cell><cell>44.9</cell><cell>71.9</cell><cell>80.3</cell><cell>2.0</cell><cell>15.3</cell><cell>54.4</cell><cell>82.8</cell><cell>89.4</cell><cell>1.0</cell><cell>6.1</cell><cell>21.8</cell><cell>41.1</cell><cell>50.6</cell><cell>10.0</cell><cell>60.5</cell></row><row><cell cols="8">Table 7. Ablation studies on different components of MCQ. Re-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">sults of zero-shot text-to-video retrieval on MSR-VTT and zero-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="8">shot action recognition on HMDB51 and UCF101 are reported.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">MSR-VTT</cell><cell cols="3">HMDB51 UCF101</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell cols="3">R@1 R@5 R@10</cell><cell cols="2">Top-1</cell><cell>Top-1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o MCQ</cell><cell>22.3</cell><cell>43.8</cell><cell>52.0</cell><cell cols="2">33.2</cell><cell>45.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Answer Random</cell><cell>23.0</cell><cell>45.5</cell><cell>55.5</cell><cell cols="2">36.9</cell><cell>50.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Answer Noun</cell><cell>24.9</cell><cell>46.2</cell><cell>58.0</cell><cell cols="2">36.2</cell><cell>51.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Answer Verb</cell><cell>23.3</cell><cell>46.7</cell><cell>57.5</cell><cell cols="2">36.3</cell><cell>51.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>MWP</cell><cell>20.6</cell><cell>39.7</cell><cell>50.1</cell><cell cols="2">29.0</cell><cell>38.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Highest-level</cell><cell>23.3</cell><cell>46.0</cell><cell>56.4</cell><cell cols="2">36.5</cell><cell>47.7</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell>26.0</cell><cell>46.4</cell><cell>56.4</cell><cell cols="2">37.7</cell><cell>53.1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 .</head><label>8</label><figDesc>Ablation study on the effects of video information when answering the questions. Results on WebVid-2M validation set for noun or verb questions are reported.</figDesc><table><row><cell></cell><cell cols="3">Answer Noun</cell><cell></cell><cell cols="2">Answer Verb</cell></row><row><cell></cell><cell cols="6">R@1 R@5 R@10 R@1 R@5 R@10</cell></row><row><cell>w/o Video</cell><cell>6.6</cell><cell>17.5</cell><cell>24.3</cell><cell>4.5</cell><cell>12.3</cell><cell>17.7</cell></row><row><cell cols="2">with Video 58.6</cell><cell>81.1</cell><cell>87.2</cell><cell>40.7</cell><cell>64.0</cell><cell>73.2</cell></row><row><cell cols="7">encoder without resorting to video tokens through Bridge-</cell></row><row><cell cols="4">Former, the results decrease sharply.</cell><cell></cell><cell></cell><cell></cell></row></table><note>Multiple Choice Questions vs. Masked Word Prediction.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 .</head><label>9</label><figDesc></figDesc><table><row><cell cols="6">The effects of the prompt "[MASK]" for noun and verb</cell></row><row><cell cols="6">representations, where "End", "Middle" and "Start" denote the</cell></row><row><cell cols="6">location of the prompt. For zero-shot text-to-video retrieval on</cell></row><row><cell cols="6">MSR-VTT, higher R@k indicates better performance. For zero-</cell></row><row><cell cols="6">shot action recognition on HMDB51 and UCF101, higher top-1</cell></row><row><cell>accuracy is better.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>MSR-VTT</cell><cell></cell><cell cols="2">HMDB51 UCF101</cell></row><row><cell>Method</cell><cell cols="3">R@1 R@5 R@10</cell><cell>Top-1</cell><cell>Top-1</cell></row><row><cell>w/o Prompt</cell><cell>23.1</cell><cell>43.5</cell><cell>54.3</cell><cell>34.8</cell><cell>45.8</cell></row><row><cell>End</cell><cell>24.2</cell><cell>45.7</cell><cell>54.4</cell><cell>33.4</cell><cell>48.5</cell></row><row><cell>Middle</cell><cell>24.3</cell><cell>43.2</cell><cell>53.9</cell><cell>33.1</cell><cell>46.4</cell></row><row><cell>Start</cell><cell>25.1</cell><cell>45.4</cell><cell>55.4</cell><cell>34.9</cell><cell>51.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 .</head><label>10</label><figDesc>Comparisons between the video encoder in our method and Frozen<ref type="bibr" target="#b5">[6]</ref>. The evaluation is performed on zero-shot text-tovideo retrieval on MSR-VTT, where higher R@k and lower MdR (Median Rank) indicate better performance. "# Params" denotes the number of parameters of the video encoder (M: million). Accurate representations for noun and verb phrases are essential. Since TextFormer is trained with full sentences, it fails to encode accurate representations for phrases when it takes a single noun or verb phrase as the input due to the lack of context. Motivated by the success of prompt engineering<ref type="bibr" target="#b34">[34]</ref>, we add "[MASK]" before the noun and verb phrase (e.g. "[MASK] [MASK] [MASK] green grass") to extract noun or verb representations from TextFormer. We show ablation studies of the prompt "[MASK]" for noun and verb representations in Table. 9, where each model is pre-trained using 1 frame. The model without the prompt "[MASK]" takes a single noun or verb phrase as inputs, and achieves the worse results on both the zero-shot text-to-video retrieval and action recognition, showing that TextFormer cannot understand the semantics accurately with a single noun or verb phrase as inputs. The model with the prompt "[MASK]" at the beginning of the phrase achieves the best results in general, and we adopt this practice in our method.</figDesc><table><row><cell>Method</cell><cell cols="5">R@1 R@5 R@10 MdR # Params</cell></row><row><cell cols="2">Frozen [6] 18.7</cell><cell>39.5</cell><cell>51.6</cell><cell>10.0</cell><cell>114M</cell></row><row><cell>Ours</cell><cell>22.3</cell><cell>43.8</cell><cell>52.0</cell><cell>9.0</cell><cell>86M</cell></row><row><cell cols="6">Prompt for Phrase Representation In our method,</cell></row><row><cell cols="6">BridgeFormer is trained to select the correct answer by con-</cell></row><row><cell cols="6">trasting noun answer representations with noun represen-</cell></row><row><cell cols="6">tations, and contrasting verb answer representations with</cell></row><row><cell cols="2">verb representations.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment Ping Luo is supported by the General Research Fund of HK No.27208720 and 17212120.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Akbari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linagzhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Hong</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11178</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Self-supervised multimodal versatile networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><surname>Jean-Baptiste Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rosalia</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">De</forename><surname>Ramapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Self-supervised learning by cross-modal audio-video clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Humam</forename><surname>Alwassel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2020. NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Noise estimation using density estimation for self-supervised multimodal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Amrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Ben-Ari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rotman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="6644" to="6652" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5803" to="5812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">iperceive: Applying common-sense reasoning to multi-modal dense video captioning and video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aman</forename><surname>Chadha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gurneet</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navpreet</forename><surname>Kaloty</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.07735</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Multimodal clustering networks for self-supervised learning from unlabeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Duarte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angie</forename><surname>Boggust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harwath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.12671</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William B Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Finegrained video-text retrieval with hierarchical graph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shizhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yida</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10638" to="10647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<imprint>
			<pubPlace>Thomas Unterthiner, Mostafa Dehghani, Matthias</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="214" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale weakly-supervised pre-training for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deepti</forename><surname>Ghadiyaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12046" to="12055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Coot: Cooperative hierarchical transformer for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Ging</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammadreza</forename><surname>Zolfaghari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Memory-augmented dense predictive coding for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="312" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Selfsupervised co-training for video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengda</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5679" to="5690" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Vilt: Vision-and-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjae</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bokyung</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03334</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Cycle-contrast for self-supervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenpeng</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomoaki</forename><surname>Yoshinaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomokazu</forename><surname>Murakami</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Est?baliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7331" to="7341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hero: Hierarchical encoder for video+ language omni-representation pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2046" to="2065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13487</idno>
		<title level="m">Use what you have: Video retrieval using representations from collaborative experts</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Univl: A unified video and language pretraining model for multimodal understanding and generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taroon</forename><surname>Bharti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.06353</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Clip4clip: An empirical study of clip for end to end video clip retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08860</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A dataset and exploration of models for understanding video data through fill-in-the-blank question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6884" to="6893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Endto-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9879" to="9889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Howto100m: Learning a text-video embedding by watching hundred million narrated video clips</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makarand</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2630" to="2640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation learning with contrastive predictive coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Support-set bottlenecks for video-text representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Evolving losses for unsupervised video representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Aj Piergiovanni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Angelova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A straightforward framework for video retrieval using clip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jes?s Andr?s</forename><surname>Portillo-Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">Carlos</forename><surname>Ortiz-Bayliss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Terashima-Mar?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mexican Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Niket Tandon, and Bernt Schiele. A dataset for movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3202" to="3212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angie</forename><surname>Boggust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhiraj</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<imprint>
			<publisher>Rameswar Panda</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09199</idno>
		<title level="m">Learning audio-visual language representations from instructional videos</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Video question answering with phrases via semantic roles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Sadhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.03762</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2556" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Learning video representations using contrastive bidirectional transformer</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A closer look at spatiotemporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6450" to="6459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fine-grained action retrieval through multiple parts-of-speech embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Wray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diane</forename><surname>Larlus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="450" to="459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Vlm: Taskagnostic video-language model pre-training for video understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prahal</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masoumeh</forename><surname>Aminzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.09996</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Videoclip: Contrastive pre-training for zero-shot video-text understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gargi</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Okhonko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armen</forename><surname>Aghajanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.14084</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Jointly modeling deep video and compositional text to bridge vision and language in a unified framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ran</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Just ask: Learning to answer questions from millions of narrated videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1686" to="1697" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Taco: Token-aware cascade contrastive learning for videotext alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11562" to="11572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Actbert: Learning globallocal video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8746" to="8755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Cross-task weakly supervised learning from instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><surname>Zhukov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramazan</forename><forename type="middle">Gokberk</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3537" to="3545" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
