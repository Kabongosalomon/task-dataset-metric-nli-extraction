<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Intra-domain Adaptation for Semantic Segmentation through Self-Supervision</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Pan</surname></persName>
							<email>feipan@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">In So Kweon KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inkyu</forename><surname>Shin</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">In So Kweon KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Rameau</surname></persName>
							<email>frameau@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">In So Kweon KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Lee</surname></persName>
							<email>seokju91@kaist.ac.kr</email>
							<affiliation key="aff0">
								<orgName type="laboratory">In So Kweon KAIST</orgName>
								<address>
									<country key="KR">South Korea</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Unsupervised Intra-domain Adaptation for Semantic Segmentation through Self-Supervision</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:45+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Convolutional neural network-based approaches have achieved remarkable progress in semantic segmentation. However, these approaches heavily rely on annotated data which are labor intensive. To cope with this limitation, automatically annotated data generated from graphic engines are used to train segmentation models. However, the models trained from synthetic data are difficult to transfer to real images. To tackle this issue, previous works have considered directly adapting models from the source data to the unlabeled target data (to reduce the inter-domain gap). Nonetheless, these techniques do not consider the large distribution gap among the target data itself (intra-domain gap). In this work, we propose a two-step self-supervised domain adaptation approach to minimize the inter-domain and intra-domain gap together. First, we conduct the interdomain adaptation of the model; from this adaptation, we separate the target domain into an easy and hard split using an entropy-based ranking function. Finally, to decrease the intra-domain gap, we propose to employ a self-supervised adaptation technique from the easy to the hard split. Experimental results on numerous benchmark datasets highlight the effectiveness of our method against existing state-of-theart approaches. The source code is available at https: //github.com/feipan664/IntraDA.git.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation aims at assigning each pixel in the image to a semantic class. Recently, convolutional neural network-based segmentation models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34]</ref> have achieved remarkable progresses, leading to various applications in computer vision systems, such as autonomous driving <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b12">13]</ref>, robotics <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24]</ref>, and disease diagnosis <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b32">33]</ref>. Training such a segmentation network requires large amounts of annotated data. However, collecting large scale datasets with pixel-level annotations for semantic segmentation is difficult since they are expensive and labor in-</p><formula xml:id="formula_0">Figure 1:</formula><p>We propose a two-step self-supervised domain adaptation technique for semantic segmentation. Previous works solely adapt the segmentation model from the source domain to the target domain. Our work also consider adapting from the clean map to the noisy map within the target domain.</p><p>tensive. Recently, photorealistic data rendered from simulators and game engines <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> with precise pixel-level semantic annotations have been utilized to train segmentation networks. However, the models trained from synthetic data are hardly transferable to real data due to the cross-domain difference <ref type="bibr" target="#b10">[11]</ref>. To address this issue, unsupervised domain adaptation (UDA) techniques have been proposed to align the distribution shift between the labeled source data and the unlabeled target data. For the particular task of semantic segmentation, adversarial learningbased UDA approaches demonstrate efficiency in aligning features at the image <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b10">11]</ref> or output <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b25">26]</ref> level. More recently, the entropy of pixel-wise output predictions proposed by <ref type="bibr" target="#b28">[29]</ref> is also used for output level alignment. Other approaches <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b37">38]</ref> involve generating pseudo labels for target data and conducting refinement via an iterative self-training process. While many models consider the single-source-single-target adaptation setting, recent works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b34">35]</ref> have proposed to address the issue of multiple source domains; it focus on the multiple-sourcesingle-target adaptation setting. Above all, previous works have mostly considered adapting models from the source data to the target data (inter-domain gap).</p><p>However, target data collected from the real world have diverse scene distributions; these distributions are caused by various factors such as moving objects, weather conditions, which leads to a large gap in the target (intra-domain gap). For example, the noisy map and clean map in the target domain, shown in <ref type="figure">Figure 1</ref>, are the predictions made by the same model on different images. While previous studies solely focus on reducing the inter-domain gap, the problem of the intra-domain gap has attracted a relatively low attention. In this paper, we present a two-step domain adaptation approach to minimize the inter-domain and intra-domain gap. Our model consists of three parts, which are presented in <ref type="figure" target="#fig_0">Figure 2</ref>, namely, 1) an inter-domain adaptation module to close inter-domain gap between the labeled source data and the unlabeled target data, 2) an entropy-based ranking system to separate target data into the an easy and hard split, and 3) an intra-domain adaptation module to close intra-domain gap between the easy and hard split (using pseudo labels from the easy subdomain). For semantic segmentation, our proposed approach achieves good performance against state-ofthe-art approaches on benchmark datasets. Furthermore, our approach outperforms previous domain adaptation approaches for digit classification.</p><p>The Contributions of Our Work. First, we introduce the inter-domain gap among target data and propose an entropybased ranking function to separate target domain into an easy and hard subdomain. Second, we propose a two-step self-supervised domain adaptation approach to minimize the inter-domain and intra-domain gap together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Unsupervised Domain Adaptation. The goal of unsupervised domain adaptation is to align the distribution shift between the labeled source and the unlabeled target data. Recently, adversarial-based UDA approaches have shown great capabilities in learning domain invariant features, even for complex tasks like semantic segmentation <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b17">18]</ref>. Adversarial-based UDA models for semantic segmentation usually involve two networks. One network is used as a generator to predict the segmentation maps of input images, which can be from the source or the target. Given features from the generator, the second network functions as a discriminator to predict the domain labels. The generator tries to fool the discriminator, so as to align the distribution shift of the features from the two domains. Besides feature level alignment, other approaches try to align domain shift at the image level or output level. At the image level, CycleGAN <ref type="bibr" target="#b36">[37]</ref> was applied in <ref type="bibr" target="#b10">[11]</ref> to build generative images for domain alignment. At the output level, <ref type="bibr" target="#b25">[26]</ref> proposes an end-to-end model involving structural output alignment for distribution shift. More recently, <ref type="bibr" target="#b28">[29]</ref> takes advantage of the entropy of pixel-wise predictions from the segmentation outputs to address the domain gap. While all the previous studies exclusively considered aligning the inter-domain gap, our approach further minimizes the intradomain gap. Thus, our technique can be combined with most existing UDA approaches for extra performance gains.</p><p>Uncertainty via Entropy. Uncertainty measurement has a strong connection with unsupervised domain adaptation. For instance, <ref type="bibr" target="#b28">[29]</ref> proposes minimizing the target entropy value of the model outputs directly or using adversarial learning <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b10">11]</ref> to close the domain gap for semantic segmentation. Also the entropy of the model outputs <ref type="bibr" target="#b29">[30]</ref> is used as a confidence measurement for transferring samples across domains <ref type="bibr" target="#b24">[25]</ref>. We propose utilizing entropy to rank target images to separate them into two an easy and hard split.</p><p>Curriculum Domain Adaptation. Our work is also related to curriculum domain adaptation <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b6">7]</ref> which deals with easy samples first. For curriculum domain adaptation on foggy scene understanding, <ref type="bibr" target="#b22">[23]</ref> proposes to adapt a semantic segmentation model from non-foggy images to synthetic light foggy images, and then to real heavy foggy images. To generalize this concept, <ref type="bibr" target="#b6">[7]</ref> decomposes the domain discrepancy into multiple smaller discrepancies by introducing unlabeled intermediate domains. However, these techniques require additional information to decompose domains. To cope with this limitation, <ref type="bibr" target="#b31">[32]</ref> focuses on learning the global and local label distributions of images as the first task to regularize the model predictions in the target domain. In contrast, we propose a simpler and data-driven approach to learn the easy target samples based on an entropy ranking system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Let S denote a source domain consisting of a set of images ? R H?W ?3 with their associated ground-truth Cclass segmentation maps ? (1, C) H?W ; similarly, let T denote a target domain containing a set of unlabeled images ? R H?W ?3 . In this section, a two-step self-supervised domain adaptation for semantic segmentation is introduced. is used separate all target data into easy split and hard split. An hyperparameter ? is introduced as a ratio of target images assigned into the easy split. In (c), an intra-domain adaptation is used to close the gap between easy split and hard split. The segmentation predictions of easy split data from G inter serve as pseudo labels. Given easy split data with pseudo labels and hard split data, D intra is used to predict whether the sample is from easy split or hard split, while G intra is trained to confuse D intra . {G intra and D intra } are optimized using the intra-domain segmentation loss L seg intra and the adversarial loss L adv intra .</p><p>The first step is the inter-domain adaptation, which is based on common UDA approaches <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b25">26]</ref>. Then, the pseudo labels and predicted entropy maps of target data are generated such that the target data can be clustered into an easy and hard split. Specifically, an entropy-based ranking system is used to cluster the target data into the easy and hard split. The second step is the intra-domain adaptation, which consists in aligning the easy split with pseudo labels to the hard split, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>. The proposed network consists of the inter-domain generator and discriminator {G inter , D inter }, and the intra-domain generator and discriminator {G intra , D intra }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Inter-domain Adaptation</head><formula xml:id="formula_1">A sample X s ? R H?W ?3 is from the source do- main with its associated map Y s . Each entry Y (h,w) s = Y (h,w,c) s c</formula><p>of Y s provides a label of a pixel (h, w) as a onehot vector. The network G inter takes X s as an input and generates a "soft-segmentation map" P s = G inter (X s ).</p><formula xml:id="formula_2">Each C-dimensional vector P (h,w,c) s c at a pixel (h, w)</formula><p>serves as a discrete distribution over C classes. Given X s with its ground-truth annotation Y s , G inter is optimized in a supervised way by minimizing the cross-entropy loss:</p><formula xml:id="formula_3">L seg inter (X s , Y s ) = ? h,w c Y (h,w,c) s log(P (h,w,c) s ). (1)</formula><p>To close the inter-domain gap between the source and target domains, <ref type="bibr" target="#b28">[29]</ref> proposes to utilize entropy maps in order to align the distribution shift of the features. The assumption of <ref type="bibr" target="#b28">[29]</ref> is that the trained models tend to produce overconfident (low-entropy) predictions for source-like images, and under-confident (high-entropy) predictions for targetlike images. Due to its simplicity and effectiveness, <ref type="bibr" target="#b28">[29]</ref> is adopted in our work to conduct inter-domain adaptation.</p><p>The generator G inter takes a target image X t as an input and produce the segmentation map P t = G inter (X t ); the entropy map I t is formulated as:</p><formula xml:id="formula_4">I (h,w) t = c ?P (h,w,c) t log(P (h,w,c) t ).<label>(2)</label></formula><p>To align the inter-domain gap, D inter is trained to predict the domain labels for the entropy maps, while G inter is trained to fool D inter ; the optimization of G inter and D inter is achieved via the following loss function:</p><formula xml:id="formula_5">L adv inter (X s , X t ) = h,w log(1 ? D inter (I (h,w) t )) + log(D inter (I (h,w) s )),<label>(3)</label></formula><p>where I s is the entropy map of X s . The loss function L adv inter and L seg inter are optimized to align the distribution shift between the source and target data. However, there remains a need for an efficient method that can minimize the intradomain gap. For this purpose, we propose to separate the target domain into an easy and hard split and to conduct an intra-domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Entropy-based Ranking</head><p>Target images collected from the real world have diverse distributions due to various weather conditions, moving objects, and shading. In <ref type="figure" target="#fig_0">Figure 2</ref>, some target prediction maps are clean 1 and others are very noisy, despite being generated from the same model. Since the intra-domain gap exists among target images, a straightforward solution is to decompose the target domain into small subdomains/splits. However, it remains a challenging task due to the lack of target labels. To build these splits, we take advantage of the entropy maps in order to determine the confidence levels of the target predictions. The generator G inter takes a target image X t as input to generate P t and the entropy map I t . On this basis, we adopt a simple yet effective way for ranking by using:</p><formula xml:id="formula_6">R(X t ) = 1 HW h,w I (h,w) t ,<label>(4)</label></formula><p>which is the mean value of entropy map I t . Given a ranking of scores from R(X t ), hyperparameter ? is introduced as a ratio to separate the target images into an easy and hard split. Let X te and X th denote a target image assigned to the easy and hard split, respectively. In order to conduct domain separation, we define ? = |Xte| |Xt| , where |X te | is the cardinality of the easy split, and |X t | is the cardinality of the whole target image set. To access the influence of ?, we conduct an ablation study on how to optimize ? in <ref type="table" target="#tab_1">Table 2</ref>. Note that we do not introduce a hyperparameter as the threshold value for separation. The reason is that the threshold value is dependent on a specific dataset. However, we choose a hyperparameter as ratio, which shows strong generalization to other datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Intra-domain Adaptation</head><p>Since no annotation is available for the easy split, directly aligning the gap between the easy and hard split is infeasible. But we propose to utilize the predictions from G inter as pseudo labels. Given an image from the easy split X te , we forward X te to G inter and obtain the prediction map P te = G inter (X te ). While P te is a "soft-segmentation map", we convert P te to P te where each entry is a one-hot vector. With the aid of pseudo labels, G intra is optimized by minimizing the cross-entropy loss:</p><formula xml:id="formula_7">L seg intra (X te ) = ? h,w c P (h,w,c) te log G intra (X te ) (h,w,c) . (5)</formula><p>To bridge the intra-domain gap between easy and hard split, we adopt the alignment on the entropy map for both splits.</p><p>An image X th from hard split is taken as input to the generator G to generate the segmentation map P th = G(X th ) and the entropy map I th . To close the intra-domain gap, the intra-domain discriminator D intra is trained to predict the split labels of I te and I th : I te is from the easy split, and I th is from the hard split. G is trained to fool D intra . The adversarial learning loss to optimize G intra and D intra is formulated as:</p><formula xml:id="formula_8">L adv intra (X te , X th ) = h,w log(1 ? D intra (I (h,w) th )) + log(D intra (I (h,w) te )).<label>(6)</label></formula><p>Finally, our complete loss function L is formed by all the loss functions:</p><formula xml:id="formula_9">L = L seg inter + L adv inter + L seg intra + L adv intra ,<label>(7)</label></formula><p>and our objective is to learn a target model G according to:</p><formula xml:id="formula_10">G * = arg min Gintra min G inter G intra max D inter D intra L.<label>(8)</label></formula><p>Since our proposed model is two-step self-supervised approach, it is difficult to minimize L in one training stage. Thus, we choose to minimize it in three stages. First, we train the inter-domain adaptation for the model to optimize G inter and D inter . Second, we generate target pseudo labels by utilizing G inter and rank all target images based on S(X t ). Finally, we train the intra-domain adaptation to optimize G intra and D intra .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we introduce the experimental details of the inter-domain and the intra-domain adaptation on semantic segmentation. <ref type="table">Table 1</ref>: The semantic segmentation results of Cityscapes validation set with models trained on GTA5 (a), SYNTHIA (b), and Synscapes (c). All the results are generated from the ResNet-101-based models. In the experiments of (a) and (b), AdvEnt <ref type="bibr" target="#b28">[29]</ref> is used as the framework for the inter-domain adaptation and intra-domain adaptation. In the experiment of (c), AdaptSegNet <ref type="bibr" target="#b25">[26]</ref> is used as the framework of the inter-domain adaptation and intra-domain adaptation. mIoU * in (b) denotes the mean IoU of 13 classes, excluding the classes with * . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>In the experiments of semantic segmentation, we adopt the setting of adaptation from the synthetic to the real domain. To conduct this series of tests, synthetic datasets including GTA5 <ref type="bibr" target="#b19">[20]</ref>, SYNTHIA <ref type="bibr" target="#b20">[21]</ref> and Synscapes <ref type="bibr" target="#b30">[31]</ref> are used as source domains, along with the real-world dataset Cityscapes <ref type="bibr" target="#b5">[6]</ref> as the target domain. Models are trained given labeled source data and unlabeled target data. Our model is evaluated on Cityscapes validation set.</p><p>? GTA5: The synthetic dataset GTA5 <ref type="bibr" target="#b19">[20]</ref> contains 24,966 synthetic images with a resolution of 1,914?1,052 and corresponding ground-truth annotations. These synthetic images are collected from a video game based on the urban scenery of Los Angeles city. The ground-truth annotations generated automatically contain 33 categories. For training, we consider only 19 categories which are compatible with the Cityscapes dataset <ref type="bibr" target="#b5">[6]</ref>, similarly to previous work.</p><p>? SYNTHIA: SYNTHIA-RAND-CITYSCAPES <ref type="bibr" target="#b20">[21]</ref> is used as another synthetic dataset. It contains 9,400 fully annotated RGB images. During the training time, we consider the 16 common categories with the Cityscapes dataset. During evaluation, 16-and 13class subsets are used to evaluate the performance.</p><p>? Synscapes: Synscapes <ref type="bibr" target="#b30">[31]</ref> is a photorealistic synthetic dataset consisting of 25,000 fully annotated RGB images with a resolution of 1,440?720. Alike Cityscape, the ground-truth annotations contain 19 categories.</p><p>? Cityscapes: As the dataset collected from real world, Cityscapes <ref type="bibr" target="#b5">[6]</ref> provides 3,975 images with fine segmentation annotations. 2,975 images are taken from the training set of Cityscapes to be used for training. The 500 images from the evaluation set of Cityscapes are used to evaluate the performance of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation.</head><p>The semantic segmentation performance is evaluated on every category using the PASCAL VOC intersection-over-union metric, i.e., IoU = TP/(TP + FP + FN) <ref type="bibr" target="#b8">[9]</ref>, where TP, FP, and FN are the number of true positive, false positive, and false negative pixels, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Implementation Details.</head><p>In the experiments for GTA5?Cityscapes and SYNTHIA?Cityscapes, we utilize the framework of AdvEnt <ref type="bibr" target="#b28">[29]</ref> to train G inter and D inter  for inter-domain adaptation; the backbone of G inter is a ResNet-101 architecture <ref type="bibr" target="#b9">[10]</ref> with pretrained parameters from ImageNet <ref type="bibr" target="#b7">[8]</ref>; the input data are labeled source images and unlabeled target images. The model for inter-domain adaptation G inter is trained for 70,000 iterations. After training, G inter is used to generate the segmentation and entropy maps for all 2,975 images from Cityscapes training set. Then, we utilize R(X t ) to get the ranking scores for all target images and to separate them into the easy and hard split based on ?. We conduct an ablation study of ? for optimization in <ref type="table" target="#tab_1">Table 2</ref>. For the intra-domain adaptation, G intra has same architecture as G inter , and D intra same as D inter ; the input data are 2,975 Cityscapes training images with pseudo labels of easy split. G intra is trained with the pretrained parameters from ImageNet and D intra from scratch, similar to AdvEnt. In addition to the previously mentioned experiments, we also conduct the experiment for Synscapes?Cityscapes. For comparison with AdaptSeg-Net <ref type="bibr" target="#b25">[26]</ref>, We apply the framework of AdaptSegNet in the experiment for the inter-domain and intra-domain adaptation.</p><p>Similarly to <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b25">[26]</ref>, we utilize the multi-level feature outputs from conv4 and conv5 for inter-domain adaptation and intra-domain adaptation. To train G inter and G intra , we apply an SGD optimizer <ref type="bibr" target="#b1">[2]</ref> with a learning rate of 2.5 ? 10 ?4 , momentum 0.9, and a weight decay 10 ?4 for training G inter and G intra . An Adam optimizer <ref type="bibr" target="#b11">[12]</ref> with a learning rate of 10 ?4 is used for training D inter and D intra .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GTA5.</head><p>In <ref type="table">Table 1</ref> (a), we compare the segmentation performance of our method with other state-of-the-art methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b28">29]</ref> on Cityscapes validation set. For a fair comparison, the baseline model is adopted from DeepLab-v2 <ref type="bibr" target="#b2">[3]</ref> with a ResNet-101 backbone. Overall, our proposed method achieves 46.3% in mean IoU. Compared to Ad-vEnt, the intra-domain adaptation from our method leads to a 2.5% improvement in mean IoU.</p><p>To highlight the relevance of the proposed intra-domain adaptation, we conduct a comparison with segmentation loss L seg intra and adversarial adaptation loss L adv intra in <ref type="table" target="#tab_2">Table 3</ref>. The baseline AdvEnt <ref type="bibr" target="#b28">[29]</ref> achieves 43.8% of mIoU. By using AdvEnt + intra-domain adaptation, which means L seg intra = 0, we obtain 45.1%, showing the effectiveness of adversarial learning for the intra-domain alignment. By applying AdvEnt + self-training, with ? = 1.0 (all pseudo labels used for self-training), which means L adv intra = 0, we achieve 45.5% of mIoU, underlying the importance of employing pseudo labels. Finally, our proposed model achieves 46.3% of mIOU (self-training + intra-domain alignment).</p><p>Admittedly, complex scenes (containing many objects) might be categorized as "hard". To provide a more representative "ranking", we adopt a new normalization by dividing the mean entropy with the number of predicted rare classes in the target image. For Cityscapes dataset, we define these rare classes as "wall, fence, pole, traffic light, traffic sign, terrain, rider, truck, bus, train, motor". The entropy normalization helps to move images with many objects to the easy split. By using the normalization, our proposed model achieves 47.0% of mIoU, as shown in <ref type="table" target="#tab_2">Table 3</ref>. Our proposed method also has limitation for some classes.</p><p>In <ref type="figure" target="#fig_1">Figure 3</ref>, we provide some visualizations of segmentation maps from our technique. The segmentation maps generated from our model trained with inter-domain alignment and intra-domain alignment are more accurate than the baseline model AdvEnt, which has been only trained with inter-domain alignment. A representative set of images belonging to the "hard" split are visible in <ref type="figure" target="#fig_2">Figure 4</ref>. After intra-domain alignment, we produce the segmentation maps shown in the (d) column. Compared with (c) column, our model can be transferred to more difficult target images.</p><p>Analysis of Hyperparameter ?. We conduct a study on finding a proper value for the hyperparameter ? in our experiment of GTA5?Cityscapes. In <ref type="table" target="#tab_1">Table 2</ref>, different values of ? are used for setting up the decision boundary for domain separation. When ? = 0.67, i.e., the ratio of |X te | to |X t | is approximately 2/3, the model achieves 46.3 of mIoU as the best performance on Cityscapes validation set.</p><p>SYNTHIA. We use SYNTHIA as the source domain and present evaluation results of the proposed method and state-of-the-art methods <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref> on Cityscapes validation set in the <ref type="table">Table 1</ref>. For a fair comparison, we also adopt the same DeepLab-v2 with the ResNet-101 architecture. Our method is evaluated on both 16-class and 13-class baselines. According to the results in <ref type="table">Table 1</ref> (b), our proposed method has achieved 41.7% and 48.9% of mean IoU on 16-class and 13-class baseline, respectively. As shown in the <ref type="table">Table 1</ref>, our model is significantly more accurate on the car and motor bike classes than existing techniques. The reason is that we apply the intra-domain adaptation to further narrow the domain gap.</p><p>Synscapes. The only work that we currently have found using Syncapes dataset is <ref type="bibr" target="#b25">[26]</ref>. Thus we use AdaptSeg-Net <ref type="bibr" target="#b25">[26]</ref> as our baseline model. In order to present a fair comparison, We only consider using vanilla-GAN in our experiments. With inter-domain and intra-domain adaptation, our model achieves 54.2% of mIoU, which higher than AdaptSegNet shown in <ref type="table">Table 1</ref> (c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussion</head><p>Theoretical Analysis. Comparing (a), (b) in <ref type="table">Table 1</ref>, GTA5 to Cityscapes is more effective than SYNTHIA to Cityscapes. We believe the reason is that GTA5 has more similar images of street scenes with Cityscapes than other synthetic datasets. We also provide a theoretical analysis here. Let H denote the hypothesis class, S and T be the source and the target domain. The theory from <ref type="bibr" target="#b0">[1]</ref> proposes to bound the expected error on the target domain T (h): ?h ? H,</p><formula xml:id="formula_11">T (h) ? S (h) + 1 2 d H (S, T ) + ?,</formula><p>where S (h) is the expected error on the source domain; d H (S, T ) = 2 sup |Pr S (h) ? Pr T (h)|, which is the distance for domain divergence; ? is considered as a constant in normal cases. Therefore, T (h) is upper bounded by S (h) and d H (S, T ) in our case. Our proposed model is to minimize d H (S, T ) by using the inter-domain and the intra-domain alignment together. If d H (S, T ) has high value, the higher upper bound in the first stage of the inter-domain adaptation affects our entropy ranking system, and the intra-domain adaptation processes. Therefore, our model is less efficient in big domain gap. With respect to the limitation, our model performance is affected by d H (S, T ) and S (h). Firstly, the larger divergence of the source and target domain leads to higher value in d H (S, T ). The upper bound of error is higher so our model would be less effective. Secondly, S (h) would be very high when the model uses small neural networks. In such case, our model would also be less effective.</p><p>Digit Classification. Our model are also capable to be applied in digit classification task. We consider the adaptation shift of MNIST?USPS, USPS?MNIST, and SVHN?MNIST. Our model is trained using the training sets: MNIST with 60,000 images, USPS with 7,291 images, standard SVHN with 73,257 images. The proposed model  is evaluated on the standard test sets: MNIST with 10,000 images and USPS with 2,007 images. In digit classification task, G inter and G intra serve as classifiers with same architecture, which is based on a variant of the LeNet architecture. In inter-domain adaptation, We utilize the framework of CyCADA <ref type="bibr" target="#b10">[11]</ref> to train G inter and D inter . In the ranking stage, we utilize G inter to generate the predictions of all target data and compute their ranking score using R(X t ).</p><p>With respect to ?, we adopted ? = 0.8 in all experiments. Our network for intra-domain adaptation is also based on CyCADA <ref type="bibr" target="#b10">[11]</ref>. In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we present a self-supervised domain adaptation to minimize the inter-domain and intra-domain gap simultaneously. We first train the model using the interdomain adaptation from existing approaches. Secondly, we produce target image entropy maps and use an entropybased ranking functions to split the target domain. Lastly, we conduct the intra-domain adaptation to further narrow the domain gap. We conduct extensive experiments on synthetic to real images in traffic scenarios. Our model can be combined with existing domain adaptation approaches. Experimental results shows that our model outperforms existing adaptation algorithms.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>The proposed self-supervised domain adaptation model contains the inter-domain generator and discriminator {G inter , D inter }, and the intra-domain generator and discriminator {G intra , D intra }. The proposed model consists of three parts, namely, (a) an inter-domain adaptation, (b) an entropy-based ranking system, and (c) an intra-domain adaptation. In (a), given the source and the unlabeled target data, D inter is trained to predict the domain label for the samples while G inter is trained to fool D inter . {G inter , D inter } are optimized by minimizing the segmentation loss L seg inter and the adversarial loss L adv inter . In (b), an entropy based function R(I t )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The example results of evaluation for GTA5?Cityscapes. (a) and (d) are the images from Cityscapes validation set and the corresponding ground-truth annotation. (b) are the predicted segmentation maps of the inter-domain adaptation [29]. (c) are the predicted maps from our technique.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The examples of entropy maps from hard split for GTA5?Cityscapes. (a) are the hard images from Cityscapes training set. (b) and (c) are the predicted entropy and segmentation maps from model trained solely by the inter-domain adaptation [29]. (d) are the improved predicted segmentation results of the hard images from our model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>40.6 76.1 23.3 16.8 36.9 36.8 40.1 83.0 34.8 84.9 59.9 37.7 78.4 20.4 20.5 7.8 27.3 52.5 45.3 AdaptSegNet [26] 94.2 60.9 85.1 29.1 25.2 38.6 43.9 40.8 85.2 29.7 88.2 64.4 40.6 85.8 31.5 43.0 28.3 30.5 56.7 52.7 Ours 94.0 60.0 84.9 29.5 26.2 38.5 41.6 43.7 85.3 31.7 88.2 66.3 44.7 85.7 30.7 53.0 29.5 36.5 60.2 54.2</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">(a) GTA5 ? Cityscapes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>veg</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>mbike</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell cols="20">Without adaptation [26] 75.8 16.8 77.2 12.5 21.0 25.5 30.1 20.1 81.3 24.6 70.3 53.8 26.4 49.9 17.2 25.9 6.5 25.3 36.0</cell><cell>36.6</cell></row><row><cell>ROAD [5]</cell><cell cols="19">76.3 36.1 69.6 28.6 22.4 28.6 29.3 14.8 82.3 35.3 72.9 54.4 17.8 78.9 27.7 30.3 4.0 24.9 12.6</cell><cell>39.4</cell></row><row><cell>AdaptSegNet [26]</cell><cell cols="19">86.5 36.0 79.9 23.4 23.3 23.9 35.2 14.8 83.4 33.3 75.6 58.5 27.6 73.7 32.5 35.4 3.9 30.1 28.1</cell><cell>42.4</cell></row><row><cell>MinEnt [29]</cell><cell cols="19">84.2 25.2 77.0 17.0 23.3 24.2 33.3 26.4 80.7 32.1 78.7 57.5 30.0 77.0 37.9 44.3 1.8 31.4 36.9</cell><cell>43.1</cell></row><row><cell>AdvEnt [29]</cell><cell cols="19">89.9 36.5 81.6 29.2 25.2 28.5 32.3 22.4 83.9 34.0 77.1 57.4 27.9 83.7 29.4 39.1 1.5 28.4 23.3</cell><cell>43.8</cell></row><row><cell>Ours</cell><cell cols="19">90.6 36.1 82.6 29.5 21.3 27.6 31.4 23.1 85.2 39.3 80.2 59.3 29.4 86.4 33.6 53.9 0.0 32.7 37.6</cell><cell>46.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(b) SYNTHIA ? Cityscapes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall  *</cell><cell>fence  *</cell><cell>pole  *</cell><cell>light</cell><cell>sign</cell><cell>veg</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>bus</cell><cell>mbike</cell><cell>bike</cell><cell cols="3">mIoU mIoU  *</cell></row><row><cell cols="18">Without adaptation [26] 55.6 23.8 74.6 9.2 0.2 24.4 6.1 12.1 74.8 79.0 55.3 19.1 39.6 23.3 13.7 25.0</cell><cell></cell><cell>33.5</cell><cell>38.6</cell></row><row><cell>AdaptSegNet [26]</cell><cell cols="17">81.7 39.1 78.4 11.1 0.3 25.8 6.8 9.0 79.1 80.8 54.8 21.0 66.8 34.7 13.8 29.9</cell><cell></cell><cell>39.6</cell><cell>45.8</cell></row><row><cell>MinEnt [29]</cell><cell cols="17">73.5 29.2 77.1 7.7 0.2 27.0 7.1 11.4 76.7 82.1 57.2 21.3 69.4 29.2 12.9 27.9</cell><cell></cell><cell>38.1</cell><cell>44.2</cell></row><row><cell>AdvEnt [29]</cell><cell cols="17">87.0 44.1 79.7 9.6 0.6 24.3 4.8 7.2 80.1 83.6 56.4 23.7 72.7 32.6 12.8 33.7</cell><cell></cell><cell>40.8</cell><cell>47.6</cell></row><row><cell>Ours</cell><cell cols="17">84.3 37.7 79.5 5.3 0.4 24.9 9.2 8.4 80.0 84.1 57.2 23.0 78.0 38.1 20.3 36.5</cell><cell></cell><cell>41.7</cell><cell>48.9</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="5">(c) Synscapes ? Cityscapes</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>road</cell><cell>sidewalk</cell><cell>building</cell><cell>wall</cell><cell>fence</cell><cell>pole</cell><cell>light</cell><cell>sign</cell><cell>veg</cell><cell>terrain</cell><cell>sky</cell><cell>person</cell><cell>rider</cell><cell>car</cell><cell>truck</cell><cell>bus</cell><cell>train</cell><cell>mbike</cell><cell>bike</cell><cell>mIoU</cell></row><row><cell cols="2">Without adaptation 81.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The ablation study on hyperparameter ? for separating the target domain into the easy and the hard split.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">GTA5 ? Cityscapes</cell><cell></cell></row><row><cell>?</cell><cell>0.0</cell><cell>0.5</cell><cell>0.6 0.67 0.7</cell><cell>1.0</cell></row><row><cell cols="5">mIoU 43.8 45.2 46.0 46.3 45.6 45.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The self-training and intra-domain adaptation gain on GTA5 ? Cityscapes.</figDesc><table><row><cell>Model</cell><cell>mIoU</cell></row><row><cell>AdvEnt [25]</cell><cell>43.8</cell></row><row><cell>AdvEnt + intra-domain adaptation</cell><cell>45.1</cell></row><row><cell>AdvEnt + self-training (? = 1.0)</cell><cell>45.5</cell></row><row><cell>Ours</cell><cell>46.3</cell></row><row><cell>Ours + entropy normalization</cell><cell>47.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>The experimental results of adaptation across digit datasets.</figDesc><table><row><cell>Model</cell><cell cols="3">MNIST ? USPS USPS ? MNIST SVHN ? MNIST</cell></row><row><cell>Source only</cell><cell>82.2? 0.8</cell><cell>69.6? 3.8</cell><cell>67.1? 0.6</cell></row><row><cell>ADDA [28]</cell><cell>89.4? 0.2</cell><cell>90.1? 0.8</cell><cell>76.0? 1.8</cell></row><row><cell>CyCADA [11]</cell><cell>95.6? 0.2</cell><cell>96.5? 0.1</cell><cell>90.4? 0.4</cell></row><row><cell>Ours</cell><cell>95.8?0.1</cell><cell>97.8?0.1</cell><cell>95.1?0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>, our proposed model achieve 95.8?0.1% of accuracy on MNIST ? USPS, 97.8?0.1% of accuracy on USPS ? MNIST, and 95.1?0.3% on SVHN ? MNIST. Our model outperforms the baseline model Cy-CADA<ref type="bibr" target="#b10">[11]</ref>.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The clean prediction map means that the prediction is confident and smooth.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This research was partially supported by the Shared Sensing for Cooperative Cars Project funded by Bosch (China) Investment Ltd. This work was also partially supported by the Korea Research Fellowship Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Science, ICT and Future Planning (2015H1D3A1066564).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large-scale machine learning with stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COMPSTAT&apos;2010</title>
		<meeting>COMPSTAT&apos;2010</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="177" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="834" to="848" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Domain adaptation for semantic segmentation with maximum squares loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2090" to="2099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Road: Reality oriented adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhua</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uwe</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Adaptation across extreme variations using unlabeled domain bridges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.02238</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">CyCADA: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stephen Lin, and In So Kweon. Visuomotor understanding for representation learning of driving scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seokju</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsik</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tae-Hyun</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongseop</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggeun</forename><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting deeper into the future of semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Neverova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="648" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Realtime semantic segmentation of crop and weed for precision agriculture robots leveraging background knowledge in cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andres</forename><surname>Milioto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Lottes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrill</forename><surname>Stachniss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2229" to="2235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image to image translation for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zak</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungnam</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4500" to="4509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Donghyeon Cho, and In So Kweon. Preserving semantic and temporal consistency for unpaired video-to-video translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanyong</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Woo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahun</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1248" to="1257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Moment matching for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinxun</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xide</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1406" to="1415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<editor>Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling</editor>
		<imprint>
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">German</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maximum classifier discrepancy for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Model adaptation with synthetic and real data for semantic dense foggy scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christos</forename><surname>Sakaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengxin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Hecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="687" to="704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automatic instrument segmentation in robot-assisted surgery using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alexey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Shvets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rakhlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alexandr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">I</forename><surname>Kalinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Iglovikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">17th IEEE International Conference on Machine Learning and Applications (ICMLA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="624" to="628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Active adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Chyi</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="739" to="748" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to adapt structured output space for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Domain adaptation for structured output via discriminative patch representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1456" to="1465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himalaya</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxime</forename><surname>Bucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2517" to="2526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cost-effective active learning for deep image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keze</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2591" to="2600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Synscapes: A photorealistic synthetic dataset for street scene parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Wrenninge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Unger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.08705</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Data augmentation using learned transformations for one-shot medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guha</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fredo</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><forename type="middle">V</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dalca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8543" to="8553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengshuang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Adversarial multiple source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanhang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Jos?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><forename type="middle">P</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">J</forename><surname>Costeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8559" to="8570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Collaborative learning of semisupervised segmentation and classification for medical images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2079" to="2088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for semantic segmentation via class-balanced self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Bvk Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="289" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Confidence regularized self-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V K</forename><surname>Vijaya Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
