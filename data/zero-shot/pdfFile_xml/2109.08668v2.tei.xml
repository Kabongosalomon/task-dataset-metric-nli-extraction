<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Primer: Searching for Efficient Transformers for Language Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
							<email>davidso@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Ma?ke</surname></persName>
							<email>wojciechm@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
							<email>hanxiaol@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
							<email>zihangd@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<country>Brain Team</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Primer: Searching for Efficient Transformers for Language Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Large Transformer models have been central to recent advances in natural language processing. The training and inference costs of these models, however, have grown rapidly and become prohibitively expensive. Here we aim to reduce the costs of Transformers by searching for a more efficient variant. Compared to previous approaches, our search is performed at a lower level, over the primitives that define a Transformer TensorFlow program. We identify an architecture, named Primer, that has a smaller training cost than the original Transformer and other variants for auto-regressive language modeling. Primer's improvements can be mostly attributed to two simple modifications: squaring ReLU activations and adding a depthwise convolution layer after each Q, K, and V projection in self-attention. Experiments show Primer's gains over Transformer increase as compute scale grows and follow a power law with respect to quality at optimal model sizes. We also verify empirically that Primer can be dropped into different codebases to significantly speed up training without additional tuning. For example, at a 500M parameter size, Primer improves the original T5 architecture on C4 auto-regressive language modeling, reducing the training cost by 4X. Furthermore, the reduced training cost means Primer needs much less compute to reach a target one-shot performance. For instance, in a 1.9B parameter configuration similar to GPT-3 XL, Primer uses 1/3 of the training compute to achieve the same one-shot performance as Transformer. We open source our models and several comparisons in T5 to help with reproducibility. 1 1 https://github.com/google-research/google-research/tree/master/primer 2 We provide details of our primitives search in TensorFlow, but the same approach can also be applied to other deep learning libraries.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformers <ref type="bibr" target="#b0">[1]</ref> have been used extensively in many NLP advances over the past few years (e.g., <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>). With scaling, Transformers have produced increasingly better performance <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, but the costs of training larger models have become prohibitively expensive.</p><p>In this paper, we aim to reduce the training costs of Transformer language models. To this end, we propose searching for more efficient alternatives to Transformer by modifying its TensorFlow computation graph <ref type="bibr" target="#b9">[10]</ref>. Given a search space of TensorFlow programs, we use evolution <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> to search for models that achieve as low of a validation loss as possible given a fixed amount of training compute. An advantage of using TensorFlow programs as the search space is that it is easier to find simple low-level improvements to optimize Transformers. We focus on decoder-only auto-regressive language modeling (LM), because of its generality and success <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref>. <ref type="bibr" target="#b1">2</ref> The discovered model, named Primer (PRIMitives searched transformER), exhibits strong performance improvements over common Transformer variants on auto-regressive language modeling. Our experiments show that Primer has the benefits of (1) achieving a target quality using a smaller training cost, <ref type="bibr" target="#b1">(2)</ref> achieving higher quality given a fixed training cost, and (3) achieving a target quality using a smaller inference cost. These benefits are robust and hold across model sizes (20M to 1.9B parameters), across compute scales (10 to 10 5 accelerator hours), across datasets (LM1B, C4, PG19 <ref type="bibr" target="#b21">[22]</ref>), across hardware platforms (TPUv2, TPUv3, TPUv4 and V100), across multiple Transformer codebases using default configurations (Tensor2Tensor, Lingvo, and T5) and across multiple model families (dense Transformers <ref type="bibr" target="#b0">[1]</ref>, sparse mixture-of-experts Switch Transformers <ref type="bibr" target="#b7">[8]</ref>, and Synthesizers <ref type="bibr" target="#b22">[23]</ref>). We open source these comparisons to help with the reproducibility of our results. <ref type="bibr" target="#b0">1</ref> Our main finding is that the compute savings of Primer over Transformers increase as training cost grows, when controlling for model size and quality. These savings follow a power law with respect to quality when using optimally sized models. To demonstrate Primer's savings in an established training setup, we compare 500M parameter Primer to the original T5 architecture, using the exact configuration used by Raffel et al. <ref type="bibr" target="#b4">[5]</ref> applied to auto-regressive language modeling. In this setting, Primer achieves an improvement of 0.9 perplexity given the same training cost, and reaches quality parity with the T5 baseline model using 4.2X less compute. We further demonstrate that Primer's savings transfer to one-shot evaluations by comparing Primer to Transformer at 1.9B parameters in a setup similar to GPT-3 XL <ref type="bibr" target="#b6">[7]</ref>. There, using 3X less training compute, Primer achieves similar performance to Transformer on both pretraining perplexity and downstream one-shot tasks.</p><p>Our analysis shows that the improvements of Primer over Transformer can be mostly attributed to two main modifications: squaring ReLU activations and adding a depthwise convolution layer after each Q, K, and V projection in self-attention. These two modifications are simple and can be dropped into existing Transformer codebases to obtain significant gains for auto-regressive language modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Search Space and Search Method</head><p>Searching Over TensorFlow Programs: To construct a search space for Transformer alternatives, we use operations from TensorFlow (TF). In this search space, each program defines the stackable decoder block of an auto-regressive language model. Given input tensors X ? R n?d that represent sequences of length n with embedding length d, our programs return tensors of the same shape. When stacked, their outputs represent next-token prediction embeddings at each sequence position. Our programs only specify model architectures and nothing else. In other words, the input and output embedding matrices themselves, as well as input preprocessing and weight optimization are not within the scope of our programs.  ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instruction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TF Primitives Vocab Generated TF Code</head><p>Instruction functions come from primitives vocab or subprogram bank. <ref type="figure">Figure 1</ref>: Overview of DNAs that define a decoder model program (i.e., an auto-regressive language model). Each DNA has a collection of subprograms, where SUBPROGRAM 0 is the MAIN() function entry point. Each subprogram is comprised of instructions, which are converted to lines of TensorFlow code. Instruction operations map to either basic TensorFlow library functions from the primitives vocabulary or one of the parent DNA's subprograms. The operation's arguments are filled using the parent instruction's argument set, which contains values for all potential operation arguments; arguments that are not used by a particular operation are simply ignored. <ref type="figure">Figure 1</ref> shows how programs are constructed in our search space. Each program is built from an evolutionary search DNA, which is an indexed collection of subprograms. SUBPROGRAM 0 is the <ref type="bibr">MAIN()</ref> function that is the execution entry point, and the other subprograms are part of the DNA's subprogram bank. Each subprogram is an indexed array of instructions with no length constraints. An instruction is an operation with a set of input arguments. The operation denotes the function that the instruction executes. Each operation maps to either a TF function from the primitives vocabulary or another subprogram in the DNA subprogram bank. The primitives vocabulary is comprised of simple primitive TF functions, such as ADD, LOG, and MATMUL (see Appendix A.1 for details). It is worth emphasizing that high-level building blocks such as self-attention are not operations in the search space, but can be constructed from our low-level operations. The DNA's subprogram bank is comprised of additional programs that can be executed as functions by instructions. Each subprogram can only call subprograms with a higher index in the subprogram bank, which removes the possibility of cycles.</p><p>Each instruction's argument set contains a list of potential argument values for each instruction operation. The set of argument fields represents the union of fields that all the operation primitives use:</p><p>? Input 1: The index of the hidden state that will be used as the first tensor input. The index of each hidden state is the index of the instruction that produced it, with the subprogram's input states at indexes 0 and 1. An example of an operation that uses this is SIN.</p><p>? Input 2: The index of the second tensor input. This is only used by operations that are binary with respect to tensor inputs. An example of an operation that uses this is ADD.</p><p>? Constant: A real valued constant. An example of an operation that uses this is MAX; tf.math.maximum(x, C) for C = 0 is how we express the Transformer's ReLU activation.</p><p>? Dimension Size: An integer representing the output dimension size for transformations that utilize weight matrices. An example of an operation that uses this is CONV 1X1, the dense projection used by the Transformer's attention projections and feed forward portions. See Appendix A.2 for how we employ relative dimensions <ref type="bibr" target="#b12">[13]</ref> to resize our models.</p><p>Our search subprograms are converted to TF programs by converting each subprogram instruction to a corresponding line of TF code, one at a time in indexing order. To create the TF line, the instruction operation is mapped to the corresponding TF primitive function or DNA subprogram, and any relevant arguments are plugged in (see Appendix A.1 for the full TF primitives vocabulary, including argument mappings); the other arguments are ignored. The TF tensor that is generated by the final instruction is taken as the subprogram output. We do not use TF Eager and so a useful property of the constructed programs is that irrelevant nodes that do not contribute to the programs' outputs are ignored as per TF's original deferred execution design <ref type="bibr" target="#b9">[10]</ref>. See <ref type="figure" target="#fig_2">Figure 2</ref> for an illustration of how subprograms are converted to TF graphs and see Appendix A.2 for more details on how TF graphs are constructed, including how we handle causal masking.  Evolutionary Search: The goal of our evolutionary search is to find the most training efficient architecture in the search space. To do this, we give each model a fixed training budget (24 TPUv2 hours) and define its fitness as its perplexity on the One Billion Words Benchmark (LM1B) <ref type="bibr" target="#b23">[24]</ref> in Tensor2Tensor <ref type="bibr" target="#b24">[25]</ref>. This approach, which we call an implicit efficiency objective by fixed training budget, contrasts previous architecture search works that explicitly aim to reduce training or inference step time when optimizing for efficiency <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. Our objective is different in that the trade-off between step time and sample efficiency is implicit. For instance, a modification that doubles step time, but triples sample efficiency is a good modification in our search, as it ultimately makes the architecture more compute efficient. Indeed, the modifications we find to be most beneficial, squaring ReLUs and adding depthwise convolutions to attention, increase training step time. However, they improve the sample efficiency of the model so much that they decrease the total compute needed to reach a target quality, by drastically reducing the number of training steps needed to get there.</p><p>The search algorithm we use is Regularized Evolution <ref type="bibr" target="#b29">[30]</ref> with hurdles <ref type="bibr" target="#b12">[13]</ref>. We configure our hurdles using a 50 th percentile passing bar and space them such that equal compute is invested in each hurdle band; this reduces the search cost by a factor of 6.25X compared to the same experiment with full model evaluations (see Appendix A.3 for more details). Additionally, we use 7 training hours as a proxy for a full day's training because a vanilla Transformer comes within 90% of its 24 hour training perplexity with just 7 hours of training. This reduces the search cost further by a factor of 3.43X, for a total compute reduction factor of 21.43X. So, although our target is to improve 24 hour performance, it only takes about 1.1 hours to evaluate an individual on average (see Appendix A.4 for more search specifics, including mutation details and hyperparameters). We run our search for ?25K individuals and retrain the top 100 individuals on the search task to select the best one.  Our search space is different from previous search spaces (see architecture search survey by <ref type="bibr" target="#b30">[31]</ref>), which are often heavily biased such that random search performs well (see analysis by <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref>). As our search space does not have this bias, 78% of random programs in our space with length equal to a Transformer program cannot train more than five minutes, due to numerical instability.</p><p>Because of this open-endedness and abundance of degenerate programs, it is necessary to initialize the search population with copies of the Transformer <ref type="bibr" target="#b12">[13]</ref> (input embedding size d model = 512, feed forward upwards projection size d f f = 2048, and number of layers L = 6) ( <ref type="figure" target="#fig_4">Figure 3</ref>). To apply this initialization to our search space, we must determine how to divide the Transformer program into subprograms. To do this, we divide along the lines of the machine learning concepts that constitute it. For instance, we create one subprogram each for self-attention, ReLU and layer norm, using commonly used implementations (see Appendix A.5 for the complete list). We call this method conceptual initialization because it introduces a bias to the search through initialization, while leaving the search space for evolution and the action space for mutations open-ended. This contrasts the large amount of previous works that introduce bias through the search space. Although some works have also explored searching spaces that are open-ended like ours on miniature tasks <ref type="bibr" target="#b34">[35]</ref>, we demonstrate that our techniques can scale to full sized deep learning regimes (see Section 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Primer</head><p>Primer:</p><p>We name the discovered model Primer, which stands for PRIMitives searched transformER (See Appendix <ref type="figure" target="#fig_2">Figure 23</ref> for the full program). Primer shows significant improvement when retrained on the search task, requiring less than half the compute of Transformer to reach the same quality ( <ref type="figure" target="#fig_7">Figure 6</ref>). In Section 4, we additionally show that Primer makes equally large gains when transferred to other codebases, training regimes, datasets, and downstream one-shot tasks.</p><p>Primer-EZ: A core motivation of this work is to develop simple techniques that can be easily adopted by language modeling practitioners. To accomplish this, we perform ablation tests across two codebases (T5 <ref type="bibr" target="#b4">[5]</ref> and Tensor2Tensor <ref type="bibr" target="#b24">[25]</ref>) and determine which Primer modifications are generally useful (Appendix <ref type="figure" target="#fig_2">Figure 26</ref>). The two that produce the most robust improvements are squaring feed forward ReLUs and adding depthwise convolution to attention multi-head projections ( <ref type="figure">Figure 4</ref>). We refer to a Transformer with just these two easy modifications as Primer-EZ; this is our recommended starting point for language modeling practitioners interested in using Primer. We now explain these modifications and then measure their empirical effectiveness.  <ref type="figure">Figure 4</ref>: The two main modifications that give Primer most of its gains: depthwise convolution added to attention multi-head projections and squared ReLU activations. These modifications are easy to implement and transfer well across codebases. We call the model with just these two modifications Primer-EZ. Blue indicates portions of the original Transformer and red signifies one of our proposed modifications.</p><p>Squared ReLU: The most effective modification is the improvement from a ReLU activation to a squared ReLU activation in the Transformer's feed forward block. Rectified polynomials of varying degrees have been studied in the context of neural network activation functions <ref type="bibr" target="#b35">[36]</ref>, but are not commonly used; to the best of our knowledge, this is the first time such rectified polynomial activations are demonstrated to be useful in Transformers. Interestingly, the effectiveness of higher order polynomials <ref type="bibr" target="#b36">[37]</ref> can also be observed in other effective Transformer nonlinearities, such as GLU <ref type="bibr" target="#b37">[38]</ref> variants like ReGLU <ref type="bibr" target="#b38">[39]</ref> (y = U x max(V x, 0) where is an element-wise product) and point-wise activations like approximate GELU <ref type="bibr" target="#b39">[40]</ref> (y = 0.5x(1 + tanh( 2/?(x + 0.044715x 3 )))). However, squared ReLU has drastically different asymptotics as x ? ? ? compared to the most commonly used activation functions: ReLU, GELU and Swish ( <ref type="figure">Figure 5</ref> left side). Squared ReLU does have significant overlap with ReGLU and in fact is equivalent when ReGLU's U and V weight matrices are the same and squared ReLU is immediately preceded by a linear transformation with weight matrix U . This leads us to believe that squared ReLUs capture the benefits of these GLU variants, while being simpler, without additional parameters, and delivering better quality ( <ref type="figure">Figure 5</ref> right side). Negative PPLX T5 C4 LM 110M Params <ref type="figure">Figure 5</ref>: Left: Squared ReLU has starkly different asymptotics compared to other common activation functions. Center: Squared ReLU has significant overlap with GLU variants <ref type="bibr" target="#b38">[39]</ref> that use activations with ReLU-like asymptotics, such as ReGLU and SwiGLU. Our experiments indicate that squared ReLU is better than these GLU variants in Transformer language models. Right: Comparison of different nonlinearities in Transformers trained on C4 auto-regressive LM for 525K steps.</p><p>Multi-DConv-Head Attention (MDHA): Another effective modification is adding 3x1 depthwise convolutions after each of the multi-head projections for query Q, key K and value V in self-attention. These depthwise convolutions are performed over the spatial dimension of each dense projection's output. Interestingly, this ordering of pointwise followed by depthwise convolution is the reverse of typical separable convolution, which we find to be less effective in Appendix A.6. We also find that wider depthwise convolution and standard convolution not only do not improve performance, but in several cases hurt it. Although depthwise convolutions have been used for Transformers before <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>, using them after each dense head projection has not been done to the best of our knowledge. MDHA is similar to Convolutional Attention <ref type="bibr" target="#b42">[43]</ref>, which uses separable convolution instead of depthwise convolution and does not apply convolution operations per attention head as we do.</p><p>Other Modifications: The other Primer modifications are less effective. Graphs for each modification can be found in Appendix A.5 and an ablation study can be found in Appendix A.7. We briefly describe the modifications and their usefulnesses here:</p><p>? Shared Q and K Depthwise Representation: Primer shares some weight matrices for Q and K. K is created using the previously described MDHA projection and Q = KW for learnable weight matrix W ? R d?d . We find that this generally hurts performance.</p><p>? Pre and Post Normalization: The standard practice for Transformers has become putting normalization before both the self-attention and feed forward transformations <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>. Primer uses normalization before self-attention but applies the second normalization after the feed forward transformation. We find this is helpful in some but not all cases.</p><p>? Custom Normalization: Primer uses a modified version of layer normalization <ref type="bibr" target="#b45">[46]</ref> that uses x(x ? ?) instead of (x ? ?) 2 , but we find this is not always effective.</p><p>? 12X Bottleneck Projection: The discovered model uses a smaller d model size of 384 (compared to the baseline's 512) and a larger d f f size of 4608 (compared to the baseline's 2048). We find this larger projection improves results dramatically at smaller sizes (?35M parameters), but is less effective for larger models, as has been previously noted <ref type="bibr" target="#b8">[9]</ref>. For this reason we do not include this modification when referencing Primer or Primer-EZ.</p><p>? Post-Softmax Spatial Gating: The discovered model has a set of per-channel learnable scalars after the attention softmax, which improves perplexity for fixed length sequences. However, these scalars cannot be applied to variable sequence lengths and so we do not include this modification in Primer for our experiments.</p><p>? Extraneous Modifications: There are a handful of additional modifications that produce no meaningful difference in the discovered architecture. For example, hidden states being multiplied by -1.12. Verifying that these modifications neither help nor hurt quality, we exclude them from discussion in the main text and do not include them when experimenting with Primer. These extraneous modifications can still be found in Appendix A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>In our experiments, we compare Primer against three Transformer variants:</p><p>? Vanilla Transformer: The original Transformer <ref type="bibr" target="#b0">[1]</ref> with ReLU activations and layer normalization <ref type="bibr" target="#b45">[46]</ref> outside of the residual path.</p><p>? Transformer+GELU: A commonly used variant of the vanilla Transformer that uses a GELU <ref type="bibr" target="#b39">[40]</ref> approximation activation function <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref>.</p><p>? Transformer++: A Transformer with the following enhancements: RMS normalization <ref type="bibr" target="#b46">[47]</ref>, Swish activation <ref type="bibr" target="#b47">[48]</ref> and a GLU multiplicative branch <ref type="bibr" target="#b37">[38]</ref> in the feed forward inverted bottleneck (SwiGLU) <ref type="bibr" target="#b38">[39]</ref>. These modifications were benchmarked and shown to be effective in T5 <ref type="bibr" target="#b48">[49]</ref>.</p><p>We conduct our comparisons across three different codebases: Tensor2Tensor (T2T) <ref type="bibr" target="#b24">[25]</ref>, T5 <ref type="bibr" target="#b4">[5]</ref>, and Lingvo <ref type="bibr" target="#b49">[50]</ref>. Tensor2Tensor is the codebase we use for searching and so a majority of our side-by-sides are done in T5 and Lingvo to prove transferability. In all cases, we use the default Transformer hyperparameters for each codebase, with regularization disabled. See Appendix A.8 for more hyperparameter details.</p><p>In the following sections, we will present our results in four main experiments on auto-regressive language modeling. First, we will show that Primer outperforms the baseline models on the search task. Next, we will show that the relationship between Primer's compute savings over Transformers and model quality follow a power law at optimal model sizes. These savings also transfer across datasets and codebases. Then, we will study Primer's gains in an established training regime and show that it enables 4.2X compute savings at a 500M parameter size using full compute T5 training.</p><p>Finally, we will demonstrate that these gains transfer to the pretraining and one-shot downstream task setup established by GPT-3 <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Search Task Comparison</head><p>We first analyze Primer's performance on the search task: LM1B language modeling with sequence length 64, ?35M model parameters, batches of 4096 tokens and 24 hours of training. We compare against the baseline models in both Tensor2Tensor (T2T) <ref type="bibr" target="#b24">[25]</ref> and T5 <ref type="bibr" target="#b4">[5]</ref> and on TPUv2s and V100 GPUs. We grade each model's performance according to how much faster it reaches the vanilla Transformer's final quality, which we will refer to as its speedup factor. <ref type="figure" target="#fig_7">Figure 6</ref> shows that Primer provides a speedup factor of 1.7X or more over Transformer in all cases. <ref type="figure" target="#fig_7">Figure 6</ref> also shows that both Primer and Primer-EZ generalize to other hardware platforms and codebases.</p><p>T r a n s f .</p><p>T r a n s f . + G E L U T r a n s f . + + P r im e r P r im e r -E Z Speedup Factor</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T2T LM1B TPUv2</head><p>T r a n s f .   Validation Loss <ref type="figure">Figure 7</ref>: Left: When sweeping over optimal model sizes, the relationship between Transformer language model quality and training compute roughly obeys a power law <ref type="bibr" target="#b8">[9]</ref>. Right: Comparison of these power law lines for varying Transformer modifications, fit with smoothed MSE loss. That the model lines are parallel to one another implies that compute savings by using superior modeling also scales as a power law with quality. Spacings between vertical dotted lines represent 2X differences in compute. Note that the x and y-axes in both plots are in log.</p><p>Next we study the scaling laws of Primer. Here we compare Primer to our baselines over many sizes by training each model using every permutation of L ? {6, 9, 12} layers, d model ? {384, 512, 1024} initial embedding size, and p ? {4, 8, 12} feed forward upwards projection ratio, creating a parameter range from 23M to 385M. The results, shown in <ref type="figure">Figure 7</ref>, corroborate previous claims that, at optimal parameters sizes, the relationship between compute and language model quality roughly follows a power law <ref type="bibr" target="#b8">[9]</ref>. That is, the relationship between validation loss, l, and training compute, c, follows the relationship l = ac ?k , for empirical constants a and k. This is represented as a line in double log space ( <ref type="figure">Figure 7</ref>): log l = ?k log c + log a. However, these lines are not the same for each architecture. The lines are roughly parallel but shifted up and down. In Appendix A.9 we show that, given a vertical spacing of log b k , parallel lines such as these indicate compute savings, s, for superior modeling also follow a power law of the form l = a</p><formula xml:id="formula_0">1 (1 ? 1/b) k s ?k .</formula><p>The intuition behind this is that b is a constant compute reduction factor for all l and thus a power law investment of training compute with relation to l results in a power law savings with relation to l as well (see Appendix A.9).  <ref type="figure">Figure 8</ref>: Pareto optimal inference comparison on LM1B. Primer demonstrates improved inference at a majority of target qualities. We observe these models have a 0.97 correlation between their train step and inference times.</p><p>Primer also has the capacity to improve inference, despite our search focusing on training compute. <ref type="figure">Figure 8</ref> shows a Pareto front comparison of quality vs. inference, when using feed forward pass timing as a proxy for inference. We use forward pass timing as a proxy for inference because there are multiple ways to decode a language model, each with varying compute costs. A more in depth study could be conducted analyzing Primer's inference performance across different decoding methods, serving platforms, datasets, etc., but that is beyond the scope of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Primer Transferability to Other Codebases, Datasets, and Model Types</head><p>We now study Primer's ability to transfer to larger datasets, PG19 and C4, in another codebase, T5. We additionally scale up to a higher compute regime that has been used as a proxy for large scale training by previous studies <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b4">5]</ref>; the batches are increased to 65K tokens, the sequence lengths are a longer 512, each decoder is 110M parameters (d model = 768, d f f = 3072, L = 12) and each model is trained to ?525K steps on 4 TPUv3 chips. We also continue training each model to 1M steps to study the effect of larger compute budgets on Primer savings. The results, shown in <ref type="figure">Figure 9</ref>, indicate that the Primer models are as strong in larger data, higher compute regimes, as they are in the smaller LM1B regime. Compared to the vanilla baseline, Primer and Primer-EZ are at least 1.8X more efficient at the end of training on both PG19 and C4.</p><p>124 Hours 236 Hours T r a n s f .</p><p>T r a n s f . + G E L U T r a n s f . + + P r i m e r P r i m e r -E Z T r a n s f .</p><p>T r a n s f . + G E L U T r a n s f . + + P r i m e r P r i m e r -E Z S w i t c h T r a n s f . S w i t c h P r i m e r S y n .</p><p>S y n . + S q r R e L U  <ref type="bibr" target="#b4">[5]</ref>. This is the same as the C4 configuration in the previous section, but uses batches of ?1M tokens, 64 TPUv3 chips and 537M parameters (d model = 1024, d f f = 8192, L = 24). Primer is 4.2X more compute efficient than the original T5 model and 2X more efficient than our strengthened Transformer++ baseline ( <ref type="table" target="#tab_4">Table 1</ref>).</p><p>The reason why savings are even better here is because, at fixed sizes, more compute invested yields higher Primer compute savings. <ref type="figure" target="#fig_9">Figure 10</ref> shows how the fraction of compute Primer needs to achieve parity with the original T5 architecture shrinks as the models are trained for longer; this is due to the asymptotic nature of both the control and variable perplexity training curves. This differs from the power law savings described in Section A.6. There, we use the optimal number of parameters for each compute budget, and so the compute saving factor, b, remains constant. For fixed model sizes, the compute saving factor grows as more compute is invested, meaning that compute savings can exceed the power law estimation. Note, this means that comparisons such as the ones given here can be "gamed" by investing more compute than is necessary for baseline models. It is for this reason that we use an exact replica of Raffel et al.'s <ref type="bibr" target="#b4">[5]</ref> training regime: to demonstrate Primer's savings in an already published training configuration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Primer Transferability to Downstream One-Shot Tasks</head><p>In our final comparison, we demonstrate Primer's improvements also hold in the pretraining and one-shot downstream task transfer regime. Recent trends in language modeling have moved towards training large models on large datasets, which is referred to as "pretraining." These models are then transferred to unseen datasets and tasks, and, without much or any additional training, demonstrate the capacity to perform well on those "downstream" tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b50">51]</ref>. In the decoder-only auto-regressive language modeling configuration we study here, the most impressive results have been achieved by GPT-3 <ref type="bibr" target="#b6">[7]</ref>, which showed that large language models can exhibit strong performance on unseen tasks given only one example -referred to as "one-shot" learning. In this section, we demonstrate that Primer's training compute savings stretch beyond reaching a target pretraining perplexity and indeed transfer to downstream one-shot task performance.</p><p>To do this, we replicate the GPT-3 pretraining and one-shot evaluation setup. This replication is not exactly the same as the one used for GPT-3 because GPT-3 was not open sourced. Thus, these experiments are not meant to compare directly to GPT-3, as there are configuration differences. Instead, these experiments are used as a controlled comparison of the Transformer and Primer architectures. We conduct these experiments in the Lingvo codebase using a proprietary pretraining dataset. The downstream tasks are configured in the same one-shot way described by Brown et al. <ref type="bibr" target="#b6">[7]</ref>, with single prefix examples fed into each model with each task's inputs. We compare <ref type="formula">(1)</ref>   <ref type="figure">Figure 11</ref>: Comparison between Transformer+GELU and Primer at 1.9B parameters and varying training compute budgets on downstream one-shot tasks, similar to GPT-3. Primer achieves slightly better performance than Transformer when given 3X less pretraining compute and substantially better performance when given the same pretraining compute. Here we stop at 72K TPUv4 hours to roughly match the quality of GPT-3 XL, but the compute savings of Primer would be larger if we let the two models run longer (see <ref type="figure" target="#fig_9">Figure 10</ref>). Note, this is a crude comparison that uses averaged scores from the 27 one-shot tasks we evaluate. See Appendix A.11 <ref type="table" target="#tab_11">(Table 6</ref> and <ref type="figure" target="#fig_2">Figure 27</ref>) for exact task scores. <ref type="figure">Figure 11</ref> shows that Primer achieves the same pretraining perplexity and one-shot downstream performance as Transformer+GELU while using 3X less compute. <ref type="table" target="#tab_11">Table 6</ref> in the Appendix gives the exact performance numbers for each of the 27 evaluated downstream tasks. Primer, despite using 3X less compute, outperforms Transfomer+GELU on 5 tasks, does worse on 1 task, and performs equivalently on the remaining 21 tasks. The same table shows that when given equivalent compute, Primer outperforms Transformer+GELU on 15 tasks, does worse on 2 tasks, and performs equivalently on the remaining 10 tasks. This result shows that not only can Primer improve language modeling perplexity, but the improvements also transfer to downstream NLP tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Limitations: There are limitations to this study. First, although our comparisons are at substantial sizes, they are still orders of magnitude smaller than state-of-the-art models such as the full-scale GPT-3 <ref type="bibr" target="#b6">[7]</ref>. Second, we focus primarily on decoder-only models, while encoder-based sequence models are still widely used <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b51">52]</ref>. In Appendix A.12, we perform encoder-decoder masked language modeling comparisons in T5, but do not study the results in significant depth. The main finding there is that, although Primer modifications improve upon vanilla Transformer, they perform only as well as Transformer++. This result suggests that architectural modifications that work well for decoder-only auto-regressive language models may not necessarily be as effective for encoder-based masked language models. Developing better encoders is a topic of our future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recommendations and Future Directions:</head><p>We recommend the adoption of Primer and Primer-EZ for auto-regressive language modeling because of their strong performance, simplicity, and robustness to hyperparameter and codebase changes. To prove their potential, we simply dropped them into established codebases and, without any changes, showed that they can give significant performance boosts. Furthermore, in practice, additional tuning could further improve their performance.</p><p>We also hope our work encourages more research into the development of efficient Transformers. For example, an important finding of this study is that small changes to activation functions can yield more efficient training. In the effort to reduce the cost of Transformers, more investment in the development of such simple changes could be a promising area for future exploration. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><formula xml:id="formula_1">(x)) x - - - SQUARE tf.math.square x - - - EXP tf.exp x - - - LOG tf.log(tf.abs(x)) x - - - C MUL tf.math.multiply x - y - ABS tf.abs x - - - RECIP tf.math.reciprocal_no_nan x - - - SIGN tf.sign x - - - COS tf.cos x - - - SIN tf.sin x - - - TANH tf.tanh x - - - MAX tf.math.maximum x - y - MIN tf.math.minimum x - y - SCALE x+tf.Variable() x - - - SHIFT x*tf.Variable() x - - - SIGMOID tf.sigmoid x - - - MASK tf.linalg.band_part input - - - CUM PROD tf.math.cumprod x - - CUM SUM tf.math.cumsum x - - - RED MEAN tf.reduce_mean input_tensor - - - RED SUM tf.reduce_sum input_tensor - - - RED MIN tf.reduce_min input_tensor - - - RED MAX tf.reduce_max input_tensor - - - RED PROD tf.reduce_prod input_tensor - - - MAT MUL tf.matmul a b - - T-MAT MUL tf.matmul(transpose_b=True) a b - - CONV 1X1 tf.layers.dense inputs - - units CONV 3X1 tf.nn.conv1d input - - filters CONV 7X1 tf.nn.conv1d input - - filters CONV 15X1 tf.nn.conv1d input - - filters CONV 31X1 tf.nn.conv1d input - - filters DCONV 3X1 tf.nn.depthwise_conv2d input - - filters DCONV 7X1 tf.nn.depthwise_conv2d input - - filters DCONV 15X1 tf.nn.depthwise_conv2d input - - filters DCONV 31X1</formula><p>tf.nn.depthwise_conv2d input --filters <ref type="table">Table 2</ref>: TensorFlow (TF) Primitives Vocabulary. "Name" is the name of the operation in our search space. "TF Function" is the TensorFlow function that the name is mapped to when a DNA instruction is being converted to a line of TensorFlow code. "Argument Mapping" describes how the values in a DNA's argument set are mapped to the corresponding TensorFlow function arguments. This vocabulary is largely constructed from the lowest level TF operations needed to create Transformers (see Appendix A.5). We additionally extend those operations to include adjacent operations; for example, we extend MAX to also include MIN, extend RED SUM to include RED PRODUCT, and extend CONV 1X1 to include CONV 3X1. We also add commonly used math primitives such as SIN and ABS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Constructing TensorFlow Graphs</head><p>TensorFlow graphs are built from DNA programs as described in Section 2 of the main text. Here we provide additional implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relative Dimensions:</head><p>We use relative dimensions <ref type="bibr" target="#b12">[13]</ref> instead of absolute dimensions for each instruction's "dimension size" argument. This allows us to resize the models to fit within our parameter limits (32M to 38M parameters). The vocabulary for these relative dimensions is <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr">64]</ref>. This vocabulary was not tuned.</p><p>Values Bank: For "constant" and "dimension size" argument fields, we create a shared bank of values that each instruction references. The constants bank holds 2 values and the dimension sizes bank holds 6 values; these numbers were not tuned. Instead of each instruction possessing their own individual values for these arguments, they instead hold an index to these shared banks. This allows multiple instructions to share the same value and to change simultaneously when that value is changed. For example, each of the individual attention multi-head projections for Q, K and V start off sharing the same output dimension size so that they all change simultaneously if that value changes. See A.4 for an example of how these bank values are mutated.</p><p>Causal Masking: An important part of teacher-forced language model training is that positions cannot "see" the token they are trying to predict. Each position should only get information from previous positions, otherwise the model will be degenerate when the targets are not provided. To enforce this causal constraint we add additional overhead to operations that move information spatially to mask out any information from future positions. For example, when applying convolutions we follow the standard practice of shifting the inputs spatially by (KERNEL WIDTH ? 1) so that each position only receives information from previous positions.</p><p>Branching: To enable multi-head capabilities for the Transformer search seed, we add a meta argument to our instructions called "branching." This argument can take any value in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref> and determines how many times that instruction is executed in parallel, with the resulting tensors being concatenated together along their embedding axes. Branching can be used with any of the TensorFlow primitives as well as with any of a DNA's subprograms. This allows us to initialize the search with multi-head self-attention by branching SUBPROGRAM 1 (self-attention) 8 times (see Appendix A.5 for subprogram implementations). Primer does not utilize this branching capability in any meaningful way, beyond using the initialized multi-head attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Resolving Dimension Mismatches:</head><p>We do not constrain how tensor dimensions can be mutated and so programs may be invalid because they perform binary operations on tensors with incompatible sizes. For example, a program may describe adding together two tensors with differing embedding sizes. To resolve these dimension mismatch issues we deterministically pseudorandomly set one of the tensor dimensions to match the other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Halving Hurdles</head><p>We configure our hurdles <ref type="bibr" target="#b12">[13]</ref> such that the top 50% of individuals passes each hurdle, according to fitness. We space the hurdles in such a way that the expected amount of compute devoted to training each hurdle band is roughly equal at the end of the search. That is, given that our maximum amount of training compute for an individual is 7 hours or 25,200 seconds (s), we construct hurdles at the 812.9s, 2438.7s, 5690.3s, and 12,193.5s marks. Thus, 1/5 of the compute budget is devoted to training every individual up to the first hurdle (812.9s), 1/5 of the compute budget is devoted to training the ?50% of individuals that are trained from the first to the second hurdle (2438.7s ? 812.9s = 1625.8s), 1/5 of the compute budget is devoted to training the ?25% of individuals that are trained from the second to the third hurdle (5690.3s ? 2438.7s = 3251.6s), etc. This configuration strategy, which we refer to as "halving hurdles," requires setting only one hyperparameter, the number of hurdles, and removes the need to set hurdle threshold values and comparison steps, as has been previously done <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b34">35]</ref>. We choose four hurdles because five hurdles would require the first hurdle to be anchored at less than ten minutes of training, which we find empirically to be too noisy of a signal. Using hurdles in this way decreases the average train time per model to 4064s or about 1 hour and 8 minutes, reducing the compute cost by a factor of ?6.2X.</p><p>This strategy is not unlike bandit algorithms such as Successive Halving <ref type="bibr" target="#b52">[53]</ref> and Hyperband <ref type="bibr" target="#b53">[54]</ref>, however we do not use a static population of individuals created a priori, but integrate our halving with the changing evolutionary population. Log Fitness <ref type="figure" target="#fig_2">Figure 12</ref>: Halving hurdles from our Primer search. Each dot represents the final fitness of an individual generated by evolution. Different "bands" form because each hurdle has a different training allowance. All bands see improvement over time, meaning that the median fitness improves for all compute allowances. This correlation between a model's performances at different training budgets allows us to reduce our total search cost by roughly a factor of 6.2X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Evolution Search Details</head><p>We use Regularized Evolution <ref type="bibr" target="#b29">[30]</ref> with a population size of 100 and a tournament selection size of 10. These values were not tuned. The mutations we use are as follows.</p><p>Mutations: To create new candidates in our search, we uniform randomly select a parent from our search population and apply a single mutation to it. We employ five different mutation types (selections and decisions are performed uniform randomly unless specified otherwise):</p><p>? Delete: Remove an instruction from a subprogram.</p><p>? Insert: Create an instruction and insert it into a subprogram.</p><p>? Delete and Insert: Perform a delete mutation followed by an insert mutation <ref type="bibr" target="#b54">[55]</ref>.</p><p>? Mutate Field: Select a field from an instruction and change its value.</p><p>? Swap: Swap the position of two instructions in a randomly selected subprogram. The input tensors for each instruction are also swapped so that the net effect is switching the positions of the instructions in the compute graph.  <ref type="figure">N (0, 1)</ref>.</p><p>After a mutation is applied, we run a light check to see if the resulting candidate's compute graph is exactly equivalent to the parent's compute graph. If it is, we perform another mutation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Transformer and Primer Program Comparisons</head><p>Here we present the programs for both the Transformer seed and the discovered Primer model. <ref type="table">Table 3</ref> is a key that maps operation names to graph symbols for subsequent graphs. <ref type="figure" target="#fig_2">Figures 13 to 22</ref> depict the subprograms for each model with the Primer changes highlighted in orange. <ref type="figure" target="#fig_2">Figure 23</ref> depicts the full compute graphs for each model, with all subprograms resolved to their constituent primitives. <ref type="figure" target="#fig_2">Figures 24 and 25</ref> depict the DNA programs for Transformer and Primer with all subprograms resolved and all instruction bank values plugged in.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head><p>Graphing symbol  <ref type="table">Table 3</ref>: Key for primitives mapped to corresponding symbols used in the following graphs.   <ref type="table">Table 4</ref>: Comparison on the search task, auto-regressive language modeling on LM1B, across two different hardware platforms (TPUv2s and V100 GPUs) and two different libraries (Tensor2Tensor and T5), using those libraries' default hyperparameters. This table contains the precise numbers for <ref type="figure" target="#fig_7">Figure 6</ref>. "Speedup" describes the fraction of compute used by each model to achieve the same results as the vanilla Transformer baseline trained with the full compute budget. Even though Primer was developed in Tensor2Tensor using TPUv2s, it shows strong performance on GPU and in T5. Perplexity is reported with respect to each library's default tokenization.</p><formula xml:id="formula_2">ADD + DIFFERENCE ? DIVIDE ? MULTIPLY ? ABS ROOT ? SQUARE x 2 EXP e x LOG Log C MUL ?C ABS | x | RECIP</formula><formula xml:id="formula_3">M ? N T-MAT MUL M ? N T CONV 1X1 Conv 1x1 CONV 3X1 Conv 3x1 CONV 7X1 Conv 7x1 CONV 15X1 Conv 15x1 CONV 31X1 Conv 31x1 DCONV 3X1 D-wise 3x1 DCONV 7X1 D-wise 7x1 DCONV 15X1 D-wise 15x1 DCONV 31X1 D-wise 31x1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Ablation and Insertion Studies</head><p>One of the core motivations of this work is to develop simple and robust Transformer modifications.</p><p>To that end, we study the individual effectiveness of each Primer modification, described in Section 3 of the main text. We measure this effectiveness using insertion and ablation studies. In the insertion studies we add each modification in isolation to a vanilla Transformer. In the ablation studies we remove each modification from Primer one at a time. We are interested in how these modifications affect performance not just in our search library, Tensor2Tensor, but also in other libraries. Thus, we perform these insertion and ablation studies in a different library, T5, as a well, and use modification transferability as the key guiding metric for our modeling recommendations.</p><p>The results of these studies are shown in <ref type="figure" target="#fig_2">Figure 26</ref>. "Normalized PPLX Delta" describes the degree to which a modification helps or hurts performance. For baseline perplexity, P b , and modification perplexity, P m , "Normalized PPLX Delta" is defined as P b ?Pm for the ablation study. These definitions differ so that a positive value always indicates that the modification is good and a negative value always indicates that the modification is bad. Three techniques are beneficial in all scenarios. The first is "12X proj," which increases the size of the Transformer feed forward upwards projection while controlling for parameters. We find this works well for smaller models but is not useful at larger sizes. The second two, MDHA and squared ReLUs, are the defining modifications of Primer-EZ, a simpler model that captures much of the gains of the full Primer. In the "Insertion Study" we insert each of the modifications into a vanilla Transformer. In the "Ablation Study," we remove each modification from Primer. "Normalized PPLX Delta" indicates the degree to which the treated models are affected by these modifications; values are normalized to be comparable across code bases and so that positive values indicate beneficial techniques in both studies. Likewise, negative values indicate harmful techniques in both studies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.8 Full Training Details</head><p>In all experiments, we use previously published hyperparameter settings that were tuned for Transformer, with regularization disabled and no additional tuning for Primer. In Tensor2Tensor (T2T) these are the TRANSFORMER_TPU hyperparameters and in T5 and Lingvo these are the open-sourced parameters used in previous T5 studies <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b48">49]</ref>. They both specify an Adafactor optimizer <ref type="bibr" target="#b55">[56]</ref>, with 10K warmup steps at a learning rate of 0.01, followed by reciprocal square root learning rate decay. T2T uses positional embeddings and subword tokenization, while T5 and Lingvo use relative attention <ref type="bibr" target="#b56">[57]</ref> and SentencePieces <ref type="bibr" target="#b57">[58]</ref>.</p><p>For LM1B, we use the T2T default settings of max sequence length of 64 and batches of 4096 tokens; this is appropriate because LM1B has an average sequence length of roughly 32. For C4 and PG19, we use the T5 default of a max sequence length of 512. For one-shot pretraining, we use a max sequence length of 1024. In Section 4.2 we use batches of 65K tokens, in Section 4.3 we use batches of 1M tokens, and in Section 4.4 we uses batches of 2M tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.9 Power Law Compute Savings Derivations</head><p>In Section 4.1 of the main text, we reproduce the results of Kaplan et al. <ref type="bibr" target="#b8">[9]</ref> and show that, at optimal parameter sizing, the relationship between language model quality and training compute follows a power law: l = ac ?k , where l is validation loss, c is training compute, and a and k are empirical constants. This is represented as a line in double log space <ref type="figure">(Figure 7)</ref>: log l = ?k log c + log a. However, these lines are not the same for each architecture we compare. The lines are roughly parallel but shifted up and down. Thus, defining the shift between two architectures' lines as log b k , we can derive the relationship of their training costs as:</p><formula xml:id="formula_4">?k log c 0 + log a 0 = ?k log c 1 + log a 0 + log b k ?k log c 0 = ?k log c 1 + log b k c ?k 0 = b k c ?k 1 c 0 = c 1 /b</formula><p>where b is a consistent reduction factor regardless of l. Compute savings, s, for using a superior architecture can now be calculated as:</p><formula xml:id="formula_5">s = c 1 ? c 0 s = c 1 ? c 1 /b = c 1 (1 ? 1/b) or c 1 = s 1 ? 1/b</formula><p>Plugging this into the original power law relationship for c 1 we get:</p><formula xml:id="formula_6">l = a 1 s 1 ? 1/b ?k l = a 1 (1 ? 1/b) k s ?k</formula><p>Thus, the relationship between quality and compute savings yielded by an improved architecture also follows a power law with coefficient a 1 (1 ? 1/b) k . This relationship is intuitive when recognizing that the compute reduction factor b is consistent for all values of l and thus a power law investment of training compute with relation to l results in a power law savings with relation to l as well.    <ref type="figure" target="#fig_2">Figure 27</ref>: Comparison between Transformer+GELU and Primer at 1.9B parameters on downstream one-shot tasks at 1/3 and full pretraining compute budgets. 95% confidence intervals are provided according to an independent t-test, using a sample of 5 pretraining weight checkpoints. Primer achieves roughly the same performance as Transformer when given 1/3 the pretraining compute and stronger performance on a majority of tasks when given the same pretraining compute. Exact numbers are presented in <ref type="table" target="#tab_11">Table 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.12 Masked Language Modeling</head><p>Encoder-decoder style masked language modeling (MLM) is not the focus of this work. However, because it was the focus of the original T5 project, we include MLM comparisons here for completeness <ref type="table">(Table 7)</ref>. Specifically, we use the exact comparison configuration used by Narang et al. <ref type="bibr" target="#b48">[49]</ref>, who benchmarked several Transformer variants; the one difference is that we only run model training one time, since this regime is not the focus of our study. For "Primer-EZ Decoder" we use a Transformer++ encoder and a Primer-EZ decoder. Our treatments demonstrate that the Primer-EZ modifications have the capacity to improve encoder-decoder MLM models, but perhaps to a lesser degree, when compared to Transformer++. We believe this indicates that decoder-only LM and encoder-decoder MLM benefit from different modeling decisions -something that could be studied in future works. We also believe that running our search on encoder-decoder MLM directly could yield modifications that are more beneficial for this task.  <ref type="table">Table 7</ref>: Masked language modeling comparison on C4 in T5 with encoder-decoder style models. These results are run in the exact same configuration as Narang et al. <ref type="bibr" target="#b48">[49]</ref>, although we only run our models once, as MLM is not the focus of our work. * indicates rows that are taken from that study.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.13 Carbon Emission Estimates</head><p>Following the recommendations of Patterson et al. <ref type="bibr" target="#b58">[59]</ref>, we release the carbon emission estimates for our largest experiments. e for 10,249 days of TPUv3 usage. Our T5 models are smaller, and so only require 687.5 TPUv3 days to train on average. We run 3 trainings (Primer, original T5 and T5++) to show Primer's improvements over baselines, yielding a total of 2062.5 TPUv3 days. When we ran our experiments, the data center 8 PUE was 1.10 instead of 1.12 and its net carbon intensity average was 0.540 MTCO 2 e/MWh instead of 0.545 MTCO 2 e/MWh. Thus, the proportional total estimate for these T5 model trainings is 46.7 MTCO 2 e * 2062. <ref type="bibr" target="#b4">5</ref> 10,249 * 1.10 1.12 * 540 545 = 8.54 MTCO 2 e. To estimate the emissions of our one-shot pretrainings in Lingvo, we measure system average power in the same manner as Patterson et al. <ref type="bibr" target="#b58">[59]</ref>. Including memory, network interface, fans, and host CPU, the average power per TPUv4 chip is 343W. We use the same equation as Patterson et al. to calculate <ref type="bibr" target="#b2">3</ref> Our CO2e accounting methodology for data center net carbon intensity does not currently fit the Greenhouse Gas (GHG) protocol for emissions reporting (Scope 2 and 3 for electricity). This deviation is due to a change in methodology where Google uses hourly life cycle emission factors, while the GHG Protocol generally relies on annual operating emission factor data. Google chooses to share these modified metrics as part of our 24/7 carbon-free energy (CFE) program, focused on our goal of achieving 100% 24/7 local CFE by 2030. Google's target for 2030 goes beyond the traditional Scope 2 rules to restrict both the location and the accounting period. This means that, instead of anywhere in a continent, the CFE purchase should be on the same geographically local grid; and instead of the accounting period being one year, the accounting should be within the same hour. <ref type="bibr" target="#b3">4</ref> While electricity consumption is relatively straightforward, strategies to reduce greenhouse gas emissions are not. For details on the distinction between conventional carbon offsets, Google's goal for 2030 of 24/7 CFE for its global data centers and campuses, and what it is doing now to set the groundwork for 2030, please see Appendix B of Patterson et al. <ref type="bibr" target="#b58">[59]</ref>. <ref type="bibr" target="#b4">5</ref> Each data center is located within a Regional Grid, which is the geographic basis for Google's 24/7 CFE goals. For our data center in Georgia, the Regional Grid is the Southern Company balancing authority. <ref type="bibr" target="#b5">6</ref> The net carbon intensity at a particular data center is based on accounting for hourly emission reductions via real time, local carbon-free energy purchases. This is calculated using the 24/7 carbon-free energy methodology, which can be reviewed in greater depth in "24/7 Carbon-Free Energy: Methodologies and Metrics" [60]. <ref type="bibr" target="#b6">7</ref> The carbon intensity values utilized in this paper are at the annual 2020 grid level for each data center in which the models were run. <ref type="bibr" target="#b7">8</ref> For our data center in Taipei, for purposes of Google's 24/7 CFE accounting, the Regional Grid is Taiwan.</p><p>CO 2 e for our 2 large scale pretrainings: 2 * 343W * 71,800h * 1.08(PUE) * 0.055 MTCO 2 e/MWh = 29.26 MTCO 2 e. 9</p><p>The emission cost for our large scale T5 and one-shot comparisons are higher than the cost of the architecture search itself. We invest in these large scale comparisons to demonstrate the potential savings of our efficient modifications. For instance, the savings for using Primer over Transformer described in Section 4.4 of the main text equates to 9.75 MTCO 2 e, which alone is ?4.7X the cost of the architecture search. Note, differences in hardware setups affect these savings. For example, the one-shot models were trained in Oklahoma, which has favorable MTCO 2 e/MWh when compared to Georgia, where the Primer search was conducted. To factor out the effects of these hardware differences, we can instead analyze Primer's return on investment in terms of FLOPs. The search for Primer cost ?2.14E+21 FLOPs. Training Transformer for the one-shot comparison cost ?2.96E+22 FLOPs, which means the compute saved by Primer is ?1.98E+22 FLOPs, given that it only requires a third of the compute to achieve the same quality. Thus, Primer's savings in the one-shot experiments are 9.24X the cost of the architecture search itself, yielding returns on investing in the search. Note that the search cost is a one-time cost and that Primer can be reused in future trainings to save more compute. For example, our largest models are roughly 100X smaller than the full scale GPT-3 <ref type="bibr" target="#b6">[7]</ref>, and so the return on our search investment can grow if Primer is scaled up to larger training configurations.  <ref type="table">Table 8</ref>: Auto-regressive language modeling comparison between Primer and various baselines, including the Evolved Transformer, controlling for training steps in T5. These are the same experiments featured in <ref type="table" target="#tab_10">Tables 4 and 5</ref>, but with the data presented to compare sample efficiency instead of training compute efficiency.</p><p>This work builds off of the Evolved Transformer <ref type="bibr" target="#b12">[13]</ref>, which also sought to discover improved sequence models using architecture search. Compute efficiency comparisons to the Evolved Transformer architecture are provided in T5 on LM1B in <ref type="table">Table 4</ref> and on C4 in <ref type="table" target="#tab_10">Table 5</ref>. Sample efficiency comparisons to the Evolved Transformer architecture are offered in <ref type="table">Table 8</ref> on those same experiments. In this section we discuss these comparisons and how they highlight the improvements of our Primer search over the Evolved Transformer search.</p><p>Firstly, our Primer search aims to improve training compute efficiency, which yields more practical results than the sample efficiency objective of So et al. <ref type="bibr" target="#b12">[13]</ref>, who controlled for number of train steps when evaluating models. Evolved Transformer is effective in this controlled-train-step regime when comparing to other baselines, as shown in <ref type="table">Table 8</ref>. When controlling for number of training steps in this way, Evolved Transformer is roughly on par with Transformer++ on C4 and is better than Transformer++ on LM1B. However, Evolved Transformer is substantially slower than all other models (see <ref type="table" target="#tab_10">Tables 4 and 5</ref>) because it is deeper; we follow the same scaling policy as So et al. of adding additional layers to control for parameters, given that an Evolved Transformer layer has significantly less parameters than a standard Transformer layer. Evolved Transformer's slowness counteracts its sample efficiency and for this reason its speedup factor is diminished on LM1B and less than 1.0 (indicating a slowdown over vanilla Transformer) on C4 (see <ref type="table" target="#tab_10">Tables 4 and 5</ref>). This limits Evolved Transformer's practicality. In contrast, Primer is designed to specifically address this shortcoming and thus delivers the practical result of substantial compute savings.</p><p>The open-ended nature of the Primer search also allows for effective modifications that were not available to the Evolved Transformer search. In fact, none of the Primer modifications (see Section 3) can be represented in the Evolved Transformer search space, aside from resizing hidden dimension sizes. This is because the Evolved Transformer search space followed a rigid ordering of components and used a vocabulary of unalterable high level building blocks. For example, normalization always preceded weighted transformations and, although there were different weighted transformations to choose from such as self-attention and GLU, those transformations could not be modified by the search. In contrast, the Primer search space allows for the modification of all initialized modulessuch as weighted transformations, activation functions and normalization functions -as well as allows for macro-level reordering, such as moving normalization after weighted transformations. We believe that this difference in openness is what allowed Primer to develop definitively superior modifications, as demonstrated not only by improved compute efficiency, but also by improved sample efficiency <ref type="table">(Table 8)</ref>, which is what Evolved Transformer was meant to optimize.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.15 Practical Discussion</head><p>The main motivation of this work is to develop simple and practical changes to Transformers that can be easily adopted. To that end, we provide answers to some questions that practitioners may ask:</p><p>? Are the Primer training compute savings going to be the same in all setups? No. Across our own provided experiments, Primer yields various compute savings. This is because the compute savings depend on hardware specifics, deep learning library operation speeds, model sample efficiencies on specific tasks, and other factors that may vary across setups. We use the exact replica of T5 training as a demonstration of what savings look like in an established configuration (4.2X), but expect results to vary across configurations. ? Can Primer improve BERT <ref type="bibr" target="#b1">[2]</ref>? This work has focused on the specific task of autoregressive language modeling, which, with the development of GPT-3, proves to be important for both traditional NLP applications as well as generative applications. We have only briefly investigated Primer's application to masked language modeling and encoder-decoder models (Appendix A.12). Our investigations show that, while Primer improves upon vanilla Transformer, it is not obviously better than Transformer++. Thus, modifications that work well for auto-regressive language modeling may not be as effective for masked language modeling. Future work could investigate if the Primer modifications can be integrated into encoder-decoder and encoder-only models in a more effective way that can improve models like BERT. Future work could also apply the search method described here to finding better encoder-based masked language models. ? Do hyperparameter configurations need to be retuned to use Primer? Our intention is for Primer modifications to not require any additional hyperparameter tuning. To that end, in our experiments we did not tune any hyperparameters, and instead used the Transformer hyperparameters from established libraries. However, Primer may work even better with additional tuning. ? Is Primer-EZ better than Primer? In our comparison experiments, we find that Primer-EZ is sometimes better than Primer in the T5 codebase. However, in application to other codebases, such as Lingvo and T2T, we find that the full Primer can give improved performance over Primer-EZ. Thus, we recommend that practitioners first try using Primer-EZ for its ease of implementation and then move on to implementing the full Primer if they are interested in achieving further gains.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>CONV 1X1: tf.layers.dense MAX: tf.math.maximum SIN: tf.math.sin tf.layers.dense(inputs=hidden_state_0, units=512)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Example of a program converted into its corresponding TensorFlow graph. Nodes that do not contribute to the program output are not executed thanks to TensorFlow's deferred execution design.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Small scale searches (10 hours on 10 TPUv2 chips) comparing conceptual initialization to random initialization. Our search space is open-ended enough that it is infeasible to search without strong initialization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Comparison on the LM1B search task using 35M parameter models. "Speedup Factor" refers to the fraction of compute each model needs to reach quality parity with the vanilla Transformer trained for 24 hours. Primer and Primer-EZ both achieve over 1.7X speedup in all cases. Note that results transfer across codebases and different hardware.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Compute savings of Primer vs. the original T5 architecture on C4 LM over time. The more compute invested in training, the higher the savings due to the asymptotic nature of both perplexity curves. Primer achieves the same performance as the original T5 architecture with 4.2X less compute.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>?</head><label></label><figDesc>Mutate Bank Value: Change the value of a relative tensor dimension or constant in the corresponding bank. The values for relative tensor dimensions are selected from their vocabulary (see Appendix A.2). The values for constants are changed according to c new := c prev ? 10 X + Y for previous value c prev , new value c new and random variables X, Y ?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 13 :Figure 14 :Figure 15 :Figure 16 :Figure 17 :Figure 18 :Figure 19 :Figure 20 :Figure 21 :Figure 22 :Figure 23 :</head><label>1314151617181920212223</label><figDesc>Main subprograms. Changes are highlighted in orange. Attention subprograms. Changes are highlighted in orange. Feed forward subprograms. Changes are highlighted in orange. Multi-head projection subprograms. Changes are highlighted in orange. Softmax subprograms. Changes are highlighted in orange. Normalization subprograms. Changes to this subprogram are realized in downstream changes to S6. Z-score normalization subprograms. Changes are highlighted in orange. Scale-shift subprograms. No changes here. Residual connection subprograms. This change is essentially a functional no-op. Activation function subprograms. Changes are highlighted in orange. Comparison of Transformer (Left) and Primer (Right) programs, with all subprograms resolved to their constituent primitives. Primer differences are highlighted in orange.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>study and Pm?P b P b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 26 :</head><label>26</label><figDesc>Investigation into transferability of Primer modifications on LM1B at ?35M parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Comparison transferring Primer to larger datasets (C4 and PG19) and different model families (Switch Transformer and Synthesizer) in a different codebase (T5) with an order of magnitude more compute than the search task. Compared to the vanilla baseline, Primer and Primer-EZ are at least 1.8X more efficient at the end of training on both PG19 and C4. In all cases, the fraction of Primer compute savings increases as more compute is invested. Primer-EZ modifications also improve Switch Transformer (550M params) and Synthesizer (145M params), showing that it is compatible with other efficient methods. Compute budgets are selected according to how long it takes each baseline to train for 525K and 1M steps. See Appendix A.10 for exact numbers.Figure 9 also shows that the Primer modifications are compatible with other efficient model families, such as large sparse mixture-of-experts like Switch Transformer [8] and efficient Transformer approx-.3 Large Scale T5 Auto-Regressive Language Model Training Comparison in compute usage to reach target qualities on C4 LM at 537M parameters using the full T5 compute scale. Target qualities are selected according to the final performances of the baseline models. Primer achieves the same quality as the original T5 architecture using 4.2X less compute.In large scale compute configurations, the Primer compute savings ratios are even higher. To demonstrate Primer's savings in an established high compute training setup, we scale up to the full T5 compute regime, copying Raffel et al. exactly</figDesc><table><row><cell>Speedup Factor</cell><cell>0.8 1.0 1.2 1.4 1.6 1.8 2.0 2.2 2.4</cell><cell>C4 127 Hours 242 Hours</cell><cell>PG19 127 Hours 242 Hours</cell><cell>C4 142 Hours 270 Hours</cell><cell>C4</cell></row><row><cell>Figure 9:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>imations like Synthesizer [23]. For these experiments, we use the T5 implementations provided by Narang et al. [49]. The Primer-EZ techniques of added depthwise convolutions and squared ReLUs reduce Switch Transformer's compute cost by a factor of 1.5X; this translates to a 0.6 perplexity im- provement when controlling for compute (see Appendix A.10). Adding squared ReLUs to Synthesizer reduces training costs by a factor of 2.0X and improves perplexity by 0.7 when fully trained.4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>a baseline 1.9B parameter Transformer (d model = 2048, d f f = 12288, L = 24) with GELU activations, meant to approximate the GPT-3 XL architecture, and (2) a full Primer without shared QK representations, which only hurt performance according to Appendix A.7. Each model is trained using batches of ?2M tokens using 512 TPUv4 chips for ?140 hours (?71.8K total accelerator hours or ?1M train steps). We once again use the T5 training hyperparemeters without any additional tuning.</figDesc><table><row><cell>Negative PPLX</cell><cell>19 16 13</cell><cell>0</cell><cell>36000 TPUv4 Hours One-Shot Pretraining 72000 Transformer Primer</cell><cell>Neg PPLX</cell><cell>-16 -13 Pretraining</cell><cell>Average Score</cell><cell>29 37</cell><cell>One-Shot QA Tasks</cell><cell>52 Multi-Choice Tasks One-Shot 57 Average Score</cell><cell>Transf. 24K Hrs Transf. 72K Hrs Primer 24K Hrs Primer 72K Hrs</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>1</head><label></label><figDesc>TensorFlow Primitives Vocabulary</figDesc><table><row><cell>Name</cell><cell>TF Function</cell><cell></cell><cell cols="2">Argument Mapping</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Input 1</cell><cell cols="2">Input 2 Constant</cell><cell>Dim Size</cell></row><row><cell>ADD</cell><cell>tf.math.add</cell><cell>x</cell><cell>y</cell><cell>-</cell><cell>-</cell></row><row><cell>DIFFERENCE</cell><cell>tf.math.subtract</cell><cell>x</cell><cell>y</cell><cell>-</cell><cell>-</cell></row><row><cell>DIVIDE</cell><cell>tf.math.divide</cell><cell>x</cell><cell>y</cell><cell>-</cell><cell>-</cell></row><row><cell>MULTIPLY</cell><cell>tf.math.multiply</cell><cell>x</cell><cell>y</cell><cell>-</cell><cell>-</cell></row><row><cell>ABS ROOT</cell><cell>tf.math.sqrt(tf.abs</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>A.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>A.10 Exact T5 Numbers for Medium Sized Experiments</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Baseline Compute @525K</cell><cell cols="3">Baseline Compute @1M</cell></row><row><cell>Model</cell><cell cols="6">Params Steps PPLX Speedup Steps PPLX Speedup</cell></row><row><cell></cell><cell></cell><cell></cell><cell>C4</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vanilla Transformer</cell><cell>110M</cell><cell>525K 20.61</cell><cell>-</cell><cell>1M</cell><cell>19.82</cell><cell>-</cell></row><row><cell>Transformer+GELU</cell><cell>110M</cell><cell>524K 20.34</cell><cell>1.20</cell><cell cols="2">998K 19.58</cell><cell>1.26</cell></row><row><cell>Transformer++</cell><cell>110M</cell><cell>524K 20.03</cell><cell>1.52</cell><cell cols="2">998K 19.28</cell><cell>1.64</cell></row><row><cell>Evolved Transformer</cell><cell>110M</cell><cell>351K 20.79</cell><cell>0.89</cell><cell cols="2">668K 19.84</cell><cell>0.98</cell></row><row><cell>Primer</cell><cell>110M</cell><cell>483K 19.82</cell><cell>1.68</cell><cell cols="2">920K 19.07</cell><cell>1.91</cell></row><row><cell>Primer-EZ</cell><cell>110M</cell><cell>471K 19.83</cell><cell>1.71</cell><cell cols="2">896K 19.07</cell><cell>1.90</cell></row><row><cell>Switch Transformer</cell><cell>550M</cell><cell>525K 17.16</cell><cell>-</cell><cell>1M</cell><cell>16.32</cell><cell>-</cell></row><row><cell>Switch Primer</cell><cell>550M</cell><cell>474K 16.56</cell><cell>1.45</cell><cell cols="2">900K 15.82</cell><cell>1.56</cell></row><row><cell>Synthesizer</cell><cell>145M</cell><cell>525K 20.35</cell><cell>-</cell><cell>1M</cell><cell>19.57</cell><cell>-</cell></row><row><cell>+ Squared ReLU</cell><cell>145M</cell><cell>523K 19.55</cell><cell>1.74</cell><cell cols="2">996K 18.83</cell><cell>1.96</cell></row><row><cell></cell><cell></cell><cell></cell><cell>PG19</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Vanilla Transformer</cell><cell>110M</cell><cell>525K 16.39</cell><cell>-</cell><cell>1M</cell><cell>15.83</cell><cell>-</cell></row><row><cell>Transformer+GELU</cell><cell>110M</cell><cell>524K 16.35</cell><cell>1.01</cell><cell cols="2">998K 15.84</cell><cell>0.95</cell></row><row><cell>Transformer++</cell><cell>110M</cell><cell>524K 16.15</cell><cell>1.18</cell><cell cols="2">998K 15.64</cell><cell>1.20</cell></row><row><cell>Primer</cell><cell>110M</cell><cell>483K 15.96</cell><cell>1.68</cell><cell cols="2">920K 15.31</cell><cell>1.81</cell></row><row><cell>Primer-EZ</cell><cell>110M</cell><cell>471K 15.84</cell><cell>1.74</cell><cell cols="2">896K 15.37</cell><cell>1.98</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>Language modeling comparison on larger datasets, transferring Primer to the T5 codebase. In this transferred regime, Primer improves upon all baselines. Furthermore, Primer-EZ not only reaches parity with Primer, but in some cases, surpasses it. Switch Transformer and Synthesizer also benefit from the Primer-EZ modifications. Compute budget comparison points are chosen according to how long it takes vanilla baselines to reach 525K and 1M training steps. Perplexities are given with respect to SentencePieces. This table has the precise numbers forFigure 9.</figDesc><table><row><cell cols="4">A.11 Performance on Individual One-Shot Tasks</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Task</cell><cell>Metric</cell><cell>Transf. 1/3</cell><cell>Transf. Full</cell><cell>Primer 1/3</cell><cell>Primer Full</cell><cell>GPT-3 XL</cell></row><row><cell>Pretraining</cell><cell>pplx</cell><cell>15.3</cell><cell>14.3</cell><cell>14.3</cell><cell>13.5</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell cols="2">Question Answering Tasks</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TriviaQA</cell><cell>acc</cell><cell cols="4">22.5 ? 0.4 26.8 ? 0.5 27.5 ? 0.4 32.2 ? 0.5</cell><cell>26.5</cell></row><row><cell>WebQs</cell><cell>acc</cell><cell>9.1 ? 0.5</cell><cell>9.6 ? 0.4</cell><cell>9.8 ? 0.8</cell><cell>10.4 ? 0.3</cell><cell>9.2</cell></row><row><cell>NQs</cell><cell>acc</cell><cell>5.8 ? 0.2</cell><cell>6.7 ? 0.2</cell><cell>7.8 ? 0.5</cell><cell>9.1 ? 0.3</cell><cell>5.4</cell></row><row><cell>SQuADv2</cell><cell>f1</cell><cell cols="3">54.2 ? 2.4 65.4 ? 2.9 64.2 ? 3.7</cell><cell>67.8 ? 1.2</cell><cell>54</cell></row><row><cell>CoQa</cell><cell>f1</cell><cell cols="4">52.5 ? 1.1 57.7 ? 1.2 59.1 ? 0.9 61.2 ? 0.7</cell><cell>66.1</cell></row><row><cell>DROP</cell><cell>f1</cell><cell cols="4">21.5 ? 0.4 23.4 ? 0.2 24.8 ? 0.5 26.5 ? 0.2</cell><cell>23</cell></row><row><cell>Quac</cell><cell>f1</cell><cell cols="3">30.1 ? 0.5 30.9 ? 0.7 28.9 ? 0.9</cell><cell>30.2 ? 0.7</cell><cell>32.3</cell></row><row><cell>LAMBADA</cell><cell>acc</cell><cell cols="3">51.5 ? 0.9 55.2 ? 1.3 54.5 ? 1.1</cell><cell>56.8 ? 0.9</cell><cell>58.3</cell></row><row><cell>QA Average</cell><cell>avg</cell><cell>30.9</cell><cell>34.5</cell><cell>34.6</cell><cell>36.8</cell><cell>34.3</cell></row><row><cell></cell><cell></cell><cell cols="2">Multi-Choice Schema Tasks</cell><cell></cell><cell></cell><cell></cell></row><row><cell>HellaSwag</cell><cell>acc</cell><cell cols="4">55.7 ? 0.3 59.5 ? 0.2 60.2 ? 0.3 63.3 ? 0.2</cell><cell>53.5</cell></row><row><cell>StoryCloze</cell><cell>acc</cell><cell cols="4">75.2 ? 0.3 75.9 ? 0.4 76.9 ? 0.2 77.5 ? 0.3</cell><cell>74.2</cell></row><row><cell>Winogrande</cell><cell>acc</cell><cell cols="4">55.4 ? 0.3 58.4 ? 0.4 58.8 ? 0.3 60.4 ? 0.2</cell><cell>59.1</cell></row><row><cell>PIQA</cell><cell>acc</cell><cell cols="4">72.6 ? 0.5 72.6 ? 0.3 73.7 ? 0.5 75.0 ? 0.4</cell><cell>74.4</cell></row><row><cell>ARC (Challenge)</cell><cell>acc</cell><cell cols="4">32.7 ? 0.4 34.4 ? 0.3 35.6 ? 0.9 37.4 ? 0.4</cell><cell>36.4</cell></row><row><cell>ARC (Easy)</cell><cell>acc</cell><cell cols="4">64.5 ? 0.5 64.9 ? 0.5 65.6 ? 0.6 67.5 ? 0.5</cell><cell>55.9</cell></row><row><cell>OpenBookQA</cell><cell>acc</cell><cell cols="4">45.3 ? 0.9 46.8 ? 0.8 47.9 ? 0.4 49.3 ? 0.5</cell><cell>46.4</cell></row><row><cell>ANLI R1</cell><cell>acc</cell><cell cols="3">33.9 ? 1.2 35.5 ? 0.2 35.5 ? 0.4</cell><cell>34.8 ? 0.3</cell><cell>34.6</cell></row><row><cell>ANLI R2</cell><cell>acc</cell><cell cols="3">33.5 ? 0.7 33.4 ? 0.5 34.5 ? 0.6</cell><cell>33.5 ? 0.4</cell><cell>32.7</cell></row><row><cell>ANLI R3</cell><cell>acc</cell><cell cols="3">34.5 ? 0.7 35.2 ? 0.1 33.0 ? 0.3</cell><cell>33.8 ? 0.5</cell><cell>33.9</cell></row><row><cell>ReCoRD</cell><cell>acc</cell><cell cols="4">84.8 ? 0.1 86.3 ? 0.2 85.8 ? 0.3 86.7 ? 0.0</cell><cell>83</cell></row><row><cell>WSC</cell><cell>acc</cell><cell cols="3">67.4 ? 0.8 66.8 ? 1.2 69.3 ? 1.3</cell><cell>68.9 ? 1.2</cell><cell>62.5</cell></row><row><cell>BoolQ</cell><cell>acc</cell><cell cols="3">58.9 ? 1.1 63.6 ? 2.1 60.7 ? 0.8</cell><cell>64.7 ? 2.0</cell><cell>63.7</cell></row><row><cell>CB</cell><cell>acc</cell><cell cols="3">56.3 ? 2.5 53.0 ? 2.7 55.4 ? 3.3</cell><cell>56.6 ? 9.6</cell><cell>48.2</cell></row><row><cell>RTE</cell><cell>acc</cell><cell cols="3">48.4 ? 1.2 53.6 ? 2.5 54.3 ? 1.5</cell><cell>52.9 ? 2.8</cell><cell>49.5</cell></row><row><cell>COPA</cell><cell>acc</cell><cell cols="3">80.2 ? 3.2 87.2 ? 1.2 84.8 ? 1.5</cell><cell>87.5 ? 1.1</cell><cell>74</cell></row><row><cell>WiC</cell><cell>acc</cell><cell cols="4">51.6 ? 0.2 51.0 ? 0.5 51.7 ? 0.1 51.8 ? 0.1</cell><cell>49.2</cell></row><row><cell>RACE-h</cell><cell>acc</cell><cell cols="4">39.4 ? 0.4 40.8 ? 0.4 40.4 ? 0.4 43.7 ? 0.3</cell><cell>42</cell></row><row><cell>RACE-m</cell><cell>acc</cell><cell cols="4">50.0 ? 1.0 52.6 ? 0.4 51.8 ? 0.8 54.0 ? 0.4</cell><cell>55.2</cell></row><row><cell>Multi-Choice Average</cell><cell>avg</cell><cell>53.1</cell><cell>54.7</cell><cell>55</cell><cell>56.2</cell><cell>54.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 6 :</head><label>6</label><figDesc>Comparison between Transformer+GELU and Primer at 1.9B parameters on downstream one-shot tasks at 1/3 and full pretraining compute budgets. One-shot sample means and standard deviations are computed using the evaluated performance of 5 weight checkpoints. Bold numbers denote improved one-shot performance and shaded numbers denote worse one-shot performance compared to Transformer with full compute that is statistically significant under an independent t-test with p-value threshold 0.05. Primer achieves the same performance as Transformer when given 1/3 the training compute and stronger performance on a majority of tasks when given the same training compute. GPT-3 XL<ref type="bibr" target="#b6">[7]</ref> scores are provided as a grounding reference point; they should not be closely compared to our results as the models have different pretraining configurations.</figDesc><table><row><cell></cell><cell>Transf. 1/3 Primer Full Primer 1/3 Transf. Full</cell><cell>31 38</cell><cell>ANLI R1</cell><cell>31 36</cell><cell>ANLI R2</cell><cell>31 37</cell><cell>ANLI R3</cell></row><row><cell>39</cell><cell>ARC (Challenge)</cell><cell>69</cell><cell>ARC (Easy)</cell><cell>68</cell><cell>BoolQ</cell><cell>62</cell><cell>CB</cell></row><row><cell>30</cell><cell></cell><cell>62</cell><cell></cell><cell>54</cell><cell></cell><cell>47</cell><cell></cell></row><row><cell>92</cell><cell>COPA</cell><cell>63</cell><cell>CoQa</cell><cell>28</cell><cell>DROP</cell><cell>64</cell><cell>HellaSwag</cell></row><row><cell>74</cell><cell></cell><cell>49</cell><cell></cell><cell>19</cell><cell></cell><cell>54</cell><cell></cell></row><row><cell>59</cell><cell>LAMBADA</cell><cell>10</cell><cell>NQs</cell><cell>51</cell><cell>OpenBookQA</cell><cell>76</cell><cell>PIQA</cell></row><row><cell>48</cell><cell></cell><cell>4</cell><cell></cell><cell>43</cell><cell></cell><cell>70</cell><cell></cell></row><row><cell>33</cell><cell>Quac</cell><cell>45</cell><cell>RACE-h</cell><cell>56</cell><cell>RACE-m</cell><cell>88</cell><cell>ReCoRD</cell></row><row><cell>26</cell><cell></cell><cell>37</cell><cell></cell><cell>47</cell><cell></cell><cell>83</cell><cell></cell></row><row><cell>58</cell><cell>RTE</cell><cell>73</cell><cell>SQuADv2</cell><cell>79</cell><cell>StoryCloze</cell><cell>33</cell><cell>TriviaQA</cell></row><row><cell>43</cell><cell></cell><cell>48</cell><cell></cell><cell>73</cell><cell></cell><cell>20</cell><cell></cell></row><row><cell>12</cell><cell>WebQs</cell><cell>53</cell><cell>WiC</cell><cell>61</cell><cell>Winogrande</cell><cell>72</cell><cell>WSC</cell></row><row><cell>6</cell><cell></cell><cell>49</cell><cell></cell><cell>53</cell><cell></cell><cell>64</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>Model Params Pretraining Log PPLX SGLUE XSum WebQ</figDesc><table><row><cell>Vanilla Transformer*</cell><cell>223M</cell><cell>1.838</cell><cell>70.97</cell><cell>17.78</cell><cell>23.02</cell></row><row><cell>Transformer+GeLU*</cell><cell>223M</cell><cell>1.838</cell><cell>73.67</cell><cell>17.86</cell><cell>25.13</cell></row><row><cell>Transformer++</cell><cell>224M</cell><cell>1.792</cell><cell>75.65</cell><cell>17.90</cell><cell>25.92</cell></row><row><cell>Primer-EZ Decoder</cell><cell>224M</cell><cell>1.787</cell><cell>76.69</cell><cell>17.87</cell><cell>24.87</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>To estimate the carbon emissions<ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4</ref> for our architecture search, we build off of the measurements taken by Patterson et al. Their emissions estimate for architecture search is 3.2 MTCO 2 e for 1360 days of TPUv2 usage<ref type="bibr" target="#b58">[59]</ref>. Here, we use 1145.8 days of TPUv2 compute for our search. Additionally, the PUE for our data center 5 at the time of our search was 1.08 instead of 1.10, and its net carbon intensity average was 0.336 MTCO 2 e/MWh instead of 0.431 MTCO 2 e/MWh.<ref type="bibr" target="#b5">6</ref>,7 Thus, the proportional emissions estimate for our architecture search experiments is 3.2 MTCO 2 e * 1145.8 1360 * 1.08 1.10 * 336 431 = 2.06 MTCO 2 e. For comparison, a round trip plane ticket from San Francisco to New York for a single passenger is ?1.2 MTCO 2 e [59] and so our search costs roughly 1.72 such plane tickets. We follow the same process of building off of the Patterson et al. measurements to estimate emissions for our large scale T5 experiments. The Patterson et al. emissions estimate for 11B parameter T5 is 46.7 tCO 2</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9">For our data center in Oklahoma, for purposes of Google's 24/7 CFE accounting, the Regional Grid is the Southwest Power Pool (SPP) Independent System Operator.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Zhen Xu for his help with infrastructure. We also thank Gabriel Bender, Hallie Cramer, Andrew Dai, Nan Du, Yanping Huang, Daphne Ippolito, Norm Jouppi, Lluis-Miquel Munguia, Sharan Narang, Ruoming Pang, David Patterson, Yanqi Zhou, and the Google Brain Team for their help and feedback.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">140</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Adiwardana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Nemade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09977</idno>
		<title level="m">Towards a human-like open-domain chatbot</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03961</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Scaling laws for neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.08361</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mart?n Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Martin Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><forename type="middle">Leon</forename><surname>Suematsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Hierarchical representations for efficient architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chrisantha</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The evolved transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Evolving normalizationactivation layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evolving artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="page" from="1423" to="1447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Evolutionary principles in self-referential learning. (on learning how to learn: The meta-meta-... hook.). Diploma thesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987" />
			<pubPlace>Germany</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Technische Universitat Munchen</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Designing neural networks through neuroevolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Risto</forename><surname>Miikkulainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="24" to="35" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Technical report, OpenAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">It&apos;s not just size that matters: Small language models are also few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07118</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Entailment as few-shot learner</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanzi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14690</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Making pre-trained language models better few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.15723</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
		<idno>abs/1911.05507</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Synthesizer: Rethinking self-attention in transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00743</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Robinson</surname></persName>
		</author>
		<editor>Interspeech</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Gouws</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Sepassi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07416</idno>
		<title level="m">Noam Shazeer, and Jakob Uszkoreit. Tensor2tensor for neural machine translation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Platformaware neural architecture search for mobile</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mnasnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2815" to="2823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ligeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
		<idno>arXiv preprint:1812.00332</idno>
		<title level="m">Proxylessnas: Direct neural architecture search on target task and hardware</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient multi-objective neural architecture search via lamarckian evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Regularized evolution for image classifier architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alok</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanping</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural architecture search: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Elsken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Hendrik</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">55</biblScope>
			<biblScope unit="page" from="1" to="21" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Random search and reproducibility for neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><forename type="middle">S</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Evaluating the search phase of neural architecture search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaicheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Sciuto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudiu</forename><surname>Musat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Can weight sharing outperform random architecture search? An investigation with tunas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Bender</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter-Jan</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Automl-zero: Evolving machine learning algorithms from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dense associative memory for pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Krotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">J</forename><surname>Hopfield</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Multiplicative interactions and where to find them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">M</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><forename type="middle">W</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pascanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05202</idno>
		<title level="m">Glu variants improve transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Bridging nonlinearities and stochastic regularizers with gaussian error linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Combining local convolution with global self-attention for reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adams</forename><forename type="middle">Wei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qanet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Conformer: Convolutionaugmented transformer for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anmol</forename><surname>Gulati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Cheng</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiahui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shibo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Interspeech</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><surname>Codella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengchen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15808</idno>
		<title level="m">CvT: Introducing convolutions to vision transformers</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Adaptive input representations for neural language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1809.10853</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huishuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.04745</idno>
		<title level="m">On layer normalization in the transformer architecture</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Root mean square layer normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Biao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prajit</forename><surname>Ramachandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05941</idno>
		<title level="m">Searching for activation functions</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>F?vry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karishma</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Malkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.11972</idno>
		<title level="m">and Colin Raffel. Do transformer modifications transfer across implementations and applications? arXiv preprint</title>
		<meeting><address><addrLine>Nan Ding, Jake Marcus, Adam Roberts</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anjuli</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smit Hinsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Laurenzo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suyog</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyuan</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohit</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiao</forename><surname>Prabhavalkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyoukjoong</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ciprian</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rohan</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajat</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Tibrewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akiko</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Eriguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Ari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parisa</forename><surname>Cherry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otavio</forename><surname>Haghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youlong</forename><surname>Good</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaac</forename><surname>?lvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Ning</forename><surname>Caswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongheng</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chieh</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ekaterina</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katrin</forename><surname>Gonina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Tomanek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zelin</forename><surname>Vanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dehao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuki</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">F</forename><surname>Irie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uri</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lingvo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>scalable framework for sequence-to-sequence modeling. ArXiv, abs/1902.08295</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Semi-supervised sequence learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Almost optimal exploration in multi-armed bandits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zohar</forename><surname>Karnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomer</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Somekh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Hyperband: A novel bandit-based approach to hyperparameter optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Jamieson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Desalvo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">185</biblScope>
			<biblScope unit="page" from="1" to="52" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Program synthesis using uniform mutation by addition and deletion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Helmuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mcphee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Spector</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Genetic and Evolutionary Computation Conference</title>
		<meeting>the Genetic and Evolutionary Computation Conference</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><forename type="middle">M</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Stern</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.04235</idno>
		<title level="m">Adafactor: Adaptive learning rates with sublinear memory cost</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Self-attention with relative position representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taku</forename><surname>Kudo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llu?s-Miquel</forename><surname>Mungu?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rothchild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maud</forename><surname>Texier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.10350</idno>
		<title level="m">Carbon emissions and large neural network training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
