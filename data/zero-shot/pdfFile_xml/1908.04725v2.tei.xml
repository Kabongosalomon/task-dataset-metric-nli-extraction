<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning elementary structures for 3D shape generation and matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Deprelle</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIGM (UMR 8049)</orgName>
								<orgName type="institution">UPE</orgName>
								<address>
									<addrLine>?cole des Ponts</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Groueix</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIGM (UMR 8049)</orgName>
								<orgName type="institution">UPE</orgName>
								<address>
									<addrLine>?cole des Ponts</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Aubry</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">LIGM (UMR 8049)</orgName>
								<orgName type="institution">UPE</orgName>
								<address>
									<addrLine>?cole des Ponts</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning elementary structures for 3D shape generation and matching</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose to represent shapes as the deformation and combination of learnable elementary 3D structures, which are primitives resulting from training over a collection of shapes. We demonstrate that the learned elementary 3D structures lead to clear improvements in 3D shape generation and matching. More precisely, we present two complementary approaches for learning elementary structures: (i) patch deformation learning and (ii) point translation learning. Both approaches can be extended to abstract structures of higher dimensions for improved results. We evaluate our method on two tasks: reconstructing ShapeNet objects and estimating dense correspondences between human scans (FAUST inter challenge). We show 16% improvement over surface deformation approaches for shape reconstruction and outperform FAUST inter challenge state of the art by 6%.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Current surface-parametric approaches for generating a surface or aligning two surfaces, such as AtlasNet <ref type="bibr" target="#b10">[11]</ref> and 3D-CODED <ref type="bibr" target="#b9">[10]</ref>, rely on alignment of one or more shape primitives to a target shape. The shape primitives can be a set of patches or a sphere, as in AtlasNet, or a human template shape, as in 3D-CODED. These approaches could easily be extended to other parametric shapes, such as blocks <ref type="bibr" target="#b21">[22]</ref>, generalized cylinders <ref type="bibr" target="#b3">[4]</ref>, or modern shape abstractions <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b27">28]</ref>. While surfaceparametric approaches have achieved state-of-the-art results for (single-view) shape reconstruction <ref type="bibr" target="#b10">[11]</ref> and 3D shape correspondences <ref type="bibr" target="#b9">[10]</ref>, they rely on hand-chosen parametric shape primitives tuned for the target shape collection and task. In this paper, we ask -what is the right set of primitives to represent a collection of diverse shapes?</p><p>To address this question, we seek to go beyond manually choosing shape primitives and automatically learn what we call "learnable elementary structures" from a shape collection, which can be used for shape reconstruction and matching. The ability to automatically learn elementary structures allows the shape generator to find a better set of primitives for a shape collection and target task. We find that learned elementary structures correspond to recurrent parts among 3D objects. For example, in <ref type="figure" target="#fig_0">Figure 1</ref>, we show automatically learned elementary structures roughly corresponding to the tail, wing, and reactor of an airplane. Moreover, we find that learning the elementary structures leads to an improvement in shape reconstruction and correspondence accuracy.</p><p>We explore two approaches for learning elementary structures -patch deformation learning and point translation learning. For patch deformation learning, similar to AtlasNet <ref type="bibr" target="#b10">[11]</ref>, we start from a surface element, such as a 2D square, and deform it into the learned structure using a multi-layer perceptron <ref type="bibr" target="#b22">[23]</ref>. This approach has the advantage that the learned elementary structures are continuous surfaces.</p><p>Its key difference with respect to AtlasNet is that the deformations, and thus the elementary structures, are common to all shapes. For point translation learning, starting from a fixed set of points, we optimize their position to reconstruct the target objects. The drawback of this approach is that it does not produce a continuous surface -only a finite set of points. However, this approach is more flexible since it can, for example, change the topology of the structure. We show how to deform and combine our learnable elementary structures to explain a given 3D shape. At inference, given the learned elementary structures, we learn to position the structures by adjustment -a linear (projective) transformation will lead to maximum interpretability, while a complex transformation parameterized by a multi-layer perceptron will make our approaches generalizations of prior shape reconstruction methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref> using optimized instead of manually defined templates. Moreover, such representation allows for disentanglement of the structure's shape and pose. We include structure learning in a deep architecture that unifies shape abstraction and deep surface deformation approaches.</p><p>We demonstrate that our architecture leads to improvements for 3D shape generation and matching -16% relative improvement over AtlasNet for generic object shape reconstruction and 6% over 3D-CODED for human shape matching on Faust <ref type="bibr" target="#b4">[5]</ref>, achieving state of the art for the latter task. Our code is available on our project webpage 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Primitive fitting is a classic topic in computer vision <ref type="bibr" target="#b21">[22]</ref>, with a large number of methods targeting parsimonious shape approximations, such as generalized cylinders <ref type="bibr" target="#b3">[4]</ref> and geons <ref type="bibr" target="#b2">[3]</ref>. Efficient fitting of these primitives attracted a lot of research efforts <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25]</ref>. Since these methods analyze shapes independently, they are not expected to use the primitives consistently across different objects, which makes the result unsuitable for discovering a common structure in a collection of shapes, performing consistent segmentation, or correspondence estimation. To address these limitations some methods optimize for consistent primitive fitting over the entire shape collection <ref type="bibr" target="#b14">[15]</ref>, or aim to discover a consistent set of parts <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b26">27]</ref>. The resulting optimization problems are usually non-convex, and thus existing solutions tend to be slow, require heuristics, and are prone to being stuck in local optima.</p><p>Learning-based techniques offer a promising alternative to hand-crafted heuristics. Zhu et al. <ref type="bibr" target="#b30">[31]</ref> use a Recurent Neural Network supervised by a traditional heuristic-based algorithm for cuboid fitting. Tulsiani et al. <ref type="bibr" target="#b27">[28]</ref> use reconstruction loss to predict parameters of the cuboids that approximate an input shape, and thus do not require any direct supervision. Several recent techniques, concurrent to our work, extend this approach by using more complex primitives that can better approximate the surface, such as anisotropic 3D Gaussians <ref type="bibr" target="#b7">[8]</ref>, categorie specifique morphable model <ref type="bibr" target="#b13">[14]</ref> or superquadrics <ref type="bibr" target="#b19">[20]</ref>. All of these techniques use a collection of simple hand-picked parametric primitives. In contrast, we propose to learn a set of deformable primitives that best approximate a collection of shapes.</p><p>One can further improve reconstruction by fitting a diverse set of primitives <ref type="bibr" target="#b16">[17]</ref> or constructive solid geometry graphs <ref type="bibr" target="#b25">[26]</ref>. These methods, however, usually do not produce consistent fitting across different shapes, and thus cannot be used to discover common shape structures or inter-shape relationships. On the other side of the spectrum, instead of simple primitives, some techniques fit deformable mesh models <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b31">32]</ref>. While they can capture complex structures, these techniques are also prone to being stuck in local optima, due to large number of degrees of freedom (e.g., mesh vertex coordinates).</p><p>Neural network architectures have been used to facilitate the mesh fitting <ref type="bibr" target="#b9">[10]</ref>, learning to predict the deformation of a template to reconstruct unstructured input point cloud. This approach is sensitive to the choice of the template. We demonstrate that our method improves the quality of the fitting by learning the structure of the reference shape. Neural mesh fitting has been also employed for geometrically and topologically diverse datasets that do not have a natural template. In these cases, meshed planes or spheres can be deformed into complex 3D structures <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">30]</ref>. We extend this line of work by proposing a technique for learning the base shapes that are further used to approximate the shapes in the collection. Learning these elementary structures enables us to more accurately and consistently reconstruct the shapes in the collection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head><p>We aim to learn shared elementary structures to reconstruct a set of 3D shapes. We visualize an overview of our approach in <ref type="figure" target="#fig_1">Figure 2</ref>. We formulate two ways to learn elementary structures -via patch deformation learning and point translation learning modules. The elementary structures are learned over the entire training set and do not depend on the input during testing. At test time, the elementary structures are deformed by adjustment modules to create the output 3D shape. These modules take as inputs features computed from the input via an encoder network and the coordinates of the elementary structure points and output the 3D coordinates of the deformed primitives.</p><p>For the task of 3D shape reconstruction, we assume that we are given a training set Z of target shapes Z ? Z. Our goal is to reconstruct the target shapes using a set of K learned elementary structures E 1 , . . . , E K , which are deformed via shape-dependent adjustment modules p 1 , . . . , p K . We represent each shape by a feature vector f (Z) computed by a point set encoder f (defined later in this section). Each adjustment module p k takes as inputs the coordinates of a point in the associated elementary structure e ? E k and the feature vector of the target shape f (Z) and outputs 3D coordinates of the corresponding point. The output shape O = p(Z) can thus be written as the union over learned and adjusted elementary structures,</p><formula xml:id="formula_0">O = p(Z) = K k=1 e?E k p k (e, f (Z)).<label>(1)</label></formula><p>If the elementary structures were unit squares or a unit sphere, then this equation would describe exactly the AtlasNet <ref type="bibr" target="#b10">[11]</ref> model. On the other hand, the 3D-CODED model <ref type="bibr" target="#b9">[10]</ref> uses an instance of Z as a single elementary structure. Generalizing these approaches, our goal is to automatically learn the elementary structures E k over a shape collection. The intuition behind our approach is that if the elementary structures E k have useful shapes to reconstruct the target, the adjustment p k should be easier to learn and more interpretable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Learnable elementary structures</head><p>For each k ? {1, . . . , K}, we start from an initial surface S k on which we sample N points to obtain an initial point cloud S k . We then pass each sampled point s k,i ? S k for i ? {1, . . . , N } through elementary structure learning modules ? k .</p><p>We consider two types of elementary structure learning module ? k .</p><p>The first type, patch deformation learning module, learns a continuous mapping d k to obtain deformed points e k,i = d k (s k,i ) starting from sampled point s k,i . The intuition behind the deformation module is that elementary structures E k should be surface elements, and can thus be deduced from the transformation of the original surfaces S k . Alternatively, we consider a point translation learning module which translates independently each of the points s k,i by a learned vector t k,i , e k,i = t k,i + s k,i . This module thus allows the network to update independently the position of each point on the surface. The result of either module results in a set of elementary structure points e k,i = ? k (s k,i ), and we write the elementary structure E k as the union of the independently deformed or translated points s k,i ? S k .</p><p>In Section 4 we will show that different choices here can be desirable depending on the application domain.</p><p>Dimensionality of the elementary structures. While it is natural to consider elementary structures as sets of 3D points, we can extend the idea to other dimensions. We experimented with 2D, 3D, and 10D elementary structures and show that while they are less interpretable, higher-dimensional structures lead to better shape reconstruction results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture details</head><p>The following describes more details of our final network.</p><p>Shape encoder. We represent the input shape as a point cloud, and we use as shape encoder a simplified version of the PointNet network <ref type="bibr" target="#b20">[21]</ref> used in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. We represent each 3D point of the input shape as a 1024 dimensional vector using a multi-layer perceptron with 3 hidden layers of 64, 128 and 1024 neurons and ReLU activations. We then apply max-pooling over all point features followed by a linear layer, producing a global shape feature used as input to the adjustment modules.</p><p>Patch deformation learning module. The patch deformation learning modules are continuousspace deformations that we learn as multi-layer perceptrons with 3 hidden layers of 128, 128 and 3 neurons and ReLU activations. This module takes as input coordinates of points in the initial structures and can compute not only a set of points <ref type="bibr" target="#b10">[11]</ref> but the full image of a surface. If this module is used, we can densely sample points on the generated surface.</p><p>Point translation learning module. The point translation learning modules learn a translation for each of the N points of the associated initial structure. While this step gives more flexibility than generating points through the patch deformation learning module, it can only be applied for a fixed number of points, similar to point-based shape generation <ref type="bibr" target="#b6">[7]</ref>.</p><p>Adjustment module. The goal of the adjustment modules p k is to reconstruct the input shape by positioning each elementary structure. The intuition is that this adjustment should be relatively simple. However, we can expect the quality of the reconstruction to increase using more complex adjustment modules. In this paper, we consider two cases:</p><p>? Linear adjustment: each adjustment module applies an affine transformation to the corresponding elementary structure. The parameters of this transformation are predicted by a multi-layer perceptron that takes as input the point cloud feature vector generated by the encoder. We use three hidden MLP layers (512, 512, 12), ReLU activation, BatchNorm layers and a hyperbolic tangent at the last layer for this module. ? MLP adjustment: each adjustment module uses a multi-layer perceptron (MLP) that takes as inputs the concatenation of the coordinates of a point from the associated elementary structure and the shape feature predicted by the shape encoder and outputs 3D coordinates. We use the same architecture as <ref type="bibr" target="#b10">[11]</ref> for this network to obtain comparable results.  . AtlasNet uses 10 patch primitives, which is the same as our approach, without the learned elementary structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Losses and training</head><p>We now discuss two scenarios in which we tested our approach.</p><p>Training with correspondences. In this scenario, we assume point correspondences across all training examples and a common template that we can use as an initial structure for all shapes. More precisely, we assume that each training shape Z is represented as an ordered set of N 3D points z 1 , . . . , z N in consistent locations on all shapes. Since all shapes are in correspondence, we consider a single elementary structure S 1 (K = 1) and N sampled points on the shape s 1,1 , . . . , s 1,N . We then train our network to minimize the following squared loss between sampled points z i on each training shape to reconstructed points starting from sampled template points s 1,i :</p><formula xml:id="formula_1">L sup (?) = Z?Z N i=1 z i ? p 1 (? 1 (s 1,i ), f (Z)) 2<label>(2)</label></formula><p>where ? are the parameters of the networks. Note that at inference, we do not need to know the correspondences of the points in the test shape, since they are processed by the point set encoder which is invariant to the order of the points. Instead, the points in the reconstruction shapes will be in correspondence with the elementary structure and by extension with each other. We use this property to predict correspondences between test shapes, following the pipeline of <ref type="bibr" target="#b9">[10]</ref>. Learning the elementary structures is the difference between our approach and 3D-CODED <ref type="bibr" target="#b9">[10]</ref> in this scenario, which leads to improved reconstruction and correspondence accuracy.</p><p>Training without correspondences. We are also able to train our system when no correspondence supervision is available during training. In this case, there are many options for our choice of elementary structures. To be comparable with AtlasNet <ref type="bibr" target="#b10">[11]</ref>, we will assume we have K elementary structures and that each initial structure S k is a unit 2D square patch. For a given training shape Z, we compute the output shape O = p(Z) according to <ref type="bibr">Equation 1</ref>, and train our network's parameters to minimize the symmetric Chamfer distance <ref type="bibr" target="#b6">[7]</ref> between the point clouds p(Z) and Z.</p><formula xml:id="formula_2">L unsup (?) = Z?Z z?Z min k?{1,...,K}, i?{1,...,N } z ? p k (? k (s k,i ), f (Z)) 2 + Z?Z K k=1 N i=1 min z?Z z ? p k (? k (s k,i ), f (Z)) 2 (3)</formula><p>where ? are the parameters of the networks. In all of our experiments, we used K = 10.</p><p>Training details. We use the Adam optimizer with a learning rate of 0.001, a batch size of 16, and batch normalization layers. We train our method using input point clouds of 2500 points when correspondences are not available and 6800 points when correspondences are available. When training using only the deformation modules d k , we resample the initial surfaces S k at each training step to minimize over-fitting. At inference time, we sample a regular grid to allow easy mesh generation. We train our model on an NVIDIA 1080Ti GPU, with a 16 core Intel I7-7820X CPU (3.6GHz), 126GB For AtlasNet, the primitives are unit squares (so we do not show the elementary structures), and we visualize seven of them for the reconstruction (similarly to our method). Contrary to AtlasNet, our learned elementary structures have limited overlap in the reconstructions and better reconstructs the shapes.</p><p>RAM and SSD storage. Training takes about 48h for most experiments. Using the trained models from the official implementation on all categories, AtlasNet-25 performance is 1.56 (see also <ref type="table" target="#tab_1">Table  1</ref> in the Atlasnet paper). Using the released code to train AtlasNet-10 yields an error of 1.55. By adding a learning rate schedule to the original implementation we decreased this error to 1.45 and report this improved baseline (see <ref type="table" target="#tab_1">Table 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we show qualitative and quantitative results of our approach on the tasks of shape reconstruction and shape matching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Generic object shape reconstruction</head><p>We evaluate our approach on non-articulated generic 3D object shapes for the task of shape reconstruction. We use the training setting without correspondences described in Section 3.3.</p><p>Dataset, evaluation criteria, baseline. We evaluate on the ShapeNet Core dataset <ref type="bibr" target="#b5">[6]</ref>.  Chamfer results are multiplied by 10 ?3 . As a baseline, we compare against AtlasNet <ref type="bibr" target="#b10">[11]</ref> with ten unit-square primitives.</p><p>Single-category shape reconstruction. For our first experiment, we trained separate networks for the different ShapeNet Core categories. <ref type="figure" target="#fig_2">Figure 3a</ref> demonstrates learned 2D elementary structures using ten 2D unit squares as initial structures S k . In <ref type="figure" target="#fig_2">Figure 3b</ref>, we show shape reconstructions using our points translation learning module with MLP adjustments. Note the emergence of symmetric and topologically complex elementary structures.</p><p>Multi-class shape reconstruction. We now evaluate how well our method generalizes when trained on multiple categories, again using 2D elementary structures with point translation learning module and MLP-adjustements. As in single-category case, we observe discovery of non-trivial 2D elementary structures <ref type="figure" target="#fig_2">(Figure 3c</ref>) that are used to accurately reconstruct the shapes <ref type="figure" target="#fig_2">(Figure 3d)</ref>, with higher fidelity than the baseline performance of AtlasNet with ten 2D square patches <ref type="figure" target="#fig_2">(Figure 3e</ref>). Note how AtlasNet is less faithful to the topology of reconstructed shapes, incorrectly synthesizing geometry in hollow areas between the back and the seat. Our quantitative evaluation in <ref type="table" target="#tab_1">Table 1</ref> confirms that AtlasNet provides less accurate reconstructions than our method.</p><p>Linear vs MLP adjustment. We evaluated networks trained in both the single-and multi-category settings with linear and MLP adjustment modules using 3D learned elementary structures <ref type="table" target="#tab_1">(Table 1</ref> left, <ref type="figure" target="#fig_3">Figure 4</ref>). In all experimental setups, we observe that the MLP adjustment offers significant quantitative improvements over restricting the network to use linear transformations of the elementary structures. This result is expected as linear adjustment allows only limited adaptation of the elementary structures for each shape. Similar to shape abstraction methods <ref type="bibr" target="#b27">[28]</ref>, linear adjustment allows a better intuition of the shape generation process but limits the reconstruction accuracy. Using MLP adjustments, however, offers the network more flexibility to faithfully reconstruct the shapes.</p><p>Patch deformation vs points translation modules. We compare using patch deformation vs points translation modules in <ref type="table" target="#tab_1">Table 1</ref>. The patch deformation learning module does not allow topological changes and discontinuities in mapping, and produces inferior results in comparison to points translation learning. On the other hand, learning patch deformations enables the estimation of the entire deformation field. Thus one can warp an arbitrary number of points or even tessellate the domain and warp the entire mesh to generate the polygonal surface, which is more amenable to tasks such as rendering.</p><p>Higher-dimensional structures. We experimented with the dimensionality of the learned elementary structures. <ref type="figure" target="#fig_2">Figures 3a and 3c</ref> suggest that learned 2D elementary structures can capture interesting topological and symmetric aspects of the data -splitting, for instance, the patch into two identical parts for the legs of the chairs. note also the variable point density. Similarly, learned 3D elementary structures with linear adjustment and patch deformation learning modules are shown in <ref type="figure" target="#fig_0">Figure 1</ref> for the airplane category. Note that they roughly correspond to meaningful parts, such as wings, tail and reactor. <ref type="figure" target="#fig_3">Figure 4</ref> shows 3D elementary structures inferred from all ShapeNet categories, where the learned structures include non-trivial elements such as symmetric planes, sharp angles, and smooth parabolic surfaces. The learned structures are often correspond to consistent parts in the reconstructions. In our quantitative evaluations ( <ref type="table" target="#tab_1">Table 1</ref>, right) we found that the results improve with the dimensionality. The improvement diminishes for higher-dimensional spaces and are more difficult to visualize and interpret.</p><p>Consistency in template elementary structures. We experimented with several initializations of our elementary structures on the ShapeNet plane category. We used the point translation learning method and a single 3D elementary structure. In <ref type="figure" target="#fig_4">Figure 5</ref>, we show our results when initializing the elementary structure with either a plane 3D model (left) or a set of random 3D points sampled uniformly (right).</p><p>Chairs <ref type="table">Table  AtlasNet</ref> 1.64 4.70 Patch.</p><p>1.56 4.82 Point.</p><p>1.34 4.45 <ref type="figure">Figure 6</ref>: Category generalization. Chamfer distance for networks trained on chairs and tested on either the chairs or tables test sets.</p><p>Notice that the learned 3D elementary structure is similar regardless of the initial template shape.</p><p>Generalization to new categories. To test the generality of our approach, we trained on the chair category using ten 2D elementary structures and tested on the table category. As shown in <ref type="figure">Figure 6</ref>, point translation learning outperforms both patch deformation learning and AtlasNet. <ref type="figure">Figure 7</ref> shows qualitatively how the elementary structures are positioned on chairs and tables. Notice how the chair and table legs are reconstructed by the same elementary structures. Number of parameters. In <ref type="figure">Figure 8</ref>, we show the number of parameters for AtlasNet and our method. Our method has less than 1% additional parameters to learn the elementary structures -2.0 ? 10 6 and 2.5 ? 10 3 for patch deformation and point translation, respectively (orders of magnitude smaller than 1.8 ? 10 8 for the full network). During inference, our approach has the same complexity as AtlasNet as the elementary structures are precomputed and remain fixed for all shapes. We also tried training AtlasNet with six layers (6-layer AN), which significantly increases the number of parameters. Our approach with points translation learning outperforms all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Human shape reconstruction and matching</head><p>We now evaluate our approach on 3D human shapes for the tasks of shape reconstruction and matching using the training setup with correspondences described in Section 3.3. For this task, we use a single elementary structure for the human body using one of the meshes as the initial structure S 1 . Since we use a single elementary structure and the shapes are deformable, we only report results using the MLP-adjustment.</p><p>Datasets, evaluation criteria, baselines. We train our method using the SURREAL dataset <ref type="bibr" target="#b28">[29]</ref>, extended to include some additional bend-over poses as in 3D-CODED <ref type="bibr" target="#b9">[10]</ref>. We use 229,984 SURREAL meshes of humans in various poses for training and 224 SURREAL meshes to test reconstruction quality. To evaluate correspondences on real data, we use the FAUST benchmark <ref type="bibr" target="#b4">[5]</ref> consisting of 200 testing scans with ? 170k vertices from the "inter" challenge, including noise and holes which are not present in our training data. As a baseline, we compared against 3D-CODED <ref type="bibr" target="#b9">[10]</ref>.  <ref type="figure">Figure 9</ref>: Initial shape (left) and learned elementary structure (right) using the deformation or points learning modules. Notice the similarity between the elementary structure learned with the different approaches.</p><p>SURREAL <ref type="bibr" target="#b28">[29]</ref> FAUST <ref type="bibr" target="#b4">[5]</ref> 3D-CODED  Results. <ref type="figure">Figure 9</ref> shows learned elementary structures using deformation or points translation learning and different initial surfaces. We observe that the learned templates are inflated, bent, and with their arm and legs in a similar pose, suggesting a reasonable amount of consistency in the properties of a desirable primitive shape for this task.</p><p>As before, we found that points translation learning provides the best reconstruction (see SURREAL column in <ref type="table" target="#tab_4">Table 2</ref>). Both of our approaches also provide lower reconstruction loss than 3D-CODED.</p><p>We used reconstruction to estimate correspondences by finding closest points on the deformed elementary structure as in 3D-CODED <ref type="bibr" target="#b9">[10]</ref>. We report correspondence error in the "FAUST" column in <ref type="table" target="#tab_4">Table 2</ref>. We observe that deformation learning provides better correspondences than points learning, also yielding state-of-the-art results and clear improvement over 3D-CODED. This result is not surprising because understanding the deformation field for the entire surface is more relevant for matching and correspondence problems. Elementary structure dimension. Similar to generic object reconstruction, we evaluate with 2D, 3D and 10D elementary structures ( <ref type="table" target="#tab_4">Table 2</ref>, right). Note that when using the patch deformation learning module we control the output size and therefore it is easy to map the input 3D template to higher-or lower-dimensional elementary structure. On the other hand the points translation learning module does not allow to change dimensionality of the input template. Hence, for 2D elementary structures we project the 3D template (front-facing human in a T-pose) to a front plane, and for 10D elementary structures we embed the 3D human into a hyper-cube, keeping higher dimensions as zero. The difference in performance is clearer for human reconstruction than for generic object reconstruction, which can be related both to the fact that humans are complex with articulations and that we use a single elementary structure for all human reconstructions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We have presented a method to take a collection of training shapes and learned common elementary structures that can be deformed and composed to consistently reconstruct arbitrary shapes. We learn consistent structures without explicit point supervision between shapes and we demonstrate that using our structures for reconstruction and correspondence tasks results in significant quantitative improvements. When trained on shape categories, these structures are often interpretable. Moreover, our deformation learning approach learns elementary structures as the deformation of continuous surfaces, resulting in output surfaces that can densely sampled and meshed at test time. Our approach opens up possibilities for other applications, such as shape morphing and scan completion.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) Input target shapes (b) Learned elementary structures (c) Our reconstructions Problem statement. We seek to automatically learn a set of primitives (called "learned elementary structures") for shape reconstruction and matching. (a) Input target shapes to reconstruct. (b) Learned elementary structures roughly corresponding to the tail, wing, and reactor of airplanes. (c) Our output reconstructions with learned elementary structures highlighted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Approach overview. At training time, we learn (a) translations t i or (b) deformations d i that transform points from the unit square S i into shared learned elementary structures. (c) At evaluation time, we transform each elementary structure E i to target shape Z using learned shape-dependent adjustment networks p i that produce points on the surface of the output shape O.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>(a) Category-specific 2D elementary structures (3 out of 10 structures) learned for chairs (left) and plane (righ).(b) Reconstructions using elementary structures with category-specific training. (c) 2D elementary structure learned from all categories (7 out of 10 structures are shown). (d) Our reconstructions using 2D elementary structures trained on all categories. (e) AtlasNet reconstruction using square patch primitives trained on all categories We visualize elementary structures using point learning and MLP adjustment modules. For all reconstruction results, we show in color the points corresponding to the visualized 2D primitives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Three (out of ten) learned 3D elementary structures learned by the point translation learning approach when training on all ShapeNet categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>3D elementary structure obtained with point learning when initializing the training from a template shape (left) or a random set of points (right). See text for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Elementary structures learned on chairs (left) used to reconstruct chairs and tables (right). Impact of number of parameters on reconstruction error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>ShapeNet reconstruction. We evaluate variants of our method for single-and multi-category reconstruction tasks. Left: Linear vs MLP adjustment, Patch Deformation vs Points Translation with 3D elementary structures. Right: different template dimensionality and deformation vs points learning modules in the multi-category setup with MLP-adjustement. We report Chamfer distance (multiplied by 10 ?3 )</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Human correspondences and reconstruction. We evaluate different variants of our method (with deformation vs points translation learning and different template dimensionality) for surface reconstruction (SURREAL column) and matching (FAUST column). We report Chamfer loss for the former and correspondence error for the latter (measured by the distance between corresponding points). Results in the left table are with 3D elementary structures, and the only difference with the 3D-CODED baseline is thus the template/elementary structure learning. The table on the right shows results with elementary structures of different dimensions.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">http://imagine.enpc.fr/ deprellt/atlasnet2</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was partly supported by ANR project EnHerit ANR-17-CE23-0008, Labex B?zout, and gifts from Adobe to ?cole des Ponts.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Articulated body deformation from range scan data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Popovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The space of human body shapes: reconstruction and parameterization from range scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Popovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Recognition-by-components: a theory of human image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Biederman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">115</biblScope>
			<date type="published" when="1987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual perception by computer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Binford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference of Systems and Control</title>
		<imprint>
			<date type="published" when="1971" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Faust: Dataset and evaluation for 3d mesh registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<idno>abs/1512.03012</idno>
		<title level="m">Shapenet: An information-rich 3d model repository. CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Learning shape templates with structured implicit functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Genova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vlasic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">A</forename><surname>Funkhouser</surname></persName>
		</author>
		<idno>abs/1904.06447</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning Consistent Segmentation of 3D Models. Computers and Graphics (Shape Modeling International)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Golovinskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">3d-coded : 3d correspondences by deep deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">AtlasNet: A Papier-M?ch? Approach to Learning 3D Surface Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Groueix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Joint-Shape Segmentation with Linear Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH Asia)</title>
		<meeting>SIGGRAPH Asia)</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A survey of simple geometric primitives detection methods for captured 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Zepeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Boubekeur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning category-specific mesh reconstruction from image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ECCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Partbased Templates from Large Collections of 3D Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Diverdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Graphics (Proc. of SIGGRAPH)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Grass: Generative recursive autoencoders for shape structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">52</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubrovina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.08988</idno>
		<title level="m">Supervised fitting of geometric primitives to 3d point clouds</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Globfit: Consistently fitting primitives by discovering global relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chrysanthou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Mitra</surname></persName>
		</author>
		<idno>52:1- 52:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGGRAPH Asia</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Superquadrics revisited: Learning 3d shape parsing beyond cuboids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Paschalidou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Machine perception of three-dimensional solids</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Roberts</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1963" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The perceptron: a probabilistic model for information storage and organization in the brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">386</biblScope>
			<date type="published" when="1958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Completion and reconstruction with primitive shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Degener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum (Proc. of Eurographics)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="503" to="512" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient ransac for point-cloud shape detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer graphics forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="214" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Csgnet: Neural shape parser for constructive solid geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5515" to="5523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Unsupervised co-segmentation of a set of shapes via descriptor-space spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Van Kaick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kleiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions on Graphics (Proc. of SIGGRAPH Asia)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">126</biblScope>
			<biblScope unit="page" from="1" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning shape abstractions by assembling volumetric primitives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning from synthetic humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">9</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Foldingnet: Point cloud auto-encoder via deep grid deformation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d-prnn: Generating shape primitives with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="900" to="909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The stitched puppet: A graphical model of 3d human shape and pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
