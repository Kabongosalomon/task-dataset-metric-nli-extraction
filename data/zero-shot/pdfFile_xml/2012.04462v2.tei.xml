<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-Objective Interpolation Training for Robustness to Label Noise</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
							<email>diego.ortego@insight-centre.org</email>
							<affiliation key="aff0">
								<orgName type="department">Insight Centre for Data Analytics</orgName>
								<orgName type="institution">Dublin City University (DCU)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
							<email>eric.arazo@insight-centre.org</email>
							<affiliation key="aff0">
								<orgName type="department">Insight Centre for Data Analytics</orgName>
								<orgName type="institution">Dublin City University (DCU)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Insight Centre for Data Analytics</orgName>
								<orgName type="institution">Dublin City University (DCU)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noel</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Insight Centre for Data Analytics</orgName>
								<orgName type="institution">Dublin City University (DCU)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mcguinness</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Insight Centre for Data Analytics</orgName>
								<orgName type="institution">Dublin City University (DCU)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-Objective Interpolation Training for Robustness to Label Noise</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:42+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks trained with standard crossentropy loss memorize noisy labels, which degrades their performance. Most research to mitigate this memorization proposes new robust classification loss functions. Conversely, we propose a Multi-Objective Interpolation Training (MOIT) approach that jointly exploits contrastive learning and classification to mutually help each other and boost performance against label noise. We show that standard supervised contrastive learning degrades in the presence of label noise and propose an interpolation training strategy to mitigate this behavior. We further propose a novel label noise detection method that exploits the robust feature representations learned via contrastive learning to estimate per-sample soft-labels whose disagreements with the original labels accurately identify noisy samples. This detection allows treating noisy samples as unlabeled and training a classifier in a semi-supervised manner to prevent noise memorization and improve representation learning. We further propose MOIT+, a refinement of MOIT by fine-tuning on detected clean samples. Hyperparameter and ablation studies verify the key components of our method. Experiments on synthetic and real-world noise benchmarks demonstrate that MOIT/MOIT+ achieves state-of-the-art results. Code is available at https://git.io/JI40X.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Building a new dataset usually involves manually labeling every sample for the particular task at hand. This process is cumbersome and limits the creation of large datasets, which are usually necessary for training deep neural networks (DNNs) in order to achieve the required performance. Conversely, automatic data annotation based on web search and user tags <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b21">22]</ref> leverages the use of larger data collections at the expense of introducing some incorrect labels. This label noise degrades DNN performance <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b51">52]</ref> and this poses an interesting challenge that has recently gained a lot of interest in the research community <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b30">31]</ref>.</p><p>In image classification problems, label noise usually involves different noise distributions <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b54">55]</ref>. In-distribution noise types consist of samples with incorrect labels, but whose image content belongs to the dataset classes. When in-distribution noise is synthetically introduced, it usually follows either an asymmetric or symmetric random distribution. The former involves label flips to classes with some semantic meaning, e.g., a cat is flipped to a tiger, while the latter does not. Furthermore, web label noise types are usually dominated by out-of-distribution samples where the image content does not belong to the dataset classes. Recent studies show that all label noise types impact DNN performance, although performance degrades less with web noise <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Robustness to label noise is usually pursued by identifying noisy samples to: reduce their contribution in the loss <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b10">11]</ref>, correct their label <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28]</ref>, or abstain their classification <ref type="bibr" target="#b41">[42]</ref>. Other methods exploit interpolation training <ref type="bibr" target="#b52">[53]</ref>, regularizing label noise information in DNN weights <ref type="bibr" target="#b12">[13]</ref>, or small sets of correctly labeled data <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b54">55]</ref>. However, most previous methods rely exclusively on classification losses and little effort has being directed towards incorporating similarity learning frameworks <ref type="bibr" target="#b31">[32]</ref>, i.e. directly learning image representations rather than a class mapping <ref type="bibr" target="#b44">[45]</ref>.</p><p>Similarity learning frameworks are very popular in computer vision for a variety of applications including face recognition <ref type="bibr" target="#b43">[44]</ref>, fine-grained retrieval <ref type="bibr" target="#b36">[37]</ref>, or visual search <ref type="bibr" target="#b34">[35]</ref>. These methods learn representations for samples of the same class (positive samples) that lie closer in the feature space than those of samples from different classes (negative samples). Many traditional methods are based on sampling pairs or triplets to measure similarities <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">19]</ref>. However, supervised and unsupervised contrastive learning approaches that consider a high number of negatives have recently received significant attention due to their success in unsupervised learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27]</ref>. In the context of label noise, there are some attempts at training with simple similarity learning losses <ref type="bibr" target="#b44">[45]</ref>, but there are, to the best of our knowledge, no works exploring more recent contrastive learning losses <ref type="bibr" target="#b23">[24]</ref>. This paper proposes Multi-Objective Interpolation Training (MOIT), a framework to robustly learn in the presence of label noise by jointly exploiting synergies between contrastive and semi-supervised learning. The former introduces a regularization of the contrastive loss in <ref type="bibr" target="#b23">[24]</ref> to learn noise-robust representations that are key for accurately detecting noisy samples and, ultimately, for semi-supervised learning. The latter performs robust image classification and boosts performance. Our MOIT+ refinement further demonstrates that fine-tuning on the detected clean data can boost performance. MOIT/MOIT+ achieves state-of-theart results across a variety of datasets (CIFAR-10/100 <ref type="bibr" target="#b25">[26]</ref>, mini-ImageNet <ref type="bibr" target="#b21">[22]</ref>, and mini-WebVision <ref type="bibr" target="#b28">[29]</ref>) with both synthetic and real-world web label noise. Our main contributions are as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">A multi-objective interpolation training (MOIT) frame-</head><p>work where supervised contrastive learning and semisupervised learning help each other to robustly learn in the presence of both synthetic and web label noise under a single hyperparameter configuration.</p><p>2. An interpolated contrastive learning (ICL) loss that imposes linear relations both on the input and the contrastive loss to mitigate the performance degradation observed for the supervised contrastive learning loss in <ref type="bibr" target="#b23">[24]</ref> when training with label noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.</head><p>A novel label noise detection strategy that exploits the noise-robust feature representations provided by ICL to enable semi-supervised learning. This detection strategy performs a k-nearest neighbor search to infer persample label distributions whose agreements with the original labels identify correctly labeled samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>A fine-tuning strategy over detected clean data (MOIT+) that further boosts performance based on simple noise robust losses from the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>We briefly review recent image classification methods aiming at mitigating the effect of label noise on DNNs and recent contrastive learning methods.</p><p>Noise rate estimation Using a label noise transition matrix can mitigate label noise <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b47">48]</ref>. Patrini et al. <ref type="bibr" target="#b35">[36]</ref> proposed to correct the softmax classification using a transition matrix. The estimation of this matrix is, however, challenging. The authors in <ref type="bibr" target="#b47">[48]</ref> estimate the matrix by exploiting detected noisy samples that are similar to anchor points (i.e. highly reliable detected clean samples), while Hendrycks et al. <ref type="bibr" target="#b17">[18]</ref> directly use a set of clean samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noisy sample rejection</head><p>Rejecting or reducing the contribution to the optimization objective of noisy samples can increase model robustness <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b21">22]</ref>. Jiang et al. <ref type="bibr" target="#b22">[23]</ref> propose a teacher-student framework where the teacher estimates per-sample weights to guide the student training. Defining per-sample weights is also exploited in <ref type="bibr" target="#b9">[10]</ref> via an unsupervised estimation of data complexity. Nguyen et al. <ref type="bibr" target="#b32">[33]</ref> iteratively refine a clean set to train on by measuring label agreements with ensembled network predictions. Cross-network disagreements and updates <ref type="bibr" target="#b10">[11]</ref> lead to robust learning by training on selected clean data <ref type="bibr" target="#b50">[51]</ref>. Also, <ref type="bibr" target="#b45">[46]</ref> propose a loss for standard training together with crossnetwork consistency to select the clean samples to train on.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Noisy label correction</head><p>Correcting noisy labels to replace or balance their influence is widely used in previous works <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b29">30]</ref>. Bootstrapping loss <ref type="bibr" target="#b37">[38]</ref> correction approaches exploit a perceptual term that introduces reliance on a new label given by either the model prediction with fixed <ref type="bibr" target="#b37">[38]</ref> or dynamic <ref type="bibr" target="#b0">[1]</ref> importance, or class prototypes <ref type="bibr" target="#b11">[12]</ref>. More recently, Liu et al. <ref type="bibr" target="#b29">[30]</ref> introduced a perceptual term that maximizes the inner product between the model output and the targets without need for per-sample weights.</p><p>Noisy label rejection Rejecting the original labels by relabeling all samples with the network predictions <ref type="bibr" target="#b40">[41]</ref> or learned label distributions <ref type="bibr" target="#b49">[50]</ref> mitigates the effect of label noise. Recently, several approaches perform semi-supervised learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref> by treating detected noisy samples as unlabeled, thus rejecting their labels while exploiting the image content. Their main differences are in the noise detection mechanism: Ding et al. <ref type="bibr" target="#b8">[9]</ref> exploit high certainty agreements between the network predictions and labels, Kim et al. <ref type="bibr" target="#b24">[25]</ref> use high softmax probabilities after performing negative learning, and Ortego et al. <ref type="bibr" target="#b33">[34]</ref> look at the agreements between the original and relabeled labels using <ref type="bibr" target="#b40">[41]</ref>.</p><p>Other label noise methods Zhang et al. <ref type="bibr" target="#b52">[53]</ref> proposed an interpolation training strategy, mixup, that greatly prevents label noise memorization and has been adopted by many other methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b29">30]</ref>. Harutyunyan et al. <ref type="bibr" target="#b12">[13]</ref> quantify the amount of memorized information via the Shannon mutual information between neural network weights and the vector of all training labels, and encourage this to be small. Thulasidasan et al. <ref type="bibr" target="#b41">[42]</ref> add an abstention class to be predicted by noisy samples due to an abstention penalty introduced in the loss. Robust loss functions are studied in several works by jointly exploiting the benefits of mean absolute error and cross-entropy losses <ref type="bibr" target="#b53">[54]</ref>, a generalized version of mutual information insensitive to noise <ref type="bibr" target="#b48">[49]</ref>, or <ref type="bibr" target="#b30">[31]</ref> combinations of robust loss functions that mutually boost each other. Furthermore, several strategies to prevent memorization can be exploited together and DivideMix <ref type="bibr" target="#b27">[28]</ref> is a good example as it uses interpolation training, crossnetwork agreements, semi-supervised learning, and label correction.  <ref type="figure">Figure 1</ref>. Multi-Objective Interpolation Training (MOIT) for improved robustness to label noise. We interpolate samples and impose the same interpolation in the supervised contrastive learning loss L ICL and the semi-supervised classification loss L SSL that we jointly use during training. Label noise detection is performed at every epoch to enable semi-supervised learning and its result is used after training to fine-tune the encoder and classifier to further boost performance.</p><p>Contrastive representation learning Recent works in self-supervised learning <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b23">24]</ref> have demonstrated the potential of contrastive based similarity learning frameworks for representation learning. These methods maximize (minimize) similarities of positive (negative) pairs. Adequate data augmentation <ref type="bibr" target="#b42">[43]</ref>, large amounts of negative samples via large batch size <ref type="bibr" target="#b4">[5]</ref> or memory banks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b46">47]</ref>, and careful network architecture designs <ref type="bibr" target="#b5">[6]</ref> are usually important for better performance. Regarding the label noise scenario for image classification, no works explore the impact of incorrect labels on contrastive learning and only Wang et al. <ref type="bibr" target="#b44">[45]</ref> incorporate a simple similarity learning objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We target learning robust feature representations in the presence of label noise. In particular, we adopt the contrastive learning approach from <ref type="bibr" target="#b23">[24]</ref> and randomly sample N images to apply two random data augmentation operations to each, thus generating two data views. The resulting</p><formula xml:id="formula_0">training minibatch {(x i , y i )} 2N i=1</formula><p>of image-label pairs x i and y i consists of 2N images. Every image is mapped to a low-dimensional representation z i by learning an encoder network f ? and a projection network g ? with parameters ? and ?. In particular, an intermediate</p><formula xml:id="formula_1">embedding v i = f ? (x i )</formula><p>is generated and subsequently transformed into the repre- </p><formula xml:id="formula_2">sentation w i = g ? (v i ). Finally, z i = w i / w i 2 is the L 2 -normalized low-dimensional</formula><formula xml:id="formula_3">L i (z i , y i ) = 1 2N yi ? 1 2N j=1 1 i =j 1 yi=yj P i,j ,<label>(1)</label></formula><formula xml:id="formula_4">P i,j = ? log exp (z i ? z j /? ) 2N r=1 1 r =i exp (z i ? z r /? ) ,<label>(2)</label></formula><p>where P i,j denotes the j-th component of the temperature ? scaled softmax distribution of inner products z i ? z j of representations from the positive pair of samples x i and x j , which can be interpreted as a probability. P i,j is aggregated in Eq. 1 across all N yi samples x j in the minibatch sharing label with x i (y i = y j ) except for the self-contrast case (i = j), as defined by the indicator function 1 B ? {0, 1} that returns 1 when condition B is fulfilled and 0 otherwise. Minimizing L i implies adjusting f ? and g ? to pull together the feature representations z i and z j when they share the same label (y i = y j ), while pushing them apart when they do not. Also, the gradient analysis in <ref type="bibr" target="#b23">[24]</ref> reveals that Eq. 1 focuses on hard positives/negatives rather than easy ones. Note that having two data views implies that L i contains an unsupervised contribution equivalent to the NT-Xent loss <ref type="bibr" target="#b4">[5]</ref>.</p><p>In the presence of label noise, Eq. 1 incorrectly selects positive/negative samples, which degrades the feature representation z (see Tab. 2). To overcome this limitation and perform robust image classification under label noise conditions, we propose a Multi-Objective Interpolation Training (MOIT) framework that consists of: i) a regularization technique to prevent memorization when training with the supervised contrastive learning loss (Sec. 3.1), ii) a semi-supervised classification strategy based on a novel label noise detection strategy that exploits the noise-robust representation z to measure agreements with the original labels y and tag noisy samples as unlabeled (Sec. 3.2), and iii) a classifier refinement on clean data to boost classification performance (Sec. 3.3). <ref type="figure">Fig. 1</ref> shows an overview of MOIT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Interpolated Contrastive Learning</head><p>Interpolation training strategies have demonstrated excellent performance in classification frameworks <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b38">39]</ref>, and have further shown promising results to prevent label noise memorization <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b21">22</ref>]. Inspired by this success, we propose Interpolated Contrastive Learning (ICL), a novel adaptation of mixup data augmentation <ref type="bibr" target="#b52">[53]</ref> for supervised contrastive learning. ICL performs convex combinations of pairs of samples as</p><formula xml:id="formula_5">x i = ?x a + (1 ? ?) x b ,<label>(3)</label></formula><p>where ? ? [0, 1] ? Beta (?, ?) and x i denotes the training sample that combines two minibatch samples x a and x b , and imposes a linear relation in the contrastive loss:</p><formula xml:id="formula_6">L MIX i = ?L i (z i , y a ) + (1 ? ?) L i (z i , y b ) .<label>(4)</label></formula><p>The first and second terms in Eq. 4 consider, respectively, positive samples from the class y a (y b ) given by the first (second) sample x a (x b ). The selection of positive/negative samples involves considering a unique class for every mixed example. However, in most cases the input samples contain two classes as a result of the interpolation, where ? determines the dominant one. We assign this dominant class to every sample for positive/negative sampling. Intuitively, ICL makes it harder to pull together clean and noisy samples with the same label, as noisy samples are interpolated with either another clean sample that provides a clean pattern beneficial for training or another noisy sample that makes it harder to memorize the noisy pattern.</p><p>Memory bank The number of positives and negatives selected for contrastive learning depends on the minibatch size and the number of dataset classes. Therefore, unless a large minibatch is used during training, few positive and negative samples are selected, which negatively affects the training process <ref type="bibr" target="#b23">[24]</ref>. To address limitations in computing resources, we introduce the memory bank proposed in <ref type="bibr" target="#b46">[47]</ref> to perform robust similarity learning despite using relatively small minibatches compared to those in <ref type="bibr" target="#b23">[24]</ref>. In particular, we define a memory to store the last M feature representations from previous minibatches and define a loss term L MEM </p><formula xml:id="formula_7">L ICL = L MIX + L MEM .<label>(5)</label></formula><p>Sec. 4.3 shows the benefits of using ICL loss instead of the original loss in <ref type="bibr" target="#b23">[24]</ref> and Sec. 4.4 demonstrates the effect of the memory bank on the overall method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Semi-Supervised Classification</head><p>The goal is to predict a class c ? {1, . . . , C} by learning a second mapping h(v) = g ? (v) to the class space, where C is the number of classes. Na?vely training a classifier in the presence of label noise leads to noise memorization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b51">52]</ref>, which degrades the performance. Semi-supervised learning, where noisy labels are discarded, can mitigate this memorization <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref>. We, therefore, propose to jointly adopt semi-supervised learning with ICL. The former boosts the performance achievable by the latter, while the latter enables accurate label noise detection necessary for good performance in the former.</p><p>Label noise detection We propose to measure agreements between the feature representation z i (robust to label noise) and the original label y i to identify mislabeled samples. To quantify this agreement, we start by estimating a class probability distribution from the representation z i by doing a k-nearest neighbor (k-NN) search:</p><formula xml:id="formula_8">p (c | x i ) = 1 K K k=1 x k ?N i 1 y k =c ,<label>(6)</label></formula><p>where N i denotes the neighbourhood of K closest images to x i according to the feature representation z. Eq. 6 then counts the number of samples per class in the local neighborhood N i and normalizes the counts to estimate a probability distribution. This distribution can be interpreted as a softlabel that can be compared with the original label to identify potential disagreements, i.e. noisy samples. However, the labels y might be noisy, thus biasing the estimation of p. We, therefore, estimate a corrected distributionp using:</p><formula xml:id="formula_9">p (c | x i ) = 1 K K k=1 x k ?N i 1? k =c ,<label>(7)</label></formula><p>where we introduce corrected labels? that are estimated taking the dominant label in N i , i.e.? = arg max c p (c | x). Finally, the disagreement between the corrected distributionp (c | x i ) and the label noise distribution given by the original label y i is measured by the cross-entropy</p><formula xml:id="formula_10">d i = ?y T i log (p) ,<label>(8)</label></formula><p>where T denotes the transpose operation. The higher d i , the higher the disagreement between distributions and the more likely x i is a noisy sample. We select clean samples for each class c based on d i using:</p><formula xml:id="formula_11">D c = {(x i , y i ) : d i ? ? c } ,<label>(9)</label></formula><p>where ? c is a per-class threshold on d i , which is dynamically defined to ensure a balanced clean set across classes. To perform this balancing, we use the median of per-class agreements between the corrected label? i and the original label y i across all classes. Sec. 4.4 illustrates the importance of this balancing strategy as well as the corrected distributionp over p for achieving better performance. Note that a k-NN noise detection that resembles Eq. 6 has been recently proposed in <ref type="bibr" target="#b3">[4]</ref>. However, we differ in that we propose a corrected version in Eq. 7 that surpasses the straightforward k-NN of Eq. 6 (see <ref type="table">Tab.</ref> 3), we use k-NN during training, and always avoid using a trusted clean set.</p><p>Semi-supervised learning We learn the classifier by performing semi-supervised learning where samples in D are considered as labeled and the remaining samples as unlabeled. To leverage these unlabeled samples, pseudo-labeling <ref type="bibr" target="#b1">[2]</ref> based on interpolated samples is applied by defining the objective</p><formula xml:id="formula_12">L SSL i = ??? T a log (h i ) ? (1 ? ?)? T b log (h i ) ,<label>(10)</label></formula><p>where the pseudo-label? a (? b ) for x a (x b ) is estimated as</p><formula xml:id="formula_13">y a = y a , x a ? D c h a , x a / ? D c ,<label>(11)</label></formula><p>whereh a is the softmax prediction for image x a without data augmentation. The final Multi-Objective Interpolation Training (MOIT) optimizes the loss:</p><formula xml:id="formula_14">L MOIT = L ICL + L SSL .<label>(12)</label></formula><p>In summary, the proposed MOIT framework enables robust training in the presence of label noise by learning robust representations via contrastive learning that help in achieving successful noise detection that discards noisy labels and enables semi-supervised learning for classification. Note that the method needs to learn useful features before performing accurate noise detection; thus we start training with? = y, ?x in L SSL , i.e. a normal supervised training. We start doing semi-supervised learning once reasonable features to search for reliable nearest neighbors in Eq. 7 are learned and the clean sample detection is made reliable. We assume that good features are available soon after reducing the learning rate, given that there is little risk of overfitting noisy labels at earlier epochs when using a high learning rate, as often reported in the literature [41, 50, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Classification refinement</head><p>Supervised pre-training on relatively clean datasets such as ImageNet <ref type="bibr" target="#b7">[8]</ref> has proved to mitigate label noise memorization <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22]</ref>. We, therefore, refine our MOIT predictions by fine-tuning f ? and re-training g ? on our detected clean set D using a constant low learning rate. We name this fine-tuning stage MOIT+. We train using mixup <ref type="bibr" target="#b52">[53]</ref> and later introduce hard bootstrapping loss correction <ref type="bibr" target="#b37">[38]</ref> to deal with possible low amounts of label noise present in D, thus defining the following training objective:</p><formula xml:id="formula_15">L MOIT + i = ?? (?y a + (1 ? ?)? a ) T log (h i ) ? (1 ? ?) (?y b + (1 ? ?)? b ) T log (h i ) ,<label>(13)</label></formula><p>where ? is the mixing coefficient from <ref type="bibr" target="#b52">[53]</ref> as we are interpolating images as explained in Eq. 3, and ? is a weight to balance the contribution of the original labels (y a and y b ) or the network predictions (? a and? b ). This training objective is similar to that used in <ref type="bibr" target="#b0">[1]</ref>, but different in that we do not train from scratch using all data, or need to infer per sample ? weights. Instead, we set ? = 0.8 as done in <ref type="bibr" target="#b37">[38]</ref> to give more importance to the original labels, which is reasonable given that the training uses the detected clean data D. Note that? a = arg max cha (? b = arg max chb ) is the network prediction for x a (x b ) without data augmentation. As commented before, MOIT+ starts with a mixup training without bootstrapping (i.e. ? = 1.0) during the initial epochs to allow adequate re-training of g ? before trusting its predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We first run experiments on the standard benchmarks for synthetic noise in CIFAR-100 <ref type="bibr" target="#b25">[26]</ref> aiming at analyzing the different components of our method. We further perform comparative evaluations against related work using synthetic label noise in CIFAR-10/100, controlled web noise in mini-ImageNet <ref type="bibr" target="#b21">[22]</ref>, and the uncontrolled web noise from the WebVision dataset <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>The CIFAR-10/100 datasets <ref type="bibr" target="#b25">[26]</ref> contain 50K (10K) small resolution images for training (test). For hyperparameter and ablation studies, we keep 5K training samples for validation using their correct labels. However, to facilitate comparison with related work, we train with the full 50K samples and use the 10K test set for evaluation (reporting accuracy in the last epoch). For noise addition, we follow the criteria in <ref type="bibr" target="#b49">[50]</ref>: symmetric noise is introduced by randomly flipping the labels of a percentage of the training set to incorrect labels; asymmetric noise uses label flips to incorrect classes "truck ? automobile, bird ? airplane, deer ? horse, cat ? dog" in CIFAR-10, whereas in CIFAR-100 label flips are done circularly within the super-classes.</p><p>Jiang et al. <ref type="bibr" target="#b21">[22]</ref> propose to use mini-ImageNet and Stanford Cars to introduce both web and symmetric indistribution noise in a controlled manner with different noise ratios. We adopt the mini-ImageNet web noise dataset for evaluation in a real scenario with several ratios, which consists of 100 classes with 50K (5K) samples for training (validation). For further evaluation against web noise, we adopt the mini-WebVision dataset <ref type="bibr" target="#b27">[28]</ref> that uses the top-50 classes from the Google image subset of WebVision <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training details</head><p>We use a PreAct ResNet-18 (PRN-18) <ref type="bibr" target="#b15">[16]</ref> as encoder network in CIFAR following <ref type="bibr" target="#b0">[1]</ref>, while for mini-ImageNet we use the ResNet-18 (RN-18) from <ref type="bibr" target="#b19">[20]</ref> used in mini-ImageNet for few-shot learning. For mini-WebVision we use a standard  <ref type="bibr">30 20 20</ref> RN-18 <ref type="bibr" target="#b14">[15]</ref>. We do not evaluate using other frameworks in mini-ImageNet or WebVision <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref> due to limitations of our computing resources. We, conversely, re-run the official implementation of top-performing and recent methods <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref> in our framework. As projection head and classifier, we always use a linear layer that maps, respectively, to a feature vector z of 128 dimensions and to the class space. <ref type="table" target="#tab_1">Table 1</ref> presents the training details for MOIT and MOIT+. We interpolated input samples as proposed in <ref type="bibr" target="#b52">[53]</ref> with ? = 1 (i.e. ? is sampled from a uniform distribution), and apply standard strong data augmentations to achieve successful contrastive learning 1 in MOIT: random resized crops, horizontal flips, color jitter and gray scale transformations. For MOIT+ and all other methods, mixup as well as standard augmentations are used (CIFAR: random horizontal flips and random 4 pixel translations, mini-ImageNet and mini-WebVision: random resized crops and random horizontal flips). We double the epochs in MOIT+ for 80% noise in CIFAR-10/100 as there are few selected clean samples, which make epochs extremely short. We always use ? = 0.1 temperature scaling for contrastive learning and increase the memory size in mini-ImageNet and mini-WebVision to deal with reduced batch size. Note that MOIT+ finetunes the model in the last epoch when training MOIT.</p><p>In practice, the noise ratio and distribution are not usually known a-priori; we therefore use a common configuration for training our method (mixup ?, k-NN parameter K, loss function, D balancing criterion, ? for MOIT+), and only modify typical hyperparameters (batch size, memory and epochs). We use the official implementations of DivideMix (DMix) <ref type="bibr" target="#b27">[28]</ref> and ELR <ref type="bibr" target="#b29">[30]</ref>. However, DMix adopts specific configurations for different datasets and even for different 1 https://github.com/HobbitLong/SupContrast noise ratios and types in the same dataset. To perform as fair as possible a comparison without degrading DMix results, we select a single parametrization of DMix in every dataset based on the most repeated configuration in <ref type="bibr" target="#b27">[28]</ref>. This affects the CIFAR configuration (CIFAR-10: ? u = 0, CIFAR-100: ? u = 150) as mini-WebVision has a unique configuration that we also adopt for mini-ImageNet. We run DMix and ELR for the same number of epochs as our method respecting suggested learning rates and equip ELR with mixup for a fair comparison with DMix and our method that both use interpolation training. Note that ELR+ in <ref type="bibr" target="#b29">[30]</ref> uses mixup, but we do not use it for comparison as it involves using a second network and a weight averaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Supervised contrastive learning and label noise</head><p>We start by analyzing supervised contrastive learning behavior in the presence of label noise and how introducing interpolation training impacts the learned representations. We evaluate the quality of representations using a weighted k-NN (k = 200) evaluation typical in unsupervised learning <ref type="bibr" target="#b20">[21]</ref>. Tab. 2 reports this evaluation using the embedding z extracted after the projection head (model from the last training epoch) and the true labels in the training set. This experiments show that Supervised Contrastive Learning (SCL) <ref type="bibr" target="#b23">[24]</ref> performance degrades when there is label noise (the noise-free accuracy of 72.66 decreases). The proposed regularization using Interpolated Contrastive Learning (ICL) mitigates label noise drops and outperforms SCL in the noisefree case, validating the utility of imposing a interpolated behavior in the contrastive loss. Note that ICL and MOIT (joint ICL and semi-supervised classification) perform worse in the asymmetric case than in the symmetric case. This occurs due to the former having label flips that keep some semantic meaning (e.g. cat?dog), while the latter does not (e.g. cat?truck). Semantic noise is more informative during ICL, which leads to better performance and less room for semi-supervised learning improvement in MOIT compared to ICL. We train SCL and ICL using a memory bank for 350 epochs with initial learning rate of 0.1, divided by 10 at epochs 200 and 300. Note that contrastive learning frameworks tend to be very sensitive to hyperparameters <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref> (learning rate, temperature, data augmentation, etc.), a behavior that we also observed when training them alone in the presence of label noise. We experimentally found that averaging the contrastive losses of the minibatch </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Label noise detection analysis</head><p>We exploit the feature representation z by searching the closest K neighbors to estimate a corrected soft-labelp in Eq. 7 and measure agreements with the original labels y. Tab. 3 shows that using this corrected soft-labelp (bottom) rather than the soft-label p from Eq. 6 (top) results in better performance due to improved label noise detection: precision and recall forp are 90.83 and 87.84 compared to 80.20 and 84.43 for p. The method is also not very sensitive to the value of K once it is set to a high enough value. We adopt K = 250 for the remaining experiments. We further study the effect of balancing the clean set D (see Tab. 4). In particular, we experiment by balancing with the minimum (Min), maximum (Max), or median (used by our method) number of agreements between corrected? and original y labels across classes. The median consistently outperforms the others as it poses a better trade-off than the Min (Max), which restricts (extends) the samples to select in classes with many (few) agreements. Here the unbalanced criterion considers as clean all samples that satisfy? = y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Joint training ablation study</head><p>Tab. 5 illustrates the effect of removing key components of our method on classification accuracy. Removing semisupervised learning (SSL) involves training the classifier using mixup, which results in substantial degradation due to label noise memorization. Removing the memory (M) decreases performance due to the limited batch size used (128), which provides few positives/negatives for supervised contrastive learning with 100 classes. Not balancing (B) the clean set D to perform SSL also decreases performance. The criterion used to select clean samples without balancing was to select every sample x satisfying the agreement? = y as studied in Sec. 4.4. Regarding the classifier refinement done by MOIT+, re-training the classifier (r-t C) and avoiding the use of strong data augmentation impact performance. The former might prevent some slight memorization behavior in the classifier occurring during MOIT, while the latter avoids the strong data augmentation that harms classification accuracy but is required for successful contrastive learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Synthetic label noise evaluation</head><p>Tables 6 and 7 evaluate the performance of MOIT and MOIT+ in, respectively, CIFAR-10 and CIFAR-100 for different levels of symmetric and asymmetric noise and report average accuracy for each dataset to ease comparison. We compare against some relevant and recent methods from the literature <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref> and demonstrate that MOIT and MOIT+ achieve state-of-the-art results. We achieve especially robust results for asymmetric noise, which is more realistic than symmetric as label flips are done considering semantic similarities between classes. We run DMix (evaluation done without ensembling both networks) and ELR, while using the remaining results from <ref type="bibr" target="#b33">[34]</ref>, which used the same network architecture and label noise criterion. DMix <ref type="bibr" target="#b27">[28]</ref> and, especially, ELR outperform our method for some noise levels, but experience important drops at high noise levels, which penalize the average performance. We stress that our label noise criterion (also adopted in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b33">34]</ref>) considers 40% noise as 0.4 probability of flipping the label to an incorrect class, and not to any class as reported in the DMix and ELR papers <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b29">30]</ref>, which results in 40% being more challenging in our setup.  <ref type="table" target="#tab_6">Tables 8 and 9</ref> illustrate the superior performance of MOIT/MOIT+ when training in the presence of web label noise in mini-ImageNet <ref type="bibr" target="#b21">[22]</ref> and mini-WebVision <ref type="bibr" target="#b27">[28]</ref>. The results demonstrate that MOIT/MOIT+ are robust to web noise and that they do not need careful re-parametrization depending on the noise level or distribution to achieve stateof-the-art performance. The results in Tab. 8 further confirm that the improvements are consistent across noise levels.</p><p>It is interesting to observe that, although MOIT+ consistently outperforms MOIT, the improvements compared to CIFAR experiments tend to be smaller. We think that a plausible explanation is the dominance of out-of-distribution samples in web-noise, which makes label correction via semi-supervised learning less beneficial. Note that we run M, DMix, EReg, and MOIT for the same number of epochs (130) in both mini-ImageNet and mini-WebVision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper proposes Multi-Objective Interpolation Training (MOIT), an approach for image classification with deep neural networks that robustly learns in the presence of both synthetic and web label noise. The key idea of MOIT is to combine supervised contrastive learning and classification in such a way that they are both robust to label noise. Interpolated Contrastive Learning regularization enables learning label noise robust representations that are used to estimate a soft-label distribution whose agreement with the original label allows identification of correctly labeled samples. MOIT then treats the remaining samples as unlabeled and trains a label noise robust image classifier in a semi-supervised manner. We further propose MOIT+, a refinement of our model by fine-tuning the model while re-training the image classifier. We conduct experiments in CIFAR-10/100 with synthetic label noise and in mini-ImageNet and mini-WebVision with web noise to demonstrate that MOIT and MOIT+ achieve state-of-the-art results when training deep neural networks with different noise distributions and levels. Future work will explore instance-dependent label noise as well as how to simplify the contrastive learning framework by using class prototypes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>representation used to learn based on the per-sample loss:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>i similar to L MIX i in Eq. 4. While L MIX i is estimated contrasting the 2N minibatch samples across them, L MEM i contrasts the 2N samples with the M memory samples, thus extending the number of positive and negative samples. The final ICL loss then aggregates the average batch and memory losses:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Training details. We always train from scratch. LR: Learning rate. B: Bootstrapping.</figDesc><table><row><cell></cell><cell cols="3">CIFAR mini-ImageNet mini-WebVision</cell></row><row><cell>Resolution</cell><cell cols="2">32 ? 32 84 ? 84</cell><cell>224 ? 224</cell></row><row><cell>Batch size</cell><cell>128</cell><cell>64</cell><cell>64</cell></row><row><cell>Mem. size</cell><cell>20K</cell><cell>100K</cell><cell>50K</cell></row><row><cell>Network</cell><cell cols="2">PRN-18 RN-18</cell><cell>RN-18</cell></row><row><cell>Epochs</cell><cell>250</cell><cell>130</cell><cell>130</cell></row><row><cell>Optimizer</cell><cell cols="3">SGD, momentum 0.9, weight decay 10 ?4</cell></row><row><cell>Initial LR</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell></row><row><cell>LR decay</cell><cell cols="2">125, 200 80, 105</cell><cell>80, 105</cell></row><row><cell>Decay factor</cell><cell>?0.1</cell><cell>?0.1</cell><cell>?0.1</cell></row><row><cell>SSL epoch</cell><cell>130</cell><cell>85</cell><cell>85</cell></row><row><cell>Decay factor</cell><cell>?0.1</cell><cell>?0.1</cell><cell>?0.1</cell></row><row><cell cols="2">Epochs (MOIT+) 70</cell><cell>50</cell><cell>50</cell></row><row><cell>LR (MOIT+)</cell><cell></cell><cell cols="2">0.001 (not reduced)</cell></row><row><cell>B epoch (MOIT+)</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Weighted k-NN evaluation in CIFAR-100. 58.32 41.00 71.11 68.00 ICL 75.30 66.38 53.60 74.34 72.04 MOIT 75.76 67.42 55.58 74.86 72.60</figDesc><table><row><cell></cell><cell></cell><cell>Symmetric Asymmetric</cell></row><row><cell></cell><cell>0%</cell><cell>40% 80% 10% 40%</cell></row><row><cell>SCL</cell><cell>72.66</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Classification accuracy for different noise detection strategies and K values for 40% asymmetric noise in CIFAR-100. Acc. 59.42 61.74 64.84 66.10 67.18 67.42 67.46 67.68 67.14 66.94 k-NN (p) Acc. 62.28 65.30 68.58 70.56 71.16 71.22 71.24 71.42 70.98 70.80Table 4. Effect on classification accuracy of the balancing strategy for the clean set D in CIFAR-100. A: Asymmetric. S: Symmetric.</figDesc><table><row><cell></cell><cell>K</cell><cell>5</cell><cell>10</cell><cell>25</cell><cell>50</cell><cell>100</cell><cell>150</cell><cell>200</cell><cell>250</cell><cell>300</cell><cell>350</cell></row><row><cell></cell><cell cols="2">k-NN (p) Unbalanced Min</cell><cell>Max</cell><cell cols="2">Median</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A-40%</cell><cell>69.58</cell><cell cols="3">52.88 62.58 71.42</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>S-40%</cell><cell>66.28</cell><cell cols="3">63.26 66.12 66.58</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">L MIX and the memory L MEM helped convergence in SCL</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">and ICL and used it in this experiment. Adding a classi-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">fication objective, as done in the proposed MOIT method,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">stabilizes this behavior and achieves better representations</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">than SCL and ICL alone (see Tab. 2).</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 .</head><label>5</label><figDesc>Ablation study for MOIT and MOIT+ in CIFAR-100.</figDesc><table><row><cell>A:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>Performance in CIFAR-10 with symmetric and asymmetric noise. (*) Denotes that we have run the algorithm. 78.93 55.06 33.09 88.81 81.69 76.04 72.50 Mix [53] 95.96 84.76 66.07 20.38 93.30 83.26 77.74 74.50 DB [1] 79.18 93.82 92.26 15.53 89.58 92.20 91.20 79.11 DMI [49] 93.88 88.33 83.24 43.67 91.11 91.16 83.99 82.20 PCIL [50] 93.89 92.72 91.32 55.99 93.14 92.85 91.57 87.35 DRPL [34] 94.08 94.00 92.27 61.07 95.50 92.98 92.84 88.96 DMix* [28] 94.27 95.12 94.11 35.36 93.77 92.47 90.04 85.02 ELR* [30] 95.49 94.49 92.56 38.23 95.25 94.66 92.88 86.22 MOIT 95.17 92.88 90.55 70.53 93.50 93.19 92.27 89.73 MOIT+ 95.65 94.08 91.95 75.83 94.23 94.31 93.27 91.33 69.11 62.78 45.67 67.09 58.59 47.44 59.35 DMI [49] 74.44 58.82 53.22 20.30 68.15 54.15 46.20 53.61 PCIL [50] 77.75 74.93 68.49 25.41 76.05 59.29 48.26 61.45 DRPL [34] 71.84 71.16 72.37 52.95 72.03 69.30 65.69 67.91 DMix* [28] 67.41 71.39 70.83 49.52 69.53 68.28 50.99 63.99 ELR* [30] 78.01 75.90 72.89 36.83 77.08 74.61 71.25 69.51 MOIT 75.83 72.78 67.36 45.63 75.49 73.34 71.55 68.85 MOIT+ 77.07 75.89 70.88 51.36 77.43 75.13 74.04 71.69 Table 8. Performance evaluation on controlled web noise in mini-ImageNet. We run all methods. Best 61.18 57.76 52.88 38.32 Last 58.96 54.60 50.40 37.32 DMix [28] Best 57.80 55.86 55.44 41.12 Last 55.84 50.30 50.94 35.42 ELR [30] Best 63.12 61.48 57.32 41.68 Last 57.38 58.10 50.62 41.68 MOIT Best 67.18 64.82 61.76 46.40 Last 64.72 63.14 60.78 45.88 MOIT+ Best 68.28 64.98 62.36 47.80 Last 67.82 63.10 61.16 46.78 4.7. Web label noise evaluation</figDesc><table><row><cell></cell><cell></cell><cell>Symmetric</cell><cell cols="2">Asymmetric</cell><cell>Avg.</cell></row><row><cell></cell><cell>0%</cell><cell cols="3">20% 40% 80% 10% 30% 40%</cell></row><row><cell cols="6">CE 93.85 Table 7. Performance in CIFAR-100 with symmetric and asymmet-</cell></row><row><cell cols="5">ric noise. (*) Denotes that we have run the algorithm.</cell></row><row><cell></cell><cell></cell><cell>Symmetric</cell><cell cols="2">Asymmetric</cell><cell>Avg.</cell></row><row><cell></cell><cell>0%</cell><cell cols="3">20% 40% 80% 10% 30% 40%</cell></row><row><cell>CE</cell><cell cols="5">74.34 58.75 42.92 8.29 68.10 53.28 44.46 50.02</cell></row><row><cell>Mix [53]</cell><cell cols="5">77.90 66.40 52.20 13.21 72.40 57.63 48.07 55.40</cell></row><row><cell>DB [1]</cell><cell cols="2">64.79 0%</cell><cell>20%</cell><cell>40%</cell><cell>80%</cell></row><row><cell>Mix [53]</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 9 .</head><label>9</label><figDesc>Performance evaluation in mini-WebVision. We run all methods.Mix<ref type="bibr" target="#b52">[53]</ref> DMix<ref type="bibr" target="#b27">[28]</ref> ELR<ref type="bibr" target="#b29">[30]</ref> MOIT MOIT+</figDesc><table><row><cell>Best 74.96</cell><cell>76.08</cell><cell>73.00</cell><cell>78.36</cell><cell>78.76</cell></row><row><cell>Last 73.76</cell><cell>74.64</cell><cell>71.88</cell><cell>77.76</cell><cell>78.72</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This publication has emanated from research conducted with the financial support of Science Foundation Ireland (SFI) under grant number SFI/15/SIRG/3283 and SFI/12/RC/2289 P2.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised Label Noise Modeling and Loss Correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pseudo-Labeling and Confirmation Bias in Deep Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A Closer Look at Memorization in Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Arpit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jastrzebski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Kanwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep k-NN for Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved Baselines with Momentum Contrastive Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Lecun</forename><surname>Hadsell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Semi-Supervised Two-Stage Approach to Learning from Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">CurriculumNet: Weakly Supervised Learning from Large-Scale Web Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Self-Learning From Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improving Generalization by Controlling Label-Noise Information in Neural Network Weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Harutyunyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Reing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">V</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identity Mappings in Deep Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Using Pre-Training Can Improve Model Robustness and Uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mazeika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hoffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ailon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6622</idno>
		<title level="m">Deep metric learning using Triplet network</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gripon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pateux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03806</idno>
		<title level="m">Leveraging the Feature Distribution in Transfer-based Few-Shot Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Unsupervised Deep Learning by Neighbourhood Discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. on Mach. Learn. (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Beyond Synthetic Noise: Deep Learning on Controlled Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Mentor-Net: Learning Data-Driven Curriculum for Very Deep Neural Networks on Corrupted Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.11362</idno>
		<title level="m">Supervised Contrastive Learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">NLNL: Negative Learning for Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Contrastive Representation Learning: A Framework and Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Le-Khac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Healy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Smeaton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DivideMix: Learning with Noisy Labels as Semi-supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02862</idno>
		<title level="m">Vision Database: Visual Learning and Understanding from Web Data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Early-Learning Regularization Prevents Memorization of Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niles-Weed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fernandez-Granda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Normalized Loss Functions for Deep Learning with Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A Metric Learning Reality Check</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Musgrave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-N</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SELF: Learning to Filter Noisy Labels with Self-Ensembling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Mummadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P N</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H P</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beggel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Towards Robust Learning with Different Label Noise Distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Metric Learning With HORDE: High-Order Regularizer for Deep Embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Picard A. Histace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jacob</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Making Deep Neural Networks Robust to Label Noise: A Loss Correction Approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SoftTriple Loss: Deep Metric Learning Without Triplet Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Training deep neural networks on noisy labels with bootstrapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">S</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="middle">J</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">RICAP: Random Image Cropping and Patching Data Augmentation for Deep CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matsubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Uehara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Machine Learning (ACML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Joint Optimization Framework for Learning with Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tanaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ikami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Combating Label Noise in Deep Learning Using Abstention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thulasidasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chennupati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mohd-Yusof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">What Makes for Good Views for Contrastive Learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">CosFace: Large Margin Cosine Loss for Deep Face Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Iterative Learning With Open-Set Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Combating noisy labels by agreement: A joint training method with co-regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Cross-Batch Memory for Embedding Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Are Anchor Points Really Indispensable in Label-Noise Learning?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">L DMI: An Informationtheoretic Noise-robust Loss Function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Probabilistic End-To-End Noise Correction for Learning With Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">How does Disagreement Help Generalization against Label Corruption</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Understanding deep learning requires re-thinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">mixup: Beyond Empirical Risk Minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sabuncu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Distilling Effective Supervision From Severe Label Noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">O</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
