<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SAITS: SELF-ATTENTION-BASED IMPUTATION FOR TIME SERIES A PREPRINT</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-05-10">May 10, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Du</surname></persName>
							<email>wenjie.du@mail.concordia.ca</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>C?t?</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
							<email>yan.liu@concordia.ca</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Concordia University Montr?al</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Ciena Corporation Ottawa</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Concordia University Montr?al</orgName>
								<address>
									<country key="CA">Canada</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SAITS: SELF-ATTENTION-BASED IMPUTATION FOR TIME SERIES A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-05-10">May 10, 2022</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Time Series ? Missing Values ? Imputation Model ? Self-Attention ? Neural Network</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Missing data in time series is a pervasive problem that puts obstacles in the way of advanced analysis. A popular solution is imputation, where the fundamental challenge is to determine what values should be filled in. This paper proposes SAITS, a novel method based on the self-attention mechanism for missing value imputation in multivariate time series. Trained by a joint-optimization approach, SAITS learns missing values from a weighted combination of two diagonally-masked self-attention (DMSA) blocks. DMSA explicitly captures both the temporal dependencies and feature correlations between time steps, which improves imputation accuracy and training speed. Meanwhile, the weightedcombination design enables SAITS to dynamically assign weights to the learned representations from two DMSA blocks according to the attention map and the missingness information. Extensive experiments quantitatively and qualitatively demonstrate that SAITS outperforms the state-of-the-art methods on the time-series imputation task efficiently and reveal SAITS' potential to improve the learning performance of pattern recognition models on incomplete time-series data from the real world.</p><p>A PREPRINT -MAY 10, 2022    imputation models, which are not in the scope of this paper, can be referred to <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>. A more detailed description of time-series imputation models is presented in Section 2, including <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Missing values can be characterized into three types: (1). Missing completely at random (MCAR), missing values are independent of any other values; (2). Missing at random (MAR), missing values depend only on observed values; (3). Missing not at random (MNAR), missing values depend on both observed and unobserved values <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. This work focuses on the MCAR case, as is standard in most of literature in the field of imputation. We uniformly sample values to be missing independently and introduce them as artificial missingness to evaluate all imputation methods used in this paper.</p><p>The self-attention mechanism is now widely applied, whereas its application on time-series imputation is still limited. Previous SOTA time-series imputation models are mostly based on recurrent neural networks (RNN), such as <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31]</ref>. Among them, the methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref> are autoregressive models that are highly susceptible to compounding errors <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b30">31]</ref>. Although the work [31] is not autoregressive, the multi-resolution imputation algorithm it proposed is made up of a loop, which can greatly slow the imputation speed. The self-attention mechanism, which is non-autoregressive and can overcome RNNs' drawbacks of slow speed and memory constraints, can avoid compounding error and be helpful to achieve better imputation quality and faster speed. This paper proposes a novel model called SAITS (Self-Attention-based Imputation for Time Series) to learn missing values by a joint-optimization training approach of imputation and reconstruction. Particularly, our contributions in this work are summarized as the following:</p><p>I. We design a joint-optimization training approach of imputation and reconstruction for self-attention models to perform missing value imputation for multivariate time series. Transformer trained by this approach outperforms SOTA methods. II. We design a model called SAITS that consists of a weighted combination of two diagonally-masked selfattention (DMSA) blocks, which emancipates SAITS from RNN and enables it to capture temporal dependencies and feature correlations between time steps explicitly. III. We conduct adequate experiments and ablation studies on three real-world public datasets to quantitatively and qualitatively evaluate our methodology and justify its design. The experimental results do not only prove that SAITS achieves the new SOTA position for the imputation accuracy, but also show SAITS' potential of facilitating pattern recognition models to learn with partially-observed time series from the real world.</p><p>This paper starts by reviewing related work in Section 2, and introduces our joint-optimization training approach and the SAITS model in Section 3. Experiments and conclusions are presented in Section 4 and 5, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>We review the prior related work of time-series imputation in the following four categories:</p><p>RNN-based Che et al. <ref type="bibr" target="#b43">[43]</ref> propose GRU-D, a gated recurrent unit (GRU) variant, to handle missing data in time series classification problems. The concept of time decay on the last observation is firstly proposed by [43] and continues to be used in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. M-RNN [30] and BRITS [21] impute missing values according to hidden states from bidirectional RNN. However, M-RNN treats missing values as constants, while BRITS treats missing values as variables of the RNN graph. Furthermore, BRITS takes correlations among features into consideration while M-RNN does not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAN-based</head><p>Models in <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31]</ref> are also RNN-based. However, due to they adopt the generative adversarial network (GAN) structure, they are listed separately as  propose GRUI (GRU for Imputation) to model temporal information of incomplete time series. Both the generator and the discriminator in their GAN model are based on GRUI. Moreover, based on <ref type="bibr" target="#b31">[32],</ref> Luo et al. [33]  propose E 2 GAN, which is an end-to-end method, comparing to the method in [32] having two stages. E 2 GAN adopts an auto-encoder based on GRUI to form its generator to ease the difficulty of model training and improve imputation performance. Liu et al. [31]  propose a non-autoregressive model called NAOMI for spatiotemporal sequence imputation, which consists of a bidirectional encoder and a multiresolution decoder. NAOMI is further enhanced by adversarial training.</p><p>All the above RNN-based imputation models are susceptible to the recurrent network structure. They are time-consuming and have memory constraints, which make it hard for them to handle long-term dependency in time series, especially when the number of time steps in data samples is big. Apart from these disadvantages, most of these models are autoregressive that leads to the problem which is called compounding error in the field of time-series analysis <ref type="bibr" target="#b41">[42]</ref>. Although NAOMI alleviates this problem, its imputation algorithm consists of a loop, which can greatly slow the imputation speed together with its RNN structure.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multivariate time-series data is ubiquitous in many application domains, for instance, transportation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>, economics <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>, healthcare <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, and meteorology <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10]</ref>. State-of-the-art (SOTA) pattern recognition methods can well handle various time-series analysis tasks on complete datasets. However, due to all kinds of reasons, including failure of collection sensors, communication error, and unexpected malfunction, missing values are common to see in time series from the real-world environment. They impair the interpretability of data and pose challenges for advanced analysis and pattern recognition tasks, for instance, classification and clustering. To learn with such incomplete time series, thinking ahead of how to deal with missing parts before modeling is an inevitable step.</p><p>Traditional missing value processing methods fall into two categories. One is deletion, which removes samples or features that are partially observed. However, deletion makes data incomplete and can yield biased parameter estimates <ref type="bibr" target="#b10">[11]</ref>. The other one is data imputation that estimates missing data from observed values <ref type="bibr" target="#b11">[12]</ref>. There are two main advantages to imputation over deletion <ref type="bibr" target="#b12">[13]</ref>: <ref type="bibr" target="#b0">(1)</ref>. Deletion introduces bias, while correctly specified imputation estimates are unbiased <ref type="bibr" target="#b13">[14]</ref>; <ref type="bibr" target="#b1">(2)</ref>. Partially observed data may still be informative and helpful to the analysis. Nevertheless, the problem of imputation is what values should be filled in. Amounts of prior work are proposed to solve this problem with statistics and machine learning methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. However, most of them require strong assumptions on missing data <ref type="bibr" target="#b20">[21]</ref>, for example, linear regression <ref type="bibr" target="#b14">[15]</ref>, mean/median averaging and k-nearest neighbors <ref type="bibr" target="#b15">[16]</ref>. Such assumptions may introduce a strong bias that influences the outcome of the analysis. Recently, much literature utilizes deep learning to solve this imputation problem and achieves SOTA results. Non-time series VAE-based Inspired by GPPVAE <ref type="bibr" target="#b44">[44]</ref> and the non-time-series imputation model HI-VAE <ref type="bibr" target="#b26">[27]</ref>, Fortuin et al. <ref type="bibr" target="#b33">[34]</ref> propose GP-VAE, a variational auto-encoder (VAE) architecture for time series imputation with a Gaussian process (GP) prior in the latent space. The GP-prior is used to help embed the data into a smoother and more explainable representation. L-VAE <ref type="bibr" target="#b34">[35]</ref> uses an additive multi-output GP-prior to accommodate auxiliary covariate information other than time. To support sparse GP approximations based on inducing points and handle missing values in spatiotemporal datasets, Ashman et al. <ref type="bibr" target="#b35">[36]</ref> propose SGP-VAE.</p><p>The GAN-based and VAE-based models are all generative ones, and they are difficult to train <ref type="bibr" target="#b27">[28]</ref>. Particularly, GAN models suffer from the problems of non-convergence and mode collapse due to their loss formulation <ref type="bibr" target="#b45">[45]</ref>. VAE models tend to involve latent variables used in the sampling and imputation, while they often do not correspond to concrete structures or distributions of the data, and this can make it difficult to interpret the imputation process for further understanding <ref type="bibr" target="#b27">[28]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention-based</head><p>Ma et al. <ref type="bibr" target="#b36">[37]</ref> apply cross-dimensional self-attention (CDSA) jointly from three dimensions (time, location, and measurement) to impute missing values in geo-tagged data, namely spatiotemporal datasets. Bansal et al. <ref type="bibr" target="#b37">[38]</ref> propose DeepMVI for missing value imputation in multidimensional time-series data. Their model includes a Transformer with a convolutional window feature and a kernel regression. Shan et al. <ref type="bibr" target="#b38">[39]</ref> propose NRTSI, a time-series imputation approach treating time series as a set of (time, data) tuples. Such a design makes NRTSI applicable to irregularly-sampled time series. The method directly uses a Transformer encoder for modeling and achieves SOTA performance in their work. Such above prior literature explores applying self-attention in the field of time-series imputation. However, CDSA <ref type="bibr" target="#b36">[37]</ref> is specifically designed for spatiotemporal data rather than general time series, and both CDSA and DeepMVI <ref type="bibr" target="#b37">[38]</ref> are not open-source, which makes it hard for other researchers to reproduce their methods and results. Regarding NRTSI <ref type="bibr" target="#b38">[39]</ref>, its algorithm design consists of two nested loops, which weaken the advantage of self-attention that is parallelly computational. Even worse, such loops can lead NRTSI to slow processing. Related work of self-attention-based models for time-series imputation is not much. There are some non-time series imputation models based on self-attention, such as AimNet <ref type="bibr" target="#b27">[28]</ref> and MAIN <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>Our methodology is made up of two parts: <ref type="bibr" target="#b0">(1)</ref>. the joint-optimization training approach of imputation and reconstruction; <ref type="bibr" target="#b1">(2)</ref>. the SAITS model, a weighted combination of two DMSA blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Joint-optimization Training Approach</head><p>A general illustration of the joint-optimization approach is shown in <ref type="figure">Figure 1</ref>. We are going to first give the definition of multivariate time series bearing missing data, then introduce the two learning objectives in detail. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Missing Mask</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimated Data Incomplete Data</head><p>Imputed and Reconstructed Fill missing part with 0 Concatenate <ref type="figure">Figure 1</ref>: A graphical overview of the joint-optimization training approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Definition of Multivariate Time-Series with Missing Values</head><p>Given a collection of multivariate time series with T time steps and D dimensions, it is denoted as</p><formula xml:id="formula_0">X = {x 1 , x 2 , ..., x t , ..., x T } ? R T ?D , where the t-th step x t = {x 1 t , x 2 t , ..., x d t , .</formula><p>.., x D t } ? R 1?D and each value in it could be missing. Accordingly, X d t represents the d-th dimension variable of the t-th step in X. To represent the missing variables in X, the missing mask vector M ? R T ?D is introduced, where</p><formula xml:id="formula_1">M d t = 1 if X d t is observed 0 if X d t is missing</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Two Learning Tasks</head><p>To well train self-attention-based imputation models on the above defined multivariate time series with missing values, a joint-optimization training approach of imputation and reconstruction is designed. This approach consists of two learning tasks: Masked Imputation Task (MIT) and Observed Reconstruction Task (ORT). Correspondingly, the training loss is accumulated from two losses: the imputation loss of MIT and the reconstruction loss of ORT.</p><p>Before illustrating the joint-optimization approach in detail, it is necessary to discuss why we need a new training approach for self-attention-based imputation models. Here, for the straightforward visualization, BRITS <ref type="bibr" target="#b20">[21]</ref> and Transformer <ref type="bibr" target="#b49">[49]</ref> are taken as examples for explanation. The former stands for mainstream RNN-based imputation methods. The latter is a standard self-attention-based model. <ref type="figure">Figure 2</ref> is plotted to make comparisons and to further illustrate the effects.</p><p>A normal way to train an RNN for imputation consists of three main steps: <ref type="bibr" target="#b0">(1)</ref>. Input time-series feature vectors X together with missing mask M to alert the model that input data has observations and missing values; <ref type="bibr" target="#b1">(2)</ref>. Let the model reconstruct the observed part of the input time series and calculate the reconstruction error in each time step as the loss;</p><p>(3). Finally, utilize the reconstruction loss to update the model. This training method is ORT. ORT works well with RNN-based models, for instance, BRITS. However, different from RNN which is autoregressive, self-attention itself is non-autoregressive and processes all input data parallelly and globally. Thus, if only trained on ORT, Transformer can distinguish the observed part from X according to M , and consequently, it will focus only on minimizing the reconstruction error on the observed values. Taking a look at <ref type="figure">Figure 2</ref>(b), Transformer (ORT)'s reconstruction MAE is much smaller than BRITS'. However, in <ref type="figure">Figure 2</ref>(a), the imputation MAE of Transformer (ORT) goes up from the beginning and is much larger than BRITS'. Transformer (ORT) ignores the missing values because there is no penalty posed to it no matter what values it fills in. As a result, ORT can only ensure that Transformer gets well trained on observed values. In other words, there is no guarantee that Transformer (ORT) can predict missing values accurately. To solve this optimization problem, we make another learning task MIT to become this guarantee and bind it together with ORT. This is how the joint-optimization training approach comes. The details of two tasks are described as follows.  <ref type="figure">Figure 2</ref>: Imputation MAE and reconstruction MAE of the models in the validation stage. All models are trained on the same data. BRITS is trained with ORT, namely in the same way as the original paper <ref type="bibr" target="#b20">[21]</ref>. Transformer (ORT) is trained with only ORT as well, i.e. without MIT. Transformer (MIT) is trained with only MIT. Transformer (ORT+MIT) is trained with the joint-optimization approach, namely with both ORT and MIT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Task #1: Masked Imputation Task (MIT)</head><p>MIT is a prediction task on artificially-masked values, which explicitly forces the model to predict missing values accurately. In MIT, for every batch input into the model, some percentage (such as 20% in our work) of observed values gets artificially masked at random. These values are not visible to the model, namely missing to the model. After artificially masking, the actual input time series is denoted asX, and its corresponding missing mask vector isM . The output estimated time series bearing reconstructions and imputations is denoted asX. To distinguish artificially-missing values and originally-missing values, the indicating mask vector I is introduced. Math definitions ofM and I are:</p><formula xml:id="formula_2">M d t = 1 ifX d t is observed 0 ifX d t is missing , I d t = 1 ifX d t is artificially masked 0 otherwise</formula><p>After the model imputes all missing values, the imputation loss is the mean absolute error (MAE) calculated between the artificially missing values and their respective imputations. The calculation of MAE and MIT loss are defined in Eq. 1 and Eq. 2 below.</p><formula xml:id="formula_3">MAE (estimation, target, mask) = D d T t |(estimation ? target) mask| d t D d T t mask d t (1) L MIT = MAE X , X, I<label>(2)</label></formula><p>Note that learning tasks similar to MIT, which mask some objects and then predict them, are commonly used to train models in NLP (Natural Language Processing) field, for example, the Cloze task <ref type="bibr" target="#b46">[46]</ref>, and the Masked Language Modeling (MLM) used to pre-train BERT <ref type="bibr" target="#b47">[47]</ref>. MIT is inspired by MLM, but the differences are: <ref type="bibr" target="#b0">(1)</ref>. MLM predicts missing tokens (time steps), while MIT predicts missing values in time steps; <ref type="bibr" target="#b1">(2)</ref>. One disadvantage of MLM is that it causes pretrain-finetune discrepancy because masking symbols used during pretraining are absent from real data in finetuning <ref type="bibr" target="#b48">[48]</ref>. However, the original objective of imputation is to predict missing or masked values. Therefore, MIT does not cause such discrepancies.</p><p>Task #2: Observed Reconstruction Task (ORT) ORT is a reconstruction task on the observed values. It is widely applied in the training of imputation models for both time-series and non-time series <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>. After model processing, observed values in the output are different from their original values, and they are called reconstructions. In our work, the reconstruction loss is MAE calculated between the observed values and their respective reconstructions, defined in Eq. 3 as below.</p><formula xml:id="formula_4">L ORT = MAE X , X,M<label>(3)</label></formula><p>In our training approach, MIT and ORT are integral. MIT is utilized to force the model to predict missing values as accurately as possible, and ORT is leveraged to ensure that the model converges to the distribution of observed data. As shown in <ref type="figure">Figure 2</ref>, both the imputation MAE and reconstruction MAE of Transformer (ORT+MIT) drop steadily. A comparison between Transformer (MIT) and Transformer (MIT+ORT) tells that MIT makes the main contribution to decreasing the imputation MAE. Compared with Transformer (MIT+ORT), Transformer (MIT) has a slightly higher imputation MAE. This proves that ORT can help models further optimize performance on the imputation task. On the reconstruction MAE, Transformer (MIT) climes up because it is not required to converge on the observed data. Furthermore, in the aspect of the reconstruction MAE in <ref type="figure">Figure 2</ref>(b), Transformer (ORT+MIT) is slightly higher than Transformer (ORT) because the gradient of the reconstruction loss gets influenced by the imputation loss. This is another piece of evidence proving that our joint-optimization approach works.</p><p>It is worth mentioning that our training approach is designed not only for time-series self-attention models but can be applied to train other imputation models. As shown in <ref type="figure">Figure 1</ref>, the imputation model in the blue box is not specified and can be replaced with other models for training. Moreover, the data can be non-time-series. We also discuss applying the joint-optimization approach to train BRITS in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SAITS</head><p>As illustrated in <ref type="figure">Figure 3</ref>, SAITS is composed of two diagonally-masked self-attention (DMSA) blocks and a weighted combination.  <ref type="figure">Figure 3</ref>: The SAITS model architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Diagonally-Masked Self-Attention</head><p>The conventional self-attention mechanism is proposed by Vaswani et al. <ref type="bibr" target="#b49">[49]</ref> to solve the language translation task. Now it is widely applied in sequence modeling. A given sequence is mapped into a query vector Q of dimension d k , a key vector K of dimension d k and a value vector V of dimension d v . The scaled dot-product can effectively calculate attention scores (or the attention map) between Q and K. After that, a softmax function is applied to obtain attention weights. The final output is attention-weighted V . The whole process is as shown in Eq. 4 below:</p><formula xml:id="formula_5">SelfAttention (Q, K, V ) = Softmax QK T ? d k V<label>(4)</label></formula><p>To enhance SAITS' imputation ability, the diagonal masks are applied inside the self-attention. As formulated in Eq. 5 and 6, the diagonal entries of the attention map (? R T ?T ) are set as ?? (set as ?1 ? 10 9 in practice) so the diagonal attention weights approach 0 after the softmax function. <ref type="figure" target="#fig_2">Figure 4</ref> gives a vivid illustration of the DMSA mechanism.</p><formula xml:id="formula_6">[DiagMask (x)] (i, j) = ?? i = j x (i, j) i = j (5) DiagMaskedSelfAttention (Q, K, V ) = Softmax DiagMask QK T ? d k V = AV, where A is attention weights<label>(6)</label></formula><p>With these diagonal masks, input values at the t-th step can not see themselves and are prohibited from contributing to their own estimations. Consequently, their estimations depend only on the input values from other (T ? 1) time steps. Such a mechanism makes DMSA able to capture the temporal dependencies and feature correlations between time steps in the high dimensional space with only one attention operation. Subsequently, the diagonally-masked multi-head attention (DiagMaskedMHA) is formulated as:</p><formula xml:id="formula_7">DiagMaskedMHA (x) = Concat (head 1 , head 2 , ..., head i , ..., head h ) W O where head i = DiagMaskedSelfAttention xW Q i , xW K i , xW V i</formula><p>, h is the number of heads, <ref type="table" target="#tab_1">T2 T3 T4 T5   T1   T2   T3   T4</ref>  To prove the effectiveness of DMSA, an ablation study is performed in Section 4.5.1. Note that attention masks are widely applied in self-attention modeling, especially in NLP field, including the diagonal masks used here, for example <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b51">51]</ref>.</p><formula xml:id="formula_8">and parameters W Q i ? R dmodel ?d k , W K i ?R dmodel ?d k , W V i ? R dmodel?dv , and W O ? R hdv?dmodel (7) T1 T2 T3 T4 T5 T1 T2 T3 T4 T5 Q K V Output A 0 0 0 0 0 T1</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Positional Encoding and Feed-Forward Network</head><p>In Transformer, Vaswani et al. <ref type="bibr" target="#b49">[49]</ref> apply the positional encoding to make use of the sequence order because there is no notion of sequence order in the original Transformer architecture. Additionally, there is a fully-connected feed-forward network applied behind each attention layer. In SAITS, both the positional encoding and the feed-forward network are kept.</p><p>The positional encoding consists of sine and cosine functions, which is formulated as Eq. 8 below. Note that p is used to refer to the positional encoding in the following equations for brevity. The feed-forward network has two linear transformations with a ReLU activation function between them, as shown in Eq. 9:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PosEnc</head><formula xml:id="formula_9">FFN (x) = ReLU (xW 1 + b 1 ) W 2 + b 2 where W 1 ? R dmodel?dffn , b 1 ? R dffn , W 2 ? R dffn?dmodel , b 2 ? R dmodel (9) 3.2.3 The First DMSA Block e = Concat X ,M W e + b e + p (10) z = {FFN(DiagMaskedMHA (e))} N<label>(11)</label></formula><formula xml:id="formula_10">X 1 = zW z + b z<label>(12)</label></formula><formula xml:id="formula_11">X =M X + 1 ?M X 1<label>(13)</label></formula><p>In the first DMSA block, the actual input feature vectorX and its missing mask vectorM are concatenated as the input. Eq. 10 projects the input to d model dimensions and adds up with the positional encoding p to produce e. W e and b e are parameters (W e ? R 2D?dmodel , b e ? R dmodel ). Operation {} N in Eq. 11 means stacking N layers. Eq. 11 transfers e to z with N stacked layers of the diagonally-masked multi-head attention and the feed-forward network 2 . Eq. 12 reduces z from d model dimensions to D dimensions and producesX 1 (Learned Representation 1). Parameter W z ? R dmodel?D and b z ? R D . In Eq. 13, missing values inX are replaced with corresponding values inX 1 to obtain the completed feature vectorX with the observed part inX kept intact. Here, is the Hadamard product, also known as the element-wise product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">The Second DMSA Block</head><formula xml:id="formula_12">? = Concat X ,M W ? + b ? + p (14) ? = {FFN (DiagMaskedMHA (?))} N<label>(15)</label></formula><formula xml:id="formula_13">X 2 = ReLU (?W ? + b ? ) W ? + b ?<label>(16)</label></formula><p>The second DMSA block takes the outputX of the first DMSA block and continues learning. Similar to Eq. 10, Eq. 14 projects the concatenation ofX andM from D dimensions to d model dimensions and then adds the result together with p to generate ?. Parameter W ? ? R 2D?dmodel , b ? ? R dmodel . Eq. 15 performs N times of nested attention functions and feed-forward networks on ? and outputs ?. In Eq. 16, to obtainX 2 (Learned Representation 2), two linear projections are applied on ? with a ReLU activation in between, where parameter</p><formula xml:id="formula_14">W ? ? R dmodel?D , b ? ? R D , W ? ? R D?D , b ? ? R D .</formula><p>Empirically, a deeper structure can learn better a representation to capture more complicated correlations in time series. Here, in Eq. 16, we apply one more non-linear layer than Eq. 12 to build a deeper block. In practice, such an operation does help achieve a better imputation performance than applying a single linear projection. The same transformation is not applied to obtainX 1 in the first DMSA block because the learnable parameters in the following weighted combination can dynamically adjust the weights forX 1 andX 2 to form betterX 3 (Learned Representation 3). Moreover, we find that even applying the same transformation here to obtainX 1 does not help achieve better results than the current design. It validates the effectiveness of our weighted combination, described as below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">The Weighted Combination Block?</head><formula xml:id="formula_15">= 1 h h i A i (17) ? = Sigmoid Concat ? ,M W ? + b ?<label>(18)</label></formula><formula xml:id="formula_16">X 3 = (1 ? ?) X 1 + ? X 2 (19) X c =M X + 1 ?M X 3<label>(20)</label></formula><p>To obtain a better learned representationX 3 , the weighted combination block is designed to dynamically weighX 1 and X 2 according to temporal dependencies and missingness information.? (? R T ?T ) in Eq. 17 is averaged from attention weights A output by multi heads in the last layer of the second DMSA block. Eq. 18 takes averaged attention weights? and missing masksM as references to produce the combining weights ? (? (0, 1) T ?D ) with the learnable parameters W ? (? R (T +D)?D ) and b ? (? R D ). Eq. 19 combinesX 1 andX 2 by weights ? to formX 3 . Finally, in Eq. 20, missing values inX are replaced with corresponding values inX 3 to produce the complement vectorX c , i.e. the imputed data.</p><p>To further discuss the rationality of the weighted combination, an ablation experiment is performed in Section 4.5.2.</p><p>Moreover, the second DMSA block and the weighted combination block are added to extend the learning process of our model and to obtain better performance. We do not apply more than two DMSA blocks because the benefit brought is marginal. Experiments and analysis are conducted to prove our points here in Section 4.5.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.6">Loss Functions of Learning Objectives</head><formula xml:id="formula_17">L ORT = 1 3 MAE X 1 , X,M + MAE X 2 , X,M + MAE X 3 , X,M<label>(21)</label></formula><formula xml:id="formula_18">L MIT = MAE X c , X, I<label>(22)</label></formula><formula xml:id="formula_19">L = L ORT + ? L MIT<label>(23)</label></formula><p>There are two learning tasks in the model training: MIT and ORT. The imputation loss of MIT (L MIT ) and the reconstruction loss of ORT (L ORT ) are both calculated by the MAE loss function ( MAE ) defined in Eq. 1, which takes three inputs: estimation, target, and mask (all of them ? R T ?D ). It calculates MAE between values indicated by mask in estimation and target. target and mask of L ORT in Eq. 21 are the input feature vectorX and its missing mask vectorM . We letX 1 andX 2 directly participate in the composition ofX 3 . Therefore, here L ORT is accumulated from three learned representations:X 1 ,X 2 andX 3 . Such an accumulated loss can lead to faster convergence speed. To ensure L ORT not too large to dominate the direction of the gradient, it is reduced by a factor of three, i.e. averaged. Inputs estimation, target and mask of L MIT in Eq. 22 are the complement feature vectorX c , the original feature vector X without artificially-masked values, and the indicating mask vector I, respectively. At last, Eq. 23 adds L ORT and L MIT together by a weighted sum, where ? is the weighting coefficient that can be tuned. ? is fixed as 1 in our experiments. Our SAITS model is updated by minimizing the final loss L.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>For the sake of the reproducibility of our results, we make our work publicly available to the community. Our data preprocessing scripts, model implementations, as well as hyper-parameter search configurations, are all available in the GitHub repository https://github.com/WenjieDu/SAITS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>In order to benchmark the proposed SAITS model, the experiments are performed on three public real-world datasets from different domains: PhysioNet 2012 Mortality Prediction Challenge 3 <ref type="bibr" target="#b4">[5]</ref>, Beijing Multi-Site Air-Quality 4 <ref type="bibr" target="#b54">[54]</ref>, and Electricity Load Diagrams 5 <ref type="bibr" target="#b55">[55]</ref>.</p><p>The descriptions of three datasets used in this work and their preprocessing details are elaborated as below. General information of all datasets is listed in <ref type="table" target="#tab_1">Table 1</ref>. Note that standardization is applied in the preprocessing of all datasets. Beijing Multi-Site Air-Quality (Air-Quality) This air-quality dataset <ref type="bibr" target="#b54">[54]</ref> includes hourly air pollutants data from 12 monitoring sites in Beijing. Data is collected from 2013/03/01 to 2017/02/28 (48 months in total). For each monitoring site, there are 11 continuous time series variables measured (e.g. PM2.5, PM10, SO2). We aggregate variables from 12 sites together so this dataset has 132 features. There are a total of 1.6% missing values in this dataset. The test set takes data from the first 10 months (2013/03 -2013/12). The validation set contains data from the following 10 months (2014/01 -2014/10). The training set takes the left 28 months (2014/11 -2017-02). To generate time series data samples, we select every 24 hours data, i.e. every 24 consecutive steps, as one sample. Similar to dataset PhysioNet-2012, 10% observed values in the validation set and test set are eliminated and held out as ground-truth for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Electricity Load Diagrams (Electricity)</head><p>This is another widely-used public dataset from UCI <ref type="bibr" target="#b55">[55]</ref>. It contains electricity consumption data (in kWh) collected from 370 clients every 15 minutes and has no missing data. The period of this dataset is from 2011/01/01 to 2014/12/31 (48 months in total). Similar to processing Air-Quality, we use the first 10 months of data (2011/01 -2011/10) as the test set, the following 10 months of data (2011/11 -2012/08) as the validation set and the left (2012/09 -2014/12) as the training set. Every 100 consecutive steps are selected as a sample to generate time-series data for model training. Due to this dataset having no missing values, we vary artificial missing rate from 10% ? 90% to eliminate observed values in the training set, validation set, and test set. This can make the comparison between our method and other SOTA models more comprehensive. Artificial missing values in the validation and test set are held out for model evaluation. Experiment results of 10% missing values are displayed in <ref type="table" target="#tab_2">Table 2</ref>. Results of 20% ? 90% missing values are shown in <ref type="table" target="#tab_4">Table 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Methods</head><p>To obtain a thorough comparison, we compare our method with two naive imputation methods and five recent SOTA deep learning models: <ref type="bibr" target="#b0">(1)</ref>. Median: missing values are filled with corresponding median values from the training set; <ref type="bibr" target="#b1">(2)</ref>. Last: in each sample, missing values are filled by the last previous observations of given features, and 0 will be filled in if there is no previous observation, ; <ref type="bibr" target="#b2">(3)</ref>. GRUI-GAN 6 <ref type="bibr" target="#b31">[32]</ref>; <ref type="bibr" target="#b3">(4)</ref>. E 2 GAN 7 <ref type="bibr" target="#b32">[33]</ref>; <ref type="bibr" target="#b4">(5)</ref>. M-RNN 8 <ref type="bibr" target="#b29">[30]</ref>; <ref type="bibr" target="#b5">(6)</ref>. GP-VAE 9 <ref type="bibr" target="#b33">[34]</ref>; <ref type="bibr" target="#b6">(7)</ref>. BRITS 10 <ref type="bibr" target="#b20">[21]</ref>. The deep learning models have already been introduced in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Setup</head><p>Three metrics are utilized to evaluate the imputation performance of methods: MAE (Mean Absolute Error), RMSE (Root Mean Square Error), and MRE (Mean Relative Error). The math definitions of three evaluation metrics are presented below. Note that errors are only computed on the values indicated by mask in the input of the equations.</p><formula xml:id="formula_20">MAE (estimation, target, mask) = D d T t |(estimation ? target) mask| d t D d T t mask d t RMSE (estimation, target, mask) = D d T t ((estimation ? target) mask) D d T t |(estimation ? target) mask| d t D d T t |target mask| d t</formula><p>The batch size is fixed as 128, and the early stopping strategy is applied in the model training. Training of models is stopped after 30 epochs without any decrease of MAE. To permit a fair comparison between the models, the hyper-parameter searches are executed for every model on each dataset, except SAITS (base). For SAITS (base), we fix its hyper-parameters to form a base model and observe its performance. SAITS (base) is also applied in ablation 6 https://github.com/Luoyonghong/Multivariate-Time-Series-Imputation-with-Generative-Adversarial -Networks 7 https://github.com/Luoyonghong/E2EGAN 8 https://github.com/jsyoon0823/MRNN 9 https://github.com/ratschlab/GP-VAE 10 https://github.com/caow13/BRITS experiments in Section 4.5 as a baseline to make the comparisons more straightforward. Please consult Appendix A for further details of models' hyper-parameters. Transformer used in this paper only includes the encoder part because the imputation problem is not treated as a generative task in this work, therefore, the decoder part is in no need. All models are trained with the Adam optimizer <ref type="bibr" target="#b57">[57]</ref> on Nvidia Quadro RTX 5000 GPUs. Our models are implemented with PyTorch <ref type="bibr" target="#b58">[58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Results</head><p>The adequate experiments performed to benchmark the performance of SAITS are made up of three parts in this section. In Subsection 4.4.1, the baseline methods and the self-attention models are impartially compared on three datasets. To further discuss the influence of imputation quality on pattern recognition tasks, an experiment in Subsection 4.4.2 is performed on a downstream classification task. In addition, we experiment to compare SAITS with NRTSI, another SOTA self-attention-based imputation model for time series. Due to bugs in the official implementation of NRTSI, it is not appropriate to put it together with other baseline methods. Therefore, we run SAITS on preprocessed datasets provided by authors of NRTSI to make a fair comparison in Subsection 4.4.3.  <ref type="table" target="#tab_2">Table 2</ref> reports the imputation performance of models on three datasets in three evaluation metrics (MAE / RMSE / MRE). On PhysioNet-2012 and Air-Quality, GRUI-GAN achieves better results than Last but is slightly inferior to Median. E 2 GAN performs better than these three methods. On Electricity, GRUI-GAN and E 2 GAN both fail because of loss explosion. We find this is caused by the long sequence length. Dataset Electricity's sequence length is 100. If the sequence length of the Air-Quality dataset is increased from 24 to 100, both GAN models will be confronted with loss explosion and fail again. M-RNN outperforms both naive imputation methods a lot on PhysioNet-2012 and Air-Quality but gets worse results than Last on Electricity. GP-VAE and BRITS both perform much better than the methods mentioned above. BRITS is the best one among baseline methods. When it comes to the self-attention-based models, Transformer surpasses BRITS obviously on datasets PhysioNet-2012 and Electricity and obtains comparable results to BRITS on Air-Quality. SAITS (base) achieves similar results to Transformer on all datasets. SAITS exceeds all baseline methods significantly on all metrics and all datasets, and it outperforms Transformer and SAITS (base) as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Imputation Performance Comparison</head><p>To show further details of the models in <ref type="table" target="#tab_2">Table 2</ref>, the parameter number and the training speed of them are listed in the following <ref type="table" target="#tab_3">Table 3</ref>. We can see that GP-VAE is the slowest model and consumes the most seconds for each epoch training. The RNN-based models are all slower than the self-attention-based models. Compared to BRITS that yields the best results in the baseline methods, SAITS takes half the training time or even less as BRITS on each epoch. Compared to Transformer, SAITS (base) has only 15% ? 30% parameters of Transformer's, but it still obtains comparable performance to Transformer. It confirms that SAITS' model structure is more efficient than Transformer on the time-series imputation task.  To further compare the performance of the imputation methods on different missing rates, we also experiment to introduce missing values into the Electricity dataset at different rates between 20% ? 90%. The results of this experiment are elaborated in <ref type="table" target="#tab_4">Table 4</ref>. The results of 10% missing rate have been displayed in <ref type="table" target="#tab_2">Table 2</ref>. Dataset Electricity is selected because it has no missing data, and it is the most complex among the three datasets because each sample has 370 features and 100 time steps (please refer to <ref type="table" target="#tab_1">Table 1</ref>). In <ref type="table" target="#tab_2">Table 2</ref>, the models achieve the highest error on the Electricity dataset, which also proves that it is the most difficult one to impute among three datasets. Therefore, Electricity is the most suitable dataset to experiment with different missing rates. GRUI-GAN and E 2 GAN are omitted because they fail on the Electricity dataset due to loss explosion as is discussed above. The baseline methods are all inferior to self-attention-based models in all cases. SAITS (base) performs better than Transformer in cases of 20%, 30%, and 40%. However, its performance becomes worse than Transformer in the left cases where missing rates become higher. This is because the hyper-parameters of SAITS (base) are fixed, and its number of parameters is limited to a low level, only 15% of Transformer (refer to <ref type="table" target="#tab_3">Table 3</ref>). Such a situation makes the capacity of SAITS (base) not enough to well handle the imputation problem with the higher missing rate. And given enough model capacity (with 78% parameters of Transformer), SAITS achieves the best performance in eight out of nine cases, demonstrating its distinct advantage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Downstream Classification Task</head><p>In the PhysioNet-2012 dataset, each sample has a label indicating if the patient is deceased that makes PhysioNet-2012 a binary-classification dataset and there are 1,707 (14.2%) samples with the positive mortality label. Therefore, mortality prediction is one of the main tasks on this dataset. However, 80% missing values make this task challenging. Similar to PhysioNet-2012, in real-world datasets, missingness often makes tasks of pattern recognition tricky. To further discuss the benefits that SAITS can bring to pattern recognition, the experiment here is conducted on a downstream classification task on the PhysioNet-2012 dataset to qualitatively compare the imputation quality of each method. Note that this experiment is inspired by prior work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref>. The idea behind this experimental design is that, if one method's imputation is better in terms of overall data quality, datasets imputed by the method should achieve better performance on downstream pattern recognition tasks, such as classification here. We first let each method impute the dataset and then train a classifier on each imputed dataset to obtain the classification results. Since this is a time-series dataset, a simple RNN classification model is employed as the classifier. This RNN classifier consists of a GRU layer followed by a fully connected layer. All hyper-parameters are fixed as follows: the learning rate (1 ? 10 ?3 ), the batch size (128), the RNN hidden size (128), the patience of the early stopping strategy <ref type="bibr" target="#b19">(20)</ref>. The classifier and the training procedure are kept exactly the same for each imputed dataset to obtain the equitably comparable results. Considering that classes in this dataset are imbalanced, metrics ROC-AUC (Area Under ROC Curve), PR-AUC (Area Under Precision-Recall Curve), and F1-score are used to measure performance. The experiment results are reported in <ref type="table" target="#tab_5">Table 5</ref>. The method names in the table annotate that the dataset is imputed by which method.</p><p>As displayed in <ref type="table" target="#tab_5">Table 5</ref>, the classifier trained on the dataset imputed by SAITS achieves the greatest results on all evaluation metrics and obtains obvious improvements than RNN-based models (1.3%, 1.9%, and 1.4% better than BRITS in ROC-AUC, PR-AUC, and F1-score). Even though comparing SAITS with Transformer, the improvements on PR-AUC and F1-score are noteworthy (increased 1.8% and 1.5% respectively), considering this is on an imbalanced and sparse dataset and the classifier is trivial. Such an indirect comparison reaches the same conclusion that the imputation quality of SAITS is the best among all methods. In the meanwhile, it demonstrates that SAITS does not only have higher accuracy in the imputation metrics but also can help improve the performance of trivial models on pattern recognition tasks on time-series datasets with missing values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Comparison with NRTSI</head><p>To make a fair comparison with NRTSI <ref type="bibr" target="#b38">[39]</ref>, another SOTA self-attention-based imputation model for time series, we tried to incorporate it into our framework of imputation models. However, its official open-source implementation in the GitHub repository https://github.com/lupalab/NRTSI has fatal bugs that stop us from reproducing their results or adding their model into our baselines. Fortunately, the authors provide their preprocessed datasets Air and Gas in their code repository. This makes us able to run SAITS on their datasets for an impartial comparison with NRTSI. Regarding the experiments in this section, we have uploaded the related dataset generating script and SAITS configuration files to our code repository to ensure our results are reproducible.</p><p>Both datasets Air <ref type="bibr" target="#b54">[54]</ref> and Gas <ref type="bibr" target="#b59">[59]</ref> are from UCI machine learning repository <ref type="bibr" target="#b55">[55]</ref>. The raw data of the Air dataset is the same as the Air-Quality dataset used in our work, but the preprocessing methods are different that results in different time steps and feature numbers. Our Air-Quality dataset is 132-dimensional and has 24 time steps, while the Air dataset is 11-dimensional and has 48 time steps. The experiment results on the datasets Air and Gas are listed in <ref type="table" target="#tab_6">Table 6</ref> and 7 respectively. The results of NRTSI are from <ref type="table" target="#tab_5">Table 5</ref> in the original paper <ref type="bibr" target="#b38">[39]</ref>, where the evaluation metric is mean squared error (MSE), so we keep using it here.    With the above results, it is obvious that SAITS outperforms NRTSI in all cases on both datasets. In particular, SAITS achieves 7% ? 39% smaller MSE (above 20% in nine out of sixteen cases) than NRTSI. To make the comparison more straightforward, the bar graphs in <ref type="figure" target="#fig_5">Figure 5</ref> are plotted to visualize the results in <ref type="table" target="#tab_6">Table 6</ref> and 7.</p><p>Besides model performance, the model parameter numbers and training speed are recorded in <ref type="table" target="#tab_8">Table 8</ref>. NRTSI's number of parameters is from Appendix B in <ref type="bibr" target="#b38">[39]</ref>. The results in <ref type="table" target="#tab_8">Table 8</ref> tell us that SAITS needs much fewer parameters than NRTSI on both datasets (only 12% and 3% of NRTSI's parameters on datasets Air and Gas respectively).</p><p>Considering the hyper-parameters of NRTSI share across the datasets and do not get adjusted accordingly in the original paper <ref type="bibr" target="#b38">[39]</ref>, NRTSI may not need so many parameters to obtain such performance. Nevertheless, NRTSI directly takes a Transformer encoder as the backbone, and it has been proven in the experiments and discussion in Section 4.4.1 that SAITS architecture is more efficient than Transformer on the imputation task. Concerning the training speed, we can not run NRTSI, so NRTSI's speed does not have records. However, according to the algorithms of NRTSI listed in Appendix A in <ref type="bibr" target="#b38">[39]</ref>, both the training procedure and imputation procedure have two nested loops, which can make NRTSI much slower than SAITS because SAITS has no loop in neither training nor imputation stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies</head><p>In this section, three ablation experiments are leveraged to discuss the rationality of SAITS architecture design. The first one 4.5.1 is to validate the improvement brought by the diagonally-masked self-attention (DMSA). The second one 4.5.2 is to discuss the necessity of the weighted combination. The third one 4.5.3 is to explain why we do not apply more than two DMSA blocks. To prove that DMSA has better imputation performance than the conventional self-attention, a comparison is made between SAITS (base) and SAITS (base, w/o) in <ref type="table" target="#tab_9">Table 9</ref>. SAITS (base, w/o) is without the diagonal masks. SAITS (base) outperforms SAITS (base, w/o) on all datasets, and this demonstrates DMSA does improve SAITS' imputation ability. During the model design process, after applying the diagonal masks to self-attention, we further think about how to enhance the imputation ability. Therefore, the second DMSA block is added to increase our model's depth and extend the learning process. Rather than simply raising the layer number of the first DMSA block that can also increase the network depth, the second DMSA block is employed as a learner to play a role of verification. Different from the first DMSA block that can only make imputation from scratch, the second DMSA block has its input containing the imputed data from the first DMSA block. Therefore, its learning target is to verify these imputation values. However, there is no guarantee that the second DMSA block can perform better than the first one. In other words, the imputations from the second DMSA block are not necessarily better than those from the first block. For example, SAITS (base, R2) achieves better performance than SAITS (base, with only 1 block) on datasets PhysioNet-2012 and Air-Quality, but performs worse on the Electricity dataset. Hence, taking imputation values from either block is not wise. Therefore, we let representations from both blocks form the final imputation together, namely in the way of the weighted combination discussed in Section 3.2.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Ablation Study of the Diagonal Masks in Self-Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Ablation Study of the Weighted Combination</head><p>We compare the weighted combination with the other two designs to discuss its rationality. As shown in <ref type="table" target="#tab_1">Table 10</ref>, one is no combination, directly taking Learned Representation 2 as the final representation, referring to SAITS (base, R2). The other is the residual combination, which combines Learned Representation 1 and 2 by a residual connection, referring to SAITS (base, Res). Compared with the residual connection, the weighted combination design parameterizes the connection process and makes it actively assign weights for the learned representations rather than being a simple addition.</p><p>As shown in <ref type="table" target="#tab_1">Table 10</ref>, SAITS (base) obtains the best results on both datasets PhysioNet-2012 and Air-Quality. On these two datasets, SAITS (base, Res) is even inferior to SAITS (base, R2). That is to say, the residual combination makes results worse. On dataset Electricity, SAITS (base) and SAITS (base, Res) achieve comparable results, and both are better than SAITS (base, R2). In summary, our weighted combination is the most practical design in all of the three.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.3">Ablation Study of the Third DMSA Block</head><p>Similar to applying the second DMSA block to obtain better performance, theoretically, we can apply more than two DMSA blocks. However, the benefit is marginal. Taking three DMSA blocks as an example, the experiments are conducted and the results are listed in <ref type="table" target="#tab_1">Table 11</ref> above.  Regarding how to combine representations from three DMSA blocks, there are still two options: residual connection and weighted combination. Residual connection is easy to implement, and SAITS (3 DMSA blocks, residual) takes this way. The weighted combination can only combine two blocks' representation at a time, so the cascade-weighted combination is used here to implement SAITS (3 DMSA blocks, cascade). The graphs in <ref type="figure">Figure 6</ref> and <ref type="figure">Figure 7</ref> are plotted to clearly illustrate both models' structure.</p><p>With the results in <ref type="table" target="#tab_1">Table 11</ref>, we can see, in general, SAITS (3 DMSA blocks, residual) and SAITS (3 DMSA blocks, cascade) do not obtain better results than SAITS, which means that adding one more block brings nothing but more parameters and computation resource waste.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper proposes SAITS, a novel self-attention-based model to impute missing values in multivariate time series. Specifically, a joint-optimization training approach is designed for self-attention-based models to perform the imputation task. Compared to BRITS, a SOTA RNN-based imputation model, SAITS reduces mean absolute error (MAE) by 12% ? 38% and achieves 2.0 ? 2.6 times faster training speed. In the comparison with another SOTA model NRTSI, which takes a Transformer as the backbone, SAITS achieves 7% ? 39% better imputation accuracy. Moreover, when Transformer is trained by our joint-optimization approach, SAITS still obtains MAE 2% ? 13% smaller than it, with comparable training speed. Especially on dataset Electricity, the most complex dataset among all three, the improvement is the biggest (13%), which means SAITS has an obvious advantage over Transformer when datasets become complex. Furthermore, the experiments also tell us that SATIS has a more efficient model structure than Transformer on the imputation task. To obtain comparable performance, SAITS needs only 15% ? 30% parameters of Transformer. Additionally, to justify the design of SAITS architecture, a series of ablation experiments are performed to further discuss the reasons for our design and prove its effectiveness. All of the experimental results lead to the same conclusion that SAITS efficiently achieves the new SOTA accuracy on the time-series imputation task. In addition to imputation accuracy that evaluates SAITS quantitatively, our empirical results in the downstream classification experiment qualitatively show that classification performance can directly get improved by letting SAITS impute the missing part, which reveals SAITS' potential of becoming a bridge for pattern recognition models to learn with incomplete time-series data.</p><p>Our future work will investigate the imputation performance of SAITS on partially-observed time series with other missing patterns. Note that we add completely-random artificial missingness in MIT because the missing pattern is assumed to be MCAR in the settings of this work. If one already knows the missing pattern of the dataset to be imputed, one can apply the specific pattern to create artificially missing values. This is intuitive and still keeps the functionality of MIT, though whether it can help improve imputation accuracy compared to applying MCAR missingness is open to discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details of Hyper-parameter Searching</head><p>We expose the details about hyper-parameter searches in this section.</p><p>General For all models, the learning rate is log-uniformly sampled between 1 ? 10 ?4 and 1 ? 10 ?2 . If applicable, the dropout rate is sampled from the values (0.0, 0.1, 0.2, 0.3, 0.4, 0.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RNN-based models</head><p>For all RNN-based models (GRUI-GAN, E 2 GAN, M-RNN, and BRITS), the RNN hidden size is sampled from the values <ref type="bibr" target="#b31">(32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512,</ref><ref type="bibr">1024)</ref>. For GRUI-GAN and E 2 GAN, the dimension of z is sampled from <ref type="bibr" target="#b31">(32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512,</ref><ref type="bibr">1024)</ref>, the number of pretrain epochs is sampled from <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20)</ref>. For GRUI-GAN, hyper-parameter ?, which controls the proportion between the masked reconstruction loss and the discriminative loss, is sampled from (0, 0.15, 0.3, 0.45). For E 2 GAN, hyper-parameter ?, which controls the weight of the discriminative loss and the squared error loss, is sampled from <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr">64)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GP-VAE</head><p>The encoder size and decoder size are sampled from values (64, 128, 256, 512, 1024). The length scale is sampled from <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b15">16)</ref> and the window size is sampled from <ref type="bibr" target="#b3">(4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr">64)</ref>. ? is sampled from (0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8). ? is set as 1.005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-attention-based models</head><p>For self-attention models (Transformer and SAITS), we sample the number of layers N from <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8)</ref>, d model from (64, 128, 256, 512, 1024), d ffn from (128, 256, 512, 1024, 2048, 4096), d v from <ref type="bibr" target="#b31">(32,</ref><ref type="bibr">64,</ref><ref type="bibr">128,</ref><ref type="bibr">256,</ref><ref type="bibr">512)</ref>, the number of heads h from <ref type="bibr" target="#b1">(2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8)</ref>. d k is set as the value of d model divided by h.</p><p>For model SAITS (base), we fix the learning rate = 0.001, the dropout rate = 0.1, N = 2, d model = 256, d ffn = 128, h = 4, d v = d k = 64. To discuss how our joint-optimization approach can influence the performance of RNN-based models, we apply it in the training of model BRITS and show experimental results in this section. <ref type="table" target="#tab_2">Table 2</ref> are used here. That is to say, hyper-parameters are kept exactly the same. The difference between the training way in the original paper <ref type="bibr" target="#b20">[21]</ref> and our joint-optimization approach is whether to apply MIT. Therefore, we use suffix "w/o MIT" to represent the original training way and suffix "w MIT" to represent our joint-optimization training approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B BRITS Trained by the joint-optimization approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BRITS models from</head><p>As displayed in <ref type="table" target="#tab_1">Table 12</ref>, BRITS (w MIT) outperforms BRITS (w/o MIT) on datasets PhysioNet-2012 and Air-Quality, but achieves worse performance on the Electricity dataset. Therefore, applying MIT in the training of BRITS can bring further improvement on some datasets, but this is not necessary, and it depends on the dataset. Note that despite BRITS (w MIT) obtains better results on datasets PhysioNet-2012 and Air-Quality, its performance is still inferior to Transformer and SAITS.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Illustration of Eq. 6. Diagonally-masked self-attention on a time-series sample with five time steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(pos, 2i) = sin pos 10000 2i d model , PosEnc(pos, 2i + 1) = cos pos 10000 2i d modelwhere pos is the time-step position, i is the dimension<ref type="bibr" target="#b7">(8)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>The visualized comparison with NRTSI on datasets Air and Gas. The percentage numbers above the bars indicate, compared with NRTSI, the amount of imputation MSE reduced by SAITS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Learned Representation 3 The 1st DMSA block The 2nd DMSA block The weighted combination block Learned Representation 1</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Imputed Data</cell></row><row><cell></cell><cell></cell><cell>Replace</cell></row><row><cell></cell><cell></cell><cell>Weighted Combine</cell></row><row><cell cols="2">Imputed Data</cell><cell>Learned Representation 2</cell></row><row><cell>Replace</cell><cell></cell><cell>Linear</cell><cell>Combining Weights</cell></row><row><cell></cell><cell></cell><cell>ReLU</cell><cell>Sigmoid</cell></row><row><cell>Linear</cell><cell></cell><cell>Linear</cell><cell>Linear</cell></row><row><cell cols="2">Add &amp; Norm</cell><cell>Add &amp; Norm</cell><cell>Concatenate</cell></row><row><cell>Feed</cell><cell></cell><cell>Feed</cell></row><row><cell>Forward</cell><cell></cell><cell>Forward</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Attention Weights</cell></row><row><cell cols="2">Add &amp; Norm</cell><cell>Add &amp; Norm</cell></row><row><cell cols="2">DiagMasked</cell><cell>DiagMasked</cell></row><row><cell>MHA</cell><cell></cell><cell>MHA</cell></row><row><cell>Positional</cell><cell></cell><cell>Positional</cell></row><row><cell>Encoding</cell><cell></cell><cell>Encoding</cell></row><row><cell>Linear</cell><cell></cell><cell>Linear</cell></row><row><cell cols="2">Concatenate</cell><cell>Concatenate</cell></row><row><cell>Feature Vectors</cell><cell cols="2">Missing Masks</cell></row><row><cell cols="4">Firstly, some fundamental components of SAITS get introduced in Subsection 3.2.1 and 3.2.2. Then</cell></row><row><cell cols="4">SAITS' three-part structure is illustrated in Subsection 3.2.3, 3.2.4, and 3.2.5, respectively. Finally, the loss functions of</cell></row><row><cell>learning tasks are discussed in Subsection 3.2.6.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>General information of three datasets used in this work. The PhysioNet 2012 challenge dataset<ref type="bibr" target="#b56">[56]</ref> contains 12,000 multivariate clinical time-series samples collected from patients in ICU (Intensive Care Unit). Each sample is recorded during the first 48 hours after admission to the ICU. Depending on the status of patients, there are up to 37 time-series variables measured, for instance, temperature, heart rate, blood pressure. Measurements might be collected at regular intervals (hourly or daily), and also may be recorded at irregular intervals (only collected as required). Not all variables are available in all samples. Note that this dataset is very sparse and has 80% missing values in total. The dataset is firstly split into the training set and the test set according to 80% and 20%. Then 20% of samples are split from the training set as the validation set. We randomly eliminate 10% of observed values in the validation set and the test set and use these values as ground truth to evaluate the imputation performance of models. Following 12 samples are dropped because of containing no time-series information at all:147514, 142731, 145611, 140501, 155655,  143656, 156254, 150309, 140936, 141264, 150649, 142998.    </figDesc><table><row><cell></cell><cell>PhysioNet-2012</cell><cell>Air-Quality</cell><cell>Electricity</cell></row><row><cell>Number of total samples</cell><cell>11,988</cell><cell>1,461</cell><cell>1,400</cell></row><row><cell>Number of features</cell><cell>37</cell><cell>132</cell><cell>370</cell></row><row><cell>Sequence length</cell><cell>48</cell><cell>24</cell><cell>100</cell></row><row><cell>Original missing rate</cell><cell>80.0%</cell><cell>1.6%</cell><cell>0%</cell></row><row><cell cols="3">PhysioNet 2012 Mortality Prediction Challenge (PhysioNet-2012)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance comparison between methods on three datasets. 10% observations in the test set are held out for evaluation. Metrics are reported in the order of MAE / RMSE / MRE. The lower, the better. Bold font indicates the best performance. GRUI-GAN and E 2 GAN have no results on Electricity because they fail in the training due to loss explosion.</figDesc><table><row><cell>Method</cell><cell>PhysioNet-2012</cell><cell>Air-Quality</cell><cell>Electricity</cell></row><row><cell>Median</cell><cell>0.726 / 0.988 / 103.5%</cell><cell>0.763 / 1.175 / 107.4%</cell><cell>2.056 / 2.732 / 110.1%</cell></row><row><cell>Last</cell><cell>0.862 / 1.207 / 123.0%</cell><cell>0.967 / 1.408 / 136.3%</cell><cell>1.006 / 1.533 / 53.9%</cell></row><row><cell>GRUI-GAN</cell><cell>0.765 / 1.040 / 109.1%</cell><cell>0.788 / 1.179 / 111.0%</cell><cell>/</cell></row><row><cell>E 2 GAN</cell><cell>0.702 / 0.964 / 100.1%</cell><cell>0.750 / 1.126 / 105.6%</cell><cell>/</cell></row><row><cell>M-RNN</cell><cell>0.533 / 0.776 / 76.0%</cell><cell>0.294 / 0.643 / 41.4%</cell><cell>1.244 / 1.867 / 66.6%</cell></row><row><cell>GP-VAE</cell><cell>0.398 / 0.630 / 56.7%</cell><cell>0.268 / 0.614 / 37.7%</cell><cell>1.094 / 1.565 / 58.6%</cell></row><row><cell>BRITS</cell><cell>0.256 / 0.767 / 36.5%</cell><cell>0.153 / 0.525 / 21.6%</cell><cell>0.847 / 1.322 / 45.3%</cell></row><row><cell>Transformer</cell><cell>0.190 / 0.445 / 26.9%</cell><cell>0.158 / 0.521 / 22.3%</cell><cell>0.823 / 1.301 / 44.0%</cell></row><row><cell>SAITS (base)</cell><cell>0.192 / 0.439 / 27.3%</cell><cell>0.146 / 0.521 / 20.6%</cell><cell>0.822 / 1.221 / 44.0%</cell></row><row><cell>SAITS</cell><cell>0.186 / 0.431 / 26.6%</cell><cell>0.137 / 0.518 / 19.3%</cell><cell>0.735 / 1.162 / 39.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Models' parameter number (in million) and training time of each epoch (in seconds) on datasets PhysioNet-2012, Air-Quality, and Electricity are listed from left to right. GRUI-GAN and E 2 GAN have no results for the Electricity dataset because they fail on this dataset due to loss explosion.</figDesc><table><row><cell></cell><cell cols="2">PhysioNet-2012</cell><cell cols="2">Air-Quality</cell><cell cols="2">Electricity</cell></row><row><cell>Model</cell><cell># of param</cell><cell>s / epoch</cell><cell># of param</cell><cell>s / epoch</cell><cell># of param</cell><cell>s / epoch</cell></row><row><cell>GRUI-GAN</cell><cell>0.16M</cell><cell>14.4</cell><cell>2.32M</cell><cell>2.0</cell><cell>/</cell><cell>/</cell></row><row><cell>E 2 GAN</cell><cell>0.08M</cell><cell>22.8</cell><cell>1.13M</cell><cell>2.2</cell><cell>/</cell><cell>/</cell></row><row><cell>M-RNN</cell><cell>0.07M</cell><cell>6.8</cell><cell>1.09M</cell><cell>1.3</cell><cell>18.63M</cell><cell>3.9</cell></row><row><cell>GP-VAE</cell><cell>0.15M</cell><cell>40.1</cell><cell>0.36M</cell><cell>8.7</cell><cell>13.45M</cell><cell>106.0</cell></row><row><cell>BRITS</cell><cell>0.73M</cell><cell>12.8</cell><cell>11.25M</cell><cell>1.9</cell><cell>7.00M</cell><cell>5.2</cell></row><row><cell>Transformer</cell><cell>4.36M</cell><cell>3.1</cell><cell>5.13M</cell><cell>0.9</cell><cell>14.78M</cell><cell>2.6</cell></row><row><cell>SAITS (base)</cell><cell>1.38M</cell><cell>2.7</cell><cell>1.56M</cell><cell>1.1</cell><cell>2.20M</cell><cell>2.1</cell></row><row><cell>SAITS</cell><cell>5.32M</cell><cell>5.0</cell><cell>3.07M</cell><cell>0.9</cell><cell>11.51M</cell><cell>2.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table><row><cell>Method</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell></row><row><cell>Median</cell><cell cols="4">2.053 / 2.726 / 109.9% 2.055 / 2.732 / 110.0% 2.058 / 2.734 / 110.2% 2.053 / 2.728 / 109.9%</cell></row><row><cell>Last</cell><cell>1.012 / 1.547 / 54.2%</cell><cell>1.018 / 1.559 / 54.5%</cell><cell>1.025 / 1.578 / 54.9%</cell><cell>1.032 / 1.595 / 55.2%</cell></row><row><cell>M-RNN</cell><cell>1.242 / 1.854 / 66.5%</cell><cell>1.258 / 1.876 / 67.3%</cell><cell>1.269 / 1.884 / 68.0%</cell><cell>1.283 / 1.902 / 68.7%</cell></row><row><cell>GP-VAE</cell><cell>1.124 / 1.502 / 60.2%</cell><cell>1.057 / 1.571 / 56.6%</cell><cell>1.090 / 1.578 / 58.4%</cell><cell>1.097 / 1.572 / 58.8%</cell></row><row><cell>BRITS</cell><cell>0.928 / 1.395 / 49.7%</cell><cell>0.943 / 1.435 / 50.4%</cell><cell>0.996 / 1.504 / 53.4%</cell><cell>1.037 / 1.538 / 55.5%</cell></row><row><cell>Transformer</cell><cell>0.843 / 1.318 / 45.1%</cell><cell>0.846 / 1.321 / 45.3%</cell><cell>0.876 / 1.387 / 46.9%</cell><cell>0.895 / 1.410 / 47.9%</cell></row><row><cell>SAITS (base)</cell><cell>0.838 / 1.264 / 44.9%</cell><cell>0.845 / 1.247 / 45.2%</cell><cell cols="2">0.873 / 1.325 / 46.7% 0.939 / 1.537 / 50.3%</cell></row><row><cell>SAITS</cell><cell cols="4">0.763 / 1.187 / 40.8% 0.790 / 1.223 / 42.3% 0.869 / 1.314 / 46.7% 0.876 / 1.377 / 46.9%</cell></row><row><cell>Method</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell>90%</cell></row><row><cell>Median</cell><cell cols="4">2.057 / 2.734 / 110.2% 2.050 / 2.726 / 109.8% 2.059 / 2.734 / 110.2% 2.056 / 2.723 / 110.1%</cell></row><row><cell>Last</cell><cell>1.040 / 1.615 / 55.7%</cell><cell>1.049 / 1.640 / 56.2%</cell><cell>1.059 / 1.663 / 56.7%</cell><cell>1.070 / 1.690 / 57.3%</cell></row><row><cell>M-RNN</cell><cell>1.298 / 1.912 / 69.4%</cell><cell>1.305 / 1.928 / 69.9%</cell><cell>1.318 / 1.951 / 70.5%</cell><cell>1.331 / 1.961 / 71.3%</cell></row><row><cell>GP-VAE</cell><cell>1.101 / 1.616 / 59.0%</cell><cell>1.037 / 1.598 / 55.6%</cell><cell>1.062 / 1.621 / 56.8%</cell><cell>1.004 / 1.622 / 53.7%</cell></row><row><cell>BRITS</cell><cell>1.101 / 1.602 / 59.0%</cell><cell>1.090 / 1.617 / 58.4%</cell><cell>1.138 / 1.665 / 61.0%</cell><cell>1.163 / 1.702 / 62.3%</cell></row><row><cell>Transformer</cell><cell>0.891 / 1.404 / 47.7%</cell><cell>0.920 / 1.437 / 49.3%</cell><cell>0.924 / 1.472 / 49.5%</cell><cell>0.934 / 1.491 / 49.8%</cell></row><row><cell>SAITS (base)</cell><cell>0.969 / 1.565 / 51.9%</cell><cell>0.972 / 1.601 / 52.0%</cell><cell>1.012 / 1.608 / 54.2%</cell><cell>1.001 / 1.630 / 53.6%</cell></row><row><cell>SAITS</cell><cell>0.892 / 1.328 / 47.9%</cell><cell cols="2">0.898 / 1.273 / 48.1% 0.908 / 1.327 / 48.6%</cell><cell>0.933 / 1.354 / 49.9%</cell></row></table><note>Performance comparison between methods on the Electricity dataset across different missing rates from 20% ? 90%. Metrics are reported in the order of MAE / RMSE / MRE. The lower, the better. Values in bold are the best.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Results of the downstream classification task on the PhysioNet-2012 dataset. Performance metrics of methods are calculated by five independent runs. The reported values are means ? standard deviations. The higher, the better. Values in bold are the best.</figDesc><table><row><cell>Method</cell><cell>ROC-AUC</cell><cell>PR-AUC</cell><cell>F1-score</cell></row><row><cell>Median</cell><cell>83.4% ? 0.4%</cell><cell>46.0% ? 0.6%</cell><cell>38.5% ? 3.1%</cell></row><row><cell>Last</cell><cell>82.8% ? 0.3%</cell><cell>46.9% ? 0.4%</cell><cell>39.5% ? 2.4%</cell></row><row><cell>GRUI-GAN</cell><cell>83.0% ? 0.2%</cell><cell>45.1% ? 0.7%</cell><cell>38.8% ? 2.0%</cell></row><row><cell>E 2 GAN</cell><cell>83.0% ? 0.2%</cell><cell>45.5% ? 0.5%</cell><cell>35.6% ? 2.0%</cell></row><row><cell>M-RNN</cell><cell>82.2% ? 0.2%</cell><cell>45.4% ? 0.6%</cell><cell>38.8% ? 3.5%</cell></row><row><cell>GP-VAE</cell><cell>83.4% ? 0.2%</cell><cell>48.1% ? 0.7%</cell><cell>40.9% ? 3.3%</cell></row><row><cell>BRITS</cell><cell>83.5% ? 0.1%</cell><cell>49.1% ? 0.4%</cell><cell>41.3% ? 1.8%</cell></row><row><cell>Transformer</cell><cell>84.3% ? 0.5%</cell><cell>49.2% ? 1.4%</cell><cell>41.2% ? 1.9%</cell></row><row><cell>SAITS (base)</cell><cell>84.6% ? 0.2%</cell><cell>49.8% ? 0.4%</cell><cell>41.5% ? 2.0%</cell></row><row><cell>SAITS</cell><cell>84.8% ? 0.2%</cell><cell>51.0% ? 0.5%</cell><cell>42.7% ? 2.8%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>The performance comparison between NRTSI and SAITS on dataset Air across different missing rates from 10% ? 80%. The evaluation metric is MSE. The lower, the better. The best results are in bold.</figDesc><table><row><cell>Method</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell>40%</cell><cell>50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell></row><row><cell>NRTSI</cell><cell>0.1230</cell><cell>0.1155</cell><cell>0.1189</cell><cell>0.1250</cell><cell>0.1297</cell><cell>0.1378</cell><cell>0.1542</cell><cell>0.1790</cell></row><row><cell>SAITS</cell><cell>0.0980</cell><cell>0.0911</cell><cell>0.0916</cell><cell>0.1021</cell><cell>0.1109</cell><cell>0.1030</cell><cell>0.1179</cell><cell>0.1409</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>The performance comparison between NRTSI and SAITS on dataset Gas across different missing rates from 10% ? 80%. The evaluation metric used here is MSE. The lower, the better. The best results are in bold.</figDesc><table><row><cell></cell><cell cols="2">Method</cell><cell></cell><cell>10%</cell><cell></cell><cell cols="2">20%</cell><cell>30%</cell><cell></cell><cell cols="2">40%</cell><cell cols="2">50%</cell><cell cols="2">60%</cell><cell>70%</cell><cell>80%</cell></row><row><cell></cell><cell cols="2">NRTSI</cell><cell></cell><cell cols="2">0.0165</cell><cell cols="2">0.0195</cell><cell cols="2">0.0196</cell><cell cols="2">0.0229</cell><cell cols="2">0.0286</cell><cell cols="2">0.0311</cell><cell>0.0362</cell><cell>0.0445</cell></row><row><cell></cell><cell cols="2">SAITS</cell><cell></cell><cell cols="2">0.0100</cell><cell cols="2">0.0123</cell><cell cols="2">0.0145</cell><cell cols="2">0.0192</cell><cell cols="2">0.0239</cell><cell cols="2">0.0289</cell><cell>0.0337</cell><cell>0.0326</cell></row><row><cell></cell><cell>0.200</cell><cell></cell><cell cols="6">Performance comparison on dataset Air</cell><cell></cell><cell></cell><cell>0.05</cell><cell></cell><cell cols="4">Performance comparison on dataset Gas</cell></row><row><cell></cell><cell>0.175</cell><cell></cell><cell>NRTSI SAITS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">21.3%</cell><cell></cell><cell></cell><cell></cell><cell>NRTSI SAITS</cell><cell></cell><cell></cell><cell>26.7%</cell></row><row><cell>Imputation MSE</cell><cell>0.075 0.100 0.125 0.150</cell><cell>26.2%</cell><cell cols="2">21.1% 23.0%</cell><cell>18.3%</cell><cell>14.5%</cell><cell>25.3%</cell><cell>23.5%</cell><cell></cell><cell>Imputation MSE</cell><cell>0.02 0.03 0.04</cell><cell>39.4%</cell><cell cols="2">36.9% 26.0%</cell><cell>16.2%</cell><cell>16.4%</cell><cell>7.1%</cell><cell>6.9%</cell></row><row><cell></cell><cell>0.050</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.01</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.025</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.000</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell cols="2">40% Missing Rate 50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell><cell></cell><cell>0.00</cell><cell>10%</cell><cell>20%</cell><cell>30%</cell><cell cols="2">40% Missing Rate 50%</cell><cell>60%</cell><cell>70%</cell><cell>80%</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">(a) Comparison on dataset Air</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>The models' parameter number (in million) and training time of each epoch (in seconds) on datasets Air and Gas are listed below. NRTSI have no results of the training speed because the original paper does not include them.</figDesc><table><row><cell></cell><cell>Air</cell><cell></cell><cell>Gas</cell><cell></cell></row><row><cell>Model</cell><cell># of param</cell><cell>s / epoch</cell><cell># of param</cell><cell>s / epoch</cell></row><row><cell>NRTSI</cell><cell>84.00M</cell><cell>/</cell><cell>84.00M</cell><cell>/</cell></row><row><cell>SAITS</cell><cell>10.00M</cell><cell>2.6</cell><cell>2.78M</cell><cell>18.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Ablation experiment results of the diagonal masks in self-attention. SAITS (base, w/o) is the exact same as SAITS (base), except it is without the diagonal masks in self-attention layers.</figDesc><table><row><cell>Model</cell><cell>PhysioNet-2012</cell><cell>Air-Quality</cell><cell>Electricity</cell></row><row><cell>SAITS (base, w/o)</cell><cell>0.200 / 0.446 / 28.5%</cell><cell>0.148 / 0.528 / 21.3%</cell><cell>0.898 / 1.504 / 48.1%</cell></row><row><cell>SAITS (base)</cell><cell cols="3">0.192 / 0.439 / 27.3% 0.146 / 0.521 / 20.6% 0.822 / 1.221 / 44.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Ablation experiment results of the weighted combination. SAITS (base, with only 1 block) does not have the second DMSA block nor the weighted-combination block, and its final representation is directly from the only DMSA block. SAITS (base, R2) directly takes Learned Representation 2 as the final representation. In other words, it has no combination of representations. SAITS (base, Res) applies a residual connection to combine Learned Representation 1 and 2.</figDesc><table><row><cell>Model</cell><cell>PhysioNet-2012</cell><cell>Air-Quality</cell><cell>Electricity</cell></row><row><cell>SAITS (base, with only 1 block)</cell><cell>0.204 / 0.496 / 29.2%</cell><cell>0.178 / 0.544 / 25.1%</cell><cell>0.876 / 1.381 / 46.9%</cell></row><row><cell>SAITS (base, R2)</cell><cell>0.199 / 0.451 / 28.4%</cell><cell>0.149 / 0.522 / 21.0%</cell><cell>0.906 / 1.456 / 48.5%</cell></row><row><cell>SAITS (base, Res)</cell><cell>0.200 / 0.477 / 28.5%</cell><cell>0.160 / 0.527 / 22.6%</cell><cell>0.819 / 1.223 / 43.7%</cell></row><row><cell>SAITS (base)</cell><cell cols="2">0.192 / 0.439 / 27.3% 0.146 / 0.521 / 20.6%</cell><cell>0.822 / 1.221 / 44.0%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Figure 6 :Learned Representation 3 The 3rd DMSA blockLearned Representation 2 The 2nd DMSA blockThe 3rd DMSA blockThe 2nd DMSA block</head><label>6</label><figDesc>Structure illustration of SAITS with three DMSA blocks (residual connected). The second DMSA block takes in data imputed by the first DMSA block, and the third DMSA block takes in data from the second one. The final output is from a residual connection of the representations produced by three DMSA blocks. Structure illustration of SAITS with three DMSA blocks (cascade weighted). The representations from the first two DMSA blocks are merged by the first weighted combination block to produce Learned Representation 3, which is used to impute data for the input of the third DMSA block. The final output is a weighted combination of the representation from the third DMSA block and Learned Representation 3.</figDesc><table><row><cell cols="2">Replace Imputed Data Learned Representation 1 Figure 7: Linear The 1st DMSA block</cell><cell cols="2">Learned Representation 3 Learned Representation 2 Linear ReLU Linear Sigmoid Combining Weights Weighted Combine</cell><cell>Replace Imputed Data</cell><cell>Replace Imputed Data Linear ReLU Linear Learned Representation 4 Weighted Combine Learned Representation 5</cell><cell>Linear Sigmoid Combining Weights The 2nd weighted combination block</cell></row><row><cell></cell><cell></cell><cell>Linear</cell><cell></cell><cell></cell></row><row><cell cols="2">Add &amp; Norm</cell><cell></cell><cell>Add &amp; Norm</cell><cell></cell><cell>Add &amp; Norm</cell><cell>Concatenate</cell></row><row><cell>Feed</cell><cell></cell><cell>Concatenate</cell><cell>Feed</cell><cell></cell><cell>Feed</cell></row><row><cell>Forward</cell><cell></cell><cell></cell><cell>Forward</cell><cell></cell><cell>Forward</cell><cell>Attention Weights</cell></row><row><cell cols="2">Add &amp; Norm</cell><cell>Attention Weights</cell><cell>Add &amp; Norm</cell><cell></cell><cell>Add &amp; Norm</cell></row><row><cell cols="2">DiagMasked</cell><cell></cell><cell>DiagMasked</cell><cell></cell><cell>DiagMasked</cell></row><row><cell>MHA</cell><cell></cell><cell>The 1st weighted</cell><cell>MHA</cell><cell></cell><cell>MHA</cell></row><row><cell></cell><cell></cell><cell>combination</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>block</cell><cell></cell><cell></cell></row><row><cell>Positional Encoding</cell><cell></cell><cell></cell><cell>Positional Encoding</cell><cell></cell><cell>Positional Encoding</cell></row><row><cell>Linear</cell><cell></cell><cell></cell><cell>Linear</cell><cell></cell><cell>Linear</cell></row><row><cell cols="2">Concatenate</cell><cell></cell><cell>Concatenate</cell><cell></cell><cell>Concatenate</cell></row><row><cell>Feature Vectors</cell><cell cols="2">Missing Masks</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Imputed Data</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>Replace</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="3">Learned Representation 4</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Residual Connection</cell></row><row><cell></cell><cell cols="2">Imputed Data</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Replace</cell><cell>Linear</cell><cell></cell><cell>Replace</cell><cell>Linear</cell></row><row><cell cols="3">Learned Representation 1</cell><cell>ReLU</cell><cell></cell><cell>Imputed Data</cell><cell>ReLU</cell></row><row><cell>The 1st DMSA block</cell><cell></cell><cell>Linear</cell><cell>Linear</cell><cell></cell><cell>Linear</cell></row><row><cell></cell><cell cols="2">Add &amp; Norm</cell><cell>Add &amp; Norm</cell><cell></cell><cell>Add &amp; Norm</cell></row><row><cell></cell><cell></cell><cell>Feed</cell><cell>Feed</cell><cell></cell><cell>Feed</cell></row><row><cell></cell><cell cols="2">Forward</cell><cell>Forward</cell><cell></cell><cell>Forward</cell></row><row><cell></cell><cell cols="2">Add &amp; Norm</cell><cell>Add &amp; Norm</cell><cell></cell><cell>Add &amp; Norm</cell></row><row><cell></cell><cell cols="2">DiagMasked</cell><cell>DiagMasked</cell><cell></cell><cell>DiagMasked</cell></row><row><cell></cell><cell></cell><cell>MHA</cell><cell>MHA</cell><cell></cell><cell>MHA</cell></row><row><cell>Positional</cell><cell></cell><cell></cell><cell></cell><cell>Positional</cell><cell>Positional</cell></row><row><cell>Encoding</cell><cell></cell><cell></cell><cell></cell><cell>Encoding</cell><cell>Encoding</cell></row><row><cell></cell><cell></cell><cell>Linear</cell><cell>Linear</cell><cell></cell><cell>Linear</cell></row><row><cell></cell><cell cols="2">Concatenate</cell><cell>Concatenate</cell><cell></cell><cell>Concatenate</cell></row><row><cell cols="2">Feature Vectors</cell><cell>Missing Masks</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>Ablation experiment results of the third DMSA block. Results of SAITS here are from Table 2 in our paper. Both SAITS (3 DMSA blocks, residual) and SAITS (3 DMSA blocks, cascade) apply the same hyper-parameters with SAITS.</figDesc><table><row><cell>Model</cell><cell>PhysioNet-2012</cell><cell>Air-Quality</cell><cell>Electricity</cell></row><row><cell cols="2">SAITS (3 DMSA blocks, residual) 0.189 / 0.620 / 27.0%</cell><cell>0.158 / 0.509 / 22.2%</cell><cell>0.740 / 1.020 / 39.6%</cell></row><row><cell cols="2">SAITS (3 DMSA blocks, cascade) 0.185 / 0.418 / 26.4%</cell><cell>0.146 / 0.512 / 20.5%</cell><cell>0.800 / 1.147 / 42.8%</cell></row><row><cell>SAITS</cell><cell>0.186 / 0.431 / 26.6%</cell><cell cols="2">0.137 / 0.518 / 19.3% 0.735 / 1.162 / 39.4%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 12 :</head><label>12</label><figDesc>Performance comparison between BRITS trained without MIT and with MIT.</figDesc><table><row><cell>Model</cell><cell>PhysioNet-2012</cell><cell>Air-Quality</cell><cell>Electricity</cell></row><row><cell>BRITS (w/o MIT)</cell><cell>0.256 / 0.767 / 36.5%</cell><cell>0.153 / 0.525 / 21.6%</cell><cell>0.847 / 1.322 / 45.3%</cell></row><row><cell>BRITS (w MIT)</cell><cell cols="2">0.251 / 0.691 / 35.8% 0.144 / 0.521 / 20.3%</cell><cell>0.910 / 1.363 / 48.7%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that the layer normalization<ref type="bibr" target="#b52">[52]</ref> and residual connection<ref type="bibr" target="#b53">[53]</ref> are applied after each attention layer and feed-forward layer in the same way as<ref type="bibr" target="#b49">[49]</ref>.Figure 3shows these details. They are suppressed here for simplicity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.physionet.org/content/challenge-2012 4 https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data 5 https://archive.ics.uci.edu/ml/datasets/ElectricityLoadDiagrams20112014</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">d t D d T t mask d t MRE (estimation, target, mask) =</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Modeling long-and short-term temporal patterns with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval, SIGIR &apos;18</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Temporal regularized matrix factorization for high-dimensional time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Forecasting stock markets using wavelet transforms and recurrent neural networks: An integrated system based on artificial bee colony algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Jung</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsiao-Fen</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chang</forename><surname>Yeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Impact of Soft Computing for the Progress of Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="2510" to="2525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The arrow of time in multivariate time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>Maria Florina Balcan and Kilian Q. Weinberger</editor>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="20" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Predicting in-hospital mortality of icu patients: The physionet/computing in cardiology challenge 2012</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikaro</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><forename type="middle">A</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger G</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing in cardiology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page">245</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mime: Multilevel medical embedding of electronic health records for predictive healthcare</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cao</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walter</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Set functions for time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bastian</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>Hal Daum? III and Aarti Singh</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ST-MVL: Filling missing values in geo-sensory time series data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuwen</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Joint Conference on Artificial Intelligence. IJCAI 2016</title>
		<meeting>the 25th International Joint Conference on Artificial Intelligence. IJCAI 2016</meeting>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Modeling continuous stochastic processes with dynamic normalizing flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhi</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Brubaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lehrmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7805" to="7815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Missing data analysis: making it work in the real world. Annual review of psychology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Graham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="549" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inference and missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="581" to="592" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining multiple imputation and meta-analysis with individual participant data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Resche-Rigon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in medicine</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bias and efficiency of multiple imputation compared with complete-case analysis for missing covariate values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Carlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in medicine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">2010</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On the estimation of arima models with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><forename type="middle">F</forename><surname>Ansley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Kohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Time Series Analysis of Irregularly Observed Data</title>
		<editor>Emanuel Parzen</editor>
		<meeting><address><addrLine>New York, NY; New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1984" />
			<biblScope unit="page" from="9" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The treatment of missing values and its effect on classifier accuracy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Acuna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caroline</forename><surname>Rodriguez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Classification, clustering, and data mining applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page" from="639" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The effects of the irregular sample and missing data in time series analysis. Nonlinear dynamics, psychology, and life sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kreindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lumsden</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="187" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unifying user-based and item-based collaborative filtering approaches by similarity fusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arjen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcel</forename><forename type="middle">J T</forename><surname>De Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reinders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;06</title>
		<meeting>the 29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;06<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="501" to="508" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Methods for the estimation of missing values in time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Fung</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiple imputation by chained equations: what is it and how does it work?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Melissa J Azur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frangakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Methods in Psychiatric Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">BRITS: Bidirectional recurrent imputation for time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yitan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">GAIN: Missing data imputation using generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsung</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Jordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<editor>Jennifer Dy and Andreas Krause</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MisGAN: Learning from incomplete data with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Cheng-Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Collagan: Collaborative gan for missing image data imputation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongwook</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Won-Jin</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong Chul</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">GAMIN: Generative adversarial multiple imputation network for highly missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seongwook</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghoon</forename><surname>Sull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mcflow: Monte carlo flow models for data imputation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">W</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wencheng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beilei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><forename type="middle">A</forename><surname>Bernal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Handling incomplete heterogeneous data using vaes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfredo</forename><surname>Naz?bal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><forename type="middle">M</forename><surname>Olmos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isabel</forename><surname>Valera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">107</biblScope>
			<biblScope unit="page">107501</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention-based learning for missing data imputation in holoclean</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoqian</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems</title>
		<editor>I. Dhillon, D. Papailiopoulos, and V. Sze</editor>
		<meeting>Machine Learning and Systems</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="307" to="325" />
		</imprint>
	</monogr>
	<note>Ihab Ilyas, and Theodoros Rekatsinas</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Antonis Nikitakis, and Konstantinos Kyriakopoulos. MAIN: multihead-attention imputation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Mouselinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyriakos</forename><surname>Polymenakos</surname></persName>
		</author>
		<idno>abs/2102.05428</idno>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Estimating missing data in temporal data streams using multi-directional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinsung</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">R</forename><surname>Zame</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1477" to="1490" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">NAOMI: Non-autoregressive multiresolution sequence imputation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multivariate time series imputation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhang</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Xiaojie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">E 2 GAN: End-to-end generative adversarial network for multivariate time series imputation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</meeting>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">2019</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">GP-VAE: Deep probabilistic time series imputation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vincent Fortuin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunnar</forename><surname>Baranchuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephan</forename><surname>Raetsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mandt</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics</title>
		<editor>Silvia Chiappa and Roberto Calandra</editor>
		<meeting>the Twenty Third International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2020-08" />
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="26" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Longitudinal variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gleb</forename><surname>Siddharth Ramchandran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kalle</forename><surname>Tikhonov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Kujanp??</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Koskinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>L?hdesm?ki</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 24th International Conference on Artificial Intelligence and Statistics</title>
		<editor>Arindam Banerjee and Kenji Fukumizu</editor>
		<meeting>The 24th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2021-04" />
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="13" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ashman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Tebbutt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Fortuin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pearce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<idno>abs/2010.10177</idno>
	</analytic>
	<monogr>
		<title level="m">Sparse gaussian process variational autoencoders</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">CDSA: crossdimensional self-attention for multivariate, geo-tagged time series imputation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Zareian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hassan</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Vetro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Fu</forename><surname>Chang</surname></persName>
		</author>
		<idno>abs/1905.09904</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Missing value imputation on multidimensional time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Parikshit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prathamesh</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunita</forename><surname>Sarawagi</surname></persName>
		</author>
		<idno>abs/2103.01600</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Nrtsi: Non-recurrent time series imputation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junier</forename><forename type="middle">B</forename><surname>Oliva</surname></persName>
		</author>
		<idno>abs/2102.03340</idno>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Inference and missing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="581" to="592" />
			<date type="published" when="1976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Statistical Analysis with Missing Data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Roderick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Little</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Donald B Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986" />
			<publisher>John Wiley &amp; Sons, Inc., USA</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Venkatraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hebert</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improving multi-step prediction of learned time series models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015-02" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">,</forename><forename type="middle">S</forename><surname>Zhengping Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Gaussian process prior variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Paolo Casale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Dalca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Saglietti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><surname>Listgarten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolo</forename><surname>Fusi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Taylor</surname></persName>
		</author>
		<title level="m">Cloze Procedure: A new tool for measuring readability. Journalism &amp; Mass Communication Quarterly</title>
		<imprint>
			<date type="published" when="1953" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="415" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">XLNet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">DISAN: Directional self-attention network for rnn/cnn-free language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Fast and accurate deep bidirectional language representations for unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joongbo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonhyung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghyun</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyomin</forename><surname>Jung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-07" />
			<biblScope unit="page" from="823" to="835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Layer normalization. ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Kaiming He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Cautionary tales on air-quality improvement in beijing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anlan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziping</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences</title>
		<imprint>
			<biblScope unit="page">473</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheeru</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casey</forename><surname>Graff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Hausdorff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mietus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stanley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Physiobank, physiotoolkit, and physionet: components of a new research resource for complex physiologic signals</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="215" to="235" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>H. Wallach, H. Larochelle, A. Beygelzimer, F. dtextquotesingle Alch?-Buc, E. Fox, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Estimation of the limit of detection in semiconductor gas sensors through linearized calibration models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Burgu?s</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Manuel</forename><surname>Jim?nez-Soto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Marco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Analytica Chimica Acta</title>
		<imprint>
			<biblScope unit="volume">1013</biblScope>
			<biblScope unit="page" from="13" to="25" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
