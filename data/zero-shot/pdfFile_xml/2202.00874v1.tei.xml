<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HTS-AT: A HIERARCHICAL TOKEN-SEMANTIC AUDIO TRANSFORMER FOR SOUND CLASSIFICATION AND DETECTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego 2 AI Lab</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Bytedance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Du</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilei</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejun</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego 2 AI Lab</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Bytedance Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Dubnov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<addrLine>San Diego 2 AI Lab</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Bytedance Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">HTS-AT: A HIERARCHICAL TOKEN-SEMANTIC AUDIO TRANSFORMER FOR SOUND CLASSIFICATION AND DETECTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Audio Classification</term>
					<term>Sound Event De- tection</term>
					<term>Transformer</term>
					<term>Token-Semantic Module</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Audio classification is an important task of mapping audio samples into their corresponding labels. Recently, the transformer model with self-attention mechanisms has been adopted in this field. However, existing audio transformers require large GPU memories and long training time, meanwhile relying on pretrained vision models to achieve high performance, which limits the model's scalability in audio tasks. To combat these problems, we introduce HTS-AT: an audio transformer with a hierarchical structure to reduce the model size and training time. It is further combined with a token-semantic module to map final outputs into class featuremaps, thus enabling the model for the audio event detection (i.e. localization in time). We evaluate HTS-AT on three datasets of audio classification where it achieves new state-of-the-art (SOTA) results on AudioSet and ESC-50, and equals the SOTA on Speech Command V2. It also achieves better performance in event localization than the previous CNN-based models. Moreover, HTS-AT requires only 35% model parameters and 15% training time of the previous audio transformer. These results demonstrate the high performance and high efficiency of HTS-AT.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Audio classification is an audio retrieval task which aims to learn a mapping from audio samples to their corresponding labels. Depending on the audio categories, it involves sound event detection <ref type="bibr" target="#b0">[1]</ref>, music instrument classification <ref type="bibr" target="#b1">[2]</ref>, among others. It establishes a foundation for many downstream applications including music recommendation <ref type="bibr" target="#b2">[3]</ref>, keyword spotting <ref type="bibr" target="#b3">[4]</ref>, music generation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref>, etc.</p><p>With burgeoning research in the field of artificial intelligence, we have seen significant promising progress in audio classification. For data collections, many datasets with different types of audio (e.g. AudioSet <ref type="bibr" target="#b6">[7]</ref>, ESC-50 <ref type="bibr" target="#b7">[8]</ref>, Speech Command <ref type="bibr" target="#b3">[4]</ref>, etc.) provide platforms for the training and evaluation of models on different subtasks. For the model design, the audio classification task is thriving based on neural-network-based models. Convolutional neural networks (CNNs) have been widely used in this field, such as DeepResNet <ref type="bibr" target="#b8">[9]</ref>, TALNet <ref type="bibr" target="#b9">[10]</ref>, PANN <ref type="bibr" target="#b10">[11]</ref>, and PSLA <ref type="bibr" target="#b11">[12]</ref>. These models leverage CNN to capture features on the audio spectrogram, and further improve their performance through the design of the depth and breadth of the network. Recently, by introducing the transformer structure <ref type="bibr" target="#b12">[13]</ref> into audio classification, the audio spectrogram transformer (AST) <ref type="bibr" target="#b13">[14]</ref> further achieves the best performance through the selfattention mechanism and the pretrained model from computer vision. In this paper, we take a further step on a transformerbased audio classification model by first analyzing remaining problems in the AST.</p><p>First, since the transformer takes the audio spectrogram as a complete sequential data, AST takes a long time to train and consumes large GPU memories. In practice, it takes about one week to train on the full AudioSet with four 12GB GPUs. One method to boost training speed is to use the ImageNet <ref type="bibr" target="#b14">[15]</ref> pretrained model in computer vision. However, this also limits the model to those pretrained hyperparameters, which reduces its scalability in more audio tasks. Indeed, we find that without pretraining, AST can only achieve the baseline performance (mAP=0.366 on AudioSet), which raises our attention to its learning efficiency on the audio data. Second, AST uses a class-token (CLS) to predict labels, making it unable to predict the start and end time of events in audio samples. Most CNN-based models naturally support the frame-level localization by empirically taking the penultimate layer's output as a event presence map. This inspires us to design a module that makes every output token of an audio transformer aware of the semantic meaning of events (i.e. a token-semantic module <ref type="bibr" target="#b15">[16]</ref>) for supporting more audio tasks (e.g. sound event detection and localization).</p><p>In this paper, we propose HTS-AT 1 , a hierarchical audio transformer with a token-semantic module for audio classification. Our contributions of HTS-AT can be listed as:</p><p>? HTS-AT achieves or equals SOTAs on AudioSet and ESC-50, and Speech Command V2 datasets. Moreover, the model without pretraining can still achieve the performance that is only 1%-2% lower than the best results.</p><p>? HTS-AT takes fewer parameters (31M vs. 87M), fewer GPU memories, and less training time (80 hrs vs. 600 hrs) than AST's to achieve the best performance.  ? HTS-AT further enables the audio transformer to produce the localization results of event only with weakly-labeled data. And it achieves a better performance than the previous CNN-based model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROPOSED MODEL</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Hierarchical Transformer with Window Attention</head><p>A typical transformer structure consumes lots of GPU memories and training time, because the length of input tokens is too long and remains unchanged in all transformer blocks from beginning to end. As a result, the machine saves the output and its gradient of each block via large GPU memories, and spends much calculation time maintaining a large global self-attention matrix. To combat these problems, as depicted in <ref type="figure" target="#fig_0">Figure 1</ref>, we propose two key designs: a hierarchical transformer structure and a window attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Encode the Audio Spectrogram</head><p>In the left of <ref type="figure" target="#fig_0">Figure 1</ref>, an audio mel-spectrogram is cut into different patch tokens with a Patch-Embed CNN of kernel size (P ? P ) and sent into the transformer in order. Different from images, the width and the height of an audio melspectrogram denote different information (i.e. the time and the frequency bin). And the length of time is usually much longer than that of frequency bins. Therefore, to better capture the relationship among frequency bins of the same time frame, we first split the mel-spectrogram into patch windows w 1 , w 2 , ..., w n and then split the patches inside each window. The order of tokens follows time?frequency?window as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. With this order, patches with different frequency bins at the same time frame will be organized adjacently in the input sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Patch-Merge and Window Attention</head><p>In the middle of <ref type="figure" target="#fig_0">Figure 1</ref>, the patch tokens are sent into several groups of transformer-encoder blocks. At the end of each group, we implement a Patch-Merge layer <ref type="bibr" target="#b16">[17]</ref> to reduce the sequence size. This merge operation is applied by first reshaping the sequence to its original 2D map ( T P ? F P , D), where D is the latent state dimension. Then it merges adjacent patches as ( T 2P ? F 2P , 4D) and finally applies a linear layer to reduce the latent dimension to ( T 2P ? F 2P , 2D). As illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>, the shape of the patch tokens is reduced by 8 times from</p><formula xml:id="formula_0">( T P ? F P , D) to ( T 8P ? F 8P , 8D</formula><p>) after 4 network groups, thus the GPU memory consumption is reduced exponentially after each group.</p><p>For each transformer block inside the group, we adopt a window attention mechanism to reduce the calculation. As shown in different color boxes in the middle right of <ref type="figure" target="#fig_0">Figure  1</ref>, we first split the patch tokens (in 2D format) into nonoverlapping (M ?M ) attention windows aw 1 , aw 2 , ..., aw k . Then we only compute the attention matrix inside each M ? M attention window. As a result, we have k window attention (WA) matrices instead of a whole global attention (GA) matrix. The computational complexities of these two mechanisms in one transformer block for f ? t audio patch tokens with the initial latent dimension D are:</p><formula xml:id="formula_1">GA: O(f tD 2 + (f t) 2 D)<label>(1)</label></formula><p>WA:</p><formula xml:id="formula_2">O(f tD 2 + M 2 f tD)<label>(2)</label></formula><p>where the window attention reduces the second complexity term by ( f t M 2 ) times. For audio patch tokens in a timefrequency-window order, each window attention module will calculate the relation in a certain range of continuous frequency bins and time frames. As the network goes deeper, the Patch-Merge layer will merge adjacent windows, thus the attention relation is calculated in a larger space. In the code implementation, we use the swin transformer block with a shifted window attention <ref type="bibr" target="#b16">[17]</ref>, a more efficient window attention mechanism. This also helps us to use the swin transformer pretrained vision model in the experiment stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Token Semantic Module</head><p>The existing AST uses a class-token (CLS) to predict the classification label, which limits it from further indicating the start and end times of events as realized in CNN-based models. In the final layer output, each token contains information about its corresponding time frames and frequency bins. We expect to convert tokens into activation maps for each labelclass (i.e. aware of semantic meaning <ref type="bibr" target="#b15">[16]</ref>). For strong-label datasets, we can let the model directly calculate the loss in specific time ranges. For weakly-labeled datasets, we can Model Pretrain #Params. mAP Ensemble-mAP Baseline <ref type="bibr" target="#b6">[7]</ref> 2.6M 0.314 -DeepRes <ref type="bibr" target="#b8">[9]</ref> 26M 0.392 -PANN <ref type="bibr" target="#b10">[11]</ref> 81M 0.434 -PSLA P <ref type="bibr" target="#b11">[12]</ref> 13.6M 0.444 0.474 AST <ref type="bibr" target="#b13">[14]</ref> 87M 0.366 -AST P <ref type="bibr" target="#b13">[14]</ref> 87M 0.459 0.475 (0.485 2 ) HTS-AT H 28.8M 0.440 -HTS-AT HC 31M 0.453 -HTS-AT HCP 31M 0.471 0.487 <ref type="table">Table 1</ref>: The mAP results on AudioSet evaluation set.</p><p>leverage the transformer to locate via its strong capability to capture the relation. In HTS-AT, as shown in the right of <ref type="figure" target="#fig_0">Figure 1</ref>, we modify the output structure by adding a tokensemantic CNN layer after the final transformer block. It has a kernel size (3, F 8P ) and a padding size (1, 0) to integrate all frequency bins and map the channel size 8D into the event classes C. The output ( T 8P , C) is regarded as a event presence map. Finally, we average the featuremap as the final vector (1, C) to compute the binary cross-entropy loss with the groundtruth labels. Apart from the localization functionality, we also expect the token-semantic module to improve the classification performance, as it considers the final output by directly grouping all tokens .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">EXPERIMENTS</head><p>In this section, we evaluate the performance of HTS-AT in four datasets: the event classification on AudioSet <ref type="bibr" target="#b6">[7]</ref>, ESC-50 <ref type="bibr" target="#b7">[8]</ref>; the keyword spotting on Speech Command V2 <ref type="bibr" target="#b3">[4]</ref>; and additionally, the event detection on DESED <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Event Classification on AudioSet</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Dataset and Training Detail</head><p>The AudioSet contains over two million 10-sec audio samples labeled with 527 sound event classes. In this paper, we follow the same training pipeline in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14]</ref> by using the full-train set (2M samples) to train our model and evaluating it on the evaluation set (22K samples). All samples are converted to mono as 1 channel by 32kHz sampling rate. We use 1024 window size, 320 hop size, and 64 mel-bins to compute STFTs and mel-spectrograms. As a result, the shape of the mel-spectrogram is (1024, 64) as we pad each 1000-frame (10-sec) sample with 24 zero-frames (T =1024, F =64). The shape of the output featuremap is (1024, 527) (C=527). The patch size is 4 ? 4, the patch window length is 256 frames, and the attention window size is 8 ? 8. Since 8 is divisible by 64, the attention window in the first layer will not span two frames with a large time difference. The latent dimension size is D=96 and the final output latent dimension is 8D=768, <ref type="bibr" target="#b1">2</ref> AST provides a second bigger ensemble result by using models with different patch settings, which is partially comparable with our settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>ESC-50 Acc.(%) Model SCV2 Acc.(%) PANN <ref type="bibr" target="#b10">[11]</ref> 90.5 RES-15 <ref type="bibr" target="#b20">[21]</ref> 97.0 AST <ref type="bibr" target="#b13">[14]</ref> 95.6 ? 0.4 AST <ref type="bibr" target="#b13">[14]</ref> 98.1 ? 0.05 ERANN <ref type="bibr" target="#b21">[22]</ref> 96.1 KWT-2 <ref type="bibr" target="#b22">[23]</ref> 97.3 ? 0.03 HTS-AT 97.0 ? 0.2 HTS-AT 98.0 ? 0.03 <ref type="table">Table 2</ref>: The accuracy score results on ESC-50 dataset and Speech Command V2 (SCV2).</p><p>which is consistent to AST. Finally, we set 4 network groups with 2, 2, 6, 2 swin-transformer blocks respectively. We follow <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> to use the balance sampler, ? = 0.5 mix-up <ref type="bibr" target="#b18">[19]</ref>, spectrogram masking <ref type="bibr" target="#b19">[20]</ref> with time-mask=128 frames and frequency-mask=16 bins, and weight averaging. The HTS-AT is implemented in Pytorch and trained via the AdamW optimizer (? 1 =0.9, ? 2 =0.999, eps=1e-8, de-cay=0.05) with a batch size of 128 (32 ? 4) in 4 NVIDIA Tesla V-100 GPUs. We apply a warm-up schedule by setting the learning rate as 0.05, 0.1, 0.2 in the first three epochs, then the learning rate is halved every ten epochs until it returns to 0.05. We use the mean average precision (mAP) to evaluate the classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">Experimental Results</head><p>In <ref type="table">Table 1</ref>, we compare our HTS-AT with different benchmark models and three self-ablated variations: (1) H: only hierarchical structure; (2) HC: with hierarchical structure and token-semantic module; and (3) HCP : (2) with pretrained vision model (the full setting). Our best setting achieves a new SOTA mAP 0.471 in a single model as a large increment from 0.459 by AST. We also ensemble six HTS-ATs with different training random seeds in the same settings to achieve the mAP as 0.487, and outperforms AST's 0.475 and 0.485. We analyze our results in two facets.</p><p>Token Semantic Module and Pretraining PSLA, AST and HTS-AT adopt the ImageNet-pretrained model, where PSLA uses the pretrained EfficientNet <ref type="bibr" target="#b24">[25]</ref>, AST uses DeiT <ref type="bibr" target="#b25">[26]</ref>, and our HTS-AT uses the swin-transformer in Swin-T/C24 setting 3 for 256 ? 256 images (256 ? 256 = 1024 ? 64 as we could transfer the same size weights). We can see that the unpretrained single HTS-AT can achieve an mAP as 0.440. It is improved to 0.453 by the addition of token semantic module, 1.8% lower than 0.471. Finally the pretrained HTS-AT achieves the new best mAP as 0.471. However, the unpretrained single AST only reflects 0.366, 9.3% lower than 0.459. These indicate that: (1) the pretrained model definitely improves the performance by building a solid prior on pattern recognition; and (2) HTS-AT shows a far better scalability to different hyperparameters than AST, since its unpretrained model can still achieve the third best performance.   <ref type="bibr" target="#b23">[24]</ref>, which are partial references since they use extra training data and are evaluated on DESED test set and its another private subset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter Size and Training Time</head><p>ters. And HTS-AT is more lightweight with 31M parameters, which is even compatible with CNN-based models. As for the estimated training time, PANN takes about 72 hours to converge and HST-AT takes about 20 ? 4 = 80 hours in V-100 GPUs; and AST takes about 150 ? 4 = 600 hours in 4 TITAN RTX GPUs 4 . The speed improvement corresponds to the less calculation and GPU memory consumption of HTS-AT, as we could feed 128 samples instead of only 12 samples in AST per batch. Therefore, we conclude that HTS-AT consumes less training time and has fewer parameters than AST's, which is more efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Evaluations on ESC-50 and Speech Command V2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Dataset and Training Detail</head><p>The ESC-50 dataset contains 2000 5-sec audio samples labeled with 50 environmental sound classes in 5 folds. We train the model for 5 times by selecting 4-fold (1600 samples) as training set and the left 1-fold (400 samples) as test set. And we repeat this experiment 3 times with different random seeds to get the mean performance and deviation. The Speech Command V2 contains 105,829 1-sec spoken word clips labled with 35 common word classes. It contains 84843, 9981, and 11005 clips for training, validation and evaluation. Similarly, we train our HTS-AT for 3 times to obtain the prediction results. We use the mean accuracy score (acc) for the evaluation on both datasets. For the data processing, we resample the ESC-50 samples into 32kHz and the Speech Command clips 16kHz. And we follow the same setting of Au-dioSet to train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Experimental Results</head><p>We use our best AudioSet-pretrained HTS-AT to train on these two dataset respectively and compare it with benchmark models (also in AudioSet or extra data pretraining). Since 1-sec and 5-sec does not take the full 10-sec input trained on AudioSet, we repeat the 1-sec and 5-sec by 10 and 2 times to make it 10-sec. As shown in <ref type="table">Table 2</ref>, the results shows that our HTS-AT achieves a new SOTA as 97.0% on ESC-50 dataset and equals the SOTA 98.0% on Speech Command V2. Our deviations are relatively smaller than AST's, indicating that HTS-AT is more stable after convergence. <ref type="bibr" target="#b3">4</ref> We make memories not exceed 12GB in V-100 in line with TITAN RTX.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Localization Performance on DESED</head><p>We additionally evaluate HTS-AT's capability to localize the sound event as start and end time in given audio samples.</p><p>We use the DESED test set <ref type="bibr" target="#b17">[18]</ref>, which contains 692 10-sec test audio samples in 10 classes with the strong labels. We mainly compare our HTS-AT with PANN. We do not include AST and PSLA since AST does not directly support the event localization and the PSLA's code is not published. We also compare it partially with models in DCASE 2021 <ref type="bibr" target="#b23">[24]</ref>, nevertheless they use extra training data and are evaluated on DESED test set and its another private subset. We use the event-based F1-score on each class as the evaluation metric, implemented by a Python library psds eval 5 . The F1-scores on all 10 classes in the DESED by different models are shown in <ref type="table" target="#tab_2">Table 3</ref>. We find that HTS-AT achieves better F1-scores on 8 classes and a better average F1-score 50.7% than PANN. When compared among leaderboard models, our model still achieves some highest scores of certain classes. However, the F1-scores on Speech and Cleaner are relatively low, indicating that there are still some improvements for a better localization performance. From the above experiments, we can conclude that HTS-AT is able to produce the specific localization output via the token-semantic module, which extends the functionality of the audio transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">CONCLUSION AND FUTURE WORK</head><p>In this paper, we propose HTS-AT: a hierarchical tokensemantic transformer for audio classification. It achieves a new SOTA on multiple datasets of different audio classification scenarios. Furthermore, the token-semantic module enables HTS-AT to locate the events start and end time. Experiments show that HTS-AT is a high performance, high scalability, and lightweight audio transformer. In the future, we notice that a partial strong labeled subset of AudioSet has just been released <ref type="bibr" target="#b26">[27]</ref>, we decide to conduct a detail localization training and evaluation work by HTS-AT to further explore its potential. Combining the audio classification model into more downstreaming tasks <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b28">29]</ref> is also considered a future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>The model architecture of HTS-AT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>When comparing the parameter size of each model, the AST has 87M parame-</figDesc><table><row><cell>Model</cell><cell cols="11">Alarm Blender Cat Dishes Dog Shaver Frying Water Speech Cleaner Average</cell></row><row><cell>PANN [11]</cell><cell>34.3</cell><cell>42.4</cell><cell>36.3</cell><cell>17.6</cell><cell>35.8</cell><cell>23.8</cell><cell>9.3</cell><cell>30.6</cell><cell>69.7</cell><cell>51.0</cell><cell>35.1</cell></row><row><cell>HTS-AT</cell><cell>48.6</cell><cell>52.9</cell><cell>67.7</cell><cell>25.0</cell><cell>48.0</cell><cell>42.9</cell><cell>60.3</cell><cell>43.0</cell><cell>46.8</cell><cell>49.1</cell><cell>48.4</cell></row><row><cell>HTS-AT -Ensemble</cell><cell>47.5</cell><cell>55.1</cell><cell>72.4</cell><cell>30.9</cell><cell>49.7</cell><cell>41.9</cell><cell>63.2</cell><cell>44.3</cell><cell>51.3</cell><cell>50.6</cell><cell>50.7</cell></row><row><cell>Zheng et al.* [24]</cell><cell>41.4</cell><cell>54.1</cell><cell>72.4</cell><cell>29.4</cell><cell cols="2">47.8 61.01</cell><cell>49.2</cell><cell>33.7</cell><cell>69.5</cell><cell>65.5</cell><cell>52.4</cell></row><row><cell>Kim et al.* [24]</cell><cell>34.7</cell><cell>59.8</cell><cell>71.6</cell><cell>40.4</cell><cell>47.3</cell><cell>26.2</cell><cell>61.8</cell><cell>32.8</cell><cell>64.9</cell><cell>66.7</cell><cell>50.6</cell></row><row><cell>Lu et al.* [24]</cell><cell>37.1</cell><cell>41.4</cell><cell>62.5</cell><cell>40.6</cell><cell>39.7</cell><cell>46.5</cell><cell>46.5</cell><cell>34.5</cell><cell>54.5</cell><cell>46.9</cell><cell>45.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>The event-based F1-scores of each class on the DESED test set. Models with * are from DCASE 2021</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/RetroCirce/HTS-Audio-Transformer arXiv:2202.00874v1 [cs.SD] 2 Feb 2022</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/microsoft/Swin-Transformer</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/audioanalytic/psds eval</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sound event detection: A tutorial</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Annamaria</forename><surname>Mesaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toni</forename><surname>Heittola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuomas</forename><surname>Virtanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">D</forename><surname>Plumbley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process. Mag</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Automatic classification of musical instrument sounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Perfecto</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffroy</forename><surname>Peeters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Dubnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of New Music Research</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning audio embeddings with user listening data for content-based music recommendation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beici</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minwei</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Speech commands: A dataset for limited-vocabulary speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<idno>abs/1804.03209</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Music sketchnet: Controllable music generation via factorized representations of pitch and rhythm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-I</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Dubnov</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Muspy: A toolkit for symbolic music generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Hao-Wen Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Audio set: An ontology and human-labeled dataset for audio events</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dylan</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Freedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">ESC: dataset for environmental sound classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karol</forename><forename type="middle">J</forename><surname>Piczak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM 2015</title>
		<imprint>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">A deep residual network for large-scale acoustic scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Logan</forename><surname>Ford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Grondin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A comparison of five multiple instance learning pooling functions for sound event detection with weak labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Panns: Large-scale pretrained audio neural networks for audio pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuqiang</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Turab</forename><surname>Iqbal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Psla: Improving audio tagging with pretraining, sampling, labeling, and aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TASLP</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Ast: Audio spectrogram transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ts-cam: Token semantic coupled attention map for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjia</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ze</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<idno>abs/2103.14030</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Sound event detection in synthetic domestic environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Serizel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Turpault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit Parag</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Salamon</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Ciss?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning efficient representations for keyword spotting with triplet loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Vygon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolay</forename><surname>Mikhaylovskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SPECOM 2021</title>
		<imprint>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Eranns: Efficient residual audio neural networks for audio pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Verbitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viacheslav</forename><surname>Vyshegorodtsev</surname></persName>
		</author>
		<idno>abs/2106.01621</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Keyword transformer: A self-attention model for keyword spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Axel</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel Tairum</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cruz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Interspeech</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Dcase 2021 challenge task 4: Sound event detection and separation in domestic environments</title>
		<ptr target="http://dcase.community/challenge2021" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficientnet: Rethinking model scaling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2019. PMLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Training data-efficient image transformers &amp; distillation through attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The benefit of temporally-strong labels in audio event classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shawn</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduardo</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fonseca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2021</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Zero-shot audio source separation through query-based learning from weakly-labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjian</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bilei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Dubnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2022</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Continuous melody generation via disentangled short-term representations and structural conditions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gus</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shlomo</forename><surname>Dubnov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
