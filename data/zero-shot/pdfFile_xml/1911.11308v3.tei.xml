<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Graph Matching Network: Learning Lawler&apos;s Quadratic Assignment Problem with Extension to Hypergraph and Multiple-graph Matching</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="20151">AUGUST 2015 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Journal Of L A T E X Class</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Files</surname></persName>
						</author>
						<title level="a" type="main">Neural Graph Matching Network: Learning Lawler&apos;s Quadratic Assignment Problem with Extension to Hypergraph and Multiple-graph Matching</title>
					</analytic>
					<monogr>
						<imprint>
							<biblScope unit="volume">14</biblScope>
							<biblScope unit="issue">8</biblScope>
							<date type="published" when="20151">AUGUST 2015 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Graph Matching</term>
					<term>Deep Learning</term>
					<term>Quadratic Assignment Problem</term>
					<term>Combinatorial Optimization</term>
					<term>Graph Neural Networks !</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph matching involves combinatorial optimization based on edge-to-edge affinity matrix, which can be generally formulated as Lawler's Quadratic Assignment Problem (QAP). This paper presents a QAP network directly learning with the affinity matrix (equivalently the association graph) whereby the matching problem is translated into a constrained vertex classification task. The association graph is learned by an embedding network for vertex classification, followed by Sinkhorn normalization and a cross-entropy loss for end-to-end learning. We further improve the embedding model on association graph by introducing Sinkhorn based matching-aware constraint, as well as dummy nodes to deal with unequal sizes of graphs. To our best knowledge, this is one of the first network to directly learn with the general Lawler's QAP. In contrast, recent deep matching methods focus on the learning of node/edge features in two graphs respectively. We also show how to extend our network to hypergraph matching, and matching of multiple graphs. Experimental results on both synthetic graphs and real-world images show its effectiveness. For pure QAP tasks on synthetic data and QAPLIB benchmark, our method can perform competitively and even surpass state-of-the-art graph matching and QAP solvers with notable less time cost. We provide a project homepage at http://thinklab.sjtu.edu.cn/project/NGM/index.html.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION AND PRELIMINARIES</head><p>G Raph matching (GM) is a fundamental problem which is NP-complete in general <ref type="bibr" target="#b2">[1]</ref>. It has various applicability and connection with vision and learning, which involves establishing node correspondences between two graphs based on the node-to-node and edge-to-edge affinity <ref type="bibr" target="#b3">[2]</ref>, <ref type="bibr" target="#b4">[3]</ref>. This differs from the point-based techniques e.g. RANSAC <ref type="bibr" target="#b5">[4]</ref> and iterative closest point (ICP) <ref type="bibr" target="#b6">[5]</ref> without considering edge information.</p><p>We start with two-graph matching, which can be written as quadratic assignment programming (QAP) <ref type="bibr" target="#b7">[6]</ref>, where X ? R n1?n2 is a (partial) permutation matrix encoding node-to-node correspondence (with constraints in second line of Eq. (1)), and vec(X) is its column-vectorized version:</p><formula xml:id="formula_0">J(X) = vec(X) Kvec(X) s.t. X ? {0, 1} n1?n2 , X1 n2 = 1 n1 , X 1 n1 ? 1 n2<label>(1)</label></formula><p>Here 1 n means column vector of length n whose elements all equal to 1, and K ? R n1n2?n1n2 is the so-called affinity matrix <ref type="bibr" target="#b8">[7]</ref>. Its diagonal and off-diagonal elements store the node-to-node and edge-to-edge affinities. For graph matching, the objective J(X) is maximized, assuming perfect matching corresponds to the highest affinity score. One Koopmans-Beckmann's QAP Lawler's QAP learning-free <ref type="bibr" target="#b10">[9]</ref>, <ref type="bibr" target="#b11">[10]</ref>, <ref type="bibr" target="#b12">[11]</ref>, <ref type="bibr" target="#b13">[12]</ref> [2], <ref type="bibr" target="#b8">[7]</ref>, <ref type="bibr" target="#b14">[13]</ref>, <ref type="bibr" target="#b15">[14]</ref>, <ref type="bibr" target="#b16">[15]</ref> learning-based <ref type="bibr" target="#b17">[16]</ref>, <ref type="bibr" target="#b18">[17]</ref> ours popular embodiment of K is fixed Gaussian kernel with Euclid distance over pairs of edge features <ref type="bibr" target="#b3">[2]</ref>, <ref type="bibr" target="#b9">[8]</ref>:</p><formula xml:id="formula_1">K ia,jb = exp ||f ij ? f ab || 2 ? 2<label>(2)</label></formula><p>where f ij is the feature vector of the edge E ij in graph 1, f ab from edge E ab in graph 2. K ia,jb is indexed by (an 1 + i, bn 1 + j) under its matrix form. Note Eq. (1) in literature is called Lawler's QAP <ref type="bibr" target="#b19">[18]</ref>, which can incorporate other special forms. For instance, the popular Koopmans-Beckmann's QAP <ref type="bibr" target="#b20">[19]</ref> is written by:</p><formula xml:id="formula_2">J(X) = tr(X F 1 XF 2 ) + tr(K p X)<label>(3)</label></formula><p>where F 1 ? R n1?n1 , F 2 ? R n2?n2 are weighted adjacency matrices. K p ? R n1?n2 is node-to-node affinity matrix. Its connection to Lawler's QAP becomes clear by letting K = F 2 ? K F 1 (? K means Kronecker product). As shown in Tab. 1, traditional learning-free solvers have been extensively studied for both QAP formulations in Eq.  <ref type="figure">Fig. 1</ref>. Overview of the proposed neural graph matching pipeline. The method directly handles Lawler's QAP based on the embedding on the association graph. We further extend it to hypergraph matching by replacing the association graph with an association hypergraph (see Sec. <ref type="bibr" target="#b4">3</ref>.3), as well as to multiple graph matching by differentiable spectral multi-matching (see Sec. 3.4).</p><p>Koopmans-Beckmann's QAP <ref type="bibr" target="#b17">[16]</ref>, <ref type="bibr" target="#b18">[17]</ref>. In this paper, we propose the first learning-based algorithm tackling the most general QAP form -Lawler's QAP, and show its generalization to higher-order and multi-graph scenarios. The above QAP models involve the second-order affinity, and can also be generalized to the higher-order case. A line of works <ref type="bibr" target="#b21">[20]</ref>, <ref type="bibr" target="#b22">[21]</ref>, <ref type="bibr" target="#b23">[22]</ref>, <ref type="bibr" target="#b24">[23]</ref> adopt tensor marginalization based model for m-order (m ? 3) hypergraph matching, resulting in a higher-order assignment problem:</p><formula xml:id="formula_3">J(x) = H ? 1 x ? 2 x . . . ? m x s.t. X1 n2 = 1 n1 , X 1 n1 ? 1 n2<label>(4)</label></formula><p>where x = vec(X) ? {0, 1} n1n2?1 is the column-vectorized form, and H is the m-order affinity tensor whose (n 1 n 2 ) m elements record the affinity between two hyperedges, operated by tensor product ? k <ref type="bibr" target="#b25">[24]</ref>:</p><p>(H ? k x) ...,i k?1 ,i k+1 ,... = n1n2 i k =1 H ...,i k?1 ,i k ,i k+1 ,... ? x i k <ref type="bibr" target="#b6">(5)</ref> where ? k can be regarded as tensor marginalization at dimension k. Details of tensor multiplication can be referred to Sec. 3.1 in <ref type="bibr" target="#b22">[21]</ref>. Most existing hypergraph matching works assume the affinity tensor is invariant w.r.t. the index of the hyperedge pairs for computational tractability. As discussed above, either graph matching or hypergraph matching problem involves solving a combinatorial optimization problem. However, the objective functions may be biased and even the mathematically optimal solution can depart from the perfect matching in reality, either due to the noise observation or limited modeling capacity, or both. In fact, traditional methods are mostly based on predefined shallow affinity with limited capacity e.g. Gaussian kernel with Euclid distance (see Eq. (2)), which has difficulty in providing enough flexibility for real-world data. This issue is partially addressed by affinity-learning based graph matching <ref type="bibr" target="#b18">[17]</ref>, <ref type="bibr" target="#b26">[25]</ref>, <ref type="bibr" target="#b27">[26]</ref>. Along this promising direction, in this paper, a novel network based solver is proposed to directly learn Lawler's QAP whereby the affinity learning is also incorporated, as shown in <ref type="figure">Fig. 1</ref>. This approach is further extended to the case of joint matching of multiple graphs, which has been an important scenario in practice and has received wide attention in literature <ref type="bibr" target="#b9">[8]</ref>, <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b29">[28]</ref>, <ref type="bibr" target="#b30">[29]</ref>, <ref type="bibr" target="#b31">[30]</ref> however learning has not been considered. Also hypergraph matching is enabled in our framework.</p><p>Specifically, the proposed matching nets consist of several learnable layers as detailed in <ref type="figure" target="#fig_4">Fig. 4</ref>: 1) CNN layers taking raw images for node (and edge) feature extraction; 2) Spline convolution layers encoding geometric features to node (and edge) features; 3) affinity metric learning layer for generating the affinity matrix i.e. the association graph; 4) vertex embedding layers using the association graph as input for vertex classification; 5) Sinkhorn net to convert the vertex score matrix into doubly-stochastic matrix. Sinkhorn technique is also adopted in the embedding module to introduce matching constraints; 6) cross-entropy loss layer whose input is the output of the Sinkhorn layer.</p><p>Note that the first two components are optional and can be treated as a plugin in the pipeline, and have nothing to do with Lawler's QAP. In contrast, the peer network <ref type="bibr" target="#b26">[25]</ref> only allows for learning of node CNN features on images and their similarity metric i.e. the component 1) and 2). In fact, <ref type="bibr" target="#b26">[25]</ref> is inapplicable to learning the QAP model. Our embedding also differs from <ref type="bibr" target="#b18">[17]</ref>, <ref type="bibr" target="#b32">[31]</ref>, <ref type="bibr" target="#b33">[32]</ref> as raw individual graphs must be required for embedding in these works. In fact, <ref type="bibr" target="#b17">[16]</ref> shows that embedding on individual graphs can deal with some special cases of Koopmans-Beckmann's QAP, which is also a special case of Lawler's QAP as discussed above. Direct learning on Lawler's QAP enables a learning-based solver for real-world combinatorial problems beyond vision, e.g. QAPLIB problem instances <ref type="bibr" target="#b34">[33]</ref>, which can not be readily handled by previous graph matching learning algorithms.</p><p>Furthermore, we devise two generalizations to the above matching network. 1) hypergraph matching by embedding a higher-order affinity tensor; 2) multiple graph matching by devising an end-to-end compatible matching synchronization module by using the popular spectral fusion technique <ref type="bibr" target="#b30">[29]</ref>, <ref type="bibr" target="#b35">[34]</ref>. The performance can also be boosted by adopting edge-embedding layers.</p><p>The highlights of this paper are summarized as follows: i) We show how to develop a deep network to directly tackle the (most) general graph matching formulation i.e. Lawler's Quadratic Assignment Problem beyond vision problems, in the sense of allowing the affinity matrix as the raw input. This is fulfilled by regarding the affinity matrix as an association graph, whose vertices can be embedded by a deep graph neural network (GNN) <ref type="bibr" target="#b40">[39]</ref> for classification, with a novel matching-aware graph convolution scheme. In contrast, existing works <ref type="bibr" target="#b18">[17]</ref>, <ref type="bibr" target="#b26">[25]</ref>, <ref type="bibr" target="#b27">[26]</ref>, <ref type="bibr" target="#b32">[31]</ref> start with individual graphs' node and edge features for affinity learning instead of pairwise affinity encoded in affinity matrix. ii) Our network solver for Lawler's QAP can be trained either in a supervised setting given ground truth node correspondence from labeled training set (e.g. for image matching), or by the final matching score without supervision (e.g. for QAPLIB problems).</p><p>iii) We extend our second-order graph matching networks to the hypergraph (third-order) matching case. This is fulfilled by building the hyperedge based association hypergraph to replace the second-order one. To our best knowledge, this is the first work for deep learning of hypergraph matching (with explicit treatment on the hyperedges). iv) We also extend our matching network to the multiplegraph matching case by end-to-end spectral multi-graph matching, with explicit treatment for stabilized learning. To our best knowledge, there is no end-to-end multiple-graph matching neural network in the existing literature. v) Experimental results on synthetic and real-world data show the effectiveness of our devised components. The extended versions for hypergraph matching and multiplegraph matching also show competitive performance. Our model can learn with the Lawler's QAP as input while state-of-the-art graph matching networks <ref type="bibr" target="#b18">[17]</ref>, <ref type="bibr" target="#b26">[25]</ref>, <ref type="bibr" target="#b32">[31]</ref> cannot. This allows for the evaluation of our network on the QAPLIB benchmark directly, which to our best knowledge, is the first test for network-based methods for QAPLIB.</p><p>The paper goes as follows. Section 2 discusses the related work to graph matching and its recent learning based methods. Section 3 describes the main approach and in Section 4 we discuss the experiments. Section 5 concludes this paper. Source code is made public available on the project page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>We mainly discuss related works and techniques on learning graph matching <ref type="bibr" target="#b41">[40]</ref>. Readers are referred to the survey <ref type="bibr" target="#b42">[41]</ref> for an enlarged overview of the topic of graph matching. Also, a more broad perspective on learning for combinatorial optimization is referred to <ref type="bibr" target="#b43">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Learning-free Graph Matching Methods</head><p>Two-graph matching and QAP. Lawler's Quadratic Assignment Problem <ref type="bibr" target="#b19">[18]</ref> is known for its application of matching two graphs by maximizing a quadratic objective function. Traveling salesman problem (TSP) and Koopmans-Beckmann's QAP are two popular variants from Lawler's QAP, with their wide range of application beyond vision, e.g. economic activities modeled by Koopmans-Beckmann's QAP <ref type="bibr" target="#b20">[19]</ref>, <ref type="bibr" target="#b34">[33]</ref>. The most general Lawler's QAP also refers to two-graph matching in pattern recognition, and is traditionally addressed in a learning-free setting. Classically, small or medium sized problems are tractable by branchand-bound with dual bound solved approximately <ref type="bibr" target="#b10">[9]</ref>, <ref type="bibr" target="#b14">[13]</ref>. Modern approximate solvers <ref type="bibr" target="#b3">[2]</ref>, <ref type="bibr" target="#b8">[7]</ref>, <ref type="bibr" target="#b15">[14]</ref>, <ref type="bibr" target="#b16">[15]</ref>, <ref type="bibr" target="#b44">[43]</ref> achieve better accuracy-speed trade-off and thus are more applicable to larger-sized problems. In two-graph matching, most methods focus on seeking approximate solution given fixed affinity model which is often set in simple parametric forms. Euclid distance in node/edge feature space together with a Gaussian kernel to derive a non-negative similarity, is widely used in the above works.</p><p>Hypergraph matching methods. Going beyond the traditional second-order graph matching, hypergraphs have been built for matching <ref type="bibr" target="#b24">[23]</ref> and their affinity is usually represented by a tensor to encode the third-order <ref type="bibr" target="#b23">[22]</ref>, <ref type="bibr" target="#b25">[24]</ref>, <ref type="bibr" target="#b45">[44]</ref> or even higher-order information <ref type="bibr" target="#b46">[45]</ref>. The advantage is that the model can be more robust against noise at the cost of exponentially increased complexity for both time and space.  We distinguish the nodes on association (hyper)graph and individual graphs for matching by terms vertex and node through this paper. The node-to-node matching problem in (b) can therefore be formulated as the vertex classification task on the association graph whose edge weights can be induced by the affinity matrix. Such a perspective is also widely taken in literature for graph matching e.g. <ref type="bibr" target="#b3">[2]</ref> and hypergraph matching <ref type="bibr" target="#b25">[24]</ref>. (d) shows a toy working example from individual graphs to final matching result: affinity matrix K is built from individual graphs, and the matching problem is equivalent to vertex classification on the association graph. Matching-aware embedding and vertex classification are applied on association graph to generate vertex scores, followed with reshaping and Sinkhorn normalization to obtain a double-stochastic matrix i.e. a convex hull of permutation matrix.</p><p>Multiple-graph matching methods. It has been recently actively studied for its practical utility against local noise and ambiguity. The hope is that the joint matching of multiple graphs can provide a better venue to fuse the information across graphs, leading to better robustness against local noise and ambiguity. Among the literature, a thread of works <ref type="bibr" target="#b29">[28]</ref>, <ref type="bibr" target="#b30">[29]</ref> first generate the pairwise matching between two graphs via certain two-graph matching solvers, and then impose cycle-consistency on the pairwise matchings to improve the matching accuracy. The other line of methods impose cycle-consistency during the iterative finding of pairwise matchings and usually can achieve better results <ref type="bibr" target="#b9">[8]</ref>, <ref type="bibr" target="#b28">[27]</ref>, <ref type="bibr" target="#b47">[46]</ref>, <ref type="bibr" target="#b48">[47]</ref>. The online setting for solving multiple graph matching is studied in <ref type="bibr" target="#b49">[48]</ref>.</p><p>Note both hypergraph or multiple graph matching paradigms try to improve the affinity model either by lifting the affinity order or imposing additional consistency regularization. As shown in the following, another possibly more effective and efficient way is adopting learning to find more adaptive affinity model parameters, or further improving the solver with deep neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Learning-based Graph Matching Methods</head><p>Non-deep learning methods. The structural SVM based supervised learning method <ref type="bibr" target="#b27">[26]</ref> incorporates earlier graph matching learning methods <ref type="bibr" target="#b50">[49]</ref>, <ref type="bibr" target="#b51">[50]</ref>, <ref type="bibr" target="#b52">[51]</ref>. Learning can also be fulfilled by unsupervised <ref type="bibr" target="#b52">[51]</ref> and semi-supervised <ref type="bibr" target="#b53">[52]</ref>. In these earlier works, no neural network is adopted until the recent seminal work <ref type="bibr" target="#b26">[25]</ref>.</p><p>Deep-learning methods. A pioneer work <ref type="bibr" target="#b17">[16]</ref> considers the alignment of graphs by embedding on individual graphs, which can be regarded a special case of Koopmans-Beckmann's QAP. Deep learning is recently applied for graph matching on images <ref type="bibr" target="#b26">[25]</ref>, whereby convolutional neural network (CNN) is used to extract node features from images followed with spectral matching and CNN is learned using a regression-like node correspondence supervision. This work is improved by introducing GNN to encode structural <ref type="bibr" target="#b18">[17]</ref>, <ref type="bibr" target="#b36">[35]</ref> or geometric <ref type="bibr" target="#b32">[31]</ref> information, with a combinatorial loss based on cross-entropy loss, and Sinkhorn network <ref type="bibr" target="#b54">[53]</ref> as adopted in <ref type="bibr" target="#b18">[17]</ref>. Yu et al. <ref type="bibr" target="#b37">[36]</ref> extends <ref type="bibr" target="#b18">[17]</ref> by edge embedding and Hungarian-based attention mechanism to stabilize end-to-end training.</p><p>We also note one recent important work on learning for graph matching, namely Learning Combinatorial Solver (LCS) <ref type="bibr" target="#b33">[32]</ref>. It follows the line of research <ref type="bibr" target="#b3">[2]</ref>, <ref type="bibr" target="#b8">[7]</ref>, <ref type="bibr" target="#b55">[54]</ref> in building an association graph from input images and solves graph matching by vertex classification on association graph. The discussion in <ref type="bibr" target="#b33">[32]</ref> is basically restricted to the image matching setting. Though there is much space to generalize their work to our case, while no explicit scheme is given in terms of addressing the most general Lawler's QAP form. Meanwhile, it is unclear in their method for how to incorporate the matching constraint in the node scoring procedure, which has been addressed by our devised Skinhorn embedding. In fact, our method is directly motivated by developing a general Lawler's QAP solver, beyond image matching. It also enjoys the flexibility of readily adopting better feature extractors e.g. <ref type="bibr" target="#b38">[37]</ref> or those beyond vision. Moreover, our work further develops hypergraph matching and multiple graph matching under the neural learning framework, which are new in literature. All these features are not well explored by LCS <ref type="bibr" target="#b33">[32]</ref>.</p><p>As summarized in Tab. 2 concerning deep neural network-based graph matching algorithms, one shortcoming of existing graph matching networks is that they cannot directly deal with the most general Lawler's QAP form which limits their applicability to tasks when no individual graph information is available (see QAPLIB -http://anjos.mgi.polymtl.ca/qaplib/). In contrast, our method can directly work with the affinity matrix, and we further extend to dealing with affinity tensor for hypergraph matching, as well as the setting under multiple graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROPOSED APPROACHES</head><p>In Sec. 3.1, we introduce the preliminary concept association graph, on which our methods are based. In Sec. 3.2, we present Neural Graph Matching (NGM) network, which can solve Lawler's QAP for two-graph matching directly. Also, we show the extension to hypergraph matching, i.e. Neural Hyper-Graph Matching (NHGM) in Sec. 3.3, and to multiple graph matching i.e. Neural Multi-Graph Matching (NMGM) in Sec. 3.4. All these three settings to our knowledge have been hardly addressed by neural network solvers before. In Sec. 3.5 we introduce NGM/NHGM/NMGM-v2 with significantly improved image matching accuracy by exploiting enhanced feature extractor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminaries</head><p>Our models aim to match weighted graph G 1 = (V 1 , E 1 ) and G 2 = (V 2 , E 2 ) (in capital letters), where the superscript means the index of graphs and the subscript means the index of nodes. Without loss of generality, |V 1 | = n 1 = n in are all inlier nodes, and |V 2 | = n 2 = n in + n out contains both inliers and optional outliers. E 1 , E 2 are attributed edge sets with second-order features in graphs and |E 1 | = n e1 , |E 2 | = n e2 . Lawler's QAP as given in Eq. (1) is relaxed via popular doubly-stochastic relaxation:</p><formula xml:id="formula_4">J(S) = vec(S) Kvec(S),<label>(6)</label></formula><formula xml:id="formula_5">S ? [0, 1] n1?n2 , S1 n2 = 1 n1 , S 1 n1 ? 1 n2</formula><p>where S is a (partial) doubly-stochastic matrix, where all its rows sum to 1 and all its column sums are ? 1.</p><formula xml:id="formula_6">For affinity matrix K ? R n1n2?n1n2 , diagonal elements K ia,ia = s v (V 1 i , V 2 a )</formula><p>are first order (node) similarities and off-diagonal elements K ia,jb = s e (E 1 ij , E 2 ab ) are second order (edge) similarities, where s v , s e are similarity measurements for nodes and edges, respectively.</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 2</ref>, graph matching can be viewed in a perspective based on the definition of the so-called associ- <ref type="bibr" target="#b8">[7]</ref> (in handwritten letters with superscript A). To avoid ambiguity between graphs and association graphs, we name the entities in graphs (V ) as nodes and the entities in association graph (V A ) as vertices. Readers should distinguish these two concepts as they will be repeatedly encountered through this paper.</p><formula xml:id="formula_7">ation graph G A = (V A , E A ) [2],</formula><p>The vertices of association graph</p><formula xml:id="formula_8">V A = V 1 ? V 2 encode candidate node-to-node correspondence V A ia = (V 1 i , V 2 a )</formula><p>corresponding to the matching matrix X i,a , therefore the vectorized assignment matrix vec(X) is equivalent to the vertex set of association graph. The edges E A represent the agreement between two pairs of correspondence</p><formula xml:id="formula_9">E A ia,jb = {(V 1 i , V 2 a ), (V 1 j , V 2 b )} modeled by K ia,jb</formula><p>, so that the offdiagonal part of affinity matrix K is equivalent to the adjacency matrix of association graph. The matching between two graphs can therefore be transformed into vertex classification on the association graph, following <ref type="bibr" target="#b3">[2]</ref>, <ref type="bibr" target="#b8">[7]</ref>. In this paper, diagonal elements K ia,ia are further assigned as vertex attributes V A , to better exploit the first-order similarities. Such formulation can also be generalized to hypergraph matching problems, with edges replaced by hyperedges, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>(c). The association graph prohibits links that violate the one-to-one matching constraint (e.g. there is no link between vertex '1a' and '1c' in <ref type="figure" target="#fig_2">Fig. 2(a)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">NGM: Neural Graph Matching for QAP</head><p>Based on the formulations and association graph introduced in Sec. 3.1, our proposed Neural Graph Matching (NGM) solves relaxed Lawler's QAP in Eq. <ref type="formula" target="#formula_4">(6)</ref>, by vertex classification via Graph Convolutional Networks (GCN) <ref type="bibr" target="#b56">[55]</ref> with novel matching-aware embedding modules. The vertex classification is performed on the association graph induced by the affinity matrix, followed by a Sinkhorn operator. As shown by existing learning-free graph matching solvers <ref type="bibr" target="#b3">[2]</ref>, <ref type="bibr" target="#b8">[7]</ref>, graph matching problem is equivalent to vertex classification on the association graph. NGM accepts either raw image (with jointly learned CNN and affinity metric), or affinity matrix (without CNN or affinity metric), and learns end-to-end from ground truth correspondence or by pure optimization for QAPLIB problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Affinity matrix building from natural images</head><p>Our graph matching net allows either affinity matrix or raw images as input. The image processing module is optional and treated as a plug-in for dealing with images, following the protocol in <ref type="bibr" target="#b26">[25]</ref> whereby the affinity matrix is built from pre-given keypoints in images. As shown in the upper half of <ref type="figure" target="#fig_4">Fig. 4</ref>, image features are extracted by learnable CNN layers such as VGG16 <ref type="bibr" target="#b57">[56]</ref>. Given two input images with labeled keypoints, we adopt CNN layers to extract per-node</p><formula xml:id="formula_10">featuresF 1 ,? 1 ? R n1?d for G 1 andF 2 ,? 2 ? R n2?d for G 2 ,</formula><p>where d is feature dimension size andF,? are extracted from different CNN layers (e.g. VGG16 relu5_1 forF and relu4_2 for?) and utilized for edge representation and node representation, respectively. Features are obtained by bi-linear interpolation on the CNN feature map. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, the connectivity of two graphs are represented b?</p><formula xml:id="formula_11">G 1 ,H 1 ? {0, 1} n1?ne1 and? 2 ,H 2 ? {0, 1} n2?ne2 , wher? A 1 =? 1H1 ,? 2 =? 2H2 are the adjacency matrices relu5_1 relu4_2 VGG16 F _ 1 F _ 2 U _ 1 U _ 2 n 1 512 edge features Eq. (7) X _ Y _ edge similarity matrix Eq. (8) X _ Y _ K e ? T Eq. (8) node similarity matrix K p T U _ 1 U _ 2</formula><p>Eq. (9) n 2 512 n 1 512 n 2 512 n e1 512 n e2 512 n e1 n e2 n 1 n 2 optional CNN extractor for NGM connectivity info K affinity matrix i.e. association graph vertex feature n 1 n 2 n 1 n 2 v 0 n 1 n 2 1 W n 1 n 2 n 1 n 2 edge feature Graph Conv.  of two graphs, and? i,k =H j,k = 1 means edge k links node i to node j. The edge representations are built by concatenating node features at both ends of the edge:</p><formula xml:id="formula_12">X = [? 1F1H 1F1 ],? = [? 2F2H 2F2 ]<label>(7)</label></formula><p>where [ ? ? ] means concatenating two matrices along columns. The node-to-node similarity matrix K p ? R n1?n2 and edge-to-edge similarity K e ? R ne1?ne2 are built via</p><formula xml:id="formula_13">K e =X?? , K p =? 1?2<label>(8)</label></formula><p>where ? ? R 2d?2d is the learnable parameter for affinity metric. The QAP affinity matrix is built following the factorized formulation of K [57]:</p><formula xml:id="formula_14">K = diag(vec(K p ))+(? 2 ? K?1 )diag(vec(K e ))(H 2 ? KH1 )<label>(9)</label></formula><p>where diag(?) means building a diagonal matrix from input vector, and ? K means Kronecker product. All the forementioned operations allow back propagation, and we adopt the efficient GPU implementation provided by <ref type="bibr" target="#b18">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Association graph construction</head><p>We derive the association graph from the affinity matrix K. The weighted adjacency matrix of association graph W comes from the off-diagonal elements of K. We denote v (k) ? R n1n2?l k as l k -dimensional vertex embeddings on layer k (starting with k = 0). The initial embeddings at k = 0 are scalar, i.e. l 0 = 1, taken from the diagonal of K.</p><formula xml:id="formula_15">W ia,jb = K ia,jb , v (0) ia = K ia,ia<label>(10)</label></formula><p>W contains both connectivity and weight information in the association graph. In case when the first-order similarity K ia,ia is absent, we can assign a constant (e.g. 1) for all v (0) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Matching aware embedding of association graph</head><p>The matching problem can be transformed to selecting the vertices in the association graph that encode the node-tonode correspondence between two input graphs, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. For vertex classification on the association graph, we use GCN <ref type="bibr" target="#b56">[55]</ref> for its effectiveness and simplicity. We define the (unweighted) adjacency matrix of association graph A ? {0, 1} n1n2?n1n2 : A ia,jb = 1 if K ia,jb &gt; 0 and otherwise 0. A ia,jb serves as an indicator whether there exists an edge between vertices ia and jb in the association graph. Recall that the vertex ia represents the matching between node i and a from separate input graphs (see <ref type="figure" target="#fig_2">Fig. 2</ref>). In association graph, an edge exists between ia and jb if and only if the node-to-node matchings i to a and j to b can co-exist, and there exists an edge-to-edge affinity score defined between edges ij and ab. Since A is symmetric, we compute the degree matrix for normalization:</p><formula xml:id="formula_16">D = diag(A1 n1n2 )<label>(11)</label></formula><p>where diag(?) builds a diagonal matrix from input vector. The vertex aggregation step is according to:</p><formula xml:id="formula_17">m (k) = D ?1 Wf m (v (k?1) ) + f v (v k?1 ), v (k) = m (k) (12)</formula><p>where the message passing function f m : R l k?1 ? R l k and vertex's self update function f v : R l k?1 ? R l k are both implemented by networks with two fully-connected layers and ReLU activation.</p><p>The above general vanilla vertex embedding procedure in Eq. <ref type="bibr" target="#b13">(12)</ref> does not consider the one-to-one assignment constraint for matching. Here we develop a matching constraint aware embedding model: in each layer a soft permutation (i.e. doubly-stochastic matrix) is scored via classifier with Sinkhorn network Classifier : R n1n2?l k ? [0, 1] n1?n2 (see discussions in Sec. 3.2.4) followed by vectorization operator vec(?). The predicted soft permutation is concatenated to vertex embeddings whereby matching information is con-</p><formula xml:id="formula_18">sidered in embedding layers. With f m , f v : R l k?1 ? R (l k ?1)</formula><p>, such a matching-aware embedding scheme is denoted as Sinkhorn embedding in the rest of the paper.</p><formula xml:id="formula_19">v (k) = [m (k) vec(Classifier(m (k) ))]<label>(13)</label></formula><p>where [ ? ? ] means concatenation. We experiment both vanilla vertex embedding in Eq. (12) and matching-aware Sinkhorn embedding in Eq. <ref type="formula" target="#formula_0">(13)</ref>, to validate the necessity of adding assignment constraint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Vertex classification with Sinkhorn network</head><p>As graph matching is equivalent to vertex classification on association graph (see <ref type="figure" target="#fig_2">Fig. 2</ref>), a vertex classifier with Sinkhorn network is adopted to predict the matching result. Specifically we use a single layer fully-connected classifier denoted by f c : R l k ? R, followed by exponential activation with regularization factor ?:</p><formula xml:id="formula_20">s (k) ia = exp ?f c (v (k) ia )<label>(14)</label></formula><p>After reshaping classification scores into R n1?n2 , oneto-one assignment constraint is enforced to s by Sinkhorn network <ref type="bibr" target="#b54">[53]</ref>, <ref type="bibr" target="#b59">[58]</ref>. It takes a non-negative square matrix as input and outputs a doubly-stochastic matrix <ref type="bibr" target="#b54">[53]</ref>, <ref type="bibr" target="#b60">[59]</ref>. As the scoring matrix can be non-square for different sizes of graphs, the input matrix S ? R n1?n2 is padded into a square one (assume n 1 ? n 2 ) with small elements e.g. = 10 ?3 . A doubly-stochastic matrix is obtained by repeatedly running:</p><formula xml:id="formula_21">S = S (1 n2 1 n2 ? S), S = S (S ? 1 n2 1 n2 )<label>(15)</label></formula><p>where means element-wise division. By taking columnnormalization and row-normalization in Eq. (15) alternatively, S converges to a doubly-stochastic matrix whose rows and columns all sum to 1. The dummy elements are discarded in the final output, whose column sum may be &lt; 1 given umatched nodes from the bigger graph. Sinkhorn operator is fully differentiable and can be efficiently implemented by automatic differentiation techniques <ref type="bibr" target="#b61">[60]</ref>. The proposed vertex classifier with Sinkhorn network is denoted as Classifier : R n1n2?l k ? [0, 1] n1?n2 , which is also mentioned in matching-aware Sinkhorn embedding in Eq. (13).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.5">Loss for end-to-end training</head><p>Recall the obtained predicted matrix S from the above procedure is a doubly-stochastic matrix. Each element can be regarded as a binary classification where each vertex should be classified to 1 (matched) or 0 (unmatched). Hence we adopt the binary cross-entropy as the final loss, given the ground truth node-to-node correspondence X gt :</p><formula xml:id="formula_22">= ? n1 i=1 n2 a=1 X gt i,a log S i,a + (1 ? X gt i,a ) log(1 ? S i,a ) (16) ?1 1 ?2 1 ?3 1 ?1 2 ?2 2 ?3 2 ?1 1 ?1 2 ?2 1 ?2 2 ?3 1 ?3 2 Fig. 5.</formula><p>Third-order affinity adopted in our hypergraph matching. It in general follows the previous hypergraph matching works <ref type="bibr" target="#b23">[22]</ref>, <ref type="bibr" target="#b25">[24]</ref>, by considering the similarity between two triangle's three inner angles.</p><p>Our approach also allows direct optimization over the affinity score objectives for QAPLIB problems, which will be discussed in Sec. 4.2 in details. All the components are differentiable. Therefore, both NGM solver and the optional CNN feature extractor can be learned via backpropagation and gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">NHGM: Neural Hypergraph Matching</head><p>For Neural Hyper-Graph Matching (NHGM), higher-order structure is exploited for more robust matching. NHGM owns a nearly identical pipeline compared to NGM, while a more general message-passing scheme is devised for vertex classification in hypergraphs, as previously shown in <ref type="bibr" target="#b62">[61]</ref>, <ref type="bibr" target="#b63">[62]</ref>. Due to the explosive computational cost (O((n 1 n 2 ) t ) with order t), here we limit hypergraph to third-order which is also in line with the majority of existing works <ref type="bibr" target="#b23">[22]</ref>, <ref type="bibr" target="#b25">[24]</ref>, while the scheme is generalizable to any order t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Association hypergraph construction</head><p>The second-order affinity matrix K is generalized to affinity tensor H t of order t in hypergraph matching. In line with the hypergraph matching literature <ref type="bibr" target="#b22">[21]</ref>, <ref type="bibr" target="#b24">[23]</ref>, <ref type="bibr" target="#b25">[24]</ref>, the thirdorder affinity tensor is specified as:</p><formula xml:id="formula_23">H 3 ?1,?2,?3 = exp ? ? ? ? ? 3 q=1 | sin ? 1 ?q ? sin ? 2 ?q | ? ? /? 3 ? ?<label>(17)</label></formula><p>where ? 1 ?q , ? 2 ?q denotes the angle in graph G 1 and G 2 of each correspondence ? q , respectively. An illustration of third order affinity can be found in <ref type="figure">Fig. 5</ref>, where the similarity between triplets of nodes is compared. Third order affinity is usually defined on geometric consistency and it preserves both scaling and rotation invariance.</p><p>Extending from the second-order association graph, a hyper association graph is built from H. The association hypergraph H A = (V A , E A ) takes node-to-node correspondence ? = (V 1 i , V 2 j ) as vertices V A (which is consistent with second-order association graph) and higher-order similarity among {(V 1 ?1 , V 2 ?1 ), ? ? ? , (V 1 ?t , V 2 ?t )} as hyperedges E A , as shown by <ref type="figure" target="#fig_2">Fig. 2(c)</ref>. Elements of H are adjacency weights for the association hypergraph accordingly. In NHGM, hypergraph convolution is defined for vertex classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Matching aware association hypergraph embedding</head><p>As an extension of Eq. (12), vertex embeddings are updated from all vertices linked by hyperedges in association hypergraph. We compute normalized degree tensor of order t:</p><formula xml:id="formula_24">D t ?1 ?1,??? ,?t = A t ?1,??? ,?t /(A t ? 2 1 ? ? ? ? t 1) ?1<label>(18)</label></formula><p>Then an aggregation scheme extended from Eq. (12) is taken:</p><formula xml:id="formula_25">p (k) = f t m (v (k?1) ), H t = (D t ?1 H t ) t+1 m (k) = t ? t H t ? t p (k) ? ? ? ? 2 p (k) + f v v (k) = m (k) vec(Classifier(m (k) ))<label>(19)</label></formula><p>where f v abbreviates f v (v (k?1) ), ? i denotes tensor product by dimension i (see Eq. <ref type="formula">(5)</ref>), denotes element-wise multiply, (?) t+1 means expanding along dimension (t + 1). f t m : R l k?1 ? R (l k ?1) is message passing function at order t. Different orders of features are fused by weighted summation with ? t .</p><p>The other modules of NHGM, including classifier and cross-entropy loss, are identical to NGM. Therefore, NGM can be viewed as a special case of NHGM, where the order is restricted to 2. The sparsity of H is exploited for both time and memory efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">NMGM: Neural Multi-graph Matching</head><p>We explore learning multi-graph matching, where the information is fused among graphs by the so-called cycleconsistency. Cycle-consistency denotes a condition where the matching between any two graphs is consistent when passed through any other graphs, i.e. X ij = X ik X kj for all i, j, k, which can be viewed as each graph matched to a n-sized reference X i ? {0, 1} n?n . The pariwise matching between G i , G j can be represented with X ij = X i X j . In this paper, we refer to the line of works <ref type="bibr" target="#b29">[28]</ref>, <ref type="bibr" target="#b30">[29]</ref>, <ref type="bibr" target="#b35">[34]</ref>, <ref type="bibr" target="#b64">[63]</ref> involving post-synchronization given initial pairwise matchings. Spectral fusion is used in NMGM for its effectiveness and simplicity, and most importantly, its ability for end-to-end training. We assume all graphs are of equal size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">Building joint matching matrix</head><p>We first obtain initial two-graph matchings by NGM to build a symmetric joint matching matrix S ? R nm?nm . For each pair G i and G j with n nodes, S ij ? [0, 1] n?n is computed by NGM as the soft (i.e. doubly-stochastic) matching matrix. For m graphs, S can be built from all combinations of pairwise matchings S ij :</p><formula xml:id="formula_26">S = ? ? ? S 00 ? ? ? S 0m . . . . . . . . . S m0 ? ? ? S mm ? ? ?<label>(20)</label></formula><p>where S is of size mn ? mn. For the diagonal part of S, S ii are all identical matrices. Note S ij are all square matrices. The objective of spectral fusion results in getting a cycleconsistent joint matching matrix?, whose innerproduct with S is maximized:</p><formula xml:id="formula_27">max S = tr(? S)<label>(21)</label></formula><p>To ensure the cycle-consistency, it holds? ij =? ik?kj for all elements in?. We may select an arbitrary k as the reference, omitting k in the subscript,? can be decomposed as matching to the reference:</p><formula xml:id="formula_28">U? =? where? = ? ? ? ?? 0 . . . S m ? ? ? ?<label>(22)</label></formula><p>under ideal condition where? i are all permutation matrices, each column of? is linearly independent and?/ ? m are n eigenvectors of? with eigenvalue m. The permutation constraint in? i is relaxed for computational feasibility and Eq. (21) results in a generalized Rayleigh problem and solved via spectral fusion, as shown follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">Differentiable spectral fusion of pairwise matchings</head><p>Multi-graph matching information can be fused by eigenvector decomposition (i.e. spectral method) on S. Based on generalized Rayleigh problem, given n nodes in each graph, we extract the eigenvectors corresponding to the topn eigenvalues of symmetric matrix S:</p><formula xml:id="formula_29">U ? U = S<label>(23)</label></formula><p>where diagonal matrix ? ? R n?n contains top-n eigenvalues and U ? R mn?n are the n corresponding eigenvectors. It has been shown that the computation of eigenvalues and eigenvectors are differentiable <ref type="bibr" target="#b65">[64]</ref> which makes them fixed components in our end-to-end learning network pipeline. The fusion of the input S can be written as follows, which can be seen as a smoothed version maximizing tr(? S):</p><formula xml:id="formula_30">S = m U U<label>(24)</label></formula><p>The gradient of eigen decomposition in Eq. <ref type="formula" target="#formula_1">(23)</ref> is <ref type="bibr" target="#b65">[64]</ref>:</p><formula xml:id="formula_31">?L ?S = U ? ? 4m ? ? Y U ?L ?? sym U sym ? ? ? ? U where Y ij = 1/(? i ? ? j ) i = j 0 i = j<label>(25)</label></formula><p>where L denotes the loss, means element-wise multiplication, A sym = (A + A )/2 and ? i = ? ii is the i-th eigenvalue. According to this backward formulation, if there exist non-distinctive eigenvalues, i.e. ? i = ? j for i = j, a numerical divided-by-zero error will be caused. This issue usually happens when cycle-consistency (i.e. S ij = S ik S kj ) is already met in the input S, under such circumstances the fused matching results are nearly identical to the original matchings. To avoid numerical issues, we assign? ij = S ij to bypass eigendecomposition if the minimum residual among top-n eigenvalues is smaller than tolerance ?, e.g. 10 ?4 . This strategy is found effective to stabilize learning.</p><p>The final matching results are obtained by differentiable Sinkhorn network:</p><formula xml:id="formula_32">S ij = Sinkhorn(exp(?? ij ))<label>(26)</label></formula><p>where the fused two-graph matching? ij is from? and exp(? ? ) performs regularization for Sinkhron. Crossentropy loss in Eq. <ref type="formula" target="#formula_0">(16)</ref> is applied to eachS ij for supervised learning, which is similar to the supervised two-graph matching case in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Improved Matching by Enhanced Feature Extractor</head><p>Since our proposed method deals with the most general Lawler's QAP, the baseline feature extractor in Sec. 3.2.1 can be replaced by other enhanced feature extraction techniques. In this section, we refer to the novel feature extractor proposed by <ref type="bibr" target="#b38">[37]</ref> in replacement of the feature extractor in Sec. 3.2.1, introducing the family of enhanced models for image matching problems -NGM/NHGM/NMGM-v2. In Sec. 3.5.1 we discuss how to build enhanced affinity matrix, and in Sec. 3.5.2 we propose a way of building hypergraph affinity tensor for NHGM-v2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">Enhanced graph matching feature</head><p>The authors of <ref type="bibr" target="#b38">[37]</ref> propose an enhanced deep learning feature extractor for graph matching problem on images based on SplineConv <ref type="bibr" target="#b66">[65]</ref> and weighted inner product, which can seamlessly fit into our pipeline. This enhanced feature extractor is summarized in the second row of <ref type="figure" target="#fig_4">Fig. 4</ref>.</p><p>With some abuse of the notations from Sec. 3.2.1, we introduce the feature extractor as follows. Given two input images, we extract node features from relu4_2 and relu5_1 of VGG16, and then concatenate them to form the feature matrices? 1 ? R n1?d ,? 2 ? R n2?d , where d is the feature dimension of the concatenated features. Then we adopt two SplineConv 1 <ref type="bibr" target="#b66">[65]</ref> layers on? 1 ,? 2 to produce refined featuresF 1 ? R n1?d ,F 2 ? R n2?d . SplineConv <ref type="bibr" target="#b66">[65]</ref> is a powerful graph convolution operator exploiting B-Spline kernel, encoding geometric features into node features. Therefore, SplineConv is suitable for feature refinement on image matching datasets. Readers are referred to the original paper for details about SplineConv. The edge features are constructed as the difference of its two nodes in the feature space:</p><formula xml:id="formula_33">X =? 1F1 ?H 1F1 ,? =? 2F2 ?H 2F2<label>(27)</label></formula><p>where?,H are the connectivity matrices in <ref type="figure" target="#fig_3">Fig. 3</ref>. As shown in the second row of <ref type="figure" target="#fig_4">Fig. 4</ref>, the enhanced feature extractor also contains a branch producing global features by max-pooling over the output of relu5_3 layer. The pooled global features from two graphs are then concatenated, and passed to a fc layer followed with tanh activation, producing global features? ? R d :</p><formula xml:id="formula_34">g = tanh(fc([? 1?2 ]))<label>(28)</label></formula><p>where [ ? ? ] means concatenation, and we compute separate? node ,? edge for node and edge features, respectively. Node and edge similarity matrices are constructed based on weighted inner-product whereby? as the weight:</p><formula xml:id="formula_35">K e =X diag(? node )? , K p =F 1 diag(? edge )F 2<label>(29)</label></formula><p>1. SplineConv is called SplineCNN in its original paper <ref type="bibr" target="#b66">[65]</ref>. To avoid the ambiguity of the term "CNN" which usually represents convolution on images while SplineCNN is convolution on graphs, we name it as SplineConv in this paper.</p><p>where diag(?) means building a diagonal matrix by placin? g on it. After obtaining K e , K p , the affinity matrix can be built by Eq. (9), and the resulting QAP is readily solved by NGM/NMGM as discussed in Sec. 3.2 and Sec. 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">Enhanced hypergraph affinity for NHGM-v2</head><p>Based on the the features defined in Sec. 3.5.1, we further design the hyperedge affinity for NHGM-v2. The hypergraph affinity is inspired by <ref type="figure">Fig. 5</ref>, however in NHGM the hypergraph affinity is based on geometric features, and in NHGM-v2 we define hypergraph affinity on the highdimensional feature space. We regard the plane in <ref type="figure">Fig. 5</ref> as the feature space formed byF 1 andF 2 , and each point in <ref type="figure">Fig. 5</ref> represents a node with features. Following Eq. <ref type="formula" target="#formula_0">(17)</ref>, the third order affinity tensor is defined as the differences of angles in the feature space:</p><formula xml:id="formula_36">H 3 ?1,?2,?3 = exp ? ? ? ? ? 3 q=1 | cos ? 1 ?q ? cos ? 2 ?q | ? ? /? 3 ? ?<label>(30)</label></formula><p>and we empirically find cos performs better than sin as in Eq. <ref type="formula" target="#formula_0">(17)</ref>, probably because the value of cos(?) grows monotonically with ? ? [0, ?] but sin(?) does not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Further Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1">Learning of problem structure</head><p>The inherent working pattern of our proposed neural solver actually learns the underlying structure of graph matching problems. With the connection between graph matching problem and association graph, the original mathematical form of Lawler's QAP transforms into a trackable structure with modern deep learning models. The problem structure is learned by GNN, resulting in a simplified Linear Assignment Problem solved with differentiable Sinkhorn algorithm. In <ref type="bibr" target="#b67">[66]</ref> some combinatorial problems over graphs are considered, where problems simplified by GNN are solved greedily. A similar scheme should generalize to other combinatorial problems, by exploiting the representative power of learning problem structures by deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">Matching-aware embedding</head><p>The matching-aware Sinkhorn embedding is proposed to add one-to-one matching constraint at shallower embedding layers. Otherwise, the matching constraint is not considered until the output Sinkhorn layer. Early involvement of matching information has been proven effective for both learning-free (RRWM <ref type="bibr" target="#b3">[2]</ref> vs SM <ref type="bibr" target="#b8">[7]</ref>) and learning based (PCA-GM vs PIA-GM <ref type="bibr" target="#b18">[17]</ref>) methods. We show the importance of Sinkhorn embedding by notable improvement in synthetic tests and ablation study on real-world images, additionally further improvement by introducing multi-head Sinkhorn embedding at the cost of increased computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.3">Gumbel sampling for optimization problems</head><p>As a common post-processing step, the gap between doubly stochastic matrix S and permutation matrix X is fulfilled by Hungarian algorithm <ref type="bibr" target="#b44">[43]</ref> in a deterministic manner. From the probabilistic point of view, S represents a distribution on the space of permutation matrices, and our cross-entropy loss minimizes the distance between probability S and ground truth distribution X gt . Permutation with the highest probability is selected by Hungarian algorithm. Such a greedy scheme is empirically successful for matching. However, there might exist better solutions from the distribution, especially when optimizing the objective score of combinatorial problems. Therefore, we switch to Gumbel-Sinkhorn <ref type="bibr" target="#b59">[58]</ref> by replacing Eq. <ref type="formula" target="#formula_0">(14)</ref> with</p><formula xml:id="formula_37">s (k) ia = exp ? g (f c (v (k) ia ) + g)<label>(31)</label></formula><p>followed by Sinkhorn algorithm. Note the added g is sampled from standard Gumbel distribution with cumulative distribution function (CDF):</p><formula xml:id="formula_38">G(x) = e ?e ?x<label>(32)</label></formula><p>which models the distribution of extreme values from another distribution. By Eq. (31) sparser doubly-stochastic matrices can be sampled from the original distribution and repeated sampling provides a batch of samples. These sparse matrices are followed by Hungarian discretization, and the objective scores are computed and the best-performing solution is chosen as the final solution. Exploration and speed can be balanced by the number of Gumbel samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>Experiments are conducted on a Linux workstation with Nvidia RTX8000 (48GB) GPU and Intel Xeon W-3175X CPU @ 3.10GHz with 128GB RAM. We test Lawler's QAP in two settings: i) synthetic point registration, which takes affinity matrix/tensor as input, and ii) QAPLIB with large-scale real-world QAP instances where the network learns to minimize the objective score. We also test our method for a vision application of Lawler's QAP in the sense that CNN features of image keypoints are learned and matched on real images. For keypoint matching, matching accuracy is computed as the percentage of correct matchings among all true matchings.</p><p>We also perform hypergraph and multiple graph matching tests to evaluate our NHGM/-v2 and NMGM/-v2. Our PyTorch implementation of NGM/-v2, NHGM/-v2, NMGM/-v2 involves a three-layer GNN, with graph feature channels l 1 = l 2 = l 3 = 16. Other hyperparameters are set as ? =? = 20, ? 2 = 1, ? 3 = 1.5. We set batch size=8. NGM, NHGM, NMGM are trained with SGD and 0.9 Nesterov momentum <ref type="bibr" target="#b68">[67]</ref>, and the v2 models are trained with Adam optimizer <ref type="bibr" target="#b69">[68]</ref>. Detailed learning rate configurations can be found in the following part of this section. Tab. 3 summarizes all our methods and their variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Synthetic Experiment for QAP Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">Protocol setting</head><p>In the synthetic experiment, sets of random points in the 2D plane are matched by comparison with other competitive learning-free graph matching solvers. For each trial, we generate 10 sets of ground truth points whose coordinates are in the plane U (0, 1) ? U (0, 1). Synthetic points are distorted by random scaling from U (1 ? ? s , 1 + ? s ) and additive random noise N (0, ? 2 n ). From each ground truth set, 200 graphs are sampled for training and 100 for testing, resulting in totally 2,000 training samples and 1,000 testing samples in one trial. We assume graph structure is unknown to the GM solver, therefore we construct the reference graph by Delaunay triangulation, and the target graph (may contain outliers) is fully connected. Outliers are also randomly sampled from U (0, 1) ? U (0, 1). By default there are 10 inliers without outlier, with ? s = 0.1, ? n = 0. We construct the same affinity matrix to formulate Lawler's QAP for all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Peer methods</head><p>As existing learning methods <ref type="bibr" target="#b18">[17]</ref>, <ref type="bibr" target="#b26">[25]</ref>, <ref type="bibr" target="#b32">[31]</ref> cannot handle learning with Lawler's QAP with a given affinity matrix, here we first compare learning-free methods: 1) SM <ref type="bibr" target="#b8">[7]</ref> considers graph matching as discovering graph cluster by spectral numerical technique; 2) RRWM <ref type="bibr" target="#b3">[2]</ref> adopts a random-walk view with reweighted jump on graph matching; 3) IPFP <ref type="bibr" target="#b70">[69]</ref> iteratively improves a given solution via integer projection; 4) PSM <ref type="bibr" target="#b71">[70]</ref> improves SM via a probabilistic view; 5) GNCCP <ref type="bibr" target="#b72">[71]</ref> is a convex-concave path-following method for graph matching and 6) BPF <ref type="bibr" target="#b15">[14]</ref> improves path following techniques by branch switching, reaching stateof-the-art performance on graph matching. Additionally, hyper-graph matching algorithm 7) RRWHM <ref type="bibr" target="#b25">[24]</ref> extending powerful RRWM to hyper-graph scenarios is also compared. Second-order affinity is integrated into third-order tensor for RRWHM following <ref type="bibr" target="#b25">[24]</ref>. In this experiment, secondorder affinity is modeled by</p><formula xml:id="formula_39">K ia,jb = exp ?(f ij ? f ab ) 2 /? 2 2</formula><p>where f ij is edge length E ij . We empirically set ? 2 2 = 5 ? 10 ?7 for all experiments. The third-order affinity model follows Eq. (17) with ? 3 = 0.1.</p><p>For fair comparison of run time, we re-implement the parallelization-friendly SM, RRWM and RRWHM solvers with GPU which can be more scalable than their original single-thread version on CPU. While the other compared methods involve iterative computing and complicated branching, which are not suitable for GPU. Thus the CPU version released by <ref type="bibr" target="#b15">[14]</ref> are compared. NGM-V means vanilla NGM without Sinkhorn embedding (see discussion between Eq. (12) and Eq. (13)) and NGM-MH means multi-  <ref type="figure">Fig. 6</ref>. Synthetic test by varying deformation level ?n, ?s, number of outliers/graphs. Note it learns a QAP solver based on the given affinity matrix/tensor, without learning any affinity model. This feature is not supported in GMN <ref type="bibr" target="#b26">[25]</ref> and PCA-GM <ref type="bibr" target="#b18">[17]</ref> thus they cannot be compared.</p><p>head Sinkhorn embedding with NGM, by concatenating additional 8 Sinkhorn channels to m (k) in Eq. <ref type="bibr" target="#b14">(13)</ref>. The smoothing technique <ref type="bibr" target="#b30">[29]</ref> is adopted for multi-matching baseline NGM-SF, where learned NGM is followed with spectral fusion. Since NGM/NHGM/NMGM-v2 share the same solver module with NGM/NHGN/NMGM, they are not compared in synthetic test. The learning rate starts at 10 ?2 decays by 10 every 5,000 steps. Multi-graph matching involves 4 graphs by default. The Hungarian algorithm is used as the common discretization step. <ref type="figure">Fig. 6</ref>(a-c) shows our proposed NGM performs comparatively with state-of-the-art solvers in matching accuracy, and can even surpass under severe random scaling <ref type="figure">Fig. 6(b)</ref>. Further improvement in accuracy is achieved via multihead Sinkhorn embedding model NGM-MH. NMGM gains steadily from NGM by fusing multi-matching information, and in <ref type="figure">Fig. 6(d)</ref> we show the improvement in NMGM by introducing more graphs, and the necessity of learning joint matching as NMGM steadily outperforms NGM-SF whose weights are from two-graph NGM. With the third-order affinity, NHGM shows state-of-the-art robustness to noise, scaling and outliers. Compared to learning-free hyper-graph matching RRWHM <ref type="bibr" target="#b25">[24]</ref>, our NHGM performs comparatively in the precense of outliers as shown in <ref type="figure">Fig. 6(c)</ref>. While it performs more robustly to noises and scaling in <ref type="figure">Fig. 6(a&amp;b)</ref>. As shown in <ref type="figure">Fig. 6(h)</ref>, NGM, NGM-V and NGM-MH are among the fastest graph matching algorithms, and due to efficient GPU parallelization, NHGM is comparatively fast against PSM <ref type="bibr" target="#b71">[70]</ref> and more time-efficient compared to state-of-the-art BPF <ref type="bibr" target="#b15">[14]</ref>. In contrast, traditional hypergraph matching algorithms e.g. RRWHM <ref type="bibr" target="#b25">[24]</ref> are usually much slower than second-order graph matching and unscalable to larger size of problems. We report the QAP objective score solved by two-graph matching methods in <ref type="figure">Fig. 6(e-g)</ref>, where interestingly our learning-based solvers reach relatively low scores compared to their corresponding accuracy. As have been discussed in Sec. 1, the QAP objective may be biased under noisy conditions i.e. the optimal solution to QAP may not correspond to true matching. Our solvers learns to ignore the noisy patterns in input affinity matrix. Such a phenomenon becomes more severe in matching real-world images, and the strength of learning-based solvers becomes more significant, as will be shown in Sec. 4.3.1 in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Result and discussions</head><p>The effectiveness of matching-aware Sinkhorn embedding is shown in the accuracy gap between NGM and NGM-V. Further improvement is achieved by multi-head Sinkhorn embedding in NGM-MH, especially with random scaling in <ref type="figure">Fig. 6(b)</ref>. As discussed in Sec. 3.6.2, NGM-V without Sinkhorn embedding works in a way similar to SM as the embedding procedure does not consider the assignment constraint, and they also perform closely to each other. On the other hand, by exploiting Sinkhorn embedding, NGM and NGM-MH are conceptually similar to RRWM as all of them try to incorporate the assignment constraint on the fly. Their performances are also very close.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Learning Real-world QAP Instances</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Experiment setting</head><p>Our NGM solves the most general Lawler's QAP, which has a wide range of applications beyond vision. Evaluation on QAPLIB <ref type="bibr" target="#b34">[33]</ref> is performed to show the capability of NGM on learning the QAP objective score, which should be minimized in QAPLIB (in contrast, objective score is maximized in graph matching). The QAPLIB contains 134 real-world QAP instances from 15 categories, e.g. planning a hospital facility layout <ref type="bibr" target="#b73">[72]</ref>. The problem size is defined as n 1 = n 2 = n from Lawler's QAP in Eq. (1), and ranges from 12 to 256. Results are reported on 133 instances with 12 ? n ? 150, as the most challenging tai256c is computationally intractable with our testbed (275GB GPU memory is required for intermediate computing). We set the loss function as the objective score of QAP, keeping the (a) Normalized objective score (lower is better) of our best-performing NGM-G5k against learning-free QAP solvers. Failed instances are plotted at the top of y-axis. The instances are firstly split into two parts based on whether NGM-G5K performs better than Sinkhorn-JA <ref type="bibr" target="#b16">[15]</ref> or not, and then sorted by the normalized score of NGM-G5k. NGM-G5k surpasses state-of-the-art learning-free method Sinkhorn-JA <ref type="bibr" target="#b16">[15]</ref> on 85 out of 133 instances. (b) Normalized objective score (lower is better) comparing different sampling settings. More samples in Gumbel-Sinkhorn guarantee a higher probability of finding better solutions, at the cost of increased computation. NGM always picks the permutation matrix with the highest probability with Sinkhorn and Hungarian algorithms, and performs similarly to NGM-G50 and NGM-G500. The order is kept in line with <ref type="figure" target="#fig_8">Fig. 7(a)</ref>.  model architecture unchanged. We formulate a optimization task where the objective score is minimized:</p><formula xml:id="formula_40">L obj = vec(S) Kvec(S)<label>(33)</label></formula><p>where S is from the output Sinkhorn layer of NGM. Given optimal L obj is reached, the learned S is a doublestochastically relaxed solution to original QAP. To explore the feasible space, Gumbel-Sinkhorn discussed in Sec. 3.6.3 is adopted during inference.</p><p>The naming of QAPLIB instances are based on the following rule: the prefix is the problem category which is the name of the author who proposes the problem, and the number means the size of the problem. If there are multiple problems with the same size, QAPLIB appends a letter starting from a to distinguish. We train one network for each set of problems with the same prefix (i.e. problems from the same category), because they usually have common structures (e.g. all bur problems are keyboard layout design problems). And the problem sizes may vary for problems in the same category (e.g. the sizes of esc problems vary from 16 to 128). We train one model for each category, and report the normalized objective score. In consideration of compact and intuitive illustration, the normalized objective score is computed with the upper bound (primal bound) provided by the up-to-date online benchmark 2 and normalized by the baseline solver spectral matching (SM) <ref type="bibr" target="#b8">[7]</ref>:   <ref type="table" target="#tab_0">category  bur chr els esc had kra lipa nug rou scr sko ste tai tho wil  total  #instances  8  14  1  19  5  3  16  15  3  3  13  3  25  3  2  133</ref>    <ref type="formula" target="#formula_2">(35)</ref>), as a signal of problem difficulty (bigger gap more difficult). Higher value denotes increased negative effect of the corresponding statistics to the final solution quality. The correlation between statistics and the difference of two methods is also listed on the last row, where higher value denotes NGM-G5k is more sensitive than Sinkhorn-JA and vice versa. with different Gumbel sampling numbers (NGM-GX, X = number of samples) are validated for their performance. The learning rate is initialized at 10 ?4 and decays by 10 every 50,000 steps. Batch size is set to 1 and the regularization of Gumbel Sinkhorn ? g = 1. Our proposed methods are compared fairly with our GPU implementation of RRWM <ref type="bibr" target="#b3">[2]</ref> and SM <ref type="bibr" target="#b8">[7]</ref>, and results provided in the paper of Sinkhorn-JA <ref type="bibr" target="#b16">[15]</ref> (runs on Intel Xeon CPU @ 2.40 GHz). For the problem instances not reported in <ref type="bibr" target="#b16">[15]</ref>, we assume Sinkhorn-JA fails to reach any feasible solution, as there is no explanation of missing instances in the original paper.</p><formula xml:id="formula_41">norm score = solved score ? upper bnd SM score ? upper bnd<label>(34)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Result and analysis</head><p>In <ref type="figure" target="#fig_8">Fig. 7(a)</ref>, our method beats RRWM <ref type="bibr" target="#b3">[2]</ref> and SM <ref type="bibr" target="#b8">[7]</ref> and is comparative and even superior against state-of-the-art Sinkhorn-JA <ref type="bibr" target="#b16">[15]</ref>. As there is no learning-based QAP solver, only non-learning methods are compared. The effectiveness of Gumbel sampling discussed in Sec. 3.6.3 is validated in <ref type="figure" target="#fig_8">Fig. 7(b)</ref>, where Gumbel-based NGM-G5k consistently outperforms deterministic NGM, which always picks the permutation with the highest probability by Hungarian algorithm. With decreased sampling number, the performance of Gumbel-based methods gradually degenerates. It suggests that more exploration over sampling space guarantees higher expectations on better solutions. Further evaluation is given in Tab. 4 and <ref type="figure" target="#fig_9">Fig. 8</ref>. With learning and Gumbel sampling, our NGM-G5k finds the best solution among 72 out of 133 instances, while state-ofthe-art learning-free solver Sinkhorn-JA <ref type="bibr" target="#b16">[15]</ref> outperforms on 46 instances so that learning-based solvers e.g. NGM can fit a wider range of problems compared to traditional solvers. More importantly, our best-performing model NGM-G5k is of a magnitude faster than Sinkhorn-JA, and adjusting the sampling number of Gumbel method enables balancing between solution quality and computational demand. Finally, we show the generalization ability among different instances by confusion matrix in <ref type="figure">Fig. 9</ref>, where NGM-G5k  <ref type="figure">Fig. 9</ref>. Generalization test by confusion matrix cross QAP problem instances, where models are learned with instances on y-axis and tested with instances on x-axis. Darker color and lower score correspond to better performance. The tasks are randomly selected from the QAPLIB benchmark. Problem sizes are shown by the numbers in instance names, and our method is insensitive to problem sizes.</p><p>is trained and tested on different randomly picked instances. Our model generalizes soundly to unseen instances with different problem sizes. In conclusion, our learning QAP solvers achieve the best accuracy-speed trade-off on QAPLIB and can generalize among different problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Further discussion</head><p>As shown in Tab. 4 and <ref type="figure" target="#fig_8">Fig. 7(a)</ref>, learning-free Sinkhorn-JA <ref type="bibr" target="#b16">[15]</ref> and our NGM-G5k performs better on separate categories of QAPLIB, e.g. Sinkhorn-JA performs better on chr and lipa instances while our method is more powerful on bur, esc, nug, scr and tai. Some statistical studies are conducted to discover the relation between model behavior and problem patterns, shedding light for future research on both learning-based and learning-free solvers. For each problem instance, some statistics are summarized from each instance's affinity matrix: problem size n,  <ref type="figure">Fig. 10</ref>. Generalization study shown as confusion matrix of our proposed NGM and NGM-v2 in line with <ref type="bibr" target="#b18">[17]</ref>, <ref type="bibr" target="#b26">[25]</ref> on Pascal VOC Keypoint. Models are trained on categories on the y-axis and tested on categories on the x-axis. Darker color denotes relatively better performance in the same column, and the averaged accuracy on both diagonal part (learned during training) and all elements (trained+generalized) of confusion matrices are reported in the brackets over confusion matrices. The train/test split follows the main experiment and eight categories are selected randomly. </p><p>It means the percentage of improvement can be made compared to best-known optima (usually solved at extremely high complexity). Pearson correlation coefficients r are computed between each gap and corresponding statistics, additionally some meaningful combinations of the statistics. Items with |r| ? 0.2 are listed in Tab. 5, where positive correlation means a negative effect on solver's performance because a lower gap is better. The correlation between problem statistics and the difference between two methods are also reported.</p><p>In the first two columns in Tab. 5, the higher sparsity (nz/n 4 , proportion of zeros in affinity matrix) makes the problem significantly more challenging for both NGM-G5k and Sinkhorn-JA, and the same conclusion holds for a larger number of zeros nz. Then the normalized standard deviation K std /K max shows some weak negative effect for NGM-G5k, but little correlation to Sinkhorn-JA. In the last five columns both methods are affected by higher degrees in association graph, while Sinkhorn-JA seems more sensitive to the normalized standard deviation of degrees d std /K, d std /d. In summary, sparsity is the key challenge for both NGM-G5k (where message passing paths are blocked) and Sinkhorn-JA (where it becomes harder to find tight lower bounds). Furthermore, learning with the association graph can restrain the noisy deviation in degrees. Future improvement may be achieved by designing graph learning models with higher capacity, and designing global communication mechanisms against sparse association graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Real Image for Joint CNN and QAP Learning</head><p>Our matching net allows for raw image input, from which a CNN is learned (see <ref type="figure" target="#fig_4">Fig. 4</ref>). We evaluate semantic keypoint matching on Pascal VOC dataset with Berkeley annotations <ref type="bibr" target="#b4">3</ref>  <ref type="bibr" target="#b74">[73]</ref> and Willow ObjectClass dataset <ref type="bibr" target="#b27">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Results on Pascal VOC Keypoint dataset</head><p>This natural image dataset <ref type="bibr" target="#b74">[73]</ref> consists of 20 instance classes with keypoint labels. We follow <ref type="bibr" target="#b18">[17]</ref>, where image pairs with inlier positions are fed into the model. We consider it a challenging dataset because instance may vary from its scale, pose and illumination, and the number of keypoints in each image varies from 6 to 23. The shallow learning method HARG-SSVM <ref type="bibr" target="#b27">[26]</ref> incorporates a fix-sized reference graph, therefore it is inapplicable to our experiment setting where instances from the same category have different inliers. As our multi-matching mechanism in Sec. 3.4 requires the same number of nodes among graphs, our multi-graph model NMGM and NMGM-v2 are not compared either.</p><p>In line with the protocol of <ref type="bibr" target="#b18">[17]</ref>, <ref type="bibr" target="#b26">[25]</ref>, we filter out those poorly annotated images which are meaningless for matching. Then we get 7,020 training samples and 1,682 testing samples. Instances are cropped around their ground truth bounding boxes and resized to 256 ? 256 before fed into the network. As discussed in Sec. 3.2.1, we adopt VGG16 backbone <ref type="bibr" target="#b57">[56]</ref> and construct the affinity matrix from the same CNN layers: relu4_2 for node features and relu5_1 for edge features. The learning rate starts at 10 ?2 and decays by 10 every 10,000 steps. For the two input images, one graph is constructed by Delaunay triangulation and the other is fully-connected. ? 3 = 10 ?4 is set for third-order affinity of NHGM.</p><p>For NGM/NHGM-v2, the feature extractor is replaced by the enhanced backbone with SplineConv and weighted inner-product affinity as discussed in Sec. 3.5, while keeping other modules unchanged. ? 3 = 0.1 is set for third-order affinity of NHGM-v2. Following <ref type="bibr" target="#b38">[37]</ref>, we set learning rate 2 ? 10 ?3 for VGG16 and 2 ? 10 ?5 for other modules.</p><p>We compare GMN <ref type="bibr" target="#b26">[25]</ref>, PCA-GM <ref type="bibr" target="#b18">[17]</ref>, LCS <ref type="bibr" target="#b33">[32]</ref>, BBGM <ref type="bibr" target="#b38">[37]</ref>, by which affinity functions are learned for graph matching. It is worth noting that we discover the experiment setting implemented by BBGM <ref type="bibr" target="#b38">[37]</ref> is easier than ours, because they filter out keypoints which are out of the bounding box but we do not. Therefore, we reimplement BBGM <ref type="bibr" target="#b38">[37]</ref> to fit our experiment setting and the reported BBGM results in this paper are slightly worse than the results in its original paper.</p><p>Results in Tab. 6 show that with CNN feature and QAP solver learned jointly, our methods surpass competing methods on most categories, especially best performs in terms of mean accuracy. Specifically, NGM surpasses deep graph matching method PCA-GM, and it is worth noting that PCA-GM incorporates explicit modeling on higherorder and cross-graph affinities, while only second-order affinity is considered in our QAP formulation (and thirdorder for hypergraph matching). With enhanced feature extractor, our NGM-v2 surpasses state-of-the-art BBGM in terms of accuracy, which is not surprising because NGM-v2 and BBGM share the same feature extractor, and NGM-v2 learns both feature extractor and the graph matching solver, but BBGM only learns the feature extractor. NHGM and NHGM-v bring further improvement with respect to NGM and NGM-v2 by exploiting hypergraph affinities. The generalization ability is further validated by confusion matrices for NGM and NGM-v2 as shown in <ref type="figure">Fig. 10</ref>. Models are trained only with categories on the x-axis and tested with all categories. The color map is determined  by the accuracy in current cell normalized by the highest accuracy in its column. As shown in the confusion matrices, both NGM and NGM-v2 own some generalization ability between visually similar categories, e.g. chair and sofa, cat and dog, bus and train. Compared to peer deep graph matching methods <ref type="bibr" target="#b18">[17]</ref>, <ref type="bibr" target="#b26">[25]</ref>, NGM and NGM-v2 fit better on the training categories on the diagonal of confusion matrices, while on the other hand NGM and NGM-v2 seem less powerful than PCA-GM when considering all categories, since most categories are irrelevant to the training category. The matching visualization is given in <ref type="figure" target="#fig_12">Fig. 11</ref>. The error patterns of NGM-v2, NHGM-v2 are similar but differ from NGM and PCA-GM, because they are based on different feature extractors. As can be seen from the difference between NGM and NGM-v2, better feature extractor can promise better matching accuracy in various categories. And NHGM-v2 improves NGM-v2 by correcting some existing errors, e.g. in "cat", "motorbike", "person" and "tvmonitor".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Results on Willow ObjectClass dataset</head><p>This natural image dataset covers 5 categories. Each category contains at least 40 images, and all instances in the same class share 10 distinctive image keypoints. We mainly evaluate multi-graph matching learning of NMGM and NMGM-v2 on Willow ObjectClass. Following the protocol in <ref type="bibr" target="#b18">[17]</ref>, we directly train our methods on the first 20 images and report testing results on the rest. The learning rate starts at 10 ?2 and decays by 10 every 500 steps.</p><p>The performance of HARG-SSVM <ref type="bibr" target="#b27">[26]</ref>, GMN <ref type="bibr" target="#b26">[25]</ref> and PCA-GM <ref type="bibr" target="#b18">[17]</ref> reported in <ref type="bibr" target="#b18">[17]</ref> are listed and compared. Two novel multi-graph matching algorithms HiPPI <ref type="bibr" target="#b75">[74]</ref> and MGM-Floyd <ref type="bibr" target="#b76">[75]</ref> are considered as learning-free baselines. Tab. 8 shows that NGM and NHGM performs comparatively to PCA-GM especially on rigid objects, and NGM-v2 and NHGM-v2 surpass the state-of-the-art BBGM <ref type="bibr" target="#b38">[37]</ref>. Our NMGM involves less number of graphs than learningfree methods HiPPI <ref type="bibr" target="#b75">[74]</ref> and MGM-Floyd <ref type="bibr" target="#b76">[75]</ref>, but achieves higher accuracy thanks to end-to-end multi-graph matching learning. Our NMGM-v2 best performs among all methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Ablation study on Pascal VOC Keypoint dataset</head><p>We validate the effectiveness of learning on both CNN and the solver by a controlled experiment. In Tab. 7, the first entry denotes whether the CNN weights are obtained from pretrained ImageNet classifier (ImgNet) <ref type="bibr" target="#b77">[76]</ref> or learned graph matching model (NGM); the second entry means either learning-free solver RRWM or learning-based solver NGM is adopted to solve QAP. The necessity of learning on both CNN and the solver is validated, and our joint CNN and solver learning method best performs among them. Learning with CNN unsurprisingly mitigates the gap between classification task and matching task, while it is worth noting the learned solver is nearly twice accurate against RRWM with ImageNet CNN (44.0% vs 24.0%), showing the robustness on our solver side against noises. Ablation study is performed on our proposed modules in NHGM-v2 on Pascal VOC Keypoint dataset, as shown in Tab. 9. The baseline model is built following NGM introduced in Sec. 3.2, but node affinity is ignored in model input, i.e. v (0) ia = 1 and Sinkhorn embedding (Sec. 3.6.2) is excluded. The effectiveness of node affinity, Sinkhorn embedding, and NGM-v2's SplineConv feature and weightedinner product affinity are validated by adding these components successively. Finally, we add hypergraph affinity proposed in Sec. 3.5.2 for NHGM-v2. Tab. 9 shows that SplineConv feature contributes significantly to the matching accuracy, and NGM can extend seamlessly to NGM-v2 because our method handles the most general form of QAP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND OUTLOOK</head><p>We have presented a novel neural graph matching network, with three main highlights: i) The first graph matching network directly learning Lawler's QAP which is general with a wide range of applications e.g. on QAPLIB beyond visual matching. This is in contrast to many existing works that can only take separate graphs as input. ii) The first deep network for hypergraph matching which involves third-order edges.</p><p>iii) The first network for deep learning of multiple graph matching. Extensive experimental results on synthetic and real-world data show the state-of-the-art performance of our approach. In particular, it shows the notable cost-efficiency advantages against learning-free methods.</p><p>In future work, we will explore more scalable approaches for handling large-scale QAP problems. For graph matching, it indicates more graphs with more nodes for matching. For its generality, we will also apply our model to more application areas beyond computer vision.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Multi</head><label></label><figDesc>(1, 3), and there exist some recent advances in learning arXiv:1911.11308v3 [cs.LG] 6 May 2021</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Graphs with affinity matrix K and affinity tensor H in (b), w.r.t (a) association graph and (c) association hypergraph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>A toy example of connectivity matrices (c)?, (d)H, and their connection to (a) the original graph, (b) adjacency matrix and (e) incidence matrix. All edges are directed, and? i,k =H j,k = 1 means edge k starts from node i and ends at node j. In incident matrix, -1 denotes the starting node and 1 denotes the ending node of all edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>The proposed NGM &amp; NGM-v2 architecture for two-graph matching. The components with blue FC layers i.e. vertex convolution, matchingaware embedding module and vertex classifier are jointly learned with optional CNN, SplineConv (in NGM-v2) and similarity metric ? (in NGM).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc># graphs (?s = 0.1, ?n = 0.02)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Normalized objective score on real-world QAPLIB instances. Only half of the instance labels are shown on x-axis due to limited space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Run time (log-scale) against problem size i.e. number of nodes for each graph. All methods except Sinkhorn-JA are implemented and executed on GPUs, as Sinkhorn-JA is challenging to be parallelized.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 11 .</head><label>11</label><figDesc>Visualization of matching results on 20 Pascal VOC Keypoint categories. Green and red represent correct and incorrect predictions, respectively. As many instances vary a lot in their pose and appearance, this dataset is considered challenging even for deep graph matching.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>The GPU memory cost and running speed during training are listed: NGM (4761MB, 13.7 pairs/s); NGM-v2 (5707MB, 14.1 pairs/s); NHGM (9251MB, 10.1 pairs/s); NHGM-v2 (38060MB, 11.0 pairs/s). NGM and NGM-v2 are comparable to PCA-GM [17] (5165MB, 14.4 pairs/s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Summary of existing learning-free and learning-based methods on two popular formulations of QAP. Note that Lawler's QAP (Eq. 1) is most general which incorporates the Koopmans-Beckmann's QAP (Eq. 3).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2</head><label>2</label><figDesc>Summary of existing literature in learning graph matching based on different types of assignment problems solved by learning, learning modules including CNN, GNN and affinity metric, where GNN embedding is performed and loss functions. KB-QAP abbreviates Koopmans-Beckmann's QAP in Eq. (3). Lawler's QAP in Eq. (1)is the most general QAP form and higher-order assignment in Eq. (4) is its higher-order extension.</figDesc><table><row><cell>method</cell><cell>learned neural net solver</cell><cell>multiple-graph</cell><cell>CNN</cell><cell>GNN module</cell><cell>embedded graph</cell><cell>affinity metric</cell><cell>loss function</cell></row><row><cell>Nowak et al. [16]</cell><cell>special case of KB-QAP</cell><cell>none</cell><cell>none</cell><cell>GCN</cell><cell>individual graphs</cell><cell>inner-product</cell><cell>multi-class cross-entropy</cell></row><row><cell>GMN [25]</cell><cell>none</cell><cell>none</cell><cell>VGG16</cell><cell>none</cell><cell>none</cell><cell>weighted exponential</cell><cell>pixel offset regression</cell></row><row><cell>Zhang et al. [31]</cell><cell>none</cell><cell>none</cell><cell>none</cell><cell>message-passing based CMPNN</cell><cell>individual graphs</cell><cell>inner-product</cell><cell>multi-class cross-entropy</cell></row><row><cell>PCA-GM [17] &amp; IPCA-GM [35]</cell><cell>special case of KB-QAP</cell><cell>none</cell><cell>VGG16</cell><cell>GCN+cross-graph convolution</cell><cell>individual graphs</cell><cell>weighted exponential</cell><cell>binary cross-entropy</cell></row><row><cell>CIE-H [36]</cell><cell>special case of KB-QAP</cell><cell>none</cell><cell>VGG16</cell><cell>edge embedding+ cross-graph conv.</cell><cell>individual graphs</cell><cell>weighted exponential</cell><cell>binary cross-entropy with Hungarian attention</cell></row><row><cell>LCS [32]</cell><cell>special case of Lawler's QAP</cell><cell>none</cell><cell>VGG16</cell><cell>GCN</cell><cell>association graph</cell><cell>fully-connected neural network</cell><cell>binary cross-entropy</cell></row><row><cell>BBGM [37]</cell><cell>none</cell><cell>unlearned fixed solver [38]</cell><cell>VGG16</cell><cell>SplineConv</cell><cell>individual graphs</cell><cell>weighted inner-product</cell><cell>Hamming distance</cell></row><row><cell>NGM (ours)</cell><cell>general case of Lawler's QAP (most general)</cell><cell>none</cell><cell>VGG16</cell><cell>matching-aware GCN</cell><cell>association graph</cell><cell>weighted exponential</cell><cell>binary cross-entropy</cell></row><row><cell>NHGM (ours)</cell><cell>higher-order assignment</cell><cell>none</cell><cell>VGG16</cell><cell>matching-aware hyper-GCN</cell><cell>association hyper-graph</cell><cell>weighted exponential</cell><cell>binary cross-entropy</cell></row><row><cell>NMGM (ours)</cell><cell>general case of Lawler's QAP (most general)</cell><cell>end-to-end spectral method</cell><cell>VGG16</cell><cell>matching-aware GCN</cell><cell>association graph</cell><cell>weighted exponential</cell><cell>binary cross-entropy</cell></row><row><cell>NGM-v2 (ours)</cell><cell>general case of Lawler's QAP (most general)</cell><cell>none</cell><cell>VGG16</cell><cell>SplineConv+ match-aware GCN</cell><cell>individual graphs +asso. graph</cell><cell>weighted inner-product</cell><cell>binary cross-entropy</cell></row><row><cell>NHGM-v2 (ours)</cell><cell>higher-order assignment</cell><cell>none</cell><cell>VGG16</cell><cell>SplineConv+match-aware hyper-GCN</cell><cell>individual graphs +asso. hyper-graph</cell><cell>weighted inner-product</cell><cell>binary cross-entropy</cell></row><row><cell>NMGM-v2 (ours)</cell><cell>general case of Lawler's QAP (most general)</cell><cell>end-to-end spectral method</cell><cell>VGG16</cell><cell>SplineConv+ match-aware GCN</cell><cell>individual graphs +asso. graph</cell><cell>weighted inner-product</cell><cell>binary cross-entropy</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3</head><label>3</label><figDesc>Notation of all our proposed methods and their variants. Matching model introduced in Sec. 3.2 /with enhanced feature in Sec. 3.5. NHGM/-v2 hypergraph matching Neural Hyper-Graph Matching model introduced in Sec. 3.3 /with enhanced feature in Sec. 3.5. NMGM/-v2 multi-graph matching Neural Multi-Graph Matching model introduced in Sec. 3.4 /with enhanced feature in Sec. 3.5. NGM-V classic QAP NGM by replacing Sinkhorn embedding in Eq. (13) with vanilla embedding in Eq. (12).</figDesc><table><row><cell>model</cell><cell>capability</cell><cell>description</cell></row><row><cell cols="3">NGM/-v2 Neural Graph NGM-MH classic QAP classic QAP NGM model with multi-head Sinkhorn embedding (8 multi-head channels).</cell></row><row><cell>NGM-SF</cell><cell>multi-graph matching</cell><cell>learned NGM model followed with learning-free spectral fusion.</cell></row><row><cell>NGM-GX</cell><cell>classic QAP</cell><cell>sample X times by Gumbel-Sinkhorn and select the solution with the best objective.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 4</head><label>4</label><figDesc>Best-performing occurrence count on QAPLIB among all instances and all tested solvers. Our NGM-G5k surpasses all competing methods on most categories and best performs on 72 out of 133 instances.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 5</head><label>5</label><figDesc>Pearson correlation coefficient r (only for |r| ? 0.2 are shown) between the listed statistics for each problem instance in QAPLIB and the corresponding gap of two methods (see Eq.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>79.4 36.4 21.0 31.2 20.8 31.1 54.4 61.2 42.7 79.4 20.2 29.9 20.3 37.7 61.2 39.6 35.6 27.5 72.1 21.6 34.8 24.4 44.2 46.0 41.9 44.7 22.0 41.7 24.3 30.0 58.6 42.0 36.0 30.4 52.3 24.5 58.8 27.4 50.1 66.9 51.2 41.0 22.1 30.8 23.0 72.5 63.4 92.4 61.0 33.3 34.2 33.0 47.3 92.1 43.5 42.3 91.7 36.3 39.9 34.6 60.4 64.2 35.8 42.8 39.3 75.4 29.5 54.5 33.0 69.0 54.1 59.3 59.3 35.3 55.8 35.1 57.2 73.1 19.2 27.9 36.0 69.1 28.0 73.5 26.7 47.1 55.8 66.1 49.1 30.6 33.9 30.7 63.1 86.2 76.4 39.7 38.4 27.0 32.8 31.6 64.2 46.7 65.8 69.1 46.0 29.5 36.9 46.2 62.1 38.1 55.1 40.2 69.7 21.4 52.5 30.8 61.0 39.5 56.2 44.0 39.5 39.9 33.8 37.8 57.8 44.3 63.0 45.7 63.9 29.1 65.3 38.1 65.6 49.2 59.7 47.5 44.8 28.4 39.7 40.9 71.7 51.4 42.1 27.2 30.2 24.6 35.6 68.8 25.5 38.6 38.5 25.0 26.4 26.5 26.7 63.9 29.3 40.3 39.4 28.7 26.1 30.3 33.8 65.6 26.0 45.9 36.5 26.9 31.3 27.7 36.4 67.0 30.8 39.2 38.1 31.2 28.5 37.3 32.0 66.5 28.2 35.9 37.0 24.4 25.0 26.4 26.9 65.9</figDesc><table><row><cell></cell><cell>bottle</cell><cell cols="7">74.7 36.9 35.1 18.1 23.2 18.6 20.7 49.3 NGM Accuracy (diag: 71.0, all: 41.5)</cell><cell></cell><cell>bottle</cell><cell cols="7">88.5 69.7 55.2 37.4 33.4 34.5 48.4 83.6 NGM-v2 Accuracy (diag: 79.5, all: 50.3)</cell><cell></cell><cell>bottle</cell><cell cols="7">81.5 66.6 43.5 41.1 28.7 29.4 27.3 71.4 PCA-GM Accuracy (diag: 65.4, all: 52.4)</cell><cell></cell><cell>bottle</cell><cell>30.1 37.0 38.1 24.6 23.9 28.0 30.3 59.5 GMN Accuracy (diag: 40.2, all: 40.5)</cell><cell>1.0</cell></row><row><cell>Training instance</cell><cell>bus car cat chair dog sofa train</cell><cell cols="7">61.9 63.0 50.1 44.1 47.0 28.7 36.5 27.3 89.9</cell><cell>Training instance</cell><cell>bus car cat chair dog sofa train</cell><cell cols="7">59.0 26.8 65.7 44.4 23.8 28.6 24.7 40.5 95.7</cell><cell>Training instance</cell><cell>bus car cat chair dog sofa train</cell><cell cols="7">36.3 36.8 63.3 41.0 42.3 25.3 34.3 30.5 80.3</cell><cell>Training instance</cell><cell>bus car cat chair dog sofa train</cell><cell>31.5 26.8 44.0 35.6 27.0 27.0 26.4 29.7 77.8</cell><cell>0.6 0.7 0.8 0.9</cell></row><row><cell></cell><cell></cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat Testing instance chair</cell><cell>dog</cell><cell>sofa</cell><cell>train</cell><cell></cell><cell></cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat Testing instance chair</cell><cell>dog</cell><cell>sofa</cell><cell>train</cell><cell></cell><cell></cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat Testing instance chair</cell><cell>dog</cell><cell>sofa</cell><cell>train</cell><cell></cell><cell></cell><cell>bottle</cell><cell>bus</cell><cell>car</cell><cell>cat Testing instance chair</cell><cell>dog</cell><cell>sofa</cell><cell>train</cell><cell>0.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>TABLE 6 81.8 60.0 81.3 87</head><label>6</label><figDesc>Matching accuracy (%) on Pascal VOC Keypoint. Best results are in bold. method aero bike bird boat bottle bus car cat chair cow table dog horse mbkie person plant sheep sofa train tv mean GMN [25] 41.6 59.6 60.3 48.0 79.2 70.2 67.4 64.9 39.2 61.3 66.9 59.8 61.1 59.8 37.2 78.2 68.0 49.9 84.2 91.4 62.4 PCA-GM [17] 49.8 61.9 65.3 57.2 78.8 75.6 64.7 69.7 41.6 63.4 50.7 67.1 66.7 61.6 44.5 81.2 67.8 59.2 78.5 90.4 64.8 IPCA-GM [35] 53.8 66.2 67.1 61.2 80.4 75.3 72.6 72.5 44.6 65.2 54.3 67.2 67.9 64.2 47.9 84.4 70.8 64.0 83.8 90.8 67.7 CIE-H [36] 49.9 63.1 70.7 53.0 82.4 75.4 67.7 72.3 42.4 66.9 69.9 69.5 70.7 62.0 46.7 85.0 70.0 61.8 80.2 91.8 67.6 LCS [32] 46.9 58.0 63.6 69.9 87.8 79.8 71.8 60.3 44.8 64.3 79.4 57.5 64.4 57.6 52.4 96.1 62.9 65.8 94.4 92.0 68.5 BBGM [37] 61.9 71.1 79.7 79.0 87.4 94.0 89.5 80.2 56.8 79.1 64.6 78.9 76.2 75.1 65.2 98.2 77.3 77.0 94.9 93.9 79.0 NGM (ours) 50.1 63.5 57.9 53.4 79.8 77.1 73.6 68.2 41.1 66.4 40.8 60.3 61.9 63.5 45.6 77.1 69.3 65.5 79.2 88.2 64.1 NHGM (ours) 52.4 62.2 58.3 55.7 78.7 77.7 74.4 70.7 42.0 64.6 53.8 61.0 61.9 60.8 46.8 79.1 66.8 55.1 80.9 88.7 64.6 NGM-v2 (ours) 61.8 71.2 77.6 78.8 87.3 93.6 87.7 79.8 55.4 77.8 89.5 78.8 80.1 79.2 62.6 97.7 77.7 75.7 96.7 93.2 80.1 NHGM-v2 (ours) 59.9 71.5 77.2 79.0 87.7 94.6 89.0 .0 78.1 76.5 77.5 64.4 98.7 77.8 75.4 97.9 92.8 80.4 mean valueK, minimum value K min , maximum value K max , standard deviation K std , number of zeros nz, mean degree (of association graph)d, minimum degree d min , maximum degree d max and standard deviation of degree d std . The performance of algorithms is represented by the gap of the solved objective score against upper bound:</figDesc><table><row><cell>gap =</cell><cell>solved score ? upper bnd solved score</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 7</head><label>7</label><figDesc>Controlled experiment by replacing NGM with unlearned ImageNet CNN and RRWM solver. The last row denotes our joint CNN-solver learning method NGM which outperforms. solver aero bike bird boat bottle bus car cat chair cow table dog horse mbkie person plant sheep sofa train tv mean ImgNet VGG16 [56] RRWM [2] 16.1 22.3 20.8 21.8 21.3 31.0 23.2 25.4 18.6 20.5 20.6 21.7 18.8 21.9 13.5 28.6 21.7 18.3 50.5 42.8 24.0 ImgNet VGG16 [56] NGM solver (ours) 30.8 42.5 44.3 33.8 39.8 52.2 49.2 53.9 27.5 42.4 29.3 49.1 45.1 45.1 24.0 48.3 49.9 29.9 70.2 73.3 44.0 NGM VGG16 (ours) RRWM [2] 41.5 54.7 54.3 50.3 67.9 74.3 70.3 60.6 42.3 59.1 48.1 57.3 59.1 56.2 40.6 69.6 63.1 52.2 76.3 87.8 59.3 NGM VGG16 (ours) NGM solver (ours) 50.1 63.5 57.9 53.4 79.8 77.1 73.6 68.2 41.1 66.4 40.8 60.3 61.9 63.5 45.6 77.1 69.3 65.5 79.2 88.2 64.1</figDesc><table><row><cell>CNN</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 8</head><label>8</label><figDesc>Matching accuracy (%) on Willow ObjectClass dataset. The first 5 rows denote learning-based peer methods. Row 6-9 show our two-graph and hypergraph matching methods. Row 10-11 indicate learning-free multi-graph matching baselines, and HiPPI<ref type="bibr" target="#b75">[74]</ref> jointly matches 40, 50, 109, 40, 66 graphs respectively for 5 Willow categories. The last 2 rows denote our two multi-graph matching learning variants.</figDesc><table><row><cell>model</cell><cell># graphs</cell><cell>car</cell><cell>duck</cell><cell>face</cell><cell cols="2">m-bike w-bottle</cell><cell>mean</cell></row><row><cell>GMN [25]</cell><cell>2</cell><cell>67.9</cell><cell>76.7</cell><cell>99.8</cell><cell>69.2</cell><cell>83.1</cell><cell>79.3</cell></row><row><cell>PCA-GM [17]</cell><cell>2</cell><cell>87.6</cell><cell>83.6</cell><cell>100.0</cell><cell>77.6</cell><cell>88.4</cell><cell>87.4</cell></row><row><cell>IPCA-GM [35]</cell><cell>2</cell><cell>90.4</cell><cell>88.6</cell><cell>100.0</cell><cell>83.0</cell><cell>88.3</cell><cell>90.1</cell></row><row><cell>BBGM [37]</cell><cell>2</cell><cell>96.8</cell><cell>89.9</cell><cell>100.0</cell><cell>99.8</cell><cell>99.4</cell><cell>97.2</cell></row><row><cell>LCS [32]</cell><cell>2</cell><cell>91.2</cell><cell>86.2</cell><cell>100.0</cell><cell>99.4</cell><cell>97.9</cell><cell>94.9</cell></row><row><cell>NGM (ours)</cell><cell>2</cell><cell>84.2</cell><cell>77.6</cell><cell>99.4</cell><cell>76.8</cell><cell>88.3</cell><cell>85.3</cell></row><row><cell>NHGM (ours)</cell><cell>2</cell><cell>86.5</cell><cell>72.2</cell><cell>99.9</cell><cell>79.3</cell><cell>89.4</cell><cell>85.5</cell></row><row><cell>NGM-v2 (ours)</cell><cell>2</cell><cell>97.4</cell><cell>93.4</cell><cell>100.0</cell><cell>98.6</cell><cell>98.3</cell><cell>97.5</cell></row><row><cell>NHGM-v2 (ours)</cell><cell>2</cell><cell>97.4</cell><cell>93.9</cell><cell>100.0</cell><cell>98.6</cell><cell>98.9</cell><cell>97.8</cell></row><row><cell>HiPPI [74]</cell><cell>?40</cell><cell>74.0</cell><cell>88.0</cell><cell>100.0</cell><cell>84.0</cell><cell>95.0</cell><cell>88.2</cell></row><row><cell>MGM-Floyd [75]</cell><cell>32</cell><cell>85.0</cell><cell>79.3</cell><cell>100.0</cell><cell>84.3</cell><cell>93.1</cell><cell>88.3</cell></row><row><cell>NMGM (ours)</cell><cell>10</cell><cell>78.5</cell><cell>92.1</cell><cell>100.0</cell><cell>78.7</cell><cell>94.8</cell><cell>88.8</cell></row><row><cell>NMGM-v2 (ours)</cell><cell>10</cell><cell>97.6</cell><cell>94.5</cell><cell>100.0</cell><cell>100.0</cell><cell>99.0</cell><cell>98.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>TABLE 9</head><label>9</label><figDesc>Ablation study of NHGM-v2 on Pascal VOC Keypoint dataset.</figDesc><table><row><cell>model</cell><cell cols="2">accuracy relative acc</cell></row><row><cell>baseline NGM</cell><cell>58.7%</cell><cell></cell></row><row><cell>+ node affinity</cell><cell>59.6%</cell><cell>+0.9%</cell></row><row><cell>+ Sinkhorn embedding</cell><cell>64.1%</cell><cell>+4.5%</cell></row><row><cell>+ SplineConv feature</cell><cell>74.3%</cell><cell>+10.2%</cell></row><row><cell>+ weighted inner-product affinity</cell><cell>80.1%</cell><cell>+5.8%</cell></row><row><cell>+ hypergraph affinity</cell><cell>80.4%</cell><cell>+0.3%</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. https://www2.eecs.berkeley.edu/Research/Projects/CS/vision/ shape/poselets/voc2011 keypoints Feb2012.tgz</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>The work is partially supported by National Key Research and Development Program of China (2020AAA0107600), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), NSFC (61972250, U19B2035).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pca-Gm</forename><surname>Bird</surname></persName>
		</author>
		<idno>1/6 NGM bird: 4/6</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pca-Gm</surname></persName>
		</author>
		<idno>person: 5/9 NGM person: 5/9</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Computers and Intractability; A Guide to the Theory of NP-Completeness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Garey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Johnson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reweighted random walks for graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="492" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A graduated assignment algorithm for graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rangarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="377" to="388" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Random sample consensus: a paradigm for model fitting with applications to image analysis and automated cartography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Fischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Bolles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="381" to="395" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Iterative point matching for registration of free-form curves and surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="152" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey for the quadratic assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Loiola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M M</forename><surname>De Abreu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Boaventura-Netto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Querido</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Operational Research</title>
		<imprint>
			<biblScope unit="volume">176</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="657" to="690" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A spectral technique for correspondence problems using pairwise constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="1482" to="1489" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-graph matching via affinity optimization with graduated consistency regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A branch and bound algorithm for the koopmansbeckmann quadratic assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Edwards</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Combinatorial optimization II</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1980" />
			<biblScope unit="page" from="35" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A linear time algorithm for the koopmans-beckmann qap linearization and related problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Punnen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Kabadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Optimization</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="200" to="209" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A branch-and-cut algorithm for quadratic assignment problems based on linearizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Erdogan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tansel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Operations Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1085" to="1106" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A novel multistart hyper-heuristic algorithm on the grid for the quadratic assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dokeroglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cosar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="10" to="25" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A branch-and-bound algorithm for the quadratic assignment problem based on the hungarian method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EJOR</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="629" to="640" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Graph matching with adaptive and branching path following</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2853" to="2867" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sinkhorn algorithm for lifted assignment problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kushinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dym</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Imaging Sciences</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="716" to="735" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Revised note on learning quadratic assignment with graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nowak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bandeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Science Workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning combinatorial embedding networks for deep graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3056" to="3065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The quadratic assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Lawler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="586" to="599" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Assignment problems and the location of economic activities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">C</forename><surname>Koopmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beckmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="page" from="53" to="76" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient high order matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chertok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Keller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2205" to="2215" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A tensor-based algorithm for high-order graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Duchenne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kweon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2383" to="2395" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Discrete hyper-graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Probabilistic graph and hypergraph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shashua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comput. Vis. Pattern Recog</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hyper-graph matching via reweighted random walks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comput. Vis. Pattern Recog</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1633" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning of graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Pattern Recog</title>
		<imprint>
			<biblScope unit="page" from="2684" to="2693" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning graphs to match</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Consistency-driven alternating optimization for multigraph matching: A unified approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="994" to="1009" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Near-optimal joint object matching via convex relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Mach. Learn. PMLR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Solving the multi-way matching problem by permutation synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pachauri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kondor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Info. Process. Systems. Citeseer</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1860" to="1868" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-image semantic matching by mining consistent features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Pattern Recog</title>
		<imprint>
			<biblScope unit="page" from="685" to="694" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep graphical feature learning for the feature matching problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5087" to="5096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning combinatorial solver for graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7568" to="7577" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">QAPLIB -a quadratic assignment problem library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Burkard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Karisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rendl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global optimization</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="391" to="403" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Practical and efficient multi-view matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Maset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrigoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fusiello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4568" to="4576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Combinatorial learning of robust deep graph matching: an embedding based approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning deep graph matching with channel-independent embedding and hungarian attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Rep</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep graph matching via blackbox differentiation of combinatorial solvers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rol?nek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zietlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Musil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Martius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="407" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A convex relaxation for multi-graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mokarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Neural Netw</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning for graph matching and related combinatorial optimization problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Hancock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Joint Conf. Artificial Intell</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A short survey of recent advances in graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Machine learning for combinatorial optimization: a methodological tour d&apos;horizon</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lodi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prouvost</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Eur. J. Operational Research</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="405" to="421" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Burkard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dellamico</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Martello</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Assignment Problems. SIAM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Adaptive discrete hypergraph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Cybern</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="765" to="779" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A flexible tensor block coordinate ascent scheme for hypergraph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gautier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comput. Vis. Pattern Recog</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5270" to="5278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Joint optimization for consistent multiple graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1649" to="1656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graduated consistency-regularized optimization for multi-graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="407" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Incremental multi-graph matching via diversity and randomness based graph clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="139" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Feature correspondence via graph matching: Models and global optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kolmogorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="596" to="609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Caetano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1048" to="1058" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Unsupervised learning for graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="28" to="45" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Semi-supervised learning and optimization for hypergraph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zanfir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2274" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Ranking via sinkhorn propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1106.1925</idno>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">On the convergence of graph matching: Graduated assignment revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eur. Conf. Comput. Vis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="821" to="835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Rep</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Learn. Rep</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Factorized graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1774" to="1789" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning latent permutations with gumbel-sinkhorn networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Rep</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Visual permutation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Santa</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3100" to="3114" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Automatic learning of edit costs based on interactive and adaptive graph recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Serratosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sol?-Ribalta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cort?s</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Graph-Based Representations in Pattern Recognition</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="152" to="163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Hypergraph convolution and hypergraph attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="page">107637</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zizhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<title level="m">Hypergraph neural networks,&quot; in AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3558" to="3565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Multi-image matching via fast alternating minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4032" to="4040" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Matrix backpropagation for deep networks with structured layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vantzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2965" to="2973" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">SplineCNN: Fast geometric deep learning with continuous b-spline kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Weichert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>M?ller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Vis. Pattern Recog</title>
		<imprint>
			<biblScope unit="page" from="869" to="877" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning combinatorial optimization algorithms over graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dilkina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Info. Process. Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6351" to="6361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Mach. Learn. PMLR</title>
		<imprint>
			<biblScope unit="page" from="1139" to="1147" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. Conf. Learn. Rep</title>
		<imprint>
			<date type="published" when="2014-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">An integer projected fixed point method for graph matching and map inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Neural Info. Process. Systems. Citeseer</publisher>
			<biblScope unit="page" from="1114" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A probabilistic approach to spectral graph matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Egozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Guterman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="18" to="27" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">An extended path following algorithm for graph-matching problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1451" to="1456" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">A hospital facility layout problem finally solved</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krarup</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Manufacturing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="487" to="496" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1365" to="1372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">HiPPI: Higher-order projected power iterations for scalable multimatching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Thunberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Swoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Conf. Comput. Vis</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Unifying offline and online multigraph matching via finding shortest paths on supergraph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Comput. Vis. Pattern Recog</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
