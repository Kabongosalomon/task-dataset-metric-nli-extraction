<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van-Quang</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Sciences</orgName>
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Suganuma</surname></persName>
							<email>suganuma@vision.is.tohoku.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Sciences</orgName>
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">RIKEN Center for AIP</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takayuki</forename><surname>Okatani</surname></persName>
							<email>okatani@vision.is.tohoku.ac.jp</email>
							<affiliation key="aff0">
								<orgName type="department">Graduate School of Information Sciences</orgName>
								<orgName type="institution">Tohoku University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">RIKEN Center for AIP</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GRIT: Faster and Better Image captioning Transformer Using Dual Visual Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:01+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Captioning</term>
					<term>Grid Features</term>
					<term>Region Features</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Current state-of-the-art methods for image captioning employ region-based features, as they provide object-level information that is essential to describe the content of images; they are usually extracted by an object detector such as Faster R-CNN. However, they have several issues, such as lack of contextual information, the risk of inaccurate detection, and the high computational cost. The first two could be resolved by additionally using grid-based features. However, how to extract and fuse these two types of features is uncharted. This paper proposes a Transformer-only neural architecture, dubbed GRIT (Grid-and Regionbased Image captioning Transformer), that effectively utilizes the two visual features to generate better captions. GRIT replaces the CNN-based detector employed in previous methods with a DETR-based one, making it computationally faster. Moreover, its monolithic design consisting only of Transformers enables end-to-end training of the model. This innovative design and the integration of the dual visual features bring about significant performance improvement. The experimental results on several image captioning benchmarks show that GRIT outperforms previous methods in inference accuracy and speed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image captioning is the task of generating a semantic description of a scene in natural language, given its image. It requires a comprehensive understanding of the scene and its description reflecting the understanding. Therefore, most existing methods solve the task in two corresponding steps; they first extract visual features from the input image and then use them to generate a scene's description. The key to success lies in the problem of how we can extract good features.</p><p>Researchers have considered several approaches to the problem. There are two primary methods, referred to as grid features <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b31">32]</ref> and region features <ref type="bibr" target="#b3">[4]</ref>. Grid features are local image features extracted at the regular grid points, often obtained directly from a higher layer feature map(s) of CNNs/ViTs. Region features are a set of local image features of the regions (i.e., bounding boxes)  detected by an object detector. The current state-of-the-art methods employ the region features since they encode detected object regions directly. Identifying objects and their relations in an image will be useful to correctly describing the image. However, the region features have several issues. First, they do not convey contextual information such as objects' relation since the regions do not cover the areas between objects. Second, there is a risk of erroneous detection of objects; important objects could be overlooked, etc. Third, computing the region feature is computationally costly, which is especially true when using a high-performance CNN-based detector, such as Faster R-CNN <ref type="bibr" target="#b39">[40]</ref>.</p><p>The grid features are extracted from the entire image, typically a high-layer feature map of a backbone network. While they do not convey object-level information, they are free from the first two issues with the region features. They may represent contextual information such as objects' relations in images, and they are free from the risk of erroneous object detection.</p><p>In this study, we consider using such region and grid features in an integrated manner, aiming to build a better model for image captioning. The underlying idea is that properly integrating the two types of features will provide a better representation of input images since they are complementary, as explained above. While a few recent studies consider their integration <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b48">49]</ref>, it is still unclear what the best way is. In this study, we reconsider how to extract each from input images and then consider how to integrate them.</p><p>There is yet another issue with the region features, usually obtained by a CNN-based detector. At the last stage of its computation, CNN-based detectors employ non-maximum suppression (NMS) to eliminate redundant bounding boxes. This makes the end-to-end training of the entire model hard, i.e., jointly training the decoder part of the image captioning model and the detector by minimizing a single loss. Recent studies detach the two parts in training; they first train a detector on the object detection task and then train only the decoder part on image captioning. This could be a drag on achieving optimal performance of image captioning.</p><p>To overcome this limitation of CNN-based detectors and also cope with their high-computational cost, we employ the framework of DETR <ref type="bibr" target="#b6">[7]</ref>, which does not need NMS. We choose Deformable DETR <ref type="bibr" target="#b59">[60]</ref>, an improved variant, for its high performance, and also replace a CNN backbone used in the original design with Swin Transformer <ref type="bibr" target="#b30">[31]</ref> to extract initial features from the input image. We also obtain the grid features from the same Swin Transformer. We input its last layer features into a simple self-attention Transformer and update them to obtain our grid features. This aims to model spatial interaction between the grid features, retrieving contextual information absent in our region features.</p><p>The extracted two types of features are fed into the second half of the model, the caption generator. We design it as a lightweight Transformer generating a caption sentence in an autoregressive manner. It is equipped with a unique crossattention mechanism that computes and applies attention from the two types of visual features to caption sentence words.</p><p>These components form a Transformer-only neural architecture, dubbed GRIT (Grid-and Region-based Image captioning Transformer). Our experimental results show that GRIT has established a new state-of-the-art on the standard image captioning benchmark of COCO <ref type="bibr" target="#b29">[30]</ref>. Specifically, in the offline evaluation using the Karpathy test split, GRIT outperforms all the existing methods without vision and language (V&amp;L) pretraining. It also performs at least on a par with SimVLM huge <ref type="bibr" target="#b47">[48]</ref> leveraging V&amp;L pretraining on 1.8B image-text pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Visual Representations for Image Captioning</head><p>Recent image captioning methods typically employ an encoder-decoder architecture. Specifically, given an image, the encoder extracts visual features; the decoder receives the visual features as inputs and generates a sequence of words. Early methods use a CNN to extract a global feature as a holistic representation of the input image <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b20">21]</ref>. Although it is simple and compact, this holistic representation suffers from information loss and insufficient granularity. To cope with this, several studies <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b31">32]</ref> employed more fine-grained grid-based features to represent input images and also used attention mechanisms to utilize the granularity for better caption generation. Later, Anderson et al. <ref type="bibr" target="#b3">[4]</ref> introduced the method of using an object detector, such as Faster R-CNN, to extract objectoriented features, called region features, showing that this leads to performance improvement in many V&amp;L tasks, including image captioning and visual question answering. Since then, region features have become the de facto choice of visual representation for image captioning. Pointing out the high computational cost of the region features, Jiang et al. <ref type="bibr" target="#b18">[19]</ref> showed that the grid features extracted by an object detector perform well on the VQA task. RSTNet <ref type="bibr" target="#b57">[58]</ref> has recently applied these grid features to image captioning.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Self-Attention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Application of Transformer in Vision/Language Tasks</head><p>Transformer has long been a standard neural architecture in natural language processing <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b38">39]</ref>, and started to be extended to computer vision tasks. Besides ViT <ref type="bibr" target="#b9">[10]</ref> for image classification, it was also applied to object detection, leading to DETR <ref type="bibr" target="#b6">[7]</ref>, followed by several variants <ref type="bibr" target="#b59">[60,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b42">43]</ref>. A recent study <ref type="bibr" target="#b49">[50]</ref> applied the framework of DETR to pretraining for various V&amp;L tasks, where they did not use it to obtain the region features.</p><p>Transformer has been applied to image captioning, where it is used as an encoder for extracting and encoding visual features and a decoder for generating captions. Specifically, Yang et al. <ref type="bibr" target="#b52">[53]</ref> proposed to use the self-attention mechanism to encode visual features. Li et al. <ref type="bibr" target="#b26">[27]</ref> used Transformer for obtaining the region features in combination with a semantic encoder that exploits knowledge from an external tagger. Several following studies proposed several variants of Transformer tailored to image captioning, such as Attention on Attention <ref type="bibr" target="#b16">[17]</ref>, X-Linear Attention <ref type="bibr" target="#b35">[36]</ref>, Memory-augmented Attention <ref type="bibr" target="#b7">[8]</ref>, etc. Transformer is naturally employed also as a caption decoder <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b47">48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Grid-and Region-based Image captioning Transformer</head><p>This section describes the architecture of GRIT (Grid-and Region-based Image captioning Transformer). It consists of two parts, one for extracting the dual visual features from an input image (Sec. 3.1) and the other for generating a caption sentence from the extracted features (Sec. 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Extracting Visual Features from Images</head><p>Backbone Network for Extracting Initial Features A lot of efforts have been made to apply the Transformer architecture to various computer vision tasks since ViT <ref type="bibr" target="#b9">[10]</ref> applied it to image classification. ViT divides an input image into small patches and computes global attention over them. This is not suitable for tasks requiring spatially dense prediction, e.g., object detection since the computational complexity increases quadratically with the image resolution.</p><p>Swin Transformer <ref type="bibr" target="#b30">[31]</ref> mitigates this issue to a great extent by incorporating operations such as patch reduction and shifted windows that support local attention. It is currently a de facto standard as a backbone network for various computer vision tasks. We employ it to extract initial visual features from the input image in our model.</p><p>We briefly summarize its structure, explaining how we extract features from the input image and send them to the components following the backbone. Given an input image of resolution H ? W , Swin Transformer computes and updates feature maps through multiple stages; it uses the patch merging layer after every stage (but the last stage) to downsample feature maps in their spatial dimension by the factor of 2. We apply another patch merging layer to downsample the last layer's feature map. We then collect the feature maps from all the stages, obtaining four multi-scale feature maps, i.e., {V l } L b l=1 where L b = 4, which have the resolution from H/8 ? W/8 to H/64 ? W/64. These are inputted to the subsequent modules, i.e., the object detector and the network for generating grid features.</p><p>Generating Region Features As in previous image captioning methods, ours also rely on an object detector to create region features. However, we employ a Transformer-based decoder framework, i.e., DETR <ref type="bibr" target="#b6">[7]</ref> instead of CNN-based detectors, such as Faster R-CNN, which is widely employed by the SOTA image captioning models <ref type="bibr" target="#b3">[4]</ref>. DETR formulates object detection as a direct set prediction problem, which makes the model free of the unideal computation for us, i.e., NMS and RoI alignment. This enables the end-to-end training of the entire model from the input image to the final output, i.e., a generated caption, and also leads to a significant reduction in computational time while maintaining the model's performance on image captioning compared with the SOTA models.</p><p>Specifically, we employ Deformable DETR <ref type="bibr" target="#b59">[60]</ref>, a variant of DETR. Deformable DETR extracts multi-scale features from an input image with its encoder part, which are fed to the decoder part. We use only the decoder part, to which we input the multi-scale features from the Swin Transformer backbone. This leads to further reduction in computational time. We will refer this decoder part as "object detector" in what follows; see <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>The object detector receives two inputs: the multi-scale feature maps generated by the backbone, and N learnable object queries</p><formula xml:id="formula_0">R 0 = {r i } N i=1 , in which r i ? R d .</formula><p>Before forwarding them into the object detector, we apply linear transformation to the multi-scale feature maps, mapping them into d-dimensional vectors as</p><formula xml:id="formula_1">V l ? W r l V l , where {W r l } L b l=1</formula><p>is a learnable projection matrix. Receiving these two inputs, the object detector updates the object queries through a stack of L r deformable layers, yielding R Lr ? R N ?d from the last layer; see <ref type="bibr" target="#b59">[60]</ref> for details. We use R Lr ? R N ?d as our region features R. We forward this to the caption generator.</p><p>Although we train it as a part of our entire model, we pretrain our "object detector" including the vision backbone on object detection before the training of image captioning. For the pretraining, we follow the procedure of Deformable DETR; placing a three-layer MLP and a linear layer on its top to predict box coordinates and class category, respectively. We then minimize a set-based global loss that forces unique predictions via bipartite matching.</p><p>Following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b56">57]</ref>, we pretrain the model (i.e., our object detector including the vision backbone) in two steps. We first train it on object detection following the training method of Deformable DETR. We then fine-tune it on a joint task of object detection and object attribute prediction, aiming to make it learn finegrained visual semantics with the following loss:</p><formula xml:id="formula_2">L v (y,?) = N i=1 [?logp? (i) (c i ) + 1 ci? =? L box (b i ,b? (i) ) object detection ?logp? (i) (a i ) attribute prediction ], (1) wherep? (i) (a i ) andp? (i) (c i ) are the attribute and class probabilities, L box (b i ,b? (i) )</formula><p>is the loss for normalized bounding box regression for object i <ref type="bibr" target="#b59">[60]</ref>.</p><p>Grid Feature Network This network receives the last one of the multi-scale feature maps from the Swin Transformer backbone, i.e.,</p><formula xml:id="formula_3">V L b ? R M ?d L b , where M = H/64 ? W/64.</formula><p>As with the input to the object detector, we apply a linear transformation with a learnable matrix</p><formula xml:id="formula_4">W g ? R d?d L b to V L b , obtaining G 0 = W g V L b</formula><p>We employ the standard self-attention Transformer having L g layers. This network updates V L b through these layers, yielding our grid features G represented as a M ? d matrix. We intend to extract contextual information hidden in the input image by modeling the spatial interaction between the grid features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Caption Generation Using Dual Visual Features</head><p>Overall Design of Caption Generator The caption generator receives the two types of visual features, the region features R ? R N ?d and the grid features G ? R M ?d , as inputs. Apart from this, we employ the basic design employed in previous studies <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b13">14]</ref> that is based on the Transformer architecture. It generates a caption sentence in an autoregressive manner; receiving the sequence of predicted words (rigorously their embeddings) at time t ? 1, it predicts the next word at time t. We employ the sinusoidal positional embedding of time step t <ref type="bibr" target="#b43">[44]</ref>; we add it to the word embedding to obtain the input x t 0 ? R d at t. The caption generator consists of a stack of L c identical layers. The initial layer receives the sequence of predicted words and the output from the last layer is input to a linear layer whose output dimension equals the vocabulary size to predict the next word. Each transformer layer has a sub-layer of masked self-attention over the sentence words and a sub-layer(s) of cross-attention between them and the visual features in this order, followed by a feedforward network (FFN) sub-layer. The masked self-attention sub-layer at the l-th layer receives an input sequence {x l?1 i } t i=0 at time step t, and computes and applies self-attention over the sequence to update the tokens with the attention mask to prevent the interaction from the future words during training.</p><p>The cross-attention sub-layer in the layer l, located after the self-attention sub-layer, fuses its output with the dual visual features by cross-attention between them, yielding A l . We consider the three design choices shown in <ref type="figure" target="#fig_2">Fig. 3</ref> and described below. We examine their performance through experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cross-attention between Caption Word and Dual Visual Features</head><p>We show three designs of cross-attention between the word features and the dual visual features (i.e., the region features R and the grid features G) as below.</p><p>Concatenated Cross-Attention The simplest approach is to concatenate the two visual features and use the resultant features as keys and values in the standard multi-head attention sub-layer, where the words serve as queries; see <ref type="figure" target="#fig_2">Fig. 3</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(a).</head><p>Sequential Cross-Attention Another approach is to perform cross-attention computation separately for the two visual features. The corresponding design is to place two independent multi-head attention sub-layers in a sequential fashion, and uses one for the grid features and the other for the region features (or the opposite combination); see <ref type="figure" target="#fig_2">Fig. 3</ref>(b). Note that their order could affect the performance.</p><p>Parallel Cross-Attention The third approach is to perform multi-head attention computation on the two visual features in parallel. To do so, we use two multihead attention mechanisms with independent learnable parameters. The detailed design is as follows. Let X l?1 = {x l?1 i } be the word features inputted to the meta-layer l containing this cross attention sub-layer. As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, they are first input to the self-attention sub-layer, converted into X ? l = {x ? i } (layer index l omitted for brevity) and then input to this cross attention sub-layer. In this sub-layer, multi-head attention (MHA) is computed with {x ? i } as queries and the region features R as keys and values, yielding attended features {a r i }. The same computation is performed in parallel with the grid features G as keys and values, yielding {a g i }. Next, we concatenate them with x ? i as [a r i ; x ? i ] and [a g i ; x ? i ], projecting them back to d-dimensional vector using learnable affine projections. Normalizing them with sigmoid into probabilities {c r i } and {c g i }, respectively, we have</p><formula xml:id="formula_5">c g i = sigmoid(W g [a g i ; x ? i ] + b g ),<label>(2)</label></formula><formula xml:id="formula_6">c r i = sigmoid(W r [a r i ; x ? i ] + b r ).<label>(3)</label></formula><p>We then multiply them with {a r i } and {a g i }, add the resultant vectors to {x ? i }, and finally feed to layer normalization, obtaining</p><formula xml:id="formula_7">A l = {a (l) i } as follows: a (l) i = LN(c g i ? a g i + c r i ? a r i + x ? i ).<label>(4)</label></formula><p>Caption Generator Losses Following a standard practice of image captioning studies, we pre-train our model with a cross-entropy loss (XE) and finetune it using the CIDEr-D optimization with self-critical sequence training strategy <ref type="bibr" target="#b40">[41]</ref>. Specifically, the model is first trained to predict the next word x * t at t = 1..T , given the ground-truth sentence x * 1:T . This is equal to minimize the following XE loss with respect to the model's parameter ?:</p><formula xml:id="formula_8">L XE (?) = ? T t=1 log p ? x * t | x * 0:t?1 .<label>(5)</label></formula><p>We then finetune the model with the CIDEr-D optimization, where we use the CIDEr score as the reward and the mean of the rewards as the reward baseline, following <ref type="bibr" target="#b7">[8]</ref>. The loss for self-critical sequence training is given by</p><formula xml:id="formula_9">L RL (?) = ? 1 k k i=1 (r(w i ) ? b) log p(w i ),<label>(6)</label></formula><p>where w i is the i-th sentence in the beam; r(?) is the reward function; and b is the reward baseline; and k is the number of samples in the batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Object Detection As mentioned earlier, we train our object detector (including the backbone) in two steps. In the first step, we train it on object detection using either Visual Genome <ref type="bibr" target="#b24">[25]</ref> or a combination <ref type="bibr" target="#b56">[57]</ref> of four datasets: COCO <ref type="bibr" target="#b29">[30]</ref>, Visual Genome, Open Images <ref type="bibr" target="#b25">[26]</ref>, and Object365 <ref type="bibr" target="#b41">[42]</ref>, depending on what previous methods we experimentally compare. In the second step, we train the model on object detection plus attribute prediction using Visual Genome. Note that following the standard practice, we exclude the duplicated samples appearing in the testing and validation splits of the COCO and nocaps <ref type="bibr" target="#b1">[2]</ref> datasets to remove data contamination. See the supplementary material for more details.</p><p>Image Captioning We conduct our experiments on the COCO dataset, the standard for the research of image captioning <ref type="bibr" target="#b29">[30]</ref>. The dataset contains 123,287 images, each annotated with five different captions. For offline evaluation, we follow the widely adopted Karpathy split <ref type="bibr" target="#b19">[20]</ref>, where 113,287, 5,000, and 5,000 images are used for training, validation, and testing respectively.</p><p>To test our method's effectiveness on other image captioning datasets, we also report the performances on the nocaps dataset and the Artemis dataset <ref type="bibr" target="#b0">[1]</ref>. See the supplementary material for more details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Evaluation Metrics We employ the standard evaluation protocol for the evaluation of methods. Specifically, we use the full set of captioning metrics: BLEU@N <ref type="bibr" target="#b36">[37]</ref>, METEOR <ref type="bibr" target="#b4">[5]</ref>, ROUGE-L <ref type="bibr" target="#b28">[29]</ref>, CIDEr <ref type="bibr" target="#b44">[45]</ref>, and SPICE <ref type="bibr" target="#b2">[3]</ref>. We will use the abbreviations, B@N, M, R, C, and S, to denote BLEU@N, METEOR, ROUGE-L, CIDEr, and SPICE, respectively.</p><p>Hyperparameters Settings In our model, we set the dimension d of each layer to 512, the number of heads to eight. We employ dropout with the dropout rate of 0.2 on the output of each MHA and FFN sub-layer following <ref type="bibr" target="#b43">[44]</ref>. We set the number of layers as L r = 6 for the object detector, as L g = 3 for the grid feature network, and as L c = 3 for the caption generator. Following previous studies, we convert all the captions to lower-case, remove punctuation characters, and perform tokenization with the SpaCy toolkit <ref type="bibr" target="#b15">[16]</ref>. We build the vocabularies, excluding the words which appear less than five times in the training and validation splits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training Details</head><p>First Stage In the first stage, we pretrain the object detector with the backbone. We consider several existing region-based methods for comparison, which employ similar pretraining of an object detector but use different datasets. For a fair comparison, we consider two settings. One uses Visual Genome for training, following most previous methods. We train our detector for 150,000 iterations with a batch size of 32. The other (results indicated with ? in what follows) uses the four datasets mentioned above, following <ref type="bibr" target="#b56">[57]</ref>. We train the detector for 125,000 iterations with a batch size of 256. In both settings, the input image is resized so that the maximum for the shorter side is 800 and for the longer side is 1333. We use Adam optimizer <ref type="bibr" target="#b23">[24]</ref> with a learning rate of 10 ?4 , decreased by 10 at iteration 120,000 and 100,000 in the first and second settings, respectively. We follow <ref type="bibr" target="#b59">[60]</ref> for other training procedures. After this, we finetune the models on object detection plus attribute prediction using Visual Genome for additional five epochs with a learning rate of 10 ?5 , following <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b56">57]</ref>. The supplementary material presents the details of implementation and experimental results on object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Second Stage</head><p>We train the entire model for the image captioning task in the second stage. We employ the standard method for word representation, i.e., linear projections of one-hot vectors to vectors of dimension d = 512. In this stage, we resize all the input images so that the maximum dimensions for the shorter side and longer side are 384 and 640, respectively. We train models, as explained earlier. Specifically, we train models with the cross-entropy loss L XE for ten epochs, in which we warmp up the learning rates for the grid feature network and the caption generator from 10 ?5 to 10 ?4 in the first epoch, while we fix those for the backbone network and the object detector at 10 ?5 . Then, we finetune the model based on the CIDEr-D optimization for ten epochs, where we set the fixed learning rate to 5 ? 10 ?6 for the entire model. We use the Adam optimizer <ref type="bibr" target="#b23">[24]</ref> with a batch size of 128. For the CIDEr-D optimization, we use beam search with a beam size of 5 and a maximum length of 20.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance of Different Configurations</head><p>Our method has several design choices. We conduct experiments to examine which configuration is the best. The results are shown in <ref type="table" target="#tab_2">Table 1</ref>. We used an identical configuration unless otherwise noted. Specifically, we use the feature extractor pretrained on the four datasets and parallel cross-attention for fusing the region and grid features.</p><p>The first block of <ref type="table" target="#tab_2">Table 1</ref>(a) shows the effects of different (pre)training strategies of the visual backbone on image captioning performance. The 'ImageNet' column shows the result of the model using a Swin Transformer backbone pretrained on ImageNet21K and the grid features alone; 'VG' and '4DS' indicate the models with a detector pretrained on Visual Genome and the four datasets, respectively. They show that using more datasets leads to better performance.</p><p>The second block of <ref type="table" target="#tab_2">Table 1</ref>(a) shows the effects of the number of object queries, or equivalently region features. The performance increases as they vary as 50, 100, and 150. We also confirmed that the performance is saturated for more region features, while the computational cost and false detection increase.</p><p>The third block shows the effect of the end-to-end training of the entire model. 'Yes' indicates the end-to-end training of the entire model and 'No' indicates training the model but the vision backbone. The results show that the end-to-end training considerably improves CIDEr score (from 139.6 to 144.3) with little sacrifice of B@4. This validates our expectation about the effectiveness of the end-to-end training; it arguably helps reduce the domain gap between object detection and image captioning.</p><p>The first block of <ref type="table" target="#tab_2">Table 1</ref>(b) shows the performances of the model employing the concatenated cross-attention and its two variants using the grid features alone or the region features alone. They show that the region features alone work better than the grid features alone, and their fusion achieves the highest performance.</p><p>The three blocks of <ref type="table" target="#tab_2">Table 1</ref>(b) show the performances of the three crossattention architectures explained in Sec. 3.2. The second block shows the two variants of the sequential cross-attention, and the third block shows the two variants of the parallel cross-attention with different gated activation functions, i.e., sigmoid and identity. By identity activation, we mean setting all the values of c g l and c r l in Eq.(4) to one. These results show that the parallel cross-attention with sigmoid activation function performs the best; the sequential cross-attention in the order G ? R attains the second best result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results on the COCO Dataset</head><p>We next show complete results on the COCO dataset by the offline and online evaluations. We present example results in the supplementary material. <ref type="table" target="#tab_3">Table 2</ref> shows the performances of our method and the current state-of-the-art methods on the offline Karpathy test split. The compared methods are as follows: grid-based methods <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b57">58]</ref>, region-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b10">11]</ref>, the methods employing both grid and region features <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b33">34]</ref>, and also the methods relying on large-scale pretraining on vision and language (V&amp;L) tasks using a large image-text corpus <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b56">57]</ref>, including SimVLM huge , a model pretrained on an extremely large dataset (i.e., 1.8 billion image-caption pairs) <ref type="bibr" target="#b47">[48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Offline Evaluation</head><p>For fair comparison with the region-based methods, we report the results of two variants of our model, one with the object detector pretrained on Visual Genome alone and the other (marked with ? ) with the object detector pretrained on the four datasets, as explained earlier. It is seen from <ref type="table" target="#tab_3">Table 2</ref> that our models, regardless of the datasets used for the detector's pretraining, outperform all the methods that do not use large-scale pretraining of vision and language tasks (i.e., the methods in the second block entitled 'w/o VL pretraining'). Moreover, our model with the detector pretrained solely on Visual Genome (i.e., 'GRIT') performs better than those relying on large-scale V&amp;L pretraining but SimVLM huge . Finally, our model with the pretrained detector on multiple datasets (i.e., 'GRIT ? ') outperforms SimVLM huge leveraging large-scale V&amp;L pretraining in CIDEr score (i.e., 144.2 vs 143.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Online Evaluation</head><p>We also evaluate our models (i.e., a single model and an ensemble of six models) on the 40K testing images by submitting their results on the official evaluation server. <ref type="table" target="#tab_4">Table 3</ref> shows the results and those of all the published methods on the leaderboard. <ref type="table" target="#tab_4">Table 3</ref> presents the metric scores based on five (c5) and 40 reference captions (c40) per image. We can see that our method achieves the best scores for all the metrics. Note that even our single model outperforms all the published methods that use ensembles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Results on the ArtEmis and nocaps Datasets</head><p>As explained above, we evaluate our method on the ArtEmis and nocaps datasets. For nocaps, we evaluate zero-shot inference performance, i.e., the performance of the model trained on COCO. For ArtEmis, we train the model in the same way as COCO except for the number of training epochs, precisely, five epochs each for the training with the XE loss and that with the CIDEr-D optimization. <ref type="table" target="#tab_5">Table 4</ref>(a) shows the results of our method on the test split of ArtEmis <ref type="bibr" target="#b0">[1]</ref>. It also show the results of existing methods reported in <ref type="bibr" target="#b0">[1]</ref>, which are grid-based <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b45">46]</ref>, region-based <ref type="bibr" target="#b7">[8]</ref>, and a nearest neighbor method using a holistic vector to encode images (denoted as H). Our method outperforms all these methods by a large margin. <ref type="table" target="#tab_5">Table 4</ref> shows the results on the nocaps dataset, including the baseline methods reported in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8]</ref>. All the models are trained on the training split of the COCO datasets and tested on the validation split of nocaps, which consists of images with novel objects and captions with unseen vocabularies. Our method surpasses all the other methods including region-based methods <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8]</ref> in both in-domain and out-of-domain images. See the supplementary material for the full results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Computational Efficiency</head><p>We measured the inference time of GRIT and two representative region-based methods, VinVL <ref type="bibr" target="#b56">[57]</ref> and M 2 Transformer <ref type="bibr" target="#b7">[8]</ref>. It is the computational time per image from image input to caption generation. Specifically, we measured the time to generate a caption of length 20 with a beam size of five on a V100 GPU. The input image resolution was set to 800 ? 1333 for VinVL and M 2 Transformer as reported in <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b56">57]</ref>. We set it to 384 ? 640 for GRIT since it already achieves higher accuracy. <ref type="figure" target="#fig_0">Figure 1</ref> shows the breakdown of the inference time for the three methods. GRIT reduces the time for feature extraction by a factor of 10 compared with the others. Similar to M 2 Transformer, GRIT has a lightweight caption generator and thus spends much less time than VinVL for generating a caption after receiving the visual features. GRIT can run with minibatch size up to 64 on a single V100 GPU, while others cannot afford large minibatch. With minibatch size ? 32, the per-image inference time decreases to about 32ms. More details are given in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Summary and Conclusion</head><p>In this paper, we have proposed a Transformer-based architecture for image captioning named GRIT. It integrates the region features and the grid features extracted from an input image to extract richer visual information from input images. Previous SOTA methods employ a CNN-based detector to extract region features, which prevents the end-to-end training of the entire model and makes to high computational costs. Using the Swin Transformer for a backbone extracting the initial visual feature, GRIT resolves these two issues by employing a DETR-based detector. Furthermore, GRIT obtains grid features by updating the feature from the same backbone using a self-attention Transformer, aiming to extract richer context information complementing the region feature. These two features are fed to the caption generator equipped with a unique cross-attention mechanism, which computes and applies attention from the dual features on the generated caption sentence. The integration of all these components led to significant performance improvement. The experimental results validated our approach, showing that GRIT outperforms all published methods by a large margin in inference accuracy and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Details for Object Detection</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Object Detection Datasets</head><p>When pretraining our model on the four datasets (i.e., Visual Genome (VG), COCO, OpenImages, and Objects365), we follow <ref type="bibr" target="#b56">[57]</ref> to build a unified training corpus with the statistics shown in <ref type="table" target="#tab_6">Table 5</ref> except that we do not use the annotations from COCO stuff <ref type="bibr" target="#b5">[6]</ref>. The resultant corpus has images with 1848 categories. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Implementation Details</head><p>For the object detector, we set the number of queries N = 150, the number of sampling points equal to 4, and the hidden dimension d = 512. The backbone network weights are intialized by the weights of Swin-Base (384?384) pretrained on ImageNet21K <ref type="bibr" target="#b30">[31]</ref>. Following <ref type="bibr" target="#b59">[60]</ref>, the loss for normalized bounding box regression for object i, L box (b i ,b? (i) ) is computed as the weighted summation of a box distance L l1 and a GIoU loss L iou :</p><formula xml:id="formula_10">L l1 (b i ,b? (i) ) = ||b i ?b? (i) || 1 ,<label>(7)</label></formula><formula xml:id="formula_11">L iou (b i ,b? (i) ) = 1 ? |b i ?b ?(i) | |b i ?b ?(i) | ? |B(b i ,b ?(i) )\b i ?b ?(i) | |B(b i ,b ?(i) )| ,<label>(8)</label></formula><formula xml:id="formula_12">L box (b i ,b? (i) ) = ? l1 L l1 (b i ,b? (i) ) + ? iou L iou (b i ,b? (i) ),<label>(9)</label></formula><p>where ? l1 = 5, ? iou = 2, and B outputs the largest box covering b i andb? (i) . We also employ two training strategies, i.e., iterative bounding box refinement and auxiliary losses; see <ref type="bibr" target="#b59">[60]</ref> and our configuration files for details. <ref type="table" target="#tab_7">Table 6</ref> shows the performance on the COCO validation split and the Visual Genome test split of our object detector compared with VinVL and BUTD <ref type="bibr" target="#b3">[4]</ref>. It is seen that the object detector of GRIT attains comparable or higher performance on the two datasets as compared with BUTD and VinVL when pretrained on the similar datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Object Detection Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional Details for Image Captioning</head><p>Class Token We prepend a class token embedding g ?cls? ? R d to G 0 before forwarding them to the grid feature network. We use this class token embedding to predict the emotion category of the input image when training an emotiongrounded model on the ArtEmis dataset; see Sec. B.2.</p><p>Boundary Tokens Following previous studies, we prepend a special token ?sos? to the beginning of captions, and append another special token ?eos? to the end of captions during training. During inference, we start the generation by setting the first token to ?sos?. Emotion Grounded Model Following <ref type="bibr" target="#b0">[1]</ref>, we also trained an emotion grounded model, which predicts the emotion associated with the caption. Specifically, we mapped the updated class embedding g ?cls? into an 8-dimensional vector using a linear projection. During training, we minimized the summation of the two losses, i.e., emotion prediction and caption generation. <ref type="table" target="#tab_9">Table 8</ref> shows the full results of different models on the test split of the Artemis dataset including the emotion grounded models. It is noted that the ground truth emotion labels are not provided during inference. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Full Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Image Captioning on the nocaps Dataset</head><p>Full results We report the full results on the validation split of the nocaps dataset for different domains, i.e., in-domain, near-domain, and out-of-domain, in <ref type="table" target="#tab_10">Table 9</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Computational Efficiency</head><p>We measured the inference time of GRIT and two representative region-based methods, VinVL <ref type="bibr" target="#b56">[57]</ref> and M 2 Transformer [8], on the same machine having a Tesla V100-SXM2 of 16GB memory with CUDA version 10.0 and Driver version 410.104. It has Intel(R) Xeon(R) Gold 6148 CPU. The comparison was conducted following <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23]</ref>. Specifically, we excluded the time of preprocessing the image and loading it to the GPU device. Also, the images are rescaled to the resolutions such that all the compared methods achieve its highest performance for image captioning. For the compared methods, we used the official implementations of M 2 Transformer 3 and VinVL 4 . Regarding feature extraction, we extracted the region features from Faster R-CNN using the original implementation 5 used by M 2 Transformer and another implementation 6 used by VinVL. It is seen that VinVL and M 2 Transformer spend considerable time on feature extraction due to the forward pass through the CNN backbone with high resolution inputs and the computationally expensive regional operations. It is also noted that VinVL introduced class-agnostic NMS operations, which reduce a great amount of time consumed by class-aware NMS operations in the standard Faster R-CNN. On the other hand, we employ a Deformable DETR-based detector to extract region features without using all such operations. <ref type="table" target="#tab_2">Table 10</ref> shows the comparison on feature extraction.</p><p>Regarding caption generation, all the methods use beam search as the decoding strategy, with beam size of 5 and the maximum caption length of 20. Both M 2 Transformer and GRIT employ a lightweight caption generator (caption decoder) having only 3 transformer layers with hidden dimension of 512 while VinVL large has 24 transformer layers with hidden dimension of 1024; see <ref type="table" target="#tab_2">Table 11</ref>. Thus, with the visual features as inputs, M 2 Transformer and GRIT spend less inference time generating words than VinVL large in the autoregressive manner.   show some examples of the captions generated by our proposed method (GRIT) and another region-based method (M 2 Transformer) given the same input images from the COCO test split. It is observed that the generated captions from GRIT are qualitatively better than those generated by the baseline method in terms of detecting and counting objects as well as describing their relationships in the given images. The inaccuracy of the captions generated by the baseline method might be due to the drawbacks of the region features extracted by a frozen pretrained object detector which produces wrong detection and lacks of contextual information.</p><p>GT-1: a child is brushing her</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Comparison of GRIT and other region-based methods for image captioning. Left: Running time per image of performing inference with beam size of five and the maximum length of 20 on a V100 GPU. Right: Their architectures</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Overview of the architecture of GRIT</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Three designs of cross-attention mechanism to use dual visual features</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 , 5 , 6 ,</head><label>456</label><figDesc>and 7</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results of ablation tests on the COCO test split. All the models are trained with the XE loss and finetuned by the CIDEr optimization.</figDesc><table><row><cell>(a)</cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell><cell></cell></row><row><cell>Factor</cell><cell cols="2">Choice CIDEr B@4</cell><cell>Cross Attention</cell><cell cols="2">Choice CIDEr B@4</cell></row><row><cell cols="3">(1) Backbone Network ImageNet 135.5 41.5</cell><cell>(1) Concatenated</cell><cell>G</cell><cell>142.1 41.7</cell></row><row><cell>-Training data</cell><cell>VG</cell><cell>142.3 41.9</cell><cell>-Visual features</cell><cell>R</cell><cell>142.9 41.9</cell></row><row><cell></cell><cell>4DS</cell><cell>144.2 42.4</cell><cell></cell><cell cols="2">[G ; R] 143.1 41.9</cell></row><row><cell>(2) Region features</cell><cell>50</cell><cell>141.4 41.9</cell><cell>(2) Sequential</cell><cell></cell><cell></cell></row><row><cell>-Number of vectors</cell><cell>100</cell><cell>141.8 41.5</cell><cell cols="3">-Sequential order G ? R 144.0 42.1</cell></row><row><cell>(trained on VG)</cell><cell>150</cell><cell>142.3 41.9</cell><cell></cell><cell cols="2">R ? G 143.6 42.1</cell></row><row><cell>(3) Training strategy</cell><cell></cell><cell></cell><cell>(3) Parallel</cell><cell></cell><cell></cell></row><row><cell>-End-to-end training</cell><cell>Yes</cell><cell>144.2 42.4</cell><cell cols="3">-Gated activation Sigmoid 144.2 42.4</cell></row><row><cell></cell><cell>No</cell><cell>139.6 42.7</cell><cell></cell><cell cols="2">Identity 143.9 41.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Offline results evaluated on the COCO Karpathy test split. 'V. E. type' indicates the type of visual features; '# VL Data' is the number of image-text pairs used for vision-language pretraining.</figDesc><table><row><cell>Method</cell><cell cols="2">V. E. # VL</cell><cell></cell><cell cols="4">Performance Metrics</cell></row><row><cell></cell><cell cols="4">Type Data B@1 B@4</cell><cell>M</cell><cell>R</cell><cell>C</cell><cell>S</cell></row><row><cell>w/ VL pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>UVLP [59]</cell><cell>R</cell><cell>3.0M</cell><cell>-</cell><cell cols="2">39.5 29.3</cell><cell>-</cell><cell>129.3 23.2</cell></row><row><cell>Oscarbase [28]</cell><cell>R</cell><cell>6.5M</cell><cell>-</cell><cell cols="2">40.5 29.7</cell><cell>-</cell><cell>137.6 22.8</cell></row><row><cell>VinVL  ? large [57]</cell><cell>R</cell><cell>8.9M</cell><cell>-</cell><cell cols="2">41.0 31.1</cell><cell>-</cell><cell>140.9 25.2</cell></row><row><cell>SimVLMhuge [48]</cell><cell>G</cell><cell>1.8B</cell><cell>-</cell><cell cols="2">40.6 33.7</cell><cell>-</cell><cell>143.3 25.4</cell></row><row><cell>w/o VL pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SAT [46]</cell><cell>G</cell><cell>-</cell><cell>-</cell><cell cols="4">31.9 25.5 54.3 106.3</cell><cell>-</cell></row><row><cell>SCST [41]</cell><cell>G</cell><cell>-</cell><cell>-</cell><cell cols="4">34.2 26.7 55.7 114.0</cell><cell>-</cell></row><row><cell>RSTNet [58]</cell><cell>G</cell><cell>-</cell><cell cols="5">81.8 40.1 29.8 59.5 135.6 23.0</cell></row><row><cell>Up-Down [4]</cell><cell>R</cell><cell>-</cell><cell cols="5">79.8 36.3 27.7 56.9 120.1 21.4</cell></row><row><cell>RFNet [22]</cell><cell>R</cell><cell>-</cell><cell cols="5">79.1 36.5 27.7 57.3 121.9 21.2</cell></row><row><cell>GCN-LSTM [55]</cell><cell>R</cell><cell>-</cell><cell cols="5">80.5 38.2 28.5 58.3 127.6 22.0</cell></row><row><cell>LBPF [38]</cell><cell>R</cell><cell>-</cell><cell cols="5">80.5 38.3 28.5 58.4 127.6 22.0</cell></row><row><cell>SGAE [52]</cell><cell>R</cell><cell>-</cell><cell cols="5">80.8 38.4 28.4 58.6 127.8 22.1</cell></row><row><cell>AoA [17]</cell><cell>R</cell><cell>-</cell><cell cols="5">80.2 38.9 29.2 58.8 129.8 22.4</cell></row><row><cell>NG-SAN [13]</cell><cell>R</cell><cell>-</cell><cell>-</cell><cell cols="4">39.9 29.3 59.2 132.1 23.3</cell></row><row><cell>GET [18]</cell><cell>R</cell><cell>-</cell><cell cols="5">81.5 39.5 29.3 58.9 131.6 22.8</cell></row><row><cell>ORT [14]</cell><cell>R</cell><cell>-</cell><cell cols="5">80.5 38.6 28.7 58.4 128.3 22.6</cell></row><row><cell>ETA [27]</cell><cell>R</cell><cell>-</cell><cell cols="5">81.5 39.3 28.8 58.9 126.6 22.6</cell></row><row><cell>M 2 Transformer [8]</cell><cell>R</cell><cell>-</cell><cell cols="5">80.8 39.1 29.2 58.6 131.2 22.6</cell></row><row><cell>X-LAN [36]</cell><cell>R</cell><cell>-</cell><cell cols="5">80.8 39.5 29.5 59.2 132.0 23.4</cell></row><row><cell>TCIC [11]</cell><cell>R</cell><cell>-</cell><cell cols="5">81.8 40.8 29.5 59.2 135.4 22.5</cell></row><row><cell>Dual Global [49]</cell><cell>R+G</cell><cell>-</cell><cell cols="5">81.3 40.3 29.2 59.4 132.4 23.3</cell></row><row><cell>DLCT [34]</cell><cell>R+G</cell><cell>-</cell><cell cols="5">81.4 39.8 29.5 59.1 133.8 23.0</cell></row><row><cell>GRIT</cell><cell>R+G</cell><cell>-</cell><cell cols="5">83.5 41.9 30.5 60.5 142.2 24.2</cell></row><row><cell>GRIT  ?</cell><cell>R+G</cell><cell>-</cell><cell cols="5">84.2 42.4 30.6 60.7 144.2 24.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Online evaluation results on the COCO image captioning dataset</figDesc><table><row><cell>Method</cell><cell>Ensemble</cell><cell>c5</cell><cell>B-1 c40</cell><cell>c5</cell><cell>B-2 c40</cell><cell>c5</cell><cell>B-3 c40</cell><cell>c5</cell><cell>B-4 c40</cell><cell>c5</cell><cell>M</cell><cell>c40</cell><cell>c5</cell><cell>R</cell><cell>c40</cell><cell>c5</cell><cell>C</cell><cell>c40</cell></row><row><cell>w/ VL pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>VinVLlarge [57]</cell><cell>?</cell><cell cols="17">81.9 96.9 66.9 92.4 52.6 84.7 40.4 74.9 30.6 40.8 60.4 76.8 134.7 138.7</cell></row><row><cell>w/o VL pretraining</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SCST [41]</cell><cell>?</cell><cell cols="17">78.1 93.7 61.9 86.0 47.0 75.9 35.2 64.5 27.0 35.5 56.3 70.7 114.7 116.7</cell></row><row><cell>Up-Down [4]</cell><cell>?</cell><cell cols="17">80.2 95.2 64.1 88.8 49.1 79.4 36.9 68.5 27.6 36.7 57.1 72.4 117.9 120.5</cell></row><row><cell>HAN [47]</cell><cell>?</cell><cell cols="17">80.4 94.5 63.8 87.7 48.8 78.0 36.5 66.8 27.4 36.1 57.3 71.9 115.2 118.2</cell></row><row><cell>GCN-LSTM [55]</cell><cell>?</cell><cell cols="17">80.8 95.2 65.5 89.3 50.8 80.3 38.7 69.7 28.5 37.6 58.5 73.4 125.3 126.5</cell></row><row><cell>SGAE [52]</cell><cell>?</cell><cell cols="17">81.0 95.3 65.6 89.5 50.7 80.4 38.5 69.7 28.2 37.2 58.6 73.6 123.8 126.5</cell></row><row><cell>AoA [17]</cell><cell>?</cell><cell cols="17">81.0 95.0 65.8 89.6 51.4 81.3 39.4 71.2 29.1 38.5 58.9 74.5 126.9 129.6</cell></row><row><cell>HIP [54]</cell><cell>?</cell><cell cols="17">81.6 95.9 66.2 90.4 51.5 81.6 39.3 71.0 28.8 38.1 59.0 74.1 127.9 130.2</cell></row><row><cell>M 2 Trans. [8]</cell><cell>?</cell><cell cols="17">81.6 96.0 66.4 90.8 51.8 82.7 39.7 72.8 29.4 39.0 59.2 74.8 129.3 132.1</cell></row><row><cell>X-LAN [36]</cell><cell>?</cell><cell cols="17">81.9 95.7 66.9 90.5 52.4 82.5 40.3 72.4 29.6 39.2 59.5 75.0 131.1 133.5</cell></row><row><cell>Dual Global [49]</cell><cell>?</cell><cell cols="17">80.8 95.1 65.6 81.3 51.1 81.3 39.1 71.2 28.9 38.4 58.9 74.4 126.3 129.2</cell></row><row><cell>DLCT [34]</cell><cell>?</cell><cell cols="17">82.4 96.6 67.4 91.7 52.8 83.8 40.6 74.0 29.8 39.6 59.8 75.3 133.3 135.4</cell></row><row><cell>GRIT  ?</cell><cell>?</cell><cell cols="17">83.7 97.4 68.5 92.8 53.9 85.3 41.5 75.6 30.3 40.2 60.2 75.9 138.3 141.8</cell></row><row><cell>GRIT  ?</cell><cell>?</cell><cell cols="17">84.1 97.6 69.4 93.5 54.9 86.3 42.5 76.8 30.9 41.0 61.2 77.1 141.3 143.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Performance on the ArtEmis and nocaps datasets</figDesc><table><row><cell cols="3">a) Performance on the ArtEmis test split</cell><cell cols="5">b) Performance on the nocaps validation split</cell></row><row><cell>Method</cell><cell>V. E.</cell><cell>Performance Metrics</cell><cell>Method</cell><cell cols="4">V.E In-Domain Out-Domain Overall</cell></row><row><cell></cell><cell cols="2">Type B@1 B@2 B@3 B@4 M R</cell><cell></cell><cell>Type C</cell><cell>S</cell><cell>C</cell><cell>S</cell><cell>C</cell><cell>S</cell></row><row><cell>NN [1]</cell><cell cols="2">H 36.4 13.9 5.4 2.2 10.2 21.0</cell><cell>NBT [2]</cell><cell cols="4">R 62.7 10.1 54.0 8.6 53.9 9.2</cell></row><row><cell>ANP [1]</cell><cell cols="2">G 39.6 13.4 4.2 1.4 8.8 20.2</cell><cell cols="5">Up-down [2] R 78.1 11.6 31.3 8.3 55.3 10.1</cell></row><row><cell>SAT [1]</cell><cell cols="2">G 53.6 29.0 15.5 8.7 14.2 29.7</cell><cell>Trans. [8]</cell><cell cols="4">R 78.0 11.0 29.7 7.8 54.7 9.8</cell></row><row><cell cols="3">M 2 Trans. [1] R 50.7 28.2 15.9 9.5 13.7 28.0</cell><cell cols="5">M 2 Trans. [8] R 85.7 12.1 38.9 8.9 64.5 11.1</cell></row><row><cell>GRIT  ?</cell><cell cols="2">R+G 70.1 40.1 20.9 11.3 16.8 33.3</cell><cell>GRIT  ?</cell><cell cols="4">R+G 105.9 13.6 72.6 11.1 90.2 12.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Statistics of the pretraining datasets for object detection.</figDesc><table><row><cell>Source</cell><cell cols="4">VG COCO Objects365 OpenImages</cell></row><row><cell>Images</cell><cell cols="2">97k 111k</cell><cell>609k</cell><cell>1.67M</cell></row><row><cell cols="2">Categories 1594</cell><cell>80</cell><cell>365</cell><cell>500</cell></row><row><cell>Sampling</cell><cell>?8</cell><cell>?8</cell><cell>?2</cell><cell>?1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Performance of object detection on the COCO and Visual Genome datasets. '4DS' denotes the four object detection datasets.</figDesc><table><row><cell>Model</cell><cell cols="3">Training Data mAP (COCO) mAP 50 (VG)</cell></row><row><cell>BUTD [4]</cell><cell>VG</cell><cell>-</cell><cell>10.2</cell></row><row><cell>VinVL [57]</cell><cell>4DS</cell><cell>50.5</cell><cell>13.8</cell></row><row><cell>GRIT</cell><cell>VG</cell><cell>33.6</cell><cell>14.2</cell></row><row><cell>GRIT  ?</cell><cell>4DS</cell><cell>50.8</cell><cell>15.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Breakdown of SPICE F-scores over various sub-categories and the CLIP scores. Transformer<ref type="bibr" target="#b7">[8]</ref>. These scores give a quantitative assessment of performance on different aspects when describing the content of images. As seen inTable 7, our method attains better scores over all sub-categories, showing significant improvement on identifying and counting objects, attributes, and relationships between objects. The table also reports the CLIP scores<ref type="bibr" target="#b14">[15]</ref> of the two methods, showing consistent improvement of our method over the compared method.B.2 Image Captioning on the ArtEmis datasetArtEmis Dataset This dataset consists of 80,031 unique images divided into the training, validation, and test splits with the ratios of 85%, 5%, and 10%, respectively. Each caption of a given image is annotated with an emotion label.</figDesc><table><row><cell>Method</cell><cell cols="7">SPICE Object Attr. Relation Color Count Size CLIP</cell></row><row><cell>Up-Down [4]</cell><cell>21.4</cell><cell cols="2">39.1 10.0</cell><cell>6.5</cell><cell cols="2">11.4 18.4 3.2</cell><cell>-</cell></row><row><cell cols="2">Transformer [8] 21.1</cell><cell>38.6</cell><cell>9.6</cell><cell>6.3</cell><cell>9.2</cell><cell>17.5 2.0</cell><cell>-</cell></row><row><cell>M 2 Trans. [8]</cell><cell>22.6</cell><cell cols="2">40.0 11.6</cell><cell>6.9</cell><cell cols="3">12.9 20.4 3.5 73.4</cell></row><row><cell>GRIT  ?</cell><cell>24.3</cell><cell cols="2">42.7 13.5</cell><cell>7.7</cell><cell cols="3">14.7 29.3 4.5 77.2</cell></row><row><cell cols="6">B.1 Image Captioning on the COCO dataset</cell><cell></cell></row><row><cell cols="8">SPICE Sub-category and CLIPscore Metrics Table 7 reports a breakdown</cell></row><row><cell cols="8">of SPICE F-scores over various sub-categories on the "Karpathy" test split, in</cell></row><row><cell cols="8">comparison with the region-based methods: Up-Down [4], vanilla Transformer</cell></row><row><cell>[8], and M 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 :</head><label>8</label><figDesc>Performance on the ArtEmis test split.</figDesc><table><row><cell>Method</cell><cell cols="2">Emotion V. E.</cell><cell>Performance Metrics</cell></row><row><cell></cell><cell cols="3">Grounded Type B@1 B@2 B@3 B@4 M</cell><cell>R</cell></row><row><cell>NN [1]</cell><cell>No</cell><cell cols="2">H 36.4 13.9 5.4 2.2 10.2 21.0</cell></row><row><cell>ANP [1]</cell><cell>No</cell><cell cols="2">G 39.6 13.4 4.2 1.4 8.8 20.2</cell></row><row><cell>M 2 Trans. [1]</cell><cell>Yes</cell><cell cols="2">R 51.1 28.2 15.4 9.0 13.7 28.6</cell></row><row><cell>M 2 Trans. [1]</cell><cell>No</cell><cell cols="2">R 50.7 28.2 15.9 9.5 14.0 28.0</cell></row><row><cell>SAT [1]</cell><cell>Yes</cell><cell cols="2">G 52.0 28.0 14.6 7.9 13.4 29.4</cell></row><row><cell>SAT [1]</cell><cell>No</cell><cell cols="2">G 53.6 29.0 15.5 8.7 14.2 29.7</cell></row><row><cell>GRIT  ?</cell><cell>Yes</cell><cell cols="2">R+G 69.3 39.4 19.2 11.1 16.5 33.0</cell></row><row><cell>GRIT  ?</cell><cell>No</cell><cell cols="2">R+G 70.1 40.1 20.9 11.3 16.8 33.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Performance on the nocaps validation split. R+G 105.9 13.6 92.16 13.05 72.6 11.1 90.2 12.8</figDesc><table><row><cell>Method</cell><cell cols="8">V.E in-domain near-domain out-domain Overall</cell></row><row><cell></cell><cell cols="2">Type C</cell><cell>S</cell><cell>C</cell><cell>S</cell><cell>C</cell><cell>S</cell><cell>C</cell><cell>S</cell></row><row><cell>NBT [2]</cell><cell>R</cell><cell cols="3">62.7 10.1 51.9</cell><cell cols="4">9.2 54.0 8.6 53.9 9.2</cell></row><row><cell>Up-down [2]</cell><cell>R</cell><cell cols="7">78.1 11.6 57.7 10.3 31.3 8.3 55.3 10.1</cell></row><row><cell>Trans. [8]</cell><cell>R</cell><cell cols="2">78.0 11.0</cell><cell>-</cell><cell>-</cell><cell cols="3">29.7 7.8 54.7 9.8</cell></row><row><cell cols="2">M 2 Trans. [8] R</cell><cell cols="2">85.7 12.1</cell><cell>-</cell><cell>-</cell><cell cols="3">38.9 8.9 64.5 11.1</cell></row><row><cell>GRIT  ?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>The inference time on feature extraction of different methods.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell>Detector</cell><cell cols="2">Regional Operations Inference Time</cell></row><row><cell cols="4">VinVL large [57] ResNeXt-152 Faster R-CNN Class-Agnostic NMS</cell><cell>304 ms</cell></row><row><cell></cell><cell></cell><cell></cell><cell>RoI Align, etc</cell><cell></cell></row><row><cell cols="4">M 2 Trans. [8] ResNet-101 Faster R-CNN Class-Aware NMS</cell><cell>736 ms</cell></row><row><cell></cell><cell></cell><cell></cell><cell>RoI Align, etc</cell><cell></cell></row><row><cell>GRIT</cell><cell>Swin-Base</cell><cell>DETR-based</cell><cell>-</cell><cell>31 ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 11 :</head><label>11</label><figDesc>The inference time on caption generation of different methods.</figDesc><table><row><cell>Method</cell><cell cols="3">No. of Layers Hidden Dim. Inference Time</cell></row><row><cell>VinVL large [57]</cell><cell>24</cell><cell>1024</cell><cell>542 ms</cell></row><row><cell>M 2 Transformer [8]</cell><cell>3</cell><cell>512</cell><cell>174 ms</cell></row><row><cell>GRIT</cell><cell>3</cell><cell>512</cell><cell>138 ms</cell></row><row><cell cols="2">B.5 Qualitative Examples</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://github.com/aimagelab/meshed-memory-transformer 4 https://github.com/pzzhang/VinVL 5 https://github.com/peteanderson80/bottom-up-attention 6 https://github.com/microsoft/scene_graph_benchmark</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>hair in the mirror GT-2: a little girl is brushing her hair in a bathroom M 2 : a young girl holding a baseball bat in a GRIT: a little girl brushing her hair with a brush GT-1: an elephant walking not to far from a rhino in a forest GT-2: an elephant and a rhino share a field with a pond M 2 : a group of elephants grazing in a field GRIT: an elephant and a rhino standing in a field GT-1: a bike is parked alongside the lake shore GT-2: a bike is parked on the grass in front of the lake M 2 : a bicycle leaning against a bridge over the water GRIT: a bike parked next to a bridge on the water GT-1: 2 female tennis players standing with their rackets GT-2: a pair of young women hold tennis balls and rackets M 2 : a woman hitting a tennis ball with a tennis racket GRIT: 2 people hold tennis rackets and balls on a court GT-1: a cat holding a toothbrush in its mouth GT-2: a cat chewing on a packaged pink toothbrush M 2 : a cat laying on top of a pair of scissors GRIT: a cat with a toothbrush in its mouth on GT-1: the boy is playing video games in his bedroom GT-2: a young man is sitting in a chair playing a video game M 2 : a young man sitting in a chair holding a wii remote GRIT: a man sitting in a chair playing a video game GT-1: a woman is taking a turkey out of the oven GT-2: a woman is taking the cooked turkey out of the oven. M 2 : a woman taking a pizza out of an oven with a GRIT: a woman taking a turkey out of an oven with GT-1: bowls on a table with meat and vegetables. GT-2: four plates of different kind of food sitting on a table M 2 : three plates of food on a wooden table with a GRIT: four bowls of food and a spoon on a table GT-1: a giraffe standing outside of a building next to a tree. GT-2: a giraffe standing in a small piece of shade. M 2 : two giraffes are standing in a zoo enclosure GRIT: a giraffe standing in the dirt next to a building    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT-1:</head><p>A person is standing near a ski-lift with a view of mountains GT-2: A man stands beside a ski lift on a mountain M 2 : a person riding a snowboard down a snow covered slope GRIT: a person on a ski lift on a snowy mountain</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GT-1:</head><p>A zombee walking down a street covered in blood GT-2: A man dressed like a zombie with other zombies around him. M 2 : a man in a suit and tie walking with a group of people GRIT: a man dressed as zombies walking down a street <ref type="figure">Fig. 7</ref>: Qualitative examples from our method (GRIT) and a region-based method (M 2 Transformer) on the COCO test images. Zoom in for better view.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Artemis: Affective language for visual art</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Achlioptas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ovsjanikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Haydarov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elhoseiny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11569" to="11579" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">nocaps: novel object captioning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8948" to="8957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spice: Semantic propositional image caption evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="382" to="398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6077" to="6086" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Meteor: An automatic metric for mt evaluation with improved correlation with human judgments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lavie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</title>
		<meeting>the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="65" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coco-stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Endto-end object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Meshed-memory transformer for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stefanini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10578" to="10587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Tcic: Theme concepts learning cross language and vision for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10936</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">You only look at one sequence: Rethinking transformer in vision through object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Normalized and geometryaware self-attention network for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10327" to="10336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image captioning: Transforming objects into words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herdade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kappeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Boakye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Soares</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Clipscore: A referencefree evaluation metric for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holtzman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Forbes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08718</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">spaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Honnibal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Montani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Attention on attention for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4634" to="4643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improving image captioning by leveraging intra-and inter-layer global representation in transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1655" to="1663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">In defense of grid features for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10267" to="10276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Neuraltalk is a python+numpy project for learning multimodal recurrent neural networks that describe images with sentences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">:</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">/</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neuraltalk</surname></persName>
		</author>
		<ptr target="https://github.com/karpathy/neuraltalk" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3128" to="3137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Reflective decoding network for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8888" to="8897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Vilt: Vision-and-language transformer without convolution or region supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bokyung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ildoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Representation Learning</title>
		<meeting>International Conference on Representation Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuznetsova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alldrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Krasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kamali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malloci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Duerig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">128</biblScope>
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Entangled transformer for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8928" to="8937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Oscar: Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rouge: A package for automatic evaluation of summaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Knowing when to look: Adaptive attention via a visual sentinel for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="375" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural baby talk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7219" to="7228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Duallevel collaborative transformer for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2286" to="2293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Senticap: Generating image descriptions with sentiments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mathews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3574" to="3580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">X-linear attention networks for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10971" to="10980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Look back and predict forward in image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8367" to="8375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technical report. OpenAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Advances in Neural Information Processing Systems</title>
		<meeting>Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Self-critical sequence training for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Marcheret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7008" to="7024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Objects365: A large-scale, high-quality dataset for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8430" to="8439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Vidt: An efficient and effective fully transformer-based object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.03921</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03762</idno>
		<title level="m">Attention is all you need</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cider: Consensus-based image description evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lawrence Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4566" to="4575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hierarchical attention network for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8957" to="8964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10904</idno>
		<title level="m">Simvlm: Simple visual language model pretraining with weak supervision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dual global enhanced transformer for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="129" to="141" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">E2e-vlp: Endto-end vision-language pre-training enhanced by visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.01804</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning</title>
		<meeting>International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Auto-encoding scene graphs for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10685" to="10694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Learning to collocate neural modules for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4250" to="4260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<title level="m">Hierarchy parsing for image captioning. Proceedings of International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2621" to="2629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Exploring visual relationship for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of European Conference on Computer Vision</title>
		<meeting>European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="684" to="699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Boosting image captioning with attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4894" to="4902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Vinvl: Revisiting visual representations in vision-language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5579" to="5588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Rstnet: Captioning with adaptive attention on visual and non-visual words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15465" to="15474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Unified vision-language pre-training for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Palangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13041" to="13049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Deformable DETR: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference of Learning Representations</title>
		<meeting>International Conference of Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">GT-1: a white cat is laying on</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
