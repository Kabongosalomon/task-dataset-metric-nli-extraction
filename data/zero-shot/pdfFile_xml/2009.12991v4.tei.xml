<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Long-Tailed Classification by Keeping the Good and Removing the Bad Momentum Causal Effect</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
							<email>kaihua001@e.ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
							<email>jianqiang.jqh@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Damo Academy</orgName>
								<address>
									<country>Alibaba Group</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
							<email>hanwangzhang@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Long-Tailed Classification by Keeping the Good and Removing the Bad Momentum Causal Effect</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>As the class size grows, maintaining a balanced dataset across many classes is challenging because the data are long-tailed in nature; it is even impossible when the sample-of-interest co-exists with each other in one collectable unit, e.g., multiple visual instances in one image. Therefore, long-tailed classification is the key to deep learning at scale. However, existing methods are mainly based on reweighting/re-sampling heuristics that lack a fundamental theory. In this paper, we establish a causal inference framework, which not only unravels the whys of previous methods, but also derives a new principled solution. Specifically, our theory shows that the SGD momentum is essentially a confounder in long-tailed classification. On one hand, it has a harmful causal effect that misleads the tail prediction biased towards the head. On the other hand, its induced mediation also benefits the representation learning and head prediction. Our framework elegantly disentangles the paradoxical effects of the momentum, by pursuing the direct causal effect caused by an input sample. In particular, we use causal intervention in training, and counterfactual reasoning in inference, to remove the "bad" while keep the "good". We achieve new state-of-the-arts on three long-tailed visual recognition benchmarks 1 : Long-tailed CIFAR-10/-100, ImageNet-LT for image classification and LVIS for instance segmentation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the classification is severely biased towards the data-rich head. On the other hand, it is good because the long-tailed distribution essentially encodes the natural inter-dependencies of classes -"TV" is indeed a good context for "controller" -any disrespect of it will hurt the feature representation learning <ref type="bibr" target="#b9">[10]</ref>, e.g., re-weighting <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> or re-sampling <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> inevitably causes under-fitting to the head or over-fitting to the tail.</p><p>Inspired by the above paradox, latest studies <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> show promising results in disentangling the "good" from the "bad", by the na?ve two-stage separation of imbalanced feature learning and balanced classifier training. However, such disentanglement does not explain the whys and wherefores of the paradox, leaving critical questions unanswered: given that the re-balancing causes under-fitting/overfitting, why is the re-balanced classifier good but the re-balanced feature learning bad? The two-stage design clearly defies the end-to-end merit that we used to believe since the deep learning era; but why does the two-stage training significantly outperform the end-to-end one in long-tailed classification?</p><p>In this paper, we propose a causal framework that not only fundamentally explains the previous methods <ref type="bibr">[15-17, 9, 11, 10]</ref>, but also provides a principled solution to further improve long-tailed classification. The proposed causal graph of this framework is given in <ref type="figure" target="#fig_0">Figure 1</ref> (a). We find that the momentum M in any SGD optimizer <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> (also called betas in Adam optimizer <ref type="bibr" target="#b19">[20]</ref>), which is indispensable for stabilizing gradients, is a confounder who is the common cause of the sample feature X (via M ? X) and the classification logits Y (via M ? D ? Y ). In particular, D denotes the X's projection on the head feature direction that eventually deviates X. We will justify the graph later in Section 3. Here, <ref type="figure" target="#fig_0">Figure 1</ref> (b&amp;c) sheds some light on how the momentum affects the feature X and the prediction Y . From the causal graph, we may revisit the "bad" long-tailed bias in a causal view: the backdoor <ref type="bibr" target="#b20">[21]</ref> path X ? M ? D ? Y causes the spurious correlation even if X has nothing to do with the predicted Y , e.g., misclassifying a tail sample to the head. Also, the mediation <ref type="bibr" target="#b21">[22]</ref> path X ? D ? Y mixes up the pure contribution made by X ? Y . For the "good" bias, X ? D ? Y respects the inter-relationships of the semantic concepts in classification, that is, the head class knowledge contributes a reliable evidence to filter out wrong predictions. For example, if a rare sample is closer to the head class "TV" and "sofa", it is more likely to be a living room object (e.g., "remote controller") but not an outdoor one (e.g., "car").</p><p>Based on the graph that explains the paradox of the "bad" and "good", we propose a principled solution for long-tailed classification. It is a natural derivation of pursuing the direct causal effect along X ? Y by removing the momentum effect. Thanks to causal inference <ref type="bibr" target="#b22">[23]</ref>, we can elegantly keep the "good" while remove the "bad". First, to learn the model parameters, we apply de-confounded training with causal intervention: while it removes the "bad" by backdoor adjustment <ref type="bibr" target="#b20">[21]</ref> who cuts off the backdoor confounding path X ? M ? D ? Y , it keeps the "good" by retaining the mediation X ? D ? Y . Second, we calculate the direct causal effect of X ? Y as the final prediction logits. It disentangles the "good" from the "bad" in a counterfactual world, where the bad effect is considered as the Y 's indirect effect when X is zero but D retains the value when X = x. In contrast to the prevailing two-stage design <ref type="bibr" target="#b10">[11]</ref> that requires unbiased re-training in the 2nd stage, our solution is one-stage and re-training free. Interestingly, as discussed in Section 4.4, we show that why the re-training is inevitable in their method and why ours can avoid it with even better performance.</p><p>On image classification benchmarks Long-tailed CIFAR-10/-100 <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b9">10]</ref> and ImageNet-LT <ref type="bibr" target="#b8">[9]</ref>, we outperform previous state-of-the-arts <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> on all splits and settings, showing that the performance gain is not merely from catering to the long tail or a specific imbalanced distribution. In object detection and instance segmentation benchmark LVIS <ref type="bibr" target="#b6">[7]</ref>, our method also has a significant advantage over the former winner <ref type="bibr" target="#b16">[17]</ref> of LVIS 2019 challenge. We achieve 3.5% and 3.1% absolute improvements on mask AP and box AP using the same Cascade Mask R-CNN with R101-FPN backbone <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Re-Balanced Training. The most widely-used solution for long-tailed classification is arguably to re-balance the contribution of each class in the training phase. It can be either achieved by resampling <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b26">27]</ref> or re-weighting <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">17]</ref>. However, they inevitably cause the under-fitting/over-fitting problem to head/tail classes. Besides, relying on the accessibility of data distribution also limits their application scope, e.g., not applicable in online and streaming data.</p><p>Hard Example Mining. The instance-level re-weighting <ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref> is also a practical solution. Instead of hacking the prior distribution of classes, focusing on the hard samples also alleviates the long-tailed issue, e.g., using meta-learning to find the conditional weights for each samples <ref type="bibr" target="#b30">[31]</ref>, enhancing the samples of hard categories by group softmax <ref type="bibr" target="#b31">[32]</ref>.</p><p>Transfer Learning/Two-Stage Approach. Recent work shows a new trend of addressing the longtailed problem by transferring the knowledge from head to tail. The sharing bilateral-branch network <ref type="bibr" target="#b9">[10]</ref>, the two-stage training <ref type="bibr" target="#b10">[11]</ref>, the dynamic curriculum learning <ref type="bibr" target="#b32">[33]</ref> and the transferring memory features <ref type="bibr" target="#b8">[9]</ref> / head distributions <ref type="bibr" target="#b33">[34]</ref> are all shown to be effective in long-tailed recognition, yet, they either significantly increase the parameters or require a complicated training strategy.</p><p>Causal Inference. Causal inference <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b34">35]</ref> has been widely adopted in psychology, politics and epidemiology for years <ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref>. It doesn't just serve as an interpretation framework, but also provides solutions to achieve the desired objectives by pursing causal effect. Recently, causal inference has also attracted increasing attention in computer vision society <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b43">[44]</ref> for removing the dataset bias in domain-specific applications, e.g., using pure direct effect to capture the spurious bias in VQA <ref type="bibr" target="#b40">[41]</ref> and NWGM for Captioning <ref type="bibr" target="#b41">[42]</ref>. Compared to them, our method offers a fundamental framework for general long-tailed visual recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">A Causal View on Momentum Effect</head><p>To systematically study the long-tailed classification and how momentum affects the prediction, we construct a causal graph <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b21">22]</ref> in <ref type="figure" target="#fig_0">Figure 1</ref> (a) with four variables: momentum (M ), object feature (X), projection on head direction (D), and model prediction (Y ). The causal graph is a directed acyclic graph used to indicate how variables of interest {M, X, D, Y } interacting with each other through causal links. The nodes M and D constitute a confounder and a mediator, respectively. A confounder is a variable that influences both correlated and independent variables, creating a spurious statistical correlation. Considering a causal graph exercise ? age ? cancer, the elder people spend more time on physical exercise after retirement and they are also easier to get cancer due to the elder age, so the confounder age creates a spurious correlation that more physical exercise will increase the chance of getting cancer. The example of a mediator would be drug ? placebo ? cure, where mediator placebo is the side effect of taking drug that prevents us from getting the direct effect of drug ? cure.</p><p>Before we delve into the rationale of our causal graph, let's take a brief review on the SGD with momentum <ref type="bibr" target="#b18">[19]</ref>. Without loss of generality, we adopt the Pytorch implementation <ref type="bibr" target="#b44">[45]</ref>:</p><formula xml:id="formula_0">v t = ? ? v t?1 momentum +g t , ? t = ? t?1 ? lr ? v t ,<label>(1)</label></formula><p>where the notations in the t-th iteration are: model parameters ? t , gradient g t , velocity v t , momentum decay ratio ?, and learning rate lr. Other versions of SGD <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref> only change the position of some hyper-parameters and we can easily prove them equivalent with each other. The use of momentum considerably dampens the oscillations caused by each single sample. In our causal graph, momentum M is the overall effect of ? ? v T ?1 at the convergence t = T , which is the exponential moving average of the gradient over all past samples with decay rate ?. Eq. (1) shows that, given fixed hyper-parameters ? and lr, each sample M = m is a function of the model initialization and the mini-batch sampling strategy, that is, M has infinite samples.</p><p>In a balanced dataset, the momentum is equally contributed by every class. However, when the dataset is long-tailed, it will be dominated by the head samples, emerging the following causal links:</p><p>(a) Decompose the gradient velocity (b) Decompose the biased feature vector ? ? ? <ref type="figure">Figure 2</ref>: Based on Assumption 1, the feature vector x can be decomposed into a discriminative feature? and a projection on head direction d M ? X. This link says that the backbone parameters used to generate feature vectors X, are trained under the effect of M . This is obvious from Eq. (1) and can be illustrated in <ref type="figure" target="#fig_0">Figure 1</ref> (b), where we visualize how the magnitudes of X change from head to tail.</p><p>(M, X) ? D. This link denotes that the momentum also causes feature vector X deviates to the head direction D, which is also determined by M . In a long-tailed dataset, few head classes possess most of the training samples, who have less variance than the data-poor but class-rich tail, so the moving averaged momentum will thus point to a stable head direction. Specifically, as shown in <ref type="figure">Figure 2</ref>, we can decompose any feature vector</p><formula xml:id="formula_1">x into x =? + d, where D = d =dcos(x,d) x .</formula><p>In particular, the head directiond is given in Assumption 1, whose validity is detailed in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumption 1</head><p>The head directiond is the unit vector of the exponential moving average features with decay rate ? like momentum, i.e.,d = x T / x T , where x t = ? ? x t?1 + x t and T is the number of the total training iterations.</p><p>Note that Assumption 1 says that the head direction is exactly determined by the sample moving average in the dataset, which does not need the accessibility of the class statistics at all. In particular, as we show in Appendix A, when the dataset is balanced, Assumption 1 also holds but suggests that X ? Y is naturally not affected by M .</p><formula xml:id="formula_2">X ? D ? Y &amp; X ? Y.</formula><p>These links indicate that the effect of X can be disentangled into an indirect (mediation) and a direct effect. Thanks to the above orthogonal decomposition: x =? + d, the indirect effect is affected by d while the direct effect is affected by?, and they together determine the total effect. As shown in <ref type="figure" target="#fig_2">Figure 4</ref>, when we change the scale parameter ? of d, the performance of the tail classes monotonically increases with ?, which inspires us to remove the mediation effect of D in Section 4.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">The Proposed Solution</head><p>Based on the proposed causal graph in <ref type="figure" target="#fig_0">Figure 1</ref> (a), we can delineate our goal for long-tailed classification: the pursuit of the direct causal effect along X ? Y . In causal inference, it is defined as Total Direct Effect (TDE) <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b21">22]</ref>:  <ref type="formula" target="#formula_3">2)</ref>) for the long-tailed classification after deconfounded training. Subtracted left:</p><formula xml:id="formula_3">arg max i?C T DE(Y i ) = [Y d = i|do(X = x)] ? [Y d = i|do(X = x 0 )],<label>(2)</label></formula><formula xml:id="formula_4">[Y d = i|do(X = x)], minus right: [Y d = i|do(X = x 0 )].</formula><p>where x 0 denotes a null input (0 in this paper). We define the causal effect as the prediction logits Y i for the i-th class. Subscript d denotes that the mediator D always takes the value d in the deconfounded causal graph model of <ref type="figure" target="#fig_0">Figure 1</ref> (a) with do(X = x), where the do-operator denotes the causal intervention <ref type="bibr" target="#b22">[23]</ref> that modifies the graph by M ? X. Thus, Eq. (2) shows an important principle in long-tailed classification: before we calculate the final TDE (Section 4.2), we need to first perform de-confounded training (Section 4.1) to estimate the "modified" causal graph parameters.</p><p>We'd like to highlight that Eq. (2) removes the "bad" while keeps the "good" in a reconcilable way. First, in training, the do-operator removes the "bad" confounder bias while keeps the "good" mediator bias, because the do-operator retains the mediation path. Second, in inference, the mediator value d is imposed in both terms to keep the "good" of the mediator bias (towards head) in logit prediction; it also removes its "bad" by subtracting the second term: the prediction when the input X is null (x 0 ) but the mediator D is still the value d when X had been x. Note that such a counterfactual minus elegantly characterizes the "bad" mediation bias, just like how we capture the tricky placebo effect: we cheat the patient to take a placebo drug, setting the direct drug effect drug ? cure to zero; thus, any cure observed must be purely due to the non-zero placebo effect drug ? placebo ? cure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">De-confounded Training</head><p>The model for the proposed causal graph is optimized under the causal intervention do(X = x), which aims to preserve the "good" feature learning from the momentum and cut off its "bad" confounding effect. We apply the backdoor adjustment <ref type="bibr" target="#b20">[21]</ref> to derive the de-confounded model:</p><formula xml:id="formula_5">P (Y = i|do(X = x)) = m P (Y = i|X = x, M = m)P (M = m) (3) = m P (Y = i, X = x|M = m)P (M = m) P (X = x|M = m) .<label>(4)</label></formula><p>As there are infinite number of M = m, it is prohibitively to achieve the above backdoor adjustment. Fortunately, the Inverse Probability Weighting <ref type="bibr" target="#b22">[23]</ref> formulation in Eq. (4) provides us a new perspective in approximating the infinite sampling (i, x)|m. For a finite dataset, no matter how many m there are, we can only observe one (i, x) given one m. In such cases, the number of m values that Eq. (4) would encounter is equal to the number of samples (i, x) available, not to the number of possible m values, which is prohibitive. In fact, thanks to the backdoor adjustment, which connects the equivalence between the originally confounded model P and the deconfounded model P with do(X), we can collect samples from the former, that act as though they were drawn from the latter. Therefore, Eq. (4) can be approximated as</p><formula xml:id="formula_6">P (Y = i|do(X = x)) ? 1 K K k=1 P (Y = i, X = x k |M = m),<label>(5)</label></formula><p>where P is the inverse weighted probability and we will drop M = m in the rest of the paper for notation simplicity and bear in mind that x still depends on m. In particular, compared to the vanilla trick, we apply a multi-head strategy <ref type="bibr" target="#b46">[47]</ref> to equally divide the channel (or dimensions) of weights and features into K groups, which can be considered as K times more fine-grained sampling.</p><p>We model P in Eq. (5) as the softmax activated probability of the energy-based model <ref type="bibr" target="#b47">[48]</ref>:</p><formula xml:id="formula_7">P (Y = i, X = x k ) ? E(i, x k ; w k i ) = ? f (i, x k ; w k i ) g(i, x k ; w k i ) ,<label>(6)</label></formula><p>where ? is a positive scaling factor akin to the inverse temperature in Gibbs distribution. Recall Assumption 1 that x k =? k + d k . The numerator, i.e., the unnormalized effect, can be implemented</p><formula xml:id="formula_8">as logits f (i, x k ; w k i ) = (w k i ) (? k + d k ) = (w k i )</formula><p>x k , and the denominator is a normalization term (or propensity score <ref type="bibr" target="#b48">[49]</ref>) that only balances the magnitude of the variables:</p><formula xml:id="formula_9">g(i, x k ; w k i ) = x k ? w k i + ? x k ,</formula><p>where the first term is a class-specific energy and the second term is a class-agnostic baseline energy.</p><p>Putting the above all together, the logit calculation for P (Y = i|do(X = x)) can be formulated as:</p><formula xml:id="formula_10">[Y = i|do(X = x)] = ? K K k=1 (w k i ) (? k + d k ) ( w k i + ?) x k = ? K K k=1 (w k i ) x k ( w k i + ?) x k .<label>(7)</label></formula><p>Interestingly, this model also explains the effectiveness of normalized classifiers like cosine classifier <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref>. We will further discuss it in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Two-stage Re-balancing (do(D)) De-confound (do(X)) Direct Effect Cosine <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref> ---LDAM <ref type="bibr" target="#b11">[12]</ref> -CDE OLTR <ref type="bibr" target="#b8">[9]</ref> -NDE BBN <ref type="bibr" target="#b9">[10]</ref> -NDE Decouple <ref type="bibr" target="#b10">[11]</ref> -NDE EQL <ref type="bibr" target="#b16">[17]</ref> ---Our method --TDE </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Total Direct Effect Inference</head><p>After the de-confounded training, the causal graph is now ready for inference. The TDE of X ? Y in Eq. <ref type="formula" target="#formula_3">(2)</ref> can thus be depicted as in <ref type="figure" target="#fig_1">Figure 3</ref>. By applying the counterfactual consistency rule <ref type="bibr" target="#b51">[52]</ref>,</p><formula xml:id="formula_11">we have [Y d = i|do(X = x)] = [Y = i|do(X = x)]</formula><p>. This indicates that we can use Eq. <ref type="formula" target="#formula_10">(7)</ref> to calculate the first term of Eq. (2). Thanks to Assumption 1, we can disentangle x by</p><formula xml:id="formula_12">x =? + d, where d = d ?d = cos(x,d) x ?d. Therefore, we have [Y d = i|do(X = x 0 )</formula><p>] that replaces th? x in Eq. <ref type="formula" target="#formula_10">(7)</ref> with zero vector, just like "cheating" the model with a null input but keeping everything else unchanged. Overall, the final TDE calculation for Eq. <ref type="formula" target="#formula_3">(2)</ref> is</p><formula xml:id="formula_13">T DE(Y i ) = ? K K k=1 (w k i ) x k ( w k i + ?) x k ? ? ? cos(x k ,d k ) ? (w k i ) d k w k i + ? ,<label>(8)</label></formula><p>where ? controls the trade-off between the indirect and direct effect as shown in <ref type="figure" target="#fig_2">Figure 4</ref>. Some classification tasks need a special "background" class to filter out samples belonging to none of the classes of interest, e.g., object detection and instance segmentation use the background class to remove nonobject regions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b23">24]</ref>, and recommender systems assume that the majority of the items are irrelevant to a user <ref type="bibr" target="#b52">[53]</ref>. In such tasks, most of the training samples are background and hence the background class is a good head class, whose effect should be kept and thus exempted from the TDE calculation. To this end, we propose a background-exempted inference that particular uses the original inference (total effect) for background class. The inference can be formulated as:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Background-Exempted Inference</head><formula xml:id="formula_14">arg max i?C (1 ? p 0 ) ? qi 1?q0 i = 0 p 0 i = 0 ,<label>(9)</label></formula><p>where i = 0 is the background class, p i = P (Y = i|do(X = x)) is the de-confounded probability that we defined in Section 4.1, q i is the softmax activated probability of the original T DE(Y i ) in Eq. <ref type="bibr" target="#b7">(8)</ref>. Note that Eq. (9) adds up to 1 from i = 0 to C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Revisiting Two-stage Training</head><p>The proposed framework also theoretically explains the previous state-of-the-arts as shown in <ref type="table" target="#tab_0">Table 1</ref>. Please see Appendix B for the detailed revisit for each method.</p><p>Two-stage Re-balancing. Na?ve re-balanced training fails to retain a natural mediation D that respects the inter-dependencies among classes. Therefore, the two-stage training is adopted by most of the re-balancing methods: imbalanced pre-training the backbone with natural D and then balanced re-training a fair classifier with the fixed backbone for feature representation. Later, we will show that the second stage re-balancing essentially plays a counterfactual role, which reveals the reason why the stage-2 is indispensable.   <ref type="bibr" target="#b8">[9]</ref>. All models were using the ResNeXt-50 backbone. The superscript ? denotes being re-implemented by our framework and hyper-parameters.</p><p>De-confounded Training. Technically, the proposed de-confounded training in Eq. <ref type="formula" target="#formula_10">(7)</ref> is the multihead classifier with normalization. The normalized classifier, like cosine classifier, has already been embraced by various methods <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b10">11]</ref> based on empirical practice. However, as we will show in <ref type="table" target="#tab_2">Table 2</ref>, without the guidance of our causal graph, their normalizations perform worse than the proposed de-confounded model. For example, methods like decouple <ref type="bibr" target="#b10">[11]</ref> only applies normalization in the 2nd stage balanced classifier training, and hence its feature learning is not de-confounded.</p><p>Direct Effect. The one-stage re-weighting/re-sampling training methods, like LDAM <ref type="bibr" target="#b11">[12]</ref>, can be interpreted as calculating Controlled Direct Effect (CDE) <ref type="bibr" target="#b22">[23]</ref>:</p><formula xml:id="formula_15">CDE(Y i ) = [Y = i|do(X = x), do(D = d 0 )] ? [Y = i|do(X = x 0 ), do(D = d 0 )]</formula><p>, where x 0 is a dummy vector and d 0 is a constant vector. CDE performs a physical intervention -re-balancing -on the training data by setting the bias D to a constant. Note that the second term of CDE is a constant that does not affect the classification. However, CDE removes the "bad" at the cost of hurting the "good" during representation learning, as D is no longer a natural mediation generated by X.</p><p>The two-stage methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> are essentially Natural Direct Effect (NDE), where the stage-2 re-balanced training is actually an intervention on D that forces the directiond do not head to any class. Therefore, when attached with the stage-1 imbalanced pre-trained features, the balanced classifier calculates the NDE:</p><formula xml:id="formula_16">N DE(Y i ) = [Y d0 = i|do(X = x)] ? [Y d0 = i|do(X = x 0 )]</formula><p>, where x 0 and d 0 are dummy vectors, because the stage-2 balanced classifier forces the logits to nullify any class-specific momentum direction; do(X = x) as stage-1 backbone is frozen and M ? X; the second term can be omitted as it is a class-agnostic constant. Besides that their stage-1 training is still confounded, as we will show in experiments, our TDE is better than NDE because the latter completely removes the entire effect of D by setting D = d 0 , which is however sometimes good, e.g., mis-classifying "warthog" as the head-class "pig" is better than "car"; TDE admits the effect by keeping D = d as a baseline and further compares the fine-grained difference via the direct effect, e.g., by admitting that "warthog" does look like "pig", TDE finds out that the tusk is the key difference between "warthog" and "pig", and that is why our method can focus on more discriminative regions in <ref type="figure">Figure 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>The proposed method was evaluated on three long-tailed benchmarks: Long-tailed CIFAR-10/-100, ImageNet-LT for image classification and LVIS for object detection and instance segmentation. The consistent improvements across different tasks demonstrate our broad application domain.</p><p>Datasets and Protocols. We followed <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b9">10]</ref> to collect the long-tailed versions of CIFAR-10/-100 with controllable degrees of data imbalance ratio ( Nmax Nmin , where N is number of samples in each category), which controls the distribution of training sets. ImageNet-LT <ref type="bibr" target="#b8">[9]</ref> is a long-tailed subset of ImageNet dataset <ref type="bibr" target="#b3">[4]</ref>. It consists of 1k classes over 186k images, where 116k/20k/50k for train/val/test sets, respectively. In train set, the number of images per class is ranged from 1,280 to 5, which </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Medium-shot Classes</head><p>Few-shot Classes warthog bighorn proboscis monkey kimono <ref type="figure">Figure 5</ref>: The visualized activation maps of the linear classifier baseline, Decouple-LWS <ref type="bibr" target="#b10">[11]</ref> and the proposed method on ImageNet-LT using the Grad-CAM <ref type="bibr" target="#b54">[55]</ref>.  <ref type="table">Table 3</ref>: Top-1 accuracy on Long-tailed CIFAR-10/-100 with different imbalance ratios. All models are using the same ResNet-32 backbone. We further adopted the same warm-up scheduler from BBN <ref type="bibr" target="#b9">[10]</ref> for fair comparisons. Evaluation. For Long-tailed CIFAR-10/-100 <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b9">10]</ref>, we evaluated Top-1 accuracy under three different imbalance ratios: 100/50/10. For ImageNet-LT <ref type="bibr" target="#b8">[9]</ref>, the evaluation results were reported as the percentage of accuracy on four splits. For LVIS <ref type="bibr" target="#b6">[7]</ref>, the evaluation metrics are standard segmentation mask AP calculated across IoU threshold 0.5 to 0.95 for all classes. These classes can also be categorized by the frequency and independently reported as AP r , AP c , AP f : subscripts r, c, f stand for rare (appeared in &lt; 10 images), common (appeared in 11 ? 100 images), and frequent (appeared in &gt; 100 images). Since we can use the LVIS to detect bounding boxes, the detection results were reported as AP bbox .</p><p>Implementation Details. For image classification on ImageNet-LT, we used ResNeXt-50-32x4d <ref type="bibr" target="#b1">[2]</ref> as our backbone for all experiments. All models were trained by using SGD optimizer with momentum ? = 0.9 and batch size 512. The learning rate was decayed by a cosine scheduler <ref type="bibr" target="#b56">[57]</ref> from 0.2 to 0.0 in 90 epochs. Hyper-parameters were chosen by the performances on ImageNet-LT val set, and we set K = 2, ? = 16, ? = 1/32, ? = 3.0. For Long-tailed CIFAR-10/-100, we changed the backbone to ResNet-32 and the training scheduler to warm-up scheduler like BBN <ref type="bibr" target="#b9">[10]</ref> for fair comparisons. All parameters except for ? are inherited from ImageNet-LT, which was set to 1.0/1.5 for CIFAR-10/-100 respectively. For instance segmentation and object detection on LVIS, we chose Cascade Mask R-CNN framework <ref type="bibr" target="#b23">[24]</ref> implemented by <ref type="bibr" target="#b57">[58]</ref>. The optimizer was also SGD with momentum ? = 0.9 and we used batch size 16 for a R101-FPN backbone. The models were trained in 20 epochs with learning rate starting at 0.02 and decaying by the factor of 0.1 at the 16-th and 19-th epochs. We selected the top 300 predicted boxes following <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref>. The hyper-parameters on LVIS were directly adopted from the ImageNet-LT, except for ? = 1.5. The main difference  <ref type="table">Table 4</ref>: All models are using the same Cascade Mask R-CNN framework <ref type="bibr" target="#b23">[24]</ref> with R101-FPN backbone <ref type="bibr" target="#b58">[59]</ref>. The reported results are evaluated on LVIS val set <ref type="bibr" target="#b6">[7]</ref>.</p><p>between image classification and object detection/instance segmentation is that the latter includes a background class i = 0, which is a head class used to make a binary decision between foreground and background. As we discussed in Section. 4.3, the Background-Exempted Inference should be used to retain the good background bias. The comparison between with and without Background-Exempted Inference is given in Appendix C.</p><p>Ablation studies. To study the effectiveness of the proposed de-confounded training and TDE inference, we tested a variety of ablation models: 1) the linear classifier baseline (no biased term); 2) the cosine classifier <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref>; 3) the capsule classifier <ref type="bibr" target="#b8">[9]</ref>, where x is normalized by the non-linear function from <ref type="bibr" target="#b53">[54]</ref>; 4) the proposed de-confounded model with normal softmax inference; 5) different versions of the TDE. As reported in <ref type="figure" target="#fig_2">Table (2,4)</ref>, the de-confound TDE achieves the best performance under all settings. The TDE inference improves all three normalized models, because the cosine and capsule classifiers can be considered as approximations to the proposed de-confounded model. To show that the mediation effect removed by TDE indeed controls the preference towards head direction, we changed the parameter ? as shown in <ref type="figure" target="#fig_2">Figure 4</ref>, resulting the smooth increasing/decreasing of the performances on tail/head classes, respectively.</p><p>Comparisons with State-of-The-Art Methods. The previous state-of-the-art results on ImageNet-LT are achieved by the two-stage re-balanced training <ref type="bibr" target="#b10">[11]</ref> that decouples the backbone and classifier. However, as we discussed in Section 4.4, this kind of approaches are less effective or efficient. In Long-tailed CIFAR-10/-100, we outperform the previous methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b9">10]</ref> in all imbalance ratios, which proves that the proposed method can automatically adapt to different data distributions. In LVIS dataset, after a simple adaptation, we beat the champion EQL <ref type="bibr" target="#b16">[17]</ref> of LVIS Challenge 2019 in <ref type="table">Table 4</ref>. All reported results in <ref type="table">Table 4</ref> are using the same Cascade Mask R-CNN framework <ref type="bibr" target="#b23">[24]</ref> and R101-FPN backbone <ref type="bibr" target="#b58">[59]</ref> for fair comparison. The EQL results were copied from <ref type="bibr" target="#b16">[17]</ref>, which were trained by 16 GPUs and 32 batch size while the proposed method only used 8 GPUs and half of the batch size. We didn't compare the EQL results on the final challenge test server, because they claimed to exploit external dataset and other tricks like ensemble to win the challenge. Note that EQL is also a re-balanced method, having the same problems as <ref type="bibr" target="#b10">[11]</ref>. We also visualized the activation maps using Grad-CAM <ref type="bibr" target="#b54">[55]</ref> in <ref type="figure">Figure 5</ref>. The linear classifier baseline and decouple-LWS <ref type="bibr" target="#b10">[11]</ref> usually activate the entire objects and some context regions to make a prediction. Meanwhile, the de-confound TDE only focuses on the direct effect, i.e., the most discriminative regions, so it usually activates on a more compact area, which is less likely to be biased towards its similar head classes. For example, to classify a "kimono", the proposed method only focuses on the discriminative feature rather than the entire body, which is similar to some other clothes like "dress".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this work, we first proposed a causal framework to pinpoint the causal effect of momentum in the long-tailed classification, which not only theoretically explains the previous methods, but also provides an elegant one-stage training solution to extract the unbiased direct effect of each instance. The detailed implementation consists of de-confounded training and total direct effect inference, which is simple, adaptive, and agnostic to the prior statistics of the class distribution. We achieved the new stage-of-the-arts of various tasks on both ImageNet-LT and LVIS benchmarks. As moving forward, we are going to 1) further validate our theory in a wider spectrum of application domains and 2) seek better feature disentanglement algorithms for more precise counterfactual effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>The positive impacts of this work are two-fold: 1) it improves the fairness of the classifier, which prevents the potential discrimination of deep models, e.g., an unfair AI could blindly cater to the majority, causing gender, racial or religious discrimination; 2) it allows the larger vocabulary datasets to be easily collected without a compulsory class-balancing pre-processing, e.g., to train autonomous vehicles, by using the proposed method, we don't need collecting as many ambulance images as normal van images do. The negative impacts could also happen when the proposed long-tailed classification technique falls into the wrong hands, e.g., it can be used to identify the minority groups for malicious purposes. Therefore, it's our duty to make sure that the long-tailed classification technique is used for the right purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Explanations of Assumption 1</head><p>To better understand the (M, X) ? D and Assumption 1, let's take a simple example. Given a learnable parameter ? ? R 2 , and its gradients of instances for class A, B approximate to (1, 1) and (-1, 1) respectively. If each of these two classes has 50 samples, the mean gradient would be (0, 1), which is the optimal gradient direction shared by both A and B. The momentum will thus accelerate on this direction that optimizes the model to fairly discriminate two classes. However, if there are 99 samples from class A and only 1 sample from class B (long-tailed dataset), the mean gradient would be (0.98, 1). In this case, the momentum direction now approximates to the class A (head) gradients, encouraging the backbone parameters to generate head-like feature vectors, i.e., creating an unfair deviation towards the head.  <ref type="figure">Figure 6</ref>: The magnitudes of classifier weights w i for each class after training with momentum ? = 0.9, where i is ranking by the number of training samples in a descending order.</p><p>Since the momentum in SGD <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref> usually dominates the gradient velocity, the effect of such a deviation is not trivial, which will eventually create the head projection D on all feature vectors generated by the backbone. It's worth noting that although there are non-linear activation layers in the backbone, due to the central limit theorem <ref type="bibr" target="#b59">[60]</ref>, the overall effect of these deviated parameters is still following the normal distribution, which means we can use the moving averaged feature to approximate this head direction, i.e., the Assumption 1 in the original paper.</p><p>In addition, even in a balanced dataset, the Assumption 1 still holds. Considering the above example, the mean gradient is (0, 1) for balanced A and B, which is not biased towards either direction: (1, 1) or (-1, 1). In other word, the D still exists for the balanced dataset, but the cos(x,d) should be almost the same for all classes. Therefore, the M ? D ? Y won't cause any preference in the balanced dataset, which naturally allows X ? Y free from the effect of M . It's also intuitively easy to understand, because when the dataset is balanced, the mean feature only represents the common patterns shared by all classes, e.g., the D in a balanced face recognition dataset is the mean face, which would be a contour of human head that not biased towards any specific face categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Revisiting Previous Methods in Long-Tailed Classification</head><p>In this section, we will revisit the previous state-of-the-arts in two aspects: the normalized classifiers and the re-balancing strategies.</p><p>Normalized Classifiers. The normalized classifiers <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b8">9]</ref> have already been widely adopted in long-tailed classification based on empirical practice. As we discussed in the Section 4, the correctly applied normalized classifiers are approximations of the proposed de-confounded training.  <ref type="table">Table 5</ref>: The results of the proposed TDE with/without Background-Exempted Inference on LVIS <ref type="bibr" target="#b6">[7]</ref> V0.5 val set. The Cascade Mask R-CNN framework <ref type="bibr" target="#b23">[24]</ref> with R101-FPN backbone <ref type="bibr" target="#b58">[59]</ref> is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BG-Exempted</head><p>However, without the guidance of the proposed causal framework, most of them are not utilized in a proper way. We define the general normalized classifier as the following equation:</p><formula xml:id="formula_17">arg max i?C P (Y = i|X = x) = e zi C c=1 e zc , where z i = ? K K k=1 (w k i ) x k N (x k , w k i ) .<label>(10)</label></formula><p>Since in most of the previous methods, K is set to 1, so we slightly abuse the notation to omit the superscript k for simplicity.</p><p>The cosine classifier <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref> is defined based on the cosine similarity, which has N (x, w i ) = x ? w i . It is commonly used in the tasks like few-shot learning <ref type="bibr" target="#b60">[61]</ref>. In <ref type="table" target="#tab_2">Table 2</ref>,3 of original paper, we have proved its effectiveness in the long-tailed classification. The capsule classifier is proposed by Liu et al. <ref type="bibr" target="#b8">[9]</ref> as the replacement of vanilla cosine classifier in OLTR. It changes the l2 norm of x into the squashing non-linear function proposed in Capsule Network <ref type="bibr" target="#b53">[54]</ref>, which allows the normalized x having a magnitude range from 0 to 1, representing the probability of x in its direction. The final normalization term can thus be defined as N (x, w i ) = ( x + 1) ? w i . However, the OLTR <ref type="bibr" target="#b8">[9]</ref> doesn't use it to de-confound the visual feature. Instead, its x is the joint embedding of the feature vector and an attentive memory vector. The Decouple <ref type="bibr" target="#b10">[11]</ref> also invents two different types of normalized classifiers: ? -norm classifier and Learnable Weight Scaling (LWS) classifier. They empirically found that the l2 norm of w i is not uniform in the long-tailed dataset, and has a positive correlation with the number of training samples for class i, as shown in <ref type="figure">Figure 6</ref>. Therefore, their normalized classifiers only normalize the w i : the ? -norm classifier is defined as N (x, w i ) = w i ? , ? ? [0, 1] while LWS is N (x, w i ) = g i , where g i is a learnable parameter. Yet, these decouple classifiers fail to de-confound the M ? X for two reasons: 1) they don't considering the confounding effect on x; 2) they only apply the normalized classifiers on the 2nd stage when the backbone has already been frozen.</p><p>Re-balancing Strategies. Both OLTR <ref type="bibr" target="#b8">[9]</ref> and Decouple <ref type="bibr" target="#b10">[11]</ref> adopt the same class-aware sampler in their 2nd stage training, which forces each class to contribute the same number of samples regardless of the size. To dynamically combine the two training stages, the BBN [10] utilizes a bilateral-branch design to smoothly transfer the sampling strategy from the imbalanced branch to the re-balancing branch, where two branches share the same set of parameters but learn from different sampling strategies, which has the same spirit as two-stage design in OLTR <ref type="bibr" target="#b8">[9]</ref> and Decouple <ref type="bibr" target="#b10">[11]</ref>. As to the EQL <ref type="bibr" target="#b16">[17]</ref>, since the re-sampling is complicated in the object detection and instance segmentation tasks, where objects from different classes co-exist in one image, they choose the re-weighted loss to balance the contributions of different classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Background-Exempted Inference</head><p>The results with and without Background-Exempted Inference are reported in <ref type="table">Table 5</ref>. As we can see, the Background-Exempted strategy successfully prevents the TDE from hurting the foregroundbackground selection. It is the key to apply TDE in tasks like object detection and instance segmentation that include one or more legitimately biased head categories, i.e., this strategy allows us to conduct TDE on a selected subset of categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D The Difference Between Re-balancing NDE and The Proposed TDE</head><p>In this section, we will further discuss the relationship between two-stage re-balancing NDE and the proposed TDE. As we discussed in Section 4.3 of original paper, the 2nd-stage re-balanced classifier essentially calculates the N DE  <ref type="figure">Figure 7</ref>: A simple one-dimensional binary classification example of conventional classifier, one-/two-stage re-balancing classifiers, and the proposed TDE. second term can be omitted because x is a dummy vector and the moving averaged d in a balanced set won't point to any specific classes, so it is actually a constant offset. Therefore, the crux of understanding the NDE would be why the 2nd-stage re-balanced training equals to the first term</p><formula xml:id="formula_18">(Y i ) = [Y d = i|do(X = x)] ? [Y d = i|do(X = x )],</formula><formula xml:id="formula_19">[Y d = i|do(X = x)].</formula><p>It is because when the backbone is frozen, it breaks the dependency between M ? X, which is a straightforward implementation of causal intervention do(X = x). The original OLTR <ref type="bibr" target="#b8">[9]</ref> violates this intervention by fine-tuning the backbone parameters in the 2nd stage, and it thus performs much worse than the Decouple-OLTR in the <ref type="table" target="#tab_2">Table 2</ref> of original paper, which freezes the backbone parameters. Meanwhile, the balanced re-sampling also brings a fair d as we discussed in the third paragraph of Section A.</p><p>To better illustrate both the similarity and the difference between re-balancing NDE and the proposed TDE, we constructed a one-dimensional binary classification example for conventional classifier, one-/two-stage re-balancing classifiers, and the proposed TDE in <ref type="figure">Figure 7</ref>, where the gaussian distribution curve represents the feature distribution generated by the backbone, and the 0 point is the classifier's decision boundary. The conventional classifier and one-stage re-balancing are fundamentally problematic, because they either cause the mismatching in the inference or learn a bad backbone model. In the meantime, both two-stage re-balancing and the proposed TDE are able to correctly remove the bias by proper adjustments. The 2nd-stage re-balanced training (NDE) fixes the backbone parameters do(X = x) learnt from 1st-stage imbalanced training, i.e., the frozen curve in the image, and then re-samples an artificially balanced data distribution to create a fair d . The overall re-balancing NDE can be considered as subtracting a bias offset from original decision boundary. Meanwhile, the proposed TDE removes the bias effect (head projection) from feature vectors. Both two types of adjustments can properly remove the head bias in this example. That's why TDE and NDE should be theoretically identical in the long-tailed classification scenario. However, the 2nd-stage re-balancing NDE has two disadvantages: 1) its adjustment requires an additional training stage to fine-tune the classifier weights, which relies on the accessibility of data distribution; 2) if non-linear modules are applied to the feature vectors, e.g., a global context layer that conducts interactions among all objects {x j } in an image, the NDE can only remove a linear approximation of this non-linear activated head bias, while the TDE would be able to maintain the natural interactions of features in both original logit term and the subtracted counterfactual term. It explains why the Decouple-OLTR in <ref type="table" target="#tab_2">Table 2</ref> of original paper doesn't perform as good as Decouple-? -norm or Decouple-LWS, because OLTR involves non-linear interactions between feature vectors and memory vectors, so a linear adjustment on classifier's decision boundary cannot completely remove the head bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Ablation Studies</head><p>The hyper-parameters used in original paper are selected according to the performances on ImageNet-LT val set as shown in <ref type="table" target="#tab_7">Table 6</ref>. To further study the multi-head strategy on different normalized  <ref type="table">Table 7</ref>: The performances of cosine classifier <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref> and capsule classifier <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b53">54]</ref>   <ref type="bibr" target="#b61">[62]</ref>. classifiers, we tested the K = 2 on cosine classifier <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51]</ref> and capsule classifier <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b53">54]</ref> in <ref type="table">Table 7</ref>. It proves that the advantage of the proposed de-confounded model doesn't come from larger K, and the multi-head fine-grained sampling can generally improves the de-confounded training, no matter what kind of normalization function we choose.</p><p>As shown in <ref type="table">Table 8</ref>,9, we tested the proposed method on different backbones. After equipped with ResNeXt-101-32x4d and ResNeXt-101-64x4d <ref type="bibr" target="#b1">[2]</ref> for ImageNet-LT <ref type="bibr" target="#b8">[9]</ref> and LVIS <ref type="bibr" target="#b6">[7]</ref> V0.5, respectively, the proposed method gains additional improvements. In ImageNet-LT dataset, we changed some hyper-parameters (K = 4, ? = 1/64.0) and increased the training epochs to 120, because of the significantly increased number of model parameters. The hyper-parameters for LVIS are still the same as original paper.</p><p>We also reported the performances of the proposed method on LVIS V0.5 evaluation test server <ref type="bibr" target="#b61">[62]</ref> in <ref type="table" target="#tab_0">Table 10</ref>, where we used ResNeXt-101-64x4d backbone and the original hyper-parameters. It's worth noting that these are single model performances, which neither exploited external dataset nor utilized any model enhancement tricks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>(a) The proposed causal graph explaining the causal effect of momentum. See Section 3 for details. (b) The mean magnitudes of feature vectors for each class i after training with momentum ? = 0.9, where i is ranking from head to tail. (c) The relative change of the performance on the basis of ? = 0.98 shows that the few-shot tail is more vulnerable to the momentum.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>The TDE inference (Eq. (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>The influence of parameter ? in Eq. (8) on ImageNet-LT val set<ref type="bibr" target="#b8">[9]</ref> shows how D controls the head/tail preference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>imitates the long-tailed distribution that commonly exists in the real world. The test and val sets were balanced and reported on four splits: Many-shot containing classes with &gt; 100 images, Medium-shot including classes with ? 20 &amp; ? 100 images, Few-shot covering classes with &lt; 20 images, and Overall for all classes. LVIS<ref type="bibr" target="#b6">[7]</ref> is a large vocabulary instance segmentation dataset with 1,230/1,203 categories in V0.5/V1.0, respectively. It contains a 57k/100k train set (V0.5/V1.0) under a significant long-tailed distribution, and relatively balanced 5k/20k val set (V0.5/V1.0) and 20k test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Revisiting the previous state-of-the-arts in our causal graph. CDE: Controlled Direct Effect. NDE: Natural Direct Effect. TDE: Total Direct Effect.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>The performances on ImageNet-LT test set</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>MethodsLVIS Version AP AP50 AP 75 AP r AP c AP f AP bbox</figDesc><table><row><cell>Focal Loss  ? [28]</cell><cell>V0.5</cell><cell>21.1 32.1</cell><cell>22.6</cell><cell>3.2 21.1 28.3</cell><cell>22.6</cell></row><row><cell>(2019 Winner) EQL [17]</cell><cell>V0.5</cell><cell>24.9 37.9</cell><cell cols="2">26.7 10.3 27.3 27.8</cell><cell>27.9</cell></row><row><cell>Baseline</cell><cell>V0.5</cell><cell>22.6 33.5</cell><cell>24.4</cell><cell>2.5 23.0 30.2</cell><cell>24.3</cell></row><row><cell>Cosine  ? [50, 51]</cell><cell>V0.5</cell><cell>25.0 37.7</cell><cell>27.0</cell><cell>9.3 25.5 30.8</cell><cell>27.1</cell></row><row><cell>Capsule  ? [9, 54]</cell><cell>V0.5</cell><cell>25.4 37.8</cell><cell>27.4</cell><cell>8.5 26.4 31.0</cell><cell>27.1</cell></row><row><cell>(Ours) De-confound</cell><cell>V0.5</cell><cell>25.7 38.5</cell><cell cols="2">27.8 11.4 26.1 30.9</cell><cell>27.7</cell></row><row><cell>(Ours) Cosine-TDE</cell><cell>V0.5</cell><cell>28.1 42.6</cell><cell cols="2">30.2 20.8 28.7 30.3</cell><cell>30.6</cell></row><row><cell>(Ours) Capsule-TDE</cell><cell>V0.5</cell><cell>28.4 42.1</cell><cell cols="2">30.8 21.1 29.7 29.6</cell><cell>30.4</cell></row><row><cell>(Ours) De-confound-TDE</cell><cell>V0.5</cell><cell>28.4 43.0</cell><cell cols="2">30.6 22.1 29.0 30.3</cell><cell>31.0</cell></row><row><cell>Baseline</cell><cell>V1.0</cell><cell>21.8 32.7</cell><cell>23.2</cell><cell>1.1 20.9 31.9</cell><cell>23.9</cell></row><row><cell>(Ours) De-confound</cell><cell>V1.0</cell><cell>23.5 34.8</cell><cell>25.0</cell><cell>5.2 22.7 32.3</cell><cell>25.8</cell></row><row><cell>(Ours) De-confound-TDE</cell><cell>V1.0</cell><cell>27.1 40.1</cell><cell cols="2">28.7 16.0 26.9 32.1</cell><cell>30.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>AP AP 50 AP 75 AP r AP c AP f AP bbox</figDesc><table><row><cell>De-confound</cell><cell></cell><cell>25.7 38.5</cell><cell>27.8 11.4 26.1 30.9</cell><cell>27.7</cell></row><row><cell>De-confound-TDE</cell><cell>False</cell><cell>23.4 35.7</cell><cell>24.9 13.1 23.6 27.1</cell><cell>24.8</cell></row><row><cell>De-confound-TDE</cell><cell>True</cell><cell>28.4 43.0</cell><cell>30.6 22.1 29.0 30.3</cell><cell>31.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>One-stage Re-balancing</head><label></label><figDesc>where the</figDesc><table><row><cell>Class A Training (imbalanced data) 0 Class B Class A 0 Class B Testing (balanced data) Mismatch (a) Baseline</cell><cell>Class A Training (re-balanced data) 0 Class B Class A 0 Class B Testing (balanced data) Bad Model (b) Training Samples</cell><cell>Re-balanced Samples (c) Two-stage Re-balancing Testing Samples Class A 0 Class B 1 st Stage Training (imbalanced data) Class A 0 Class B 2 nd Stage Training (re-balanced data) Class A 0 Class B Testing (balanced data)</cell><cell>Class A Training (imbalanced data) 0 Class B Class A 0 Class B Testing (balanced data) (d) The Proposed TDE</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Hyper-parameters selection based on performances of ImageNet-LT val set, where for ? means that TDE inference is not included. The backbone we used here is ResNeXt-50-32x4d.</figDesc><table><row><cell>K</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="3">Many-shot Medium-shot Few-shot Overall</cell></row><row><cell cols="3">1 16.0 1/32.0</cell><cell></cell><cell>69.8</cell><cell>42.8</cell><cell>14.9</cell><cell>49.4</cell></row><row><cell cols="3">4 16.0 1/32.0</cell><cell></cell><cell>69.0</cell><cell>42.3</cell><cell>13.1</cell><cell>48.6</cell></row><row><cell>2</cell><cell cols="2">8.0 1/32.0</cell><cell></cell><cell>69.5</cell><cell>31.3</cell><cell>1.6</cell><cell>42.0</cell></row><row><cell cols="3">2 32.0 1/32.0</cell><cell></cell><cell>68.6</cell><cell>41.3</cell><cell>13.0</cell><cell>47.9</cell></row><row><cell cols="3">2 16.0 1/16.0</cell><cell></cell><cell>69.3</cell><cell>44.0</cell><cell>14.2</cell><cell>49.7</cell></row><row><cell cols="3">2 16.0 1/64.0</cell><cell></cell><cell>69.9</cell><cell>43.3</cell><cell>14.7</cell><cell>49.6</cell></row><row><cell cols="3">2 16.0 1/32.0</cell><cell></cell><cell>69.5</cell><cell>43.9</cell><cell>15.2</cell><cell>49.8</cell></row><row><cell cols="4">2 16.0 1/32.0 2.5</cell><cell>66.2</cell><cell>49.8</cell><cell>29.4</cell><cell>53.3</cell></row><row><cell cols="4">2 16.0 1/32.0 3.0</cell><cell>64.5</cell><cell>50.0</cell><cell>32.6</cell><cell>53.3</cell></row><row><cell cols="4">2 16.0 1/32.0 3.5</cell><cell>62.5</cell><cell>49.9</cell><cell>36.0</cell><cell>52.9</cell></row><row><cell cols="2">Methods</cell><cell></cell><cell cols="4">#heads K Many-shot Medium-shot Few-shot Overall</cell></row><row><cell cols="2">Cosine  ? [50, 51]</cell><cell></cell><cell>1</cell><cell>67.3</cell><cell>41.3</cell><cell>14.0</cell><cell>47.6</cell></row><row><cell cols="2">Cosine  ? [50, 51]</cell><cell></cell><cell>2</cell><cell>67.5</cell><cell>42.1</cell><cell>14.1</cell><cell>48.1</cell></row><row><cell cols="2">Capsule  ? [9, 54]</cell><cell></cell><cell>1</cell><cell>67.1</cell><cell>40.0</cell><cell>11.2</cell><cell>46.5</cell></row><row><cell cols="2">Capsule  ? [9, 54]</cell><cell></cell><cell>2</cell><cell>67.7</cell><cell>41.3</cell><cell>12.6</cell><cell>47.6</cell></row><row><cell cols="3">(Ours) De-confound</cell><cell>1</cell><cell>67.3</cell><cell>41.8</cell><cell>15.0</cell><cell>47.9</cell></row><row><cell cols="3">(Ours) De-confound</cell><cell>2</cell><cell>67.9</cell><cell>42.7</cell><cell>14.7</cell><cell>48.6</cell></row><row><cell cols="3">(Ours) Cosine-TDE</cell><cell>1</cell><cell>61.8</cell><cell>47.1</cell><cell>30.4</cell><cell>50.5</cell></row><row><cell cols="3">(Ours) Cosine-TDE</cell><cell>2</cell><cell>63.0</cell><cell>47.3</cell><cell>31.0</cell><cell>51.1</cell></row><row><cell cols="3">(Ours) Capsule-TDE</cell><cell>1</cell><cell>62.3</cell><cell>46.9</cell><cell>30.6</cell><cell>50.6</cell></row><row><cell cols="3">(Ours) Capsule-TDE</cell><cell>2</cell><cell>62.4</cell><cell>47.9</cell><cell>31.5</cell><cell>51.2</cell></row><row><cell cols="3">(Ours) De-confound-TDE</cell><cell>1</cell><cell>62.5</cell><cell>47.8</cell><cell>32.8</cell><cell>51.4</cell></row><row><cell cols="3">(Ours) De-confound-TDE</cell><cell>2</cell><cell>62.7</cell><cell>48.8</cell><cell>31.6</cell><cell>51.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :Table 9 :</head><label>89</label><figDesc>under different number of head K on ImageNet-LT test set. Other hyper-parameters are fixed. The performances of the proposed method under different backbones in ImageNet-LT test set. Methods Backbone AP AP 50 AP 75 AP r AP c AP f AP bbox De-confound-TDE X101-FPN 30.4 45.1 32.9 21.1 31.8 32.3 33.1 The performances of the proposed method under different backbones in LVIS V0.5 val set. Methods AP AP 50 AP 75 AP r AP c AP f Baseline 19.4 29.8 20.6 3.9 21.9 30.8 De-confound 20.8 31.8 22.1 7.4 22.7 31.2 De-confound-TDE 23.0 35.2 24.1 12.7 24.5 30.7 Table 10: The single model performances of the proposed method on LVIS V0.5 evaluation test server</figDesc><table><row><cell>Methods</cell><cell>Backbone</cell><cell cols="5">Many-shot Medium-shot Few-shot Overall</cell></row><row><cell>Baseline</cell><cell>ResNeXt-50</cell><cell>66.1</cell><cell></cell><cell>38.4</cell><cell>8.9</cell><cell>45.0</cell></row><row><cell>De-confound</cell><cell>ResNeXt-50</cell><cell>67.9</cell><cell></cell><cell>42.7</cell><cell>14.7</cell><cell>48.6</cell></row><row><cell cols="2">De-confound-TDE ResNeXt-50</cell><cell>62.7</cell><cell></cell><cell>48.8</cell><cell>31.6</cell><cell>51.8</cell></row><row><cell>Baseline</cell><cell>ResNeXt-101</cell><cell>68.7</cell><cell></cell><cell>42.5</cell><cell>11.8</cell><cell>48.4</cell></row><row><cell>De-confound</cell><cell>ResNeXt-101</cell><cell>68.9</cell><cell></cell><cell>44.3</cell><cell>16.5</cell><cell>50.0</cell></row><row><cell cols="2">De-confound-TDE ResNeXt-101</cell><cell>64.7</cell><cell></cell><cell>50.0</cell><cell>33.0</cell><cell>53.3</cell></row><row><cell>Baseline</cell><cell cols="2">R101-FPN 22.6 33.5</cell><cell>24.4</cell><cell cols="2">2.5 23.0 30.2</cell><cell>24.3</cell></row><row><cell>De-confound</cell><cell cols="2">R101-FPN 25.7 38.5</cell><cell cols="3">27.8 11.4 26.1 30.9</cell><cell>27.7</cell></row><row><cell cols="3">De-confound-TDE R101-FPN 28.4 43.0</cell><cell cols="3">30.6 22.1 29.0 30.3</cell><cell>31.0</cell></row><row><cell>Baseline</cell><cell cols="2">X101-FPN 26.4 39.5</cell><cell>28.4</cell><cell cols="2">7.4 28.1 32.0</cell><cell>28.5</cell></row><row><cell>De-confound</cell><cell cols="2">X101-FPN 28.4 41.9</cell><cell cols="3">30.6 13.3 29.5 32.9</cell><cell>30.5</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pareto, zipf and other power laws</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Economics letters</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="19" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lvis: A dataset for large vocabulary instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5356" to="5364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Distribution-balanced loss for multi-label classification in long-tailed datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingqiu</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-scale long-tailed recognition in an open world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bbn: Bilateral-branch network with cumulative learning for long-tailed visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiu-Shen</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao-Min</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Jiashi Feng, and Yannis Kalantidis. Decoupling representation and classifier for long-tailed recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Albert</forename><surname>Gordo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning imbalanced datasets with label-distribution-aware margin loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaidi</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Arechiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1567" to="1578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Class-balanced loss based on effective number of samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9268" to="9277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cost-sensitive learning of deep feature representations from imbalanced data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Salman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ferdous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Togneri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Relay backpropagation for effective learning of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouchen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="467" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ashwin Bharambe, and Laurens van der Maaten. Exploring the limits of weakly supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Equalization loss for long-tailed object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingru</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changbao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanquan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changqing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the importance of initialization and momentum in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1139" to="1147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the momentum term in gradient descent learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ning Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="145" to="151" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Causal diagrams for empirical research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Direct and indirect effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th conference on uncertainty in artificial intelligence</title>
		<meeting>the 17th conference on uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Causal inference in statistics: A primer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madelyn</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cascade r-cnn: Delving into high quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaowei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Smote: synthetic minority over-sampling technique</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nitesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><forename type="middle">W</forename><surname>Chawla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">O</forename><surname>Bowyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Philip</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kegelmeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="321" to="357" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Class imbalance and cost sensitivity: why under-sampling beats over-sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Drummond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Holte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on learning from imbalanced datasets II</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2003" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to segment the tail</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Meta-weight-net: Learning an explicit mapping for sample weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixuan</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanping</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongben</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deyu</forename><surname>Meng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.09050</idno>
		<title level="m">Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rethinking class-balanced methods for long-tailed visual recognition from a domain adaptation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhammad</forename><forename type="middle">Abdullah</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Overcoming classifier imbalance for long-tail object detection with balanced group softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jintao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Dynamic curriculum learning for imbalanced data classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep representation learning on long-tailed data: A learnable embedding augmentation perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuchu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaopeng</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The Book of Why: The New Science of Cause and Effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><surname>Mackenzie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Basic Books</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mediation analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><forename type="middle">J</forename><surname>David P Mackinnon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew S</forename><surname>Fairchild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Psychol</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The statistics of causal inference: A view from political methodology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Keele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analysis</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mediation analysis in epidemiology: methods, interpretation and bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Richiardi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rino</forename><surname>Bellocco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniela</forename><surname>Zugna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of epidemiology</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unbiased scene graph generation from biased training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Two causal principles for improving visual dialog</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04315</idno>
		<title level="m">Counterfactual vqa: A cause-effect look at language bias</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Deconfounded image captioning: A causal retrospect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03923</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Causal intervention for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiansheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Interventional few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongqi</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title/>
		<ptr target="https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html" />
	</analytic>
	<monogr>
		<title level="j">SGD implementation in PyTorch</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A three-way decomposition of a total effect into direct, indirect, and interactive effects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tyler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vanderweele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epidemiology</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">A tutorial on energy-based learning. Predicting structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An introduction to propensity score methods for reducing the effects of confounding in observational studies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate behavioral research</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Dynamic few-shot visual learning without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Low-shot learning with imprinted weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">On the consistency rule in causal inference: axiom, definition, assumption, or theorem?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epidemiology</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="872" to="875" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Bpr: Bayesian personalized ranking from implicit feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3856" to="3866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ramprasaath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Batra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">mixup: Beyond empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ICLR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangmiao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuyang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wansen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiarui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dazhi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenchen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianheng</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qijie</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Buyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingdong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07155</idno>
		<title level="m">Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Applied statistics and probability for engineers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Douglas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George C</forename><surname>Montgomery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Runger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A closer look at few-shot classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title/>
		<idno>LVIS v0.5</idno>
		<ptr target="https://evalai.cloudcv.org/web/challenges/challenge-page/473/overview" />
	</analytic>
	<monogr>
		<title level="j">Evaluation Server</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
