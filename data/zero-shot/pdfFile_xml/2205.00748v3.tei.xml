<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual networks based 3D Multi-Person Pose Estimation from Monocular Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>MAY YYYY 1</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Robby</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
						</author>
						<title level="a" type="main">Dual networks based 3D Multi-Person Pose Estimation from Monocular Video</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE</title>
						<imprint>
							<biblScope unit="volume">XXX</biblScope>
							<date type="published">MAY YYYY 1</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Monocular 3D human pose estimation has made progress in recent years. Most of the methods focus on single persons, which estimate the poses in the person-centric coordinates, i.e., the coordinates based on the center of the target person. Hence, these methods are inapplicable for multi-person 3D pose estimation, where the absolute coordinates (e.g., the camera coordinates) are required. Moreover, multi-person pose estimation is more challenging than single pose estimation, due to inter-person occlusion and close human interactions. Existing top-down multi-person methods rely on human detection (i.e., top-down approach), and thus suffer from the detection errors and cannot produce reliable pose estimation in multi-person scenes. Meanwhile, existing bottom-up methods that do not use human detection are not affected by detection errors, but since they process all persons in a scene at once, they are prone to errors, particularly for persons in small scales. To address all these challenges, we propose the integration of top-down and bottom-up approaches to exploit their strengths. Our top-down network estimates human joints from all persons instead of one in an image patch, making it robust to possible erroneous bounding boxes. Our bottom-up network incorporates human-detection based normalized heatmaps, allowing the network to be more robust in handling scale variations. Finally, the estimated 3D poses from the top-down and bottom-up networks are fed into our integration network for final 3D poses. To address the common gaps between training and testing data, we do optimization during the test time, by refining the estimated 3D human poses using high-order temporal constraint, re-projection loss, and bone length regularizations. We also introduce a two-person pose discriminator that enforces natural two-person interactions. Finally, we apply a semi-supervised method to overcome the 3D ground-truth data scarcity. Our evaluations demonstrate the effectiveness of the proposed method and its individual components. Our code and pretrained models are available publicly: https://github.com/3dpose/3D-Multi-Person-Pose.</p><p>Index Terms-3D multi-person pose estimation, semi-supervised learning, test time optimization. !</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>3D multi-person pose estimation from a monocular video is useful for many real-world applications (e.g., <ref type="bibr" target="#b57">[58]</ref>, <ref type="bibr" target="#b58">[59]</ref>, <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b61">[62]</ref>). However, this multi-person estimation is challenging not only because of the inter-person occlusion but also because of the necessity to estimate 3D poses in an absolute coordinate system (e.g., the camera coordinates), where each person is located properly to the other persons reflecting their locations in the real scenes <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b37">[38]</ref>. However, the progress in 3D human pose estimation in the last few years mostly lies in single-person case. Existing methods can be generally grouped into top-down or bottom-up approaches, where the top-down methods employ a human detection method to detect each person and then perform human pose estimation, while the bottomup methods estimate all human keypoints simultaneously and then group them to form one or several skeletons.</p><p>Multi-person pose estimation methods can be grouped into top-down and bottom-up approaches. Top-down methods first use human detection to detect every person in an image, and then process the cropped image patch of each detected person individually. The benefit of the top-down approach is that human detection can ensure that in each image patch there is only one target person. Moreover, the size of the image patches can be normalized, alleviating <ref type="bibr">?</ref>  the variation of human scales. The downside of the topdown approach is that, if human detection fails to detect one or a few persons, then there is no chance to predict the poses for those persons. Unlike top-down methods, bottom-up methods do not rely on human detection to detect each person, instead, they simultaneously detect all possible keypoints in a given image, and then group them to form individual human poses. The disadvantage of bottomup methods is that the whole image is processed at once and no person-wise normalization can be performed, and thus the accuracy of pose estimation for small-scale persons particularly can be affected.</p><p>Based on the pros and cons of top-down and bottom-up methods, it is clear that neither one is suitable for all scenarios. Top-down methods can miss some persons in the case of occlusions, while bottom-up methods can not achieve good accuracy for small-scale persons. Such observation motivates us to develop a dual network for multi-person 3D pose estimation that integrates both the top-down and bottom-up networks, to robustly handle the challenging cases including occlusions and small-scale persons. <ref type="table" target="#tab_1">Table 1</ref> summarizes the top-down and bottom-up methods in 2D and 3D human pose estimation. The majority of them are top-down and bottom-up 2D human pose estimation, followed by methods in 3D human pose estimation for single persons, which use the person-centric coordinates. Few methods are proposed to handle 3D multi-person pose estimation, particularly those that employ the bottomup approach. <ref type="table">Table 2</ref> summarizes the differences between single-person and multi-person 3D pose estimation. Single-arXiv:2205.00748v3 [cs.CV] 6 May 2022</p><p>Top-down / Bottom-up 2D / 3D</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coordinate system Methods</head><p>Top-down 2D human pose estimation 2D image coordinate <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref> 3D human pose estimation 3D person-centric <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref> [24], <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> 3D camera-centric <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b37">[38]</ref> Bottom-up 2D human pose estimation 2D image coordinate <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b40">[41]</ref>, <ref type="bibr" target="#b41">[42]</ref>, <ref type="bibr" target="#b42">[43]</ref>, <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b45">[46]</ref>, <ref type="bibr" target="#b46">[47]</ref>, <ref type="bibr" target="#b47">[48]</ref>, <ref type="bibr" target="#b48">[49]</ref>, <ref type="bibr" target="#b49">[50]</ref> [51], <ref type="bibr" target="#b51">[52]</ref> 3D human pose estimation 3D person-centric N/A 3D camera-centric <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>  Human3.6M, 3DHP, HumanEva, Penn Action <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>  <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref> [22], <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>  <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b30">[31]</ref> 3D multi-person pose Camera-centric (absolute coordinate) PCK abs , AP root 25 , MPRE <ref type="bibr" target="#b34">[35]</ref>, F1 value <ref type="bibr" target="#b53">[54]</ref> MuPoTS-3D, JTA, 3DPW <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b55">[56]</ref>, <ref type="bibr" target="#b56">[57]</ref> [38], <ref type="bibr" target="#b54">[55]</ref> TABLE 2 Summary of the differences between 3D human pose estimation and multi-person pose estimation.</p><p>person methods use the person-centric coordinates, while multi-person methods use the camera-centric coordinates. This difference further influences the metrics and datasets used for evaluations.</p><p>Existing top-down 3D pose estimation methods rely considerably on human detection to localize each person, prior to estimating the joints within the detected bounding boxes, e.g., <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b34">[35]</ref>. These methods show promising performance for single-person 3D-pose estimation <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b27">[28]</ref>, yet since they treat each person individually, they have no awareness of non-target persons and the possible interactions. When multiple persons occlude each other, human detection also becomes unreliable. Moreover, when target persons are closely interacting with each other, the pose estimator may be misled by the nearby persons, e.g., predicted joints may come from the nearby non-target persons.</p><p>Recent bottom-up methods (e.g., <ref type="bibr" target="#b52">[53]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>) achieve comparable performance in multi-person datasets without using human detection. As bottom-up methods consider multiple persons simultaneously and, in certain cases, may better distinguish the joints of different persons. Unfortunately, without using detection, bottom-up methods suffer from the scale variations (i.e., no bounding box to normalize image patch), and the pose estimation accuracy is compromised, rendering inferior performance compared with topdown approaches <ref type="bibr" target="#b48">[49]</ref>. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, neither top-down nor bottom-up approach alone can handle all the challenges at once, particularly the challenges of: inter-person occlusion, close interactions, and human-scale variations.</p><p>In this paper, we aim to integrate the top-down and bottom-up approaches to achieve more accurate and robust 3D multi-person pose estimation from a monocular video. To achieve this, we introduce a top-down network to es-timate human joints inside each detected bounding box. Unlike existing top-down methods that only estimate one human pose given a bounding box, our top-down network predicts 3D poses for all persons inside the bounding box. Our top-down network generates the joint heatmaps and feeds them to our bottom-up network, which enables our bottom-up network to handle the scale variations. Finally, the estimated 3D poses from both top-down and bottom-up networks are processed by our integration network to obtain the final estimated 3D poses given an image sequence.</p><p>Unlike existing methods' pose discriminators, which are designed solely for a single person, and consequently cannot enforce natural inter-person interactions, we propose a twoperson pose discriminator that enforces two-person natural interactions. We also employ semi-supervised learning to mitigate the data scarcity problem where 3D ground-truth data is limited. To address the domain gap problem between the training and testing data, we do optimization during test time. In particular, we propose novel approaches to refine the estimated 3D human poses through high-order temporal constraint, reprojection loss, and bone-length regularization.</p><p>This paper is based on our conference paper <ref type="bibr" target="#b62">[63]</ref>. Unlike our conference version, however, we add test time optimization to handle the gap between training and testing data in Section 3.5, which is critical for our method to process unseen videos. For this test-time optimization to work, we propose new unsupervised losses, i.e., high-order temporal constraints in Eq. 14, reprojection loss in Eq. 10, and bone-length regularization in Eq. 15. We also provide more analysis for inter-person pose discriminator in <ref type="table">Table 5</ref>, detailed information of the semi-supervised learning part in <ref type="figure" target="#fig_9">Fig. 7</ref>, and more extensive qualitative comparisons against SOTA methods on MuPoTS and JTA datasets in <ref type="figure" target="#fig_0">Fig. 10</ref> and</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Frame 140</head><p>Frame 150 Frame 180</p><p>Topdown</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input frames</head><p>Our results  <ref type="bibr" target="#b34">[35]</ref>, the bottom-up method is SMAP <ref type="bibr" target="#b54">[55]</ref>. The input images are from MuPoTS-3D dataset <ref type="bibr" target="#b55">[56]</ref>. The top-down method suffers from inter-person occlusion and the bottom-up method is sensitive to scale variations (i.e., the 3D poses of the two persons in the back are inaccurately estimated). Our method substantially outperforms the state-ofthe-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottomup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11</head><p>. In summary, our contributions are listed as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We introduce a novel two-branch framework, where the top-down network detects multiple persons and the bottom-up network incorporates the normalized image patches in its process. Our framework gains benefits from the two networks, and at the same time, overcomes their shortcomings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We employ multi-person pose estimation for our top-down network, which can effectively handle the inter-person occlusion and interactions caused by detection errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We incorporate human detection information into our bottom-up network so that it can better handle the scale variation, which addresses the problem in existing bottom-up methods.</p><p>? Unlike the existing discriminators that focus on single person pose, we introduce a novel discriminator that enforces the validity of human poses of close pairwise interactions in the camera-centric coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>?</head><p>We propose high-order temporal constraint and bone length loss for test time optimization to improve the generalization capability of our method on testing videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Top-Down Monocular 3D Human Pose Estimation Existing top-down 3D human pose estimation methods commonly use human detection as an essential part of their methods to estimate person-centric 3D human poses <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b63">[64]</ref>. They demonstrate promising performance on single-person evaluation datasets <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b65">[66]</ref>, unfortunately the performance decreases in multi-person scenarios, due to inter-person occlusion or close interactions <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b27">[28]</ref>. Moreover, the produced person-centric 3D poses cannot be used for multi-person scenarios, where cameracentric 3D-pose estimation is needed. Top-down methods process each person independently, leading to inadequate awareness of the existence of other persons nearby. As a result, they perform poorly on multi-person videos where inter-person occlusion and close interactions are commonly present. Rogez et al. <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b66">[67]</ref> develop a pose proposal network to generate bounding boxes and then perform pose estimation individually for each person. Recently, unlike previous methods that perform person-centric pose estimation, Moon et al. <ref type="bibr" target="#b34">[35]</ref> propose a top-down 3D multiperson pose-estimation method that can estimate the poses for all persons in an image in the camera-centric coordinates. However, the method still relies on detection and processes each person independently; hence it is likely to suffer from inter-person occlusion and close interactions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bottom-Up Monocular 3D Human Pose Estimation</head><p>A few bottom-up methods have been proposed <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b53">[54]</ref>, <ref type="bibr" target="#b54">[55]</ref>. Fabbri et al. <ref type="bibr" target="#b53">[54]</ref> introduce an encoder-decoder framework to compress a heatmap first, and then decompress it back to the original representations in the test time for fast HD image processing. Mehta et al. <ref type="bibr" target="#b33">[34]</ref> propose to identify individual joints, compose full-body joints, and enforce temporal and kinematic constraints in three stages for real-time 3D motion capture. Li et al. <ref type="bibr" target="#b35">[36]</ref> develop an integrated method with lower computation complexity for human detection, person-centric pose estimation, and human depth estimation from an input image. Lin et al. <ref type="bibr" target="#b36">[37]</ref> formulate the human depth regression as a bin index estimation problem for multi-person localization in the camera coordinate system. Zhen et al. <ref type="bibr" target="#b54">[55]</ref> estimate the 2.5D representation of body parts first and then reconstruct cameracentric multi-person 3D poses. These methods benefit from the nature of the bottom-up approach, which can process multiple persons simultaneously without relying on human detection. However, since all persons are processed at the same scale, these methods are inevitably sensitive to human scale variations, which limits their applicability on wild videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Top-Down and Bottom-Up Combination</head><p>Earlier non-deep learning methods exploring the combination of top-down and bottom-up approaches for human pose estimation are in the forms of data-driven belief propagation, different classifiers for joint location and skeleton, or probabilistic Gaussian mixture modelling <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b68">[69]</ref>, <ref type="bibr" target="#b69">[70]</ref>. Recent deep learning based methods that attempt to make use of both top-down and bottom-up information are mainly on estimating 2D poses <ref type="bibr" target="#b70">[71]</ref>, <ref type="bibr" target="#b71">[72]</ref>, <ref type="bibr" target="#b72">[73]</ref>, <ref type="bibr" target="#b73">[74]</ref>. <ref type="bibr">Hu</ref>    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Integrated results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inter-person Discriminator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test Time Optimization for Human Pose Estimation</head><p>Although supervised learning approaches show promising results in human pose estimation, it is unavoidable to encounter images/videos in testing which are not seen in training datasets in terms of appearance, motion, pose, occlusions, etc. As a result, existing deep learning based pose estimation methods may not perform well on testing data. A few recent works explore to mitigate these issues <ref type="bibr" target="#b74">[75]</ref>, <ref type="bibr" target="#b75">[76]</ref>, <ref type="bibr" target="#b76">[77]</ref>, <ref type="bibr" target="#b77">[78]</ref>. Zhang et al. <ref type="bibr" target="#b74">[75]</ref> propose to check the validity of estimated 2D pose and the consistency of lifted 3D poses from randomly projected 2D poses to refine the estimated 3D human pose. Cheng et al. <ref type="bibr" target="#b75">[76]</ref> propose to use 2D pose estimator's confidence to weight a re-projection loss in inference stage to make pose corrections when estimated 3D pose is erroneous but the 2D pose is more accurate where 2D pose estimator is trained on dataset with larger variations in appearance and pose (i.e., 2D annotation is easier to obtain compared to 3D ground-truth). Shimada et al. <ref type="bibr" target="#b76">[77]</ref> propose to enforce physical constraints to ensure the estimated 3D human poses are physically plausible. Su et al. <ref type="bibr" target="#b77">[78]</ref> propose a neural radiance fields (NeRF) based 3D pose correction framework where estimated 3D pose of a person is used as input for a customized NeRF to render the person to compute a image difference loss against input image with the person to correct the initial 3D pose estimated. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Top-Down Network</head><p>Given a human detection bounding box, existing top-down methods estimate full-body joints of one person. Consequently, if there are multiple persons inside the box or partially out-of-bounding box body parts, the full-body joint estimation are likely to be erroneous. <ref type="figure" target="#fig_3">Fig. 3</ref> shows such failure examples of existing methods. In contrast, our method produces the heatmaps for all joints inside the bounding box (i.e., enlarged to accommodate inaccurate detection), and estimates the ID for each joint to group them into corresponding persons, similar to <ref type="bibr" target="#b41">[42]</ref>. Given an input video, for every frame we apply a human detector <ref type="bibr" target="#b0">[1]</ref>, and crop the image patches based on the detected bounding boxes. A 2D pose detector <ref type="bibr" target="#b48">[49]</ref> is applied to each patch to generate heatmaps for all human joints, such as shoulder, pelvis, ankle, and etc. Specifically, our topdown loss of 2D pose heatmap is an L2 loss between the predicted and ground-truth heatmaps, formulated as: where H andH are the predicted and ground-truth heatmaps, respectively. Having obtained the 2D pose heatmaps, a directed GCN network is used to refine the potentially incomplete poses caused by occlusions or partially out-of-bounding box body parts, and two TCNs are used to estimate both personcentric 3D pose and camera-centric root depth based on a given sequence of 2D poses similar to <ref type="bibr" target="#b37">[38]</ref>. As the TCN requires the input sequence of the same instance, a pose tracker <ref type="bibr" target="#b78">[79]</ref> is used to track each instance in the input video. We also apply data augmentation in training our TCN so that it can handle occlusions <ref type="bibr" target="#b27">[28]</ref>.</p><formula xml:id="formula_0">L T D hmap = |H ?H| 2 2 ,<label>(1)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Bottom-Up Network</head><p>Top-down methods perform estimation inside the bounding boxes, and thus are lack of global awareness of other persons, leading to difficulties to estimate poses in the cameracentric coordinates. To address this problem, we further propose a bottom-up network that processes multiple persons simultaneously. Since the bottom-up pose estimation suffers from human scale variations, we concatenate the heatmaps from our top-down network with the original input frame as the input of our bottom-up network. With the guidance of the top-down heatmaps, which are the results of the object detector and pose estimation based on the normalized boxes, the estimation of the bottom-up network will be more robust to scale variations. Our bottom-up network outputs four heatmaps : a 2D pose heatmap, ID-tag map, relative depth map, and root depth map. The 2D pose heatmap and ID-tag map are defined in the same way as in the previous section (3.1). The relative depth map refers to the depth map of each joint with respect to its root (pelvis) joint. The root depth map represents the depth map of the root joint.</p><p>In particular, the loss functions L BU hmap and L BU id for the heatmap and ID-tag map are similar to <ref type="bibr" target="#b41">[42]</ref>. In addition, we apply the depth loss to the estimations of both the relative depth map h rel and the root depth h root . Please see <ref type="figure" target="#fig_1">Fig. 2</ref> for example of the four estimated heatmaps from the bottomup network. For N persons and K joints, the loss can be formulated as:</p><formula xml:id="formula_1">L depth = 1 N K n k |h k (x nk , y nk ) ? d nk | 2 ,<label>(2)</label></formula><p>where h is the depth map and d is the ground-truth depth value. Note that, for the pelvis (i.e., the root joint), the depth is a camera-centric depth. For other joints, the depth is relative with respect to the corresponding root joint.</p><p>We group the heatmaps into instances (i.e., persons), and retrieve the joint locations using the same procedure as in the top-down network. Moreover, the values of the cameracentric depth of the root joint z root and the relative depth for the other joints z rel k are obtained by retrieving from the corresponding depth maps where the joints (i.e., root or others) are located. Specifically:</p><formula xml:id="formula_2">z root i = h root (x root i , y root i ) (3) z rel i,k = h rel k (x i,k , y i,k )<label>(4)</label></formula><p>where i, k refer to the i th instance and k th joint, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Integration with Interaction-Aware Discriminator</head><p>Having obtained the results from the top-down and bottomup networks, we first need to find the corresponding poses between the results from the two networks, i.e., the topdown pose P T D i and bottom-up pose P BU j belong to the same person. Note that P stands for camera-centric 3D pose throughout this paper.</p><p>Given two pose sets from bottom-up network P BU and top-down network P T D , we match the poses from both sets, in order to form pose pairs. The similarity of two poses is defined as:</p><formula xml:id="formula_3">Sim i,j = K k=0 min(c BU i,k , c T D j,k )OKS(P BU i,k , P T D j,k ),<label>(5)</label></formula><p>where:</p><formula xml:id="formula_4">OKS(x, y) = exp(? d(x, y) 2 2s 2 ? 2 ),<label>(6)</label></formula><p>OKS stands for object keypoint similarity <ref type="bibr" target="#b7">[8]</ref>, which measures the joint similarity of a given joint pair. d(x, y) is the Euclidean distance between two joints. s and ? are two controlling parameters. , respectively. Having computed the similarity matrix between the two sets of poses P T D and P BU according to the Sim i,j definition, the Hungarian algorithm <ref type="bibr" target="#b79">[80]</ref> is used to obtain the matching results.</p><p>Once the matched pairs are obtained, we feed each pair of the 3D poses and the confidence score of each joint to our integration network. Our integration network consists of 3 fully connected layers, which outputs the final estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Integration Network Training</head><p>To train the integration network, we take some samples from the ground-truth 3D poses. We apply data augmentation: 1) random masking the joints with a binary mask M kpt to simulate occlusions; 2) random shifting the joints to simulate the inaccurate pose detection; and 3) random zeroing one from a pose pair to simulate unpaired poses. The loss of the integration network is an L2 loss between the predicted 3D pose and its groundtruth:</p><formula xml:id="formula_5">L int = 1 K k |P k ?P k | 2 ,<label>(7)</label></formula><p>where K is the number of the estimated joints. P andP are the estimated and ground-truth 3D poses, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Inter-Person Discriminator</head><p>For training the integration network, we propose a novel inter-person discriminator. Unlike most existing discriminators for human pose estimation (e.g. <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b30">[31]</ref>), where they can only discriminate the plausible 3D poses of one person, we propose an interaction-aware discriminator to enforce the interaction of a pose pair is natural and reasonable, which not only includes the existing single-person discriminator, but also generalize to interacting persons. Specifically, our discriminator contains two sub-networks: D 1 , which is dedicated for one personcentric 3D poses; and, D 2 , which is dedicated for a pair of camera-centric 3D poses from two persons. We apply the following loss to train the network, which is formulated as:</p><formula xml:id="formula_6">L dis = log(C) + log(1 ? C)<label>(8)</label></formula><p>where:</p><formula xml:id="formula_7">C = 0.25(D 1 (P a ) + D 1 (P b )) + 0.5D 2 (P a , P b ) C = 0.25(D 1 (P a ) + D 1 (P b )) + 0.5D 2 (P a ,P b )<label>(9)</label></formula><p>where P a , P b are the estimated poses of person a and person b, respectively.P are the estimated and ground-truth 3D poses, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Semi-Supervised Training</head><p>Semi-supervised learning is an effective technique to improve the network performance, particularly when the data with ground-truths are limited. A few works also explore how to make use of the unlabeled data <ref type="bibr" target="#b78">[79]</ref>, <ref type="bibr" target="#b80">[81]</ref>, <ref type="bibr" target="#b81">[82]</ref>. We first train our network with the 3D ground-truth dataset only, and then use the trained network to generate the pseudo-labels of unlabelled data, which are then used to fine tune the network. The generated pseudo-labels cannot be directly used because some of them are likely incorrect. Unlike recent noisy student training strategy <ref type="bibr" target="#b82">[83]</ref>, where data with ground-truth labels and pseudo-labels are mixed to train the student network by adding various types of noise (i. e., augmentations, dropout, etc), we use reprojection loss and multi-perspective loss to correct the errors in the pseudo-labels. Our Semisupervised Learning (SSL) pipeline is shown in <ref type="figure" target="#fig_5">Fig. 4</ref>. First, we use the trained model to generate the pseudo-label of the unlabelled data, which is the COCO dataset in our experiment. Note that, we use only the images, and not the 2D ground-truths of the joints to mimic the unlabelled data scenario. Therefore, we use two consistency terms to measure the quality of all the pseudo-labels: the reprojection loss and multi-perspective loss.</p><p>As the pose variations of 2D datasets are more abundant than those of 3D datasets, e.g. COCO compared to H36M, the estimated 2D poses are more robust than the estimated 3D poses in terms of different environments and poses. Existing reprojection loss <ref type="bibr" target="#b25">[26]</ref> measures the deviation between generated 3D poses and detected 2D poses. Unlike this, we make use of the confidence of the joints from the 2D pose heatmap as weight in computating the reprojection loss to adjust adaptively how much we should enforce the reprojected 3D poses to match the estimated 2D poses based on the confidence of the joints. Thus, the reprojection loss is formulated as:</p><formula xml:id="formula_8">L rep = 1 K K k=1 C k |rep(X 3D,k ) ? X 2D,k | 2<label>(10)</label></formula><p>where the X 3D is the predicted 3D pose from the network, and X 2D stands for the 2D estimations from our multiperson 2D pose estimator. rep(?) is the reprojection function from 3D to 2D. K stands for the number of joints in total. Moreover, the loss is a weighted sum of each joint's confidence score C k , which is the maximum value of the joint's heatmap. A multi-perspective loss is used as an additional measure to enforce the consistency of the predicted 3D poses from different viewing angles <ref type="bibr" target="#b80">[81]</ref>. As shown in <ref type="figure" target="#fig_5">Fig. 4</ref>, given a pseudo-label 3D pose P pse 3D , we randomly rotate the pose along y axis (i.e., y-axis is perpendicular to the ground plane) to obtain P pse 3D , and re-project it to the 2D coordinates. Finally, we predict the P pse 3D based on the 2D projection from P pses 3D . The reprojection loss measures the deviation between the projection of generated 3D poses and the detected 2D poses. Since there are more abundant data variations in 2D pose dataset compared to 3D pose dataset (e.g., COCO is much larger compared to H36M), the 2D estimator is expected to be more reliable than its 3D counterpart. Therefore, minimizing a reprojection loss is helpful to improve the accuracy of 3D pose estimation.</p><p>The multi-perspective loss, L mp , measures the consistency of the predicted 3D poses from different viewing angles. This loss indicates the reliability of the predicted 3D poses. Based on the two terms, our semi-supervised loss, L SSL , is formulated as,</p><formula xml:id="formula_9">L SSL = w(L rep + L mp ) + L dis ,<label>(11)</label></formula><p>where w is a weighting factor to balance the contribution of the reprojection and multi-perspective losses. In the train-ing stage, w first focuses on easy samples and gradually includes the hard samples. The weight, w, is formulated as:</p><formula xml:id="formula_10">w = softmax( E rep r ) + softmax( E mp r ),<label>(12)</label></formula><p>where r is the number of training epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Test Time Optimization</head><p>Unlike the semi-supervised training in Section 3.4, where the network is fine-tuned with the extra data. In this section, the purpose of test time optimization is to refine the estimated 3D poses at the inference time without permanently updating the network itself. As the training process focuses on fitting an optimal model only on the training set, and is not aware of the gap between training and testing data, test time optimization brings performance improvement by adopting the prediction from trained model to the testing data. The domain gap can be observed from both temporal and spatial perspectives. First, the speed of motion varies from video to video. In training data, the speed of motion is fixed and cannot cover all possible variations even with augmentations, which results in the gap from testing data. Since the TCN takes temporal information into consideration, the change of motion speed can drag down the accuracy of predicted 3D poses. Second, since there is a limited variation of bone lengths in the training data, the TCN learns a strong prior of fixed bone length. This will result in inaccurate predictions because the same 2D poses can correspond to different 3D poses with different bone lengths. To this end, we propose two strategies for inference stage optimization with two regularizations: trajectory, reprojection, and bonelength regularization.</p><p>First, we introduce the trajectory regularization to restrict the motion from unusual motions. Different motions follow a certain trajectory which can be expressed as a motion function f m (t). Taking a Taylor expansion of this function, we can approximate the motion function by a high-order polynomial. Our target is to predict the pose at the t th future frame given the poses in frames 0 to t ? 1, which is formulated as:</p><formula xml:id="formula_11">f m (t) = c 0 + c 1 t + c 2 t 2 + c 3 t 3 + ....<label>(13)</label></formula><p>As an approximation, we take the first 3 orders and approximate the coefficients c 0 , c 1 , c 2 , c 3 using linear regression. The temporal trajectory constraint is the mean squared error between the estimated pose P t and the trajectory f m (t) at frame t:</p><formula xml:id="formula_12">L traj = (P t ? f m (t)) 2 .<label>(14)</label></formula><p>Second, we propose to utilize the reprojection regularization to make the predicted 3D joints to be consistent with 2D joints. The same function as Eq. (10) is used as the reprojection loss. Third, as the bone length should be consistent for the same person across all frames in one video, we propose to use a bone length constraint to regularize the variation of each estimated 3D pose sequence. We first set up latent variables B 0 , B 1 , ... to represent the lengths of all bones in human skeleton, and the bone loss is represented as:</p><formula xml:id="formula_13">L bone = t i (B i,t ? B i,t ) 2 ,<label>(15)</label></formula><p>whereB i is the bone length of estimated poses. By minimizing the equation above, we concurrently minimize the variance of bone lengths across the whole video, as well as the latent variables. In summary, the loss function for the test time optimization is:</p><formula xml:id="formula_14">L T T O = L traj + c rep L rep + c bone L bone ,<label>(16)</label></formula><p>where c rep and c bone are the coefficients for reprojection and bone losses. Empirically, we observe that using the twostage training strategy with different coefficients between different losses will result in better convergence of the testtime optimization process. First, we train with c rep = 0.1 and c bone = 1 with 3000 iterations. Then, we increase the c rep to 100 to restrict more on the reprojection consistency for 3000 more iterations. We refer to the first step as "onestage" and to the whole process as "two-stage" optimization strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>We use MuPoTS-3D <ref type="bibr" target="#b55">[56]</ref> and JTA <ref type="bibr" target="#b83">[84]</ref> datasets to evaluate the camera-centric 3D multi-person pose estimation performance by following the existing methods <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b53">[54]</ref> and their training protocols (i.e., train, test split). In addition, we use 3DPW <ref type="bibr" target="#b84">[85]</ref> to evaluate person-centric 3D multiperson pose estimation performance following <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b86">[87]</ref>. We also perform evaluation on the widely used Human3.6M dataset <ref type="bibr" target="#b64">[65]</ref> for person-centric 3D human pose estimation following <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. MuPoTS-3D <ref type="bibr" target="#b55">[56]</ref> is a 3D multi-person testing set that consists of &gt;8000 frames of 5 indoor and 15 outdoor scenes, and its corresponding training set is augmented from 3DHP, called MuCo-3DHP. The ground-truth 3D pose of each person in a video is obtained from a multi-view markerless motion capture system, which is suitable for evaluating 3D multi-person pose estimation performance in both personcentric and camera-centric coordinates. Following <ref type="bibr" target="#b34">[35]</ref>, the training set (MuCo-3DHP) is used for training our bottomup network, and MuPoTS-3D is used only for performance evaluation.</p><p>JTA <ref type="bibr" target="#b83">[84]</ref> is a synthesized dataset from Grand Theft Auto V (GTA-V) game scene including various illumination, viewpoints, and occlusion. It is a multi-person dataset with at most 32 persons appearing in one frame. In addition, the images also demonstrate large person size variation as the crowd spread from close to the camera and far from the camera in various scenes. Because of these reasons, even though it is a synthetic dataset, we want to evaluate it. The dataset contains 512 videos, in which there are 256, 128, 128 for training, validation and testing, respectively. We follow the work <ref type="bibr" target="#b53">[54]</ref> to estimate the F1 score under different distance thresholds as a performance evaluation metric.</p><p>Human3.6M <ref type="bibr" target="#b64">[65]</ref> is widely used for 3D human pose estimation. The dataset contains 3.6 million single-person images where an actor performs different activities in mocap studio at each video clip, so it is suitable for evaluation of 3D single-person pose estimation. Therefore, Human3.6M is used for person-centric pose estimation evaluation to demonstrate the performance of the proposed method against other person-centric human pose estimation methods. Following previous works <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>, the subject 1,5,6,7,8 are used for training, and 9 and 11 for testing.</p><p>3DPW <ref type="bibr" target="#b84">[85]</ref> is an outdoor multi-person video dataset for 3D human pose reconstruction. Following previous methods <ref type="bibr" target="#b85">[86]</ref>, <ref type="bibr" target="#b86">[87]</ref>, we use 3DPW for testing without any fine-tuning. The ground-truth of 3DPW is SMPL 3D mesh model <ref type="bibr" target="#b87">[88]</ref>, where the definition of joints differs from what is used in 3D human pose estimation (skeleton-based) like Human3.6M, so 3DPW is rarely used in the evaluation of skeleton-based methods <ref type="bibr" target="#b88">[89]</ref>.</p><p>As evaluation on 3DPW cannot objectively reflect the performance of the skeleton-based methods, due to different definitions of joints. We select the top 3000 frames with the largest IoU between the target person (i.e., the person with 3D ground-truth label) and other persons based on detection out of 3DPW test set to create an inter-person occlusion subset, and then perform evaluation on it. The IoU statistics of the 3DPW test set is shown in <ref type="figure" target="#fig_6">Fig. 5</ref>, and the threshold at 3000 th frame is 0.26.</p><p>In fact, the error on this subset is still not a good performance indicator, the performance change of a method between the full testing set and this subset can measure how well the method can handle the inter-person occlusion problem. As shown in <ref type="table" target="#tab_1">Table 12</ref>, our method shows the smallest error increase among all the existing methods, which demonstrates that our method is indeed capable of handling inter-person occlusion more effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We use HRNet-w32 <ref type="bibr" target="#b8">[9]</ref> as the backbone network for both multi-person pose estimators in the top-down and bottomup networks. The top-down network is trained for 100 epochs on the COCO dataset <ref type="bibr" target="#b89">[90]</ref> with the Adam optimizer and learning rate 0.001. The bottom-up network is trained for 50 epochs with the Adam optimizer and learning rate 0.001 on a combined dataset of MuCO <ref type="bibr" target="#b90">[91]</ref> and COCO <ref type="bibr" target="#b89">[90]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Metrics</head><p>Since the majority of 3D human pose estimation methods produce person-centric 3D poses, to be able to compare, we  <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b34">[35]</ref>. Since we focus on 3D multi-person camera-centric pose estimation, we also use the metrics designed for evaluating performance in the camera coordinate system, including average precision of 3D human root location (AP root 25 ) and PCK abs , which is PCK without root alignment to evaluate the absolute camera-centric coordinates from <ref type="bibr" target="#b34">[35]</ref>, and F1 value following <ref type="bibr" target="#b53">[54]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Studies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of the Major Components</head><p>Ablation studies are performed to validate the effectiveness of each sub-module of our framework as in <ref type="table">Table 3</ref>. We validate our top-down network by using an existing top-down pose estimator (i.e., detection of one full-body joints) as a baseline, abbreviated as TD (w/o MP) to compare to our top-down network denoted as TD (w MP). We also validate our bottom-up network by using existing bottom-up heatmap estimation (i.e., estimate persons at the same scale) as a baseline, named BU (w/o CH) to compare to our bottom-up network, called BU (w CH). To evaluate our integration network, we use three baselines. The first is a straightforward integration by combining existing TD and BU networks, called TD + BU   last row of <ref type="table">Table 3</ref>, it is the result of our full model, which is TD + BU (IN) + SSL + TTO. As shown in <ref type="table">Table 3</ref>, we observe that our top-down network, bottom-up network, and integration network clearly outperform their corresponding baselines. Our top-down network tends to have better person-centric 3D pose estimation results compared with our bottom-up network, because the top-down network benefits from not only the multiperson pose estimator, but also the GCN and TCN that help to deal with inter-occluded poses. On the contrary, our bottom-up network achieves better performance for the root joint estimation, because it estimates the root depth based on a full image; while the root depth of the top-down network is estimated based on an individual skeleton. Our integration network demonstrates superior performance compared to hard or linear combining the poses from the top-down and bottom-up networks, which validates its effectiveness. Moreover, we observe that the proposed semi-supervised learning and test time optimization further improve the performance of 3D multi-person pose estimation. Semi-Supervised Learning Other than validating our topdown and bottom-up networks, we also perform ablation analysis on our semi-supervised learning. In <ref type="table" target="#tab_6">Table 4</ref>, we show the result of using reprojection loss, multi-perspective loss, reprojection loss with the proposed discriminator, and reprojection + multi-perspective loss with discriminator. We find that the reprojection loss is more useful than the multi-perspective loss because it leverages the information from the 2D pose estimator, which is trained with 2D human pose datasets with larger variations in human pose and appearance . More importantly, we observe that our proposed interaction-aware discriminator demonstrates the largest performance improvement compared with the other modules, proving the importance of enforcing the validity of the interaction between persons. Note that, since <ref type="table" target="#tab_6">Table 4</ref> is focused on evaluating the semi-supervised learning part, the test time optimization is not used in this experiment. Inter-Person Discriminator To validate our choice in the inter-person discriminator to check the interaction between two persons, we conduct parameter analysis in terms of the number of poses to check in the discriminator as illustrated   in <ref type="table">Table 5</ref>. Checking the pose validity for only one person (results of Dis1), which is what most existing methods do, shows improvement against baseline. However, when checking the validity of pair of poses (results of Dis2), we observe the accuracy improves in both person-centric and camara-centric pose estimation metrics. Further improving the number of poses to three does not help but negatively affect the performance. Thus, this analysis supports the use of the pair of poses in validity checking for the interperson discriminator. Note that, since <ref type="table">Table 5</ref> is focused on evaluating the inter-person discriminator part, the test time optimization is not used in this experiment. Test Time Optimization We also provide ablation study results of the test time optimization to understand the contribution of each loss term as shown in <ref type="table" target="#tab_8">Table 6</ref>. The baseline method in the table is the GCN network <ref type="bibr" target="#b37">[38]</ref> with 2D ground truth (w. GT) or 2D pose estimation <ref type="bibr" target="#b8">[9]</ref> (w/o GT). The first column in <ref type="table" target="#tab_8">Table 6</ref> shows the results of using the ground truth of 2D pose; the second column illustrates the results of using 2D estimator instead. In both cases, we observe that our TTO regularization is helpful to reduce the error on top of the baseline. In particular, it is observed that the reprojection regularization brings the largest error reduction compared with the baseline. By adding each additional regularization or optimization strategy, the error is further reduced, combining all the losses and strategies achieves the highest performance in terms of MPJPE metric, which validates the proposed test time optimization is helpful to refine the estimated 3D poses at the inference stage. Temporal Window Analysis Finally, we analyze the performance of different temporal window lengths for different motion trajectory order in our trajectory regularization on the Human3.6M dataset using ground-truth 3D poses as shown in <ref type="table" target="#tab_9">Table 7</ref>. It is observed that using temporal windows 2, 5, 5 for order 1, 2, 3 achieves the lowest error, which   are the parameters we choose for the following experiments. Same as before, since <ref type="table" target="#tab_9">Table 7</ref> is focused on evaluating the temporal window lengths, TTO is not used in this experiment. Runtime Analysis <ref type="table" target="#tab_11">Table 8</ref> shows the runtime of each component of the proposed method. We use HRNet-w32 <ref type="bibr" target="#b8">[9]</ref> as the backbone network for both multi-person pose estimators in the top-down and bottom-up networks. The input resolution for bottom-up network is 512 and 256 for multi-person pose estimator in top-down branch. For object detector, we use Faster RCNN <ref type="bibr" target="#b91">[92]</ref> with ResNeXt <ref type="bibr" target="#b92">[93]</ref> backbone. The temporal window length is set to 243 for TCN. For the TTO, we run both steps with 3000 iterations on the whole video and compute the runtime per frame. The runtime speed is tested on single RTX2080Ti GPU with I7-9900k CPU. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Quantitative Evaluation</head><formula xml:id="formula_15">Method t = 0.4m t = 0.8m t = 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 10</head><p>Quantitative results on JTA dataset. F1 values are reported based on different threshold t when the point is considered "true positive" when the distance from corresponding distance is less than t. Best in bold, second best underlined.</p><p>To evaluate the performance for 3D multi-person camera-centric pose estimation in both indoor and outdoor scenarios, we perform evaluations on MuPoTS-3D as summarized in <ref type="table" target="#tab_12">Table 9</ref>. The results show that our cameracentric multi-person 3D pose estimation outperforms the SOTA <ref type="bibr" target="#b35">[36]</ref> on P CK abs by 2.3%. We also perform personcentric 3D pose estimation evaluation using P CK where we outperform the SOTA method <ref type="bibr" target="#b36">[37]</ref> by 2.1%. The evaluation on MuPoTS-3D shows that our method outperforms the state-of-the-art methods in both camera-centric and personcentric 3D multi-person pose estimation as our framework overcomes the weaknesses of both bottom-up and top-down networks and at the same time benefits from their strengths. Note that, our method in <ref type="table" target="#tab_12">Table 9</ref> means our full model, including the test time optimization, same for the following evaluations in <ref type="table" target="#tab_1">Table 10</ref>, 11, 12. Following recent work <ref type="bibr" target="#b53">[54]</ref>, we also perform evaluations on JTA, which is a synthetic dataset acquired from computer games, to further validate the effectiveness of our method for camera-centric 3D multi-person pose estimation. As shown in <ref type="table" target="#tab_1">Table 10</ref>, our method is superior over the SOTA method <ref type="bibr" target="#b53">[54]</ref> (e.g., our result shows 12.6% improvement on F1 value, t = 0.4m) on this challenging dataset where both inter-person occlusion and large person scale variation present, which again illustrate that our proposed method can handle these challenges in 3D multi-person pose estimation.</p><p>Human3.6M is widely used for evaluating 3D singleperson pose estimation. As our method is focused on dealing with inter-person occlusion and scale variation, we do not expect our method to perform significantly better than the SOTA methods. <ref type="table" target="#tab_1">Table 11</ref> summarizes the quantitative evaluation on Human3.6M where our method is comparable with the SOTA methods <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b94">[95]</ref> on person-centric 3D human pose evaluation metrics (i.e., MPJPE and PA-MPJPE).</p><p>3DPW is an outdoor multi-person 3D human shape reconstruction dataset. It is unfair to compare the errors between skeleton-based methods with ground-truth defined on SMPL model <ref type="bibr" target="#b87">[88]</ref> due to the different definitions of joints <ref type="bibr" target="#b88">[89]</ref>. We run human detection on all frames and create an occlusion subset where the frames with the large overlay between persons are selected. The performance drop between the full testing test of 3DPW and the occlusion subset can effectively tell if a method can handle interperson occlusion, which is shown in <ref type="table" target="#tab_1">Table 12</ref>. We observe that our method shows the least performance drop from the testing set to the subset, which demonstrates our method is indeed more robust to inter-person occlusion.  from person scale variation where the person who is far from the camera is missing in frame 280 as well as interocclusion (e.g., frame 365 and 340). Our bottom-up network is robust to scale variance, but fragile to the out-of-image poses as our discriminator is not used here (e.g., frame 365 and 330). Moreover, our top-down network produces reasonable relative poses with the aid of GCN and TCNs. However, there exists an error of camera-centric root depth in our top-down network, because our top-down network estimates root depth based on individual 2D poses and lacks global awareness (e.g., frame 280). Finally, our full model benefits from both networks and produces the best 3D pose estimations among these baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Qualitative Evaluation</head><p>To further demonstrate the performance of our method compared with the SOTA 3D multi-person pose estimation methods, we provide additional qualitative results of our method compared with that of the SOTA bottom-up method SMAP <ref type="bibr" target="#b54">[55]</ref> and the SOTA top-down method RootNet <ref type="bibr" target="#b34">[35]</ref> on four video clips from MuPoTS dataset, as shown in <ref type="figure" target="#fig_0">Fig.  10</ref>. The errors of existing methods are highlighted in red circles or arrows, where we clearly observe our method outperforms the existing methods and can provide accurate camera-centric 3D multi-person pose estimation when interperson occlusions present in the videos. Moreover, as we reported our quantitative performance on JTA dataset in <ref type="table" target="#tab_1">Table 10</ref>, we also provide the qualitative results of our method compared with that of the SOTA method reported and released their trained model on the JTA dataset <ref type="bibr" target="#b53">[54]</ref> in <ref type="figure" target="#fig_0">Fig. 11</ref>. The two video clips in <ref type="figure" target="#fig_0">Fig. 11</ref> show both interperson occlusions and large multi-person scale variation where we observe our method can handle both challenges well and produce accurate camera-centric 3D multi-person pose estimation compared with LoCO <ref type="bibr" target="#b53">[54]</ref>.</p><p>We also provide results of the estimated 3D poses in novel viewpoints and the estimated 2D poses overlaid on input images as in <ref type="figure" target="#fig_9">Fig. 7</ref>. Our estimated camera-centric 3D poses visualized from different angles further validate the  effectiveness of the proposed method, where the depth of each person is estimated reasonably well from the results of different angles.</p><p>Qualitative comparisons of the proposed TTO module are shown in <ref type="figure">Fig. 9</ref>, where results of our full model with and without TTO on two wild videos are provided. In both video clips, we observe that the predicted 3D human poses without TTO are inaccurate (i.e., highlighted in red circles), which are affected by poor illumination or occlusion. In contrast, the results with TTO are improved, where the inaccuracies are fixed. Thus, these qualitative results demonstrate the effectiveness of the proposed TTO module on wild video. <ref type="figure" target="#fig_10">Fig. 8</ref> shows the failure cases where the images are from the PoseTrack dataset <ref type="bibr" target="#b43">[44]</ref>. There are two major kinds of failure cases of our method on the wild videos. First, the persons that are constantly heavily occluded in a video are likely to be missed by our method, as shown in the left column of <ref type="figure" target="#fig_10">Fig. 8</ref>. In such cases, the occluded person usually has only few joints with high confidence across frames, which makes it difficult to group them together to form a complete human pose. Thus, robustly dealing with incomplete or even extremely incomplete human poses is still a challenge to be solved in the future. A second kind of failure cases is the extreme human poses, as shown in the right column of <ref type="figure" target="#fig_10">Fig. 8</ref>. The extreme human poses are rare, which are usually not well represented in the training dataset. This can be viewed as an out-of-distribution problem, where the extreme human poses are out of the distribution of the training data (i.e., training dataset is dominated by regular or normal human poses.). As a result, both 2D and 3D pose estimation cannot handle these images well. Addressing this issue is another future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Failure Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We have proposed a novel method for monocular-video 3D multi-person pose estimation, which addresses the problems of inter-person occlusion and close interactions. We introduced the integration of top-down and bottomup approaches to exploit their strengths. A novel interperson pose discriminator is proposed to enforce the validity of human poses of close pairwise interactions. Semisupervised learning and test time optimization are proposed to further improve the accuracy of 3D multi-person pose estimation. In addition, we introduce unsupervised losses, i.e. the high-order temporal constraint, reprojection loss, and bone-length regularization.,which enable the optimization in test time. This optimization is critical in improving our performance, particularly in the cases where the gaps between the training and testing data are relatively significant. Our quantitative and qualitative evaluations show the effectiveness of our method compared to the state-of-the-art baselines. For each video clip, the first row is the frames from the video clip; the second row is the result of SMAP; the third row is the result of RootNet; the fourth row is the result of our method. It is observed from these results that the SOTA methods suffer from inter-person occlusions while our method can handle these challenges and produce accurate camera-centric 3D multi-person pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LoCO</head><p>Ours <ref type="figure" target="#fig_0">Fig. 11</ref>. Result of our method compared with that of LoCO <ref type="bibr" target="#b53">[54]</ref> (i.e., a SOTA method released trained model on JTA) on JTA dataset. Results from two video clips are included: top and bottom separated by the dashed line. For each video clip, the first row is the frames from the video clip; the second row is the result of LoCO; the third row is the result of our method. These results show that on this synthetic datasets, our method is able to produce more accurate and robust 3D multi-person pose estimation compared with other methods. We use red circle to indicate the wrong results of LoCO and green circle to point out the corresponding correct results of our method. In the first row of the top video clip, due the four persons are far from the camera which are small, we use four red arrows to indicate each of them. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bo</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Incorrect 3D multi-person pose estimation from existing top-down (2nd row) and bottom-up (3rd row) methods. The top-down method is RootNet</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>The overview of our framework. Our proposed method comprises four major components: 1) A top-down network to estimate fine-grained instance-wise 3D pose. 2) A bottom-up network to generate a global-aware camera-centric 3D pose. 3) An integration network to generate final estimation based on paired poses from top-down and bottom-up to take benefits from both networks. 4) A test time optimization process to refine the obtained integrated 3D poses for the final result. Note that the semi-supervised learning part is a training strategy so it is not included in this figure. neural network (CNN). Tang et al. [72] develop a framework with bottom-up inference followed by top-down refinement based on a compositional model of the human body. Cai et al. [73] introduce a spatial-temporal graph convolutional network (GCN) that uses both bottom-up and top-down features. These methods explore to benefit from top-down and bottom-up information. However, they are not suitable for 3D multi-person pose estimation because the fundamental weaknesses in both top-down and bottom-up methods are not addressed completely, which include inter-person occlusion caused detection and joints grouping errors, and the scale variation issue. Li et al. [74] adopt LSTM and combine bottom-up heatmaps with human detection for 2D multi-person pose estimation. They address occlusion and detection shift problems. Unfortunately, they use a bottom-up network and only add the detection bounding box as the top-down information to group the joints. Hence, their method is essentially still bottom-up and thus still vulnerable to human scale variations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2</head><label>2</label><figDesc>shows our pipeline, which consists of four major parts to accomplish the multi-person camera-centric 3D human pose estimation: a top-down network for fine-grained instance-wise pose estimation, a bottom-up network for global-aware pose estimation, an integration network to integrate the estimation results of the top-down and bottomup networks with inter-person pose discriminator, and a test time optimization process to refine and obtain the final 3D human poses. Moreover, a semi-supervised training process is proposed to enhance the 3D pose estimation based on reprojection consistency.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Examples of estimated heatmaps of human joints. The left image shows the input frame overlaid with an inaccurate detection bounding box (i.e., only one person detected). The middle image shows the estimated heatmap of existing top-down methods. The right image shows the heatmap of our top-down network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Sim i,j measures the similarity between the i th 3D pose P BU i from the bottom-up network and the j th 3D pose P T D j from the top-down network over K joints. Note that both poses from top-down P T D and bottom-up P BU are camera-centric; thus, the similarity is measured based on the camera coordinate system. The c BU i,k and c T D j,k are the confidence values of joint k for 3D poses P BU i and P T D j</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 4 .</head><label>4</label><figDesc>The illustration of our SSL pipeline. The SSL aims to keep two consistency: reprojection and multi-perspective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Interaction IoUs of 3DPW test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(w/o MP, CH). The second is hard integration, abbreviated TD + BU (hard), where the top-down person-centric pose is always used, plus the root depth from the bottom-up network. The third is linear integration, abbreviated TD + BU (linear), where the person-centric 3D pose from topdown is combined with its corresponding bottom-up one based on the confidence values of the estimated heatmap. Compared against these baselines, the proposed integration solution is abbreviated as TD + BU (IN), we also provide the result where pose matching is not used, abbreviated TD + BU (w/o PM) to show the effect of pose matching. To evaluate other major components in our method, we also provide the following baselines: TD + BU (IN) + SSL where semi-supervised learning is used; TD + BU (IN) + TTO where test time optimization is used but no SSL. In the Method AP root 25</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 6</head><label>6</label><figDesc>shows the comparison among a SOTA bottom-up method SMAP<ref type="bibr" target="#b54">[55]</ref>, our bottom-up network, top-down network, and full model. We observe that SMAP suffers</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Frame 0?90?45?F ig. 7 .</head><label>7</label><figDesc>Qualitative results of the estimated 2D poses overlaying on input images and the estimated 3D poses visualized in novel viewpoints (virtual camera rotated by 0, 45, 90 degrees clockwise). Different colors are used for different persons in both 2D and 3D human poses for better visualization purpose. Top frame from Posetrack dataset, bottom example from 3DPW dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 8 .</head><label>8</label><figDesc>Failure cases of our method. Example frames are from PoseTrack dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Wang received the PhD degree from School of Computing at the University of Utah in 2015. He is currently with CtrsVision. He was previously with GE Global Research and Tencent America. His research interests include 3D computer vision and machine learning with application to human pose estimation, spatial and temporal visual data modeling and analysis. Robby T. Tan received the PhD degree in computer science from the University of Tokyo. He is now an associate professor at both Yale-NUS College and ECE (Electrical and Computing Engineering), National University of Singapore. His research interests include computer vision and deep learning, particularly in the domains of low level vision (bad weather/nighttime, color analysis, physics-based vision, optical flow, etc.), human pose/motion analysis, and applications of deep learning in healthcare. He is a member of the IEEE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Yu Cheng and Robby T. Tan are with Yale-NUS College and the Department of Electrical and Computer Engineering, National University of Singapore, Singapore. E-mail: e0321276@u.nus.edu, robby.tan@nus.edu.sg ? Bo Wang is with CtrsVision (USA). E-mail: hawk.rsrch@gmail.com Manuscript received May 31, 2021.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1</head><label>1</label><figDesc>Summary of the top-down and bottom-up 2D/3D human pose estimation methods and the coordinate systems of the obtained human pose results.</figDesc><table><row><cell>Task</cell><cell>Coordinate system</cell><cell>Metric</cell><cell>Dataset</cell><cell>Publication</cell></row><row><cell></cell><cell></cell><cell>MPJPE,</cell><cell></cell><cell></cell></row><row><cell>3D human pose</cell><cell>Person-centric (relative coordinate)</cell><cell>PA-MPJPE, PCK,</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>AU C rel</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE 4</head><label>4</label><figDesc>Ablation study on MuPoTS-3D dataset. Rep, MP, and dis stand for reprojection, multi-perspective, and discriminator. Best in bold, second best underlined.</figDesc><table><row><cell>Method</cell><cell>AP root 25</cell><cell>AU C rel</cell><cell cols="2">PCK PCK abs</cell></row><row><cell>Baseline</cell><cell>46.3</cell><cell>43.4</cell><cell>77.2</cell><cell>40.7</cell></row><row><cell>Dis1</cell><cell>46.1</cell><cell>48.9</cell><cell>87.3</cell><cell>46.1</cell></row><row><cell>Dis2</cell><cell>46.3</cell><cell>49.9</cell><cell>89.1</cell><cell>46.8</cell></row><row><cell>Dis3</cell><cell>46.3</cell><cell>49.7</cell><cell>89.0</cell><cell>46.7</cell></row><row><cell></cell><cell></cell><cell>TABLE 5</cell><cell></cell><cell></cell></row><row><cell cols="5">Parameter analysis of different number of poses to check in the</cell></row><row><cell cols="5">inter-person discriminator on MuPoTS-3D dataset. Dis1, Dis2, and Dis3</cell></row><row><cell cols="5">stand for checking one, two, and three poses in the inter-person</cell></row><row><cell></cell><cell cols="3">discriminator. Best in bold.</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE 6</head><label>6</label><figDesc>Ablation study of test time optimization on Human3.6M dataset. MPJPE (w. GT) means 2D ground truth is used, MPJPE (w/o GT) means 2D pose estimation is used instead of the ground truth. Best in bold, second best underlined.</figDesc><table><row><cell cols="4">Order 1 Order 2 Order 3 MPJPE</cell></row><row><cell>2</cell><cell>0</cell><cell>0</cell><cell>9.18</cell></row><row><cell>3</cell><cell>0</cell><cell>0</cell><cell>9.24</cell></row><row><cell>5</cell><cell>0</cell><cell>0</cell><cell>10.51</cell></row><row><cell>2</cell><cell>3</cell><cell>3</cell><cell>5.88</cell></row><row><cell>2</cell><cell>5</cell><cell>5</cell><cell>5.84</cell></row><row><cell>2</cell><cell>7</cell><cell>7</cell><cell>5.93</cell></row><row><cell>3</cell><cell>3</cell><cell>3</cell><cell>5.94</cell></row><row><cell>3</cell><cell>5</cell><cell>5</cell><cell>7.21</cell></row><row><cell>3</cell><cell>7</cell><cell>7</cell><cell>7.71</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>TABLE 7</head><label>7</label><figDesc>Ablation study on the number of temporal window lengths for different motion trajectory order using ground-truth 3D poses on Human3.6M</figDesc><table /><note>dataset. The error is measured by MPJPE. Best in bold.s</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>TABLE 8</head><label>8</label><figDesc>Runtime of each component.</figDesc><table><row><cell>Group</cell><cell>Method</cell><cell cols="2">PCK PCK abs</cell></row><row><cell></cell><cell>Mehta et al. [56]</cell><cell>65.0</cell><cell>n/a</cell></row><row><cell>Person-</cell><cell>Rogez et al., [32]</cell><cell>70.6</cell><cell>n/a</cell></row><row><cell>centric</cell><cell>Cheng et al. [28]</cell><cell>74.6</cell><cell>n/a</cell></row><row><cell></cell><cell>Cheng et al. [31]</cell><cell>80.5</cell><cell>n/a</cell></row><row><cell></cell><cell>Moon et al. [35]</cell><cell>82.5</cell><cell>31.8</cell></row><row><cell>Camera-</cell><cell>Lin et al. [37]</cell><cell>83.7</cell><cell>35.2</cell></row><row><cell>centric</cell><cell>Zhen et al. [55]</cell><cell>80.5</cell><cell>38.7</cell></row><row><cell></cell><cell>Li et al. [36]</cell><cell>82.0</cell><cell>43.8</cell></row><row><cell></cell><cell>Cheng et al. [38]</cell><cell>87.5</cell><cell>45.7</cell></row><row><cell></cell><cell>Our method</cell><cell>89.6</cell><cell>48.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>TABLE 9</head><label>9</label><figDesc></figDesc><table><row><cell>Quantitative evaluation on multi-person 3D dataset, MuPoTS-3D. Best</cell></row><row><cell>in bold, second best underlined.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>TABLE 12</head><label>12</label><figDesc>Examples of results from our whole framework compared with different baseline results on two video clips on 3DPW dataset. First row shows the images from two video clips; second row shows the results from SMAP [55]; third row shows the result of of our bottom-up (BU) network; fourth row shows the results of our top-down (TD) network; last row shows the results of our full model. Wrong estimations are labeled with red circles. Quantitative evaluation using PA-MPJPE on original 3DPW test set and its occlusion subset. * denotes extra 3D datasets were used in training.Best in bold, second best underlined.</figDesc><table><row><cell cols="2">Frame 240</cell><cell>Frame 280</cell><cell>Frame 300</cell><cell>Frame 365</cell><cell>Frame 305</cell><cell>Frame 315</cell><cell>Frame 330</cell><cell>Frame 340</cell></row><row><cell>Input</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SMAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BU</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>TD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Full</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fig. 6. Dataset</cell><cell></cell><cell>Method</cell><cell>PA-MPJPE</cell><cell>?</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Doersch et al. [64]</cell><cell>74.7</cell><cell>n/a</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Kanazawa et al. [86]</cell><cell>72.6</cell><cell>n/a</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Original</cell><cell></cell><cell>Arnab et al. [30]</cell><cell>72.2</cell><cell>n/a</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Cheng et al. [31]</cell><cell>71.8</cell><cell>n/a</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Sun et al. [87]</cell><cell>69.5</cell><cell>n/a</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Kolotouros et al. [95]*</cell><cell>59.2</cell><cell>n/a</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Kocabas et al., [96]*</cell><cell>51.9</cell><cell>n/a</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Our method</cell><cell>61.7</cell><cell>n/a</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Cheng et al. [31]</cell><cell>92.3</cell><cell>+20.5</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Sun et al. [87]</cell><cell>84.4</cell><cell>+14.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Subset</cell><cell cols="2">Kolotouros et al. [95]*</cell><cell>79.1</cell><cell>+19.9</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Kocabas et al., [96]*</cell><cell>72.2</cell><cell>+20.3</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Our method</cell><cell>72.4</cell><cell>+9.5</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>Fig. 9. Qualitative results of TTO module. Major improvements of the TTO module are highlighted in red circles. Video clips are from PoseTrack dataset.Fig. 10. Results of our method compared with that of SMAP [55] (i.e., the SOTA bottom-up method) and RootNet [35] (i.e., the SOTA top-down method) on MuPoTS dataset. Results from four video clips are included: top-left, top-right, bottom-left, and bottom-right.</figDesc><table><row><cell>Frame 0</cell><cell>Frame 50</cell><cell>Frame 100</cell><cell>Frame 150</cell><cell>Frame 30</cell><cell>Frame 45</cell><cell>Frame 60</cell><cell>Frame 80</cell></row><row><cell>Frames</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>W/o TTO</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>With TTO</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Frame 100</cell><cell>Frame 120</cell><cell>Frame 380</cell><cell>Frame 400</cell><cell>Frame 300</cell><cell>Frame 365</cell><cell>Frame 330</cell><cell>Frame 340</cell></row><row><cell>SMAP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>RootNet</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ours</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>This research is supported by the National Research Foundation, Singapore under its Strategic Capability Research Centres Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2961" to="2969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4903" to="4911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A cascaded inception of inception network with attention modulated feature fusion for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Integral human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="529" to="545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7103" to="7112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rmpe: Regional multiperson pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2334" to="2343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A coarse-fine network for keypoint localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3028" to="3037" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Simple baselines for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="466" to="481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Crowdpose: Efficient crowded scenes pose estimation and a new benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4966" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2640" to="2649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 international conference on 3D vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="506" to="516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Towards 3d human pose estimation in the wild: a weakly-supervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="398" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vnect: Realtime 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3d human pose estimation= 2d pose estimation+ matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7035" to="7043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generating multiple hypotheses for 3d human pose estimation with mixture density network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9887" to="9895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by predicting depth on joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3467" to="3475" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cascaded deep monocular 3d human pose estimation with evolutionary training data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Pratama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-T</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6173" to="6183" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Locally connected network for monocular 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">3d human pose estimation in the wild by adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5255" to="5264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ordinal depth supervision for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7307" to="7316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploiting temporal information for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R I</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="69" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">3d human pose estimation in video with temporal convolutions and semisupervised training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pavllo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7753" to="7762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Repnet: Weakly supervised training of an adversarial reprojection network for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7782" to="7791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">3d human pose estimation with 2d marginal heatmaps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nibali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prendergast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1477" to="1485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Occlusionaware networks for 3d human pose estimation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="723" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation by generation and ordinal ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Varigonda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bindal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2325" to="2334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Exploiting temporal context for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3395" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3d human pose estimation using spatio-temporal networks with explicit occlusion training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="8118" to="8125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Lcr-net++: Multiperson 2d and 3d pose detection in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1146" to="1161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Monoloco: Monocular 3d pedestrian localization and uncertainty estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Xnect: Real-time multi-person 3d motion capture with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elgharib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="82" to="83" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Camera distance-aware topdown approach for 3d multi-person pose estimation from a single rgb image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Moon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hmor: Hierarchical multi-person ordinal relations for monocular multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Hdnet: Human depth estimation for multiperson camera-space localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Graph and temporal convolutional networks for 3d multi-person pose estimation in monocular videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4929" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="34" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7291" to="7299" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Associative embedding: Endto-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2277" to="2287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Arttrack: Articulated multi-person tracking in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Levinkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6457" to="6465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Posetrack: Joint multi-person pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2011" to="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Personlab: Person pose estimation and instance segmentation with a bottom-up, part-based, geometric embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="269" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Openpose: Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hidalgo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Multi-person articulated tracking with spatial and temporal embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5664" to="5673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Pifpaf: Composite fields for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11" to="977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Higherhrnet: Scale-aware representation learning for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5386" to="5395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Differentiable hierarchical graph grouping for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="718" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Bottom-up human pose estimation via disentangled keypoint regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14" to="676" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Rethinking the heatmap regression for bottom-up human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="13" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Pandanet: Anchor-based single-shot multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Benzine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chabot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Luvison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">C</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Achard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6856" to="6865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Compressed volumetric heatmaps for multi-person 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lanzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Alletto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7204" to="7213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Smap: Single-shot multi-person absolute 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Conference on 3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="120" to="130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Single-stage multi-person pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6951" to="6960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">A review of 3d human pose estimation algorithms for markerless motion capture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Desmarais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mottet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Slangen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Montesinos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06449</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multi-task deep learning for real-time 3d human pose estimation and action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luvizon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Picard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Vr content creation and exploration with deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-Q</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F.-L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Visual Media</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="28" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Mvor: A multi-view rgb-d operating room dataset for 2d and 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srivastav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Issenhuth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadkhodamohammadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Mathelin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Padoy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.08180</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Game plan: What ai can do for football, and what football can do for ai</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tuyls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Omidshafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hennes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Spearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Waskett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09192</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Monocular 3d multiperson pose estimation by integrating top-down and bottom-up networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Sim2real transfer learning for 3d human pose estimation: motion to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="12" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Hu-man3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Lcr-net: Localizationclassification-regression for human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3433" to="3441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Learning to estimate human pose with data driven belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="747" to="754" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Combined topdown/bottom-up human articulated pose estimation using adaboost learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 20th International Conference on Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3670" to="3673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Integration of bottom-up/topdown approaches for 2d pose estimation using probabilistic gaussian modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Makris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Nebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="242" to="255" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down reasoning with hierarchical rectified gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5600" to="5609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Deeply learned compositional models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="190" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Exploiting spatial-temporal relationships for 3d pose estimation via graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Thalmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2272" to="2281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation using bounding box constraint and lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2653" to="2663" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Inference stage optimization for cross-scenario 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<pubPlace>NeurIPS</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Multi-scale networks for 3d human poseestimation with inference stage optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.06844</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Physcap: Physically plausible monocular 3d motion capture in real time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shimada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golyanik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">A-nerf: Surface-free human 3d pose refinement via neural rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zollhoefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.06199</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Self-supervised keypoint correspondences for multi-person pose estimation and tracking in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Umer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doering</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval research logistics quarterly</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="83" to="97" />
			<date type="published" when="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Unsupervised 3d pose estimation with geometric self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Drover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5714" to="5724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<monogr>
		<title level="m" type="main">3d human shape and pose from a single low-resolution image with self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Jeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2007.13666</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Self-training with noisy student improves imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Learning to detect and track visible and occluded body joints in a virtual world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabbri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lanzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calderara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Palazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vezzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="430" to="446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Recovering accurate 3D human pose in the wild using IMUs and a moving camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Marcard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Henschel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="614" to="631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Learning 3d human dynamics from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Felsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Human mesh recovery from monocular images via a skeleton-disentangled representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5349" to="5358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM transactions on graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<monogr>
		<title level="m" type="main">Posenet3d: Unsupervised 3d human shape and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ranade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tyagi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03473</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Single-shot multi-person 3d pose estimation from monocular rgb</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
		<ptr target="http://gvv.mpi-inf.mpg.de/projects/SingleShotMultiPerson" />
	</analytic>
	<monogr>
		<title level="m">Sixth International Conference on</title>
		<imprint>
			<date type="published" when="2018-09" />
		</imprint>
	</monogr>
	<note>3D Vision (3DV)</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="91" to="99" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05431</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<monogr>
		<title level="m" type="main">Yolov3: An incremental improvement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.02767</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Learning to reconstruct 3d human pose and shape via model-fitting in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kolotouros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2252" to="2261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Vibe: Video inference for human body pose and shape estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kocabas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Athanasiou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5253" to="5263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">He is now a Ph.D. candidate in ECE (Electrical and Computing Engineering) in National University of Singapore. His research interests include human pose estimation, facial recognition and object detection</title>
		<imprint>
			<date type="published" when="2018" />
			<pubPlace>Singapore</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Yu Cheng received the bachelor&apos;s degree in Electrical and Electronic Engineering from Nanyang Technological University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
