<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Automatic Natural Image Matting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jizhizi</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Zhang</surname></persName>
							<email>jing.zhang1@sydney.edu.au</email>
							<affiliation key="aff0">
								<orgName type="institution">The University of Sydney</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
							<email>dacheng.tao@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="department">JD Explore Academy</orgName>
								<address>
									<region>JD.com</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Automatic Natural Image Matting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T07:57+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic image matting (AIM) refers to estimating the soft foreground from an arbitrary natural image without any auxiliary input like trimap, which is useful for image editing. Prior methods try to learn semantic features to aid the matting process while being limited to images with salient opaque foregrounds such as humans and animals. In this paper, we investigate the difficulties when extending them to natural images with salient transparent/meticulous foregrounds or non-salient foregrounds. To address the problem, a novel endto-end matting network is proposed, which can predict a generalized trimap for any image of the above types as a unified semantic representation. Simultaneously, the learned semantic features guide the matting network to focus on the transition areas via an attention mechanism. We also construct a test set AIM-500 that contains 500 diverse natural images covering all types along with manually labeled alpha mattes, making it feasible to benchmark the generalization ability of AIM models. Results of the experiments demonstrate that our network trained on available composite matting datasets outperforms existing methods both objectively and subjectively. The source code and dataset are available at https://github.com/JizhiziLi/AIM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural image matting refers to estimating a soft foreground from a natural image, which is a fundamental process for many applications, e.g., film post-production and image editing <ref type="bibr" target="#b1">[Chen et al., 2013;</ref>. Since image matting is a highly ill-posed problem, previous methods usually adopt auxiliary user input, e.g. trimap <ref type="bibr" target="#b6">[Sun et al., 2004;</ref><ref type="bibr" target="#b0">Cai et al., 2019]</ref>, scribble <ref type="bibr" target="#b3">[Levin et al., 2007]</ref>, or background image <ref type="bibr" target="#b5">[Sengupta et al., 2020]</ref> as constraints. While traditional methods estimate the alpha value by sampling neighboring pixels <ref type="bibr" target="#b6">[Wang and Cohen, 2007]</ref> or defining affinity metrics for alpha propagation <ref type="bibr" target="#b3">[Levin et al., 2008]</ref>, deep learning-based approaches solve it by learning discriminative representations from a large amount of labeled data and predicting alpha matte directly <ref type="bibr" target="#b4">Li and Lu, 2020]</ref>. However, extra manual effort is required while generating such auxiliary inputs, which makes these methods impractical in automatic industrial applications. To address the limitations, automatic image matting (AIM) has attracted increasing attention recently , which refers to automatically extracting the soft foreground from an arbitrary natural image. Prior AIM methods  solve this problem by learning semantic features from the image to aid the matting process but is limited to images with salient opaque foregrounds, e.g., human <ref type="bibr">[Shen et al., 2016;</ref>, animal . It is difficult to extend them to images with salient transparent/meticulous foregrounds or non-salient foregrounds due to their limited semantic representation ability.</p><p>To address this issue, we dig into the problem of AIM by investigating the difficulties of extending it to all types of natural images. First, we propose to divide natural images into three types according to the characteristics of foreground alpha matte. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, the three types are: 1) SO (Salient Opaque): images that have salient foregrounds with opaque interiors, e.g. human, animal; 2) STM (Salient Transparent/Meticulous): images that have salient foregrounds with transparent or meticulous interiors, e.g. glass, plastic bag; and 3) NS (Non-Salient): images with non-salient foregrounds, e.g. smoke, grid, raindrop. Some examples are shown in <ref type="figure" target="#fig_0">Figure 1</ref>. Then, we systematically analyze the ability of baseline matting models for each type of image in terms of understanding global semantics and local matting details. We find that existing methods usually learn implicit semantic features or use an explicit semantic representation that is defined for a specific type of image, i.e., SO. Consequently, they are inefficient to handle different types of images with various characteristics in foreground alpha mattes, e.g., salient opaque/transparent and non-salient foregrounds.</p><p>In this paper, we make the first attempt to address the problem by devising a novel automatic end-to-end matting network for all types of natural images. First, we define a simple but effective unified semantic representation for the above three types by generalizing the traditional trimap according to the characteristics of foreground alpha matte. Then, we build our model upon the recently proposed effective GFM model  with customized designs. Specifically, 1) we use the generalized trimap as the semantic representation in the semantic decoder to adapt it for all types of images; 2) we exploit the effective SE attention  in the semantic decoder to learn better semantic features to handle different characteristics of foreground alpha mattes; 3) we improve the interaction between the semantic decoder and matting decoder by devising a spatial attention module, which guides the matting decoder to focus on the details in the transition areas. These customized designs prove to be effective for AIM rather than trivial tinkering, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>Besides, there is not a test bed for evaluating AIM models on different types of natural images. Previous methods use composition images by pasting different foregrounds on background images from COCO dataset  for evaluation, which may introduce composition artifacts and are not representative for natural images as mentioned in . Some recent works collect natural test images with manually labeled alpha mattes, however, they are limited to specific types of images, such as portrait images <ref type="bibr" target="#b3">[Ke et al., 2020]</ref> or animal images , which are not suitable for comprehensive evaluation of matting models for AIM. To fill this gap, we establish a benchmark AIM-500 by collecting 500 diverse natural images covering all three types and many categories and manually label their alpha mattes.</p><p>The main contributions of this paper are threefold: 1) we make the first attempt to investigate the difficulties of automatic natural image matting for all types of natural images; 2) we propose a new matting network with customized designs upon a reference architecture that are effective for AIM on different types of images; and 3) we establish the first natural images matting benchmark AIM-500 by collecting 500 natural images covering all three types and manually labeling their alpha mattes, which can serve as a test bed to facilitate future research on AIM.</p><p>2 Rethinking the Difficulties of AIM Matting without auxiliary inputs. Prevalent image matting methods <ref type="bibr" target="#b6">[Sun et al., 2021;</ref><ref type="bibr">Cho et al., 2016]</ref> solve the problem by leveraging auxiliary user inputs, e.g. trimap <ref type="bibr" target="#b6">[Tang et al., 2019]</ref>, scribble <ref type="bibr" target="#b3">[Levin et al., 2007]</ref>, or background image <ref type="bibr" target="#b5">[Sengupta et al., 2020]</ref>, which provide strong constraints on the solution space. Specifically, given a trimap, the matting models only need to focus on the transition area and distinguish the details by leveraging the available foreground and background alpha matte information. However, there is usually little chance to obtain auxiliary information in real-world automatic application scenarios. Thereby, AIM is more challenging since the matting models need to understand the holistic semantic partition of foreground and background of an image, which may belong to different types as been described previously. Nevertheless, AIM is more appealing to automatic applications and worth more research efforts.</p><p>Matting on natural images. Since it is difficult to label accurate alpha mattes for natural images at scale, there are no publicly available large-scale natural image matting datasets. Usually, foreground images are obtained by leveraging chroma keying from images captured with a green background screen . Nevertheless, the amount of available foregrounds is only about 1,000. To build more training images, they are synthesized with different background images from public datasets like <ref type="bibr">COCO [Lin et al., 2014]</ref>. However, synthetic images contain composite artifacts and semantic ambiguity . Matting models trained on them may have a tendency to find cheap features from these composite artifacts and thus overfit on the synthetic training images, resulting in a poor generalization performance on real-world natural images. To address this domain gap issue between synthetic training images and natural test images, some efforts have been made in <ref type="bibr" target="#b2">Hou and Liu, 2019]</ref>. As for evaluation, previous methods  also adopt synthetic dataset  for evaluating AIM models, which is a bias evaluation setting. For example, a composite image may contain multiple objects including the original foreground object in the candidate background image, but the ground truth is only the single foreground object from the foreground image. Besides, the synthetic test images may also contain composite artifacts, making it less possible to reveal the overfitting issue.</p><p>Matting on all types of images. AIM aims to extract the soft foreground from an arbitrary natural image, which may have foreground objects with either opaque interior, transparent/meticulous interior, or non-salient foregrounds like texture, fog, or water drops. However, existing AIM methods are limited to a specific type of images with opaque foregrounds, e.g., human <ref type="bibr">[Shen et al., 2016;</ref><ref type="bibr" target="#b4">Li et al., 2021;</ref><ref type="bibr" target="#b3">Ke et al., 2020]</ref> and animals . <ref type="bibr">DAPM [Shen et al., 2016]</ref> first generates a coarse foreground shape mask and uses it as an auxiliary input for following matting process. LateFusion  predicts foreground and background separately and uses them in a subsequent fusion process. <ref type="bibr">HAttMat-ting [Qiao et al., 2020]</ref> predicts foreground profile and guides the matting process to refine the precise boundary. GFM  predicts foreground, background, and transition area simultaneously and combines them with the matting result at the transition area as the final matte. Although they are effective for images with salient and opaque foregrounds, extending them to all types of images is not straightforward. In the context of AIM, the term "semantic" is more related to foreground and background rather than the semantic category of foreground objects. Since there are very different characteristics for the three types of images, it is hard to learn useful semantic features to recognize foreground and background, especially for images with transparent objects or non-salient foregrounds, without explicit supervisory signals.</p><p>3 Proposed Methods</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Improved Backbone for Matting</head><p>In this work, we choose <ref type="bibr">ResNet-34 [He et al., 2016]</ref> as the backbone network due to its lightweight and strong representation ability. However, ResNet-34 is originally designed to solve high-level classification problem, while both highlevel semantic features and low-level detail features should be learned to solve AIM. To this end, we improve the vanilla ResNet-34 with a simple customized modification to make it better suitable for the AIM task.</p><p>In the first convolutional layer conv1 in ResNet-34, they use a stride of 2 to reduce the spatial dimension of feature maps. Followed by a max-pooling layer, the output dimension is 1/4 of the original image after the first block. While these two layers can significantly reduce computations and increase the receptive field, it may also lose many details, which are not ideal for image matting. To customize the backbone for AIM, we modify the stride of conv1 from 2 to 1 to keep the spatial dimension of the feature map as the original image size to retain local detail features. To retain the receptive field, we add two max-pooling layers with a stride of 2. Besides, we change the stride of all the first convolutional layers in stage1-stage4 of ResNet-34 from 2 to 1 and add a max-pooling layer with a stride of 2 accordingly. It is noteworthy that the indices in the max-pooling layers are also kept and will be used in the corresponding max-unpooling layers in the local matting decoder to preserve local details, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. We retrain the customized ResNet-34 on ImageNet and use it as our backbone network. Experiment results demonstrate it outperforms the vanilla one in terms of both objective metrics and subjective visual quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Unified Semantic Representation</head><p>As discussed in Section 2, the characteristics of different types of images are very different. In order to provide explicit semantic supervisory signals for the semantic decoder to learn useful semantic features and partition the image into foreground, background, and transition areas, we propose a unified semantic representation.</p><p>For an image belonging to SO type, there are always foreground, background, and transition areas. Thereby, we adopt the traditional trimap as the semantic representation, which can be generated by erosion and dilation from the ground truth alpha matte. For an image belonging to STM type, there are no explicit foreground areas. In other words, the foreground should be marked as a transition area and a soft foreground alpha matte should be estimated. Thereby, we use a duomap as its semantic representation, which is a 2-class map denoting the background and transition areas accordingly. For an image belonging to NS type, it is hard to mark all the explicit foreground and background areas, since the foreground is always entangled with the background. Thereby, we use a unimap as its semantic representation, which is a 1-class map denoting the whole image as the transition area.</p><p>To use a unified semantic representation for all the three types of images, we derive the trimap, duomap, and unimap from the traditional trimap as follows:</p><formula xml:id="formula_0">? ? ? U i = T i , type = SO U i = 1.5T i ? T 2 i , type = ST M U i = 0.5, type = N S ,<label>(1)</label></formula><p>where T represents the traditional trimap obtained by erosion and dilation from the ground truth alpha matte. For every pixel i, T i ? {0, 0.5, 1}, where the background area is 0, the foreground area is 1, and the transition area is 0.5, respectively. U i is the unified semantic representation at pixel i. Note that for images belonging to type STM and NS, the traditional trimaps always contain trivial foreground and background pixels as shown in <ref type="figure" target="#fig_1">Figure 2</ref>(b), which are very difficult for the semantic decoder to predict. Instead, our unified representations are a pure duomap or unimap as shown in <ref type="figure" target="#fig_1">Figure 2(c)</ref>, which represents the holistic foreground objects or denotes there are no salient foreground objects. In order to predict the unified semantic representation, we redesign the semantic decoder in GFM  as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. Specifically, we also use five blocks in the decoder. For each decoder block, there are three sequential 3 ? 3 convolutional layers and an upsampling layer. To further increase the capability of the decoder for learning discriminative semantic features, we adopt the Squeeze-and-Excitation (SE) attention module  after each decoder block to re-calibrate the features, thereby selecting the most informative features for predicting unified representations and filtering out the less useful ones. We also adopt pyramid pooling module (PPM)  to enlarge the receptive field. The upsampled PPM features are concatenated with the output of SE module and used as the input for the next decoder block. We use the cross-entropy loss to supervise the training of the semantic decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Guided Matting Process</head><p>As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, there are six blocks following a U-Net structure <ref type="bibr">[Ronneberger et al., 2015]</ref> in matting decoder.. Each block contains three sequential 3 ? 3 convolutional layers. The encoder feature is concatenated with first decoder block output and fed into the following block. The output then being concatenated with the corresponding encoder output and passes a max unpooling layer with reversed indices to recover fine structural details, serves as input of next block.</p><p>Motivated by previous study proving that attention mechanism can provide support on learning discriminating representations , we devise a spatial attention module to guide the matting process by leveraging the learned semantic features from the semantic decoder to focus on extracting details only within transition area. Specifically, the output feature from the last decoder block in the semantic decoder is used to generate spatial attention, since it is more related to semantics. Then, it goes through a max-pooling layer and average pooling layer along the channel axis, respectively. The pooled features are concatenated and go through a convolutional layer and a sigmoid layer to generate a spatial attention map. We use it to guide the matting decoder to attend to the transition area via an element-wise production operation and an element-wise sum operation.</p><p>Given the predicted unified representation U and guided matting result M from the semantic decoder and matting decoder, we can derive the final alpha matte ? as follows:</p><formula xml:id="formula_1">? = (1 ? 2 ? |U ? 0.5|) ? M + 2 ? |U ? 0.5| ? U. (2)</formula><p>We adopt the commonly used alpha loss  and Laplacian loss <ref type="bibr" target="#b2">[Hou and Liu, 2019]</ref> on the predicted alpha matte M and the final alpha matte ? to supervise the matting decoder. Besides, we also use the composition loss  on the final alpha matte ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Benchmark: Automatic Image Matting-500</head><p>As discussed in Section 2, previous work  evaluate their models either on synthetic test set such as Comp-1k  or in-house test set of natural images limited to specific types, e.g., human and animal images . In this paper, we establish the first natural image matting test set AIM-500, which contains 500 high-resolution real-world natural images from all three types and many categories. We collect the images from free-license websites and manually label the alpha mattes with professional software. The shorter side of each image is at least 1080 pixels. In <ref type="table" target="#tab_0">Table 1</ref>, we compare AIM-500 with other matting test sets including DAPM <ref type="bibr">[Shen et al., 2016]</ref>, Comp-1k , HAtt , and AM-2k  in terms of the volume, whether or not do they provide natural original images, the amount of three types images, and the object classes. We can see that AIM-500 is larger and more diverse than others, making it suitable for benchmarking AIM models. AIM-500 contains 100 portrait images, 200 animal images, 34 images  with transparent objects, 75 plant images, 45 furniture images, 36 toy images, and 10 fruit images. We present some examples and their alpha mattes in <ref type="figure" target="#fig_3">Figure 4</ref>. Note that due to the privacy concern, all the portrait images have no identifiable information and are ready to release.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We trained our model and other representative matting models on the combination of matting datasets Comp-1k , HAtt [Qiao et al., 2020] and AM-2k . To reduce the domain gap issue, we adopted the highresolution background dataset BG-20k and the composition route RSSN proposed in  to generate the training data. We composited each foreground image from Composition-1k and HAtt with five background images and each foreground image from AM-2k with two background images. The total number of training images is 8,710. To achieve better performance, we also adopted a type-wise data augmentation and transfer learning strategy during training.</p><p>Type-wise data augmentation. After inspecting realworld natural images, we observed that NS images usually have a bokeh effect on the background. To simulate this effect, for all NS images, we added the blur effect on the background as RSSN to make the foreground prominent.</p><p>Transfer learning. Although data augmentation could increase the number of training data, the number of original foreground images and their classes are indeed small. To mitigate the issue, we leveraged the salient object detection dataset DUTS  for training since it contains more real-world images and classes. However, since images in DUTS have small resolutions (about 300?400) and little fine details, we only used it for pre-training and adopted a transfer learning strategy to finetune the pre-trained model further on the above synthetic matting dataset. We trained our model on a single NVIDIA Tesla V100 GPU with batch size as 16, Adam as optimizer. When pretraining on DUTS, we resized all images to 320 ? 320, set learning rate as 1?10 ?4 , trained for 100 epochs. During finetuning on the synthetic matting dataset, we randomly crop a patch with a size in {640?640, 960?960, 1280?1280} from each image and resized it to 320 ? 320, set the learning rate as 1 ? 10 ?6 , and trained for 50 epochs. It took about 1.5 days to train the model. We adopted the hybrid test strategy  with the scale factors 1/3 and 1/4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Objective and Subjective Results</head><p>We compare our model with several state-of-the-art matting methods including SHM , <ref type="bibr">LF [Qiao et al., 2020]</ref>, <ref type="bibr">HAtt [Qiao et al., 2020]</ref>, GFM , DIM , and an salient object detection method <ref type="bibr">U2NET [Qin et al., 2020]</ref> on AIM-500. For LF, U2NET, and GFM, we used the code provided by the author. For SHM, HAtt, and DIM, we re-implemented the code since they are not available. Since DIM required a trimap for training, we used the ground truth trimap as the auxiliary input and denoted it as DIM * . We adopted the same transfer learning strategy while training all the models. We chose the commonly used sum of absolute differences (SAD), mean squared error (MSE), Mean Absolute Difference (MAD), Connectivity(Conn.), Gradient(Grad.) <ref type="bibr" target="#b5">[Rhemann et al., 2009]</ref>, as the main metrics, and also calculated the SAD within transition areas, SAD by type and category for comprehensive evaluation. The results of objective metrics are summarized in Table 2. Some visual results are presented in <ref type="figure" target="#fig_8">Figure 5</ref> 1 .</p><p>As shown in <ref type="table" target="#tab_2">Table 2</ref>, our model achieves the best in all metrics among all AIM methods and outperforms DIM * in most of the metrics. Although DIM * performs better in some SO category, e.g., animal, human, furniture, it indeed uses ground truth trimaps as auxiliary inputs while our model does not have such a requirement. Nevertheless, our model still outperforms DIM * in the transition areas as well as on the STM and NS types, implying that the semantic decoder predicts accurate semantic representation and the matting decoder has a better ability for extracting alpha details. Besides, DIM is very sensitive to the size of trimap and may produce a bad result when there is a large transition area, e.g. boundary of the bear in <ref type="figure" target="#fig_8">Figure 5</ref>. U2NET can estimate the rough foregrounds for SO images, but it fails to handle STM and NS images, e.g. the crystal stone and the net, implying that a single decoder is overwhelmed to learn both global semantic features and local detail features to deal with all types of images since they have different characteristics.</p><p>SHM uses a two-stage network to predict trimap and the final alpha, which may accumulate the semantic error and mislead the subsequent matting process. Consequently, it obtains large SAD errors in the whole image as well as in the transition area, i.e., 170.44 and 69.41. Some failure results can be found in the bear and crystal stone in <ref type="figure" target="#fig_8">Figure 5</ref>. LF adopts a classification network to distinguish foreground and background, but it is difficult to adapt to STM and NS images as they do not have explicit foreground and background, thereby resulting in large average SAD errors. HATT tries to learn foreground profile to support boundary detail matting. However, there are no explicit supervisory signal for the semantic branch, which makes it difficult to learn explicit semantic representations. As a strong baseline model, GFM outperforms other AIM methods, but it is still worse than us, i.e., 52.66 to 43.92, especially for STM and NS images, as seen from the crystal stone and the net. The results demonstrate that the customized designs in the network matter a lot for dealing   different types of images. Generally, our method achieves the best performance both objectively and subjectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We present the ablation study results in <ref type="table" target="#tab_3">Table 3</ref>. There are several findings. Firstly, the proposed unified semantic representation is more effective than the traditional trimap, which helps to reduce the SAD from 81.08 to 76.90. Secondly, the transfer learning strategy is very effective by leveraging the large scale DUTS dataset for pre-training, reducing the SAD from 75.90 to 52.66. Thirdly, all the customized designs including max-pooling in the backbone, SE attention module in the semantic decoder, and spatial attention based on semantic features to guide the matting decoder, are useful and complementary to each other. For example, our method that uses all these designs reduces SAD from 52.66 to 43.92. It provides explicit and non-trivial supervisory signals for the semantic decoder, facilitating it to learn effective semantic features. It is noteworthy that although our model without MP has lower MSE, however, the visual results of the alpha matte obtained by our model with MP is better, e.g., with clearer details. Thereby, we choose it as our final model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we investigate the difficulties in automatic image matting, including matting without auxiliary inputs, matting on natural images, and matting on all types of images. We propose a unified semantic representation for all types by introducing the new concepts of duomap and unimap, proved to be useful. We devise an automatic matting network with several customized new designs to improve its capability for AIM. Moreover, we establish the first natural image test set AIM-500 to benchmark AIM models, which can serve as a test bed to facilitate future research.</p><p>In this supplementary material, we present the network structure detail of our model, more details about the proposed natural image matting test set AIM-500, more visual results of our model and state-of-the-art methods on AIM-500, and more analysis of the ablation study.</p><p>1 More details about our model 1.1 Network structure <ref type="table" target="#tab_2">Table 2</ref> shows the detailed structure of some essential parts in our network including the shared encoder, the semantic decoder based on the unified semantic representation, and the guided matting decoder. Our network has 55.3M parameters and 191.52GMac computational complexity. It takes about 0.1633s for the network to process a 800?800 image.</p><p>2 More details about AIM-500</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Compare with synthetic validation set</head><p>As we mentioned in the paper, due to the unavailability of natural test with labeled alpha mattes, previous AIM methods evaluate their results on the synthetic test set, which is generated by pasting high-resolution foregrounds from matting dataset  on backgrounds from low-resolution COCO dataset  or PAS-CAL VOC <ref type="bibr" target="#b8">[Everingham et al., 2010]</ref>.</p><p>However, as can be seen from <ref type="figure" target="#fig_1">Figure 2</ref>, such procedure results in large bias with real-world natural images and wrong label because of the two reasons, 1) Pasting other salient objects on backgrounds will make the original ground truth unreasonable, for examples, pasting a transparent cup in front of another cup or pasting a squirrel in front of a lot of people. Such cases appear a lot in the synthetic matting test set and result in the biased evaluation of AIM models; and 2) Synthetic images have non-negligible composite artifacts, performing well on such a test set does not necessarily imply a good generalization ability on real-world images.</p><p>In contrast to them, the proposed natural image matting test set named AIM-500 contains natural images rather than composite images with manually labeled alpha mattes as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. We believe AIM-500 can serve as a good test bed to benchmark AIM models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">More examples of AIM-500</head><p>AIM-500 contains 7 categories including portrait, animal, fruit, furniture, toy, plant, and transparent objects. Here we show more examples of each category in <ref type="figure" target="#fig_0">Figure 1</ref>. As can be seen, images in AIM-500 have diverse foregrounds and backgrounds. 3 More results of Experiment 3.1 More visual results on AIM-500</p><p>In <ref type="figure" target="#fig_3">Figure 4</ref> and <ref type="figure" target="#fig_8">Figure 5</ref>, we compare our model with several state-of-the-art methods <ref type="bibr">U2NET [Qin et al., 2020]</ref>, SHM , <ref type="bibr">LF [Zhang et al., 2019]</ref>, GFM , and DIM+Trimap  on AIM-500. It can be seen that our method achieves the best results on all three types of images. U2Net and GFM fail to extract the fine detail. SHM and LF have many errors on semantic background or foreground. Even with extra ground truth trimap as input, DIM performs worse than us especially when the trimap region is large, e.g., SO images.</p><p>3.2 More of ablation study SE attention block We adopted five Squeeze-and-Excitation attention modules  in the semantic decoder to arXiv:2107.07235v1 [cs.CV] 15 Jul 2021 re-calibrate the features and select the most informative features for predicting unified representations. Here we provide objective and subjective results of it. In <ref type="table" target="#tab_0">Table 1</ref>, we show the results of IOU (Intersection over Union) and the accuracy of the predicted unified semantic representations by the semantic decoder with or without SE blocks. We also show some visual results in <ref type="figure" target="#fig_2">Figure 3</ref>. As can be seen, for all three types of images, SE blocks benefit the semantic decoder to predict better unified semantic representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SE Blocks</head><p>IOU Accuracy ? 0.7002 0.8756 0.7393 0.8900 <ref type="table" target="#tab_0">Table 1</ref>: IOU and accuracy of the predicted unified semantic representations with or without SE blocks in the semantic decoder.     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Matting results of GFM and our method on three types of natural images, i.e., SO, STM, NS, from the top to the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>(a) Three types of images. (b) The traditional trimap representation. (c) The unified semantic representations, i.e., trimap, duomap, and unimap, respectively. White: foreground, Black: background, Gray: transition area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>The structure of our matting network for AIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Some examples from our AIM-500 benchmark.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>More examples from the proposed AIM-500.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 2 :</head><label>2</label><figDesc>Comparison between our proposed natural images matting test set AIM-500 and previous synthetic matting test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 :</head><label>3</label><figDesc>Visual results of the predicted unified semantic representations with or without SE blocks in the semantic decoder.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Additional visual results on AIM-500.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Additional visual results on AIM-500.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc></figDesc><table><row><cell>Dataset DAPM Comp-1k HAtt AM-2k AIM-500</cell><cell>Volume Natural SO STM NS 200 ? 200 0 0 50 ? 28 17 5 50 ? 30 11 9 200 ? 200 0 0 500 ? 424 43 33</cell><cell>Class Portrait Mixed Mixed Animal Mixed</cell></row></table><note>: Comparison between AIM-500 with other matting test set.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>.0921 0.1012 170.67 115.29 69.41 154.56 204.67 329.9 229.71 174.65 141.49 333.24 157.24 166.81 126.04 97.31 170.97 LF 191.74 0.0667 0.1130 181.26 63.51 78.13 177.98 220.22 331.34 243.18 167.90 131.96 276.13 228.94 249.70 224.50 287.40 223.79 HATT 479.17 0.2700 0.2806 473.98 238.63 114.23 509.75 338.11 270.07 372.64 579.96 484.85 264.35 433.96 299.19 447.01 401.73 415.</figDesc><table><row><cell>Whole Image MAD U2NET 83.46 0.0348 0.0493 82.14 SAD MSE Conn. SHM 170.44 086 Tran. SAD-Type SAD-Category Grad. SAD SO STM NS Avg. Animal Human Transp. Plant Furni. Toy Fruit Avg. 51.02 43.37 69.69 120.59 211.98 134.09 67.67 89.50 210.34 75.72 87.20 54.64 52.24 91.04 GFM 52.66 0.0213 0.0313 52.69 46.11 37.43 35.45 123.15 181.90 113.50 28.18 27.61 190.50 75.77 80.94 51.42 27.87 68.90 DIM  *  49.27 0.0147 0.0293 47.10 29.30 49.27 19.51 115.42 345.33 160.09 16.41 15.10 273.96 95.60 43.06 34.67 17.00 70.83 Ours 43.92 0.0161 0.0262 43.18 33.05 30.74 31.80 94.02 134.31 86.71 26.39 24.68 148.68 54.03 62.70 53.15 37.17 58.11</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Quantitative results on AIM-500. DIM * denotes the DIM method using ground truth trimap as an extra input. Tran.: Transition Area, Transp.: Transparent, Furni.: Furniture.Figure 5: Some visual results of different methods on AIM-500. More results can be found in the supplementary material.</figDesc><table><row><cell>UNI TL MP SE SA SAD 81.08 0.0363 0.0480 MSE MAD 76.90 0.0296 0.0456 52.66 0.0213 0.0313 51.23 0.0205 0.0307 48.52 0.0195 0.0287 48.95 0.0207 0.0293 44.50 0.0158 0.0262 43.92 0.0161 0.0262</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation study results. UNI: unified semantic representations; TL: transfer learning; MP: backbone with max pooling; SE: SE attention; SA: spatial attention.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>? 64 ? 320 ? 320 conv (7 ? 7, 64, stride 1, padding 3) + BN + ReLU M0 N ? 64 ? 160 ? 160 maxpool (3 ? 3, stride 2, padding 1, return indices) M1 N ? 64 ? 80 ? 80 maxpool (3 ? 3, stride 2, padding 1, return indices) E1 N ? 64 ? 80 ? 80 ResNet-34 [He et al., 2016] layer 1 (stride 1) M2 N ? 64 ? 40 ? 40 maxpool (3 ? 3, stride 2, padding 1, return indices) E2 N ? 128 ? 40 ? 40 ResNet-34 [He et al., 2016] layer 2 (stride 1)</figDesc><table><row><cell>Block</cell><cell>Output size</cell><cell>Improved Backbone</cell><cell>Detail</cell></row><row><cell cols="4">E0 N M3 N ? 128 ? 20 ? 20 E3 N ? 256 ? 20 ? 20 M4 N ? 256 ? 10 ? 10 E4 N ? 512 ? 10 ? 10 P P M N ? 512 ? 10 ? 10 SD4 N ? 256 ? 20 ? 20 SE4 N ? 256 ? 20 ? 20 SD3 N ? 128 ? 40 ? 40 SE3 N ? 128 ? 40 ? 40 SD2 N ? 64 ? 80 ? 80 SE2 N ? 64 ? 80 ? 80 SD1 N ? 64 ? 160 ? 160 Unified Semantic Decoder maxpool (3 ? 3, stride 2, padding 1, return indices) ResNet-34 [He et al., 2016] layer 3 (stride 1) maxpool (3 ? 3, stride 2, padding 1, return indices) ResNet-34 [He et al., 2016] layer 4 (stride 1) PSPModule [Zhao et al., 2017] (512, multiscale=1,3,5) conv (3 ? 3, 512, stride 1) + BN + ReLU conv (3 ? 3, 512, stride 1) + BN + ReLU conv (3 ? 3, 256, stride 1) + BN + ReLU upsample(2) avgpool + linear(256, 16) + ReLU + linear(16, 256) + Sigmoid conv (3 ? 3, 256, stride 1) + BN + ReLU conv (3 ? 3, 256, stride 1) + BN + ReLU conv (3 ? 3, 128, stride 1) + BN + ReLU upsample(2) avgpool + linear(128, 8) + ReLU + linear(8, 128) + Sigmoid conv (3 ? 3, 128, stride 1) + BN + ReLU conv (3 ? 3, 128, stride 1) + BN + ReLU conv (3 ? 3, 64, stride 1) + BN + ReLU upsample(2) avgpool + linear(64, 4) + ReLU + linear(4, 64) + Sigmoid conv (3 ? 3, 64, stride 1) + BN + ReLU conv (3 ? 3, 64, stride 1) + BN + ReLU conv (3 ? 3, 64, stride 1) + BN + ReLU upsample(2) SE1 N ? 64 ? 160 ? 160 avgpool + linear(64, 4) + ReLU + linear(4, 64) + Sigmoid SD0 N ? 64 ? 320 ? 320 conv (3 ? 3, 64, stride 1) + BN + ReLU conv (3 ? 3, 64, stride 1) + BN + ReLU upsample(2) SP A N ? 1 ? 320 ? 320 maxpool + avgpool + conv (7 ? 7, 1, padding 3) SE0 N ? 64 ? 320 ? 320 avgpool + linear(64, 4) + ReLU + linear(4, 64) + Sigmoid SD ? f inal N ? 3 ? 320 ? 320 conv (3 ? 3, 3, stride 1, padding 1) Semantic Guided Matting Decoder M D5 N ? 512 ? 10 ? 10 conv (3 ? 3, 512, dilation 2, padding 2) + BN + ReLU conv (3 ? 3, 512, dilation 2, padding 2) + BN + ReLU conv (3 ? 3, 512, dilation 2, padding 2) + BN + ReLU M D4 N ? 256 ? 10 ? 10 conv (3 ? 3, 512, stride 1) + BN + ReLU conv (3 ? 3, 512, stride 1) + BN + ReLU conv (3 ? 3, 256, stride 1) + BN + ReLU M U4 N ? 256 ? 20 ? 20 max-unpool (2 ? 2, stride 2) M D3 N ? 128 ? 20 ? 20 conv (3 ? 3, 256, stride 1) + BN + ReLU conv (3 ? 3, 256, stride 1) + BN + ReLU conv (3 ? 3, 128, stride 1) + BN + ReLU M U3 N ? 128 ? 40 ? 40 max-unpool (2 ? 2, stride 2) M D2 N ? 64 ? 40 ? 40 conv (3 ? 3, 128, stride 1) + BN + ReLU conv (3 ? 3, 128, stride 1) + BN + ReLU conv (3 ? 3, 64, stride 1) + BN + ReLU M U2 N ? 64 ? 80 ? 80 max-unpool (2 ? 2, stride 2) M D1 N ? 64 ? 80 ? 80 conv (3 ? 3, 64, stride 1) + BN + ReLU conv (3 ? 3, 64, stride 1) + BN + ReLU conv (3 ? 3, 64, stride 1) + BN + ReLU M U1 N ? 64 ? 160 ? 160 max-unpool (2 ? 2, stride 2) M U0 N ? 64 ? 320 ? 320 max-unpool (2 ? 2, stride 2) M D0 N ? 64 ? 320 ? 320 conv (3 ? 3, 64, stride 1) + BN + ReLU conv (3 ? 3, 64, stride 1) + BN + ReLU SP AR N ? 64 ? 320 ? 320 SPA + SPA ?M D0 M D ? f inal N ? 1 ? 320 ? 320 conv (3 ? 3, 3, stride 1, padding 1) Matting Fusion</cell></row><row><cell>M F</cell><cell></cell><cell></cell><cell></cell></row></table><note>N ? 1 ? 320 ? 320 pixel-wise sum and multiplication.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table /><note>The network structure of our matting model. The input size is N ? 3 ? 320 ? 320, where N is the batch size.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">More results can be found in the supplementary material.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The University of Sydney, Sydney, Australia 2 JD Explore Academy,Beijing, China  jili8515@uni.sydney.edu.au, jing.zhang1@sydney.edu.au, dacheng.tao@gmail.com   </note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Disentangled image matting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Natural image matting using deep convolutional neural networks</title>
	</analytic>
	<monogr>
		<title level="m">Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition</title>
		<editor>Yu-Wing Tai, and Inso Kweon</editor>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Context-aware image matting for simultaneous foreground and alpha estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">; Qiqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Is a green screen really necessary for real-time portrait matting? ArXiv, abs</title>
	</analytic>
	<monogr>
		<title level="m">Alex Rav-Acha, and Dani Lischinski. Spectral matting. IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1699" to="1712" />
		</imprint>
	</monogr>
	<note>IEEE Transactions on Pattern Analysis and Machine Intelligence</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Natural image matting via guided contextual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaoyi</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">;</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.16188</idno>
		<idno>arXiv:2104.14222</idno>
	</analytic>
	<monogr>
		<title level="m">Privacy-preserving portrait matting</title>
		<editor>Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll?r, and C Lawrence Zitnick. Microsoft</editor>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>ICCV</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">U2-net: Going deeper with nested ustructure for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computerassisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note>ECCV</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learningbased sampling for natural image matting</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>Wang and Cohen</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="315" to="321" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Jing Zhang and Dacheng Tao. Empowering things with intelligence: A survey of the progress, challenges, and opportunities in artificial intelligence of things</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge. International journal of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Attention-guided hierarchical structure aggregation for image matting</title>
		<idno type="arXiv">arXiv:2010.16188</idno>
	</analytic>
	<monogr>
		<title level="m">Xuebin Qin, Zichen Zhang, Chenyang Huang, Masood Dehghan, Osmar R Zaiane, and Martin Jagersand. U2-net: Going deeper with nested ustructure for salient object detection. Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page">107404</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>CVPR</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A late fusion cnn for digital matting</title>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>CVPR</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
