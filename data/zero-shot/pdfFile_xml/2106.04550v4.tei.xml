<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DETReg: Unsupervised Pretraining with Region Priors for Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Bar</surname></persName>
							<email>amir.bar@cs.tau.ac.il</email>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
							<affiliation key="aff4">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vadim</forename><surname>Kantorov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colorado</forename><forename type="middle">J</forename><surname>Reed</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Berkeley AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Bar-Ilan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Berkeley AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Berkeley AI Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tel-Aviv University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DETReg: Unsupervised Pretraining with Region Priors for Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent self-supervised pretraining methods for object detection largely focus on pretraining the backbone of the object detector, neglecting key parts of detection architecture. Instead, we introduce DETReg, a new self-supervised method that pretrains the entire object detection network, including the object localization and embedding components. During pretraining, DETReg predicts object localizations to match the localizations from an unsupervised region proposal generator and simultaneously aligns the corresponding feature embeddings with embeddings from a self-supervised image encoder. We implement DETReg using the DETR family of detectors and show that it improves over competitive baselines when finetuned on COCO, PASCAL VOC, and Airbus Ship benchmarks. In low-data regimes, including semi-supervised and few-shot learning settings, DETReg establishes many state-of-the-art results, e.g., on COCO we see a +6.0 AP improvement for 10-shot detection and over 2 AP improvements when training with only 1% of the labels. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Object detection is a key task in computer vision, yet it largely relies on the availability of human-annotated training datasets. Building such datasets is not only costly but sometimes infeasible for privacy-sensitive applications such as medical imaging or personal photos <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b67">68]</ref>. Fortunately, recent advancements in self-supervised representation learning have substantially reduced the amount of labeled data needed for a variety of applications, including object detection <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Despite this recent progress, current approaches are limited in their ability to learn good representations for object detection because they do not pretrain the entire object detection network, specifically the localization and region embedding components. Most recent works (e.g., <ref type="bibr" target="#b0">1</ref> Code: https://www.amirbar.net/detreg/. <ref type="figure">Figure 1</ref>. Top class-agnostic object detections after pretraining. Self-supervised pretraining methods, such as SwAV <ref type="bibr" target="#b5">[6]</ref>, pretrain only the detector's backbone, so the object localizations following the pretraining stage solely depend on the random initialization of the localization components (green). UP-DETR <ref type="bibr" target="#b15">[16]</ref> pretrains the entire detection network, but since its pretraining operates by re-identifying random regions, it does not specialize in localizing objects after the pretraining (orange dashes). Our model, DETReg, pretrains the entire detection network using object-centric pretraining, and following the pretraining stage can localize objects (blue).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SwAV Backbone Only</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DETReg (ours) Object-Centric</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>UP-DETR Random Region</head><p>SwAV <ref type="bibr" target="#b5">[6]</ref>, ReSim <ref type="bibr" target="#b61">[62]</ref>, InsLoc <ref type="bibr" target="#b66">[67]</ref>) follow the same pretraining playbook for the detection network as a supervised image-classification-based pretraining, where only the CNN backbone can be initialized from the pretrained model. While the recent UP-DETR <ref type="bibr" target="#b15">[16]</ref> method pretrains a full detection architecture, it still does not localize objects within the image, but rather random image regions.</p><p>In this work, we present a model for Detection with Transformers using Region priors (DETReg), which unlike existing pretraining methods, learns to both localize and encode objects simultaneously in the unsupervised pretraining stage -see <ref type="figure">Figure 1</ref>. DETReg involves two object-centric and category-agnostic pretraining tasks: an Object Localization Task to localize objects, and an Object Embedding Task to encode an object's visual properties. Taken together, these tasks pretrain the entire detection network -see <ref type="figure">Figure 2</ref> for an overview. A final object classification head can then be finetuned with a small number of labels yielding better performance than existing methods.</p><p>DETReg's object localization task uses simple region proposal methods for class-agnostic bounding-box supervision <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b53">54]</ref>. These methods require little or no training data and can produce region proposals at a high recall. For example, Selective Search <ref type="bibr" target="#b53">[54]</ref>, the region proposal method we adopt in DETReg, uses object cues such as continuity in color, hierarchy, and edges to extract object proposals. DETReg builds upon these region priors to learn a class-agnostic detector during pretraining.</p><p>DETReg's object embedding task aims to predict the embeddings of a separate self-supervised image encoder evaluated on object regions. Self-supervised image encoders, e.g., SwAV <ref type="bibr" target="#b5">[6]</ref>, learn transformation-invariant embeddings, so training the detector to predict these values distills the learned invariances into the detector's embeddings. Thus, the object embedding head learns representations that are robust to transformations such as translation or image cropping.</p><p>We conduct an extensive evaluation of DETReg on standard object detection benchmarks like MS COCO <ref type="bibr" target="#b40">[41]</ref> and PASCAL VOC <ref type="bibr" target="#b17">[18]</ref>, and on an aerial images dataset, Airbus Ship Detection <ref type="bibr" target="#b0">[1]</ref>. We find that DETReg improves the performance using two state-of-the-art base architectures compared to challenging baselines, especially when small amounts of annotated data are available.</p><p>Quantitatively, DETReg improves over a backbone-only image-classification pretraining baseline by 4 AP points on PASCAL VOC, 1.6 AP points on MS COCO, and 1.2 AP points on Airbus Ship Detection. Additionally, DETReg outperforms pretraining baselines in semi-supervised learning when using 1% to 10% of data, and on 10 and 30 shot. Taken together, these results indicate that pretraining an entire detection network, including region proposal prediction and embedding components, is beneficial and that our specific DETReg model realizes new SOTA performance by taking advantage of this object-centric self-supervised pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Self-supervised pretraining. Recent work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b42">43]</ref> has shown that self-supervised pretraining can generate powerful representations for transfer learning, even outperforming its supervised counterparts on challenging vision benchmarks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b60">61]</ref>. The learned representations transfer well to image classification but the improvement is less significant for instance-level tasks, such as object detection and instance segmentation <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b69">70]</ref>. More recently, a number of works <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b61">62,</ref><ref type="bibr" target="#b63">64]</ref> focused on learning backbones that can transfer to object detection. In contrast to these works, we pretrain the entire detection network. As we show, pretraining the backbone with an image-patch-based task does not necessarily empower the model to learn what and where an object is, and adding weak supervision from the region priors proves beneficial.</p><p>Our approach is also different from semi-supervised object detection approaches <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b64">65]</ref> and few-shot detection approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b68">69]</ref> as we initialize the detector from a pretrained DETReg model without further modifying the architecture. Therefore, these approaches can be viewed as complementary to DETReg.</p><p>End-to-end object detection. Detection with transformers (DETR) <ref type="bibr" target="#b4">[5]</ref> builds the first fully end-to-end object detector and eliminates the need for components such as anchor generation and non-maximum suppression (NMS) postprocessing. This model has quickly gained traction in the machine vision community. However, the original DETR suffers from slow convergence and limited sample efficiency. Deformable DETR <ref type="bibr" target="#b70">[71]</ref> introduced a deformable attention module to attend to a sparsely sampled small set of prominent key elements, and achieved better performance compared to DETR with reduced training epochs. We therefore use Deformable DETR as our base detection architecture.</p><p>Both DETR and Deformable DETR adopt the supervised pretrained backbone (ResNet <ref type="bibr" target="#b29">[30]</ref>) on ImageNet. UP-DETR <ref type="bibr" target="#b15">[16]</ref> pretrains DETR in a self-supervised way by detecting and reconstructing the random patches from the input image. Instead, we additionally adopt region priors from unsupervised region proposal algorithms to provide weak supervision for pretraining, which has an explicit notion of objects rather than the random patches used by UP-DETR.</p><p>Region proposals. A rich study of region proposals methods exists in the object detection literature <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b71">72]</ref>. Grouping based method, Selective Search <ref type="bibr" target="#b54">[55]</ref>, and window scoring based approach, Objectness <ref type="bibr" target="#b1">[2]</ref> are two early and well known proposal methods, which have been widely adopted and supported in major software libraries (e.g., OpenCV <ref type="bibr" target="#b3">[4]</ref>). Selective Search greedily merges superpixels to generate proposals. Objectness relies on visual cues such as multi-scale saliency, color contrast, edge density and superpixel straddling to identify likely regions.</p><p>While the field has largely shifted to learning-based approaches, the key benefit of these models is that they require little or no training data, and can produce region proposals at a high recall <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b53">54]</ref>. This provides a cheap, albeit noisy, source of supervision. Hosang et al. <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref> offer a comprehensive analysis over the various region proposals methods, and Selective Search is among the top performing approaches in terms of recall. Here, we seek weak supervision from the region proposals generated by Selective Search, which has been widely adopted and proven successful in the well-known detectors such as R-CNN <ref type="bibr" target="#b23">[24]</ref> and Fast R-CNN <ref type="bibr" target="#b22">[23]</ref>. However, our approach is not limited to Selective Search and can employ other proposal methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">DETReg</head><p>DETReg is a self-supervised method to fully pretrain object detectors, including their region localization and embedding components. At a high level, DETReg operates by predicting object localizations that match those from an unsupervised region proposal generator, while simultaneously aligning the corresponding feature embeddings with embeddings from a self-supervised image encoder, see <ref type="figure">Figure 2</ref>.</p><p>The key idea underlying DETReg is to formulate pretext tasks that are similar to the tasks performed during supervised object detection, so that improved pretraining transfers to the object detector. We built DETReg based on the DETR family of detectors <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b70">71]</ref> due to their implementation simplicity and performance, though other architectures can easily plug into DETReg. Next, we review DETR, and in the following subsections, we present the object localization and embedding pretext tasks that form the core of DETReg.</p><p>DETR summary: DETR detects up to N objects in an image by iteratively applying attention and feed-forward layers over N object query vectors of a transformer decoder and over the input image features. The last layer of the decoder results in N image-dependent query embeddings that are used to predict bounding box coordinates and object categories. Formally, consider an input image x ? R H?W ?3 . DETR uses x to calculate N image-dependent query embeddings v 1 , . . . , v N with v i ? R d . This is achieved by passing the image through a backbone, followed by a transformer, and processing of the query vectors <ref type="bibr" target="#b4">[5]</ref>. Then, two prediction heads are applied to v i . The first, f box : R d ? R 4 , predicts the bounding boxes. The second, f cat : R d ? R L , outputs a distribution over L object categories, including a background "no object" category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Object Localization Task</head><p>DETReg's object localization pretraining task uses simple region proposal methods for class-agnostic bounding-box supervision (see the orange arrows in <ref type="figure">Figure 2</ref>). We use the output from these methods as they require limited or no training data and can produce region proposals at a high recall <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b53">54]</ref>. We use Selective Search <ref type="bibr" target="#b53">[54]</ref> as the primary region proposal method for training DETReg as it is widely available in off-the-shelf computer vision libraries and requires no training data. Selective Search uses object cues such as continuity in color and edges to extract object proposals, and DETReg further builds upon these region priors to learn a class-agnostic detector.</p><p>Region proposal methods take an image and produce a large set of region proposals at a high recall rate, where some of the regions are likely to contain objects. However, they have low precision and do not output category information, see <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. Since the content of non-object boxes tends to be more variable than of object boxes, we expect that deep models can be trained to recognize the visual properties of objects even when given noisy labels.</p><p>Thus, the Object Localization pretraining task takes a set of M boxes b 1 , . . . , b M (where b i ? R 4 ) output by an unsupervised region proposal method and optimizes a loss that minimizes the difference between the detector box predictions (the output of the f box MLP) and these M boxes. Similar to DETR, the loss involves matching the predicted boxes and these M boxes, a process we detail in Section 3.3.</p><p>Common region proposal methods attempt to sort the region proposals such that proposals that are more likely to be objects appear first, however, the number of proposals is typically large, and the ranking is not precise. Therefore, we explore methods to choose the best regions to use during training. We consider three policies for selecting boxes: Top-K uses the top-K proposals from the algorithm. Random-K uses K random proposals, which may yield lower quality proposals but encourages exploration. Importance Sampling relies on the region proposal method ranking but also encourages more diverse proposals. Formally, let b 1 , . . . , b n be a set of n sorted region proposals, where the b i has rank i. Let X i be a random variable indicating whether we will output the b i . We assign the sampling probability for X i to be: P r(X i = 1) ? ?log(i/n). To determine if a box should be included, we randomly sample from its respective distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Object Embedding Task</head><p>In the supervised training of object detectors, every box is associated with a class category of the object, which is not available in an unsupervised setting. Therefore, to learn a strong object embedding, we encode each box region b i via a separate encoder network and obtain embeddings z i that are used as a target for the DETReg embeddings? i (see the black arrows in <ref type="figure">Figure 2</ref>).</p><p>The separate encoding network that produces z i could be jointly trained by following similar bootstrapping techniques from works such as BYOL <ref type="bibr" target="#b25">[26]</ref> or DINO <ref type="bibr" target="#b6">[7]</ref>. However, for training stability and to reduce the convergence time, we leverage a pretrained, self-supervised model whose embeddings are invariant to many image transformations, e.g. blurring and color distortions. Here we primarily use a SwAV <ref type="bibr" target="#b5">[6]</ref> pretrained model as it is one of the strongest performing methods for pretraining image classifiers and has readily available code and pretrained models.</p><p>To predict a corresponding object embedding? i in the detector, we introduce an additional MLP f emb : R d ? R d that predicts the object embedding? i from the corresponding DETR query embedding, v i . This encourages v i to capture the information that is useful for category prediction. The loss is the L 1 loss between? i and z i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">DETReg Pretraining</head><p>Here, we formally describe how DETReg optimizes the localization and embedding tasks during pretraining. Assume that our region proposal method returns M object proposals which are used to generate M bounding boxes b i and object descriptors z i for i ? {1, . . . , M }, and let</p><formula xml:id="formula_0">y i = (b i , z i ) with y = {y i } M i=1 .</formula><p>DETReg is trained such that its N outputs align with y.</p><p>Let v 1 , . . . , v K denote the image-dependent query em-beddings calculated by DETR (i.e., the output of the last layer of the DETR decoder). DETReg has three prediction heads: f box which outputs predicted bounding boxes, f cat which predicts if the box is object or background, and f emb which reconstructs the object embedding descriptor. Denote these outputs as</p><formula xml:id="formula_1">:b i = f box (v i ),? i = f emb (v i ),p i = f cat (v i ), and define? i = (b i ,? i ,p i ) and? = {? i } N i=1</formula><p>. Following DETR training, we assume that the number of DETR queries N is larger than M , so we pad y to obtain N tuples, and assign a label c i ? {0, 1} to each box in y to indicate whether it is a region proposal (c i = 1) or padded proposal (c i = 0); see the green arrows in <ref type="figure">Figure 2</ref>. With the DETR family of detectors <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b70">71]</ref>, there are no assumptions on the order of the labels or the predictions and therefore we first match the objects of y to the ones in? via the Hungarian bipartite matching algorithm <ref type="bibr" target="#b37">[38]</ref>. Specifically, we find the permutation ? that minimizes the optimal matching cost between y and?:</p><formula xml:id="formula_2">? = arg min ??? N N i L match (y i ,? ?(i) )<label>(1)</label></formula><p>Where L match is the pairwise matching cost matrix as defined in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b70">71]</ref> and ? N is the set of all permutations over {1 . . . N }. Using the optimal ?, we define the loss as:</p><formula xml:id="formula_3">L(y,?) = N i=1 ? f L class (c i ,p ? (i) )+ 1 {ci? =0} (? b L box (b i ,b ? (i) )+ ? e L emb (z i ,? ?(i) ))<label>(2)</label></formula><p>Where L class is the class loss, that can be implemented via Cross Entropy Loss or Focal Loss <ref type="bibr" target="#b39">[40]</ref>, and L box is based on the the L 1 loss and the Generalized Intersection Over Union (GIoU) loss <ref type="bibr" target="#b49">[50]</ref>. Finally, we define L emb to be the L 1 loss:</p><formula xml:id="formula_4">L emb (z i , z j ) = ?z i ?? j ? 1<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We first describe the implementation details and datasets used for our experimentation. We then report how DETReg performs on the object detection tasks when finetuned on full and low-data regimes, including few-shot learning, and semi-supervised learning. Finally, we conclude with ablations, analyses, and visualizations from DETReg. Implementation. Based on the ablations presented in Section 4.5, the default experiment settings are as follows (see the Suppl. for all details). We initialize the ResNet50 backbone of DETReg with SwAV <ref type="bibr" target="#b5">[6]</ref>, which was pretrained with multi-crop views for 800 epochs on IN1K, and fix it throughout the pretraining stage. In the object embedding branch, f emb and f box are MLPs with 2 hidden layers of size 256 Baselines. We compare DETReg to several closely related state-of-the-art pretraining approaches for object detection with transformers: using a SwAV <ref type="bibr" target="#b5">[6]</ref> backbone, a fully pretrained UP-DETR <ref type="bibr" target="#b15">[16]</ref>, and a supervised baseline backbone.</p><p>Experiments. To evaluate DETReg, we finetuned it on three different datasets: MS COCO <ref type="bibr" target="#b40">[41]</ref>, PASCAL VOC <ref type="bibr" target="#b17">[18]</ref>, and Airbus Ship Detection <ref type="bibr" target="#b0">[1]</ref>. We perform an extensive comparison on MS COCO and finetune using similar training schedules as previously reported in <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b70">71]</ref>  <ref type="table" target="#tab_0">Table 2</ref> shows that DETReg improves by 2.5 (AP) points over SwAV on PASCAL VOC and by 1.2 (AP) on Airbus. For reference, by using a specialized architecture for ship detection that builds on a ResNet50 backbone, as well as leveraging the pixel-level annotations, <ref type="bibr" target="#b44">[45]</ref> obtains a box AP score of 76.1 on this dataset, 4.9 points lower than DETReg, which only uses the bounding box annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object Detection in Low-Data Regimes</head><p>These experiments test how DETReg performs when a small amount of annotated data is available for finetuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretraining.</head><p>We pretrain DETReg based on Deformable DETR <ref type="bibr" target="#b70">[71]</ref> for 5 epochs on ImageNet (IN1K). Baselines. We consider recent approaches for pretraining ResNet50 backbone for object detection: InstLoc <ref type="bibr" target="#b66">[67]</ref>, ReSim <ref type="bibr" target="#b61">[62]</ref> and SwAV <ref type="bibr" target="#b5">[6]</ref>, for each we use the publicly released checkpoint. We report the ?AP w.r.t. the supervised variant, which utilizes a ResNet50 pretrained on IN1K. Experiments. We test the representations learned by DE-TReg when transferring with limited amounts of labeled data (up to 1024 labeled images), randomly sampled from MS COCO train2017 and use val2017 for evaluation. We train all methods for up to 2, 000 epochs or until the validation performance stops improving. Results. <ref type="figure" target="#fig_0">Figure 3</ref> shows the results, where the y-axis reports the difference in AP compared to the supervised variant. The results indicate that DETReg consistently outperforms other pretraining strategies, when using Deformable DETR on low data regime. For example, when using only 256 images, DETReg improves the average precision (AP) score by 4.1 points compared to 1.1 for SwAV and 0.5 for ReSim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Few-Shot Object Detection</head><p>These experiments test how DETReg extends to the fewshot settings established in existing literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pretraining.</head><p>We pretrain DETReg based on Deformable DETR <ref type="bibr" target="#b70">[71]</ref> for 5 epochs on ImageNet (IN1K). Baselines. We consider Deformable DETR with a supervised pretrained backbone as the most direct baseline as its architecture and training strategy mirror DETReg. We also report the results of recent few-shot approaches, which utilize different underlying object detectors. Concurrent to our work, Meta-DETR <ref type="bibr">[</ref>  <ref type="table">Table 3</ref>. Few-shot detection evaluation on COCO. We trained the model on the 60 base classes and then evaluate the model performance on the 20 novel categories, following the data split used in <ref type="bibr" target="#b55">[56]</ref>. We show that DETReg outperforms previous few-shot object detectors by a large margin through simple fine-tuning on the few-shot datasets.</p><p>include its results to encourage unified reporting, even when experimental settings are not perfectly aligned. Experiments. Following the standard few-shot protocol for object detection <ref type="bibr" target="#b55">[56]</ref>, we finetune DETReg on the full data of 60 base classes, which contain around 99K labeled images. Then, we finetune on a balanced set of all 80 classes, where every class has k ? {10, 30} object instances. We use the splits from <ref type="bibr" target="#b55">[56]</ref> and report the performance on the novel 20 classes. The results are shown in <ref type="table">Table 3</ref>. <ref type="table" target="#tab_3">Table 4</ref> shows an extreme few-shot setup where DETReg is finetuned on the balanced few-shot set without the intermediate finetuning on base classes. We consider finetuning the decoder only (ft-decoder) and the full model (ft-full). Results. <ref type="table">Table 3</ref> shows that DETReg improves over supervised pretraining and achieves state-of-the-art on the standard few-shot setting when using base classes. Note that DETReg only uses a simple fine-tuning strategy, while other methods may include more complicated episodic training. <ref type="table" target="#tab_3">Table 4</ref> shows that DETReg achieves competitive fewshot performance even when the model is not trained on the abundant base class data. As a reference point, TFA <ref type="bibr" target="#b55">[56]</ref> is a previous fine-tuning method that trains on the abundant base class data, and we can see that DETReg outperforms it without additional supervision from the base class data.  <ref type="table">Table 5</ref>. Object detection using k% of the labeled data on COCO. The models are trained on train2017 using k% and then evaluated on val2017.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Semi-supervised Learning</head><p>50 epochs on MS COCO train2017 without labels.</p><p>Baselines. We compare DETReg with a Deformable DETR model initialized with a supervised backbone from IN1K pretraining, which is the most direct baseline as all experiments are carried out on the same architecture and training data. We consider recent approaches for pretraining ResNet50 backbone for object detection like ReSim <ref type="bibr" target="#b61">[62]</ref> and SwAV <ref type="bibr" target="#b5">[6]</ref>, for each we use the publicly released checkpoint. Experiments. We finetuned DETReg on random k% of train2017 data for k ? {1, 2, 5, 10}, until convergence (validation performance stopped improving). In each setting, we train 5 different models with different random seeds and report the mean and standard deviation.</p><p>Results. <ref type="table">Table 5</ref> shows that DETReg outperforms existing pretraining methods, including a consistent improvement over the supervised pretraining baseline. We include a more broad comparison in the <ref type="table">Supplementary Table 9</ref>, where we also compare to approaches that leverage both the labeled and unlabeled data via auxiliary losses <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b64">65</ref>].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">DETReg Analysis</head><p>This section further explores and justifies the architectural and algorithmic choices used in the main experiments. Design Ablations. <ref type="table" target="#tab_4">Table 6</ref> examines the contribution of the object localization and object embedding tasks in DE-TReg. To quantify the importance of using object-centric region proposals, we train DETReg while randomly shuffling the proposal box locations across images, as indicated by  <ref type="table">Table 7</ref>.</p><p>Class agnostic object proposal evaluation on MS COCO val2017. The models are trained on IN100 and for each method, we consider the top 100 proposals. We show DETReg identifies objects more effectively than the previous methods. "Shuffle" in the "Proposals" column. Second, to assess the contribution of the embedding loss L emb , we evaluate DE-TReg with different coefficients ? e ? {0, 1, 2}. Finally, we validate that performance does not drop when freezing the backbone during training, i.e. that the performance benefits stem from the core DETReg contributions. All models are trained on IN100 for 50 epochs and finetuned on MS COCO. <ref type="table" target="#tab_4">Table 6</ref> justifies our design choices: shuffling the region proposals across images led to a 11.3 AP drop indicating that the object-centric proposal are important. We further see that the embedding loss L emb has a relatively consistent performance improvements with changes of ? 2 AP for all setting, and we select ? e = 1 based on these results. Finally, the performance of DETReg with and without freezing the backbone encoder is relatively consistent with changes of 0.3 AP points between the two settings. Class Agnostic Object Detection. We examine the class agnostic performance of DETReg variants discussed in Section 3, as well as region proposal and pretraining approaches. The results reported in <ref type="table">Table 7</ref> indicate that DETReg variants achieve improved performance over other pretraining approaches including solely using Selective Search. This indicates that coupling the object embedding and localization components in the DETReg model improves the localization ability. In addition, we observe that the Top-K region proposal selection strategy performs best in these ablations. Robustness to different proposal methods. We test how DETReg performs when pretrained with Selective Search proposals compared to Edge Box region proposals <ref type="bibr" target="#b71">[72]</ref>. Specifically, we pretrain DETReg on IN100 and finetune on MS COCO with 2% and 10% of random data. We find that both variants perform similarly well with AP of 21.8 vs. 21.0 for 2% and a similar result of 36.2 for 10%.</p><formula xml:id="formula_5">? ?x ?I ? ? ?y ?I ? ? ?z ?I ?</formula><p>Visualizing DETReg. <ref type="figure" target="#fig_1">Figure 4</ref> shows qualitative examples of DETReg unsupervised box predictions with Deformable DETR. Additionally, it shows the Saliency Map <ref type="bibr" target="#b51">[52]</ref> of the x/y bounding box center and the object embedding with respect to the input image I. The first three columns show the attention focusing on the object edges for the x/y predictions and z for the predicted object embedding. The final column shows a case where the background plays a more important role than the object in the embedding. We believe this may be due to the CNN-based encoder focusing on the textures rather than the shapes in the region as discussed in <ref type="bibr" target="#b20">[21]</ref>, and we view further exploration of such characteristics as an intriguing direction for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations</head><p>DETReg's localization pretraining task uses simple region proposal methods for class-agnostic bounding-box supervision <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b53">54]</ref>. While <ref type="table">Table 7</ref> indicates that DETReg performance can improve beyond these methods, DETReg class-agnostic results remain far behind supervised counterparts. Furthermore, our experiments focused on DETR [5]related architectures, but it may be possible that DETReg applies to more traditional detection architectures, which we leave for future work to explore. Finally, while DETReg improves training time, transformer-based object detectors still require significant computational resources to train.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented DETReg, an unsupervised pretraining approach for object detection with transformers using region priors. Through extensive empirical study, we showed DE-TReg learns representations in the unsupervised pretraining stage that lead to improvements in downstream performance for two different transformer models across three different datasets and many settings. We believe unsupervised pretraining holds the potential for positive social impact, mainly because it can utilize unlabeled data and reduce the need for massive amounts of labeled data which can be very expensive for fields like Medical Imaging. We do not anticipate a negative impact specific to our approach, but as with any model, we recommend careful validation before deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>We start by providing the full implementation details of DETReg and include the complete PASCAL VOC results. We then follow with additional analysis of DETReg pretraining as well as class agnostic performance and visualization. Implementation Details. Based on the ablations presented in Section 4.5, the default experiment settings are as follows. For region proposals, we compute Selective Search boxes online using the "fast" preset of the OpenCV implementation <ref type="bibr" target="#b3">[4]</ref> and unless otherwise noted, we use the DETReg Top-K region selection variant (see Section 3.1) and set K = 30 proposals per-image. We initialize the ResNet50 backbone of DETReg with SwAV <ref type="bibr" target="#b5">[6]</ref>, which was pretrained with multi-crop views for 800 epochs on IN1K, and fix it throughout the pretraining stage. A similar SwAV encoder is used to encode region proposals, which are first cropped and resized to 128x128. In the object embedding branch, f emb and f box are MLPs with 2 hidden layers of size 256 followed by a ReLU <ref type="bibr" target="#b43">[44]</ref> nonlinearity. The output sizes of f emb and f box are 512 and 4. f cat is implemented as a single fully-connected layer with 2 outputs. We run the pretraining experiments using a batch size of 24 per GPU on an NVIDIA DGX, V100 x8 GPUs machine, following the hyperparameter settings and image augmentations from existing works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b70">71]</ref>. Similarly, cropped regions are augmented before being fed to the encoder to obtain embeddings z i . When finetuning, we drop the f emb branch, and set the size of the last fully-connected layer of f cat to be the number of classes in the target dataset plus a background class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Object Detection in Full Data Regimes</head><p>We reported DETReg results on the PASCAL VOC benchmark in Section 4.1. Here we include the full table, containing more past pretraining approaches using three different object detectors (see <ref type="table" target="#tab_5">Table 8</ref>). We observe that using the Deformable-DETR detector, the supervised pretraining baseline is superior to past pretraining approaches and that DE-TReg pretraining improves over it by 4 points (AP).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Semi-supervised Learning</head><p>We reported DETReg results and comparisons to other pretraining approaches like <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b61">62]</ref> when using limited amounts of data. In <ref type="table">Table 9</ref>, we include comparisons to semi-supervised works <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b64">65]</ref> that leverage both the labeled and unlabeled data in training via auxiliary losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DETReg Analysis</head><p>In Section 4.5 we analyzed DETReg, including the model ablations, class agnostic results, visualization and robustness. Here we further examine the pretrained DETReg model including the class agnostic results, and TopK selection policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Detector AP AP 50 AP 75 Supervised FRCN     <ref type="table">Table 9</ref>. Object detection using k% of the labeled data on COCO. The models are trained on train2017 using k% and then evaluated on val2017. Methods like <ref type="bibr" target="#b41">[42]</ref> utilize auxiliary losses during the training stage using unlabeled data, whereas DETReg utilizes unlabeled data during the pretraining stage only. policy. We report the precision and recall in <ref type="figure" target="#fig_3">Figure 5</ref>. In this paper, we have used K = 30 (see <ref type="figure">Figure 7</ref>), which emphasizes precision over recall. This might imply that DETReg performs well given high precision proposals.</p><p>DETReg Slots Visualization. We examine the learned object queries slots (see <ref type="figure" target="#fig_4">Figure 6</ref>) and observe they are similar to those in Deformable DETR, despite not using any human annotated data. Nevertheless, the Deformable DETR slots have greater variance with respect to locations and they tend to specialize more in particular boxes shapes.</p><p>Class Agnostic Object Detection. The quantitative results in Section 4.5 indicate that DETReg improves over Selective Search. The included qualitative examples of DE-TReg on MS COCO (see <ref type="figure" target="#fig_5">Figure 8</ref>) supports a similar conclusion, indicating that DETReg outperforms Selective Search but still much behind the ground truth labeled data. <ref type="figure">Figure 7</ref>. TopK Selective Search proposals on ImageNet. Using K=30, the proposals typically cover objects and parts-of-objects in the image. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Model comparison in low-data regimes. ?AP improvement over the supervised baseline, where the x-axis shows the total number of images used during training. We fix the Deformable DETR architecture across all methods and finetuned it using publically released ResNet50 weights of different methods on MS COCO train2017 and evaluate on val2017.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>DETReg visualization. We show the gradient norms from the unsupervised DETReg detection with respect to the input image I for (top) the x coordinate of the object center, (middle) the y coordinate of the object center, (bottom) the feature-space embedding, z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Top-K proposals performance of Selective Search. Using different values of K, w evaluate the class agnostic performance of Selective Search on MS COCO 2017 validation split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>DETReg slots specialize in specific areas in the image and uses a variety of box sizes much like Deformable DETR. Each square corresponds to a DETR slot, and shows the location of its bounding box predictions. We compare 10 random slots of the supervised Deformable DETR (top) and unsupervised DETReg (bottom) decoder for the MS COCO 2017 val dataset. Each point shows the center coordinate of the predicted bounding box, where following a similar plot in [5], a green point represents a square bounding box, a orange point is a large horizontal bounding box, and a blue point is a large vertical bounding box. Deformable DETR has been trained on MS COCO 2017 data, while DETReg has only been trained on unlabeled ImageNet data. Similar DETReg and Deformable DETR slots were manually chosen for illustration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Class Agnostic object detection visualization. Examples predictions using Selective Search and DETReg on random MS COCO images. For every image annotated with M boxes, only the top M predictions are shown.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 2 .</head><label>2</label><figDesc>DETReg 41.9 +1.4 61.9 +1.1 44.1 +1.5 +0.9 63.7 +0.7 46.6 +1.3 +0.8 64.1 +0.4 49.9 +1.experiments test how well DETReg performs when a fully annotated dataset is available for finetuning. Pretraining. We pretrain two variants of DETReg based on DETR [5] and Deformable DETR [71] detectors for 5 and 60 epochs on IN1K and IN100, respectively, where the pretraining schedules are set by proportionally adjusting the schedules used in UP-DETR to equate to the more efficient Deformable DETR schedules [71]. Object detection finetuned on PASCAL VOC and Airbus Ship data. The model is finetuned on PASCAL VOC trainval07+2012 and evaluated on test07 (left), and Airbus Ship Detection finetuned on the train split and evaluated on the 3k test images (right). All models are based on Deformable DETR [71]. Bold values indicate an improvement ? 0.3 AP.</figDesc><table><row><cell cols="4">Pretraining Detector Epochs AP</cell><cell>AP 50</cell><cell>AP 75</cell></row><row><cell>Supervised</cell><cell></cell><cell></cell><cell>39.5</cell><cell>60.3</cell><cell>41.4</cell></row><row><cell>SwAV [6] UP-DETR</cell><cell>DETR</cell><cell>150</cell><cell>39.7 40.5</cell><cell>60.3 60.8</cell><cell>41.7 42.6</cell></row><row><cell>Supervised</cell><cell></cell><cell></cell><cell>40.8</cell><cell>61.2</cell><cell>42.9</cell></row><row><cell>SwAV [6] UP-DETR</cell><cell>DETR</cell><cell>300</cell><cell>42.1 42.8</cell><cell>63.1 63.0</cell><cell>44.5 45.3</cell></row><row><cell cols="4">DETReg 43.7 Supervised 44.5</cell><cell>63.6</cell><cell>48.7</cell></row><row><cell>SwAV [6] UP-DETR</cell><cell>DDETR</cell><cell>50</cell><cell>45.2 44.7</cell><cell>64.0 63.7</cell><cell>49.5 48.6</cell></row><row><cell>DETReg</cell><cell></cell><cell></cell><cell>45.5</cell><cell></cell><cell></cell></row></table><note>3 Table 1. Object detection results when trained on MS COCO train2017 and evaluated on val2017. Both DETReg and UP-DETR are pretrained on IN1K under comparable settings, while supervised and SwAV only pretrain the backbone of the object detector. We explore both the DETR and Deformable DETR (DDETR) architectures; for compatibility with prior work, we fine- tuned the DETR for 150/300 epochs and DDETR for 50 epochs.followed by a ReLU [44] nonlinearity. The output sizes of f emb and f box are 512 and 4. f cat is implemented as a sin- gle fully-connected layer with 2 outputs. Unless otherwise noted, we use the DETReg Top-K region selection variant (see Section 3.1) and set K = 30 proposals per-image. Datasets. We use the following datasets: ImageNet IL- SRVC 2012 (IN1K) dataset contains 1.2M images with 1000 class categories. As done in prior work [6, 10, 62, 64], we use the unlabeled IN1K data for pretraining. Similar to other works [27, 28, 62], we use a subset of IN1K called IN100 that contains ?125K images and 100 class categories for several ablation studies. MS COCO [41] is a popular ob- ject detection benchmark that contains 121K labeled images, where objects from 80 object categories are annotated with bounding boxes. PASCAL VOC [18] contains ?20K natu- ral images where the objects from 21 classes are annotated. To explore a dataset with different visual properties than the typical object-centric benchmarks, we use the Airbus Ship Detection dataset [1], which contains ?231K satellite images annotated with bounding boxes of ships. Follow- ing [45], we convert the segmentation masks to bounding boxes and use a 42.5K image subset, with 3K test/val splits.4.1. Object Detection in Full Data Regimes</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>These experiments test how DETReg compares to semisupervised methods, where small amounts of labeled data and large amounts of unlabeled data are used during training. Pretraining. We pretrain DETReg (Deformable DETR) for Few-shot object detection without training on the COCO base classes. To test DETReg's performance on extreme few-shot scenarios, we conduct evaluation where DETReg is finetuned only on the K-shot COCO subsets. DETReg outperforms previous methods such as TFA<ref type="bibr" target="#b55">[56]</ref> that use base class data.</figDesc><table><row><cell cols="2">Model</cell><cell></cell><cell cols="2">Detector</cell><cell cols="2">Novel AP 10 30</cell><cell>Novel AP 75 10 30</cell></row><row><cell cols="3">TFA [56] (w/base)</cell><cell cols="2">FRCN</cell><cell cols="3">10.0 13.7 9.3 13.4</cell></row><row><cell cols="2">DDETR-ft-full</cell><cell></cell><cell></cell><cell></cell><cell cols="3">0.03 0.01 0.04 0.02</cell></row><row><cell cols="3">DDETR-ft-decoder DETReg-ft-decoder</cell><cell cols="2">DDETR</cell><cell cols="3">3.3 10.2 2.7 10.7 10.2 17.9 11.1 19.2</cell></row><row><cell cols="3">DETReg-ft-full</cell><cell></cell><cell></cell><cell cols="3">10.6 18.0 11.6 19.6</cell></row><row><cell>Method</cell><cell>Detector</cell><cell>1%</cell><cell></cell><cell></cell><cell>2%</cell><cell>COCO</cell><cell>5%</cell><cell>10%</cell></row><row><cell>Supervised</cell><cell></cell><cell cols="6">11.31 ? 0.3 15.22 ? 0.32 21.33 ? 0.2 26.34 ? 0.1</cell></row><row><cell>SwAV ReSim</cell><cell>DDETR</cell><cell cols="6">11.79 ? 0.3 11.07 ? 0.4 15.26 ? 0.26 21.48 ? 0.1 26.56 ? 0.3 16.02 ? 0.4 22.81 ? 0.3 27.79 ? 0.2</cell></row><row><cell>DETReg</cell><cell></cell><cell cols="2">14.58 ? 0.3</cell><cell cols="4">18.69 ? 0.2 24.80 ? 0.2 29.12 ? 0.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6 .</head><label>6</label><figDesc>Ablation studies. This tables ablates region proposal sampling strategies, values of ?emb, and whether to freeze backbones with DETReg trained on IN100 and finetuned on MS COCO. Shuffling the region proposals across images led to a 11.3 AP drop, L emb has a consistent performance, and freezing the backbone does not significantly change the performance.</figDesc><table><row><cell>Proposals</cell><cell>L emb</cell><cell></cell><cell cols="4">Frozen BB Lclass ? Lbox ?</cell><cell>AP</cell></row><row><cell>Shuffle</cell><cell>?e = 0</cell><cell></cell><cell></cell><cell></cell><cell>11.3</cell><cell>.044</cell><cell>32.0</cell></row><row><cell>Top-K</cell><cell>?e = 0</cell><cell></cell><cell></cell><cell></cell><cell>9.50</cell><cell>.037</cell><cell>43.3</cell></row><row><cell>Top-K</cell><cell>?e = 1</cell><cell></cell><cell></cell><cell></cell><cell>8.81</cell><cell>.037</cell><cell>45.1</cell></row><row><cell>Top-K</cell><cell>?e = 2</cell><cell></cell><cell></cell><cell></cell><cell>9.14</cell><cell>.039</cell><cell>43.8</cell></row><row><cell>Top-K</cell><cell>?e = 1</cell><cell></cell><cell>?</cell><cell></cell><cell>8.61</cell><cell>.037</cell><cell>45.4</cell></row><row><cell>Method</cell><cell></cell><cell cols="6">AP AP50 AP75 R@1 R@10 R@100</cell></row><row><cell>UP-DETR [16]</cell><cell></cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.4</cell></row><row><cell>Rand. Prop.</cell><cell></cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.8</cell></row><row><cell cols="3">Selective Search [54] 0.2</cell><cell>0.5</cell><cell>0.1</cell><cell>0.2</cell><cell>1.5</cell><cell>10.9</cell></row><row><cell cols="2">ImpSamp (ours)</cell><cell>0.7</cell><cell>2.0</cell><cell>0.1</cell><cell>0.3</cell><cell>1.8</cell><cell>9.0</cell></row><row><cell cols="2">Random-K (ours)</cell><cell>0.7</cell><cell>2.4</cell><cell>0.2</cell><cell>0.5</cell><cell>2.9</cell><cell>11.7</cell></row><row><cell>Top-K (ours)</cell><cell></cell><cell>1.0</cell><cell>3.1</cell><cell>0.6</cell><cell>0.6</cell><cell>3.6</cell><cell>12.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 8 .</head><label>8</label><figDesc>Object detection finetuned on PASCAL VOC. The model is finetuned on PASCAL VOC trainval07+2012 and evaluated on test07. Models are based on Faster-RCNN [49] (FRCN), DETR [5], and Deformable DETR [71] (DDETR). Bold values indicate an improvement ? 0.3 AP.</figDesc><table><row><cell></cell><cell>1.3</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.2</cell><cell></cell><cell></cell></row><row><cell>AP50 (%)</cell><cell>1.1</cell><cell></cell><cell></cell></row><row><cell></cell><cell>1.0</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.9</cell><cell></cell><cell></cell></row><row><cell></cell><cell>816 32</cell><cell>64</cell><cell>128 Top-K</cell><cell>256</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Improved Encoder, improved DETReg. We test how DETReg performs when object embeddings are obtained with different image encoders. Specifically, we pretrain DETReg on IN100 using SwAV trained for 400 epochs compared to a superior variant trained for 800 epochs with multicrops. We finetune on MS COCO with 1% data and observe the improved encoder achieves 1 AP improvement (27.7 vs 26.7). TopK selection policy. Using Selective Search, we examine the class agnostic performance when using TopK Supervised Pretraining DDETR 11.31 ? 0.3 15.22 ? 0.32 21.33 ? 0.2 26.34 ? 0.1 SwAV 11.79 ? 0.3 16.02 ? 0.4 22.81 ? 0.3 27.79 ? 0.2 ReSim 11.07 ? 0.4 15.26 ? 0.26 21.48 ? 0.1 26.56 ? 0.3 DETReg 14.58 ? 0.3 18.69 ? 0.2 24.80 ? 0.2 29.12 ? 0.2</figDesc><table><row><cell>Approach Detector Auxiliary FRCN DETReg Method 1% 2% CSD [34] 10.5 ? 0.1 13.9 ? 0.1 COCO 18.6 ? 0.1 5% STAC [53] 14.0 ? 0.6 18.3 ? 0.3 24.4 ? 0.1 U-T [42] 20.8 ? 0.1 24.3 ? 0.1 28.3 ? 0.1 S-T [65] 20.5 ? 0.4 ? 30.7 ? 0.1</cell><cell>10% 22.5 ? 0.1 28.6 ? 0.2 31.5 ? 0.1 34.0 ? 0.1</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Sayna Ebrahimi and Jitendra Malik for helpful feedback and discussions. This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant ERC HOLI 819080). Prof. Darrell's group was supported in part by DoD including DARPA's LwLL and/or SemaFor programs, as well as BAIR's industrial alliance programs. This work was completed in partial fulfillment for the Ph.D degree of the first author.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">airbus ship detection challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Airbus</surname></persName>
		</author>
		<idno>Ac- cessed: 2021-09-30</idno>
		<ptr target="https://www.kaggle.com/c/airbus-ship-detection" />
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What is an object?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vittorio</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pablo</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordi</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferran</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The OpenCV Library. Dr. Dobb&apos;s Journal of Software Tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">End-toend object detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Carion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kirillov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="page">13</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments. NeurIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14294,2021.4</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Cpmc: Automatic object segmentation using constrained parametric min-cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1312" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lstd: A low-shot transfer detector for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yali</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoyou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Should i look at the head or the tail? dual-awareness attention for few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-I</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueh-Cheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Cheng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Hsiang</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Fong</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston H</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hsu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12152</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Global contrast based salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Niloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolei</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Philip</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi-Min</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="569" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bing: Binarized normed gradients for objectness estimation at 300fps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Ming</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen-Yan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3286" to="3293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">UP-DETR: Unsupervised pre-training for object detection with transformers. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhigang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolun</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yugeng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junying</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Category-independent object proposals with diverse ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="222" to="234" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fewshot object detection with attention-rpn and multi-relation detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="4013" to="4022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Generalized few-shot object detection without forgetting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4527" to="4536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patricia</forename><surname>Rubisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wieland</forename><surname>Felix A Wichmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brendel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.12231</idno>
		<title level="m">Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning representations by predicting bags of visual words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyros</forename><surname>Gidaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>P?rez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scaling and benchmarking self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent-a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Azar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rethinking imagenet pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Efficient visual pretraining with contrastive detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Skanda</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jo?o</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Carreira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10957</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">What makes for effective detection proposals?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="814" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">How good are detection proposals, really?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.6962</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Consistency-based semi-supervised learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jisoo</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungeui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeesoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">nips</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Few-shot object detection via feature reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Repmet: Representative-based metric learning for classification and few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Shtok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sivan</forename><surname>Harary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Aides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raja</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Geodesic object proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="725" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The hungarian method for the assignment problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Harold W Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Naval research logistics quarterly</title>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="83" to="97" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Transformation invariant few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3094" to="3102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Unbiased teacher for semi-supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Cheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Wen</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bichen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Vajda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.09480</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Self-supervised learning of pretext-invariant representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR, 2020</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Attention mask r-cnn for ship detection and segmentation from remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxuan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingliang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward K</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="9325" to="9334" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Addressing the exorbitant cost of labeling medical images with active learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saba</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Oktay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Javier</forename><surname>Alvarez-Valle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujeeth</forename><surname>Bharadwaj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning in Medical Imaging and Analysis</title>
		<imprint>
			<date type="published" when="2021-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Yolo9000: better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7263" to="7271" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Selfsupervised pretraining improves self-supervised pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Colorado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ani</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sayna</forename><surname>Nrusimha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vivek</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Vijaykumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Guillory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Metzger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12718</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generalized intersection over union: A metric and a loss for bounding box regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamid</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byungseok</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wuhyun</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ildoo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungwoong</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06122</idno>
		<title level="m">Spatially consistent representation learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">A simple semi-supervised learning framework for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.04757</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jasper Rr Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Koen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Jasper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theo</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnold Wm</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Frustratingly simple few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06957</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Dense contrastive learning for self-supervised visual pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rufeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.09157,2020.12</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Metalearning to detect rare objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martial</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9925" to="9934" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multiscale positive sample refinement for few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songtao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="456" to="472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Meta-rcnn: Meta learning for few-shot object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiongwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Doyen</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1679" to="1687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Region similarity representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Colorado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.12902</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Few-shot object detection and viewpoint estimation for objects in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="192" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Detco: Unsupervised contrastive learning for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohang</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.04803</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">End-toend semi-supervised object detection with soft teacher</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengde</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fangyun</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zicheng</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09018</idno>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Meta r-cnn: Towards general solver for instance-level low-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaopeng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziliang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anni</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Instance localization for self-supervised detection pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ceyuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">iprivacy: image privacy protection by identifying sensitive objects via deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baopeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhong</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1005" to="1016" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Meta-detr: Few-shot object detection via unified image-level meta-learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gongjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiwen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijian</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.11731</idno>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Cheaper pre-training lunch: An efficient paradigm for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongzhan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="258" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Deformable DETR: Deformable transformers for end-to-end object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04159</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<monogr>
		<title level="m" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
