<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Cell segmentation and tracking using CNN-based distance predictions and a graph-based matching strategy</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Scherr</surname></persName>
							<email>*tim.scherr@kit.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Automation and Applied Informatics</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<settlement>Eggenstein-Leopoldshafen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katharina</forename><surname>L?ffler</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Automation and Applied Informatics</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<settlement>Eggenstein-Leopoldshafen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Biological and Chemical Systems -Biological Information Processing</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<settlement>Eggenstein-Leopoldshafen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>B?hland</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Automation and Applied Informatics</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<settlement>Eggenstein-Leopoldshafen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Mikut</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Automation and Applied Informatics</orgName>
								<orgName type="institution">Karlsruhe Institute of Technology</orgName>
								<address>
									<settlement>Eggenstein-Leopoldshafen</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Cell segmentation and tracking using CNN-based distance predictions and a graph-based matching strategy</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The accurate segmentation and tracking of cells in microscopy image sequences is an important task in biomedical research, e.g., for studying the development of tissues, organs or entire organisms. However, the segmentation of touching cells in images with a low signal-to-noise-ratio is still a challenging problem. In this paper, we present a method for the segmentation of touching cells in microscopy images. By using a novel representation of cell borders, inspired by distance maps, our method is capable to utilize not only touching cells but also close cells in the training process. Furthermore, this representation is notably robust to annotation errors and shows promising results for the segmentation of microscopy images containing in the training data underrepresented or not included cell types. For the prediction of the proposed neighbor distances, an adapted U-Net convolutional neural network (CNN) with two decoder paths is used. In addition, we adapt a graph-based cell tracking algorithm to evaluate our proposed method on the task of cell tracking. The adapted tracking algorithm includes a movement estimation in the cost function to re-link tracks with missing segmentation masks over a short sequence of frames. Our combined tracking by detection method has proven its potential in the IEEE ISBI 2020 Cell Tracking Challenge (http://celltrackingchallenge.net/) where we achieved as team KIT-Sch-GE multiple top three rankings including two top performances using a single segmentation model for the diverse data sets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>State-of-the-art microscopy imaging techniques such as light-sheet fluorescence microscopy imaging enable to investigate cell dynamics with single-cell resolution <ref type="bibr" target="#b1">[1,</ref><ref type="bibr" target="#b2">2]</ref>. This allows to study cell migration and proliferation in tissue development and organ formation at early embryonic stages. Establishing the required complete lineage of each cell, however, requires a virtually error-free segmentation and tracking of individual cells over time <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b3">3]</ref>. A manual data analysis is unfeasible, due to the large amount of data acquired with modern imaging techniques. In addition, low-resolution objects are very difficult to detect even for human experts. Deep learning-based cell segmentation methods have proven to outperform traditional methods even on very diverse 2D data sets <ref type="bibr" target="#b4">[4]</ref>. However, state-of-the-art cell tracking methods often still need a time-consuming manual cell track curation, e.g., using EmbryoMiner <ref type="bibr" target="#b5">[5]</ref> or the Massive Multi-view Tracker (MaMuT) <ref type="bibr" target="#b6">[6]</ref>. Especially for low signal-to-noise ratio and 3D data, further method development is required for both cell segmentation and cell tracking <ref type="bibr" target="#b7">[7]</ref>. show a crop of the simulated Cell Tracking Challenge data set Fluo-N2DH-SIM+ <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b9">9]</ref>. Generated boundaries (c) and borders (d) can be used to split touching cells. Many training data sets contain only few touching cells resulting in few training samples for borders and boundaries between cells. The combination of cell distances (e) with neighbor distances (f) is aimed to solve this problem since models can also learn from close cells.</p><p>Traditional segmentation methods, such as TWANG for the segmentation of roundish objects <ref type="bibr" target="#b8">[8]</ref>, are often designed for a specific application. These methods commonly consist of sophisticated combinations of pre-processing filters, e.g., Gaussian or median filters, and segmentation operations, e.g., a region adaptive thresholding followed by a watershed transform <ref type="bibr" target="#b9">[9]</ref>. To reach a reasonable segmentation quality, such traditional methods need to be carefully adapted to the cell type and imaging conditions. Therefore, expert knowledge is needed. In contrast, deep learning-based segmentation methods shift the expert knowledge needed to the model design and to the training process. Thus, less expert knowledge is needed for the application of a trained model and to fine-tune the post-processing which is often kept very simple. A review of cell segmentation methods is provided in <ref type="bibr" target="#b10">[10]</ref>.</p><p>To improve the generalization ability of a trained deep learning model, a preferably diverse and large annotated data set is needed. This fact is especially problematic when dealing with touching cells since this case is usually underrepresented in training data sets. Therefore, models for cell boundary or border prediction (see <ref type="figure" target="#fig_0">Fig 1)</ref> are often not able to handle touching cells well. The result are merged cells, due to gaps in predicted cell boundaries and borders between touching cells <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b12">12]</ref>. To overcome this problem, several approaches have been proposed. In <ref type="bibr" target="#b11">[11]</ref>, models are trained to predict adapted thicker borders and smaller cells, which can decrease the amount of merged cells. <ref type="bibr" target="#b12">[12]</ref> utilizes new gap and touching classes with J regularization. <ref type="bibr" target="#b13">[13]</ref> combines distance transforms for single cell nuclei with discrete boundaries. A center vector encoding which is aimed to be more robust to label inconsistencies is proposed in <ref type="bibr" target="#b14">[14]</ref>, whereas in <ref type="bibr" target="#b15">[15]</ref>, horizontal and vertical gradient maps are used. To improve the generalization ability of a model for cell types with only few or no annotated images, a generative adversarial network-based image style transfer to generate augmented training samples of that cell types has been used in <ref type="bibr" target="#b16">[16]</ref>. An advantage of border-based approaches is that a deep learning model is enforced to focus on touching cells that are underrepresented in the training data. However, border-based approaches still have the shortcoming that only touching cells can be used to train the border prediction.</p><p>Although deep learning methods have been successfully applied to multi object tracking on natural images <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref>, there are only few deep learning approaches for cell tracking <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20]</ref>. In <ref type="bibr" target="#b19">[19]</ref>, cells are simultaneously segmented and tracked by combining a recurrent hourglass network with a pixel-wise metric embedding learning. <ref type="bibr" target="#b20">[20]</ref> proposes a particle-filter-based motion model in combination with a CNN-based observation model. However, cell tracking is still dominated by traditional tracking approaches <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b21">21]</ref>. One reason is the lack of high quality annotations as provided in natural image tracking benchmarks <ref type="bibr" target="#b22">[22]</ref><ref type="bibr" target="#b23">[23]</ref><ref type="bibr" target="#b24">[24]</ref>. Thus, training data are often not available. Another aspect that complicates the task of cell tracking are cell death and division events, which do not occur in natural image tracking data. Therefore, traditional tracking algorithms with comparably few parameters and explicit modeling of cell division events still dominate cell tracking benchmarks <ref type="bibr" target="#b7">[7]</ref>. The comparison of cell tracking algorithms in <ref type="bibr" target="#b7">[7]</ref> shows that the majority of tracking approaches uses an adapted version of nearest neighbors, a graph-based linking or multi hypothesis tracking. In <ref type="bibr" target="#b21">[21]</ref>, the Viterbi algorithm is applied to track cells. A joint model for segmentation and tracking is proposed in <ref type="bibr" target="#b25">[25]</ref> where model parameters are learned based on Bayes risk minimization.</p><p>In this paper, we propose a novel representation of cell borders, the neighbor distances, to solve the challenging problem of segmenting touching cells of various types in the absence of large training data sets. Thus far, problems of border prediction approaches are the sensitivity to annotation inconsistencies, and that only touching cells provide border information in the training. The neighbor distances are aimed to be less sensitive to annotation inconsistencies, and enable to learn also from close cells. This additional information in the training process results in a more robust border prediction. Similar to <ref type="bibr" target="#b13">[13]</ref>, we combine our border predictions with cell distances to further prevent the erroneous merging of close cells. However, in contrast to <ref type="bibr" target="#b13">[13]</ref>, we remove the bottlenecks of non-robust discrete boundaries and of the feature fusion layers. This results in a simplified architecture and training process and in less merged cells. For the cell tracking, we adapt a coupled minimum-cost flow algorithm to include an object movement estimation. In addition, our formulation is able to link fragmented tracks due to missing segmentation masks in a short sequence of frames. The remainder of this paper covers the methodology we use to detect and segment cells and the subsequent cell tracking. In the results section, we demonstrate the quality of our introduced method on data from the Cell Tracking Challenge <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b9">9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Materials and methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cell segmentation using CNN-based distance predictions</head><p>For cell segmentation, we train a deep learning model to predict cells and cell borders, followed by a post-processing with a seed extraction and a seed-based watershed segmentation. A key for the successful application of supervised deep learning methods in the absence of large training data sets is to introduce representations that allow to use as much information as possible. Thus, instead of discrete cell boundary ( <ref type="figure" target="#fig_0">Fig 1c)</ref> and cell border representations <ref type="figure" target="#fig_0">(Fig 1d)</ref>, we combine cell distances <ref type="figure" target="#fig_0">(Fig 1e,</ref>  <ref type="bibr" target="#b13">[13]</ref>) with novel neighbor distances <ref type="figure" target="#fig_0">(Fig 1f)</ref>. These representations allow incorporating the regional information not only from touching cells but also from close cells resulting in more robust deep learning models. A segmentation network based on the U-Net architecture <ref type="bibr" target="#b27">[27]</ref>, modified similar to <ref type="bibr" target="#b13">[13]</ref>, is utilized as the backbone of the method. An overview of the proposed method provides </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cell distances and neighbor distances</head><p>The cell distances, as shown in <ref type="figure" target="#fig_0">Fig 1e,</ref> are generated from ground truth data by computing the Euclidean distance transform for each cell independently. Adjacent cells are treated as background in this step and the distance transform is normalized into the range [0, 1]. Thus, each pixel of a cell represents the normalized distance to the nearest pixel not belonging to this cell. The cell distance prediction alone is sufficient to obtain seeds for the post-processing. However, a precondition is that the CNN has learned to deal with cell distances of touching cells. By combining cell distances with the novel neighbor distances the erroneous merging of touching cells is prevented. Overview of the proposed segmentation method using distance predictions (adapted from <ref type="bibr" target="#b13">[13]</ref>). The CNN consists of a single encoder that is connected to both decoder paths. The network is trained to predict cell distances and neighbor distances that are used for the watershed-based post-processing. The input image shows a crop of the Cell Tracking Challenge data set Fluo-N2DH-GOWT1 <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b9">9]</ref>.   <ref type="figure" target="#fig_4">(Fig 3c)</ref> is calculated. The distance transform is cut to the cell size and normalized ( <ref type="figure" target="#fig_4">Fig 3d)</ref> followed by an inversion <ref type="figure" target="#fig_4">(Fig 3e)</ref>. The normalization to the range [0, 1] is required to suppress neighbor distances for cells without close neighbors. To further reduce the erroneously merging of cells, gaps between close cells are closed by applying a grayscale closing <ref type="figure" target="#fig_4">(Fig 3g)</ref>. Finally, to get a steeper decline within cells, a scaling is applied by taking the closed neighbor distances <ref type="figure" target="#fig_4">(Fig 3g)</ref> to the power of three <ref type="figure" target="#fig_4">(Fig 3h)</ref>. This confines the neighbor distances to the outer cell area and therefore eases the seed extraction in the post-processing. An advantage of the neighbor distances is that they also provide information in the training process when cells are close but do not touch. This can be seen in   The difference images between the first row and the second row show that the changes for the distance labels are smoother. Shown is a crop of the Cell Tracking Challenge data set Fluo-N2DH-SIM+ <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b9">9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Robustness of neighbor distances to annotation inconsistencies</head><p>Fig 4 shows that the neighbor distances are more robust to annotation inconsistencies than boundaries and borders, i.e., a cell was morphologically eroded and another cell dilated resulting in masks that only differ in single pixels. The location of the discrete boundaries and borders change, meaning that a prediction of the initial border is considered incorrect and penalized in the training. This makes it difficult to train models well on small data sets. In contrast, the proposed continuous neighbor distance shows a smooth change. Therefore, the influence of annotation inconsistencies on the training process is reduced resulting in a more robust training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Architecture</head><p>The architecture is based on the U-Net architecture <ref type="bibr" target="#b27">[27]</ref>. Instead of a single decoder path, two parallel decoder paths are used allowing each path to focus on features related to the desired output. In addition, the feature detection in the shared encoder branch of the network is trained using backpropagated information from both decoder branches. The maximum pooling layers are replaced with 2D convolutional layers with stride 2 and kernel size 3. Additionally, batch normalization layers are added. The number of feature maps is doubled from 64 feature maps to a maximum of 1024 in the encoder path and halved in each decoder path correspondingly. To avoid the need of cropping before concatenation, zero padding is applied in the convolutional layers to keep the feature map size consistent. The rectified linear unit activation function is used within the network and a linear activation for the output layers. <ref type="figure">Fig 2 provides</ref> an overview of the architecture, i.e., convolutional and downsampling layers are summarized into blocks. Challenge data set Fluo-N3DL-TRIC <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b9">9]</ref>.</p><p>Watershed post-processing <ref type="figure" target="#fig_8">Fig 5</ref> shows the main steps of the post-processing. The cell distance prediction P cell and the neighbor distance prediction P neighbor are smoothed to avoid the erroneous splitting of cells in the seed extraction step:</p><formula xml:id="formula_0">P cell = P cell * G(?) ,<label>(1)</label></formula><formula xml:id="formula_1">P neighbor = P neighbor * G(?) ,<label>(2)</label></formula><p>with G(?) representing a Gaussian kernel with standard deviation ? and * being the convolution operator. Then, the region to flood P mask with a seed-based watershed is extracted from the smoothed cell distance prediction by applying a threshold mask :</p><formula xml:id="formula_2">P mask =P cell &gt; mask .<label>(3)</label></formula><p>To obtain the seeds, the smoothed and squared neighbor distance prediction is subtracted from the cell prediction and the threshold seed is applied:</p><formula xml:id="formula_3">P seeds = P cell ?P 2 neighbor &gt; seed .<label>(4)</label></formula><p>Depending on the cell size, the squaring can be omitted or replaced by an even steeper function to fine-tune the seed extraction. Seeds with an area smaller than 3 px 2 are removed. For 3D and 3D+t data, detected merged cells in z-direction can be split by increasing the seed extraction threshold seed till multiple seeds are found for the merged cells. For the detection of merged cells, a priori knowledge about cell sizes or an outlier detection can be used. Inspired by the Dual U-Net architecture <ref type="bibr" target="#b13">[13]</ref>, we first attempted to enforce the CNN to predict an additional seed output from the cell distances and the neighbor distances. However, our traditional post-processing provided better results in tests and enables a fine-tuning to cell types not included in the training data. In addition, it simplifies the architecture and the training process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cell tracking</head><p>Cell tracking aims to reconstruct the lineage of cells, by linking related cells over time. This linking task is trivial in case of low cell density, error-free segmentation, and high temporal resolution resulting in negligible cell movements between adjacent frames. However, especially for low signal-to-noise-ratio images with touching and dividing cells, fragmented tracks can occur. To re-link fragmented tracks, we match tracks without assigned cells over a short sequence of frames and add a coarse position estimation to the cost function. The proposed algorithm is capable of tracking all segmented cells in an image sequence as well as tracking only a subset, e.g., a selection of manually marked cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Initialization</head><p>The tracking algorithm traverses the image sequence</p><formula xml:id="formula_4">I = {I t | 0 ? t ? T } forwards,</formula><p>where I t is the image at time point t and T the number of time points. A track is initialized for each segmented object in the first frame. For data sets with marked objects in the first frame, tracks are only initialized for marked objects. It is assumed that the object movement between successive frames is small compared to the overall image size. Therefore, for each tracked object a rectangular region of interest (ROI) is defined as a search space for the same object in the next frame. The initial center of each ROI is set to the median position calculated from the first assigned segmentation mask of each track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Movement estimation</head><p>The tracking step consists of a movement estimation followed by a graph-based matching strategy. To estimate the movement of an object, the image frames I t and I t+1 are cropped to the object ROI. Then, a phase correlation <ref type="bibr" target="#b28">[28]</ref> is calculated between the image crops to estimate a shift between those. The object movement is the shift between the image crops which is given by the position of the maximum peak of the phase correlation. Based on the estimated object movement, the ROI at time point t + 1 is adapted for each object individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph-based matching strategy</head><p>All tracks with no successors and their last assigned segmentation mask within time span {t ? ?t, . . . , t} are considered active. Therefore, tracks with missing segmentation masks over at most ?t time points can be re-linked. Next, for each active track a set of potential matching candidates is selected based on its ROI at time point t + 1. Active tracks and potential matching candidates are matched by using an adapted version of the coupled minimum-cost flow algorithm proposed in <ref type="bibr" target="#b29">[29]</ref>. The algorithm minimizes the overall cost by selecting edges in the graph with minimal cost subject to a set of constraints. The constraints model flow, appearance/disappearance of objects, and splitting/merging of objects. For an in depth introduction please see <ref type="bibr" target="#b29">[29]</ref>. <ref type="figure">Fig 6 shows</ref> our adapted graph structure of the coupled minimum-cost flow algorithm. The following adaptations are applied: the appearance cost of objects is set to 0, as spurious tracks will be filtered out by the subsequent post-processing. This appears to be advantageous in scenarios with the objective to track only a few selected objects. The disappearance cost is set to the length of the largest edge of the ROI instead of using appearance-based features. Therefore, tracks with missing segmentation masks can be assigned to the disappearance node as well. The merging node proposed in <ref type="bibr" target="#b29">[29]</ref> is removed, as it only models the merging of two objects per time point. The matching (1) Add nodes for segmented objects (O)</p><formula xml:id="formula_5">t ? ?t . . . t t + 1 . . . O O O O (2) Add split nodes (S) t ? ?t . . . t t + 1 . . . O O O O S (3) Add appear (A) &amp; delete (D) nodes t ? ?t . . . t t + 1 . . . O O O O S A D (4) Final graph t ? ?t . . . t t + 1 . . . O O O O S S ? S + A D Fig 6.</formula><p>Graph construction steps exemplary for four segmented objects. Edges added in a construction step are black, edges added in previous steps are gray. The gray nodes (O) correspond to segmented objects. The segmented objects from {t ? ?t, ..., t} are the last matched objects of all active tracks, whereas the segmented objects of t + 1 are not matched to tracks yet. The blue node models the appearance of objects (A), the red node the disappearance of objects (D), and the green node split events (S). Split event nodes (S) are added for each pair of objects at t + 1. Therefore, a split event node (S) has exactly two outgoing edges but can have several ingoing edges from object nodes (O). Source (S ? ) and sink nodes (S + ) are added for the formulation as coupled minimum cost flow problem.</p><p>cost c s,n between track s and potential matching candidate n is adapted to:</p><formula xml:id="formula_6">c s,n = p s t+1 ? p n t+1 2 ,<label>(5)</label></formula><p>wherep s t+1 is the estimated position of the tracked object s at time point t + 1 and p n t+1 is the position of the potential matching candidate. The estimated positionp s t+1 is given by:</p><formula xml:id="formula_7">p s t+1 = p s t + d s t,t+1 ,<label>(6)</label></formula><p>where d s t,t+1 is the estimated shift of the ROI of track s between time points t and t + 1. p s t is the position of the tracked object at time point t. The cost of split events are computed based on the size and position of the tracked object s and its potential successor candidates n and k:</p><formula xml:id="formula_8">c s,(n,k) = p s t+1 ? 1 2 (p n t+1 + p k t+1 ) 2 if C s = 1, ? else,<label>(7)</label></formula><p>where c s,(n,k) are the split costs and C s the split condition. In practice, we set ? to ten times the disappearance cost. The split condition C s is given by:</p><formula xml:id="formula_9">C s = ? ? ? 1 if V n t+1 V k t+1 &gt; ?, V n t+1 +V k t+1 V s t last &lt; ?, p n t+1 ? p k t+1 2 &lt; ?, 0 else. (8) V n t+1 , V k t+1</formula><p>and V s t last are the sizes of the segmentation masks of successor candidates n and k, and of the last assigned object to the track s at time point t last , respectively. The successor candidates are sorted so V k t+1 ? V n t+1 holds. ?, ? and ? are hyper-parameters. A possible parametrization of those hyper-parameters is provided in the results section. The split condition ensures that successors are of similar size, have a combined size similar to the size of the predecessor object, and should be reasonably close to each other.</p><p>Each active track is only linked to segmented objects overlapping with its ROI, reducing the number of edges in the graph. As all active tracks are added to the graph  <ref type="figure">Fig 7.</ref> Cell Tracking Challenge data set structure. Since no ground truths are publicly available for the challenge sets, the two provided training sets need to be split into a set used for training and a set for evaluation.</p><p>and not only segmented objects between successive time points, tracks with missing segmentation masks over a short sequence of frames can be linked. A solution of the matching problem is then found using integer linear programming.</p><p>For data sets with the aim to track all segmented objects, each non-matched object at time point t + 1 is initialized as a new track.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-processing</head><p>In the post-processing step, missing segmentation masks are added by placing masks at the linearly interpolated positions between t last and t + 1. Furthermore, trajectories of length one without any predecessor and successors are removed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data set</head><p>We conduct our experiments with data from the Cell Tracking Challenge <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b9">9]</ref>. For each cell type, the provided data sets are split into two training sets with publicly available ground truths, and two challenge sets (see <ref type="figure">Fig 7)</ref>. For our experiments, we use selected data from one training set to train models and evaluate on the other. The provided annotations consist of gold truth (GT) instance segmentation masks, interlinked GT cell seeds for cell detection and tracking, and computer-generated instance segmentation masks, referred to as silver truth (ST). The ST annotations, computed from a majority vote of submitted algorithms of former challenge participants, can include segmentation errors. The GT segmentation masks not necessarily include all cells in a frame.</p><p>For four data sets, we manually selected segmentation GTs where all cells in a frame are annotated, and STs that do not show obvious segmentation errors. 27 GTs of the data set BF-C2DL-HSC (Mouse hematopoietic stem cells), 15 GTs of the data set BF-C2DL-MuSC (Mouse muscle stem cells), 16 STs of the data set Fluo-N2DL-HeLa (HeLa cells), and 3 GT slices of the 3D data set Fluo-N3DH-CE (C. elegans developing embryo) fulfilled our requirements. This heterogeneous data set will be referred to as CTC training set and consists of 268 crops of size 256 px?256 px including 52 crops for validation. A difficulty is that the CTC training set contains comparatively few touching cells, whereas in the evaluation the segmentation of touching cells is important, especially for late time points after many cell divisions. Each cell type is evaluated separately using all detection and segmentation GTs of the second set (see <ref type="figure" target="#fig_0">Fig S1)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation criteria</head><p>For evaluation, we use the performance measures of the Cell Tracking Challenge. The normalized acyclic oriented graph matching measure for detection DET is used to evaluate object level segmentation errors <ref type="bibr" target="#b30">[30]</ref>. Pixel level segmentation errors are evaluated with the Jaccard similarity index based measure SEG. The normalized acyclic oriented graph matching measure TRA is used to evaluate the tracking <ref type="bibr" target="#b30">[30]</ref>. The overall performances for the Cell Segmentation Benchmark (CSB) and Cell Tracking Benchmark (CTB) are calculated as follows:</p><formula xml:id="formula_10">OP CSB = 0.5 ? (DET + SEG),<label>(9)</label></formula><p>OP CTB = 0.5 ? (SEG + TRA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Parameter selection</head><p>The proposed segmentation method has three adjustable post-processing parameters: the mask threshold mask , the seed threshold seed and the standard deviation ? of the Gaussian smoothing. We fix them to: mask = 0.09, seed = 0.5, ? = (1.5, 1.5) for 2D/2D+t data, and ? = (1.5, 1.5, 0.5) for 3D/3D+t data. In practice, a fine-tuning of these parameters is only needed if cells are too small or too large (? mask ) and if multiple splits or merges occur (? seed ). For the tracking, we computed cell division and movement statistics from tracking ground truth data and chose the following parameters experimentally: ?t = 3 (dimensionless difference of frames), ? = 0.5, ? = 1.2 and ? = 2 ? D V s t last with the number of image dimensions D ? {2, 3}. The ROI is set to 150 px?150 px for 2D data sets, and to (100 px) 3 for 3D data sets. For some large 3D+t data sets, e.g., Fluo-N3DL-TRIC and Fluo-N3DL-TRIF of the Cell Tracking Challenge, the ROI is reduced to (60 px) <ref type="bibr" target="#b3">3</ref> . Due to the observed variety of the cell division and movement statistics over the different data sets, we expect improved tracking results by fine-tuning the tracking parameters to each data set individually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared segmentation methods</head><p>The proposed segmentation method is compared with boundary and border prediction methods <ref type="figure" target="#fig_0">(Fig 1c, Fig 1d)</ref>, adapted borders <ref type="bibr" target="#b11">[11]</ref>, the Dual U-Net <ref type="bibr" target="#b13">[13]</ref>, and the J4 method proposed in <ref type="bibr" target="#b12">[12]</ref>.</p><p>For the boundary and border prediction methods, we adapt our proposed architecture and use a single decoder path with a three channel output: background, cell, and boundary/border. Instead of the linear activation, the softmax activation is applied in the output layer.</p><p>For the adapted borders <ref type="bibr" target="#b11">[11]</ref>, we use our proposed network architecture with two decoder paths. One decoder path is trained to predict binary cell masks (sigmoid activation), the other to predict background, eroded cells and adapted borders (softmax activation).</p><p>The Dual U-Net method <ref type="bibr" target="#b13">[13]</ref> uses a similar architecture compared to ours but maxpooling layers and a feature fusion block. Intermediate predictions of discrete boundaries (sigmoid activation) and cell distances (linear activation) are forwarded to the feature fusion block which predicts the final segmentation map (sigmoid activation). We removed in our comparison the dropout layer since none of the other compared methods use dropout.</p><p>The last method in our comparison is the J4 method <ref type="bibr" target="#b12">[12]</ref> which uses J regularization to tackle the class imbalance problem. The J4 method predicts a four channel output: background, cell, touching, and gap. We use the same architecture with softmax activation in the last layer as for the boundary and border method. <ref type="table">Table 1</ref>. Boundary and border information in the CTC training set. Stated are the ratios of boundary/border pixels to all pixels. For the proposed neighbor distances only pixels with a value greater than 0.5 are counted in this comparison. Nevertheless, pixels with smaller neighbor distance values can provide information in the training process as well. For the J4 method <ref type="bibr" target="#b12">[12]</ref>, ratios of the touching and of the gap class are provided. Detailed information about the post-processing of the compared methods is provided in the supplementary file S2. Similar to the proposed method, seeds with an area smaller than 3 px 2 are removed for all methods. <ref type="table">Table 1</ref> shows the boundary and border information in the CTC training set. The proposed method can utilize more border information in the training process. For the boundary method the most information is resulting from non-touching cells.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Training settings, inference and experimental environment</head><p>For each method, eleven models are trained. This allows to evaluate the robustness of the training. Models are trained with a batch size of 8 using the Adam optimizer <ref type="bibr" target="#b31">[31]</ref> and the learning rate is initialized with 8 ? 10 For inference, each frame of a time series is min-max normalized independently into the range [-1, 1], whereas the whole volume is normalized for 3D data. The normalized data are processed frame-by-frame with 3D data being processed slice-wise. The CNN model inputs are zero-padded if necessary.</p><p>We performed the experiments using a system with two NVIDIA TITAN RTX GPUs, Ubuntu 18.04, and a Intel Core i9-9900K CPU with 64 GB RAM. The methods are implemented in Python and PyTorch is used as deep learning framework. Implementations of the proposed method and of the compared methods are available at https://bitbucket.org/t scherr/cell-segmentation-and-tracking/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Segmentation results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BF-C2DL-HSC</head><p>The segmentation results of the Mouse hematopoietic stem cells in <ref type="figure" target="#fig_10">Fig 8 show</ref> that the proposed segmentation method provides the best cell detection. The SEG score, which evaluates pixel level errors, is mainly limited due to the fact that the predicted cells in the proposed method are slightly too large as indicated in <ref type="figure" target="#fig_10">Fig 8g.</ref> These results can be even further improved by fine-tuning the mask threshold. Surprisingly, boundaries can be learned almost as good as adapted borders and better than simple borders. A possible explanation is the small amount of touching cells in the CTC training set which prevents from learning simple borders <ref type="figure" target="#fig_10">(Fig 8c)</ref>. The Dual U-Net method suffers from some uncertain regions in the final segmentation map prediction <ref type="figure" target="#fig_10">(Fig 8e top)</ref> resulting in false negatives and split cells. The limitation of the J4 method is that the touching and the gap class are quite similar for this data set. This results in an oversegmentation and imperfect cell shapes since the gap class is considered to be background. The latter limits mainly the SEG score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fluo-N3DH-CE</head><p>For the segmentation of the 3D data set Fluo-N3DH-CE, we do not apply the mentioned splitting of cells that are detected as merged. This enables a better comparison of the methods since the almost binary predictions of the other methods do not allow such a simple splitting post-processing. In addition to the 3D nature of this data set, the low signal-to-noise-ratio and the use of only 3 slices of that cell type in the training set makes the segmentation difficult. Again, the proposed method shows the best results whereas boundaries are often unsharp and not closed resulting in merged cells as shown in <ref type="figure" target="#fig_11">Fig 9.</ref> Borders and adapted borders do not appear anymore and cannot be used to split cells. Especially in late frames after many cell divisions, the boundary and border segmentation methods break down and predict only a few very large objects. In contrast, the J4 method shows an improved splitting of touching cells. However, also the J4 method and the Dual U-Net method decrease in segmentation performance in late frames whereas the proposed method still provides a reasonable segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fluo-N2DL-HeLa</head><p>HeLa cells provide the largest quantity of cells from a specific cell type in the CTC training set resulting in the methods performing more similar, as shown in <ref type="figure" target="#fig_0">Fig 10.</ref> For this cell type, the adapted border method shows its advantages over the boundary method by predicting robust borders. The models trained with the J4 method learned to predict and differentiate gap and touching class for this cell type very well resulting in the best performance of all methods. However, the proposed method performs also well for this cell type. The Dual U-Net method suffers from merged objects. This is probably due to non-closed boundaries which induce the merging of cells in the feature fusion layer. Our approach with more robust neighbor distances and a traditional post-processing avoids this.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BF-C2DL-MuSC</head><p>Mouse muscle stem cells are difficult to segment since they change their shape from small roundish objects to elongated objects. Both cell states are shown in <ref type="figure" target="#fig_0">Fig 11.</ref> The Dual U-Net method provides the best segmentation of elongated cells, however, roundish cells are sometimes merged. Nevertheless, the better segmentation of the elongated cells compensates this. The J4 method in contrast suffers from oversegmentation on this cell type, resulting in lower scores. As for the data set BF-C2DL-HSC, the proposed method can handle the small roundish cells well resulting in the second best method for this cell type. The segmentation problem of the elongated cells can be solved using the training data of both BF-C2DL-MuSC training data subsets. This is shown in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cell Tracking Challenge</head><p>For our submission to the 5th IEEE ISBI 2020 Cell Tracking Challenge as team KIT-Sch-GE, we combined our segmentation method with our adapted tracking approach. We selected a training data set similarly to the CTC training set. The data set consists of 997 crops of size 256 px?256 px of carefully selected Cell Tracking Challenge data, CBIA      HL60 cell line data <ref type="bibr" target="#b32">[32]</ref>, BBBC038 drosophila images <ref type="bibr" target="#b4">[4]</ref>, and generated semi-synthetic data <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b34">34]</ref>. A more detailed description of the data set, data set specific segmentation and tracking parameters, and executables can be found on the challenge website. For our submission we manually selected a segmentation model from three trained models. To avoid issues with the TRA measure, frames without any tracked object are replaced by the tracking result of the temporally closest frame. For the Fluo-N3DH-CE data set cells are split if their volume is bigger than <ref type="bibr" target="#b4">4</ref> 3 times the mean object volume at that time point by iteratively increasing the seed extraction threshold seed .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cell Segmentation Benchmark</head><p>In the Cell Segmentation Benchmark, we achieved eight top three rankings, including two first places, and three fourth places, all of them with the same model (see <ref type="table" target="#tab_2">Table 2</ref>). The performances of the highlighted data sets with no or almost no training data used imply a good utilization of training data and a good generalization ability of our model. A comparison of the scores for the data set BF-C2DL-MuSC on the CSB benchmark and the previous experiment in <ref type="figure" target="#fig_0">Fig 11</ref> shows an improved performance. We assume this is due to the larger amount of elongated cells in the training data. Furthermore, the results show that our proposed 2D segmentation with 3D post-processing approach performs well on 3D data. An exemplary segmentation result is shown in <ref type="figure" target="#fig_0">Fig 12.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Cell Tracking Benchmark</head><p>In the Cell Tracking Benchmark, we achieved nine top three rankings, including two first places, and a fourth place (see <ref type="table" target="#tab_2">Table 2</ref>). Exemplarily, some tracking results of our approach are shown in <ref type="figure" target="#fig_0">Fig 13.</ref> Some tracks show jumps, visible as long straight lines, possibly due to some remaining linking errors in our adapted tracking approach. However, none of the competing tracking approaches yields perfect tracking results for all cells. The multiple top performances in the Cell Tracking Benchmark show that our tracking approach, which combines a movement estimation and a graph-based matching show that cells can be segmented well even on this challenging data set. strategy, belongs to the best performing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>Trained on a data set with few touching samples, our proposed segmentation method outperforms all compared methods for at least three of the four cell types evaluated. This is due to the fact that the neighbor distances enable our method to learn from close cells which results in additional information in the training process (see <ref type="table">Table 1</ref>) and the fact that this information can be easily combined with the cell distances. The differences between the segmentation results of both discrete border methods show how important the utilization of border information is. We want to emphasize that for the proposed method the seed and the mask thresholds can be adjusted for each cell type and for each trained model separately. This improves the segmentation results shown in <ref type="figure" target="#fig_0">Fig 8 -11</ref>. The other discrete methods do not allow to do so since the needed sigmoid or softmax activation functions prevent a major fine-tuning of the post-processing. However, to allow a better comparison, we fixed the post-processing parameters of our proposed method in our experiments. The results of the J4 method on the HeLa data set show that specialized loss functions work very well, at least for the dominating cell type in the training set. So far, our approach only uses standard loss functions.</p><p>Our successful participation in the Cell Segmentation Benchmark and the Cell Tracking Benchmark show that our proposed tracking by detection method yields excellent results in cell segmentation and cell tracking. Especially the success on data sets with only little or very sparse annotated training data, i.e., data with only very few cells in a frame annotated, shows the advantages of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>The segmentation and tracking of touching and dividing cells of different types is a challenging task. In this work, a new cell segmentation method using a combination of cell distances and neighbor distances is proposed. The segmentation method utilizes information from touching and close cells in the training process. Therefore, it shows an improved generalization ability for cell types underrepresented or absent in the training data set compared to border and boundary prediction methods. This advantage enables to segment even cell types with no or almost no annotated training data available. Our success in the Cell Segmentation Benchmark emphasizes the strengths of our segmentation method. Our adapted tracking algorithm, which uses a movement estimation with a graph-based matching strategy, can handle cell divisions and missing segmentation masks over a short sequence of frames. The combination of the tracking with our proposed segmentation method resulted in top performances at the Cell Tracking Benchmark.</p><p>As future research, we plan to further improve the segmentation performance using a larger and on ImageNet pre-trained encoder or mixed convolution blocks <ref type="bibr" target="#b35">[35]</ref>, testtime augmentation <ref type="bibr" target="#b36">[36]</ref>, and the synthetic generation of new training samples <ref type="bibr" target="#b16">[16]</ref>. In addition, studies about how cell features, e.g., size, shape and texture, influence the generalization ability to new cell types are needed. A long-term goal is to develop a user-friendly-software for the segmentation and tracking of a large variety of cell types using a well-trained segmentation model. Including tunable post-processing parameters facilitates an adaptation of the cell and neighbor distances to new data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fluo-N2DL-HeLa Test Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fluo-N3DH-CE Test Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BF-C2DL-HSC Test Set BF-C2DL-HSC</head><p>Training GT Images 02</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BF-C2DL-HSC Test Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BF-C2DL-MuSC Test Set BF-C2DL-MuSC</head><p>Training GT Images 02 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BF-C2DL-MuSC Test Set</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Boundary method</head><p>The boundary method (BM) predicts a three channel output (background, cell, boundary). Activation function is the softmax activation and a weighted sum of Dice loss (channel-wise) and cross entropy loss is used to train this method. For the watershed segmentation binary masks P BM,mask and seeds P BM,seeds are extracted from the single channels P BM ( ) with ? {background, cell, boundary}:</p><formula xml:id="formula_12">BM,mask = { 1 if arg max BM ( ) = cell , 0 else ,<label>(s1)</label></formula><formula xml:id="formula_13">P BM,seeds = P BM (cell) ? [ 1 ? P BM (boundary) ] &gt; 0.5 .<label>(s2)</label></formula><p>BM,mask represents the value of P BM,mask at pixel ( , ). The resulting mask is equal to one at pixels with maximum prediction in the cell channel. For 3D data, Eq. s1 needs to be adapted to the voxel coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Border method</head><p>The border method (BoM) predicts a three channel output (background, cell, border). Activation function is the softmax activation and a weighted sum of Dice loss (channel-wise) and cross entropy loss is used to train this method. For the watershed segmentation binary masks P BoM,mask and seeds P BoM,seeds are extracted from the single channels P BoM ( ) with ? {background, cell, border}: </p><p>BoM,mask represents the value of P BoM,mask at pixel ( , ). The resulting mask is equal to one at pixels with maximum prediction in the cell channel. For 3D data, Eq. s3 needs to be adapted to the voxel coordinates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adapted border method</head><p>The adapted border method (ABoM) predicts a single channel output (cell) used for the mask creation and a three channel output (background, eroded cell, adapted border) used for seed detection. The sigmoid activation function is applied to the single channel cell output and the softmax activation to the three channel output. A weighted sum of Dice loss and cross entropy loss is used to train both outputs. The total loss function is the sum of the loss functions of each output. For the watershed segmentation binary masks P ABoM,mask and seeds P ABoM,seeds are extracted from the single channels P ABoM ( ) with ? {cell, background, eroded cell, adapted border}: </p><formula xml:id="formula_15">P</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dual U-Net method</head><p>The Dual-U-Net method predicts a single channel cell distance output (linear activation), a single channel boundary output (sigmoid activation) and a single channel segmentation map output (sigmoid activation). To learn the cell distances a mean square error loss is used, and for each other output a weighted sum of Dice loss and cross entropy loss. The total loss function is a weighted sum of the three losses. The weighting is needed to avoid that only one loss is minimized and was adjusted to the CTC training set. The intermediate cell distance and boundary outputs are only needed in the training process. For inference, only the segmentation output based on the intermediate predictions is used. In the corresponding publication, no specific post-processing is mentioned. However, we applied a simple morphological erosion after thresholding the segmentation map which reduces the merging of cells in some cases. To get the initial cell shapes we applied a watershed segmentation to the thresholded segmentation map with the eroded cells as seeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J4 method</head><p>The J4 method predicts a four channel output (background, cell, touching, gap). As activation function softmax activation is used. To train the network, a weighted sum of cross entropy loss and regularization loss is used as loss function. Instead of using the cross entropy loss definition of the corresponding publication, we used a PyTorch implementation which is numerically more stable. Since the regularization loss was much larger than the cross entropy loss on our data (regardless of the cross entropy implementation) we scaled it in a way that the regularizaton loss was the dominating loss part after minimizing the cross entropy loss, as suggested in the corresponding publication.</p><p>For the watershed segmentation binary masks P J4,mask and seeds P J4,seeds are extracted from the single channels P J4 ( ) with ? {background, cell, touching, gap} </p><p>J4,mask represents the value of P J4,mask at pixel ( , ). The resulting mask is equal to one at pixels with maximum prediction in the cell channel or in the touching channel (touching class can only occur within cells, gaps within background). For 3D data, Eq. s7 needs to be adapted to the voxel coordinates.</p><p>We also tested to just use the arg max of the cell class but this resulted in some additional merged cells. Since merged cells are penalized stronger by the metric than false positives/splits the method benefits from our post-processing in our comparison.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig 1 .</head><label>1</label><figDesc>Training data representations for the training of deep learning models. Image (a) and ground truth (b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Fig 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig 2. Overview of the proposed segmentation method using distance predictions (adapted from [13]). The CNN consists of a single encoder that is connected to both decoder paths. The network is trained to predict cell distances and neighbor distances that are used for the watershed-based post-processing. The input image shows a crop of the Cell Tracking Challenge data set Fluo-N2DH-GOWT1 [7, 9].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig 3 .</head><label>3</label><figDesc>Main steps of the neighbor distance creation. After the automated selection of a cell (a), indicated with red, the selected cell and the background are converted to foreground (white in b) while the other cells are converted to background (black in b). Then, the distance transform is calculated (c), cut to the cell region and normalized (d). After inversion (e), the steps are repeated for the remaining cells (f). Finally, the grayscale closed neighbor distances (g) are scaled (h). Shown is a crop of the Broad Bioimage Benchmark Collection data set BBBC039v1<ref type="bibr" target="#b26">[26]</ref>.Fig 3 showsthe generation of the neighbor distances in which each pixel of a cell represents the inverse normalized distance to the nearest pixel of the closest neighboring cell. Therefore, a background-foreground conversion step is applied for each cell independently(Fig 3b)and the Euclidean distance transform</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Fig 3h (bottom right cell and bottom left cell) and is especially advantageous for training data sets with few touching cells providing only little border information in the training process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig 4 .</head><label>4</label><figDesc>Robustness of training data representations to annotation inconsistencies. Small changes in the ground truth, simulated with morphological erosions and dilations, result in different boundaries and borders (first and second row).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig 5 .</head><label>5</label><figDesc>Overview of the watershed post-processing for segmentation. The post-processing consists of a threshold-based seed extraction and mask creation, and a watershed. The predictions show a 2D crop of the Cell Tracking</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>? 4 .</head><label>4</label><figDesc>After 12 subsequent epochs without validation loss improvement, the learning rate is multiplied by 0.25 till a minimum learning rate of 6 ? 10 ?5 is reached. The training is stopped when 28 subsequent epochs without improvement occurred or 200 epochs are reached. To learn the cell distances and the neighbor distances, PyTorch's SmoothL1Loss is used and both losses are added. The loss functions used to train the compared methods are provided in the supplementary file S2. During training the augmentations flipping (probability: 75 %), scaling (30 %), rotation (30 %), contrast changing (30 %), blurring (30 %), and noise (30 %) are applied randomly in this order, and the training images are min-max normalized into the range [-1, 1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig 8 .</head><label>8</label><figDesc>Segmentation results on the BF-C2DL-HSC test set. Shown are raw predictions and segmentations of a 140 px?140 px test image crop (a-g, best OP CSB models). For multi-channel outputs, channels are color-coded (cell/seed class: white, boundary/border/touching class: red, gap class: blue). The plot at the bottom shows the evaluation on the test set (h).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig 9 .</head><label>9</label><figDesc>Segmentation results on the Fluo-N3DH-CE test set. Shown are raw predictions and segmentations of a 140 px?140 px test image crop (a-g, best OP CSB models). For multi-channel outputs, channels are color-coded (cell/seed class: white, boundary/border/touching class: red, gap class: blue). The plot at the bottom shows the evaluation on the test set (h). Note: this is a 3D data set and the erroneous merging of cells can result from any of the slices a cell appears.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig 10 .</head><label>10</label><figDesc>Segmentation results on the Fluo-N2DL-HeLa test set. Shown are raw predictions and segmentations of a 140 px?140 px test image crop (a-g, best OP CSB models). For multi-channel outputs, channels are color-coded (cell/seed class: white, boundary/border/touching class: red, gap class: blue). The plot at the bottom shows the evaluation on the test set (h).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig 11 .</head><label>11</label><figDesc>Segmentation results on the BF-C2DL-MuSC test set. Shown are raw predictions and segmentations of a 360 px?360 px test image crop (a-g, best OP CSB models). For multi-channel outputs, channels are color-coded (cell/seed class: white, boundary/border/touching class: red, gap class: blue). The plot at the bottom shows the evaluation on the test set (h).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig 12 .</head><label>12</label><figDesc>Segmentation result of the Fluo-N3DH-CE challenge data. The maximum intensity projection of the raw data (left) and of the segmentation (right)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Fig 13 .</head><label>13</label><figDesc>Tracking results on the Fluo-N2DL-HeLa challenge data. The first raw image is overlaid with the tracks starting in the first frame. For better visibility, tracks starting in later frames are excluded.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig S1 .1</head><label>S1</label><figDesc>Training and test data set collection. The ground truths of the challenge sets from the Cell Tracking Challenge are not publicly available. Thus, the two training data sets are split into a training set and cell type specific test sets for our segmentation experiments. For the training data set only fully annotated segmentation GTs and good quality STs can be used to train models well. For evaluation, all segmentation and detection GTs can be used.Supplementary file S2: post-processing and loss functions of the compared methods Tim Scherr 1 *, Katharina L?ffler 1,2 , Moritz B?hland 1 , Ralf Mikut 1 Institute for Automation and Applied Informatics, Karlsruhe Institute of Technology, Eggenstein-Leopoldshafen, Germany, 2 Institute of Biological and Chemical Systems -Biological Information Processing, Karlsruhe Institute of Technology, Eggenstein-Leopoldshafen, Germany * tim.scherr@kit.edu This supplementary file describes the post-processing of the compared methods in detail. The compared methods and our proposed method are based on a seed and mask extraction which are used for a watershed segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>BoM,mask = { 1 P</head><label>1</label><figDesc>if arg max BM ( ) = cell , BoM,seeds = P BoM (cell) ? [ 1 ? P BoM (border) ] &gt; 0.5 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>ABoM,mask = P ABoM (cell) &gt; 0.5, (s5) P ABoM,seeds = P ABoM (eroded cell) ? [ 1 ? P ABoM (adapted border) ] &gt; 0.5.(s6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>arg max J4 ( ) = cell , 1 if arg max J4 ( ) = touching , 0 else , (s7) P J4,seeds = P J4 (cell) ? [ 1 ? P J4 (touching) ] ? [ 1 ? P J4 (gap) ] &gt; 0.5 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Cell Tracking Benchmark and Cell Segmentation Benchmark results (5th edition). Top 3 rankings in the overall performances OP CSB and OP CTB are written in bold. The corresponding Cell Tracking Benchmark and Cell Segmentation Benchmark leaderboards are available on the Cell Tracking Challenge website. No data of that cell type used to train the segmentation model. ? ? ? 5 slices of that cell type used to train the segmentation model. Two scores for each benchmark (CSB/CTB) due to a different treatment of additionally detected and segmented cells.</figDesc><table><row><cell>Data Set</cell><cell>SEG</cell><cell>DET</cell><cell>TRA</cell><cell>OP CSB</cell><cell>OP CTB</cell><cell>Ranking OP CSB</cell><cell>Ranking OP CTB</cell></row><row><cell>BF-C2DL-HSC</cell><cell>0.750</cell><cell>0.974</cell><cell>0.929</cell><cell>0.862</cell><cell>0.840</cell><cell>2nd</cell><cell>3rd</cell></row><row><cell>BF-C2DL-MuSC</cell><cell>0.702</cell><cell>0.977</cell><cell>0.967</cell><cell>0.839</cell><cell>0.835</cell><cell>1st</cell><cell>1st</cell></row><row><cell>Fluo-C3DH-H157  ?</cell><cell>0.789</cell><cell>0.949</cell><cell>0.948</cell><cell>0.869</cell><cell>0.869</cell><cell>4th</cell><cell>4th</cell></row><row><cell>Fluo-C3DL-MDA231  ? ?</cell><cell>0.616</cell><cell>0.851</cell><cell>0.820</cell><cell>0.733</cell><cell>0.718</cell><cell>3rd</cell><cell>3rd</cell></row><row><cell>Fluo-N2DH-GOWT1</cell><cell>0.828</cell><cell>0.950</cell><cell>0.949</cell><cell>0.889</cell><cell>0.889</cell><cell>14th</cell><cell>12th</cell></row><row><cell>Fluo-N2DL-HeLa</cell><cell>0.895</cell><cell>0.992</cell><cell>0.989</cell><cell>0.944</cell><cell>0.942</cell><cell>3rd</cell><cell>3rd</cell></row><row><cell>Fluo-N3DH-CE  ? ?</cell><cell>0.729</cell><cell>0.930</cell><cell>0.886</cell><cell>0.830</cell><cell>0.808</cell><cell>1st</cell><cell>1st</cell></row><row><cell>Fluo-N3DH-CHO</cell><cell>0.871</cell><cell>0.945</cell><cell>0.948</cell><cell>0.908</cell><cell>0.909</cell><cell>3rd</cell><cell>3rd</cell></row><row><cell>Fluo-N3DL-DRO</cell><cell>0.562</cell><cell>0.761</cell><cell>-</cell><cell>0.661</cell><cell>-</cell><cell>4th</cell><cell>-</cell></row><row><cell>Fluo-N3DL-TRIC  ?</cell><cell>0.821 / 0.766  *</cell><cell>0.961</cell><cell>0.809</cell><cell>0.891</cell><cell>0.787</cell><cell>2nd</cell><cell>2nd</cell></row><row><cell>Fluo-N3DL-TRIF  ?</cell><cell>0.601 / 0.573  *</cell><cell>0.926</cell><cell>0.788</cell><cell>0.763</cell><cell>0.680</cell><cell>3rd</cell><cell>3rd</cell></row><row><cell>Fluo-N2DH-SIM+</cell><cell>0.800</cell><cell>0.949</cell><cell>0.945</cell><cell>0.875</cell><cell>0.873</cell><cell>9th</cell><cell>7th</cell></row><row><cell>Fluo-N3DH-SIM+</cell><cell>0.668</cell><cell>0.937</cell><cell>0.933</cell><cell>0.802</cell><cell>0.800</cell><cell>4th</cell><cell>2nd</cell></row><row><cell>?</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>*</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">September 26, 2020  </note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Andreas Bartschat for proofreading, the organizers of the Cell Tracking Challenge, and all data set providers.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author contributions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>We are grateful for funding by the Helmholtz Association in the programs BioInterfaces in Technology and Medicine (TS, RM) and the Helmholtz Information &amp; Data Science</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing interests</head><p>The authors have declared that no competing interests exist.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The funders had no role in study design, data collection and analysis, decision to publish, or preparation of the manuscript</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">(</forename><surname>School For Health</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Kl</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wholeanimal functional and developmental imaging with isotropic spatial resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Chhetri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Amat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>H?ckendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Lemon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Keller</surname></persName>
		</author>
		<idno type="DOI">10.1038/nmeth.3632</idno>
	</analytic>
	<monogr>
		<title level="j">Nat Methods</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1171" to="1178" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An ensemble-averaged, cell density-based digital model of zebrafish embryo development derived from light-sheet microscopy data with single-cell resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Kobitski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Otte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Takamiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?fer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mertes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stegmaier</surname></persName>
		</author>
		<idno type="DOI">10.1038/srep08601</idno>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">8601</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Reconstructing embryonic development</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Khairy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Keller</surname></persName>
		</author>
		<idno type="DOI">10.1002/dvg.20698</idno>
	</analytic>
	<monogr>
		<title level="j">Genesis</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="488" to="513" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Nucleus segmentation across imaging experiments: the 2018 Data Science Bowl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>Karhohs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Cimini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ackerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Haghighi</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41592-019-0612-7</idno>
	</analytic>
	<monogr>
		<title level="j">Nat Methods</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1247" to="1253" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">EmbryoMiner: a new framework for interactive knowledge discovery in large-scale cell tracking data of developing embryos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Traub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schlagenhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Takamiya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Antritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bartschat</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pcbi.1006128</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multi-view light-sheet imaging and tracking with the MaMuT software reveals the cell lineage of a direct developing arthropod limb. eLife</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Tinevez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pietzsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stamataki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Harich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guignard</surname></persName>
		</author>
		<idno type="DOI">10.7554/eLife.34410</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">34410</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">An objective comparison of cell-tracking algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ma?ka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Haubold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Harder</surname></persName>
		</author>
		<idno type="DOI">10.1038/nmeth.4473</idno>
	</analytic>
	<monogr>
		<title level="j">Nat Methods</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1141" to="1152" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast segmentation of stained nuclei in terabyte-scale, time resolved 3d microscopy image stacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stegmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Otte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kobitski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bartschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">U</forename><surname>Nienhaus</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0090036</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A benchmark for comparison of cell tracking algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ma?ka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ederra</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btu080</idno>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1609" to="1617" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cell segmentation methods for label-free contrast microscopy: review and comprehensive comparison</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vicar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Balvan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jaros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jug</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kolar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Masarik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gumulec</surname></persName>
		</author>
		<idno type="DOI">10.1186/s12859-019-2880-8</idno>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page">360</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Best practices in deep learning-based segmentation of microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Scherr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bartschat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reischl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stegmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mikut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 28. Workshop Computational Intelligence</title>
		<meeting>28. Workshop Computational Intelligence<address><addrLine>Dortmund, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="175" to="195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">J regularization improves imbalanced multiclass segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fag</forename><surname>Pe?a</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pdm</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">T</forename><surname>Tarr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">I</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Meyerowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 17th International Symposium on Biomedical Imaging (ISBI); 2020</title>
		<imprint>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual U-Net for the segmentation of overlapping glioma nuclei</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2019.2924744</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="84040" to="84052" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate nuclear segmentation with center vector encoding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inf Process Med Imaging</title>
		<imprint>
			<biblScope unit="page" from="394" to="404" />
			<date type="published" when="2019" />
			<publisher>Springer International Publishing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hover-Net: simultaneous segmentation and classification of nuclei in multi-tissue histology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">D</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sea</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Azam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwak</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2019.101563</idno>
	</analytic>
	<monogr>
		<title level="j">Med Image Anal</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">101563</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">nucleAIzer: a parameter-free deep learning framework for nucleus segmentation using image style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hollandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szkalisity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tasnadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Molnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mathe</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cels.2020.04.003</idno>
	</analytic>
	<monogr>
		<title level="j">Cell Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="453" to="458" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep learning in video multi-object tracking: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ciaparrone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">L</forename><surname>S?nchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tabik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Troiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tagliaferri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Herrera</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2019.11.023</idno>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">381</biblScope>
			<biblScope unit="page" from="61" to="88" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video object segmentation and tracking: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.1145/3391743</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans Intell Syst Technol</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Segmenting and tracking cell instances with cosine embeddings and recurrent hourglass networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Payer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>?tern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Urschler</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2019.06.015</idno>
	</analytic>
	<monogr>
		<title level="j">Med Image Anal</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="106" to="119" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cell tracking using deep neural networks with multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.imavis.2016.11.010</idno>
	</analytic>
	<monogr>
		<title level="j">Image Vis Comput</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="142" to="153" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Global linking of cell tracks using the Viterbi algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keg</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jalden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Blau</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2014.2370951</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Med Imaging</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="911" to="929" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">CVPR19 Tracking and Detection Challenge: how crowded can it get?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dendorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rezatofighi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1906.04567" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">The 2017 DAVIS Challenge on video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1704.00675" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">YouTube-VOS: a large-scale video object segmentation benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1809.03327" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Coupling cell detection and tracking by temporal feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sixta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seebach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schnittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Flach</surname></persName>
		</author>
		<idno type="DOI">10.1007/s00138-020-01072-7</idno>
	</analytic>
	<monogr>
		<title level="j">Mach Vis Appl</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Annotated high-throughput microscopy image sets for validation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ljosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Sokolnicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Carpenter</surname></persName>
		</author>
		<idno type="DOI">10.1038/nmeth.2083</idno>
	</analytic>
	<monogr>
		<title level="j">Nat Methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">637</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">U-Net: convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<editor>Navab N, Hornegger J, Wells WM, Frangi AF</editor>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The phase correlation image alignment method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Kuglin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Hines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Cybernetics and Society</title>
		<meeting>the IEEE International Conference on Cybernetics and Society</meeting>
		<imprint>
			<date type="published" when="1975" />
			<biblScope unit="page" from="163" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Coupled minimum-cost flow cell tracking for high-throughput quantitative analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Padfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rittscher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roysam</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2010.07.006</idno>
	</analytic>
	<monogr>
		<title level="j">Med Image Anal</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="650" to="668" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Cell tracking accuracy measurement based on comparison of acyclic oriented graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ma?ka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">V</forename><surname>Sorokin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Matula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ortiz-De Sol?rzano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kozubek</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0144959</idno>
	</analytic>
	<monogr>
		<title level="j">PLOS ONE</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adam: a method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generation of digital phantoms of cell nuclei and simulation of image formation in 3d image cytometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kozubek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stejskal</surname></persName>
		</author>
		<idno type="DOI">10.1002/cyto.a.20714</idno>
	</analytic>
	<monogr>
		<title level="j">Cytometry A</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="494" to="509" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">New methods to improve large-scale microscopy image analysis with prior knowledge and uncertainty</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stegmaier</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<pubPlace>ME, KIT, Karlsruhe, GER. Karlsruhe, GER</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Generating semi-synthetic validation benchmarks for embryomics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stegmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Arz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Otte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kobitski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">U</forename><surname>Nienhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 13th International Symposium on Biomedical Imaging ISBI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="684" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Segmentation of cell images based on improved deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2020.3001571</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2169" to="3536" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Test-time augmentation for deep learning-based cell segmentation on microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Moshkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kertesz-Farkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hollandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Horvath</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41598-020-61808-3</idno>
	</analytic>
	<monogr>
		<title level="j">Sci Rep</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5068</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
