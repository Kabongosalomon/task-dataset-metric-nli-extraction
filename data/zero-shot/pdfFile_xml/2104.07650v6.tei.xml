<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction ACM Reference Format</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>April 25-29, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
							<email>xiang_chen@zju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
							<email>zhangningyu@zju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
							<email>chuanqi.tcq@alibaba-inc.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
							<email>f.huang@alibaba-inc.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
							<email>luo.si@alibaba-inc.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
							<email>huajunsir@zju.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhi</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">AZFT Joint Lab for Knowledge Engine</orgName>
								<orgName type="institution" key="instit1">Zhejiang University</orgName>
								<orgName type="institution" key="instit2">Hangzhou Innovation Center Hangzhou</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">Zhejiang University AZFT Joint Lab for Knowledge Engine Hangzhou Innovation Center Hangzhou</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="laboratory">Zhejiang University AZFT Joint Lab for Knowledge Engine Hangzhou Innovation Center Hangzhou</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Alibaba Group Hangzhou</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Alibaba Group Hangzhou</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="laboratory">Zhejiang University AZFT Joint Lab for Knowledge Engine Hangzhou Innovation Center Hangzhou</orgName>
								<address>
									<settlement>Zhejiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction ACM Reference Format</title>
					</analytic>
					<monogr>
						<title level="j" type="main">Virtual Event</title>
						<meeting> <address><addrLine>Lyon, France</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">April 25-29, 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3485447.3511998</idno>
					<note>ACM ISBN 978-1-4503-9096-5/22/04. . . $15.00. 2022. KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. In Proceedings of the ACM Web Conference 2022 (WWW &apos;22), April 25-29, 2022, Virtual Event, Lyon, France. ACM, New York, NY, USA, 11 pages. https: //</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS ? Computing methodologies ? Information extraction KEYWORDS Relation Extraction</term>
					<term>Prompt-tuning</term>
					<term>Knowledge-aware * Corresponding author 1 https://githubcom/zjunlp/KnowPrompt</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, prompt-tuning has achieved promising results for specific few-shot classification tasks. The core idea of prompt-tuning is to insert text pieces (i.e., templates) into the input and transform a classification task into a masked language modeling problem. However, for relation extraction, determining an appropriate prompt template requires domain expertise, and it is cumbersome and timeconsuming to obtain a suitable label word. Furthermore, there exists abundant semantic and prior knowledge among the relation labels that cannot be ignored. To this end, we focus on incorporating knowledge among relation labels into prompt-tuning for relation extraction and propose a Knowledge-aware Prompt-tuning approach with synergistic optimization (KnowPrompt). Specifically, we inject latent knowledge contained in relation labels into prompt construction with learnable virtual type words and answer words. Then, we synergistically optimize their representation with structured constraints. Extensive experimental results on five datasets with standard and low-resource settings demonstrate the effectiveness of our approach. Our code and datasets are available in GitHub 1 for reproducibility.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Relation Extraction (RE) aims to extract structured knowledge from unstructured text and plays a critical role in information extraction and knowledge base construction. RE appeals to many researchers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b45">45,</ref><ref type="bibr" target="#b50">50,</ref><ref type="bibr" target="#b58">58,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b62">62]</ref> due to the capability to extract textual information and benefit many web applications, e.g., information retrieval, web mining, and question answering.</p><p>Previous self-supervised pre-trained language models (PLMs) such as BERT <ref type="bibr" target="#b10">[10]</ref> have achieved state-of-the-art (SOTA) results in lots of RE benchmarks. However, since fine-tuning requires adding extra classifiers on top of PLMs and further training the models under classification objectives, their performance heavily depends on time-consuming and labor-intensive annotated data, making it hard to generalize well. Recently, a series of studies using prompt-tuning <ref type="bibr" target="#b11">[11,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b27">27,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b44">44]</ref> to address this issue: adopting the pre-trained LM directly as a predictor by completing a cloze task to bridge the gap between pre-training and fine-tuning. Prompt-tuning fuses the original input with the prompt template to predict [MASK] and then maps the predicted label words to the corresponding class sets, which has induced better performances for PLMs on few-shot tasks. As shown in <ref type="figure" target="#fig_0">Figure 1 (a)</ref>, a typical prompt for text classification consists of a template (e.g. "&lt; 1 &gt; It is [MASK] ") and a set of label words ("great", "terrible"etc.) as candidates to predict <ref type="bibr">[MASK]</ref>. PLMs predict ("great", "terrible", etc.) at the masked position to determine the label of the sentence "&lt; 1 &gt;". In a nutshell, prompt-tuning involves template engineering and verbalizer engineering, which aims to search for the best template and an answer space <ref type="bibr" target="#b35">[35]</ref>.</p><p>Despite the success of prompt-tuning PLMs for text classification tasks, there are still several non-trivial challenges for RE with arXiv:2104.07650v6 [cs.CL] 23 Jan 2022 <ref type="bibr">[CLS]</ref> Hamilton is the first British champion.</p><p>[SEP]</p><p>[MASK]  prompt-tuning as follows: on the one hand, determining the appropriate prompt template for RE requires domain expertise, and autoconstructing a high-performing prompt with input entities often requires additional computation cost for generation and verification <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b42">42,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b46">46]</ref>; on the other hand, the computational complexity of the label word search process is very high (e.g., usually exponentially depending on the number of categories) when the length of the relation label varies, and it is non-trivial to obtain a suitable target label word in the vocabulary to represent the specific relation label. For example, the relation labels of : _ _ ? and : _ _? cannot specify a single suitable label word in the vocabulary. In addition, there exists rich semantic knowledge among relation labels and structural knowledge implications among relational triples, which cannot be ignored. For example, as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (b) and (c), if a pair of entities contains the semantics of "person" and "country", the prediction probability of the [MASK] on the relation "org:city_of_headquarters" will be lower. Conversely, the relation also restricts the types of its subject and object entity. Previous studies <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">13,</ref><ref type="bibr" target="#b33">33]</ref> indicate that incorporating the relational knowledge will provide evidence for RE.</p><p>To address those issues, we take the first step to inject knowledge into learnable prompts and propose a novel Knowledge-aware Prompt-tuning with synergistic optimization (KnowPrompt) approach for RE. We construct prompt with knowledge injection via learnable virtual answer words and virtual type words to alleviate labor-intensive prompt engineering ( ?4.1). To be specific, instead of a regular verbalizer that mapping from one label word in vocabulary to the particular class, we creatively propose to leverage learnable virtual answer words by injecting in semantic knowledge to represent relation labels. Furthermore, we assign learnable virtual type words surrounding entities to hold the role of weakened Type Marker <ref type="bibr" target="#b66">[66]</ref>, which are initialized with prior knowledge maintained in relation labels. Notably, we innovatively leverage learnable virtual type words to dynamically adjust according to context rather than utilizing annotation of the entity type, which may not be available in datasets. Since there exist implicit structural constraints among entities and relations, and virtual words should be consistent with the surrounding contexts, we introduce synergistic optimization to obtain optimized virtual type and answer words ( ?4.2). Concretely, we propose a context-aware prompt calibration method with implicit structural constraints to inject structural knowledge implications among relational triples and associate prompt embeddings with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Relation Extraction</head><p>Relation Extraction (RE) involves extracting the relation between two given entities based on their related context, which plays an essential task in information extraction and knowledge base construction. Early approaches involve pattern-based methods <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b29">29]</ref>, CNN/RNN-based <ref type="bibr" target="#b56">[56,</ref><ref type="bibr" target="#b62">62,</ref><ref type="bibr" target="#b65">65]</ref> and graph-based methods <ref type="bibr" target="#b19">[19,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b61">61]</ref>. With the recent advances in pre-trained language models <ref type="bibr" target="#b10">[10]</ref>, applying PLMs as the backbone of RE systems <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b48">48,</ref><ref type="bibr" target="#b53">53,</ref><ref type="bibr" target="#b57">57,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b67">67]</ref> has become standard procedure. Several studies have shown that BERT-based models significantly outperform both RNN and graph-based models <ref type="bibr" target="#b30">[30,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b54">54]</ref>. Meanwhile, a series of knowledgeenhanced PLMs have been further explored, which use knowledge bases as additional information to enhance PLMs. Among them, MTB <ref type="bibr" target="#b2">[3]</ref> propose matching the blanks based on BERT, which is a REoriented pre-trained method to learn relational patterns from text. SPANBERT <ref type="bibr" target="#b30">[30]</ref> adopt knowledge to enhance learning objectives, KNOWBERT <ref type="bibr" target="#b38">[38]</ref> propose to incorporate knowledge into input features, and LUKE <ref type="bibr" target="#b52">[52]</ref> leverage knowledge to improve model architectures. We compare with this line of work here for their promotion comes from relational knowledge of external sources. In contrast to them, we focus on learning from the text itself in the paper. Recently, Xue et al. <ref type="bibr" target="#b51">[51]</ref> propose a multi-view graph based on BERT, achieving SOTA performance both on TACRED-Revisit <ref type="bibr" target="#b0">[1]</ref> and DialogRE <ref type="bibr" target="#b54">[54]</ref>. Thus, we also choose the latest graph methods based on BERT for RE as our baselines to demonstrate the effectiveness of our KnowPrompt. Some previous studies <ref type="bibr" target="#b9">[9]</ref> have focused on the few-shot setting since available annotated instances may be limited in practice. Dong et al. <ref type="bibr" target="#b14">[14]</ref>, Gao et al. <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b17">17]</ref>, Han et al. <ref type="bibr" target="#b23">[23]</ref>, Qu et al. <ref type="bibr" target="#b39">[39]</ref>, Yu et al. <ref type="bibr" target="#b55">[55]</ref> propose approaches for few-shot RE based on meta-learning or metric learning, with the aim of developing models that can be trained with only a few labeled sentences and nonetheless generalize well. In contrast to previous N-way K-shot approaches, Gao et al. <ref type="bibr" target="#b15">[15]</ref> utilize a setting that is relatively practical both for acquiring a few annotations (e.g., 16 examples per class) and efficiently training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Prompt-tuning</head><p>Prompt-tuning methods are fueled by the birth of GPT-3 <ref type="bibr" target="#b7">[7]</ref> and have achieved outstanding performance in widespread NLP tasks. With appropriate manual prompts, series of studies <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41]</ref> have been proposed, demonstrating the advancement of prompttuning. Hu et al. <ref type="bibr" target="#b28">[28]</ref> propose to incorporate external knowledge into the verbalizer with calibration. Ding et al. <ref type="bibr" target="#b12">[12]</ref> apply prompttuning to entity typing with prompt-learning by constructing an entity-oriented verbalizer and templates. To avoid labor-intensive prompt design, automatic searches for discrete prompts have been extensively explored. Gao et al. <ref type="bibr" target="#b15">[15]</ref>, Schick et al. <ref type="bibr" target="#b42">[42]</ref> first explore the automatic generation of ans words and templates. Shin et al. <ref type="bibr" target="#b46">[46]</ref> further propose gradient-guided search to generate the template and label word in vocabulary automatically. Recently, some continuous prompts have also been proposed <ref type="bibr" target="#b21">[21,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b34">34,</ref><ref type="bibr" target="#b36">36]</ref>, which focus on utilizing learnable continuous embeddings as prompt templates rather than label words. However, these works can not adapted to RE directly.</p><p>For relation extraction, Han et al. <ref type="bibr" target="#b22">[22]</ref> proposes a model called PTR, which creatively applies logic rules to construct prompts with several sub-prompts. Compared with their approach, our approach has three significant differences. Firstly, we propose virtual answer words to represent specific relation labels rather than multiple sub-prompt in PTR. Essentially, our method is modelagnostic that can be applied to generative LMs, while PTR fails due to its sub-prompt mechanism. Secondly, we construct prompt with knowledge injection via learnable virtual type words and virtual answer words to alleviate labor-intensive prompt engineering rather than predefined rules; thus, our method is more flexible and can generalize to different RE datasets easily. Thirdly, we synergistically optimize virtual type words and answer words with knowledge constraints and associate prompt embeddings with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">BACKGROUND</head><p>An RE dataset can be denoted as D = {X, Y}, where X is the set of examples and Y is the set of relation labels. For each example = { 1 , 2 , . . . , . . . }, the goal of RE is to predict the relation ? Y between subject entity and object entity (since one entity may have multiple tokens, we simply utilize and to represent all entities briefly.). and W is a randomly initialized matrix that needs to be optimized. The parameters of L and W are fine-tuned by minimizing the cross-entropy loss over ( | ) on the entire X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fine-tuning PLMs for RE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Prompt-Tuning of PLMs</head><p>Prompt-tuning is proposed to bridge the gap between the pretraining tasks and downstream tasks. The challenge is to construct an appropriate template T (?) and label words V, which are collectively referred to as a prompt P. For each instance , the template is leveraged to map to prompt the input prompt = ( ). Concretely, template T (?) involves the location and number of added additional words. V refers to a set of label words in the vocabulary of a language model L, and M : Y ? V is an injective mapping that connects task labels to label words V. In addition to retaining the original tokens in , one or more [MASK] is placed into prompt for L to fill the label words. As L can predict the right token at the masked position, we can formalize ( | ) with the probability distribution over V at the masked position, that is,</p><formula xml:id="formula_0">( | ) = ([MASK] = M ( )| prompt ).</formula><p>Taking the binary sentiment classification task described as an example, we set the template (?) = " ? It is <ref type="bibr">[MASK]</ref>. " and map to prompt = " It is <ref type="bibr">[MASK]</ref>. ". We can then obtain the hidden vector of [MASK] by encoding prompt by L and produce a probability distribution ([MASK]| prompt ), describing which tokens of V are suitable for replacing the [MASK] word. Since previous study for prompt-learning involves searching or generating label words here, we simply set M ( = " ") ? " " and M ( = " ") ? " " as examples. According to whether L predicts "great" or "terrible", we can identify if the label of instance is either or .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>In this section, we introduce our Knowledge-aware Prompt-tuning with synergistic optimization (KnowPrompt) approach to be aware of semantic and prior knowledge contained in relation labels for relation extraction. As shown in <ref type="figure" target="#fig_2">Figure 2</ref>, we elucidate the details of how to construct ( ?4.1), optimize ( ?4.2) the KnowPrompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Prompt Construction with Knowledge Injection</head><p>Because a typical prompt consists of two parts, namely a template and a set of label words, we propose the construction of virtual type words and virtual answer words with knowledge injection for the RE task. Entity Knowledge Injection. Note that Type Marker [66] methods can additionally introduce the type information of entities to improve performance but require additional annotation of type information, which is not always available in datasets. However, we can obtain the scope of the potential entity types with prior knowledge contained in a specific relation, rather than annotation. For instance, given the relation "per:country_of_birth", it is evident that the subject entity matching this relation belongs to "person" and the object entity matching this relation belongs to "country". Intuitively, we estimate the prior distributions and over the candidate set and of potential entity types, respectively, according to the relation class, where the prior distributions are estimated by frequency statistics. Take and of partial relation labels listed in the <ref type="table" target="#tab_2">Table 1</ref> as an example, the prior distributions for can be counted as:</p><formula xml:id="formula_1">= {"</formula><p>" : 3/6, " " : 3/6}. Because of this, we assign virtual type words around the entities, which are initialized with aggregated embeddings of the set of potential entity types. Since initialized virtual type words are not precise types for specific entities, those learnable virtual type words can dynamically adjust according to context and play the weakened role of Type Marker for RE. The specific initialization method is as follows:</p><formula xml:id="formula_2">e [ ] = ?? ? e ( (C )) ,<label>(1)</label></formula><formula xml:id="formula_3">e [ ] = ?? ? e C ,<label>(2)</label></formula><p>where? <ref type="bibr">[ ]</ref> and? [ ] represent the embeddings of virtual type words surrounding the subject and object entities, (?) is the deduplication operations on sets, and e is the word-embedding</p><formula xml:id="formula_4">(b) KnowPrompt [CLS]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Steve Jobs</head><p>, co-founder of Apple .</p><p>[SEP]</p><p>[MASK]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Steve Jobs</head><p>Relation Embedding Head  layer of L. Since the virtual type words designed based on the prior knowledge within relation labels can initially perceive the range of entity types, it can be further optimized according to context to express semantic information close to the actual entity type, holding the role similar to Typer Marker. Relation Knowledge Injection. Previous studies on prompttuning usually form a one-one mapping between one label word in the vocabulary and one task label by automatic generation, which maintains large computational complexity of the search process and fails to leverage the abundant semantic knowledge in relation labels for RE. To this end, we assume that there exists a virtual answer word ? ? V ? in the vocabulary space of PLMs, which can represent the implicit semantics of the relation. From this perspective, we expand the MLM Head layer of L with extra learnable relation embeddings as the virtual answer word sets V ? to completely represent the corresponding relation labels Y. Thus, we can reformalize ( | ) with the probability distribution over V ? at the masked position. We propose to encodes semantic knowledge about the label and facilitates the process of RE. Concretely, we set the</p><formula xml:id="formula_5">= [ 1 , 2 , ..., ] and C = [C 1 , C 2 , ..., C ]</formula><p>, where represent the probability distribution over the candidate set C of the semantic words of relation by disassembling the relation label , is the number of relation labels. Furthermore, we adopt the weighted average function for to average embeddings of each words among C to initialize these relation embeddings, which can inject the semantic knowledge of relations. The specific decomposition process is shown in <ref type="table" target="#tab_2">Table 1</ref>, and the learnable relation embedding of virtual answer word ? = M ( ) is initialized as follows:?</p><formula xml:id="formula_6">[ ] ( ? ) = ? e (C ) ,<label>(3)</label></formula><p>where? [ ] ( ? )is the embedding of virtual label word ? , e represents the word-embedding layer of L. It is noticed that the knowledgeable initialization process of virtual answer words may be regarded as a great anchor; we can further optimize them based on context to express optimal semantic information, leading to better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Synergistic Optimization with Knowledge Constraints</head><p>Since there exist close interaction and connection between entity types and relation labels, and those virtual type words as well as answer words should be associated with the surrounding context, we further introduce a synergistic optimization method with implicit structural constraints over the parameter</p><formula xml:id="formula_7">set {? [ ] ,? [ ] ,? [ ] (V ? )} of</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>virtual type words and virtual answer words.</head><p>Context-aware Prompt Calibration. Although our virtual type and answer words are initialized based on knowledge, they may not be optimal in the latent variable space. They should be associated with the surrounding context. Thus, further optimization is necessary by perceiving the context to calibrate their representation. Given the probability distribution ( | ) = ( [MASK] = V ? | prompt ) over V ? at the masked position, we optimize the virtual type words as well as answer words by the loss function computed as the cross-entropy between y and ( | ) as follows: per:country_of_birth person country {"country", "of", "birth" } per:data_of_death person data {"data", "of", "death" } per:schools_attended person organization {"school", "attended'} org:alternate_names organization organization {"alternate", "names" } org:city_of_headquarters organization city {"city", "of", "headquarters" } org:number_of_employees/members organization number {"number", "of", "employees", "members" } where |X| represents the numbers of the training dataset. The learnable words may adaptively obtain optimal representations for prompt-tuning through a synergistic type and answer optimization.</p><formula xml:id="formula_8">J [MASK] = ? 1 |X| ?? ?X y log ( | ),<label>(4)</label></formula><p>Implicit Structured Constraints. To integrate structural knowledge into KnowPrompt, we adopt additional structured constraints to optimize prompts. Specifically, we use a triplet ( , , ) to describe a relational fact; here, , represent the virtual types of subject and object entities, respectively, and is the relation label within a predefined set of answer words V ? . In KnowPrompt, instead of using pre-trained knowledge graph embeddings 2 , we directly leverage the output embedding of virtual type words and virtual answer words through LMs to participate in the calculation. We define the loss J struct of implicit structured constraints as follows:</p><formula xml:id="formula_9">J structured = ? log ( ? (s, o)) ? ?? =1 1 log ( (s ? i , o ? i ) ? ),<label>(5)</label></formula><formula xml:id="formula_10">(s, o) = ?s + r ? o? 2 ,<label>(6)</label></formula><p>where ( ? , , ? ) are negative samples, is the margin, refers to the sigmoid function and is the scoring function. For negative sampling, we assign the correct virtual answer words at the position of [MASK] and randomly sample the subject entity or object entity and replace it with an irrelevant one to construct corrupt triples, in which the entity has an impossible type for the current relation. of virtual type words and virtual answer words with a large learning rate 1 to obtain the optimal prompt as follows:</p><formula xml:id="formula_11">J = J [MASK] + J structured ,<label>(7)</label></formula><p>where is the hyperparameter, and J structured and J <ref type="bibr">[MASK]</ref> are the losses for the KE and [MASK] prediction, respectively. Second, based on the optimized virtual type words and answer words, we utilize the object function J [MASK] to tune the parameters of the PLM with prompt (optimizing overall parameters) with a small learning rate 2 . For more experimental details, please refer to the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Datasets</head><p>For comprehensive experiments, we carry out our experiments on five RE datasets: SemEval 2010 Task 8 (SemEval) <ref type="bibr" target="#b26">[26]</ref>, DialogRE <ref type="bibr" target="#b54">[54]</ref>, TACRED <ref type="bibr" target="#b63">[63]</ref>, TACRED-Revisit <ref type="bibr" target="#b0">[1]</ref>, Re-TACRED <ref type="bibr" target="#b47">[47]</ref>. Statistical details are provided in <ref type="table" target="#tab_3">Table 2</ref> and Appendix A:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Settings</head><p>For fine-tuning vanilla PLMs and our KnowPrompt, we utilize RoBERT_large for all experiments to make a fair comparison (except for DialogRE, we adopt RoBERTa_base to compare with previous methods). For test metrics, we use micro 1 scores of RE as the primary metric to evaluate models, considering that 1 scores can assess the overall performance of precision and recall. We use different settings for standard and low-resource experiments.All detailed settings for our KnowPrompt, Fine-tuning and PTR can be found in the Appendix B, C and D. Standard Setting. In the standard setting, we utilize full D train to fine-tune. Considering that entity information is essential for models to understand relational semantics, a series of knowledgeenhanced PLMs have been further explored using knowledge graphs as additional information to enhance PLMs. Specifically, we select SpanBERT <ref type="bibr" target="#b30">[30]</ref>, KnowBERT <ref type="bibr" target="#b38">[38]</ref>, LUKE <ref type="bibr" target="#b52">[52]</ref>, and MTB <ref type="bibr" target="#b2">[3]</ref> as our strong baselines, which are typical models that use external knowledge to enhance learning objectives, input features, model architectures, or pre-training strategies. We also compare several SOTA models on DialogRE, in which one challenge is that each entity pair has more than one relation.</p><p>Low-Resource Setting. We conducted 8-, 16-, and 32-shot experiments following LM-BFF <ref type="bibr" target="#b15">[15,</ref><ref type="bibr" target="#b22">22]</ref> to measure the average performance across five different randomly sampled data based on every experiment using a fixed set of seeds S seed . Specifically, we sample instances of each class from the initial training and validation sets to form the few-shot training and validation sets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Main Results</head><p>Standard Result. As shown in <ref type="table" target="#tab_4">Table 3</ref>, the knowledge-enhanced PLMs yield better performance than the vanilla Fine-tuning. This result illustrates that it is practical to inject task-specific knowledge to enhance models, indicating that simply fine-tuning PLMs cannot perceive knowledge obtained from pre-training. Note that our KnowPrompt achieves improvements over all baselines and even achieves better performance than those knowledgeenhanced models, which use knowledge as data augmentation or architecture enhancement during fine-tuning. On the other hand, even if the task-specific knowledge is already contained in knowledge-enhanced PLMs such as LUKE, KnowBERT SpanBERT and MTB , it is difficult for fine-tuning to stimulate the knowledge for downstream tasks. Overall, we believe that the development of prompt-tuning is imperative and KnowPrompt is a simple and effective prompt-tuning paradigm for RE.</p><p>Low-Resource Result. From <ref type="table" target="#tab_5">Table 4</ref>, KnowPrompt appears to be more beneficial in low-resource settings. We find that Know-Prompt consistently outperforms the baseline method Fine-tuning, GDPNet, and PTR in all datasets, especially in the 8-shot and 16shot experiments. Specifically, our model can obtain gains of up to 22.4% and 13.2% absolute improvement on average compared with fine-tuning. As increases from 8 to 32, the improvement in our KnowPrompt over the other three methods decreases gradually. For 32-shot, we think that the number of labeled instances is sufficient. Thus, those rich semantic knowledge injected in our approach may induce fewer gains. We also observe that GDPNet even performs worse than Fine-tuning for 8-shot, which reveals that the complex SOTA model in the standard supervised setting may fall off the altar when the data are extremely scarce.</p><p>Comparison between KnowPrompt and Prompt Tuning Methods. The typical prompt-tuning methods perform outstandingly on text classification tasks (e.g., sentiment analysis and NLI), such as LM-BFF, but they don't involve RE application. Thus we cannot rerun their code for RE tasks. To our best knowledge, PTR is the only method that uses prompts for RE, which is a wonderful job and works in the same period as our KnowPrompt. Thus, we make a comprehensively comparative analysis between KnowPrompt and PTR, and summarize the comparison in <ref type="table" target="#tab_8">Table 7</ref>. The specific analysis is as follows:</p><p>Firstly, PTR adopt a fixed number of multi-token answer form and LM-BFF leverage actual label word with single-token answer form, while KnowPrompt propose virtual answer word with single-token answer form. Thus, PTR needs to manually formulate rules, which is more labor-intensive. LM-BFF requires expensive label search due to its search process exponentially depending on the number of categories.</p><p>Secondly, essentially attributed to to the difference of answer form, our KnowPrompt and LM-BFF is model-agnostic and can be plugged into different kinds of PLMs (As show in <ref type="figure" target="#fig_4">Figure 3</ref>, our method can adopted on GPT-2), while PTR fails to generalize to generative LMs due to it's nultiple discontinuous [MASK] prediction.</p><p>Thirdly, above experiments, demonstrates that KnowPrompt is comprehensively comparable to the PTR, and can perform better in low-resource scenarios. Especially for DialogRE, a multi-label classification task, our method exceeded PTR by approximately 5.4 points in the standard supervised settings. It may be attributed to the rule method used by PTR that forcing multiple mask predictions will confuse multi-label predictions.</p><p>In a nutshell, the above analysis proves that KnowPrompt is more flexible and widely applicable; meanwhile, it can be aware of knowledge and stimulate it to serve downstream tasks better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study on KnowPrompt</head><p>Effect of Virtual Answer Words Modules: To prove the effects of the virtual answer words and its knowledge injection, we conduct the ablation study, and the results are shown in <ref type="table" target="#tab_6">Table 5</ref>. For -VAW, we adopt one specific token in the relation label as the label word without optimization, and for -Knowledge Injection for VAW, we randomly initialize the virtual answer words to conduct optimization. Specifically, removing the knowledge injection for virtual answer words has the most significant effect, causing the relation F1 score  to drop from 74.3% to 52.5% in the 8-shot setting. It also reveals that the injection of semantic knowledge maintained in relation labels is critical for relation extraction, especially in few-shot scenarios. Effect of Virtual Type Words Modules: We also conduct an ablation study to validate the effectiveness of the design of virtual type words. As for -VTW, we directly remove virtual type words, and for -Knowledge Injection for VTW, we randomly initialize the virtual type words to conduct optimization. In the 8-shot setting, the performance of the directly removing virtual type words drops from 74.3 to 72.8, while randomly initialized virtual type words decrease the performance to 68.1, which is much lower than 72.8. This phenomenon may be related to the noise disturbance caused by random initialization, while as the instance increase, the impact of knowledge injection gradually diminishes. Despite this, it still demonstrates that our design of knowledge injection for virtual type words is effective for relation extraction. Effect of Structured Constrains: Moreover, -Structured Constraints refer to the model without implicit structural constraints, which indicates no direct correlations between entities and relations. The result demonstrates that structured constraints certainly improve model performance, probably, because they can force the virtual answer words and type words to interact with each other better.</p><p>Overall, the result reveals that all modules contribute to the final performance. We further notice that virtual answer words with knowledge injection are more sensitive to performance and highly beneficial for KnowPrompt, especially in low-resource settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ANALYSIS AND DISCUSSION 6.1 Can KnowPrompt Applied to Other LMs?</head><p>Since we focus on MLM (e.g., RoBERTa) in the main experiments, we further extend our KnowPrompt to autoregressive LMs like GPT-2. Specifically, we directly append the prompt template with [MASK] at the end of the input sequence for GPT-2. We further apply the relation embedding head by extending the word embedding layer in PLMs; thus, GPT2 can generate virtual answer words. We first notice that fine-tuning leads to poor performance with high variance in the low-resource setting, while KnowPrompt based on RoBERTa or GPT-2 can achieve impressive improvement with low variance compared with Fine-tuning. As shown in <ref type="figure" target="#fig_4">Figure 3</ref>, Know-Prompt based on GPT-2 obtains the results on par of the model with RoBERTa-large, which reveals our method can unearth the potential of GPT-2 to make it perform well in natural language understanding tasks such as RE. This finding also indicates that our method is model-agnostic and can be plugged into different kinds of PLMs.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Interpreting Representation Space of Virtual Answer Words</head><p>Since the embeddings of virtual answer words {? [ ] (V ? )} are initialized with semantic knowledge of relation type itself, and further learned in continuous space, it is intuitive to explore what precisely the optimized virtual answer word is. We use t-SNE and normalization to map the embedding to 3 dimension space and make a 3D visualization of several sampled virtual answer words in the TACRED-Revisit dataset. We also get the top3 tokens nearest the virtual answer word by calculating the 2 distance between the embedding of the virtual answer word and the actual word in the vocabulary. For example, " : _ " referred to as green ? in <ref type="figure" target="#fig_5">Figure 4</ref> represents the relation type, which is learned by optimizing virtual answer words in vocabulary space, and the " "," ? " and " " referred to as green ? are the words closest to the it. It reveals that virtual answer words learned in vocabulary space are semantic and intuitive. To some extent, our proposed virtual answer words are similar to prototypical representation for relation labels. This inspired us to further expand knowprompt into the field of prototype representation learning in the future, which can also be applied to other NLP tasks with prompt-tuning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Interpreting Representation Space of virtual type words</head><p>Since we initialize the virtual type words with the average embedding of candidate types of head and tail entities through prior knowledge maintained in the relation labels, and synergistically optimize them ({? [ ] ,? [ ] }) with virtual answer words based on context. To this end, we further conduct further analysis to investigate that what semantics do the optimized type words express and whether virtual type words can adaptively reflect the entity types based on context as shown in <ref type="table" target="#tab_7">Table 6</ref>. Specifically, we apply the MLM head over the position of the virtual type words to get the output representation and get the top-3 words in vocabulary nearest the virtual type words according to the 2 distance of embeddings between virtual type words and other words. We observe that thanks to the synergistic optimization with knowledge constraints, those learned virtual type words can dynamically adjust according to context and play a reminder role for RE. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSION AND FUTURE WORK</head><p>In this paper, we present KnowPrompt for relation extraction, which mainly includes knowledge-aware prompt construction and synergistic optimization with knowledge constraints. In the future, we plan to explore two directions, including: (i) extending to semisupervised setting to further leverage unlabelled data; (ii) extending to lifelong learning, whereas prompt should be optimized with adaptive tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A DETAILED STATISTICS OF DATASET</head><p>For comprehensive experiments, we carry out our experiments on five relaction extraction datasets: TACRED <ref type="bibr" target="#b63">[63]</ref>, TACREV <ref type="bibr" target="#b0">[1]</ref>, Re-TACRED <ref type="bibr" target="#b47">[47]</ref>, SemEval 2010 Task 8 (SemEval) <ref type="bibr" target="#b26">[26]</ref>, and Dialo-gRE <ref type="bibr" target="#b54">[54]</ref>. A brief introduction to these data is as follows: TACRED: one large-scale sentence-level relation extraction dataset drawn from the yearly TACKBP4 challenge, which contains more than 106K sentences. It involves 42 different relations (41 common relation types and a special "no relation" type). The subject mentions in TACRED are person and organization, while object mentions are in 16 fine-grained types, including date, number, etc.</p><p>TACRED-Revisit: one dataset built based on the original TA-CRED dataset. They find out and correct the errors in the original development set and test set of TACRED, while the training set was left intact.</p><p>Re-TACRED: another version of TACRED dataset. They address some shortcomings of the original TACRED dataset, refactor its training set, development set and test set. Re-TACRED also modifies a few relation types, finally resulting in a dataset with 40 relation types.</p><p>SemEval: a traditional dataset in relation classification containing 10, 717 annotated examples covering 9 relations with two directions and one special relation "no_relation".</p><p>DialogRE: DialogRE is the first human-annotated dialogue-level RE dataset. It contains 1,788 dialogues originating from the complete transcripts of a famous American television situation comedy. It is multi-label classification, as each entity pair may posses more than one relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B IMPLEMENTATION DETAILS FOR KNOWPROMPT</head><p>This section details the training procedures and hyperparameters for each of the datasets. We utilize Pytorch to conduct experiments with 8 Nvidia 3090 GPUs. All optimizations are performed with the AdamW optimizer with a linear warmup of learning rate over the first 10% of gradient updates to a maximum value, then linear decay over the remainder of the training. Gradients are clipped if their norm exceeded 1.0, margin , and weight decay on all non-bias parameters are set to 1, 0.001 and 0.01. A grid search is used for hyperparameter tuning (maximum values bolded below).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Standard Supervised Setting</head><p>The hyper-parameter search space is shown as follows:</p><p>? learning rate 1 of synergistic optimization for virtual template and anchor words. [5e-5,1e-4, 2e-4 ] ? learning rate 2 of optimization for overall parameters. [1e-5, 2e-5, 3e-5, 5e-5] ? number epochs 5 (for dialogre as 20)</p><p>? batch size: 16 (for tacrev, retacred and dialogre as 8)</p><p>? max seq length: 256 (for tacrev, retacred and dialogre as 512) ? gradient accumulation steps: 1 (for dialogre as 4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Low-Resource Setting</head><p>The hyper-parameter search space is shown as follows:</p><p>? learning rate 1 of synergistic optimization for virtual template and anchor words: [5e-5,1e-4, 2e-4 ] ? learning rate 2 of optimization for overall parameters: [1e-5, 2e-5, 3e-5, 5e-5] ? number of epochs: 30 ? batch size: 16 (for tacrev, retacred and dialogre as 8)</p><p>? max seq length: 256 (for tacrev, retacred and dialogre as 512) ? gradient accumulation steps: 1 (for dialogre as 4)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C IMPLEMENTATION DETAILS FOR FINE-TUNING</head><p>The fine-tuning method is conducted as shown in <ref type="figure" target="#fig_2">Figure 2</ref>, which is both equipped with the same entity marker in the raw text for a fair comparison. The hyper-parameters such as batch size, epoch, and learning rate are the same as KnowPrompt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D IMPLEMENTATION DETAILS FOR PTR</head><p>Since PTR does not conduct experiments on DialogRE in standard supervised setting and SemEval and DialogRE in few-shot settings, we rerun its public code to supplement the experiments we described above with these data and scenarios. As for SemEval, the experiment process completely follows the original setting in his code, while for DialogRE, we modify his code to more adapt to the setting of this data set. The specific hyper-parameters such as batch size, epoch, and learning rate are the same as KnowPrompt.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Examples of prompt-tuning to stimulate the knowledge of PLMs by formalizing specific tasks as clozestyle tasks. The P and C in dashed balls represents the virtual type words with semantics close to person and country.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Given a pre-trained language model (PLM) L for RE, previous finetuning methods first convert the instance = { 1 , . . . , . . . } into an input sequence of PLM, such as [CLS] [SEP]. The PLM L encodes the input sequence into the corresponding output hidden vectors such as h = {h [CLS] , h 1 , h , . . . , h , . . . , h [SEP] }. Normally, a [CLS] head is utilized to compute the probability distribution over the class set Y with the softmax function (?| ) = Softmax(Wh [CLS] ), where h [CLS] is the output embedding of [CLS]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Model architecture of fine-tuning for RE (Figure a), and proposed KnowPrompt (Figure b) approach (Best viewed in color). The answer word described in the paper refers to the virtual answer word we proposed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>RoBERT-large vs. GPT-2 results on TACRED-Revisit dataset regarding different K (instances per class).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>A 3D visualization of several relation representations (virtual answer words) optimized in KnowPrompt on TACRED-Revisit dataset using t-SNE and normalization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The cast is uniformly excellent and relaxed . [SEP]</figDesc><table><row><cell cols="2">[CLS] It is</cell><cell cols="2">[MASK]</cell><cell cols="2">. [SEP]</cell><cell>(a)</cell></row><row><cell>Prompt for Text Classification</cell><cell></cell><cell></cell><cell></cell><cell cols="2">MLM</cell><cell>great terrible</cell><cell>positive negative</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>(b)</cell></row><row><cell></cell><cell cols="2">Hamilton</cell><cell></cell><cell></cell><cell cols="2">British [SEP]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>per:country_of_birth</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MLM</cell><cell>org:city_of_headquarters</cell></row><row><cell>KnowPrompt for</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>per:country_of_residence</cell></row><row><cell>Relation Extraction</cell><cell></cell><cell cols="4">mutual restrict</cell><cell>(c)</cell></row><row><cell>P</cell><cell cols="2">Hamilton</cell><cell cols="2">[MASK]</cell><cell>British</cell><cell>C</cell><cell>[SEP]</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>per:country_of_birth</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>MLM</cell><cell>org:city_of_headquarters</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>per:country_of_residence</cell></row><row><cell cols="3">P person</cell><cell cols="3">C country</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">virtual type words</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Examples of some relations of the datasets TACREV, and relation-specific C ,C and C .</figDesc><table><row><cell>Relation Labels</cell><cell>C</cell><cell>C</cell><cell>C (Disassembled Relation Prepared for Virtual Answer Words)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Statistics for RE datasets used in the paper, including numbers of relations and instances in the different split. For dialogue-level DialogRE, instance refers to the number of documents.</figDesc><table><row><cell>SemEval</cell><cell>6,507</cell><cell>1,493</cell><cell>2,717</cell><cell>19</cell></row><row><cell>DialogRE</cell><cell>5,963</cell><cell>1,928</cell><cell>1,858</cell><cell>36</cell></row><row><cell>TACRED</cell><cell cols="2">68,124 22,631</cell><cell>15,509</cell><cell>42</cell></row><row><cell>TACRED-Revisit</cell><cell cols="2">68,124 22,631</cell><cell>15,509</cell><cell>42</cell></row><row><cell>Re-TACRED</cell><cell cols="2">58,465 19,584</cell><cell>13,418</cell><cell>40</cell></row><row><cell cols="2">4.3 Training Details</cell><cell></cell><cell></cell><cell></cell></row></table><note>Dataset # Train. # Val. # Test. # Rel.Our approach has a two-stage optimization procedure. First, we syn- ergistically optimize the parameter set {? [ ] ,? [ ] ,? [ ] (V ? )}2 Note that pre-trained knowledge graph embeddings are heterogeneous compared with pre-trained language model embeddings.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Standard RE performance of 1 scores (%) on different test sets. "w/o" means that no additional data is used for pretraining and fine-tuning, yet "w/" means that the model uses extra data for tasks. It is worth noting that " ?" indicates we exceptionally rerun the code of KnowPrompt and PTR with RoBERT_base for a fair comparison with current SOTA models on DialogRE. Subscript in red represents advantages of KnowPrompt over the best results of baselines. Best results are bold.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Standard Supervised Setting</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Extra Data</cell><cell>SemEval</cell><cell cols="4">DialogRE ? TACRED TACRED-Revisit Re-TACRED</cell></row><row><cell></cell><cell></cell><cell cols="2">Fine-tuning pre-trained models</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Fine-tuning-[Roberta]</cell><cell>w/o</cell><cell>87.6</cell><cell>57.3</cell><cell>68.7</cell><cell>76.0</cell><cell>84.9</cell></row><row><cell>SpanBERT [30]</cell><cell>w/</cell><cell>-</cell><cell>-</cell><cell>70.8</cell><cell>78.0</cell><cell>85.3</cell></row><row><cell>KnowBERT [38]</cell><cell>w/</cell><cell>89.1</cell><cell>-</cell><cell>71.5</cell><cell>79.3</cell><cell>89.1</cell></row><row><cell>LUKE [52]</cell><cell>w/</cell><cell>-</cell><cell>-</cell><cell>72.7</cell><cell>80.6</cell><cell>-</cell></row><row><cell>MTB [3]</cell><cell>w/</cell><cell>89.5</cell><cell>-</cell><cell>70.1</cell><cell>-</cell><cell>-</cell></row><row><cell>GDPNet [51]</cell><cell>w/o</cell><cell>-</cell><cell>64.9</cell><cell>71.5</cell><cell>79.3</cell><cell>-</cell></row><row><cell>Dual [2]</cell><cell>w/o</cell><cell>-</cell><cell>67.3</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell cols="4">Prompt-tuning pre-trained models</cell><cell></cell><cell></cell></row><row><cell>PTR-[Roberta] [22]</cell><cell>w/o</cell><cell>89.9</cell><cell>63.2</cell><cell>72.4</cell><cell>81.4</cell><cell>90.9</cell></row><row><cell>KnowPrompt-[Roberta]</cell><cell>w/o</cell><cell cols="3">90.2 (+0.3) 68.6 (+5.4) 72.4 (-0.3)</cell><cell>82.4 (+1.0)</cell><cell>91.3 (+0.4)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Low-resource RE performance of 1 scores (%) on different test sets. We use = 8, 16, 32 (# examples per class) for few-shot experiments. Subscript in red represents the advantages of KnowPrompt over the results of Fine-tuning.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Low-Resource Setting</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Split Methods</cell><cell>SemEval</cell><cell>DialogRE ?</cell><cell>TACRED</cell><cell cols="2">TACRED-Revisit Re-TACRED</cell><cell>Average</cell></row><row><cell>Fine-tuning</cell><cell>41.3</cell><cell>29.8</cell><cell>12.2</cell><cell>13.5</cell><cell>28.5</cell><cell>25.1</cell></row><row><cell>GDPNet</cell><cell>42.0</cell><cell>28.6</cell><cell>11.8</cell><cell>12.3</cell><cell>29.0</cell><cell>24.7</cell></row><row><cell>K=8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PTR</cell><cell>70.5</cell><cell>35.5</cell><cell>28.1</cell><cell>28.7</cell><cell>51.5</cell><cell>42.9</cell></row><row><cell cols="4">KnowPrompt 74.3 (+33.0) 43.8 (+14.0) 32.0 (+19.8)</cell><cell>32.1 (+18.6)</cell><cell cols="2">55.3 (+26.8) 47.5 (+22.4)</cell></row><row><cell>Fine-tuning</cell><cell>65.2</cell><cell>40.8</cell><cell>21.5</cell><cell>22.3</cell><cell>49.5</cell><cell>39.9</cell></row><row><cell>GDPNet</cell><cell>67.5</cell><cell>42.5</cell><cell>22.5</cell><cell>23.8</cell><cell>50.0</cell><cell>41.3</cell></row><row><cell>K=16</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PTR</cell><cell>81.3</cell><cell>43.5</cell><cell>30.7</cell><cell>31.4</cell><cell>56.2</cell><cell>48.6</cell></row><row><cell cols="4">KnowPrompt 82.9 (+17.7) 50.8 (+10.0) 35.4 (+13.9)</cell><cell>33.1 (+10.8)</cell><cell cols="2">63.3 (+13.8) 53.1 (+13.2)</cell></row><row><cell>Fine-tuning</cell><cell>80.1</cell><cell>49.7</cell><cell>28.0</cell><cell>28.2</cell><cell>56.0</cell><cell>48.4</cell></row><row><cell>GDPNet</cell><cell>81.2</cell><cell>50.2</cell><cell>28.8</cell><cell>29.1</cell><cell>56.5</cell><cell>49.2</cell></row><row><cell>K=32</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PTR</cell><cell>84.2</cell><cell>49.5</cell><cell>32.1</cell><cell>32.4</cell><cell>62.1</cell><cell>52.1</cell></row><row><cell cols="2">KnowPrompt 84.8 (+4.7)</cell><cell>55.3 (+3.6)</cell><cell>36.5 (+8.5)</cell><cell>34.7 (+6.5)</cell><cell>65.0 (+9.0)</cell><cell>55.3 (+6.9)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on SemEval, VAW and VTW refers to virtual answer words and type words.</figDesc><table><row><cell>Method</cell><cell cols="4">K=8 K=16 K=32 Full</cell></row><row><cell>KnowPrompt</cell><cell>74.3</cell><cell>82.9</cell><cell>84.8</cell><cell>90.2</cell></row><row><cell>-VAW</cell><cell>68.2</cell><cell>72.7</cell><cell>75.9</cell><cell>85.2</cell></row><row><cell>-Knowledge Injection for VAW</cell><cell>52.5</cell><cell>78.0</cell><cell>80.2</cell><cell>88.0</cell></row><row><cell>-VTW</cell><cell>72.8</cell><cell>80.3</cell><cell>82.9</cell><cell>88.7</cell></row><row><cell cols="2">-Knowledge Injection for VTW 68.8</cell><cell>79.5</cell><cell>81.6</cell><cell>88.5</cell></row><row><cell>-Structured Constrains</cell><cell>73.5</cell><cell>81.2</cell><cell>83.6</cell><cell>89.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Interpreting representation of virtual type words. We obtain the hidden state h [sub] , h [obj] through the PLM, then adopt MLM Head over them to explore which words in the vocabulary is nearest the virtual type words. It sold [ 1 ] ALICO [/ 1 ] to [ 2 ] MetLife Inc [ 2 ] for $ 162 billion. Ismael Rukwago [/ 1 ], a senior [ 2 ] ADF [ 2 ] commander, denied any involvement.</figDesc><table><row><cell>Input Example of our KnowPrompt</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="6">: Comparative statistics between KnowPrompt and</cell></row><row><cell cols="6">PTR, including (1)Answer Form of prompt; (2) labor-</cell></row><row><cell cols="6">intensive; (3) MA refers to whether model-agnostic ; (4) ML</cell></row><row><cell cols="6">refers to the ability of multi-label learning; and (4) CC refers</cell></row><row><cell cols="3">to the computational complexity.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Method # Answer Form # Labor # MA</cell><cell># ML</cell><cell># CC</cell></row><row><cell>LM-BFF</cell><cell>single-token</cell><cell>normal</cell><cell>yes</cell><cell>-</cell><cell>high</cell></row><row><cell>PTR</cell><cell>multi-token</cell><cell>normal</cell><cell>no</cell><cell cols="2">normal norm</cell></row><row><cell>Ours</cell><cell>single-token</cell><cell>small</cell><cell>yes</cell><cell>better</cell><cell>norm</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">TACRED Revisited: A Thorough Evaluation of the TACRED Relation Extraction Task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Alt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandra</forename><surname>Gabryszak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonhard</forename><surname>Hennig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2020</title>
		<meeting>ACL 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Semantic Representation for Dialogue Modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linfeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL/IJCNLP 2021</title>
		<meeting>ACL/IJCNLP 2021</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Matching the Blanks: Distributional Similarity for Relation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Livio Baldini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Soares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Fitzgerald</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kwiatkowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL/IJCNLP</title>
		<meeting>ACL/IJCNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">RECON: Relation Extraction using Knowledge Graph Context in a Graph Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anson</forename><surname>Bastos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Nadgeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuldeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isaiah</forename><forename type="middle">Onando</forename><surname>Mulang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeedeh</forename><surname>Shekarpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hoffart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Kaul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1673" to="1685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Aligning Knowledge Base and Document Embedding Models Using Regularized Multi-Task Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Baumgartner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bibek</forename><surname>Paudel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Dell&amp;apos;aglio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abraham</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">11136</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">PADA: A Prompt-based Autoregressive Approach for Adaptation to Unseen Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eyal</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Oved</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12206</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<title level="m">Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners</title>
		<meeting><address><addrLine>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note>Proceedings of NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Relational Learning of Pattern-Match Rules for Information Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Elaine</forename><surname>Califf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<publisher>AAAI Press / The MIT Press</publisher>
			<date type="published" when="1999" />
			<biblScope unit="page" from="328" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Low-resource Learning with Knowledge Graphs: A Comprehensive Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaoyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxia</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><forename type="middle">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Horrocks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.10006</idno>
		<ptr target="https://arxiv.org/abs/2112.10006" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Prompt-Learning for Fine-Grained Entity Typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Gee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10604</idno>
		<ptr target="https://arxiv.org/abs/2108.10604" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong-Gee</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.10604</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">2021. Prompt-Learning for Fine-Grained Entity Typing. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural relation extraction for knowledge base enrichment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerhard</forename><surname>Bayu Distiawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhong</forename><surname>Weikum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="229" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Leyu Lin, and Maosong Sun. 2020. Meta-Information Guided Meta-Learning for Few-Shot Relation Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2020</title>
		<meeting>COLING 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Making Pre-trained Language Models Better Few-shot Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hybrid Attention-Based Prototypical Networks for Noisy Few-Shot Relation Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Leyu Lin, and Maosong Sun. 2020. Neural Snowball for Few-Shot Relation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI 2020</title>
		<meeting>AAAI 2020</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">PPT: Pretrained Prompt Tuning for Few-shot Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.04332</idno>
		<ptr target="https://arxiv.org/abs/2109.04332" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning Latent Forests for Medical Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoshun</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shay B</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3651" to="3657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Attention Guided Graph Convolutional Networks for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhijiang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">WARP: Word-level Adversarial ReProgramming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Hambardzumyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrant</forename><surname>Khachatrian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL/IJCNLP 2021</title>
		<meeting>ACL/IJCNLP 2021</meeting>
		<imprint>
			<date type="published" when="2021-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">PTR: Prompt Tuning with Rules for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.11259</idno>
		<ptr target="https://arxiv.org/abs/2105.11259" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">FewRel: A Large-Scale Supervised Few-shot Relation Classification Dataset with State-of-the-Art Evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">LOREM: Languageconsistent Open Relation Extraction from Unstructured Text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Harting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepideh</forename><surname>Mesbah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Lofi</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380252</idno>
		<ptr target="https://doi.org/10.1145/3366423.3380252" />
	</analytic>
	<monogr>
		<title level="m">WWW &apos;20: The Web Conference 2020</title>
		<editor>Yennun Huang, Irwin King, Tie-Yan Liu, and Maarten van Steen</editor>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-20" />
			<biblScope unit="page" from="1830" to="1838" />
		</imprint>
	</monogr>
	<note>ACM / IW3C2</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Towards a Unified View of Parameter-Efficient Transfer Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junxian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunting</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.04366</idno>
		<ptr target="https://arxiv.org/abs/2110.04366" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su</forename><forename type="middle">Nam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>S?aghdha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Pad?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenza</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Szpakowicz</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/S10-1006/" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of SemEval</title>
		<meeting>SemEval</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.02035</idno>
		<ptr target="https://arxiv.org/abs/2108.02035" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Knowledgeable Prompt-tuning: Incorporating Knowledge into Prompt Verbalizer for Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huadong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.02035</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning information extraction patterns from examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing</title>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">SpanBERT: Improving Pre-training by Representing and Predicting Spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<ptr target="https://transacl.org/ojs/index.php/tacl/article/view/1853" />
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The Power of Scale for Parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Lester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08691</idno>
		<ptr target="https://arxiv.org/abs/2104.08691" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">-Efficient Prompt Tuning. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Logic-guided Semantic Representation Learning for Zero-Shot Relation Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2967" to="2978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Improving Relation Extraction with Knowledge-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kezhi</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuefeng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="229" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Prefix-Tuning: Optimizing Continuous Prompts for Generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL/IJCNLP 2021</title>
		<meeting>ACL/IJCNLP 2021</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinlan</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengbao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroaki</forename><surname>Hayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Graham</forename><surname>Neubig</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.13586</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengxiao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Qian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10385</idno>
		<ptr target="https://arxiv.org/abs/2103.10385" />
		<title level="m">Zhilin Yang, and Jie Tang. 2021. GPT Understands</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bartolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alastair</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08786</idno>
		<title level="m">Fantastically Ordered Prompts and Where to Find Them: Overcoming Few-Shot Prompt Order Sensitivity</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Knowledge Enhanced Contextual Word Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidur</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<ptr target="https://www.aclweb.org/anthology/D19-1005" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP. 43-54</title>
		<meeting>EMNLP-IJCNLP. 43-54</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fewshot Relation Extraction via Bayesian Meta-learning on Relation Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Louis-Pascal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ICML 2020</title>
		<meeting>ICML 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Prompt programming for large language models: Beyond the few-shot paradigm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laria</forename><surname>Reynolds</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Mcdonell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of CHI</title>
		<meeting>eeding of CHI</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">How Many Data Points is a Prompt Worth?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Teven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.08493</idno>
		<ptr target="https://arxiv.org/abs/2103.08493" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">It&apos;s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.07118</idno>
		<ptr target="https://arxiv.org/abs/2009.07118" />
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL 2021</title>
		<meeting>EACL 2021</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A Trigger-Sense Memory Flow Framework for Joint Entity and Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongliang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yechun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Lu</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442381.3449895</idno>
		<ptr target="https://doi.org/10.1145/3442381.3449895" />
	</analytic>
	<monogr>
		<title level="m">WWW &apos;21: The Web Conference 2021</title>
		<editor>Jure Leskovec, Marko Grobelnik, Marc Najork, Jie Tang, and Leila Zia</editor>
		<meeting><address><addrLine>Slovenia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-04-19" />
			<biblScope unit="page" from="1704" to="1715" />
		</imprint>
	</monogr>
	<note>ACM / IW3C2</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yasaman</forename><surname>Razeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2020</title>
		<meeting>EMNLP 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Stoica</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08398</idno>
		<ptr target="https://arxiv.org/abs/2104.08398" />
		<title level="m">Emmanouil Antonios Platanios, and Barnab?s P?czos. 2021. Re-TACRED: Addressing Shortcomings of the TACRED Dataset</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Lun</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.09841</idno>
		<title level="m">Ningyu Zhang, and Yefeng Zheng. 2020. Finding influential instances for distantly supervised relation extraction</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Enriching Pre-trained Language Model with Entity Information for Relation Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanchan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of theCIKM</title>
		<meeting>theCIKM</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Curriculum-Meta Learning for Order-Robust Continual Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongtong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuekai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reza</forename><surname>Haffari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guilin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoqiang</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01926</idno>
		<ptr target="https://arxiv.org/abs/2101.01926" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">GDPNet: Refining Latent Multi-View Graph for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuzhao</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eng Siong</forename><surname>Chng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI 2021</title>
		<meeting>AAAI 2021</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">LUKE: Deep Contextualized Entity Representations with Entity-aware Self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP 2020</title>
		<meeting>EMNLP 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Contrastive Triple Extraction with Generative Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Dialogue-Based Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2020</title>
		<meeting>ACL 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bridging Text and Knowledge with Multi-Prototype Embedding for Few-Shot Relational Triple Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.563</idno>
		<ptr target="https://doi.org/10.18653/v1/2020.coling-main.563" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING. International Committee on Computational Linguistics</title>
		<meeting>COLING. International Committee on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6399" to="6410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Document-level Relation Extraction as Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuanqi</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mosha</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luo</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2021/551</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2021/551" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI, Zhi-Hua Zhou (Ed.). ijcai.org</title>
		<meeting>IJCAI, Zhi-Hua Zhou (Ed.). ijcai.org</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3999" to="4006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Attention-Based Capsule Network with Dynamic Routing for Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanling</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Long-tail Relation Extraction via Knowledge Graph Embeddings and Graph Convolution Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanlin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">AliCG: Fine-grained and Evolvable Conceptual Graph Construction for Semantic Search at Alibaba</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianghuai</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongbin</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaixiao</forename><surname>Tou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nengwei</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447548.3467057</idno>
		<ptr target="https://doi.org/10.1145/3447548.3467057" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD. ACM</title>
		<meeting>KDD. ACM</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3895" to="3905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Graph Convolution over Pruned Dependency Trees Improves Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Position-aware Attention and Supervised Data Improve Slot Filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP</title>
		<meeting>EMNLP</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Position-aware attention and supervised data improve slot filling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="https://nlp.stanford.edu/pubs/zhang2017tacred.pdf" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP. 35-45</title>
		<meeting>EMNLP. 35-45</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Bin Qin, Xu Ming, and Yefeng Zheng. 2021. PRGC: Potential Relation and Global Correspondence Based Joint Relational Triple Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hengyi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL/IJCNLP 2021</title>
		<meeting>ACL/IJCNLP 2021</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyu</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL</title>
		<meeting>ACL</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">An Improved Baseline for Sentencelevel Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.01373</idno>
		<ptr target="https://arxiv.org/abs/2102.01373" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">NERO: A Neural Rule Grounding Framework for Label-Efficient Relation Extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenxuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongtao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ren</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380282</idno>
		<ptr target="https://doi.org/10.1145/3366423.3380282" />
	</analytic>
	<monogr>
		<title level="m">WWW &apos;20: The Web Conference 2020</title>
		<editor>Huang, Irwin King, Tie-Yan Liu, and Maarten van Steen</editor>
		<meeting><address><addrLine>Taipei, Taiwan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-20" />
			<biblScope unit="page" from="2166" to="2176" />
		</imprint>
	</monogr>
	<note>ACM / IW3C2</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
