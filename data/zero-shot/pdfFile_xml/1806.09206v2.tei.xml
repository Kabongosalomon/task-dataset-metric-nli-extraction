<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">N-Gram Graph: Simple Unsupervised Representation for Graphs, with Applications to Molecules</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Mehmet</roleName><forename type="first">Shengchao</forename><surname>Liu</surname></persName>
							<email>shengchao@cs.wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
								<address>
									<settlement>Madison</settlement>
									<region>WI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furkan</forename><surname>Demirel</surname></persName>
							<email>demirel@cs.wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
								<address>
									<settlement>Madison</settlement>
									<region>WI</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
							<email>yliang@cs.wisc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Sciences</orgName>
								<orgName type="institution">University of Wisconsin-Madison</orgName>
								<address>
									<settlement>Madison</settlement>
									<region>WI</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">N-Gram Graph: Simple Unsupervised Representation for Graphs, with Applications to Molecules</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T19:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning techniques have recently been adopted in various applications in medicine, biology, chemistry, and material engineering. An important task is to predict the properties of molecules, which serves as the main subroutine in many downstream applications such as virtual screening and drug design. Despite the increasing interest, the key challenge is to construct proper representations of molecules for learning algorithms. This paper introduces the N-gram graph, a simple unsupervised representation for molecules. The method first embeds the vertices in the molecule graph. It then constructs a compact representation for the graph by assembling the vertex embeddings in short walks in the graph, which we show is equivalent to a simple graph neural network that needs no training. The representations can thus be efficiently computed and then used with supervised learning methods for prediction. Experiments on 60 tasks from 10 benchmark datasets demonstrate its advantages over both popular graph neural networks and traditional representation methods. This is complemented by theoretical analysis showing its strong representation and prediction power.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Raw Molecule Data. This work views a molecule as a graph, where each atom is a vertex and each bond is an edge. Suppose there are m vertices in the graph, denoted as i ? {0, 1, ..., m ? 1}. Each vertex has useful attribute information, like the atom symbol and number of charges in the molecular graphs. These vertex attributes are encoded into a vertex attribute matrix V of size m ? S, where S is the number of attributes. An example of the attributes for vertex i is:</p><p>where V i,0 is the atom symbol, V i,1 counts the atom degree, V i,6 and V i,7 indicate if it is an acceptor or a donor. Details are listed in Appendix E. Note that the attributes typically have discrete values. The bonding information is encoded into the adjacency matrix A ? {0, 1} m?m , where A i,j = 1 if and only if two vertices i and j are linked. We let G = (V, A) denote a molecular graph. Sometimes there are additional types of information, like bonding types and pairwise atom distance in the 3D Euclidean space used by <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b25">26]</ref>, which are beyond the scope of this work.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Increasingly, sophisticated machine learning methods have been used in non-traditional application domains like medicine, biology, chemistry, and material engineering <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b8">9]</ref>. This paper focuses on a prototypical task of predicting properties of molecules. A motivating example is virtual screening for drug discovery. Traditional physical screening for drug discovery (i.e., selecting molecules based on properties tested via physical experiments) is typically accurate and valid, but also very costly and slow. In contrast, virtual screening (i.e., selecting molecules based on predicted properties via machine learning methods) can be done in minutes for predicting millions of molecules. Therefore, it can be a good filtering step before the physical experiments, to help accelerate the drug discovery process and significantly reduce resource requirements. The benefits gained then depend on the prediction performance of the learning algorithms.</p><p>A key challenge is that raw data in these applications typically are not directly well-handled by existing learning algorithms and thus suitable representations need to be constructed carefully. Unlike image or text data where machine learning (in particular deep learning) has led to significant achievements, the most common raw inputs in molecule property prediction problems provide only highly abstract representations of the chemicals (i.e., graphs on atoms with atom attributes).</p><p>To address the challenge, various representation methods have been proposed, mainly in two categories. The first category is chemical fingerprints, the most widely used feature representations in aforementioned domains. The prototype is the Morgan fingerprints <ref type="bibr" target="#b41">[42]</ref> (see <ref type="figure" target="#fig_5">Figure S1</ref> for an example). The second category is graph neural networks (GNN) <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b57">58]</ref>. They view the molecules as graphs with attributes, and build a computational network tailored to the graph structure that constructs a embedding vector for the input molecule and feed into a predictor (classifier 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada. arXiv:1806.09206v2 <ref type="bibr">[cs.</ref>LG] <ref type="bibr" target="#b10">11</ref> Nov 2019 or regression model). The network is trained end-to-end on labeled data, learning the embedding and the predictor at the same time.</p><p>These different representation methods have their own advantages and disadvantages. The fingerprints are simple and efficient to calculate. They are also unsupervised and thus each molecule can be computed once and used by different machine learning methods for different tasks. Graph neural networks in principle are more powerful: they can capture comprehensive information for molecules, including the skeleton structure, conformational information, and atom properties; they are trained end-to-end, potentially resulting in better representations for prediction. On the other hand, they need to be trained via supervised learning with sufficient labeled data, and for a new task the representation needs to retrained. Their training is also highly non-trivial and can be computationally expensive. So a natural question comes up: can we combine the benefits by designing a simple and efficient unsupervised representation method with great prediction performance?</p><p>To achieve this, this paper introduces an unsupervised representation method called N-gram graph. It views the molecules as graphs and the atoms as vertices with attributes. It first embeds the vertices by exploiting their special attribute structure. Then, it enumerates n-grams in the graph where an n-gram refers to a walk of length n, and constructs the embedding for each n-gram by assembling the embeddings of its vertices. The final representation is constructed based on the embeddings of all its n-grams. We show that the graph embedding step can also be formulated as a simple graph neural network that has no parameters and thus requires no training. The approach is efficient, produces compact representations, and enjoys strong representation and prediction power shown by our theoretical analysis. Experiments on 60 tasks from 10 benchmark datasets show that it gets overall better performance than both classic representation methods and several recent popular graph neural networks.</p><p>Related Work. We briefly describe the most related ones here due to space limitation and include a more complete review in Appendix A. Firstly, chemical fingerprints have long been used to represent molecules, including the classic Morgan fingerprints <ref type="bibr" target="#b41">[42]</ref>. They have recently been used with deep learning models <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b33">34]</ref>. Secondly, graph neural networks are recent deep learning models designed specifically for data with graph structure, such as social networks and knowledge graphs. See Appendix B for some brief introduction and refer to the surveys <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b56">57]</ref> for more details. Since molecules can be viewed as structured graphs, various graph neural networks have been proposed for them. Popular ones include <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b57">58]</ref>. Finally, graph kernel methods can also be applied (e.g., <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>). The implicit feature mapping induced by the kernel can be viewed as the representation for the input. The Weisfeiler-Lehman kernel <ref type="bibr" target="#b48">[49]</ref> is particularly related due to its efficiency and theoretical backup. It is also similar in spirit to the Morgan fingerprints and closely related to the recent GIN graph neural network <ref type="bibr" target="#b57">[58]</ref>.</p><p>the corresponding n-gram shows up in the sentence. Therefore, the dimension of an n-gram vector is |V | n for a vocabulary V , and the vector c <ref type="bibr" target="#b0">(1)</ref> is just the count vector of the words in the sentence. The n-gram representation has been shown to be a strong baseline (e.g., <ref type="bibr" target="#b52">[53]</ref>). One drawback is its high dimensionality, which can be alleviated by using word embeddings. Let W be a matrix whose i-th column is the embedding of the i-th word. Then f (1) = W c <ref type="bibr" target="#b0">(1)</ref> is just the sum of the word vectors in the sentence, which is in lower dimension and has also been shown to be a strong baseline (e.g., <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b5">6]</ref>). In general, an n-gram can be embedded as the element-wise product of the word vectors in it. Summing up all n-gram embeddings gives the embedding vector f (n) . This has been shown both theoretically and empirically to preserve good information for downstream learning tasks even using random word vectors (e.g., <ref type="bibr" target="#b2">[3]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">N-gram Graph Representation</head><p>Our N-gram graph method consists of two steps: first embed the vertices, and then embed the graph based on the vertex embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Vertex Embedding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neighbor</head><p>. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neighbor</head><p>. . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Neighbor</head><p>. . . <ref type="figure" target="#fig_5">Figure 1</ref>: The CBoW-like neural network g. Each small box represents one attribute, and the gray color represents the bit one since it is one-hot encoded. Each long box consists of S attributes with length K. 1 is the summation of all the embeddings of the neighbors of vertex i, where W ? R r?K is the vertex embedding matrix. 2 is a fully-connected neural network, and the final predictions are the attributes of vertex i. s</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Padding</head><formula xml:id="formula_0">W W W 1 Vertex i . . . 2</formula><p>The typical method to embed vertices in graphs is to view each vertex as one token and apply an analog of CBoW <ref type="bibr" target="#b40">[41]</ref> or other word embedding methods (e.g., <ref type="bibr" target="#b27">[28]</ref>). Here we propose our variant that utilizes the structure that each vertex has several attributes of discrete values. <ref type="bibr" target="#b0">1</ref> Recall that there are S attributes; see Section 2. Suppose the j-th attribute takes values in a set of size k j , and let K = S?1 j=0 k j . Let h j i denote a one-hot vector encoding the j-th attribute of vertex i, and let h i ? R K be the concatenation</p><formula xml:id="formula_1">h i = [h 0 i ; . . . ; h S?1 i ].</formula><p>Given an embedding dimension r, we would like to learn matrices W j ? R r?kj whose -th column is an embedding vector for the -th value of the j-th attribute. Once they are learned, we let W ? R r?K be the concatenation W = [W 0 , W 1 , . . . , W S?1 ], and define the representation for vertex i as</p><formula xml:id="formula_2">f i = W h i .<label>(1)</label></formula><p>Now it is sufficient to learn the vertex embedding matrix W . We use a CBoW-like pipeline; see Algorithm 1. The intuition is to make sure the attributes h i of a vertex i can be predicted from the h j 's in its neighborhood. Let C i denote the set of vertices linked to i. We will train a neural network h i = g({h j : j ? C i }) so that its output matches h i . As specified in <ref type="figure" target="#fig_5">Figure 1</ref>, the network g first computes j?Ci W h j and then goes through a fully connected network with parameter ? to get? i . Given a dataset S = {G j = (V j , A j )}, the training is by minimizing the cross-entropy loss:</p><formula xml:id="formula_3">min W,? G?S i?G 0? &lt;S cross-entropy(h i ,? i ), subject to [? 0 i ; . . . ;? S?1 i ] = g({h j : j ? C i }). (2)</formula><p>Note that this requires no labels, i.e., it is unsupervised. In fact, W learned from one dataset can be used for another dataset. Moreover, even using random vertex embeddings can give reasonable performance. See Section 5 for more discussions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Vertex Embedding</head><formula xml:id="formula_4">F (1) = F = [f1, . . . , fm], f (1) = F (1) 1 3: for each n ? [2, T ] do 4: F (n) = (F (n?1) A) F 5: f (n) = F (n) 1 6: end for Output: fG = [f (1) ; . . . ; f (T ) ]</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Embedding</head><p>The N-gram graph method is inspired by the N-gram approach in NLP, extending it from linear graphs (sentences) to general graphs (molecules). It views the graph as a Bag-of-Walks and builds representations on them. Let an n-gram refer to a walk of length n in the graph, and the n-gram walk set refer to the set of all walks of length n. The embedding f p ? R r of an n-gram p is simply the element-wise product of the vertex embeddings in that walk. The embedding f (n) ? R r for the n-gram walk set is defined as the sum of the embeddings for all n-grams. The final N-gram graph representation up to length T is denoted as f G ? R T r , and defined as the concatenation of the embeddings of the n-gram walk sets for n ? {1, 2, . . . , T }. Formally, given the vertex embedding f i for vertex i,</p><formula xml:id="formula_5">f p = i?p f i , f (n) = p:n-gram f p , f G = [f (1) ; . . . ; f (T ) ],<label>(3)</label></formula><p>where is the Hadamard product (element-wise multiplication), i.e., if p = (1, 2, 4), then f p = f 1 f 2 f 4 . Now we show that the above Bag-of-Walks view is equivalent to a simple graph neural network in Algorithm 2. Each vertex will hold a latent vector. The latent vector for vertex i is simply initialized to be its embedding f i . At iteration n, each vertex updates its latent vector by element-wise multiplying it with the sum of the latent vectors of its neighbors. Therefore, at the end of iteration n, the latent vector on vertex i is the sum of the embeddings of the walks that ends at i and has length n, and the sum of the all latent vectors is the embedding of the n-gram walk set (with proper scaling). Let F (n) be the matrix whose i-th column is the latent vector on vertex i at the end of iteration n, then we have Algorithm 2 for computing the N-gram graph embeddings. Note that this simple GNN has no parameters and needs no training. The run time is O(rT (m + m e )) where r is the vertex embedding dimension, T is the walk length, m is the number of vertices, and m e is the number of edges.</p><p>By construction, N-gram graph is permutation invariant, i.e., invariant to permutations of the orders of atoms in the molecule. Also, it is unsupervised, so can be used for different tasks on the same dataset, and with different machine learning models. More properties are discussed in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Analysis</head><p>Our analysis follows the framework in <ref type="bibr" target="#b2">[3]</ref>. It shows that under proper conditions, the N-gram graph embeddings can recover the count statistics of walks in the graph, so there is a classifier on the embeddings competitive to any classifier on the count statistics. Note that typically the count statistics can recover the graph. So this shows the strong representation and prediction power. Our analysis makes one mild simplifying assumption:</p><p>? For computing the embeddings, we exclude walks that contain two vertices with exactly the same attributes.</p><p>This significantly simplifies the analysis. Without it, it is still possible to do the analysis but needs a complicated bound on the difference introduced by such walks. Furthermore, we conducted experi-ments on embeddings excluding such walks which showed similar performance (see Appendix I). So analysis under the assumption is sufficient to provide insights for our method. <ref type="bibr" target="#b1">2</ref> The analysis takes the Bayesian view by assuming some prior on the vertex embedding matrix W . This approach has been used for analyzing word embeddings and verified by empirical observations <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b31">32]</ref>. To get some intuition, consider the simple case when we only have S = 1 attribute and consider the 1-gram embedding f <ref type="bibr" target="#b0">(1)</ref> . Recall that f <ref type="bibr" target="#b0">(1)</ref> </p><formula xml:id="formula_6">= p:1-gram i?p f i = i f i = W i h i .</formula><p>Define c (1) := i h i which is the count vector of the occurrences of different types of 1-grams (i.e., vertices) in the graph, and we have f (1) = W c <ref type="bibr" target="#b0">(1)</ref> . It is well known that there are various prior distributions over W such that it has the Restricted Isometry Property (RIP), and if additionally c <ref type="bibr" target="#b0">(1)</ref> is sparse, then c <ref type="bibr" target="#b0">(1)</ref> can be efficiently recovered by various methods in the field of compressed sensing <ref type="bibr" target="#b21">[22]</ref>. This means that f <ref type="bibr" target="#b0">(1)</ref> preserves the information in c <ref type="bibr" target="#b0">(1)</ref> . The preservation then naturally leads to the prediction power <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b2">3]</ref>. Such an argument can be applied to the general case when S &gt; 1 and f (n) with n &gt; 1. We summarize the results below and present the details in Appendix C.</p><p>Representation Power. Given a graph, let us define the bag-of-n-cooccurrences vector c (n) as follows (slightly generalizing <ref type="bibr" target="#b2">[3]</ref>). Recall that S is the number of attributes, and K = S?1 j=0 k j where k j is the number of possible values for the j-th attribute, and the value on the i-th vertex is denoted as V i,j . Definition 1 Given a walk p = (i 1 , . . . , i n ) of length n, the vector e (j) p ? R ( k j n ) is defined as the onehot vector for the j-th attribute values {V i1,j , . . . , V in,j } along the walk. The bag-of-n-cooccurrences vector c (n) is the concatenation of c So c (j) (n) is the histogram of different values of the j-th attribute along the path, and c (n) is a concatenation over all the attributes. It is in high dimension S?1 j=0 kj n . The following theorem then shows that f (n) is a compressed version and preserves the information of bag-of-n-cooccurrences.</p><p>Theorem 1 If r = ?(ns 3 n log K) where s n is the sparsity of c (n) , then there is a prior distribution over W so that f (n) = T (n) c (n) for a linear mapping T (n) . If additionally c (n) is the sparsest vector satisfying f (n) = T (n) c (n) , then with probability 1 ? O(S exp(?(r/S) 1/3 )), c (n) can be efficiently recovered from f (n) .</p><p>The sparsity assumption of c (n) can be relaxed to be close to the sparsest vector (e.g., dense but only a few coordinates have large values), and then c (n) can be approximately recovered. This assumption is justified by the fact that there are a large number of possible types of n-gram while only a fraction of them are presented frequently in a graph. The prior distribution on W can be from a wide family of distributions; see the proof in Appendix C. This can also help explain that using random vertex embeddings in our method can also lead to good prediction performance; see Section 5. In practice, the W is learned and potentially captures better similarities among the vertices.</p><p>The theorem means that f G preserves the information of the count statistics c (n) (1 ? n ? T ). Note that typically, there are no two graphs having exactly the same count statistics, so the graph G can be recovered from f G . For example, consider a linear graph b ? c ? d ? a, whose 2-grams are (b, c), (c, d), (d, a). From the 2-grams it is easy to reconstruct the graph. In such cases, f G can be used to recover G, i.e., f G has full representation power of G.</p><p>Prediction Power. Consider a prediction task and let D (g) denote the risk of a prediction function g over the data distribution D.</p><p>Theorem 2 Let g c be a prediction function on the count statistics c <ref type="bibr">[T ]</ref> . In the same setting as in Theorem 1, with probability 1 ? O(T S exp(?(r/S) 1/3 )), there is a function g f on the N-gram graph embeddings f G with risk D (g f ) = D (g c ).</p><p>So there always exists a predictor on our embeddings that has performance as good as any predictor on the count statistics. As mentioned, in typical cases, the graph G can be recovered from the counts.</p><p>Then there is always a predictor as good as the best predictor on the raw input G. Of course, one would like that not only f G has full information but also the information is easy to exploit. Below we provide the desired guarantee for the standard model of linear classifiers with 2 -regularization. Consider the binary classification task with the logistic loss function (g, y) where g is the prediction and y is the true label. Let D (?) = E D [ (g ? , y)] denote the risk of a linear classifier g ? with weight vector ? over the data distribution D. Let ? * denote the weight of the classifier over c <ref type="bibr">[n]</ref> minimizing</p><formula xml:id="formula_7">D . Suppose we have a dataset {(G i , y i )} M i=1 i.i.d. sampled from D, and? is the weight over f G which is learned via 2 -regularization with regularization coefficient ?: ? = arg min ? 1 M M i=1 ( ?, f Gi , y i ) + ? ? 2 .<label>(4)</label></formula><p>Theorem 3 Assume that f G is scaled so that f G 2 ? 1 for any graph from D. There exists a prior distribution over W , such that with r = ?( ns 3 max 2 log K) for s max = max{s n : 1 ? n ? T } and appropriate choice of regularization coefficient, with probability 1 ? ? ? O(T S exp(?(r/S) 1/3 )), the? minimizing the 2 -regularized logistic loss over the N-gram graph embeddings f Gi 's satisfies</p><formula xml:id="formula_8">D (?) ? D (? * ) + O ? * 2 + 1 M log 1 ? .<label>(5)</label></formula><p>Therefore, the linear classifier over the N-gram embeddings learned via the standard 2 -regularization have performance close to the best one on the count statistics. In practice, the label may depend nonlinearly on the count statistics or the embeddings, so one prefers more sophisticated models. Empirically, we can show that indeed the information in our embeddings can be efficiently exploited by classical methods like random forests and XGBoost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Here we evaluate the N-gram graph method on 60 molecule property prediction tasks, comparing with three types of representations: Weisfeiler-Lehman Kernel, Morgan fingerprints, and several recent graph neural networks. The results show that N-gram graph achieves better or comparable performance to the competitors.</p><p>Methods. <ref type="bibr" target="#b2">3</ref>  <ref type="table" target="#tab_1">Table 1</ref> lists the feature representation and model combinations. Weisfeiler-Lehman (WL) Kernel <ref type="bibr" target="#b48">[49]</ref>, Support Vector Machine (SVM), Morgan Fingerprints, Random Forest (RF), and XGBoost (XGB) <ref type="bibr" target="#b14">[15]</ref> are chosen since they are the prototypical representation and learning methods in these domains. Graph CNN (GCNN) <ref type="bibr" target="#b1">[2]</ref>, Weave Neural Network (Weave) <ref type="bibr" target="#b32">[33]</ref>, and Graph Isomorphism Network (GIN) <ref type="bibr" target="#b57">[58]</ref> are end-to-end graph neural networks, which are recently proposed deep learning models for handling molecular graphs. Datasets. We test 6 regression and 4 classification datasets, each with multiple tasks. Since our focus is to compare the representations of the graphs, no transfer learning or multi-task learning is considered. In other words, we are comparing each task independently, which gives us 28 regression tasks and 32 classification tasks in total. See <ref type="table" target="#tab_5">Table S5</ref> for a detailed description of the attributes for the vertices in the molecular graphs from these datasets. All datasets are split into five folds and with cross-validation results reported as follows.</p><p>? Regression datasets: Delaney <ref type="bibr" target="#b17">[18]</ref>, Malaria <ref type="bibr" target="#b22">[23]</ref>, CEP <ref type="bibr" target="#b28">[29]</ref>, QM7 <ref type="bibr" target="#b7">[8]</ref>, QM8 <ref type="bibr" target="#b42">[43]</ref>, QM9 <ref type="bibr" target="#b45">[46]</ref>. ? Classification datasets: Tox21 <ref type="bibr" target="#b50">[51]</ref>, ClinTox <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b6">7]</ref>, MUV <ref type="bibr" target="#b44">[45]</ref>, HIV <ref type="bibr" target="#b0">[1]</ref>.</p><p>Evaluation Metrics. Same evaluation metrics are utilized as in <ref type="bibr" target="#b55">[56]</ref>. Note that as illustrated in Appendix D, labels are highly skewed for each classification task, and thus ROC-AUC or PR-AUC is used to measure the prediction performance instead of accuracy.</p><p>Hyperparameters. We tune the hyperparameter carefully for all representation and modeling methods. More details about hyperparameters are provided in Section Appendix F. The following subsections display results with the N-gram parameter T = 6 and the embedding dimension r = 100.   Performance. <ref type="table" target="#tab_2">Table 2</ref> summarizes the prediction performance of the methods on all 60 tasks. Since (1) no method can consistently beat all other methods on all tasks, and (2) for datasets like QM8, the error (MAE) of the best models are all close to 0, we report both the top-1 and top-3 number of tasks each method obtained. Such high-level overview can help better understand the model performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NR -A R NR -A R-LB D NR -A hR NR -</head><p>Complete results are included in Appendix H.</p><p>Overall, we observe that N-gram graph, especially using XGBoost, shows better performance than the other methods. N-gram with XGBoost is in top-1 for 21 out of 60 tasks, and is in top-3 for 48. On some tasks, the margin is not large but the advantage is consistent; see for example the tasks on the dataset Tox21 in <ref type="figure" target="#fig_2">Figure 2</ref>(a). On some tasks, the advantage is significant; see for example the tasks u0, u298, h298, g298 on the dataset QM9 in <ref type="figure" target="#fig_2">Figure 2</ref>(b).</p><p>We also observe that random forest on Morgan fingerprints has performance beyond general expectation, in particular, better than the recent graph neural network models on the classification tasks. One possible explanation is that we have used up to 4000 trees and obtained improved performance compared to 75 trees as in <ref type="bibr" target="#b55">[56]</ref>, since the number of trees is the most important parameter as pointed out in <ref type="bibr" target="#b36">[37]</ref>. It also suggests that Morgan fingerprints indeed contains sufficient amount of information for the classification tasks, and methods like random forest are good at exploiting them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transferable Vertex</head><p>Embedding. An intriguing property of the vertex embeddings is that they can be transferred across datasets. We evaluate N-Gram graph with XGB on Tox21, using different vertex embeddings: trained on Tox21, random, and trained on other datasets. See details in Appendix G.1. <ref type="table" target="#tab_3">Table 3</ref> shows that embeddings from other datasets can be used to get comparable results. Even random embeddings can get good results, which is explained in Section 4. Computational Cost. <ref type="table" target="#tab_4">Table 4</ref> depicts the representation construction time of different methods.</p><p>Since vertex embeddings can be amortized across different tasks on the same dataset or even transferred, the main runtime of our method is from the graph embedding step. It is relatively efficient, much faster than the GNNs and the kernel method, though Morgan fingerprints can be even faster. Comparison to models using 3D information. What makes molecular graphs more complicated is that they contain 3D information, which is helpful for making predictions <ref type="bibr" target="#b25">[26]</ref>. Deep Tensor Neural Networks (DTNN) <ref type="bibr" target="#b46">[47]</ref> and Message-Passing Neural Networks (MPNN) <ref type="bibr" target="#b25">[26]</ref> are two graph neural networks that are able to utilize 3D information encoded in the datasets. <ref type="bibr" target="#b3">4</ref> Therefore, we further compare our method to these two most advanced GNN models, on the two datasets QM8 and QM9 that have 3D information. The results are summarized in <ref type="table" target="#tab_5">Table 5</ref>. The detailed results are in <ref type="table" target="#tab_1">Table S17</ref> and the computational times are in <ref type="table" target="#tab_1">Table S18</ref>. They show that our method, though not using 3D information, still gets comparable performance. Effect of r and T . We also explore the effect of the two key hyperparameters in N-Gram graph: the vertex embedding dimension r and the N-gram length T . <ref type="figure" target="#fig_4">Figure 3</ref> shows the results of 12 classification tasks on the Tox21 dataset are shown in, and <ref type="figure" target="#fig_2">Figure S2</ref> shows the results on 3 regression tasks on the datasets Delaney, Malaria, and CEP. They reveal that generally, r does not affect the model performance while increasing T can bring in significant improvement. More detailed discussions are in appendix K.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper introduced a novel representation method called N-gram graph for molecule representation. It is simple, efficient, yet gives compact representations that can be applied with different learning methods. Experiments show that it can achieve overall better performance than prototypical traditional methods and several recent graph neural networks.</p><p>The method was inspired by the recent word embedding methods and the traditional N-gram approach in natural language processing, and can be formulated as a simple graph neural network. It can also be used to handle general graph-structured data, such as social networks. Concrete future works include applications on other types of graph-structured data, pre-training and fine-tuning vertex embeddings, and designing even more powerful variants of the N-gram graph neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Related Work</head><p>There are a large number of works along the line of machine learning for molecules and we review the more related ones here.</p><p>The adoption of sophisticated machine learning methods, in particular deep learning methods, has been recent trend in the domains of medicine, biology, chemistry, etc <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b8">9]</ref>. Deep learning methods started to capture the attention among scientists in the drug discovery domain from Merck Molecular Activity Challange <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b16">17]</ref>. Efforts expanded to investigate the benefits of multi-task deep neural networks, frequently showing outstanding performance when comparing with shallow models <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b36">37]</ref>. All of these works used Morgan fingerprints as input representations.</p><p>Another option for molecule representation is the SMILES string <ref type="bibr" target="#b53">[54]</ref>. SMILES can be treated as a sequence of atoms and bonds, and each molecule has a unique canonical SMILES string among a frequently vast set of noncanonical, but completely valid, SMILES strings. Therefore, attempts were made to make SMILES feed into more complicated neural networks. <ref type="bibr" target="#b30">[31]</ref> applied recurrent neural network language model (RNN) and convolutional neural networks (CNN) on SMILES, and showed that CNN is best when evaluated on the log-loss. SMILES as the representation is now common in molecule generation tasks. <ref type="bibr" target="#b26">[27]</ref> first applied SMILES for automatic molecule design, and <ref type="bibr" target="#b33">[34]</ref> proposed using a parser tree on SMILES so as to produce more grammatically-valid molecules, where the input is the one-hot encoded rules. On the other hand, <ref type="bibr" target="#b36">[37]</ref> showed the limitation of SMILES and itself as a structured data is hard to interpret, and thus SMILES are not used in our experiments.</p><p>Molecular descriptors <ref type="bibr" target="#b49">[50]</ref> is another representation, but it requires heuristically coming up with descriptors and dynamically adjusting it to tasks, which is not easy and requires a lot of domain knowledge. Therefore molecular descriptors are not considered in this paper since one of the goal here is to get a generalized feature representation.</p><p>Recent works started to explore the graph representation, and the benefit is its capability to encode the structured data. <ref type="bibr" target="#b18">[19]</ref> first utilized message passing on graphs. At each step, this method passes the hidden message layer to the intermediate feature layer. The summed-up neural fingerprints are then fed into neural networks as features. Following this line of research, <ref type="bibr" target="#b1">[2]</ref> made small adaptations by using the last message layer as feature inputs for neural network, and <ref type="bibr" target="#b59">[60]</ref> proposed a differential pooling layer to learn the hierarchical information.</p><p>Other variants introduced different modules. <ref type="bibr" target="#b32">[33]</ref> proposed a new module called weave for delivering information among atoms and bonds, and <ref type="bibr" target="#b38">[39]</ref> used a weave operation with forward and backward operations across a molecule graph. <ref type="bibr" target="#b35">[36]</ref> utilized edge information, and <ref type="bibr" target="#b19">[20]</ref> generalized it into a message passing network framework, highlighting the importance of spatial information.</p><p>Viewing the molecules as graphs, the kernel method can be applied by using existing graph kernels (e.g., <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b48">49]</ref>). The implicit feature mapping induced by the kernel can be viewed as the representation for the input. The Weisfeiler-Lehman kernel <ref type="bibr" target="#b48">[49]</ref> is particularly related due to its efficiency and theoretical backup. It is also similar in spirit to the Morgan fingerprints and closely related to the recent GIN graph neural network <ref type="bibr" target="#b57">[58]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Background and Preliminaries</head><p>Generally, molecules can be viewed as graphs on atoms together with attribute information of the atoms, and we assume our molecule datasets are given in the format. <ref type="bibr" target="#b4">5</ref> To apply learning methods, they are converted to feature vectors (fingerprints), or are directly handled by specifically designed learning models (graph neural networks). The fingerprints or the hidden layers of graph neural networks are regarded as the representations or embeddings of the graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Raw Data: Representation as Graphs With Vertex Attributes</head><p>Nearly all molecules can be potentially represented as a graph, where each atom is a vertex and each bond is an edge. Suppose there are m vertices in the graph, denoted as i ? {0, 1, ..., m ? 1}. Each vertex entails useful attribute information, like the atom symbol and number of charges for atom vertices. These vertex attributes are encoded into a vertex attribute matrix V ? {0, 1} m?S , where S is the number of attributes. A concrete example is given by the following:</p><formula xml:id="formula_9">V i,? = [V i,0 , V i,1 , . . . , V i,6 , V i,7 ], atom symbol V i,0 ? {C, Cl, I, F, . . .}, atom degree V i,1 ? {0, 1, 2, 3, 4, 5, 6}, . . . is acceptor V i,6 ? {0, 1}, is donor V i,7 ? {0, 1}.<label>(6)</label></formula><p>Note that the attributes typically have discrete values.</p><p>The bonding information is encoded into the adjacency matrix A ? {0, 1} m?m , where A i,j = 1 if and only if two vertices i and j are linked.</p><p>We let G = (V, A) denote a molecular graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Fingerprints</head><p>We review two prototype methods here. Morgan fingerprints and its variants <ref type="bibr" target="#b41">[42]</ref> have been one of the most widely used featurization methods in virtual screening. It is an iterative algorithm that encodes the circular substructures of the molecule as identifiers at increasing levels with each iteration. In each iteration, hashing is applied to generate new identifiers, and thus, there is a chance that two substructures are represented by the same identifier. In the end, a list of identifiers encoding the substructures is folded to bit positions of a fixed-length bit string.  Another prototypical method, Simplified Molecular Input Line Entry System (SMILES) <ref type="bibr" target="#b53">[54]</ref>, is a character sequence describing molecular structures. There are some inherent issues in SMILES, the biggest being that molecules cannot be simply represented as a linear sequence: the properties of drug-like organic molecules usually have dependence on ring structures and tree-like branching, whose information is lost in a linear sequence. Our experiments show that it generally achieves worse performance than the other methods, so it is not considered as a competitor in the experimental section.</p><p>One example of molecule as a graph is shown in <ref type="figure" target="#fig_5">Figure S1</ref>, together with its Morgan fingerprint and SMILES molecule representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Graph Neural Networks</head><p>In recent works, message passing has been dominant in graph neural networks <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b56">57]</ref>. A GNN keeps a vector h i for each vertex i and uses some neighborhood aggregation strategy that iteratively updates the vector by aggregating those of its neighbors. After t iterations, each vertex is able to capture the information of the vertices at most t-hops away. Formally, the k-th iteration is to compute</p><formula xml:id="formula_10">f (k) i = AGGREGATE (k) ({h (k?1) j : j ? Neighbor(i)}), h (k) i = COMBINE (k) (h (k?1) i , f (k) i ),<label>(7)</label></formula><p>where h (k) i is the value of h i at the k-th iteration, h</p><p>i is typically initialized to the attribute vector of the vertex, and AGGREGATE (k) and COMBINE (k) are carefully chosen functions. The representation for the whole graph is then some aggregation of the vertex vectors. Such a framework has been used in the domains of molecules, but in general needs to be carefully specialized to this setting, see, e.g., <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b32">33]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Complete Proofs for Theoretical Analysis C.1 Preliminary</head><p>Here we provide a brief review of related concepts in the field of compressed sensing that are important for our analysis, following <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">32]</ref>. For a review with details, please refer to <ref type="bibr" target="#b21">[22]</ref>.</p><p>The primary goal of compressed sensing is to recover a high-dimensional k-sparse signal x ? R N from a few linear measurements. Here, being k-sparse means that x has at most k non-zero entries, i.e., |x| 0 ? k. In the noiseless case, we have a design matrix A ? R d?N and the measurement vector is z = Ax. The optimization formulation is then</p><formula xml:id="formula_12">minimize x x 0 subject to Ax = z<label>(8)</label></formula><p>where x 0 is 0 norm of x , i.e., the number of non-zero entries in x . The assumption that x is the sparsest vector satisfying Ax = z is equivalent to that x is the optimal solution for <ref type="bibr" target="#b7">(8)</ref>.</p><p>Unfortunately, the 0 -minimization in <ref type="formula" target="#formula_12">(8)</ref> is NP-hard. The typical approach in compressed sensing is to consider its convex surrogate using 1 -minimization:</p><formula xml:id="formula_13">minimize x x 1 subject to Ax = z<label>(9)</label></formula><p>where x 1 = i |x i | is the 1 norm of x . The fundamental question is when the optimal solution of (8) is equivalent to that of (9), i.e., when exact recovery is guaranteed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.1 The Restricted Isometry Property</head><p>One common condition for recovery is the Restricted Isometry Property (RIP):</p><formula xml:id="formula_14">Definition 2 A ? R d?N is (X , )-RIP for some subset X ? R N if for any x ? X , (1 ? ) x 2 ? Ax 2 ? (1 + ) x 2 .</formula><p>We will abuse notation and say (k, )-RIP if X is the set of all k-sparse x ? R N .</p><p>Introduced by <ref type="bibr" target="#b12">[13]</ref>, RIP has been used to show to guarantee exact recovery.</p><p>Theorem 4 (Restatement of Theorem 1.1 in <ref type="bibr" target="#b11">[12]</ref>) Suppose A is (2k, )-RIP for an &lt; ? 2 ? 1. Letx denote the solution to <ref type="bibr" target="#b8">(9)</ref>, and let x k denote the vector x with all but the k-largest entries set to zero. Then</p><formula xml:id="formula_15">x ? x 1 ? C 0 x k ? x 1 and x ? x 2 ? C 0 k ?1/2 x k ? x 1 .</formula><p>In particular, if x is k-sparse, the recovery is exact.</p><p>Furthermore, it has been shown that A is (k, )-RIP with overwhelming probability when d = ?(k log N k ) and <ref type="figure" target="#fig_5">1}(?i, j)</ref>.</p><formula xml:id="formula_16">? dA ij ? N (0, 1)(?i, j) or ? dA ij ? U{?1,</formula><p>For our purpose, we also concern about whether the -way column Hadamard-product of A has RIP.</p><p>Definition 3 ( -way Column Hadamard Product) Let A be a d ? N matrix, and let be a natural integer. The -way column Hadamard-product of A is a d ? N matrix denoted as A ( ) , whose columns indexed by a sequence 1 ? i 1 &lt; i 2 ? ? ? &lt; i ? d is the element-wise product of the i 1 , i 2 , . . . , i -th columns of A, i.e., (i 1 , i 2 , . . . , i )-th column in</p><formula xml:id="formula_17">A ( ) is A i1 A i2 ? ? ? A i where A j for j ? [N ] is the j-th column in A.</formula><p>We have the following theorems:</p><p>Theorem 5 (Restatement of Theorem 4.1 in <ref type="bibr" target="#b31">[32]</ref>) Let X be an n ? d matrix, and let A be a d ? N random matrix with independent entries R ij such that E[R ij ] = 0, E[R ij ] = 1, and |R ij | ? ? almost surely. <ref type="figure" target="#fig_5">Let ? (0, 1)</ref>, and let k be an integer satisfying sr(X) ? C? 8 2 k 2 log N 2 k for some universal constant C &gt; 0. Then with probability at least 1 ? exp(?c 2 sr(X)/(k 2 ? 8 )) for some universal constant c &gt; 0, the matrix XA ( ) / X F is (k, )-RIP.</p><p>Here, sr(X) = X 2 F / X 2 is the stable rank of X. In our case, we will apply the theorem with X being</p><formula xml:id="formula_18">I d?d / ? d where I d?d ? R d?d is the identity matrix.</formula><p>Theorem 6 (Restatement of Theorem 4.3 in <ref type="bibr" target="#b31">[32]</ref>) Let X be an n ? d matrix, and let A be a d ? N random matrix with independent entries R ij such that E[R ij ] = 0, E[R ij ] = 1, and |R ij | ? ? almost surely. Let ? 3 be a constant. Let ? (0, 1), and let k be an integer satisfying sr(X) ? C? 4 2 k 3 log N k for some universal constant C &gt; 0. Then with probability at least 1 ? exp(?c 2 sr(X)/(k 2 ? 4 )) for some universal constant c &gt; 0, the matrix XA ( ) / X F is (k, )-RIP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.2 Compressed Learning</head><p>Given that Ax preserves the information of sparse x when A is RIP, it is then natural to study the performance of a linear classifier learned on Ax compared to that of the best linear classifier on x. Our analysis will use a theorem from <ref type="bibr" target="#b2">[3]</ref> that generalizes that of <ref type="bibr" target="#b9">[10]</ref>. </p><formula xml:id="formula_19">Let X ? R N denote X = {x : x ? R N , x 0 ? k, x 2 ? B}. Let {(x i , y i )} M i=1</formula><formula xml:id="formula_20">? A = arg min ? 1 M M i=1 ( ?, Ax i , y i ) + ? ? 2<label>(10)</label></formula><p>where ? is the regularization coefficient.</p><formula xml:id="formula_21">Theorem 7 (Restatement of Theorem 4.2 in [3]) Suppose A is (?X , )-RIP. Then with probability at least 1 ? ?, A D (? A ) ? D (? * ) + O ? B ? * + 1 M log 1 ?</formula><p>for appropriate choice of C. Here, ?X = {x ? x : x, x ? X } for any X ? R N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Representation Power</head><p>In this subsection, we provide the proof of Theorem 1.</p><p>We begin by defining the distribution over the vertex embedding matrix W . Recall that k j is the number of possible values for the j-th attribute. Suppose we have numbers r j ? (0, r) so that S?1 j=0 r j = r whose values will be specified later. Let</p><formula xml:id="formula_22">W = ? ? ? U 0 0 ? ? ? 0 0 U 1 ? ? ? 0 ? ? ? ? ? ? ? ? ? ? ? ? 0 0 ? ? ? U S?1 ? ? ? (11)</formula><p>where U j ? R rj ?kj . Now let's specify U j . Let the entries in U j 's are independent random variables, and let the entries be uniform from {?1, 1} with some scaling factor c u , i.e., (U j ) ik ? c u ?U{?1, 1}, where the value of c u will be determined later. <ref type="bibr" target="#b5">6</ref> Now, let (U j ) (n) denote the n-way column Hadamard product of U j , and let</p><formula xml:id="formula_23">T (n) = ? ? ? ? (U 0 ) (n) 0 ? ? ? 0 0 (U 1 ) (n) ? ? ? 0 ? ? ? ? ? ? ? ? ? ? ? ? 0 0 ? ? ? (U S?1 ) (n) ? ? ? ?<label>(12)</label></formula><p>Then it can be verified that</p><formula xml:id="formula_24">f (n) = T (n) c (n) .<label>(13)</label></formula><p>Now we can apply Theorem 5 for n = 2 and Theorem 6 for n ? 3 on each (U j ) (n) . Let s n,j denote the sparsity of c (j) (n) . Then with r j ? ?(ns 3 n,j log k j ) and appropriate set scaling factor c u , we have that with probability at least 1 ? exp(?cr j /s 2 n,j ), (U j ) (n) is (2s n,j , )-RIP for = 0.1. This then means that f n can be exactly recovered from c (n) by Theorem 4. Now, by setting r = ?(ns 3 n log K) where s n = S?1 j=0 s n,j , we can choose r j 's satisfying r j = ?(ns 3 n,j log k j + r/S). Furthermore, we have r j /s 2 n,j = ?(r </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Prediction Power</head><p>Proof of Theorem 2. Theorem 2 is a direct consequence of Theorem 1.</p><p>By Theorem 1, under the conditions, we have that there exists a mapping M (n) from f G to c (n) . Therefore, there exists a mapping M <ref type="bibr">[T ]</ref> from f G to c <ref type="bibr">[T ]</ref> , by applying M (n) 's on each blocks of f G , respectively. Now, define g f = g c ? M <ref type="bibr">[T ]</ref> , such that</p><formula xml:id="formula_25">g f (f G ) = g c ? M [T ] (f G ) = g c (c [T ] ), so D (g f ) = D (g c ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof of Theorem 3 Let</head><formula xml:id="formula_26">T [T ] = ? ? ? T (1) 0 ? ? ? 0 0 T (2) ? ? ? 0 ? ? ? ? ? ? ? ? ? ? ? ? 0 0 ? ? ? T (T ) ? ? ?<label>(14)</label></formula><p>where T (n) (1 ? n ? T ) is defined as in <ref type="bibr" target="#b11">(12)</ref>. Then it can be verified that</p><formula xml:id="formula_27">f G = T [T ] c [T ] .<label>(15)</label></formula><p>Under the specified conditions we have that with high probability T (n) 's are (2s n , )-RIP, so T [T ] is (?X , )-RIP. Since the logistic loss is 1-Lipschitz convex, the statement follows from Theorem 7, while the failure probability follows from a union bound.    <ref type="table" target="#tab_5">Tables S5 and S6</ref> show the types of feature attributes for the atoms in the molecules of the datasets used in our experiments. Also in Appendix L, we can observe that the selection of feature attribute values, especially adding more atom symbols, has very limited improvement.   <ref type="bibr" target="#b34">[35]</ref> and follow the hyperparameters from benchmark <ref type="bibr" target="#b43">[44]</ref>: the number of bits is 1024 and radius is 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Task Specification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Atom Feature Specification</head><p>Graph Neural Networks. For Graph CNN, Weave Neural Network, Deep Tensor Neural Network, and Message-Passing Neural Network, we follow the optimal hyperparameter schemes provided in <ref type="bibr" target="#b55">[56]</ref>. Note that they are tuned for each of these datasets, respectively, to guarantee the optimality.</p><p>N-Gram Graph. The hyperparameters for N-gram graph are included in <ref type="table" target="#tab_11">Table S7</ref>, and the effects of two important hyperparameters (random dimension r and n-gram number T ) will be discussed in Appendix K. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Hyperparameters for Modeling</head><p>For other baseline models, we run a grid search for hyperparameter sweeping, including Weisfeiler-Lehman Graph Kernel in <ref type="table" target="#tab_12">Table S8</ref>, random forest in <ref type="table" target="#tab_13">Table S9</ref>, XGBoost in <ref type="table" target="#tab_1">Table S10</ref>, and Graph Isomorphism Network <ref type="table" target="#tab_1">Table S11</ref>.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Vertex Embedding</head><p>The CBoW-like neural network structure is displayed in <ref type="figure" target="#fig_5">Figure 1</ref>. Though the vertex embedding step is unsupervised, we still follow the 5-fold cross-validation, so as not to touch the test set before prediction. In other words, we will create 5 CBoW-like models for each task (or dataset 7 ) and each vertex embedding dimension R. We report the test accuracy during vertex embedding in <ref type="table" target="#tab_1">Table S12</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.1 Transferable Vertex Embedding</head><p>The complete process for getting <ref type="table" target="#tab_3">Table 3</ref> is as follows.</p><p>Vertex Embedding. Train the unsupervised CBoW model for vertex embedding W on all the molecules from the source dataset. For random projection, we just initialize parameters of the CBoW model under the Gaussian distribution, and only molecules for that task is used if it comes from Tox21, i.e., the non-transfer case.</p><p>Graph Embedding. Apply W on molecules from target task for the graph embedding, f G . Then train the model based on f G .  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Complete Results on 60 Regression and Classification Tasks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I N-Gram Walk vs. N-Gram Path</head><p>We compare N-Gram Path (the version of N-gram graph method that excludes walks containing two vertices with the same attribute values) and N-Gram Walk (the version of N-gram graph method that does not exclude such walks) on each of the 60 tasks. The same vertex embeddings are used, and both random forest (RF) and XGBoost (XGB) are experimented on top of the N-Gram graph embeddings. All regression tasks are shown in <ref type="table" target="#tab_1">Table S15</ref> and all classification tasks are shown in <ref type="table" target="#tab_1">Table S16</ref>, and we can see that N-Gram Path is comparable to N-Gram Walk.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J Additional Experiments on Datasets with 3D Information</head><p>Since 3D information of the atoms in the molecules is important for making predictions <ref type="bibr" target="#b25">[26]</ref>, we also performed experiments comparing out method to two recent models designed to exploit 3D information: Deep Tensor Neural Networks (DTNN) <ref type="bibr" target="#b46">[47]</ref> and Message-Passing Neural Networks (MPNN) <ref type="bibr" target="#b25">[26]</ref>. We evaluated them on the two datasets QM8 and QM9 that have 3D information.</p><p>The detailed results are in <ref type="table" target="#tab_1">Table S17</ref> and the summary is in <ref type="table" target="#tab_5">Table 5</ref>. The computational time can be referred to <ref type="table" target="#tab_1">Table S18</ref>. The results show that our method, though not using 3D information, can get comparable performance. <ref type="table" target="#tab_1">Table S17</ref>: Here we include the performance on 2 regression datasets with 9 models. All experiments are done on a 5-fold cross-validation, and the mean evaluation of 5 runs is reported here. The top-3 models are bolded, and the best model is underlined.  We run N-gram graph on 12 classification tasks from "Toxicology in the 21st Century" <ref type="bibr" target="#b50">[51]</ref>. We tested the effects of vertex embedding dimension r and N-gram parameter T on the prediction performance measured by ROC-AUC. The results are shown in <ref type="figure" target="#fig_4">Figure 3</ref>.</p><p>As observed from <ref type="figure" target="#fig_4">Figure 3</ref>, for the 12 tasks from Tox21, there generally exists a raise as T gets higher. This makes sense since it covers more information as we are looking more steps ahead. Besides, the ROC-AUC values on the test set are not increasing as r increases. Two possible reasons for this: (1) Data is insufficient. As shown in Tables S1 to S4, all Tox21 tasks have less than 10,000 molecules.</p><p>(2) ROC-AUC reveals the ranking of predictions, while other evaluation metrics, like RMSE shown in <ref type="figure" target="#fig_2">Figure S2</ref>, are likely to measure the predictions in a finer-grained way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K.2 On 3 Regression Tasks (Delaney, Malaria, CEP)</head><p>We run N-gram graph on 3 regression tasks, Delaney, Malaria, and CEP. We tested the effects of vertex embedding dimension r and N-gram parameter T .</p><p>Similarly to <ref type="figure" target="#fig_4">Figure 3</ref>, increasing T can help reduce the loss, while different vertex embedding dimension, i.e.r, presents comparatively unstable performance. Performance on the three regression tasks in <ref type="figure" target="#fig_2">Figure S2</ref> fluctuates a lot as r and T increases. One conjecture is that such high variance is caused by the data insufficiency. However, we can still conclude that for each machine learning algorithm, r = 100 and T = 6 are reasonable to choose. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>L Exploring the Effects of Atom Features</head><p>To further prove that different atom features are not biasing the graph neural networks, we compare the different atom attribute schemes. In N-Gram graph, we are using <ref type="table" target="#tab_5">Table S5</ref> (called new attribute scheme), while in the benchmark paper <ref type="bibr" target="#b55">[56]</ref>, it has more atom symbols, and may not include attributes like "is acceptor" or "is donor" (called original attribute scheme). We did a statistical test to measure the difference from two atom attribute schemes as in <ref type="table" target="#tab_1">Table S19</ref>. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>= p e (j) p with the sum over all paths p of length n. Furthermore, let the count statistics c [T ] be the concatenation of c (1) , . . . , c (T ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>A ro ma ta se NR -E R NR -E R-LB D NR -PP AR -g am ma SR -A RE SR -A TA D5 SR -H SE SR -M MP SR -ROC-AUC of the best models on Tox21 (Morgan+RF, GCNN, N-gram+XGB). Larger is better. MAE of the best models on QM9 (GCNN, Weave, N-gram+XGB). Smaller is better.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Performance of the best models on the datasets Tox21 and QM9, averaged over 5-fold cross-validation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Effects of vertex embedding dimension r and N-gram dimension T on 12 tasks from Tox21: the effect of r and T on ROC-AUC. x-axis: the hyperparameter T ; y-axis: ROC-AUC. Different lines correspond to different methods and different values of r.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>A 1 -</head><label>1</label><figDesc>bit at a particular position indicates the presence of a substructure (or multiple substructures if they are all hashed to this position) and a 0-bit indicates the absence of corresponding substructures. Due to the hashing collisions, it is difficult to interpret such fingerprints and examine how the machine learning systems utilize them.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure S1 :</head><label>S1</label><figDesc>Illustration of the Morgan fingerprint and SMILES molecule representations. The molecule is displayed on a 2D space. The corresponding canonical SMILES is c1cc(oc1C(=O)Nc2nc(cs2)C(=O)OCC)Br, and Morgan Fingerprints is, for example, [000000...00100100100...000000].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>be a set of M samples i.i.d. from some distribution over X ? {?1, 1}. Let denote a ? -Lipschitz convex loss function. Let D (?) denote the risk of a linear classifier with weight ? ? R N , i.e., D (?) = E[ ( ?, x , y)], and let ? * denote a minimizer of D (?). Let A D (?) denote the risk of a linear classifier with weight ? ? R d over Ax, i.e., A D (? A ) = E[ ( ? A , Ax , y)], and let? A denote the weight learned with 2 -regularization over {(Ax i , y i )} i :</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1 / 3 j</head><label>13</label><figDesc>) = ?((r/S) 1/3 ), so the failure probability is bounded by S exp(?c(r/S) 1/3 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure S2 :</head><label>S2</label><figDesc>Effects of vertex embedding dimension r and N-gram dimension T on tasks Delaney, Malaria and CEP: how the RMSE on validation set changes as different r and T .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Feature Representation</cell><cell>Model</cell></row><row><cell>Weisfeiler-Lehman Graph Kernel</cell><cell>SVM</cell></row><row><cell>Morgan Fingerprints</cell><cell>RF, XGB</cell></row><row><cell>Graph Neural Network</cell><cell>GCNN, Weave, GIN</cell></row><row><cell>N-Gram Graph</cell><cell>RF, XGB</cell></row></table><note>Feature representation for each different machine learning model. Both Morgan fingerprints and N-gram graph are used with Random Forest (RF) and XGBoost (XGB).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Performance overview: (# of tasks with top-1 performance, # of tasks with top-3 performance) is listed for each model and each dataset. For cases with no top-3 performance on that dataset are left blank. Some models are not well tuned or too slow and are left in "-".</figDesc><table><row><cell>Dataset</cell><cell># Task</cell><cell>Eval Metric</cell><cell>WL SVM</cell><cell>Morgan RF</cell><cell>Morgan XGB</cell><cell>GCNN</cell><cell>Weave</cell><cell>GIN</cell><cell>N-Gram RF</cell><cell>N-Gram XGB</cell></row><row><cell>Delaney</cell><cell>1</cell><cell>RMSE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>1, 1</cell><cell>-</cell><cell>0, 1</cell><cell>0, 1</cell></row><row><cell>Malaria</cell><cell>1</cell><cell>RMSE</cell><cell></cell><cell>1, 1</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>0, 1</cell><cell>0, 1</cell></row><row><cell>CEP</cell><cell>1</cell><cell>RMSE</cell><cell></cell><cell>1, 1</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>0, 1</cell><cell>0, 1</cell></row><row><cell>QM7</cell><cell>1</cell><cell>MAE</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0, 1</cell><cell>-</cell><cell>0, 1</cell><cell>1, 1</cell></row><row><cell>QM8</cell><cell>12</cell><cell>MAE</cell><cell></cell><cell>1, 4</cell><cell>0, 1</cell><cell>7, 12</cell><cell>2, 6</cell><cell>-</cell><cell>0, 2</cell><cell>2, 11</cell></row><row><cell>QM9</cell><cell>12</cell><cell>MAE</cell><cell>-</cell><cell></cell><cell>0, 1</cell><cell>4, 7</cell><cell>1, 8</cell><cell>-</cell><cell>0, 8</cell><cell>7, 12</cell></row><row><cell>Tox21</cell><cell>12</cell><cell>ROC-AUC</cell><cell>0, 2</cell><cell>0, 7</cell><cell></cell><cell>0, 2</cell><cell>0, 1</cell><cell></cell><cell>3, 12</cell><cell>9, 12</cell></row><row><cell>clintox</cell><cell>2</cell><cell>ROC-AUC</cell><cell>0, 1</cell><cell></cell><cell></cell><cell>1, 2</cell><cell>0, 1</cell><cell></cell><cell></cell><cell>1, 2</cell></row><row><cell>MUV</cell><cell>17</cell><cell>PR-AUC</cell><cell>4, 12</cell><cell>5, 11</cell><cell>5, 11</cell><cell></cell><cell></cell><cell>0, 7</cell><cell>2, 4</cell><cell>1, 6</cell></row><row><cell>HIV</cell><cell>1</cell><cell>ROC-AUC</cell><cell></cell><cell>1, 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0, 1</cell><cell>0, 1</cell></row><row><cell>Overall</cell><cell>60</cell><cell></cell><cell>4, 15</cell><cell>9, 25</cell><cell>5, 13</cell><cell>12, 23</cell><cell>4, 18</cell><cell>0, 7</cell><cell>5, 31</cell><cell>21, 48</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>AUC-ROC of N-Gram graph with XGB on 12 tasks from Tox21. Six vertex embeddings are considered: non-transfer (trained on Tox21), vertex embeddings generated randomly and learned from 4 other datasets.</figDesc><table><row><cell></cell><cell>Non-Transfer</cell><cell>Random</cell><cell>Delaney</cell><cell>CEP</cell><cell>MUV</cell><cell>Clintox</cell></row><row><cell>NR-AR</cell><cell>0.791</cell><cell>0.790</cell><cell>0.785</cell><cell>0.787</cell><cell>0.796</cell><cell>0.780</cell></row><row><cell>NR-AR-LBD</cell><cell>0.864</cell><cell>0.846</cell><cell>0.863</cell><cell>0.849</cell><cell>0.864</cell><cell>0.867</cell></row><row><cell>NR-AhR</cell><cell>0.902</cell><cell>0.895</cell><cell>0.903</cell><cell>0.892</cell><cell>0.901</cell><cell>0.903</cell></row><row><cell>NR-Aromatase</cell><cell>0.869</cell><cell>0.858</cell><cell>0.867</cell><cell>0.848</cell><cell>0.858</cell><cell>0.866</cell></row><row><cell>NR-ER</cell><cell>0.753</cell><cell>0.751</cell><cell>0.752</cell><cell>0.740</cell><cell>0.735</cell><cell>0.747</cell></row><row><cell>NR-ER-LBD</cell><cell>0.838</cell><cell>0.820</cell><cell>0.843</cell><cell>0.820</cell><cell>0.827</cell><cell>0.847</cell></row><row><cell>NR-PPAR-gamma</cell><cell>0.851</cell><cell>0.809</cell><cell>0.862</cell><cell>0.813</cell><cell>0.832</cell><cell>0.857</cell></row><row><cell>SR-ARE</cell><cell>0.835</cell><cell>0.823</cell><cell>0.841</cell><cell>0.814</cell><cell>0.835</cell><cell>0.842</cell></row><row><cell>SR-ATAD5</cell><cell>0.860</cell><cell>0.830</cell><cell>0.844</cell><cell>0.817</cell><cell>0.845</cell><cell>0.857</cell></row><row><cell>SR-HSE</cell><cell>0.812</cell><cell>0.777</cell><cell>0.806</cell><cell>0.768</cell><cell>0.805</cell><cell>0.810</cell></row><row><cell>SR-MMP</cell><cell>0.918</cell><cell>0.909</cell><cell>0.918</cell><cell>0.902</cell><cell>0.916</cell><cell>0.919</cell></row><row><cell>SR-p53</cell><cell>0.868</cell><cell>0.856</cell><cell>0.869</cell><cell>0.841</cell><cell>0.856</cell><cell>0.870</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Representation construction time in seconds. One task from each dataset as an example. Average over 5 folds, and including both the training set and test set.</figDesc><table><row><cell>Task</cell><cell>Dataset</cell><cell>WL CPU</cell><cell>Morgan FPs CPU</cell><cell>GCNN GPU</cell><cell>Weave GPU</cell><cell>GIN GPU</cell><cell>Vertex, Emb GPU</cell><cell>Graph, Emb GPU</cell></row><row><cell>Delaney</cell><cell>Delaney</cell><cell>2.46</cell><cell>0.25</cell><cell>39.70</cell><cell>65.82</cell><cell>-</cell><cell>49.63</cell><cell>2.90</cell></row><row><cell>Malaria</cell><cell>Malaria</cell><cell>128.81</cell><cell>5.28</cell><cell>377.24</cell><cell>536.99</cell><cell>-</cell><cell>1152.80</cell><cell>19.58</cell></row><row><cell>CEP</cell><cell>CEP</cell><cell>1113.35</cell><cell>17.69</cell><cell>607.23</cell><cell>849.37</cell><cell>-</cell><cell>2695.57</cell><cell>37.40</cell></row><row><cell>QM7</cell><cell>QM7</cell><cell>60.24</cell><cell>0.98</cell><cell>103.12</cell><cell>76.48</cell><cell>-</cell><cell>173.50</cell><cell>10.60</cell></row><row><cell>E1-CC2</cell><cell>QM8</cell><cell>584.98</cell><cell>3.60</cell><cell>382.72</cell><cell>262.16</cell><cell>-</cell><cell>966.49</cell><cell>33.43</cell></row><row><cell>mu</cell><cell>QM9</cell><cell>-</cell><cell>19.58</cell><cell>9051.37</cell><cell>1504.77</cell><cell>-</cell><cell>8279.03</cell><cell>169.72</cell></row><row><cell>NR-AR</cell><cell>Tox21</cell><cell>70.35</cell><cell>2.03</cell><cell>130.15</cell><cell>142.59</cell><cell>608.57</cell><cell>525.24</cell><cell>10.81</cell></row><row><cell>CT-TOX</cell><cell>Clintox</cell><cell>4.92</cell><cell>0.63</cell><cell>62.61</cell><cell>95.50</cell><cell>135.68</cell><cell>191.93</cell><cell>3.83</cell></row><row><cell>MUV-466</cell><cell>MUV</cell><cell>276.42</cell><cell>6.31</cell><cell>401.02</cell><cell>690.15</cell><cell>1327.26</cell><cell>1221.25</cell><cell>25.50</cell></row><row><cell>HIV</cell><cell>HIV</cell><cell>2284.74</cell><cell>17.16</cell><cell>1142.77</cell><cell>2138.10</cell><cell>3641.52</cell><cell>3975.76</cell><cell>139.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison of model using 3D information. On two regression datasets QM8 and QM9, and evaluated by MAE. N-Gram does not include any spatial information, like the distance between each atom pair, yet its performance is very comparative to the state-of-the-art methods.</figDesc><table><row><cell>Dataset</cell><cell># Task</cell><cell>WL SVM</cell><cell>Morgan RF</cell><cell>Morgan XGB</cell><cell>GCNN</cell><cell>Weave</cell><cell>DTNN</cell><cell>MPNN</cell><cell>N-Gram RF</cell><cell>N-Gram XGB</cell></row><row><cell>QM8</cell><cell>12</cell><cell></cell><cell>1, 4</cell><cell>0, 1</cell><cell>4, 10</cell><cell>0, 3</cell><cell>0, 5</cell><cell>5, 6</cell><cell>0, 2</cell><cell>2, 5</cell></row><row><cell>QM9</cell><cell>12</cell><cell>-</cell><cell></cell><cell></cell><cell>0, 4</cell><cell>0, 1</cell><cell>7, 10</cell><cell>1, 9</cell><cell>0, 5</cell><cell>4, 7</cell></row><row><cell>Overall</cell><cell>24</cell><cell></cell><cell>1, 4</cell><cell>0, 1</cell><cell>4, 14</cell><cell>0, 4</cell><cell>7, 15</cell><cell>6, 15</cell><cell>0, 7</cell><cell>6, 12</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table S1 :</head><label>S1</label><figDesc>Number of positives and all molecules on 12 Tox21 tasks.</figDesc><table><row><cell>Task</cell><cell cols="3">Num of Positives Total Number Positive Ratio (%)</cell></row><row><cell>NR-AR</cell><cell>304</cell><cell>7332</cell><cell>4.14621</cell></row><row><cell>NR-AR-LBD</cell><cell>237</cell><cell>6817</cell><cell>3.47660</cell></row><row><cell>NR-AhR</cell><cell>783</cell><cell>6592</cell><cell>11.87803</cell></row><row><cell>NR-Aromatase</cell><cell>298</cell><cell>5853</cell><cell>5.09141</cell></row><row><cell>NR-ER</cell><cell>784</cell><cell>6237</cell><cell>12.57015</cell></row><row><cell>NR-ER-LBD</cell><cell>347</cell><cell>7014</cell><cell>4.94725</cell></row><row><cell>NR-PPAR-gamma</cell><cell>186</cell><cell>6505</cell><cell>2.85934</cell></row><row><cell>SR-ARE</cell><cell>954</cell><cell>5907</cell><cell>16.15033</cell></row><row><cell>SR-ATAD5</cell><cell>262</cell><cell>7140</cell><cell>3.66947</cell></row><row><cell>SR-HSE</cell><cell>378</cell><cell>6562</cell><cell>5.76044</cell></row><row><cell>SR-MMP</cell><cell>912</cell><cell>5834</cell><cell>15.63250</cell></row><row><cell>SR-p53</cell><cell>414</cell><cell>6814</cell><cell>6.07573</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table S2</head><label>S2</label><figDesc></figDesc><table><row><cell cols="4">: Number of positives and all molecules on 2 ClinTox tasks</cell></row><row><cell>Task</cell><cell cols="3">Num of Positives Total Number Positive Ratio (%)</cell></row><row><cell>CT_TOX</cell><cell>112</cell><cell>1469</cell><cell>7.62423</cell></row><row><cell>FDA_APPROVED</cell><cell>1375</cell><cell>1469</cell><cell>93.60109</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table S3 :</head><label>S3</label><figDesc>Number of positives and all molecules on 17 MUV tasks.</figDesc><table><row><cell>Task</cell><cell cols="3">Num of Positives Total Number Positive Ratio (%)</cell></row><row><cell>MUV-466</cell><cell>27</cell><cell>14844</cell><cell>0.18189</cell></row><row><cell>MUV-548</cell><cell>29</cell><cell>14737</cell><cell>0.19678</cell></row><row><cell>MUV-600</cell><cell>30</cell><cell>14734</cell><cell>0.20361</cell></row><row><cell>MUV-644</cell><cell>30</cell><cell>14633</cell><cell>0.20502</cell></row><row><cell>MUV-652</cell><cell>29</cell><cell>14903</cell><cell>0.19459</cell></row><row><cell>MUV-689</cell><cell>29</cell><cell>14606</cell><cell>0.19855</cell></row><row><cell>MUV-692</cell><cell>30</cell><cell>14647</cell><cell>0.20482</cell></row><row><cell>MUV-712</cell><cell>28</cell><cell>14415</cell><cell>0.19424</cell></row><row><cell>MUV-713</cell><cell>29</cell><cell>14841</cell><cell>0.19540</cell></row><row><cell>MUV-733</cell><cell>28</cell><cell>14691</cell><cell>0.19059</cell></row><row><cell>MUV-737</cell><cell>29</cell><cell>14696</cell><cell>0.19733</cell></row><row><cell>MUV-810</cell><cell>29</cell><cell>14646</cell><cell>0.19801</cell></row><row><cell>MUV-832</cell><cell>30</cell><cell>14676</cell><cell>0.20442</cell></row><row><cell>MUV-846</cell><cell>30</cell><cell>14714</cell><cell>0.20389</cell></row><row><cell>MUV-852</cell><cell>29</cell><cell>14658</cell><cell>0.19784</cell></row><row><cell>MUV-858</cell><cell>29</cell><cell>14775</cell><cell>0.19628</cell></row><row><cell>MUV-859</cell><cell>24</cell><cell>14751</cell><cell>0.16270</cell></row><row><cell cols="4">Table S4: Number of positives and all molecules on 1 HIV task.</cell></row><row><cell>Task</cell><cell cols="3">Num of Positives Total Number Positive Ratio (%)</cell></row><row><cell>HIV</cell><cell>1425</cell><cell>41023</cell><cell>3.47366</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table S5 :</head><label>S5</label><figDesc>d = 42 features are divided into S = 8 attributes. Each feature attribute corresponds to one type of atom property, including atom symbol, atom degree, atom charge, etc. Note that it is hard to enumerate all the values for the atom properties, so we use the last bit 'Unknown' as the placeholder to catch the missing symbols.</figDesc><table><row><cell>id</cell><cell>digit</cell><cell>property</cell><cell>values</cell></row><row><cell>0</cell><cell>0-9</cell><cell>atom symbol</cell><cell>[C, Cl, I, F, O, N, P, S, Br, Unknown]</cell></row><row><cell>1</cell><cell>10-16</cell><cell>atom degree</cell><cell>[0, 1, 2, 3, 4, 5, Unknown]</cell></row><row><cell>2</cell><cell>17-23</cell><cell>number of Hydrogen</cell><cell>[0, 1, 2, 3, 4, 5, Unknown]</cell></row><row><cell>3</cell><cell>24-29</cell><cell>implicit valence</cell><cell>[0, 1, 2, 3, 4, Unknown]</cell></row><row><cell>4</cell><cell>30-35</cell><cell>atom charge</cell><cell>[-2, -1, 0, 1, 2, Unknown]</cell></row><row><cell>5</cell><cell>36-37</cell><cell>is aromatic</cell><cell>[no, yes]</cell></row><row><cell>6</cell><cell>38-39</cell><cell>is acceptor</cell><cell>[no, yes]</cell></row><row><cell>7</cell><cell>40-41</cell><cell>is donor</cell><cell>[no, yes]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table S6 :</head><label>S6</label><figDesc>On datasets QM8 and QM9, due to the input format of the molecule file, we cannot extract the atom attributes like the number of hydrogen, is-acceptor and is-donor property (while keeping its 3D information at the same time). So following<ref type="bibr" target="#b55">[56]</ref>, only d = 32 features, i.e., S = 5 attributes are considered.</figDesc><table><row><cell>id</cell><cell>digit</cell><cell>property</cell><cell>values</cell></row><row><cell>0</cell><cell>0-9</cell><cell>atom symbol</cell><cell>[C, Cl, I, F, O, N, P, S, Br, Unknown]</cell></row><row><cell>1</cell><cell>10-16</cell><cell>atom degree</cell><cell>[0, 1, 2, 3, 4, 5, Unknown]</cell></row><row><cell>2</cell><cell>17-23</cell><cell>implicit valence</cell><cell>[0, 1, 2, 3, 4, 5, Unknown]</cell></row><row><cell>3</cell><cell>24-29</cell><cell>atom charge</cell><cell>[-2, -1, 0, 1, 2, Unknown]</cell></row><row><cell>4</cell><cell>30-31</cell><cell>is aromatic</cell><cell>[no, yes]</cell></row><row><cell cols="3">F Hyperparameter Tuning</cell><cell></cell></row></table><note>F.1 Hyperparameters for Representation Morgan Fingerprints. To generate Morgan fingerprints, we use the public package RdKit</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table S7 :</head><label>S7</label><figDesc>Hyperparameter sweeping for N-Gram Graph. We have S = 8 feature attributes.</figDesc><table><row><cell>Hyperparameters</cell><cell>Candidate values</cell></row><row><cell>Random Dimension r</cell><cell>50, 100</cell></row><row><cell>N-Gram Num T</cell><cell>2, 4, 6</cell></row><row><cell>Embedding Structure</cell><cell>[Embedding -&gt; Sum], [Embedding -&gt; Mean]</cell></row><row><cell>Neural Network</cell><cell>[r, 20, S], [r, 100, S] , [r, 100, 20, S]</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table S8 :</head><label>S8</label><figDesc>Hyperparameter sweeping for Weisfeiler-Lehman Graph Kernel.</figDesc><table><row><cell>Hyperparameters</cell><cell>Candidate values</cell></row><row><cell>Number of Step</cell><cell>1, 2, 3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table S9 :</head><label>S9</label><figDesc>Hyperparameter sweeping for Random Forest.</figDesc><table><row><cell>Hyperparameters</cell><cell>Candidate values</cell></row><row><cell>Number of Trees</cell><cell>100, 4000</cell></row><row><cell>Max Features</cell><cell>None, sqrt, log2</cell></row><row><cell>Min Samples Leaf</cell><cell>1, 10, 100, 1000</cell></row><row><cell>Class Weight</cell><cell>None, balanced_subsample, balanced</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table S10 :</head><label>S10</label><figDesc>Hyperparameter sweeping for XGBoost.</figDesc><table><row><cell>Hyperparameters</cell><cell>Candidate values</cell></row><row><cell>Max Depth</cell><cell>5, 10, 50, 100</cell></row><row><cell>Learning Rate</cell><cell>1, 3e-1, 1e-1, 3e-2</cell></row><row><cell>Number of Trees</cell><cell>30, 100, 300, 1000, 3000</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table S11 :</head><label>S11</label><figDesc>Hyperparameter sweeping for Graph Isomorphism Network.</figDesc><table><row><cell>Hyperparameters</cell><cell>Candidate values</cell></row><row><cell>Max Depth</cell><cell>2, 3, 5</cell></row><row><cell>Hidden Dimension</cell><cell>30, 50</cell></row><row><cell>Epoch</cell><cell>100, 300</cell></row><row><cell>Optimizer</cell><cell>SGD, Adam</cell></row><row><cell>Learning Rate Scheduler</cell><cell>None, ReduceLROnPlateau, StepLR</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table S12 :</head><label>S12</label><figDesc>The mean accuracy of 5-fold cross-validation in vertex embedding. The accuracy measures how the CBoW-like neural network can accurately predict the vertex attributes.</figDesc><table><row><cell>Task/Dataset</cell><cell>Accuracy(%), r = 50</cell><cell>Accuracy(%), r = 100</cell></row><row><cell>Delaney</cell><cell>0.924 ? 0.001</cell><cell>0.924 ? 0.001</cell></row><row><cell>Malaria</cell><cell>0.937 ? 0.001</cell><cell>0.938 ? 0.000</cell></row><row><cell>CEP</cell><cell>0.923 ? 0.001</cell><cell>0.923 ? 0.000</cell></row><row><cell>QM7</cell><cell>0.898 ? 0.001</cell><cell>0.897 ? 0.001</cell></row><row><cell>QM8</cell><cell>0.988 ? 0.000</cell><cell>0.988 ? 0.000</cell></row><row><cell>QM9</cell><cell>0.987 ? 0.000</cell><cell>0.987 ? 0.001</cell></row><row><cell>NR-AR</cell><cell>0.916 ? 0.000</cell><cell>0.916 ? 0.000</cell></row><row><cell>NR-AR-LBD</cell><cell>0.915 ? 0.001</cell><cell>0.914 ? 0.003</cell></row><row><cell>NR-AhR</cell><cell>0.916 ? 0.000</cell><cell>0.915 ? 0.000</cell></row><row><cell>NR-Aromatase</cell><cell>0.915 ? 0.000</cell><cell>0.915 ? 0.000</cell></row><row><cell>NR-ER</cell><cell>0.915 ? 0.001</cell><cell>0.914 ? 0.001</cell></row><row><cell>NR-ER-LBD</cell><cell>0.916 ? 0.001</cell><cell>0.915 ? 0.001</cell></row><row><cell>NR-PPAR-gamma</cell><cell>0.914 ? 0.000</cell><cell>0.915 ? 0.000</cell></row><row><cell>SR-ARE</cell><cell>0.915 ? 0.000</cell><cell>0.915 ? 0.000</cell></row><row><cell>SR-ATAD5</cell><cell>0.916 ? 0.000</cell><cell>0.916 ? 0.000</cell></row><row><cell>SR-HSE</cell><cell>0.915 ? 0.000</cell><cell>0.915 ? 0.000</cell></row><row><cell>SR-MMP</cell><cell>0.913 ? 0.001</cell><cell>0.914 ? 0.001</cell></row><row><cell>SR-p53</cell><cell>0.915 ? 0.000</cell><cell>0.915 ? 0.001</cell></row><row><cell>Clintox</cell><cell>0.911 ? 0.001</cell><cell>0.911 ? 0.000</cell></row><row><cell>MUV-466</cell><cell>0.940 ? 0.001</cell><cell>0.940 ? 0.000</cell></row><row><cell>MUV-548</cell><cell>0.932 ? 0.001</cell><cell>0.931 ? 0.001</cell></row><row><cell>MUV-600</cell><cell>0.938 ? 0.000</cell><cell>0.937 ? 0.001</cell></row><row><cell>MUV-644</cell><cell>0.932 ? 0.001</cell><cell>0.932 ? 0.000</cell></row><row><cell>MUV-652</cell><cell>0.939 ? 0.000</cell><cell>0.939 ? 0.001</cell></row><row><cell>MUV-689</cell><cell>0.942 ? 0.001</cell><cell>0.942 ? 0.001</cell></row><row><cell>MUV-692</cell><cell>0.934 ? 0.001</cell><cell>0.934 ? 0.000</cell></row><row><cell>MUV-712</cell><cell>0.938 ? 0.001</cell><cell>0.938 ? 0.001</cell></row><row><cell>MUV-713</cell><cell>0.938 ? 0.000</cell><cell>0.937 ? 0.000</cell></row><row><cell>MUV-733</cell><cell>0.937 ? 0.001</cell><cell>0.937 ? 0.001</cell></row><row><cell>MUV-737</cell><cell>0.940 ? 0.001</cell><cell>0.939 ? 0.001</cell></row><row><cell>MUV-810</cell><cell>0.937 ? 0.000</cell><cell>0.937 ? 0.000</cell></row><row><cell>MUV-832</cell><cell>0.937 ? 0.001</cell><cell>0.936 ? 0.001</cell></row><row><cell>MUV-846</cell><cell>0.938 ? 0.000</cell><cell>0.938 ? 0.000</cell></row><row><cell>MUV-852</cell><cell>0.937 ? 0.001</cell><cell>0.937 ? 0.001</cell></row><row><cell>MUV-858</cell><cell>0.938 ? 0.001</cell><cell>0.938 ? 0.001</cell></row><row><cell>MUV-859</cell><cell>0.939 ? 0.001</cell><cell>0.939 ? 0.001</cell></row><row><cell>HIV</cell><cell>0.920 ? 0.001</cell><cell>0.919 ? 0.001</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_17"><head>Table S13 :</head><label>S13</label><figDesc>Here we include the performance on 28 regression tasks with 7 models. All experiments are done on a 5-fold cross-validation, and the mean evaluation of 5 runs is reported here. The top-3 models are bolded, and the best model is underlined.</figDesc><table><row><cell>Task</cell><cell>Eval Metric</cell><cell>WL SVM</cell><cell>Morgan RF</cell><cell>Morgan XGB</cell><cell>GCNN</cell><cell>Weave</cell><cell>N-Gram RF</cell><cell>N-Gram XGB</cell></row><row><cell>Delaney</cell><cell>RMSE</cell><cell>1.265</cell><cell>1.168</cell><cell>3.063</cell><cell>0.825</cell><cell>0.687</cell><cell>0.769</cell><cell>0.731</cell></row><row><cell>Malaria</cell><cell>RMSE</cell><cell>1.094</cell><cell>0.983</cell><cell>1.943</cell><cell>1.144</cell><cell>1.487</cell><cell>1.022</cell><cell>1.019</cell></row><row><cell>CEP</cell><cell>RMSE</cell><cell>1.800</cell><cell>1.300</cell><cell>3.049</cell><cell>1.493</cell><cell>2.846</cell><cell>1.399</cell><cell>1.366</cell></row><row><cell>QM7</cell><cell>MAE</cell><cell>176.750</cell><cell>127.662</cell><cell>110.230</cell><cell>76.637</cell><cell>62.560</cell><cell>57.747</cell><cell>53.919</cell></row><row><cell>E1-CC2</cell><cell>MAE</cell><cell>0.032</cell><cell>0.008</cell><cell>0.008</cell><cell>0.006</cell><cell>0.007</cell><cell>0.008</cell><cell>0.007</cell></row><row><cell>E2-CC2</cell><cell>MAE</cell><cell>0.023</cell><cell>0.010</cell><cell>0.010</cell><cell>0.008</cell><cell>0.007</cell><cell>0.009</cell><cell>0.008</cell></row><row><cell>f1-CC2</cell><cell>MAE</cell><cell>0.072</cell><cell>0.014</cell><cell>0.015</cell><cell>0.014</cell><cell>0.018</cell><cell>0.015</cell><cell>0.015</cell></row><row><cell>f2-CC2</cell><cell>MAE</cell><cell>0.081</cell><cell>0.032</cell><cell>0.033</cell><cell>0.031</cell><cell>0.036</cell><cell>0.033</cell><cell>0.031</cell></row><row><cell>E1-PBE0</cell><cell>MAE</cell><cell>0.034</cell><cell>0.008</cell><cell>0.008</cell><cell>0.006</cell><cell>0.006</cell><cell>0.008</cell><cell>0.007</cell></row><row><cell>E2-PBE0</cell><cell>MAE</cell><cell>0.029</cell><cell>0.010</cell><cell>0.010</cell><cell>0.007</cell><cell>0.008</cell><cell>0.008</cell><cell>0.008</cell></row><row><cell>f1-PBE0</cell><cell>MAE</cell><cell>0.068</cell><cell>0.012</cell><cell>0.013</cell><cell>0.012</cell><cell>0.014</cell><cell>0.013</cell><cell>0.013</cell></row><row><cell>f2-PBE0</cell><cell>MAE</cell><cell>0.078</cell><cell>0.026</cell><cell>0.027</cell><cell>0.024</cell><cell>0.027</cell><cell>0.025</cell><cell>0.024</cell></row><row><cell>E1-CAM</cell><cell>MAE</cell><cell>0.033</cell><cell>0.007</cell><cell>0.007</cell><cell>0.006</cell><cell>0.006</cell><cell>0.007</cell><cell>0.007</cell></row><row><cell>E2-CAM</cell><cell>MAE</cell><cell>0.025</cell><cell>0.009</cell><cell>0.009</cell><cell>0.006</cell><cell>0.006</cell><cell>0.008</cell><cell>0.007</cell></row><row><cell>f1-CAM</cell><cell>MAE</cell><cell>0.073</cell><cell>0.013</cell><cell>0.014</cell><cell>0.013</cell><cell>0.016</cell><cell>0.014</cell><cell>0.014</cell></row><row><cell>f2-CAM</cell><cell>MAE</cell><cell>0.080</cell><cell>0.028</cell><cell>0.028</cell><cell>0.026</cell><cell>0.031</cell><cell>0.028</cell><cell>0.026</cell></row><row><cell>average</cell><cell></cell><cell>0.052</cell><cell>0.015</cell><cell>0.015</cell><cell>0.013</cell><cell>0.015</cell><cell>0.015</cell><cell>0.014</cell></row><row><cell>mu</cell><cell>MAE</cell><cell>-</cell><cell>0.548</cell><cell>0.533</cell><cell>0.482</cell><cell>0.624</cell><cell>0.562</cell><cell>0.535</cell></row><row><cell>alpha</cell><cell>MAE</cell><cell>-</cell><cell>3.787</cell><cell>2.672</cell><cell>0.685</cell><cell>1.034</cell><cell>0.722</cell><cell>0.612</cell></row><row><cell>homo</cell><cell>MAE</cell><cell>-</cell><cell>0.006</cell><cell>0.006</cell><cell>0.004</cell><cell>0.005</cell><cell>0.005</cell><cell>0.005</cell></row><row><cell>lumo</cell><cell>MAE</cell><cell>-</cell><cell>0.007</cell><cell>0.006</cell><cell>0.004</cell><cell>0.005</cell><cell>0.006</cell><cell>0.005</cell></row><row><cell>gap</cell><cell>MAE</cell><cell>-</cell><cell>0.008</cell><cell>0.008</cell><cell>0.006</cell><cell>0.008</cell><cell>0.007</cell><cell>0.007</cell></row><row><cell>r2</cell><cell>MAE</cell><cell>-</cell><cell>94.815</cell><cell>82.516</cell><cell>64.775</cell><cell>42.095</cell><cell>72.846</cell><cell>59.137</cell></row><row><cell>zpve</cell><cell>MAE</cell><cell>-</cell><cell>0.009</cell><cell>0.007</cell><cell>0.001</cell><cell>0.002</cell><cell>0.001</cell><cell>0.000</cell></row><row><cell>cv</cell><cell>MAE</cell><cell>-</cell><cell>1.505</cell><cell>1.166</cell><cell>0.524</cell><cell>0.374</cell><cell>0.434</cell><cell>0.334</cell></row><row><cell>u0</cell><cell>MAE</cell><cell>-</cell><cell>16.410</cell><cell>12.736</cell><cell>2.460</cell><cell>1.465</cell><cell>0.429</cell><cell>0.427</cell></row><row><cell>u298</cell><cell>MAE</cell><cell>-</cell><cell>16.410</cell><cell>12.757</cell><cell>2.671</cell><cell>1.560</cell><cell>0.429</cell><cell>0.428</cell></row><row><cell>h298</cell><cell>MAE</cell><cell>-</cell><cell>16.411</cell><cell>12.752</cell><cell>2.542</cell><cell>1.414</cell><cell>0.428</cell><cell>0.428</cell></row><row><cell>g298</cell><cell>MAE</cell><cell>-</cell><cell>16.414</cell><cell>12.750</cell><cell>2.466</cell><cell>2.359</cell><cell>0.428</cell><cell>0.428</cell></row><row><cell>average</cell><cell></cell><cell>-</cell><cell>13.823</cell><cell>11.476</cell><cell>6.474</cell><cell>4.187</cell><cell>6.357</cell><cell>5.152</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head>Table S14 :</head><label>S14</label><figDesc>Here we include the performance on 32 classification tasks with 8 models. All experiments are done on a 5-fold cross-validation, and the mean evaluation of 5 runs is reported here. The top-3 models are bolded, and the best model is underlined.</figDesc><table><row><cell>Task</cell><cell>Eval Metric</cell><cell>WL SVM</cell><cell>Morgan RF</cell><cell>Morgan XGB</cell><cell>GCNN</cell><cell>Weave</cell><cell>GIN</cell><cell>N-Gram RF</cell><cell>N-Gram XGB</cell></row><row><cell>NR-AR</cell><cell>ROC-AUC</cell><cell>0.759</cell><cell>0.781</cell><cell>0.780</cell><cell>0.793</cell><cell>0.789</cell><cell>0.755</cell><cell>0.797</cell><cell>0.791</cell></row><row><cell>NR-AR-LBD</cell><cell>ROC-AUC</cell><cell>0.843</cell><cell>0.868</cell><cell>0.853</cell><cell>0.851</cell><cell>0.835</cell><cell>0.826</cell><cell>0.871</cell><cell>0.864</cell></row><row><cell>NR-AhR</cell><cell>ROC-AUC</cell><cell>0.879</cell><cell>0.900</cell><cell>0.894</cell><cell>0.894</cell><cell>0.870</cell><cell>0.880</cell><cell>0.894</cell><cell>0.902</cell></row><row><cell>NR-Aromatase</cell><cell>ROC-AUC</cell><cell>0.849</cell><cell>0.828</cell><cell>0.780</cell><cell>0.839</cell><cell>0.819</cell><cell>0.818</cell><cell>0.858</cell><cell>0.869</cell></row><row><cell>NR-ER</cell><cell>ROC-AUC</cell><cell>0.716</cell><cell>0.731</cell><cell>0.722</cell><cell>0.731</cell><cell>0.712</cell><cell>0.688</cell><cell>0.747</cell><cell>0.753</cell></row><row><cell>NR-ER-LBD</cell><cell>ROC-AUC</cell><cell>0.794</cell><cell>0.806</cell><cell>0.795</cell><cell>0.806</cell><cell>0.808</cell><cell>0.778</cell><cell>0.827</cell><cell>0.838</cell></row><row><cell>NR-PPAR-gamma</cell><cell>ROC-AUC</cell><cell>0.819</cell><cell>0.844</cell><cell>0.805</cell><cell>0.817</cell><cell>0.794</cell><cell>0.800</cell><cell>0.856</cell><cell>0.851</cell></row><row><cell>SR-ARE</cell><cell>ROC-AUC</cell><cell>0.803</cell><cell>0.814</cell><cell>0.800</cell><cell>0.799</cell><cell>0.771</cell><cell>0.788</cell><cell>0.826</cell><cell>0.835</cell></row><row><cell>SR-ATAD5</cell><cell>ROC-AUC</cell><cell>0.819</cell><cell>0.854</cell><cell>0.829</cell><cell>0.825</cell><cell>0.778</cell><cell>0.814</cell><cell>0.857</cell><cell>0.860</cell></row><row><cell>SR-HSE</cell><cell>ROC-AUC</cell><cell>0.798</cell><cell>0.782</cell><cell>0.770</cell><cell>0.772</cell><cell>0.751</cell><cell>0.723</cell><cell>0.798</cell><cell>0.812</cell></row><row><cell>SR-MMP</cell><cell>ROC-AUC</cell><cell>0.887</cell><cell>0.886</cell><cell>0.878</cell><cell>0.894</cell><cell>0.887</cell><cell>0.866</cell><cell>0.911</cell><cell>0.918</cell></row><row><cell>SR-p53</cell><cell>ROC-AUC</cell><cell>0.835</cell><cell>0.859</cell><cell>0.796</cell><cell>0.834</cell><cell>0.795</cell><cell>0.819</cell><cell>0.859</cell><cell>0.868</cell></row><row><cell>average</cell><cell></cell><cell>0.813</cell><cell>0.827</cell><cell>0.806</cell><cell>0.821</cell><cell>0.798</cell><cell>0.791</cell><cell>0.841</cell><cell>0.842</cell></row><row><cell>CT_TOX</cell><cell>ROC-AUC</cell><cell>0.837</cell><cell>0.788</cell><cell>0.840</cell><cell>0.872</cell><cell>0.859</cell><cell>0.823</cell><cell>0.857</cell><cell>0.873</cell></row><row><cell>FDA_APPROVED</cell><cell>ROC-AUC</cell><cell>0.851</cell><cell>0.784</cell><cell>0.830</cell><cell>0.875</cell><cell>0.836</cell><cell>0.848</cell><cell>0.825</cell><cell>0.874</cell></row><row><cell>average</cell><cell></cell><cell>0.833</cell><cell>0.787</cell><cell>0.847</cell><cell>0.868</cell><cell>0.834</cell><cell>0.837</cell><cell>0.837</cell><cell>0.870</cell></row><row><cell>MUV-466</cell><cell>PR-AUC</cell><cell>0.046</cell><cell>0.076</cell><cell>0.058</cell><cell>0.003</cell><cell>0.017</cell><cell>0.060</cell><cell>0.058</cell><cell>0.086</cell></row><row><cell>MUV-548</cell><cell>PR-AUC</cell><cell>0.178</cell><cell>0.230</cell><cell>0.259</cell><cell>0.065</cell><cell>0.065</cell><cell>0.070</cell><cell>0.073</cell><cell>0.094</cell></row><row><cell>MUV-600</cell><cell>PR-AUC</cell><cell>0.023</cell><cell>0.021</cell><cell>0.017</cell><cell>0.004</cell><cell>0.006</cell><cell>0.013</cell><cell>0.007</cell><cell>0.009</cell></row><row><cell>MUV-644</cell><cell>PR-AUC</cell><cell>0.149</cell><cell>0.185</cell><cell>0.225</cell><cell>0.034</cell><cell>0.025</cell><cell>0.124</cell><cell>0.046</cell><cell>0.064</cell></row><row><cell>MUV-652</cell><cell>PR-AUC</cell><cell>0.164</cell><cell>0.095</cell><cell>0.039</cell><cell>0.020</cell><cell>0.021</cell><cell>0.022</cell><cell>0.085</cell><cell>0.118</cell></row><row><cell>MUV-689</cell><cell>PR-AUC</cell><cell>0.030</cell><cell>0.025</cell><cell>0.094</cell><cell>0.011</cell><cell>0.013</cell><cell>0.021</cell><cell>0.026</cell><cell>0.046</cell></row><row><cell>MUV-692</cell><cell>PR-AUC</cell><cell>0.003</cell><cell>0.010</cell><cell>0.003</cell><cell>0.004</cell><cell>0.003</cell><cell>0.006</cell><cell>0.005</cell><cell>0.005</cell></row><row><cell>MUV-712</cell><cell>PR-AUC</cell><cell>0.208</cell><cell>0.119</cell><cell>0.158</cell><cell>0.062</cell><cell>0.075</cell><cell>0.192</cell><cell>0.134</cell><cell>0.151</cell></row><row><cell>MUV-713</cell><cell>PR-AUC</cell><cell>0.036</cell><cell>0.057</cell><cell>0.024</cell><cell>0.007</cell><cell>0.011</cell><cell>0.007</cell><cell>0.026</cell><cell>0.026</cell></row><row><cell>MUV-733</cell><cell>PR-AUC</cell><cell>0.076</cell><cell>0.080</cell><cell>0.046</cell><cell>0.011</cell><cell>0.005</cell><cell>0.016</cell><cell>0.021</cell><cell>0.047</cell></row><row><cell>MUV-737</cell><cell>PR-AUC</cell><cell>0.058</cell><cell>0.056</cell><cell>0.060</cell><cell>0.008</cell><cell>0.017</cell><cell>0.005</cell><cell>0.084</cell><cell>0.080</cell></row><row><cell>MUV-810</cell><cell>PR-AUC</cell><cell>0.139</cell><cell>0.186</cell><cell>0.215</cell><cell>0.010</cell><cell>0.006</cell><cell>0.033</cell><cell>0.013</cell><cell>0.022</cell></row><row><cell>MUV-832</cell><cell>PR-AUC</cell><cell>0.365</cell><cell>0.556</cell><cell>0.508</cell><cell>0.029</cell><cell>0.032</cell><cell>0.388</cell><cell>0.229</cell><cell>0.280</cell></row><row><cell>MUV-846</cell><cell>PR-AUC</cell><cell>0.369</cell><cell>0.299</cell><cell>0.407</cell><cell>0.219</cell><cell>0.250</cell><cell>0.397</cell><cell>0.250</cell><cell>0.220</cell></row><row><cell>MUV-852</cell><cell>PR-AUC</cell><cell>0.405</cell><cell>0.173</cell><cell>0.300</cell><cell>0.159</cell><cell>0.131</cell><cell>0.337</cell><cell>0.214</cell><cell>0.238</cell></row><row><cell>MUV-858</cell><cell>PR-AUC</cell><cell>0.079</cell><cell>0.090</cell><cell>0.018</cell><cell>0.006</cell><cell>0.003</cell><cell>0.051</cell><cell>0.014</cell><cell>0.015</cell></row><row><cell>MUV-859</cell><cell>PR-AUC</cell><cell>0.004</cell><cell>0.004</cell><cell>0.007</cell><cell>0.005</cell><cell>0.004</cell><cell>0.003</cell><cell>0.007</cell><cell>0.006</cell></row><row><cell>average</cell><cell></cell><cell>0.154</cell><cell>0.146</cell><cell>0.158</cell><cell>0.041</cell><cell>0.047</cell><cell>0.119</cell><cell>0.088</cell><cell>0.099</cell></row><row><cell>HIV</cell><cell>ROC-AUC</cell><cell>0.800</cell><cell>0.849</cell><cell>0.827</cell><cell>0.805</cell><cell>0.663</cell><cell>0.785</cell><cell>0.828</cell><cell>0.830</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head>Table S15 :</head><label>S15</label><figDesc>Comparison of N-Gram Path-based graph and N-Gram Walk-based graph on 28 regression tasks. All experiments are done on a 5-fold cross-validation, and the mean evaluation of 5 runs is reported here. N-Gram Walk-based graph with XGB trained on top of it can excel on 27 out of the 28 regression tasks, while N-Gram Path achieves comparable performance.</figDesc><table><row><cell>Task</cell><cell>Eval Metric</cell><cell>N-Gram, RF Path</cell><cell>N-Gram, XGB Path</cell><cell>N-Gram, RF Walk</cell><cell>N-Gram, XGB Walk</cell></row><row><cell>Delaney</cell><cell>RMSE</cell><cell>0.866</cell><cell>0.746</cell><cell>0.769</cell><cell>0.731</cell></row><row><cell>Malaria</cell><cell>RMSE</cell><cell>1.036</cell><cell>1.027</cell><cell>1.022</cell><cell>1.019</cell></row><row><cell>CEP</cell><cell>RMSE</cell><cell>1.506</cell><cell>1.350</cell><cell>1.399</cell><cell>1.366</cell></row><row><cell>QM7</cell><cell>MAE</cell><cell>73.745</cell><cell>57.361</cell><cell>57.747</cell><cell>53.919</cell></row><row><cell>E1-CC2</cell><cell>MAE</cell><cell>0.011</cell><cell>0.009</cell><cell>0.008</cell><cell>0.007</cell></row><row><cell>E2-CC2</cell><cell>MAE</cell><cell>0.011</cell><cell>0.009</cell><cell>0.009</cell><cell>0.008</cell></row><row><cell>f1-CC2</cell><cell>MAE</cell><cell>0.017</cell><cell>0.016</cell><cell>0.015</cell><cell>0.015</cell></row><row><cell>f2-CC2</cell><cell>MAE</cell><cell>0.036</cell><cell>0.034</cell><cell>0.033</cell><cell>0.031</cell></row><row><cell>E1-PBE0</cell><cell>MAE</cell><cell>0.011</cell><cell>0.009</cell><cell>0.008</cell><cell>0.007</cell></row><row><cell>E2-PBE0</cell><cell>MAE</cell><cell>0.010</cell><cell>0.009</cell><cell>0.008</cell><cell>0.008</cell></row><row><cell>f1-PBE0</cell><cell>MAE</cell><cell>0.015</cell><cell>0.014</cell><cell>0.013</cell><cell>0.013</cell></row><row><cell>f2-PBE0</cell><cell>MAE</cell><cell>0.028</cell><cell>0.027</cell><cell>0.025</cell><cell>0.024</cell></row><row><cell>E1-CAM</cell><cell>MAE</cell><cell>0.010</cell><cell>0.008</cell><cell>0.007</cell><cell>0.007</cell></row><row><cell>E2-CAM</cell><cell>MAE</cell><cell>0.010</cell><cell>0.008</cell><cell>0.008</cell><cell>0.007</cell></row><row><cell>f1-CAM</cell><cell>MAE</cell><cell>0.017</cell><cell>0.015</cell><cell>0.014</cell><cell>0.014</cell></row><row><cell>f2-CAM</cell><cell>MAE</cell><cell>0.031</cell><cell>0.030</cell><cell>0.028</cell><cell>0.026</cell></row><row><cell>average</cell><cell></cell><cell>0.017</cell><cell>0.016</cell><cell>0.015</cell><cell>0.014</cell></row><row><cell>mu</cell><cell>MAE</cell><cell>0.629</cell><cell>0.588</cell><cell>0.562</cell><cell>0.535</cell></row><row><cell>alpha</cell><cell>MAE</cell><cell>0.868</cell><cell>0.759</cell><cell>0.722</cell><cell>0.612</cell></row><row><cell>homo</cell><cell>MAE</cell><cell>0.006</cell><cell>0.006</cell><cell>0.005</cell><cell>0.005</cell></row><row><cell>lumo</cell><cell>MAE</cell><cell>0.007</cell><cell>0.006</cell><cell>0.006</cell><cell>0.005</cell></row><row><cell>gap</cell><cell>MAE</cell><cell>0.009</cell><cell>0.008</cell><cell>0.007</cell><cell>0.007</cell></row><row><cell>r2</cell><cell>MAE</cell><cell>88.431</cell><cell>67.876</cell><cell>72.846</cell><cell>59.137</cell></row><row><cell>zpve</cell><cell>MAE</cell><cell>0.001</cell><cell>0.001</cell><cell>0.001</cell><cell>0.000</cell></row><row><cell>cv</cell><cell>MAE</cell><cell>0.613</cell><cell>0.498</cell><cell>0.434</cell><cell>0.334</cell></row><row><cell>u0</cell><cell>MAE</cell><cell>1.382</cell><cell>0.592</cell><cell>0.429</cell><cell>0.427</cell></row><row><cell>u298</cell><cell>MAE</cell><cell>1.384</cell><cell>0.594</cell><cell>0.429</cell><cell>0.428</cell></row><row><cell>h298</cell><cell>MAE</cell><cell>1.380</cell><cell>0.591</cell><cell>0.428</cell><cell>0.428</cell></row><row><cell>g298</cell><cell>MAE</cell><cell>1.382</cell><cell>0.593</cell><cell>0.428</cell><cell>0.428</cell></row><row><cell>average</cell><cell></cell><cell>8.035</cell><cell>6.001</cell><cell>6.357</cell><cell>5.152</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head>Table S16 :</head><label>S16</label><figDesc>Comparison of N-Gram Path-based graph and N-Gram Walk-based graph on 32 classification tasks. All experiments are done on a 5-fold cross-validation, and the mean evaluation of 5 runs is reported here. N-Gram Walk-based graph with RF and XGB trained on top of it can excel on 5 and 15 tasks out of 32 respectively. Besides, the average performance trained with RF and XGB is better when using N-Gram walk-based graph, except for XGB on HIV. Overall, N-Gram Walk is better while N-Gram Path achieves comparable performance.</figDesc><table><row><cell>Task</cell><cell>Eval Metric</cell><cell>N-Gram, RF Path</cell><cell>N-Gram, XGB Path</cell><cell>N-Gram, RF Walk</cell><cell>N-Gram, XGB Walk</cell></row><row><cell>NR-AR</cell><cell>ROC-AUC</cell><cell>0.797</cell><cell>0.788</cell><cell>0.797</cell><cell>0.791</cell></row><row><cell>NR-AR-LBD</cell><cell>ROC-AUC</cell><cell>0.860</cell><cell>0.857</cell><cell>0.871</cell><cell>0.864</cell></row><row><cell>NR-AhR</cell><cell>ROC-AUC</cell><cell>0.890</cell><cell>0.896</cell><cell>0.894</cell><cell>0.902</cell></row><row><cell>NR-Aromatase</cell><cell>ROC-AUC</cell><cell>0.856</cell><cell>0.863</cell><cell>0.858</cell><cell>0.869</cell></row><row><cell>NR-ER</cell><cell>ROC-AUC</cell><cell>0.742</cell><cell>0.750</cell><cell>0.747</cell><cell>0.753</cell></row><row><cell>NR-ER-LBD</cell><cell>ROC-AUC</cell><cell>0.823</cell><cell>0.840</cell><cell>0.827</cell><cell>0.838</cell></row><row><cell>NR-PPAR-gamma</cell><cell>ROC-AUC</cell><cell>0.837</cell><cell>0.832</cell><cell>0.856</cell><cell>0.851</cell></row><row><cell>SR-ARE</cell><cell>ROC-AUC</cell><cell>0.824</cell><cell>0.834</cell><cell>0.826</cell><cell>0.835</cell></row><row><cell>SR-ATAD5</cell><cell>ROC-AUC</cell><cell>0.858</cell><cell>0.848</cell><cell>0.857</cell><cell>0.860</cell></row><row><cell>SR-HSE</cell><cell>ROC-AUC</cell><cell>0.790</cell><cell>0.795</cell><cell>0.798</cell><cell>0.812</cell></row><row><cell>SR-MMP</cell><cell>ROC-AUC</cell><cell>0.904</cell><cell>0.912</cell><cell>0.911</cell><cell>0.918</cell></row><row><cell>SR-p53</cell><cell>ROC-AUC</cell><cell>0.847</cell><cell>0.850</cell><cell>0.859</cell><cell>0.868</cell></row><row><cell>average</cell><cell></cell><cell>0.833</cell><cell>0.834</cell><cell>0.841</cell><cell>0.842</cell></row><row><cell>CT_TOX</cell><cell>ROC-AUC</cell><cell>0.838</cell><cell>0.858</cell><cell>0.857</cell><cell>0.873</cell></row><row><cell>FDA_APPROVED</cell><cell>ROC-AUC</cell><cell>0.816</cell><cell>0.854</cell><cell>0.825</cell><cell>0.874</cell></row><row><cell>average</cell><cell></cell><cell>0.810</cell><cell>0.855</cell><cell>0.837</cell><cell>0.870</cell></row><row><cell>MUV-466</cell><cell>PR-AUC</cell><cell>0.056</cell><cell>0.077</cell><cell>0.058</cell><cell>0.086</cell></row><row><cell>MUV-548</cell><cell>PR-AUC</cell><cell>0.088</cell><cell>0.100</cell><cell>0.073</cell><cell>0.094</cell></row><row><cell>MUV-600</cell><cell>PR-AUC</cell><cell>0.008</cell><cell>0.014</cell><cell>0.007</cell><cell>0.009</cell></row><row><cell>MUV-644</cell><cell>PR-AUC</cell><cell>0.061</cell><cell>0.093</cell><cell>0.046</cell><cell>0.064</cell></row><row><cell>MUV-652</cell><cell>PR-AUC</cell><cell>0.096</cell><cell>0.151</cell><cell>0.085</cell><cell>0.118</cell></row><row><cell>MUV-689</cell><cell>PR-AUC</cell><cell>0.027</cell><cell>0.025</cell><cell>0.026</cell><cell>0.046</cell></row><row><cell>MUV-692</cell><cell>PR-AUC</cell><cell>0.004</cell><cell>0.003</cell><cell>0.005</cell><cell>0.005</cell></row><row><cell>MUV-712</cell><cell>PR-AUC</cell><cell>0.088</cell><cell>0.085</cell><cell>0.134</cell><cell>0.151</cell></row><row><cell>MUV-713</cell><cell>PR-AUC</cell><cell>0.052</cell><cell>0.015</cell><cell>0.026</cell><cell>0.026</cell></row><row><cell>MUV-733</cell><cell>PR-AUC</cell><cell>0.017</cell><cell>0.015</cell><cell>0.021</cell><cell>0.047</cell></row><row><cell>MUV-737</cell><cell>PR-AUC</cell><cell>0.038</cell><cell>0.035</cell><cell>0.084</cell><cell>0.080</cell></row><row><cell>MUV-810</cell><cell>PR-AUC</cell><cell>0.017</cell><cell>0.054</cell><cell>0.013</cell><cell>0.022</cell></row><row><cell>MUV-832</cell><cell>PR-AUC</cell><cell>0.176</cell><cell>0.297</cell><cell>0.229</cell><cell>0.280</cell></row><row><cell>MUV-846</cell><cell>PR-AUC</cell><cell>0.245</cell><cell>0.223</cell><cell>0.250</cell><cell>0.220</cell></row><row><cell>MUV-852</cell><cell>PR-AUC</cell><cell>0.188</cell><cell>0.189</cell><cell>0.214</cell><cell>0.238</cell></row><row><cell>MUV-858</cell><cell>PR-AUC</cell><cell>0.007</cell><cell>0.016</cell><cell>0.014</cell><cell>0.015</cell></row><row><cell>MUV-859</cell><cell>PR-AUC</cell><cell>0.012</cell><cell>0.005</cell><cell>0.007</cell><cell>0.006</cell></row><row><cell>average</cell><cell></cell><cell>0.079</cell><cell>0.087</cell><cell>0.088</cell><cell>0.099</cell></row><row><cell>HIV</cell><cell>ROC-AUC</cell><cell>0.826</cell><cell>0.833</cell><cell>0.828</cell><cell>0.830</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head>Table S18 :</head><label>S18</label><figDesc>Representation construction time in seconds. One task from each dataset as an example. Average over 5 folds, and including both the training set and test set. Exploring the Effects of r and T K.1 On 12 ClassificationTasks (Tox12)</figDesc><table><row><cell>K</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Task</cell><cell>Dataset</cell><cell>WL CPU</cell><cell>Morgan FPs CPU</cell><cell>GCNN GPU</cell><cell>Weave GPU</cell><cell>DTNN GPU</cell><cell>MPNN GPU</cell><cell>GIN GPU</cell><cell>Vertex Emb GPU</cell><cell>Graph Emb GPU</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head>Table S19 :</head><label>S19</label><figDesc>For each message-passing graph method, we compare the performance on 12 Tox21 tasks. The null hypothesis here is that means are the same, so rejection=False means we should accept the null hypothesis. Thus, this table shows that two attribute schemes contain very similar information with respect to the performance.</figDesc><table><row><cell>Group 1</cell><cell>Group 2</cell><cell cols="2">mean diff reject</cell></row><row><cell cols="2">GCNN new attribute scheme GCNN original attribute scheme</cell><cell>-0.0012</cell><cell>False</cell></row><row><cell cols="2">Weave new attribute scheme Weave original attribute scheme</cell><cell>0.0008</cell><cell>False</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">If there are numeric attributes, they can be simply padded to the learned embedding for the other attributes.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">We don't present the version of our method excluding such walks due to its higher computational cost.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The code is available at https://github.com/chao1224/n_gram_graph. Baseline implementation follows<ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b43">44]</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Weave<ref type="bibr" target="#b32">[33]</ref> is also using the distance matrix, but it is the distance on graph, i.e.the length of shortest path between each atom pair, not the 3D Euclidean distance.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">There can be other formats of raw data (such as 2D projections of the molecules), or missing data entries (such as missing attribute information for an atom). These are not considered here for simplicity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">In fact, the entries can be c times any distribution that has mean 0, variance 1, and is almost surely bounded by a constant.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">For regression tasks like QM8, QM9, and Clintox, all the molecules are sharing the same splits since they don't have any restrictions like missing labels or stratified splits.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by FA9550-18-1-0166. The authors would also like to acknowledge computing resources from the University of Wisconsin-Madison Center for High Throughput Computing and support provided by the University of Wisconsin-Madison Office of the Vice Chancellor for Research and Graduate Education with funding from the Wisconsin Alumni Research Foundation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Aids antiviral screen data</title>
		<ptr target="https://wiki.nci.nih.gov/display/NCIDTPdata/AIDS+Antiviral+Screen+Data" />
		<imprint>
			<biblScope unit="page" from="2017" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Low data drug discovery with one-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Altae-Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Central Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="283" to="293" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A compressed sensing view of unsupervised text embeddings, bag-of-n-grams, and lstm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikunj</forename><surname>Saunshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kiran</forename><surname>Vodrahalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A latent variable model approach to pmi-based word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="385" to="399" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Linear algebraic structure of word senses, with applications to polysemy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association of Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="483" to="495" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple but tough-to-beat baseline for sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Integrated deep learned transcriptomic and structure-based predictor of clinical trials outcomes. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeny</forename><surname>Artem V Artemov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Putin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Vanhaelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aliper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Ozerov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhavoronkov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">95653</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">970 million druglike small molecules for virtual screening in the chemical universe database gdb-13</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Louis</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reymond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Chemical Society</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page" from="8732" to="8733" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Machine learning for molecular and materials science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugh</forename><surname>Davies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olexandr</forename><surname>Cartwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aron</forename><surname>Isayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walsh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">559</biblScope>
			<biblScope unit="issue">7715</biblScope>
			<biblScope unit="page">547</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Compressed learning: Universal sparse dimensionality reduction and learning in the measurement domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Calderbank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sina</forename><surname>Jafarpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Schapire</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
	<note type="report_type">Techical Report</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Next-generation machine learning for biological networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Diogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><forename type="middle">M</forename><surname>Camacho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rani</forename><forename type="middle">K</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James J</forename><surname>Costello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The restricted isometry property and its implications for compressed sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Candes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comptes rendus mathematique</title>
		<imprint>
			<biblScope unit="volume">346</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="589" to="592" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Decoding by linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terence</forename><surname>Candes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on information theory</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4203" to="4215" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">The rise of deep learning in drug discovery. Drug discovery today</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ola</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Olivecrona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Blaschke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Opportunities and obstacles for deep learning in biology and medicine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travers</forename><surname>Ching</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Himmelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Brett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandr</forename><forename type="middle">A</forename><surname>Beaulieu-Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><forename type="middle">T</forename><surname>Kalinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Way</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul-Michael</forename><surname>Ferrero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Agapow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of The Royal Society Interface</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">141</biblScope>
			<biblScope unit="page">20170387</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep learning how i did it: Merck 1st place interview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<ptr target="http://blog.kaggle.com/2012/11/01/deep-learning-how-i-did-it-merck-1st-place-interview" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ESOL: Estimating Aqueous Solubility Directly from Molecular Structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Delaney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Computer Sciences</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1000" to="1005" />
			<date type="published" when="2004-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Convolutional Networks on Graphs for Learning Molecular Fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Prediction errors of molecular machine learning models lower than hybrid dft error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Faber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Hutchison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O Anatole Von</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical theory and computation</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5255" to="5264" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast graph representation learning with PyTorch Geometric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop on Representation Learning on Graphs and Manifolds</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A mathematical introduction to compressive sensing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Foucart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Holger</forename><surname>Rauhut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bull. Am. Math</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="151" to="165" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Thousands of chemical starting points for antimalarial lead identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco-Javier</forename><surname>Gamo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laura</forename><forename type="middle">M</forename><surname>Sanz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaume</forename><surname>Vidal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristina</forename><surname>De Cozar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose-Luis</forename><surname>Lavandera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dana</forename><forename type="middle">E</forename><surname>Vanderwall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darren</forename><forename type="middle">V S</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samiul</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><forename type="middle">E</forename><surname>Peishoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lon</forename><forename type="middle">R</forename><surname>Cardon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">F</forename><surname>Garcia-Bustos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">465</biblScope>
			<biblScope unit="issue">7296</biblScope>
			<biblScope unit="page" from="305" to="310" />
			<date type="published" when="2010-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A data-driven approach to predicting successes and failures of clinical trials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kaitlyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gayvert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Neel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Madhukar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Elemento</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cell chemical biology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1294" to="1301" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Doina Precup and Yee Whye Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
	<note>International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Automatic chemical design using a data-driven continuous representation of molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>G?mez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jennifer</forename><forename type="middle">N</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><surname>Miguel Hern?ndez-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjam?n</forename><surname>S?nchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Sheberla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">D</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Central Science</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The Harvard Clean Energy Project: Large-Scale Computational Screening and Design of Organic Photovoltaics on the World Community Grid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Hachmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Olivares-Amaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sule</forename><surname>Atahan-Evrenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Amador-Bedolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roel</forename><forename type="middle">S</forename><surname>S?nchez-Carrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aryeh</forename><surname>Gold-Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><surname>Vogt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><forename type="middle">M</forename><surname>Brockway</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Physical Chemistry Letters</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="2241" to="2251" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05584</idno>
		<title level="m">Representation learning on graphs: Methods and applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanis?aw</forename><surname>Jastrz?bski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damian</forename><surname>Le?niak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">Marian</forename><surname>Czarnecki</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.06289</idno>
		<title level="m">Learning to smile (s)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasad</forename><surname>Shiva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Kasiviswanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rudelson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05510</idno>
		<title level="m">Restricted isometry property under high correlations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="595" to="608" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooks</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos? Miguel Hern?ndez-Lobato</forename><surname>Paige</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01925</idno>
		<title level="m">Grammar variational autoencoder</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Rdkit: Open-source cheminformatics software</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Landrum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Practical model selection for prospective virtual screening. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengchao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moayad</forename><surname>Alnammi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">F</forename><surname>Ericksen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Voter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Keck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Wildman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gitter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">337956</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deep neural nets as a method for quantitative structure-activity relationships</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junshui</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Sheridan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Liaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Svetnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="263" to="274" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning a Local-Variable Model of Aromatic and Conjugated Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">K</forename><surname>Matlock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Na</forename><forename type="middle">Le</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S. Joshua</forename><surname>Swamidass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Central Science</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="52" to="62" />
			<date type="published" when="2018-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Merck molecular activity challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Merck</surname></persName>
		</author>
		<ptr target="https://www.kaggle.com/c/MerckActivity" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The generation of a unique machine description for chemical structures-a technique developed at chemical abstracts service</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hl Morgan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Documentation</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="1965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Electronic spectra from tddft and machine learning in chemical space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghunathan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Hartmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enrico</forename><surname>Tapavicza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O Anatole Von</forename><surname>Lilienfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of chemical physics</title>
		<imprint>
			<biblScope unit="volume">143</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">84111</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Deep Learning for the Life Sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Eastman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Walters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<ptr target="https://www.amazon.com/Deep-Learning-Life-Sciences-Microscopy/dp/1492039837" />
		<imprint>
			<date type="published" when="2019" />
			<pubPlace>O&apos;Reilly Media</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Maximum unbiased validation (muv) data sets for virtual screening based on pubchem bioactivity data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Knut</forename><surname>Rohrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baumann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="184" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Enumeration of 166 billion organic small molecules in the chemical universe database gdb-17</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Ruddigkeit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruud</forename><surname>Van Deursen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Louis</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reymond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2864" to="2875" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Quantum-chemical insights from deep tensor neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kristof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Farhad</forename><surname>Sch?tt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Arbabzadah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><forename type="middle">R</forename><surname>Chmiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tkatchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">13890</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Kernel methods for pattern analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nello</forename><surname>Cristianini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Weisfeiler-lehman graph kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nino</forename><surname>Shervashidze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Schweitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">Jan</forename><surname>Van Leeuwen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Mehlhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2539" to="2561" />
			<date type="published" when="2011-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Molecular descriptors for chemoinformatics: volume I: alphabetical listing/volume II: appendices, references</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Todeschini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viviana</forename><surname>Consonni</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">41</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Tox21 Data Challenge. Tox21 data challenge</title>
		<ptr target="https://tripod.nih.gov/tox21/challenge/" />
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep learning as an opportunity in virtual screening</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?nter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marvin</forename><surname>Steijaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>J?rg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Ceulemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Baselines and bigrams: Simple, good sentiment and topic classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="90" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Smiles. 2. algorithm for generation of unique smiles notation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Weininger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Weininger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><forename type="middle">L</forename><surname>Weininger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Chemical Information and Computer Sciences</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="101" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Towards universal paraphrastic sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.08198</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenqin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bharath</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caleb</forename><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip S</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.00596</idno>
		<title level="m">A comprehensive survey on graph neural networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">On the dimensionality of word embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="895" to="906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.08804</idno>
		<title level="m">Hierarchical graph representation learning withdifferentiable pooling</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08434</idno>
		<title level="m">Graph neural networks: A review of methods and applications</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
