<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning from Crowds by Modeling Common Confusions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhendong</forename><surname>Chu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongning</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Virginia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning from Crowds by Modeling Common Confusions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T09:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Crowdsourcing provides a practical way to obtain large amounts of labeled data at a low cost. However, the annotation quality of annotators varies considerably, which imposes new challenges in learning a high-quality model from the crowdsourced annotations. In this work, we provide a new perspective to decompose annotation noise into common noise and individual noise and differentiate the source of confusion based on instance difficulty and annotator expertise on a per-instance-annotator basis. We realize this new crowdsourcing model by an end-to-end learning solution with two types of noise adaptation layers: one is shared across annotators to capture their commonly shared confusions, and the other one is pertaining to each annotator to realize individual confusion. To recognize the source of noise in each annotation, we use an auxiliary network to choose from the two noise adaptation layers with respect to both instances and annotators. Extensive experiments on both synthesized and real-world benchmarks demonstrate the effectiveness of our proposed common noise adaptation solution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The availability of large amounts of labeled data is often a prerequisite for applying supervised learning solutions in practice. Crowdsourcing makes it possible to collect massive labeled data in both time-and cost-efficient manner <ref type="bibr">(Buecheler et al. 2010)</ref>. However, because of varying and unknown expertise of annotators, crowdsourced labels are usually noisy, which naturally lead to an important research problem: how to train an accurate learning model with only crowdsourced annotations?</p><p>The first step to estimate an accurate learning model from crowdsourced annotations is to properly model the generation of such data. In this work, we focus on the crowdsourced classification problem. The seminal work from <ref type="bibr" target="#b5">Dawid and Skene (1979)</ref> (known as the DS model) assumes that each annotator has his/her own class-dependent confusion when providing annotations to instances. This is modeled by an annotator-specific confusion matrix, whose entries are the probability of flipping one class into another. The DS model has become the cornerstone of most learning from crowds solutions; and mainstream solutions perform label aggregation prior to classifier training: their key difference lies on Copyright ? 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. different label aggregation methods based on the DS model <ref type="bibr" target="#b22">(Venanzi et al. 2014;</ref><ref type="bibr" target="#b27">Zhang et al. 2014;</ref><ref type="bibr" target="#b24">Whitehill et al. 2009</ref>). Recent developments focus more on unified solutions, where variants of the Expectation-Maximization (EM) algorithm are proposed to integrate label aggregation and classifier training <ref type="bibr" target="#b0">(Albarqouni et al. 2016;</ref><ref type="bibr" target="#b4">Cao et al. 2019;</ref><ref type="bibr" target="#b16">Raykar et al. 2010)</ref>. Typically, such solutions treat the classifier's predictions as latent variables, which are then mapped to the observed crowdsourced labels using individual confusion matrices of annotators. <ref type="bibr" target="#b18">Rodrigues and Pereira (2018)</ref> further fuse label inference and classifier training in an endto-end approach using neural networks, where the gradient from label aggregation is directly propagated to estimate the annotators' confusion matrices. <ref type="bibr" target="#b21">Tanno et al. (2019)</ref> propose a similar solution but encourage the annotator confusion matrix to be close to an identity matrix by trace regularization.</p><p>All existing DS-model-based solutions assume noise in crowdsourced labels is only caused by individual annotators' expertise. However, it is not uncommon that different annotators would share common confusion about the labels. For example, when a bird in an image is too small, every annotator has a chance to confuse it with an airplane because of the background sky. We hypothesize that on an instance the annotator is confident about, he/she is more likely to use his/her expertise to provide a label (i.e., introducing individualized noise), while he/she would use common sense to label those unconfident ones. We empirically evaluate this hypothesis on two public crowdsourcing datasets, one for image labeling and one for music genre classification (more details of the datasets can be found in the Experiment Section), and visualize the results in <ref type="figure">Figure 1</ref>. On both datasets, there are quite some commonly made mistakes across annotators. For example, on the image labeling dataset La-belMe, 61.0% annotators mistakenly labeled street as inside city and 44.1% of them mislabeled open country as forest; on the music classification dataset, 63.6% annotators mislabeled metal as rock and 38.6% of them mislabeled disco as pop. The existence of such shared confusions across annotators directly affects label aggregation: the majority of annotators are not necessarily correct, as their mistakes are no longer independent (e.g., those large off-diagonal entries in <ref type="figure">Figure 1</ref>). This is against the fundamental assumption in the DS model, and strongly urges new noise modeling to better handle real-world crowdsourced data.</p><p>Moving beyond the independent noise assumption in the arXiv:2012.13052v2 <ref type="bibr">[cs.</ref>LG] 12 Jun 2021  <ref type="figure">Figure 1</ref>: Analysis of commonly made mistakes across annotators on two real-world crowdsourcing datasets. The value of each entry in the heatmap denotes the percentage of annotators with this confusion pair (e.g., mistakenly label street as inside city on LabelMe dataset).</p><p>family of DS models <ref type="bibr" target="#b5">(Dawid and Skene 1979;</ref><ref type="bibr" target="#b18">Rodrigues and Pereira 2018)</ref>, we decompose annotation noise into two sources, common noise and individual noise, and differentiate the source of noise based on both annotators and instances. We refer to the annotation confusions shared across annotators as common noise, and model it by a global confusion matrix shared by all annotators. In the meanwhile, we also maintain annotator-specific confusion matrices for individual noise modeling. We still treat ground-truth labels of instances as latent variables, but map them to noisy annotations by two parallel confusion matrices, to capture these different sources of noise. We determine the choice of confusion matrices on a per-instance-annotator basis, by explicitly modeling of annotator expertise and instance difficulty <ref type="bibr" target="#b24">(Whitehill et al. 2009;</ref><ref type="bibr" target="#b26">Yin et al. 2017)</ref>. To leverage the power of representation learning to model annotator expertise and instance difficulty, we realize all our model components using neural networks. In particular, we model the two types of confusion matrices as two parallel noise adaptation layers <ref type="bibr" target="#b6">(Goldberger and Ben-Reuven 2016)</ref>. For each annotator-instance pair, the classifier first maps the instance to a latent class label, then an auxiliary network decides which noise adaptation layer to map the latent class label to the observed annotation. Cross-entropy loss is counted on the predicted annotations for end-to-end training of these components. We name this approach CoNAL -learning from crowds with common noise adaptation layers. Extensive experiments show considerable improvement of our new noise modeling approach against a rich set of baselines on two synthesized datasets, including a fully synthesized dataset and one based on CIFAR-10 dataset with various settings of noise generation, as well as two real-world datasets, e.g., La-belMe for image classification, and Music for music genre classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Works</head><p>Several existing studies focused on modeling the different roles of instance and annotator in crowdsourced data. <ref type="bibr" target="#b24">Whitehill et al. (2009)</ref> model the accuracy of each annotation, which depends on instance difficulty and annotator expertise, to weigh each instance in final majority vote. <ref type="bibr" target="#b23">Welinder et al. (2010)</ref> model each annotator as a multi-dimensional classifier and consider instance difficulty as single dimension latent variable. <ref type="bibr" target="#b28">Zhou et al. (2012)</ref> propose a minimax entropy principle on a probability distribution over annotators, instances and annotations, in which by minimizing entropy instance confusability and annotator expertise are naturally inferred. <ref type="bibr" target="#b12">Khetan and Oh (2016)</ref> and <ref type="bibr" target="#b20">Shah, Balakrishnan, and Wainwright (2016)</ref> consider generalized DS models which model the instance difficulty. Instead of simply using a single scalar to model instance difficulty and annotator expertise as in previous works, we model them by learning their corresponding representations via an auxiliary network, which can better capture the shared statistical pattern across observed annotations. Our method is closely related to several existing DS-based models considering relations among annotators; but it is also clearly distinct from them. <ref type="bibr" target="#b11">Kamar, Kapoor, and Horvitz (2015)</ref> use a global confusion matrix to capture the identical mistakes by all annotators, and it is designed to replace the individual matrix when observations of an annotator are rare. Moreover, the choice of confusion matrix in this solution only depends on the number of annotations an annotator provided. This unnecessarily reflects the annotator expertise, as the task assignment is typically out of their control in crowdsourcing. <ref type="bibr" target="#b22">Venanzi et al. (2014)</ref> and <ref type="bibr" target="#b9">Imamura, Sato, and Sugiyama (2018)</ref> cluster annotators to generate their own confusion matrices from a shared communitywide confusion matrix. However, the above approaches still assume a single underlying noise source, and thus they do not consider the difference between global (or communitylevel) and individual confusions. <ref type="bibr" target="#b15">Li, Rubinstein, and Cohn (2019)</ref> explore the correlation of annotation across annotators by classifying them into auxiliary subtypes under different ground-truth classes. However, the characteristics of each annotator are missing since they are only represented by a specific subtype. In our work, we still characterize individual annotators by modeling their own confusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Common Confusion Modeling in Crowdsourced Data</head><p>In this section, we formulate our problem-solving framework for training classifiers directly from crowdsourced labels, based on the insight of common confusion modeling across annotators. We first describe the notations and our probabilistic modeling of the noisy annotation process, considering both common and individual confusions. This probabilistic model of noisy annotations is the basis of the endto-end neural solution we develop in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Notations and Probabilistic Modeling</head><p>Assume we have N instances labeled by R annotators out of C possible classes. We define x i as the feature vector of the i-th instance and y r i as its label provided by the r-th annotator. Denote z i as the unobservable ground-truth label for the i-th instance, which is considered as a latent variable sampled from a multinomial distribution parameterized by {p(z i = c|x i )} C c=1 . For simplicity, we collectively define</p><formula xml:id="formula_0">X = {x i } N i=1 , Y = {y r i } N,R i=1,r=1 and Z = {z i } N i=1 .</formula><p>The final goal of learning from crowds is to obtain the classifier P (Z|X) only with crowdsourced annotations Y .</p><p>Similar to the DS-based models (see <ref type="figure" target="#fig_2">Figure 2a</ref> for reference), the confusion of the r-th annotator is measured by an annotator-specific confusion matrix ? r , in which the (z, z )-element ? r z,z denotes the probability that annotator r will label the true label z as z . Aside from individual confusion, the key assumption of our solution is that annotation mistakes can also be introduced by common confusion, which is modeled by a globally shared confusion matrix ? g across all annotators. We define the confusion matrices set as ? = {? 1:R , ? g }. We associate a Bernoulli random variable s r i ? B(? r i ) with each annotation y r i to differentiate the source of noise on it: s r i =1 if the confusion is caused by the common noise, where w r i is the probability of the global confusion matrix being chosen by annotator r on instance i (see <ref type="figure" target="#fig_2">Figure 2b</ref>). Denote the set of parameters governing the generation of s r i across all annotations as ?. Suggested by the successful practice in modeling crowdsourced data, we also impose the following two commonly made assumptions: 1) each annotator provides their annotations independently <ref type="bibr" target="#b5">(Dawid and Skene 1979)</ref>; and 2) each annotation is independent from the instance's features given the ground-truth labels <ref type="bibr" target="#b25">(Yan et al. 2014;</ref><ref type="bibr" target="#b18">Rodrigues and Pereira 2018)</ref>. We should note the first assumption is not contradicting to our common confusion modeling: as the annotators can independently choose the shared common noise model to generate their annotations, the resulting observed annotations are no longer independent across annotators. As a result, the complete data likelihood of observed annotations under our model can be defined as,</p><formula xml:id="formula_1">p (Y, Z|X, ?, ?) = N i=1 R r=1 C z=1 p (y r i |z i ; ?, ? r i )p(z i |x i ) ,</formula><p>(1) p (y r i |z i ; ?, ? r i ) = ? r i p (y r i |z i , ? g ) + (1 ? ? r i ) p (y r i |z i , ? r ) . Based on the above imposed problem structure, we derive an information-theoretical lower bound about the resulting noise modeling quality. Let? be the estimated true labels of all instances. Noise modeling quality is measured by the error rate given by L(?,</p><formula xml:id="formula_2">Z) = 1 N N i=1 I (? i = z i ),</formula><p>where I(?) is an indicator function. Given the ground-truth instance-specific class distribution ? i = {? ic } C c=1 and confusion matrices ?, we have the following theorem about the lower bound of minimax error rate of our model. Theorem 1. The minimax error rate of our model is lower bounded by</p><formula xml:id="formula_3">inf?sup Z?[C] N E L(?, Z) (2) ? 1 N 2 log C N i=1 F (? i , ?, ?) ? log 2 N 2 log C , F (? i , ?, ?) =H(? i ) ? R r=1 C c=1 C c =1 ? ic ? ic ? r i KL(? g c * ? g c * ) + (1 ? ? r i ) KL (? r c * ? r c * ) .</formula><p>where H(? i ) = ? C c=1 ? ic log? ic is the entropy of ground-truth class distribution and ? c * is the c-th row in confusion matrix ?. The proof and further discussion of Theorem 1 is provided in Appendix A.  Remarks. This result extends the known lower bound result of DS models <ref type="bibr" target="#b9">(Imamura, Sato, and Sugiyama 2018)</ref>. Lower bound on the error rate measures the difficulty of a crowdsourcing problem. Theorem 1 suggests the proposed decomposition has the potential to further reduce the lower bound, i.e., to obtain better inferred true labels. To understand this result, we should first note that the lower bound mainly depends on the KL distance between the class distributions conditioned on different ground-truth classes, as defined in F (? i , ?, ?), i.e., how two different classes will be confused with other classes. The more different they are (i.e., a larger KL distance), the easier one can differentiate the two from the observed noisy labels. For example, consider a crowdsourced dataset where an annotator labels a set of instances as airplane; but among them, 50% cases should be bird, and the other 50% should be spacecraft. Intuitively, without any additional knowledge, it is hard to determine the true label when he/she labels an instance as airplane. And this is asserted by Theorem 1: If we only used a single confusion matrix for this annotator, the conditional class distributions for bird and spacecraft will be pushed closer, because their entries on airplane are close. This causes a smaller KL term in F (? i , ?, ?) between bird and spacecraft (e.g., setting ? r i =0 for all instances in annotator r). But if we knew that the confusion between bird and airplane is caused by common noise, and the confusion between spacecraft and airplane is caused by individual noise, these mistakes could be attributed to two confusion matrices separately, which eliminates the misleading similarity between the conditional probabilities for bird and spacecraft caused by airplane.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>End-to-end Learning Framework</head><p>To apply our noise modeling in crowdsourced data, we need to estimate the confusion matrices ? together with the classifier. Instead of building a vanilla tabular model for them, we realize them using neural models, to take advantage of the power of representation learning. In particular, we map the output of the classifier to noisy annotations by two types of confusion layers, which we refer to as noise adaptation layers <ref type="bibr" target="#b6">(Goldberger and Ben-Reuven 2016)</ref>. We also introduce an auxiliary network that takes both annotator and instance as input to predict the choice of these two noise adaptation layers. Since we treat the ground-truth label of an instance as a latent variable, the Expectation Maximization (EM) algorithm becomes a natural choice for model learning, as typically done in literature <ref type="bibr" target="#b0">(Albarqouni et al. 2016;</ref><ref type="bibr" target="#b18">Rodrigues and Pereira 2018;</ref><ref type="bibr" target="#b1">Bertsekas 2014)</ref>. For the integrity of work, we provide the derived EM algorithm in Appendix B for interested readers. However, the EM-based algorithm has several clear drawbacks in our solution: 1) In crowdsourced data, because the annotators typically only label a small proportion of instances, EM-based algorithm becomes very sensitive to the initialization of model parameters. It can easily cause instability issues in training a neural network model. 2) In every EM iteration, we need to retrain the neural network, which causes a huge overhead when handling large networks. Instead, we take an end-toend approach to jointly perform latent variable inference and model parameter estimation. We define cross-entropy loss on the observed annotations and use error back-propagation to update the classifier's output and the network parameters simultaneously.</p><formula xml:id="formula_4">e 1:R x i Classifier ? Aux.Net W a ? 1:R i ? ? 1:R i ? (1 ? ? 1:R i ) + = f i W g f i W 1:R f i h 1:R i input parallel noise adaptation layers backbone model predicted anno. dist.</formula><p>Figure 3: Overview of our framework for classification with 3 classes and R annotators.</p><p>We construct a neural network classifier with non-linear intermediate layers and a softmax output layer. The probability distribution of the predicted true label z i given the instance feature vector x i is thus specified as p ? (z i |x i ), where ? is the network parameter set including the softmax layer. We denote the immediate output of the classifier as f i = f (x i ) ? R C . We then use noise adaptation layers to map the classifier's output into noisy annotations, which are implemented by introducing additional softmax output layers on top of the output layer of the classifier (see overview in <ref type="figure">Figure 3</ref>). The weight matrices of the noise adaptation layers resemble confusion matrices ? in a probabilistic sense. The output of the noise adaptation layer is thus the probability distribution of predicted annotation p W (? r i |f (x i )), where W is the parameter set of the noise adaptation layer.</p><p>We consider two types of noise adaptation layers: one individual noise adaptation layer for every annotator parameterized by W r , and a common noise adaptation layer shared across all annotators parameterized by W g . The final probability distribution of annotations is obtained as,</p><formula xml:id="formula_5">p(? r i |x i ) = ? r i p W g (? r i |f (x i )) + (1 ? ? r i ) p W r (? r i |f (x i )).</formula><p>where ? r i governs the distribution that the mistake of annotator r on instance i is caused by common confusion ? g , denoted by the noise source indicator s r i . As s r i is unobservable, we introduce an auxiliary network to model s r i ? B(? r i ) by parameterizing it over annotator expertise and instance difficulty, both of which are modeled via learnt representations by the auxiliary network. Specifically, as in our problem setup, every instance is associated with raw features, the auxiliary network takes instance feature x i as input for learning instance i's embedding v i . The same can be applied to annotator r, if any raw feature e r is available about the annotator, otherwise we use its onehot encoding as input for learning annotator embedding u r .</p><p>Then ? r i can be obtained as follows</p><formula xml:id="formula_6">, v i = W v x i + b v , u r = W u e r + b u , ? r i = ?(u r v i ).</formula><p>( <ref type="formula">3)</ref> where (W v , b v ) and (W u , b u ) are weight matrices and bias terms for annotator and instance embeddings, and ? is a sigmoid function. To simplify our notations, we collectively refer the parameters in this auxiliary network as W a . To avoid the magnitude of learnt u and v becoming extremely large or small, which causes numerical issues in estimating ? r i , we normalize the learnt annotator and instance embeddings before computing their inner product.</p><p>Based on the above full specifications of our probabilistic modeling using neural networks, we are ready to estimate the network parameters. We can easily verify that, maximizing the likelihood of observed annotations given the input feature vectors as defined in Eq <ref type="formula">(1)</ref> is equivalent to minimizing the cross-entropy loss between the observed annotations and predicted annotation distributions,</p><formula xml:id="formula_7">L(?, W g , W 1:R , W a ) = ? 1 N N i=1 R r=1 C j=1 y r ij log p j (? r i |x i ).</formula><p>where y r ij = 1 if y r i = j; otherwise y r ij = 0; and p j (? r i |x i ) refers to the j-th entry of the predicted annotation distribution. All parameters can be trained by back-propagation using gradient descent techniques, such as Adam <ref type="bibr" target="#b13">(Kingma and Ba 2014)</ref> and SGD <ref type="bibr" target="#b7">(Goodfellow, Bengio, and Courville 2016)</ref>. Once trained, in the testing phase, we can directly use the classifier to make predictions on new instances.</p><p>The gradient flow in back-propagation reveals how our common confusion modeling handles crowdsourced data. In the context of classification, we can simply view the introduced noise adaptation layer as performing a projection of gradients; and with a slight abuse of notations, we denote the output of our noise adaptation layers as h r</p><formula xml:id="formula_8">i = ? r i W g f i + (1 ? ? r i )W r f i .</formula><p>Under the chain rule, the gradients are naturally decoupled with respect to different sources of noise,</p><formula xml:id="formula_9">?L ?f i = R r=1 ?L ?h r i ?h r i ?f i = R r=1 ? r i ?L ?h r i W g +(1?? r i ) ?L ?h r i W r .</formula><p>(4) It clearly shows confusion matrices reshape the gradients, which informs the classifier layer what the true label should be on an instance given its noisy annotations. The importance of each confusion matrix in shaping the classifier is determined by ? r i , which infers the source of noise based on annotator expertise and instance difficulty.</p><p>The gradients in Eq (4) also suggest a potential bottleneck of our proposed solution: if the common and individual noise adaptation layers are unidentifiable, we cannot correctly attribute the noise, which is the key for our solution to perform according to Theorem 1. To avoid this, we add 2norm on the difference between the common and individual noise adaptation layers as a regularization term, to enforce them to be different. This presents our final loss function,</p><formula xml:id="formula_10">L(?, W g , W 1:R , W a ) = ? 1 N N i=1 R r=1 C j=1 y r ij log p j (? r i |x i ) ? ? R r=1 W g ? W r 2</formula><p>where ? is a hyper-parameter to control regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>We evaluate our method on both synthesized and real-world datasets. We consider a rich set of related solutions as our baselines, which can be divided into two categories: 1) Methods with simple noise models. DL-MV: it learns a neural network classifier with labels aggregated by majority voting. DL-CL <ref type="formula">(</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on Synthesized Datasets</head><p>We evaluate the proposed method under various settings of synthesized data. Particularly, we demonstrate the effectiveness of our model with different (1) common confusion types;</p><p>(2) common noise strength, which is defined as the sum of off-diagonal entries in the common confusion matrix; and (3) proportion of common noise, which reflects the percentage of annotations introduced by common confusion. Datasets description. We generate synthesized crowdsourced data on two datasets, where we directly manipulate the number of annotators and annotation generation under a variety of settings. On the Synthetic dataset, we completely synthesized everything. We first sample a mean vector for every class and then sample instance features from a multivariate Gaussian distribution parameterized by this mean vector. In particular, we randomly generate 10,000 instances with 6 classes, which are split into a 8,000-instance training set, a 1,000-instance validation set and a 1,000-instance testing set. The CIFAR-10 dataset is generated based on the CIFAR-10 image classification dataset <ref type="bibr" target="#b14">(Krizhevsky, Hinton et al. 2009</ref>). It consists of 60,000 32 ? 32 color images from 10 classes, which are split into a 40,000-instance training set, a 10,000-instance validation set and a 10,000-instance testing set. Image features are used to train the neural classifier on this dataset. In both datasets, each instance in the training set is labeled by averaging 3 randomly selected annotators out of 30 in total. Synthesizing annotations. We consider two representative noise patterns in common noise: (1) Asymmetric confusion. Every class is mapped to another uniformly chosen class (2) Symmetric confusion. On Synthetic dataset, two random classes are paired and flipped into each other. And on CIFAR-10 dataset, we manually paired similar classes (e.g., bird and airplane) to be flipped with each other. For individual confusion matrices, we use asymmetric confusion. We generate one global confusion matrix, and one individual confusion matrix for every annotator. In our experiments, the range of common noise strength is set to [0.4, 0.8], while the individual noise strength of annotators is fixed to 0.7. In both noise generation patterns, the noise strength is evenly distributed among the chosen off-diagonal entries.</p><p>To control the source of noise in each annotation, i.e., s r i , we randomly generate a set of annotator features u, which are not disclosed to the learners. Given instance feature vector v i and annotator feature vector u r , we compute ? r i by Eq (3) with the ground-truth weight matrices (W u , b u ) and (W v , b v ). These weight matrices are not disclosed to the learner. The bias terms are used to control the average proportion of common noise across annotations into a range of [0.3, 0.7]. When we generate annotation y r i for instance i by annotator r, we first sample s r i ? B(? r i ). If s r i = 1, the common confusion matrix ? g will be used; otherwise, individual confusion matrix ? r will be used. Then we sample y r i from the chosen confusion matrix based on the true label z i of this instance. We also include a special case that the proportion is 0, where there is no common confusion.</p><p>In our experiments, when studying the influence of common noise strength on the learnt classifier, the average proportion of common noise is controlled to be around 0.5. When studying the influence of the proportion of common noise in each annotation, the common and individual noise strength is controlled to 0.4 and 0.7 respectively. Backbone networks &amp; training details. On the Synthetic dataset, we apply a simple network with only one fully connected (FC) layer (with 128 units and ReLU activations), along with a softmax output layer, using 50% dropout. On the CIFAR-10 dataset, we follow the setting of <ref type="bibr" target="#b4">Cao et al. (2019)</ref> to use VGG-16 as the backbone network. We trained the network using the Adam optimizer (Kingma and Ba 2014) with default parameters and learning rate searched from {0.02, 0.01, 0.005}. The dimension of annotator and instance embedding is chosen from {20, 40, 60, 80}. The regularization term ? is searched from {10 ?4 , 10 ?5 , 10 ?6 }. All experiments are repeated 5 times with different random seeds. Model selection is achieved by choosing the model with the highest accuracy on the validation set. We report mean and standard deviation of test accuracy on the five runs. To make the comparisons fair, all the evaluated methods used the same backbone networks. We implement our framework with PyTorch, and run it on a CentOS system with one NVIDIA 2080Ti GPU with 10 GB memory. Results. We report the results on the CIFAR-10 dataset in <ref type="figure" target="#fig_3">Figure 4</ref>, where our solution demonstrated consistent improvement against all baselines across all settings. The observation on the Synthetic dataset is similar, and we present the results in Appendix C due to space limit. All the baselines assumed single source of noise, i.e., annotator-specific noise; as a result, they are heavily influenced when noise become complicated, e.g., a large proportion of mistakes from common confusion and the strength of common noise is strong. Our solution is less sensitive to the environment by decomposing and separately modeling the confusion. When there is no common confusion, the empirical result shows no significant difference between our solution and baselines in this extreme setting, which should also be expected. But we argue that this extreme setting rarely holds in reality, as annotators always share some commonsense about the world.</p><p>All models are influenced by symmetric common noise, which directly makes the swapped classes similar. Based on the lower bound provided in Theorem 1, similar conditional class distributions in the confusion matrices will make the problem more difficult, so that the degeneration of all methods are expected under symmetric confusion. In the most extreme case where the proportion of common noise is set to 0.7 and the common noise strength is set to 0.6, nearly 42% annotations are pairwise flipped. However, our method can still outperform baselines with a large margin. Mix-MIG is believed to be robust to correlated mistakes if high-quality annotator exists. However, our experiments show that common confusion poisoned the classifier obtained in Max-MIG even though every annotator is of high quality (individual noise strength is set to 0.7). DL-CL and Anno-Reg failed because they could not differentiate the source of noise, such that the gradients from the modeled annotations cannot be properly adjusted to update the classifier. Both Doctor Net and DL-MV are based on majority vote, so that they fail when the annotations across annotators are no longer independent, i.e., caused by the common confusion. Compared to methods with complex noise models, DL-GLAD directly models the annotation accuracy, which is not suitable for class-dependent confusion. DL-WC clusters correlated annotators to share confusion matrix, which can reduce the influence of common confusion. But the expertise of each annotator is missing, which leads to its bad performance. AggNet shows the advantage of directly learning from annotations rather than from aggregated labels. But it still assumes the only noise source thus cannot handle common noise well.</p><p>To understand how accurate our solution can distinguish common and individual noise, we report the learnt weights of noise adaptation layers against the ground-truth confusion matrices on the CIFAR-10 dataset in <ref type="figure" target="#fig_5">Figure 5</ref>. In this experiment, we set the common noise strength to 0.7 and the proportion of common noise to 0.5. We can find that in most cases the ground-truth common noise pattern is well recovered, especially under the asymmetric noise pattern.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments on Real-world Datasets</head><p>Datasets description. We consider two real-world datasets. LabelMe <ref type="bibr" target="#b18">(Rodrigues and Pereira 2018;</ref><ref type="bibr" target="#b19">Russell et al. 2008)</ref> is an image classification dataset, consists of 2,688 images from 8 classes, where 1,000 of them are labeled by annotators from Amazon Mechanical Turk (AMT) 1 and the remainings are used for validation and testing. Each image is labeled by an average of 2.5 annotators, with a mean accuracy of 69.2%. Standard data augmentation techniques are used on training data, including horizontal flips, rescaling and shearing, to enrich the training set to 10,000 images. Music (Rodrigues, Pereira, and Ribeiro 2014) is a music genre classification dataset, consisting of 1,000 samples of songs with 30 seconds length from 10 music genres, where 700 of them are labeled by AMT annotators and the rest are used for testing. Each sample is labeled by an average of 4.2 annotators, with a mean annotation accuracy of 73.2%. Backbone networks &amp; training details. For LabelMe dataset, we followed the setting of <ref type="bibr" target="#b18">Rodrigues and Pereira (2018)</ref>: we apply a pre-trained VGG-16 network followed by a FC layer with 128 units and ReLU activations, and a softmax output layer, using 50% dropout. For Music dataset, we use the same FC layer and softmax layer as LabelMe. Batch normalization <ref type="bibr" target="#b10">(Ioffe and Szegedy 2015)</ref> is performed 1 https://www.mturk.com/ in each layer. Other hyper-parameters are the same as the synthesized experiments. Results. As reported in <ref type="table">Table 1</ref>, CoNAL achieved new stateof-the-art performance on both real-world datasets. In particular, we looked into the accuracy on classes where commonly made mistakes across annotators are observed (see <ref type="figure">Figure 1</ref>). For example, for open country on LabelMe, its accuracy in CoNAL is 67.21%, while the best baseline Max-MIG only achieved 54.19%. The good performance aligns with our analysis in Theorem 1, by differentiating common and individual confusions, it is easier to find the true labels. We provide the visualization of the learned confusion matrices and the training and testing accuracy plots on real-world datasets in Appendix C. Influence of the regularization term ?. We studied the influence of different ? in <ref type="table" target="#tab_2">Table 2</ref>. The results show by enforcing the noise adaptation layers to be different, the performance is improved on both datasets. The value of ? also matters, and 10 ?5 achieves best performance empirically.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion &amp; Future works</head><p>In this paper, we study the problem of learning from crowds with noisy annotations. Aside from the widely employed independent noise assumptions across annotators, we decompose annotation noise into common and individual confusions. We used neural networks to realize our probabilistic modeling of crowdsourced data, and estimate each component in our solution in an end-to-end fashion. Extensive empirical evaluations confirm the advantage of our solution in learning from complicated real-world crowdsourced data. Our solution is also flexible: it can be easily applied to any existing neural classifiers by simply connecting with the proposed noise adaptation layers. In our current solution, all annotators share the same global confusion matrix. An interesting extension is to consider group-wise confusion, where we keep a shared confusion matrix for each annotator group, and identify the groups by optimization. It is also worthwhile to extend the solution to a proactive setting, e.g., probe annotators for more annotations so as to improve common confusion modeling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethics statement</head><p>Our study focuses on tackling an urgent problem in this deep learning era: learning from crowds. High-quality labels are needed for real-world deep learning applications; however, they are typically difficult and expensive to collect in practice. Hence, we propose to directly learn from labels given by non-expert annotators, considering both common mistakes and individualized mistakes. On the one hand, industrial applications will benefit from this work since nonexpert labels are both cost-and time-effective to enable deployment of deep learning systems. On the other hand, our work also has academic impact. Our method can be applied to new research problems where high-quality labeled data is rare but crowdsourced labels are easy to obtain, such as medical image classification. The potential issue of common noise modeling is it might open the door for adversarial annotators. When previously modeled independently, they need to provide a large number of annotations to poison a learner. But if an attacker gets access to common noise, he/she only needs to provide a few annotations consistent with the common noise to amplify the influence of common noise. This will also make other ordinary annotators inadvertently contribute to the attack. Another potential issue of learning from crowds is when modeling annotator expertise, we are learning an annotator profile, which has risk in disclosing their privacy, especially in privacy sensitive annotation problems. Data masking or distortion (e.g., differential privacy) is needed to protect annotators' privacy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof of Theorem 1</head><p>Proof. In our setting, the ground-truth class distribution ? i depends on the instance features. Then the minimax error rate of the crowdsourcing problem can be lower bounded by the following,</p><formula xml:id="formula_11">inf?sup Z?[C] N E L(?, Z) ? 1 N 2 logC N i=1 R(? i , ? ) ? log2 N 2 logC (5) where R(? i , ? ) = H(? i ) ? R r=1 C c=1 C c =1 ? ic ? ic KL(? r c * ? r c * )<label>(6)</label></formula><p>and ? = {? r } R r=1 denotes the set of annotator-level confusion matrices. We use ? to differentiate with our defined individual confusion matrix in the main paper. The proof of Eq (5) is similar to <ref type="bibr" target="#b9">(Imamura, Sato, and Sugiyama 2018)</ref>. Based on our new noise generation assumption, the annotation noise can be decomposed by common noise and individual noise. Thus we can further bound the minimax error rate under this noise assumption.</p><p>Under our new noise assumption, we can evaluate the confusion matrix on a per-instance-annotator basis. Specifically, in each annotation, the effective confusion matrix is a weighted combination of the global and individual confusion matrices, where the weight is w r i . In a mixture model, the Kullback-Leibler divergence can be decomposed accordingly by,</p><formula xml:id="formula_12">KL(? r c * ? r c * ) = KL(? r i ? g c * + (1 ? ? r i )? r c * ? r i ? g c * + (1 ? ? r i )? r c * ) ? KL(? r i ? r i ) + ? r i KL(? g c * ? g c * ) + (1 ? ? r i ) KL(? r c * ? r c * ) (7) = ? r i KL(? g c * ? g c * ) + (1 ? ? r i ) KL(? r c * ? r c * )<label>(8)</label></formula><p>where ? r i = (? r i , 1 ? ? r i ). The inequality can be derived by the log-sum inequality. Substitute Eq (8) back to Eq (6), we can get the new term F (?, ?, ?) in Theorem 1. Plug it back into Eq (5), we can get the refined result in our Theorem 1,</p><formula xml:id="formula_13">inf?sup Z?[C] N E L(?, Z) ? 1 N 2 log C N i=1 F (? i , ?, ?) ? log 2 N 2 log C Corollary 1.1. When N ? 2log2</formula><p>maxF (?i,?,?) , increasing the number of instances N will decrease the error rate bound.</p><p>Proof.</p><formula xml:id="formula_14">1 N 2 log C N i=1 F (? i , ?, ?) ? log 2 N 2 log C ? maxF (? i , ?, ?) N log C ? log 2 N 2 log C ,</formula><p>When the gradient of the upper bound is less than 0, the upper bound will decrease when N is growing. This can be achieved by setting N by the following,</p><formula xml:id="formula_15">? 1 N 2 maxF (? i , ?, ?) log C + 2log2 N 3 log C ? 0 ? N ? 2log2 maxF (? i , ?, ?)</formula><p>Remarks. The corollary shows when the number of instances is growing, the label aggregation quality gets improved. Also, we need to point out the structure of confusion matrices ? is more important than the number of classes C in this lower bound. With a larger KL distance between every pair of rows in ?, we can expect an improved error lower bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. EM algorithm for learning from crowds by modeling common confusions</head><p>The EM algorithm is a generic solution for aggregating crowdsourced labels in classic crowdsourcing problems <ref type="bibr" target="#b5">(Dawid and Skene 1979;</ref><ref type="bibr" target="#b9">Imamura, Sato, and Sugiyama 2018)</ref>, and it can also be used under our common confusion assumption. Though we have pointed out the main drawbacks of EM-based algorithms in our solution framework, we still list the procedures of using EM algorithm in our problem for interested readers. In particular, we demonstrate a two-step solution, where the latent indicator s r i is drawn from a Bernoulli distribution directly parameterized by ? r i and the ground-truth label z i is drawn from a multinomial distribution p ? (z i |x i ) parameterized by ?, which is essentially the soft-classifier we are estimating from the crowdsourced data. Once the ground-truth labels {z i } N i=1 on instances are inferred, we estimate the parameters ? in p ? (z i |x i ) by treating the inferred labels as ground-truth. When the instance features are unavailable, we can use another multinomial distribution p(z i |?) to replace p ? (z i |x i ), where ? = {? c } C c=1 is the corresponding Dirichelet prior, to perform answer aggregation by EM as well.</p><p>Under our common confusion assumption, the conditional probability p(y r i |z i ) can be written as</p><formula xml:id="formula_16">p(y r i |z i ; ?, ? r i ) = s r i ={0,1} p(s r i |w r i )p(y r i |z i , s r i , ? r )</formula><p>Based on this conditional probability, we derive the EM procedure to infer the ground-truth labels as follows. In the E-step, we estimate hidden ground-truth label z i and latent indicator s r i . The posterior q(z i ) and q(s r i ) are obtained using Bayes' rule, <ref type="figure">Figure 6</ref>: Results on Synthetic dataset.</p><formula xml:id="formula_17">q(z i = c) ? p ?0 (z i = c|x i ) R r=1 p(y r i |z i = c; ? 0 , ? r i0 ), q(s r i = 1) ? ? r i0 p(y r i |z i , ? g 0 ), q(s r i = 0) ? (1 ? ? r i0 ) p(y r i |z i , ? r 0 )</formula><p>. where ? 0 , ? 0 and ? 0 are the current estimated parameters. In the M-step, we update the parameters of neural network ?, proportion of common noise ? and confusion matrices ?. The proportion of common noise and confusion matrices have closed-form solutions by using the Lagrange multiplier method <ref type="bibr" target="#b1">(Bertsekas 2014)</ref>,</p><formula xml:id="formula_18">? r i = q(s r i = 1) ? g c,l = N i=1 R r=1 q(z i = c)q(s r i = 1)I(y r i = l) N i=1 R r=1 q(z i = c)q(s r i = 1) , ? r c,l = N i=1 q(z i = c)q(s r i = 0)I(y r i = l) N i=1 q(z i = c)q(s r i = 0)</formula><p>To update the neural network parameter ?, we follow the approach in <ref type="bibr" target="#b6">(Goldberger and Ben-Reuven 2016;</ref><ref type="bibr" target="#b0">Albarqouni et al. 2016</ref>) and use the inferred posterior of groundtruth q(z i ) as the target. Specifically, we compute the crossentropy loss and backpropagate the error using stochastic gradient optimization techniques such as Adam <ref type="bibr" target="#b13">(Kingma and Ba 2014)</ref>. For the generic setting where instance features are unavailable, we can update the class distribution ? using its closed-form solution,</p><formula xml:id="formula_19">? c = 1 N N i=1 q(z i = c)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional experiment results</head><p>Results on Synthetic dataset. <ref type="figure">Figure 6</ref> presents the test accuracy on Synthetic dataset under the same settings as we described in Section 3. We report mean and standard deviation of test accuracy on five runs. The results align with our analysis in Section 3. Under the asymmetric confusion, our proposed approach is robust to the settings of common noise strength and the proportion of common noise. Under the symmetric confusion, all methods' performance is influenced (becomes worse); however, CoNAL still outperforms baselines with a large margin by differentiating the source of noise. <ref type="figure" target="#fig_6">Figure 7</ref> shows the learnt weights of noise adaptation layers against the ground-truth confusion matrices on the Synthetic dataset. We set the common noise strength to 0.7 and the average proportion of common noise around 0.5. We reconstruct the confusion matrix from the learnt weights by normalizing them using softmax on each row. From the results, we can clearly observe most confusion matrices (especially the confusion matrix for common noise) are well recovered under both confusion settings. Visualization of learnt confusion matrices on real-world datasets. We provide visualization of learnt confusion matrices on both real-world datasets in <ref type="figure">Figure 8</ref> and 9. We can clearly observe these two types of learnt confusion matrices, i.e., for common confusion and individual confusion, capture different mistake patterns across annotators. For example, on LabelMe dataset, the commonly made mistake from inside city to street was covered by the learnt common confusion. The same observation is also obtained on the Music dataset, such as the common mistake from jazz to blues is reflected in our learnt common confusion matrix. On the other hand, the individual noise on annotators captures their own specific mistakes. For example, on LabelMe dataset, both annotator 1 and 2 confused about tall building and inside city; but this mistake does not appear in annotator 3, 4 and 5, nor the global confusion matrix.</p><p>We also notice some low-quality annotators in the Music dataset, such as annotator 1 (with ground-truth annotation accuracy of 0.182) and annotator 5 (with ground-truth annotation accuracy of 0.108), whose annotations are almost random. By separately modeling the annotation noise at a per-annotation basis, our solution reduces the influence from such low-quality annotators in learning the common noise model and maintains the quality of inferred true labels overall.</p><p>We visualized the distribution of inferred proportion of common noise (i.e., ? r i ) across annotations to better understand how CoNAL differentiates the source of noise in individual annotations. We rank the instances by their average ? r i over all annotators who have labeled this instance in a descending order. Then we count the frequency of ground-truth labels in the top 50% and bottom 50% instances respectively and report the results in <ref type="figure">Figure 10</ref>. The larger the average ? r i in an instance is, the more likely the annotators made similar mistakes on it (i.e., the common confusion matrix can better explain the observed annotations on this instance). On La-belMe dataset, we can observe that annotators tend to make similar annotations on forest (with ground-truth annotation entropy 0.660) and mountain (with ground-truth annotation entropy 0.192), but make their own mistakes on open country (with ground-truth annotation entropy 1.287) and inside city (with ground-truth annotation entropy 1.116). In other words, the annotations on forest and mountain are much more consistent than those on open country and inside city. This observation can also be explained by the learnt confusion matrices. From the learnt global confusion matrix, we can observe confusion patterns in open country and inside city are quite scattered, and different annotators (e.g., all those five visualized annotators) have distinct confusions. While for forest and mountain, the global confusion matrix correctly maps them to the correct annotation, and individual annotators might occasionally make their own mistakes, e.g., annotator 3. Similar observations are also obtained on the Music dataset, where annotators tend to make similar mistakes on hiphop and reggae, and make their own distinct mistakes on jazz and rock.</p><p>Discussion about overparameterized models. To prove that the improved performance of our solution comes from its unique modeling of crowdsourced data other than simply an increased number of parameters to fit, we compare our model with the overparameterized DL-CL (Rodrigues and Pereira 2018), which has a similar structure as ours to capture individual confusions, but without the notion of modeling common confusion. <ref type="bibr" target="#b18">Rodrigues and Pereira (2018)</ref> discussed that simply adding more parameters can make the output of the learnt classifier lose its interpretability as a shared ground-truth estimate across annotators, so that they only used one softmax layer for each annotator upon the classifier's output layer. We add another softmax layer for each annotator, to introduce more parameters but avoid losing the interpretability of the bottleneck layer, we name it as DL-CL_Over.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>DL-CL DL-CL_Over CoNAL #Params in NAL R ? C 2 2 ? R ? C 2 (R + 1) ? C 2  We present the results on real-world datasets, along with the number of parameters in the noise adaptation layers (NAL). Even though DL-CL_Over has the most number of parameters to fit, its performance did not increase but decreased, which indicates that blindly adding more parameters will not help model crowdsourced data. Our model adds a global noise adaptation layer, which has fewer parameters than DL-CL_Over. The results prove the advantage of our model comes from its unique design to annotation confusions, but not simply more parameters to fit. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Music</head><p>Top 50% Bottom 50% <ref type="figure">Figure 10</ref>: ?-label distribution on real-world datasets. We rank the instances by average ? over all annotators and visualize the ground-truth label distribution of top 50% and bottom 50% instances.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Graphical model presentations of DS model and our common noise model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Results on CIFAR-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc><ref type="bibr" target="#b18">Rodrigues and Pereira 2018)</ref>: it learns a neural classifier with designated layers to fit individual annotator confusions (so-called crowd layer). Anno-Reg<ref type="bibr" target="#b21">(Tanno et al. 2019)</ref>: it improves DL-CL by imposing additional trace regularization on individual confusion matrices. Doctor Net<ref type="bibr" target="#b8">(Guan et al. 2018)</ref>: it learns a neural network for every annotator's annotations and aggregates the networks' output by weighted majority voting. Max-MIG<ref type="bibr" target="#b4">(Cao et al. 2019)</ref>: it jointly estimates a neural classifier and a label aggregation network using an information-theoretical loss function. 2) Methods with complex noise models. DL-GLAD: it learns a neural classifier with labels aggregated by GLAD<ref type="bibr" target="#b24">(Whitehill et al. 2009</ref>), where annotator ability and instance difficulty are modeled. DL-WC: it learns a neural classifier with labels aggregated by WC<ref type="bibr" target="#b9">(Imamura, Sato, and Sugiyama 2018)</ref>, where similar annotators are clustered to share the same confusion matrix. AggNet<ref type="bibr" target="#b0">(Albarqouni et al. 2016)</ref>: an EM-based deep model considering annotator sensitivity and specificity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Comparison between ground truth confusion matrices and learned ones on CIFAR-10 dataset. The top row is the result of asymmetric common noise. The bottom row is the result of symmetric common noise. on both datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Comparison between ground truth confusion matrices and learned ones on Synthetic dataset. The top row is the result of asymmetric common noise. The bottom row is the result of symmetric common noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Learnt global confusion matrix and individual confusion matrices of 5 annotators on LabelMe dataset. Learnt global confusion matrix and individual confusion matrices of 5 annotators on Music dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>83?0.34 83.27?0.52 82.12?0.43 82.77?0.48 85.33?0.61 83.12?0.34 82.74?0.33 84.75?0.27 87.12?0.55 Music 72.53?0.41 81.46?0.53 76.58?0.47 79.12?0.36 81.37?0.33 77.82?0.37 75.76?0.24 81.92?0.41 84.06?0.42 Table 1: Test accuracy on two real-world crowdsourcing datasets.</figDesc><table><row><cell>DL-MV</cell><cell>DL-CL Doctor Net Anno-Reg Max-MIG DL-GLAD DL-WC</cell><cell>AggNet</cell><cell>CoNAL</cell></row><row><cell>LabelMe 79.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>LabelMe 85.68?0.38 86.61?0.41 87.12?0.55 86.26?0.47 Music 82.14?0.31 83.52?0.25 84.06?0.42 82.98?0.37</figDesc><table><row><cell>?</cell><cell>0</cell><cell>10 ?4</cell><cell>10 ?5</cell><cell>10 ?6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Model performance under different ?.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>LabelMe 83.27?0.52 82.34?0.34 87.12?0.55 Music 81.46?0.53 80.47?0.27 84.06?0.42</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Comparison with the overparameterized model.</figDesc><table /><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>We thank our anonymous reviewers for their helpful comments. This work was supported by NSF 1718216, 1553568, and Department of Energy DE-EE0008227.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Aggnet: deep learning from crowds for mitosis detection in breast cancer histology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Albarqouni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Baur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Achilles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Demirci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on medical imaging</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1313" to="1321" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Constrained optimization and Lagrange multiplier methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Academic press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Buecheler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Sieg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>F?chslin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pfeifer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Crowdsourcing, open innovation and collective intelligence in the scientific method: a research agenda and operational framework</title>
	</analytic>
	<monogr>
		<title level="m">The 12th International Conference on the Synthesis and Simulation of Living Systems</title>
		<meeting><address><addrLine>Odense, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2010-08-23" />
			<biblScope unit="page" from="679" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Max-mig: an information theoretic approach for joint learning from crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13436</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Maximum likelihood estimation of observer error-rates using the EM algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Skene</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series C (Applied Statistics)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="20" to="28" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Training deep neural-networks using a noise adaptation layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ben-Reuven</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Who said what: Modeling individual labelers improves classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gulshan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Analysis of minimax error rate for crowdsourcing and its application to worker clustering model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Imamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.04551</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Identifying and accounting for task-dependent bias in crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Third AAAI Conference on Human Computation and Crowdsourcing</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Achieving budget-optimality with adaptive schemes in crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4844" to="4852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploiting worker correlation for label aggregation in crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3886" to="3895" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning from crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">C</forename><surname>Raykar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Valadez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Florin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bogoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Moy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1297" to="1322" />
			<date type="published" when="2010-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Gaussian process classification and active learning with multiple annotators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="433" to="441" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep learning from crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Rodrigues</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">LabelMe: a database and web-based tool for image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="157" to="173" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A permutation-based model for crowd labeling: Optimal estimation and robustness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">B</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.09632</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning from noisy labels by regularized estimation of annotator confusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tanno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saeedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11244" to="11253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Community-based bayesian aggregation models for crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Venanzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guiver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kazai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shokouhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on World wide web</title>
		<meeting>the 23rd international conference on World wide web</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The multidimensional wisdom of crowds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2424" to="2432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Whose vote should count more: Optimal integration of labels from labelers of unknown expertise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Whitehill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Ruvolo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2035" to="2043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning from multiple annotators with varying expertise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rosales</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="291" to="327" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Aggregating crowd wisdoms with label-aware autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 26th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1325" to="1331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spectral methods meet EM: A provably optimal algorithm for crowdsourcing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1260" to="1268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning from the wisdom of crowds by minimax entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2195" to="2203" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
