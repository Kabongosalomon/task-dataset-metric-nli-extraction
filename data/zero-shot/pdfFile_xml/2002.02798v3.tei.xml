<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">How to Train Your Neural ODE: the World of Jacobian and Kinetic Regularization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Finlay</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn-Henrik</forename><surname>Jacobsen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levon</forename><surname>Nurbekyan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><forename type="middle">M</forename><surname>Oberman</surname></persName>
						</author>
						<title level="a" type="main">How to Train Your Neural ODE: the World of Jacobian and Kinetic Regularization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training neural ODEs on large datasets has not been tractable due to the necessity of allowing the adaptive numerical ODE solver to refine its step size to very small values. In practice this leads to dynamics equivalent to many hundreds or even thousands of layers. In this paper, we overcome this apparent difficulty by introducing a theoretically-grounded combination of both optimal transport and stability regularizations which encourage neural ODEs to prefer simpler dynamics out of all the dynamics that solve a problem well. Simpler dynamics lead to faster convergence and to fewer discretizations of the solver, considerably decreasing wall-clock time without loss in performance. Our approach allows us to train neural ODE-based generative models to the same performance as the unregularized dynamics, with significant reductions in training time. This brings neural ODEs closer to practical relevance in large-scale applications.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent research has bridged dynamical systems, a workhorse of mathematical modeling, with neural networks, the defacto function approximator for high dimensional data. The great promise of this pairing is that the vast mathematical machinery stemming from dynamical systems can be leveraged for modelling high dimensional problems in a dimension-independent fashion.</p><p>Connections between neural networks and ordinary differential equations (ODEs) were almost immediately noted after residual networks <ref type="bibr" target="#b13">(He et al., 2016)</ref> were first proposed.   Indeed, it was observed that there is a striking similarity between ResNets and the numerical solution of ordinary differential equations <ref type="bibr" target="#b12">Haber &amp; Ruthotto, 2017;</ref><ref type="bibr" target="#b27">Ruthotto &amp; Haber, 2018;</ref><ref type="bibr" target="#b3">Chen et al., 2018;</ref>. In these works, deep networks are interepreted as discretizations of an underlying dynamical system, where time indexes the "depth" of the network and the parameters of the discretized dynamics are learned. An alternate viewpoint was taken by neural ODEs <ref type="bibr" target="#b3">(Chen et al., 2018)</ref>, where the dynamics of the neural network are approximated by an adaptive ODE solver on the fly. This latter approach is quite compelling as it does not require specifying the number of layers of the network beforehand. Furthermore, it allows the learning of homeomorphisms without any structural constraints on the function computed by the residual block.</p><p>Neural ODEs have shown great promise in the physical sciences <ref type="bibr" target="#b20">(K?hler et al., 2019)</ref>, in modeling irregular time series <ref type="bibr" target="#b26">(Rubanova et al., 2019)</ref>, mean field games <ref type="bibr" target="#b28">(Ruthotto et al., 2019)</ref>, continuous-time modeling <ref type="bibr" target="#b35">(Yildiz et al., 2019;</ref><ref type="bibr" target="#b16">Kanaa et al., 2019)</ref>, and for generative modeling through normalizing flows with free-form Jacobians .</p><p>arXiv:2002.02798v3 [stat.ML] 23 Jun 2020</p><p>Recent work has even adapted neural ODEs to the stochastic setting <ref type="bibr" target="#b23">(Li et al., 2020)</ref>. Despite these successes, some hurdles still remain. In particular, although neural ODEs are memory efficient, they can take a prohibitively long time to train, which is arguably one of the main stumbling blocks towards their widespread adoption.</p><p>In this work we reduce the training time of neural ODEs by regularizing the learned dynamics, complementing other recent approaches to this end such as augmented neural ODEs <ref type="bibr" target="#b8">(Dupont et al., 2019)</ref>. Without further constraints on their dynamics, high dimensional neural ODEs may learn dynamics which minimize an objective function, but which generate irregular solution trajectories. See for example <ref type="figure" target="#fig_1">Figure 1b</ref>, where an unregularized flow exhibits undesirable properties due to unnecessarily fluctuating dynamics. As a solution, we propose two theoretically motivated regularization terms arising from an optimal transport viewpoint of the learned map, which encourage well-behaved dynamics (see 1a left). We empirically demonstrate that proper regularization leads to significant speed-up in training time without loss in performance, thus bringing neural ODEs closer to deployment on large-scale datasets. Our methods are validated on the problem of generative modelling and density estimation, as an example of where neural ODEs have shown impressive results, but could easily be applied elsewhere.</p><p>In summary, our proposed regularized neural ODE (RN-ODE) achieves the same performance as the baseline, while reducing the wall-clock training time by many hours or even days.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Neural ODEs &amp; Continuous normalizing flows</head><p>Neural ODEs simplify the design of deep neural networks by formulating the forward pass of a deep network as the solution of a ordinary differential equation. Initial work along these lines was motivated by the similarity of the evaluation of one layer of a ResNet and the Euler discretization of an ODE. Suppose the block in the t-th layer of a ResNet is given by the function f (x, t; ?), where ? are the block's parameters. Then the evaluation of this layer of the ResNet is simply</p><formula xml:id="formula_0">x t+1 = x t + f (x t , t; ?). Now, instead consider the following ODE ? = f (z, t; ?) z(0) = x (ODE)</formula><p>The Euler discretization of this ODE with step-size ? is z t+1 = z t + ? f (z t , t; ?), which is nearly identical to the forward evaluation of the ResNet's layer (setting step-size ? = 1 gives equality). Armed with this insight, <ref type="bibr" target="#b3">Chen et al. (2018)</ref> suggested a method for training neural networks based on (ODE) which abstain from a priori fixing step-size. Chen et al.'s method is a continuous-time generalization of residual networks, where the dynamics are generated by an adaptive ODE solver that chooses step-size on-the-fly.</p><p>Because of their adaptive nature, neural ODEs can be more flexible than ResNets in certain scenarios, such as when trading between model speed and accuracy. Moreover given a fixed network depth, the memory footprint of neural ODEs is orders of magnitude smaller than a standard ResNet during training. They therefore show great potential on a host of applications, including generative modeling and density estimation. An apparent drawback of neural ODEs is their long training time: although a learned function f (? ; ?) may generate a map that solves a problem particularly well, the computational cost of numerically integrating (ODE) may be so prohibitive that it is not tractable in practice. In this paper we demonstrate this need not be so: with proper regularization, it is possible to learn f (? ; ?) so that (ODE) is easily and quickly solved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">FFJORD</head><p>In density estimation and generative modeling, we wish to estimate an unknown data distribution p(x) from which we have drawn N samples. Maximum likelihood seeks to approximate p(x) with a parameterized distribution p ? (x) by minimizing the Kullback-Leibler divergence between the two, or equivalently minimizing</p><formula xml:id="formula_1">J(p ? ) = ? 1 N N i=1 log p ? (x i )<label>(1)</label></formula><p>Continuous normalizing flows <ref type="bibr" target="#b3">Chen et al., 2018)</ref> parameterize p ? (x) using a vector field f :</p><formula xml:id="formula_2">R d ? R ? R d as follows.</formula><p>Let z(x, T ) be the solution map given by running the dynamics (ODE) for fixed time T . Suppose we are given a known distribution q at final time T , such as the normal distribution. Change of variables tells us that the distribution p ? (x) may be evaluated through</p><formula xml:id="formula_3">log p ? (x) = log q (z(x, T )) + log det | ? z(x, T )| (2)</formula><p>Evaluating the log determinant of the Jacobian is difficult. <ref type="bibr" target="#b11">Grathwohl et al. (2019)</ref> exploit the following identity from fluid mechanics <ref type="bibr">(Villani, 2003, p 114</ref>)</p><formula xml:id="formula_4">? ?t log det | ? z(x, t)| = div (f ) (z(x, t), t))<label>(3)</label></formula><p>where div (?) is the divergence operator 1 , div (f ) (x) = i ? xi f i (x). By the fundamental theorem of calculus, we may then rewrite (2) in integral form</p><formula xml:id="formula_5">log p ? (x) = log q (z(x, T )) + T 0 div (f ) (z(x, s), s) ds</formula><p>(4) Remark 2.1 (Divergence trace estimate). In , the divergence is estimated using an unbiased Monte-Carlo trace estimate <ref type="bibr" target="#b15">(Hutchinson, 1990;</ref><ref type="bibr" target="#b0">Avron &amp; Toledo, 2011)</ref>,</p><formula xml:id="formula_6">div (f ) (x) = E ?N (0,1) T ? f (x) (5)</formula><p>By using the substitution (4), the task of maximizing loglikelihood shifts from choosing p ? to minimize (1), to learning the flow generated by a vector field f . This results in a normalizing flow with a free-form Jacobian and reversible dynamics, and was named FFJORD by Grathwohl et al..</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The need for regularity</head><p>The vector field learned through FFJORD that maximizes the log-likelihood is not unique, and raises troubling problems related to the regularity of the flow. For a simple example, refer to <ref type="figure" target="#fig_1">Figure 1</ref>, where we plot two normalizing flows, both mapping a toy one-dimensional distribution to the unit Gaussian, and where both maximize the loglikelihood of exactly the same sample of particles. <ref type="figure" target="#fig_1">Figure  1a</ref> presents a "regular" flow, where particles travel in straight lines that travel with constant speed. In contrast, <ref type="figure" target="#fig_1">Figure 1b</ref> shows a flow that still maximizes the log-likelihood, but that has undesirable properties, such as rapidly varying local trajectories and non-constant speed.</p><p>From this simple motivating example, the need for regularity of the vector field is apparent. Without placing demands on the vector field f , it is entirely possible that the learned dynamics will be poorly conditioned. This is not just a theoretical exercise: because the dynamics must be solved with a numerical integrator, poorly conditioned dynamics will lead to difficulties during numerical integration of (ODE). Indeed, later we present results demonstrating a clear correlation between the number of time steps an adaptive solver takes to solve (ODE), and the regularity of f .</p><p>How can the regularity of the vector field be measured? One motivating approach is to measure the force experienced by a particle z(t) under the dynamics generated by the vector field f , which is given by the total derivative of f with respect to time</p><formula xml:id="formula_7">df (z, t) dt = ? f (z, t) ?? + ?f (z, t) ?t (6) = ? f (z, t) ? f (z, t) + ?f (z, t) ?t<label>(7)</label></formula><p>Well conditioned flows will place constant, or nearly constant, force on particles as they travel. Thus, in this work we propose regularizing the dynamics with two penalty terms, one term regularizing f and the other ? f . The first penalty, presented in Section 3, is a measure of the distance travelled under the flow f , and can alternately be interpreted as the kinetic energy of the flow. This penalty term is based off of numerical methods in optimal transport, and encourages particles to travel in straight lines with constant speed. The second penalty term, discussed in Section 4, performs regularization on the Jacobian of the vector field. Taken together the two terms ensure that the force experienced by a particle under the flow is constant or nearly so.</p><p>These two regularizers will promote dynamics that follow numerically easy-to-integrate paths, thus greatly speeding up training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Optimal transport maps &amp; Benamou-Brenier</head><p>There is a remarkable similarity between density estimation using continuous time normalizing flows, and the calculation of the optimal transport map between two densities using the Benamou-Brenier formulation <ref type="bibr" target="#b2">(Benamou &amp; Brenier, 2000;</ref><ref type="bibr" target="#b29">Santambrogio, 2015)</ref>. While a review of optimal transport theory is far outside the scope of this paper, here we provide an informal summary of key ideas relevant to continuous normalizing flows. The quadratic-cost optimal transport map between two densities p(x) and q(x) is a map z :</p><formula xml:id="formula_8">R d ? R d minimizing the transport cost M (z) = x ? z(x) 2 p(x) dx (8) subject to the constraint that A q(z) dz = z ?1 (A) p(x) dx,</formula><p>in other words that the measure of any set A is preserved under the map z. In a seminal work, <ref type="bibr" target="#b2">Benamou &amp; Brenier (2000)</ref> showed that rather than solving for minimizers of (8) directly, an indirect (but computationally efficient) method is available by writing z(x, T ) as the solution map of a flow under a vector field f (as in (ODE)) for time T , by minimizing</p><formula xml:id="formula_9">min f ,? T 0 f (x, t) 2 ? t (x) dxdt (9a) subject to ?? t ?t = ? div (? t f ) ,<label>(9b)</label></formula><formula xml:id="formula_10">? 0 (x) = p, (9c) ? T (z) = q.<label>(9d)</label></formula><p>The objective function <ref type="formula" target="#formula_1">(18a)</ref> is a measure of the kinetic energy of the flow. The constraint (18b) ensures probability mass is conserved. The latter two constraints guarantee the learned distribution agrees with the source p and target q. Note that the kinetic energy <ref type="formula" target="#formula_1">(18a)</ref> is an upper bound on the transport cost, with equality only at optimality.</p><p>The optimal flow f minimizing (18) has several particularly appealing properties. First, particles induced by the optimal flow f travel in straight lines. Second, particles travel with constant speed. Moreover, under suitable conditions on the source and target distributions, the optimal solution map is unique <ref type="bibr" target="#b33">(Villani, 2008)</ref>. Therefore the solution map z(x, t) is entirely characterized by the initial and final posi-</p><formula xml:id="formula_11">tions: z(x, t) = (1 ? t T )z(x, 0) + t T z(x, T ).</formula><p>Consequently, given an optimal f it is extraordinarily easy to solve (ODE) numerically with minimal computational effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Linking normalizing flows to optimal transport</head><p>Now suppose we wish to minimize (18a), with q(z) a unit normal distribution, and p(x) a data distribution, unknown to us, but from which we have drawn N samples, and which we model as a discrete distribution of Dirac masses. Enforcing the initial condition is trivial because we have sampled from p directly. The continuity equation <ref type="formula" target="#formula_1">(18b)</ref> need not be enforced because we are tracking a finite number of sampled particles. However the final time condition ? T = q cannot be implemented directly, since we do not have direct control on the form ? T (z) takes. Instead, introduce a Kullback-Leibler term to (18a) penalizing discrepancy between ? T and q. This penalty term has an elegant simplification when p(x) is modeled as a distribution of a finite number of masses, as is done in generative modeling. Setting ? 0 = p ? a brief derivation yields</p><formula xml:id="formula_12">KL(? T ||q) = ? 1 N N i=1 log p ? (x i )<label>(10)</label></formula><p>With this simplification <ref type="formula" target="#formula_1">(18a)</ref> becomes</p><formula xml:id="formula_13">J ? (f ) = ? N N i=1 T 0 f (z i , t) 2 dt ? 1 N N i=1 log p ? (x i )<label>(11)</label></formula><p>For further details on this derivation consult the supplementary materials.</p><p>The connection between the Benamou-Brenier formulation of the optimal transport problem on a discrete set of points and continuous normalizing flows is apparent: the optimal transport problem (11) is a regularized form of the continuous normalizing flow optimization problem (1). We therefore expect that adding a kinetic energy regularization term to FFJORD will encourage solution trajectories to prefer straight lines with constant speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Unbiased Frobenius norm regularization of the Jacobian</head><p>Algorithm 1 RNODE: regularized neural ODE training of FFJORD Input: data X = {x i }, i = 1, ? ? ? , N , dynamics f (? ; ?), final time T , regularization strength ? J and ? K initialize ? while ? not converged do Sample from standard normal distribution Sample minibatch {x j } of size m from X Set z j (0) = x j , l j (0) = E j (0) = n j = 0 Numerically solve up to time T the system</p><formula xml:id="formula_14">? ? ? ? ? ? ? ? ?? j = f (z j , t; ?) l j = T ? f (z j , t; ?) ? j = f (z j , t; ?) 2 n j = T ? f (z j , t; ?) 2 Compute L(?) = 1 m m j=1 ? log q(z j (T )) ? l j (T ) + ? J n j (T ) + ? K E j (T )</formula><p>Compute ? ? L(?) using the adjoint sensitivity method by numerically solving the adjoint equations Update ? ? ? ? ? ? ? L(?) end while Moreover, in particle-based methods, the kinetic energy term forces dynamics to travel in straight lines only on data seen during training, and so the regularity of the map is only guaranteed on trajectories taken by training data. The issue here is one of generalization: the map may be irregular on off-distribution or perturbed images, and cannot be remedied by the kinetic energy term during training alone. In the context of generalization, Jacobian regularization is analagous to gradient regularization, which has been shown to improve generalization <ref type="bibr" target="#b7">(Drucker &amp; LeCun, 1992;</ref><ref type="bibr" target="#b24">Novak et al., 2018)</ref>.</p><p>For these reasons, we also propose regularizing the Jacobian through its Frobenius norm. The Frobenius norm ? F of a real matrix A can be thought of as the 2 norm of the matrix A vectorized</p><formula xml:id="formula_15">A F = i,j a 2 ij<label>(12)</label></formula><p>Equivalently it may be computed as</p><formula xml:id="formula_16">A F = tr(AA T )<label>(13)</label></formula><p>and is the Euclidean norm of the singular values of a matrix. In trace form, the Frobenius norm lends itself to estimation using a Monte-Carlo trace estimator <ref type="bibr" target="#b15">(Hutchinson, 1990</ref>; Avron &amp; Toledo, 2011). For real matrix B, an unbiased estimate of the trace is given by</p><formula xml:id="formula_17">tr(B) = E ?N (0,1) T B<label>(14)</label></formula><p>where is drawn from a unit normal distribution. Thus the squared Frobenius norm can be easily estimated by setting B = AA T .</p><p>Turning to the Jacobian ? f (z) of a vector valued function f : R d ? R d , recall that the vector-Jacobian product T ? f (z) may be quickly computed through reverse-mode automatic differentiation. Therefore an unbiased Monte-Carlo estimate of the Frobenius norm of the Jacobian is readily available</p><formula xml:id="formula_18">? f (z) 2 F = E ?N (0,1) T ? f (z) ? f (z) T (15) = E ?N (0,1) T ? f (z) 2<label>(16)</label></formula><p>Conveniently, in the FFJORD framework the quantity T ? f (z) must be computed during the estimate of the probability distribution under the flow, in the Monte-Carlo estimate of the divergence term (5). Thus Jacobian Frobenius norm regularization is available with essentially no extra computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Algorithm description</head><p>All together, we propose modifying the objective function of the FFJORD continuous normalizing flow  with the two regularization penalties of Sections 3 &amp; 4. The proposed method is called RNODE, short for regularized neural ODE. Pseudo-code of the method is <ref type="table">Table 1</ref>. Log-likelihood (in bits/dim) and training time (in hours) on validation images with uniform dequantization. Results on clean images are found in the supplemental materials. For comparison we report both the results of the original FFJORD paper  and our own independent run of FFJORD ("vanilla") on CIFAR10 and MNIST. Vanilla FFJORD did not train on ImageNet64 (denoted by "x"). Also reported are results for other flow-based generative modeling papers. Our method (FFJORD with RNODE) has comparable log-likelihood as FFJORD but is significantly faster.  presented in Algorithm 1. The optimization problem to be solved is</p><formula xml:id="formula_19">min f 1 N d N i=1 ? log q(z(x i , T )) ? T 0 div (f ) (z(x i , s), s) ds +? K T 0 f (z(x i , s), s) 2 ds +? J T 0 ? z f (z(x i , s), s) 2 F ds<label>(17)</label></formula><p>where z(x, t) is determined by numerically solving (ODE). Note that we take the mean over number of samples and input dimension. This is to ensure that the choice of regularization strength ? K and ? J is independent of dimension size and sample size.</p><p>To compute the three integrals and the log-probability under q of z(x, T ) at final time T , we augment the dynamics of the ODE with three extra terms, so that the entire system solved by the numerical integrator is</p><formula xml:id="formula_20">? ? ? ? ? ? ? ? ? ? ? ? ? ? ?? = f (z, t) l = div (f ) (z, t) E = f (z, t) 2 n = ? f (z, t) 2 F z(0) = x, E(0) = l(0) = n(0) = 0 (RNODE)</formula><p>Here E, l, and n are respectively the kinetic energy, the log determinant of the Jacobian, and the integral of the Frobenius norm of the Jacobian.</p><p>Both the divergence term and the Jacobian Frobenius norm are approximated with Monte-Carlo trace estimates. In our implementation, the Jacobian Frobenius estamate reuses the computatian T ? f from the divergence estimate for efficiency. We remark that the kinetic energy term only requires the computation of a dot product. Thus just as in FFJORD, our implementation scales linearly with the number of time steps taken by the ODE solver.</p><p>Gradients of the objective function with respect to the network parameters are computed using the adjoint sensitivity method <ref type="bibr" target="#b25">(Pontryagin et al., 1962;</ref><ref type="bibr" target="#b3">Chen et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental design</head><p>Here we demonstrate the benefits of regularizing neural ODEs on generative models, an application where neural ODEs have shown strong empirical performance. We use four datasets: CIFAR10 <ref type="bibr" target="#b21">(Krizhevsky &amp; Hinton, 2009</ref><ref type="bibr">), MNIST (LeCun &amp; Cortes, 1998</ref>, downsampled ImageNet (64x64) (van den <ref type="bibr" target="#b31">Oord et al., 2016)</ref>, and 5bit CelebA-HQ (256x256) <ref type="bibr" target="#b17">(Karras et al., 2017)</ref>. We use an identical neural architecture to that of <ref type="bibr" target="#b11">Grathwohl et al. (2019)</ref>. The dynamics On MNIST and CIFAR10 we train with a batch size of 200 and train for 100 epochs on a single GPU 3 , using the Adam optimizer <ref type="bibr" target="#b18">(Kingma &amp; Ba, 2015)</ref> with a learning rate of 1e?3. On the two larger datasets, we train with four GPUs, using a per-GPU batch size of respectively 3 and 50 for CelebA-HQ and ImageNet. Data is preprocessed by perturbing with uniform noise followed by the logit transform.</p><p>The reference implementation of FFJORD solves the dynamics using a Runge-Kutta 4(5) adaptive solver <ref type="bibr" target="#b6">(Dormand &amp; Prince, 1980)</ref> with error tolerances 1e?5 and initial step size 1e?2. We have found that using less accurate solvers on the reference implementation of FFJORD results in numerically unstable training dynamics. In contrast, a simple fixed-grid four stage Runge-Kutta solver suffices for RN-ODE during training on MNIST and CIFAR10, using a step size of 0.25. The step size was determined based on a simple heuristic of starting with 0.5 and decreasing the 3 GeForce RTX 2080 Ti step size by a factor of two until the discrete dynamics were stable and achieved good performance. The Runge-Kutta 4(5) adaptive solver was used on the two larger datasets. We have also observed that RNODE improves the training time of the adaptive solvers as well, requiring many fewer function evaluations; however in Python we have found that the fixed grid solver is typically quicker at a specified number of function evaluations. At test time RNODE uses the same adaptive solver as FFJORD.</p><p>We always initialize RNODE so that f (z, t) = 0; thus training begins with an initial identity map. This is done by zeroing the parameters of the last layer in each piece (block), following <ref type="bibr" target="#b10">Goyal et al. (2017)</ref>. The identity map is an appropriate choice because it has zero transport cost and zero Frobenius norm. Moreover the identity map is trivially solveable for any numerical solver, thus training begins without any effort required on the solver's behalf.</p><p>On all datasets we set both the kinetic energy regularization coefficient ? K and the Jacobian norm coefficient ? J to 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Results</head><p>A comparison of RNODE against FFJORD and other flowbased generative models is presented in <ref type="table">Table 1</ref>. We report both our running of "vanilla" FFJORD and the results as originally reported in . We highlight that RNODE runs roughly 2.8x faster than FFJORD on both datasets, while achieving or surpassing the performance of FFJORD. This can further be seen in <ref type="figure" target="#fig_2">Figure 2</ref> where we plot bits per dimension ( ? 1 d log 2 p(x), a normalized measure of log-likelihood) on the validation set as a function of training epoch, for both datasets. Visual inspection of the sample quality reveals no qualitative difference between regularized and unregularized approaches; refer to <ref type="figure" target="#fig_6">Figure 6</ref>. Generated images for downsampled ImageNet and CelebA-HQ are deferred to the supplementary materials; we provide smaller generated images for networks trained on CelebA-HQ 64x64 in <ref type="figure" target="#fig_4">Figure 4</ref>.</p><p>Surprisingly, our run of "vanilla" FFJORD achieved slightly better performance than the results reported in . We suspect the discrepancy in performance and run times between our implementation of FFJORD and that of the original paper is due to batch size: Grathwohl et al. use a batch size of 900 and train on six GPUs, whereas on MNIST and CIFAR10 we use a batch size of 200 and train on a single GPU.</p><p>We were not able to train vanilla FFJORD on ImageNet64, due to numerical underflow in the adaptive solver's time step. This issue cannot be remedied by increasing the solver's error tolerance, for this would bias the log-likelihood estimates on validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Ablation study on MNIST</head><p>In <ref type="figure" target="#fig_5">Figure 5</ref>, we compare the effect of each regularizer by itself on the training dynamics with the fixed grid ODE solver on the MNIST dataset. Without any regularization at all, training dynamics are numerically unstable and fail after just under 50 epochs. This is precisely when the Jacobian norm grows large; refer to <ref type="figure" target="#fig_5">Figure 5a</ref>. <ref type="figure" target="#fig_5">Figure 5a</ref> demonstrates that each regularizer by itself is able to control the Jacobian norm. The Jacobian regularizer is better suited to this task, although it is interesting that the kinetic energy regularizer also improves the Jacobian norm. Unsurprisingly <ref type="figure" target="#fig_5">Figure 5b</ref> demonstrates the addition of the kinetic energy regularizer encourages flows to travel a minimal distance. In addition, we see that the Jacobian norm alone also has a beneficial effect on the distance particles travel. Overall, the results support our theoretical reasoning empirically.</p><p>8. Previous generative flows inspired by optimal transport <ref type="bibr" target="#b36">Zhang et al. (2018)</ref> define a neural ODE flow where the dynamics are given as the gradient of a scalar potential function. This interpretation has deep connections to optimal transport: the optimal transport map is the gradient of a convex potential function. Yang &amp; Karniadakis (2019) continue along these lines, and define an optimal transport again as a scalar potential gradient. <ref type="bibr" target="#b34">Yang &amp; Karniadakis (2019)</ref> enforce that the learned map is in fact an optimal transport map by penalizing their objective function with a term measuring violations of the continuity equation. <ref type="bibr" target="#b28">Ruthotto et al. (2019)</ref> place generative flows within a broader context of mean field games, and as an example consider a neural ODE gradient potential flow solving the optimal transport problem in up to 100 dimensions. We also note the recent work of <ref type="bibr" target="#b30">Twomey et al. (2019)</ref>, who proposed regularizing neural ODEs with an Euler-step discretization of the kinetic energy term to enforce 'straightness', although connections to optimal transport were not discussed.</p><p>When a flow is the gradient of a scalar potential, the change of variables formula (4) simplifies so that the divergence term is replaced by the Laplacian of the scalar potential. Although mathematically parsimonious and theoretically well-motivated, we chose not to implement our flow as the gradient of a scalar potential function due to computational constraints: such an implementation would require 'triple backprop' (twice to compute or approximate the Laplacian, and once more for the parameter gradient). <ref type="bibr" target="#b28">Ruthotto et al. (2019)</ref> circumvented this problem by utilizing special structural properties of residual networks to efficiently compute the Laplacian.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Discussion</head><p>In practice, RNODE is simple to implement, and only requires augmenting the dynamics (ODE) with two extra scalar equations (one for the kinetic energy term, and another for the Jacobian penalty). In the setting of FFJORD, because we may recycle intermediary terms used in the divergence estimate, the computational cost of evaluating these two extra equations is minimal. RNODE introduces two extra hyperparameters related to the strength of the regularizers; we have found these required almost no tuning.</p><p>Although the problem of classification was not considered in this work, we believe RNODE may offer similar improvements both in training time and the regularity of the classifier learned. In the classification setting we expect the computional overhead of calculating the two extra terms should be marginal relative to gains made in training time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.">Conclusion</head><p>We have presented RNODE, a regularized method for neural ODEs. This regularization approach is theoretically well-motivated, and encourages neural ODEs to learn wellbehaved dynamics. As a consequence, numerical integration of the learned dynamics is straight forward and relatively easy, which means fewer discretizations are needed to solve the dynamics. In many circumstances, this allows for the replacement of adaptive solvers with fixed grid solvers, which can be more efficient during training. This leads to a substantial speed up in training time, while still maintaining the same empirical performance, opening the use of neural ODEs to large-scale applications.</p><p>f (z(x, t), t) 2 dt ? log p ? (x)</p><p>Finally, if we assume that {x i } N i=1 are iid sampled from p, we obtain the empirical objective function</p><formula xml:id="formula_22">? N N i=1 T 0 f (z(x i , t), t) 2 dt ? 1 N N i=1 log p ? (x i ) (22)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Additional results</head><p>Here we present additional generated samples on the two larger datasets considered, CelebA-HQ and ImageNet64. In addition bits/dim on clean images are reported in <ref type="table" target="#tab_2">Table 2</ref>. . Quality of FFJORD RNODE generated images on CelebA-HQ. We use temperature annealing, as described in <ref type="bibr" target="#b19">(Kingma &amp; Dhariwal, 2018)</ref>, to generate visually appealing images, with T = 0.5, . . . , 1. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Optimal transport map and a generic normalizing flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Log-likelihood (measured in bits/dim) on the validation set as a function of wall-clock time. Rolling average of three hours, with 90% confidence intervals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Number of function evaluations vs Jacobian Frobenius norm of flows on CIFAR10 during training with vanilla FFJORD, using an adaptive ODE solver.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Quality of generated samples samples on 5bit CelebA-HQ64 with RNODE. Here temperature annealing<ref type="bibr" target="#b19">(Kingma &amp; Dhariwal, 2018)</ref> with T = 0.7 was used to generate visually appealing images. For full sized CelebA-HQ256 samples, consult the supplementary materials.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Ablation study of the effect of the two regularizers, comparing two measures of flow regularity during training with a fixed step-size ODE solver. Figure 5a: mean Jacobian Frobenius norm as a function of training epoch. Figure 5b: mean kinetic energy of the flow as a function of training epoch. Figure 5c: number of function evaluations.are defined by a neural network f (z, t; ?(t)) :R d ? R + ? R d where ?(t) is piecewise constant in time.On MNIST we use 10 pieces; CIFAR10 uses 14; downsampled ImageNet uses 18; and CelebA-HQ uses 26 pieces. Each piece is a 4-layer deep convolutional network comprised of 3x3 kernels and softplus activation functions. Intermediary layers have 64 hidden dimensions, and time t is concatenated to the spatial input z. The integration time of each piece is [0, 1]. Weight matrices are chosen to imitate the multi-scale architecture of Real NVP<ref type="bibr" target="#b5">(Dinh et al., 2017)</ref>, in that images are 'squeezed' via a permutation to halve image height and width but quadruple the number of channels. Divergence of f is estimated using the Gaussian Monte-Carlo trace estimator with one sample of fixed noise per solver time-step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Quality of generated samples samples with and without regularization on MNIST, left, and CIFAR10, right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Quality of FFJORD RNODE generated images on ImageNet-64.(a) real (b) T = 0.5 (c) T = 0.6 (d) T = 0.7 (e) T = 0.8 (f) T = 0.9 (g) T = 1 Figure 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Department of Mathematics &amp; Statistics, McGill University, Montr?al, Qu?bec, Canada 2 Vector Institute, University of Toronto, Toronto, Ontario, Canada 3 Department of Mathematics, UCLA, California, USA. Correspondence to: Chris Finlay &lt;christopher.finlay@mcgill.ca&gt;. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s).</figDesc><table /><note>1</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Additional results and model statistics of FFJORD RNODE. Here we report validation bits/dim on both validation images, and on validation images with uniform variational dequantization (ie perturbed by uniform noise). We also report number of trainable model parameters.</figDesc><table><row><cell>DATASET</cell><cell cols="3">BITS/DIM (CLEAN) BITS/DIM (DIRTY) # PARAMETERS</cell></row><row><cell>MNIST</cell><cell>0.92</cell><cell>0.97</cell><cell>8.00e5</cell></row><row><cell>CIFAR10</cell><cell>3.25</cell><cell>3.38</cell><cell>1.36e6</cell></row><row><cell>IMAGENET64</cell><cell>3.72</cell><cell>3.83</cell><cell>2.00e6</cell></row><row><cell cols="2">CELEBA-HQ256 0.72</cell><cell>1.04</cell><cell>4.61e6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In the normalizing flow literature divergence is typically written explicitly as the trace of the Jacobian, however we use div (?) which is more common elsewhere.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Refering to equation(7), one can see that even if f is regularized to be small, via a kinetic energy penalty term, if the Jacobian is large then the force experienced by a particle may also still be large. As a result, the error of the numerical integrator can be large, which may lead an adaptive solver to make many function evaluations. This relationship is apparent inFigure 3, where we empirically demonstrate the correlation between the number of function evaluations of f taken by the adaptive solver, and the size of the Jacobian norm of f . The correlation is remarkably strong: dynamics governed by a poorly conditioned Jacobian matrix require the adaptive solver to take many small time steps.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2"><ref type="bibr" target="#b19">(Kingma &amp; Dhariwal, 2018)</ref> trained with 40 GPUs for a week; in contrast we train with four GPUs in just under a week.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Resources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, and companies sponsoring the Vector Institute (www.vectorinstitute.ai/#partners).</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A. Details of Section 3.1: <ref type="bibr">Benamou-Brenier</ref> formulation in Lagrangian coordinates</p><p>The Benamou-Brenier formulation of the optimal transportation (OT) problem in Eulerian coordinates is</p><p>The connection between continuous normalizing flows (CNF) and OT becomes transparent once we rewrite (18) in Lagrangian coordinates. Indeed, for regular enough velocity fields f one has that the solution of the continuity equation</p><p>The relation ? t = z(?, t) p means that for arbitrary test function ? we have that</p><p>Therefore <ref type="formula">(18)</ref> can be rewritten as</p><p>Note that ? t is eliminated in this formulation. The terminal condition (18d) is trivial to implement in Eulerian coordinates (grid-based methods) but not so simple in Lagrangian ones (19d) (grid-free methods). To enforce (19d) we introduce a penalty term in the objective function that measures the deviation of z(?, T ) p from q. Thus, the penalized objective function is</p><p>where ? &gt; 0 is the penalization strength. Next, we observe that this objective function can be written as an expectation with respect to x ? p. Indeed, the Kullback-Leibler divergence is invariant under coordinate transformations, and therefore</p><p>Hence, multiplying the objective function in (20) by ? and ignoring the f -independent term E x?p log p(x) we obtain an equivalent objective function E x?p ? T 0</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Randomized algorithms for estimating the trace of an implicit symmetric positive semidefinite matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Avron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Toledo</surname></persName>
		</author>
		<idno type="DOI">10.1145/1944345.1944349</idno>
		<ptr target="https://doi.org/10.1145/1944345.1944349" />
	</analytic>
	<monogr>
		<title level="j">J. ACM</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Invertible residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jacobsen</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v97/behrmann19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML 2019</title>
		<editor>Chaudhuri, K. and Salakhutdinov, R.</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML 2019<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-15" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="573" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A computational fluid mechanics solution to the Monge-Kantorovich mass transfer problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-D</forename><surname>Benamou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Brenier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerische Mathematik</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="375" to="393" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neural Ordinary Differential Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="6572" to="6583" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Residual flows for invertible generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Behrmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">; H M</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/9183-residual-flows-for-invertible-generative-modeling" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>R.</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
			<biblScope unit="page" from="9913" to="9923" />
		</imprint>
	</monogr>
	<note>Wallach,</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Density estimation using real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HkpbnH9lx" />
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24" />
		</imprint>
	</monogr>
	<note type="report_type">Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A family of embedded Runge-Kutta formulae</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Dormand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Prince</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational and applied mathematics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="26" />
			<date type="published" when="1980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improving generalization performance using double backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="DOI">10.1109/72.165600</idno>
		<idno>doi: 10.1109/ 72.165600</idno>
		<ptr target="https://doi.org/10.1109/72.165600" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="991" to="997" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Augmented neural ODEs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Dupont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W. ; H M</forename><surname>Teh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gar</forename></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8577-augmented-neural-odes" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>nett, R.</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
			<biblScope unit="page" from="3134" to="3144" />
		</imprint>
	</monogr>
	<note>Wallach,</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Proposal on Machine Learning via Dynamical Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<idno type="DOI">10.1007/s40304-017-0103-z</idno>
		<ptr target="https://doi.org/10.1007/s40304-017-0103-z" />
	</analytic>
	<monogr>
		<title level="j">Communications in Mathematics and Statistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2017-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Accurate, large minibatch SGD: training imagenet in 1 hour. CoRR, abs/1706.02677</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1706.02677" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">FFJORD: free-form continuous dynamics for scalable reversible generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=rJxgknCcK7" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Stable architectures for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Inverse Problems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">14004</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving flow-based generative models with variational dequantization and architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Flow++</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v97/ho19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning, ICML</title>
		<editor>Chaudhuri, K. and Salakhutdinov, R.</editor>
		<meeting>the 36th International Conference on Machine Learning, ICML<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A stochastic estimator of the trace of the influence matrix for Laplacian smoothing splines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Hutchinson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics-Simulation and Computation</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="433" to="450" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simple video generation using neural ODEs. Workshop on Learning with Rich Experience</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kanaa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Voleti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation. CoRR, abs/1710.10196</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1710.10196" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Glow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/8224-glow-generative-flow-with-invertible-1x1-convolutions" />
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<editor>R.</editor>
		<meeting><address><addrLine>NeurIPS; Montr?al, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-12-08" />
			<biblScope unit="page" from="10236" to="10245" />
		</imprint>
	</monogr>
	<note>Generative flow with invertible 1x1 convolutions</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Equivariant flows: sampling configurations for multi-body systems with symmetric energies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>K?hler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>No?</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.00753</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="http://www.cs.toronto.edu/?kriz/cifar.html" />
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<ptr target="http://yann.lecun.com/exdb/mnist/" />
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Scalable gradients for stochastic differential equations. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2001.01328" />
		<imprint>
			<date type="published" when="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sensitivity and generalization in neural networks: an empirical study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Novak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Abolafia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJC2SzZCW" />
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018</title>
		<meeting><address><addrLine>BC, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Vancouver</publisher>
			<date type="published" when="2018-05-03" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The mathematical theory of optimal processes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Pontryagin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mishchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Boltyanskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gamkrelidze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Latent ordinary differential equations for irregularly-sampled time series</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5321" to="5331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep neural networks motivated by partial differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Haber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A machine learning framework for solving high-dimensional mean field game and mean field control problems. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ruthotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Osher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nurbekyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Fung</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1912.01825" />
		<imprint>
			<date type="published" when="1825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Santambrogio</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-319-20828-2_6</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-20828-26" />
		<title level="m">Benamou-Brenier and other continuous numerical methods</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="219" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Neural ODEs with stochastic vector field mixtures. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Twomey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kozlowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Santos-Rodr?guez</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1905.09905" />
		<imprint>
			<date type="published" when="1905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Pixel recurrent neural networks. CoRR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>abs/1601.06759</idno>
		<ptr target="http://arxiv.org/abs/1601.06759" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Topics in Optimal Transportation. Graduate studies in mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>American Mathematical Society</publisher>
			<biblScope unit="volume">9780821833124</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Villani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Transport</surname></persName>
		</author>
		<ptr target="https://books.google.ca/books?id=hV8o5R75tkC" />
		<title level="m">Old and New. Grundlehren der mathematischen Wissenschaften. Springer Berlin Heidelberg</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Potential flow generator with L 2 Optimal Transport regularity for generative models. CoRR, abs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1908.11462" />
		<imprint>
			<date type="published" when="1908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">ODE2VAE: deep generative second order ODEs with Bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yildiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heinonen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">; H M</forename><surname>L?hdesm?ki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Buc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
		<ptr target="http://papers.nips.cc/paper/9497-ode2vae-deep-generative-second-order-odes-with-bayesian-neural-networks" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<editor>R.</editor>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-14" />
			<biblScope unit="page" from="13412" to="13421" />
		</imprint>
	</monogr>
	<note>Wallach,</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Monge-Amp?re flow for generative modeling. CoRR, abs/1809.10188</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1809.10188" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
