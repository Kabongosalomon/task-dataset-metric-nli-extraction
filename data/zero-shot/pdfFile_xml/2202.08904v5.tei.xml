<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SGPT: GPT Sentence Embeddings for Semantic Search</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niklas</forename><surname>Muennighoff</surname></persName>
							<email>muennighoff@stu.pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SGPT: GPT Sentence Embeddings for Semantic Search</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Decoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or finetuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.</p><p>Performance Taking advantage of the available scale of decoders has the potential to produce new state-of-the-art results in search. Available encoders are orders of magnitude smaller <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">38]</ref>. Google search, for example, processes an estimated 4 billion searches daily <ref type="bibr" target="#b18">[19]</ref>, thus better search models could have wide-reaching impacts.</p><p>Compute Savings Large-scale pre-trained decoders have been reused for different tasks via prompting or fine-tuning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b8">9]</ref>. A well-performing method to extract embeddings from billion parameter decoders may prevent the need to train and maintain separate encoder and decoder models. Training just one large decoder and reusing it for search prevents additional cost to the environment [2].</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Semantic search consists of two parts: Search refers to finding the top k answers from a document corpus given a query. Semantic refers to understanding the documents and queries beyond keywords. Transformers <ref type="bibr" target="#b43">[45]</ref> are the dominant semantic architecture <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b42">44]</ref> competing with non-semantic models like BM25 <ref type="bibr" target="#b39">[41]</ref>. Search applications like Google <ref type="bibr" target="#b25">[26]</ref> or Bing <ref type="bibr" target="#b52">[54]</ref> rely on transformers to provide semantically relevant results. However, they have been limited to BERT-like encoder-only transformers <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b52">54,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b37">39,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Meanwhile, GPT-like decoder-only transformers <ref type="bibr" target="#b33">[35]</ref> have been the focus of recent scaling efforts of up to 540 billion parameters <ref type="bibr" target="#b8">[9]</ref>. Increasing language model parameters has been repeatedly shown to improve downstream zero-shot and fine-tuning performance on a variety of language tasks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b8">9]</ref>. For example, increasing scale has allowed decoder-only transformers to outperform all encoder-only and catch-up with encoder-decoder transformers on the SuperGLUE benchmark <ref type="bibr" target="#b45">[47,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>However, the related fields of semantic search and language embeddings have not been part of the proliferation of decoders. They are dominated by comparatively small encoders <ref type="bibr" target="#b42">[44]</ref>, as it remains unclear how to extract semantically meaningful embeddings from decoders and use them for semantic search. Methods to do so are desirable for two reasons: In this work, we propose SGPT to apply decoder-only transformers to semantic search and extract meaningful sentence embeddings from them. We distinguish four settings: Cross-Encoder vs Bi-Encoder, Symmetric vs Asymmetric. See <ref type="figure" target="#fig_0">Figure 1</ref> and ?2.</p><p>In the Bi-Encoder setting, we propose SGPT-BE using position-weighted mean pooling and contrastive fine-tuning of only bias tensors (BitFit <ref type="bibr" target="#b51">[53]</ref>). We show that BitFit is competitive with full fine-tuning performance for both encoders (SBERT) <ref type="bibr" target="#b37">[39]</ref> and decoders (SGPT) despite changing &lt;0.1% of pre-trained parameters. When controlling for size, our decoders closely trail the performance of encoders. When scaling up, SGPT-BE-5.8B sets state-of-the-art results on BEIR and USEB for asymmetric and symmetric search.</p><p>In the Cross-Encoder setting, we propose SGPT-CE using log probability extraction of pre-trained GPT models. The method is applicable to symmetric or asymmetric search by changing the prompt. At scale, the model sets an unsupervised state-of-the-art on BEIR.</p><p>In summary, our contributions are three-fold:</p><p>? For SGPT-BE in ?4, we develop a new pooling method and show the usefulness of bias-only fine-tuning for embeddings. At 5.8B parameters, it produces the best natural language embeddings available by a margin of 7% for the example of semantic search.</p><p>? For SGPT-CE in ?3, we show how to use GPT for search via log probabilities without fine-tuning. At 6.1B parameters, it has the best unsupervised performance on BEIR by a margin of 8%.</p><p>? We provide free, more performant alternatives to commonly used endpoints, such as Ope-nAI's Search and Similarity Embeddings and the OpenAI Search endpoint available at https://github.com/Muennighoff/sgpt.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In this section, we explain two dimensions fundamental to our work: Cross-Encoders vs Bi-Encoders and Symmetric vs Asymmetric Search. We highlight work in those areas relevant to ours.</p><p>Cross-Encoders encode query and document at the same time. BERT <ref type="bibr" target="#b9">[10]</ref> is used as a Cross-Encoder by separating the query from the document with a [SEP ] token. They are then passed through the transformer network together. Each new query requires k forward passes given a corpus of k documents. There is no existing literature on using GPT models as Cross-Encoders, however, we suspect the OpenAI Search API [32] uses a GPT-based Cross-Encoder. We include results based on querying their API, as well as a BERT-based state-of-the-art Cross-Encoder in our benchmarks.</p><p>Bi-Encoders encode query and document separately. SBERT <ref type="bibr" target="#b37">[39]</ref> extends BERT to the Bi-Encoder setting via supervised fine-tuning and a pooling operation across the sequence output. The resulting document vectors can be cached. A new query requires only one forward pass through the transformer to produce the query vector. The query vector can then be scored against the cached document vectors with a similarity function. Embeddings from Bi-Encoders can be used for non-search tasks such as clustering or as input features of machine learning models <ref type="bibr" target="#b38">[40]</ref>. While non-semantic models like keyword-based BM25 <ref type="bibr" target="#b39">[41]</ref> remain extensively used, the field increasingly focuses on neural models using transformers <ref type="bibr" target="#b43">[45]</ref>. Contriever <ref type="bibr" target="#b19">[20]</ref> shows the utility of unsupervised contrastive training for pretrained encoders. Their best model that we compare with also adds supervised fine-tuning as a third training stage. GTR <ref type="bibr" target="#b28">[29]</ref> explores the effect of scaling up encoders on semantic search tasks also in a three-stage training setup. They find that despite keeping the embedding size fixed, more parameters increase the performance of encoders. Usage of GPT models as Bi-Encoders remains limited. Rather, there has been interest in using them as generators to produce search training data for encoders <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b2">3]</ref>. Previous work has studied the differences in embeddings produced by various language models including BERT and GPT <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b5">6]</ref>. They have found GPT-2 embeddings to underperform on various word embedding benchmarks <ref type="bibr" target="#b10">[11]</ref>. Concurrently to our work, the first trained GPT-based Bi-Encoder, cpt-text, was proposed <ref type="bibr" target="#b26">[27]</ref>. They use a pre-trained decoder and employ two additional training stages, contrastive unsupervised pre-training and supervised fine-tuning. Their models are used for the OpenAI Similarity and Search Embeddings API <ref type="bibr" target="#b30">[31]</ref>. Our Bi-Encoders differ from theirs in that we simplify the training process to only fine-tuning, use a novel pooling method and only train bias parameters. cpt-text is most similar to our Bi-Encoders, hence we include their results in our benchmarks.</p><p>Cross-Encoders tend to outperform Bi-Encoders <ref type="bibr" target="#b41">[43]</ref>, but are slower as vectors cannot be cached and reused. To balance the trade-offs, multi-stage architectures have been proposed <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">34,</ref><ref type="bibr" target="#b20">21]</ref>. In a two-stage re-ranking setup, the first model processes the entire corpus and the second model is only used on the top k documents returned by the first. In ?3, we use Bi-Encoder (BM25 <ref type="bibr" target="#b39">[41]</ref>) + Cross-Encoder re-ranking.</p><p>Asymmetric Search means queries and documents are not interchangeable. Finding answers given a question is an asymmetric search problem. Commonly, documents are much longer than queries <ref type="bibr" target="#b42">[44]</ref>. We evaluate asymmetric search experiments on BEIR <ref type="bibr" target="#b42">[44]</ref>, a recently proposed benchmark consisting of 19 asymmetric search datasets.</p><p>Symmetric Search means queries and documents are interchangeable. Finding duplicate questions, where both queries and documents are questions, is a symmetric search problem. We evaluate symmetric search experiments on USEB <ref type="bibr" target="#b47">[49]</ref>, Quora from BEIR <ref type="bibr" target="#b42">[44]</ref> and STS-B <ref type="bibr" target="#b6">[7]</ref>. In Quora, queries are question titles and documents are question texts. They are often the same with average word lengths of 9.53 and 11.44, respectively <ref type="bibr" target="#b42">[44]</ref>. Hence, we consider it more of a symmetric search task. We include Quora in both symmetric and asymmetric experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SGPT Cross-Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Asymmetric Search</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Method</head><p>Given a query q, and a document corpus D, we are interested in the most likely document d * . Using Bayes' Theorem this can be expressed as:</p><formula xml:id="formula_0">d * = arg max d?D P (d|q) = arg max d?D P (q|d)P (d) P (q) = arg max d?D P (q|d)P (d)<label>(1)</label></formula><p>Note that P (q) is irrelevant as it is always the same when taking the arg max over D. Due to variable document lengths and contents it is easier to compare P (q|d) than P (d|q). We hence compute the joint probability of the query tokens q i,..,n given the document tokens embedded in a prompt P as p(q i , ..., q n |p 1 , ..., p i?1 ) ignoring P (d). As long as P (d) does not vary excessively across the corpus D, this simplification should produce reasonable scores.    <ref type="bibr" target="#b4">[5]</ref>. In brackets are numbers for cpt-text models recently provided in <ref type="bibr" target="#b26">[27]</ref>. They differ likely due to removing the language modeling head.</p><p>In practice, we use log probabilities <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b34">36]</ref>, computed via the log of the softmax of the model output.</p><p>To have a constant query length n + 1 ? i and avoid abrupt text changes, documents are truncated from the left until the input fits the model's maximum sequence length. We apply these methods to re-rank top k documents returned by BM25 <ref type="bibr" target="#b39">[41]</ref>. While re-ranking with BM25 bottlenecks performance, it speeds up experiments. It is not a necessary part of the architecture and therefore not depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>We experiment with publicly available pre-trained decoder transformers with 125M, 1.3B, 2.7B and 6.1B parameters <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b46">48]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Results</head><p>We perform a search over 12 prompts using the MSMARCO <ref type="bibr" target="#b27">[28]</ref> dataset as provided in BEIR <ref type="bibr" target="#b42">[44]</ref>.</p><p>The prompts and results are in Appendix ?B.1. We select the prompt with the best score, P G .</p><p>In <ref type="table" target="#tab_1">Table 1</ref>, we benchmark the resulting SGPT-CE (SGPT-Cross-Encoder). We compare with OpenAI's Search endpoint, which is to be distinguished from their Embeddings endpoint. Please refer to <ref type="table" target="#tab_10">Table  6</ref> in the Bi-Encoder section for a benchmark with the OpenAI Embeddings endpoint. We provide parameter estimates for the OpenAI model names in <ref type="table" target="#tab_2">Table 2</ref>. We also compare with the current stateof-the-art on BEIR <ref type="bibr" target="#b42">[44]</ref>, a BERT-based Cross-Encoder. BM25+CE consists of a pre-trained BERT model that is further fine-tuned on MS-MARCO <ref type="bibr" target="#b27">[28]</ref> in a supervised fashion <ref type="bibr" target="#b42">[44]</ref>. SGPT-CE consists solely of the pre-trained GPT model. However, SGPT-CE-6.1B has almost 15x more parameters than BM25+CE significantly increasing latency. In the Re-rank Top 100 setting, the top 100 documents as returned by BM25 are re-ranked by the respective model. While SGPT-CE-6.1B wins on more datasets than the encoder-based state-of-the-art, its average score is worse. This can be alleviated by not using the same prompt P G for all datasets. We show in ?3.2 that SGPT-CE-6.1B can beat BM25+CE on Quora by changing the prompt.</p><p>In <ref type="figure" target="#fig_1">Figure 2</ref>, we investigate how performance scales with model size. As we are in a re-ranking setup, the Cross-Encoder performance is bounded by the documents returned by BM25. We provide the BM25 bounds and additional model results in Appendix ?A. In a Re-rank Top 10 setting, the model is significantly bottlenecked by BM25. SGPT-CE-6.1B reaches around 80% of the maximum possible performance. We hence observe high jumps in performance for datasets like HotpotQA <ref type="bibr" target="#b50">[52]</ref> or TREC-COVID <ref type="bibr" target="#b44">[46]</ref> as we move to top 100. In fact, the 0.791 nDCG@10 on TREC-COVID in <ref type="table" target="#tab_1">Table 1</ref> is not possible in a Re-rank Top 10 setting as the bound is at 0.750. From the results, we infer that performance scales both as we re-rank more documents or increase model size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Symmetric Search</head><p>We use the same methods outlined in ?3.1.1, but adapt the prompt for symmetric search. We show this on the example of Quora in <ref type="table" target="#tab_4">Table 3</ref>. In ?2, we have explained why Quora is closer to symmetric search than asymmetric search. We search over several prompts on the smaller 125M parameter model and use the best one on the large model. By doing so, SGPT-CE-6.1B improves by 6% outperforming all Quora results in <ref type="table" target="#tab_1">Table 1</ref>. We hypothesize that further customizing the prompt for each dataset could significantly improve performance. However, we highlight that searching prompts for all possible input types may not be feasible in practice and is not considered true few-shot learning <ref type="bibr" target="#b31">[33]</ref>. Hence, the prompt we find for Quora may not generalize well to other symmetric search datasets, a key limitation of this method.  Like in ?3.1.1, we first experiment with decoder transformers that have only gone through unsupervised pre-training. In the Bi-Encoder setting, a pooling operation is commonly applied to the model's hidden states to reduce them to a vector whose size is irrespective of sequence length. SBERT <ref type="bibr" target="#b37">[39]</ref> showed that a MEAN pooling mechanism outperforms [CLS] and MAX strategies for a BERT encoder. Due to the causal attention mask in an auto-regressive decoder transformer, tokens do not attend to future tokens like in an encoder transformer. Hence, only the last token has attended to all tokens in a sequence. To account for this information mismatch, we propose to give later tokens a higher weight using a position-weighted mean pooling method:</p><formula xml:id="formula_1">v = S i=1 w i h i where w i = i S i=1 i<label>(2)</label></formula><p>where S is the sequence length, h i the ith hidden state and v the query or document embedding. We compare weighted mean pooling with last token pooling, where the hidden state of the final token is the embedding, and regular mean pooling.</p><p>We follow recent work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b26">27]</ref> and perform supervised contrastive learning with in-batch negatives. Given matching query-doc pairs</p><formula xml:id="formula_2">{q (i) , d (i) } M i=1</formula><p>, we optimize the cost function:</p><formula xml:id="formula_3">J CL (?) = 1 M M i=1 log exp(? ? ?(f ? (q (i) ), f ? (d (i) ))) M j=1 exp(? ? ?(f ? (q (i) ), f ? (d (j) )))<label>(3)</label></formula><p>where f ? is the SGPT model outputting a fixed-size vector, ? cosine similarity and ? a temperature parameter set to 20 in our experiments. We use GradCache <ref type="bibr" target="#b13">[14]</ref> to train with large batch sizes in a limited memory setting. We train on SNLI <ref type="bibr" target="#b3">[4]</ref> and MNLI <ref type="bibr" target="#b49">[51]</ref>. We limit the model sequence length to 75 tokens during both training and inference.</p><p>We fine-tune only bias parameters and freeze the rest of the model. This has been recently proposed as BitFit <ref type="bibr" target="#b51">[53]</ref> for BERT encoders. It has been shown to be competitive with full fine-tuning in various scenarios <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b48">50,</ref><ref type="bibr" target="#b23">24]</ref>. <ref type="table">Table 4</ref> shows the number of parameters trained for BitFit models. Due to fewer gradient updates, BitFit significantly reduces GPU memory and time required per step. Further, adding a BitFit checkpoint to an instance with an existing full model will only require storing the different biases. An instance already serving a 22.5GB fp32 GPT-J-6B model requires an additional 22MB of storage to serve an SGPT-5.8B-bitfit model.  <ref type="table">Table 4</ref>: SGPT parameter overview. Due to the removal of the final language modeling head SGPT-BE-5.8B has 206M parameters less than SGPT-CE-6.1B or GPT-J-6.1B. GPT-Neo models tie the language modeling head weights with the input embeddings, hence there is no parameter difference. <ref type="figure">Figure 3</ref>: Performance on USEB <ref type="bibr" target="#b47">[49]</ref> by taking the embeddings from certain layers. S models are fine-tuned on the same data with the same hyperparameters. Dashed, solid and dotted lines are last token, mean and weighted mean pooling, respectively. Shades of red are transformer encoders, while shades of blue are decoders. The 0th layer is the embeddings prior to the first transformer layer. <ref type="figure">Figure 3</ref> shows average precisions on USEB <ref type="bibr" target="#b47">[49]</ref> across different methods and layers. Similar to previous work <ref type="bibr" target="#b10">[11]</ref>, we find that in the unsupervised setting, decoder transformers (GPT) strongly underperform encoders (BERT). However, after fine-tuning on the same dataset with the same hyperparameters, decoders (SGPT) with 125M parameters closely trail the 110M parameter encoder (SBERT) for the 12th layer. Weighted mean pooling outperforms mean and last token pooling for SGPT 125M. When increasing SGPT size ten-fold, the last layer performance (24th layer) increases beyond that of SBERT models. The performance difference of weighted mean pooling compared to mean pooling further widens for SGPT 1.3B.  <ref type="bibr" target="#b47">[49]</ref>. Despite training on less than 0.1% of parameters BitFit models are within +2 to -2% of fully fine-tuned ones. BitFit degrades performance more for decoders than encoders. This could be due to the missing bias parameters, see <ref type="table">Table 4</ref>. <ref type="bibr" target="#b51">[53]</ref> highlights the importance of the query bias vector for BERT, which is not present for SGPT models. SGPT-5.8B-weightedmean-nli-bitfit sets an out-of-domain state-of-theart on USEB, but is outperformed by models trained in-domain in <ref type="bibr" target="#b47">[49]</ref>. We observed performance gains by increasing the training batch size. SGPT-5.8B-weightedmean-nli-bitfit is trained with a   <ref type="bibr" target="#b47">[49]</ref>. However, fragments may be in-domain due to the large pre-training data of the transformer models. SGPT-0.1B-weightedmean-nli performs 2% worse than SBERT-base-nli-v2 on USEB, but improves on Quora by 1%. Note that there is still a size difference of 14% between the two models. ?: Results from <ref type="bibr" target="#b47">[49]</ref> except when marked with ?. CQADupstack and SciDocs differ from the same-name datasets in BEIR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Results</head><p>batch size of 1024. In Appendix ?A, we provide results using a lower batch size and additional ablations. Results for Ada and Curie were obtained by querying the OpenAI Similarity Embeddings API in March 2022. They correspond to the cpt-text similarity models from <ref type="bibr" target="#b26">[27]</ref> and we provide their parameters in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Asymmetric Search</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Method</head><p>If not otherwise specified, we follow the same setup as in ?4.1.1. For asymmetric search, we train on MS-MARCO <ref type="bibr" target="#b27">[28]</ref>. We limit the model sequence length to 300 tokens during both training and inference. We follow concurrent work <ref type="bibr" target="#b26">[27]</ref> and add enclosing brackets to help the model distinguish between query and document. We embed the tokens of query q in two brackets as [q 0?n ]. For documents, we use curly brackets: {d 0?n }. We add the token ids of the brackets to the already tokenized text to avoid the tokens intermingling. We refer to these special brackets as specb. <ref type="table" target="#tab_10">Table 6</ref> benchmarks SGPT-BE-5.8B (SGPT-5.8B-weightedmean-msmarco-specb-bitfit) on BEIR <ref type="bibr" target="#b42">[44]</ref> with: (a) BM25 <ref type="bibr" target="#b39">[41]</ref>, a non-semantic fast baseline (b) SGPT-CE-6.1B from ?3 (c) BM25+CE <ref type="bibr" target="#b42">[44]</ref>, the current overall state-of-the-art on BEIR (d) TAS-B <ref type="bibr" target="#b16">[17]</ref>, the original Bi-Encoder state-of-the-art on BEIR (e) Contriever <ref type="bibr" target="#b19">[20]</ref>, a similar training scheme as <ref type="bibr" target="#b26">[27]</ref> but using an encoder transformer (f ) GTR-XXL <ref type="bibr" target="#b28">[29]</ref>, the current Bi-Encoder state-of-the-art on BEIR with 4.8 billion parameters using the BERT-like encoder transformer of T5 <ref type="bibr" target="#b36">[38]</ref> (g) cpt-text, a GPT-like decoder transformer architecture concurrently proposed in <ref type="bibr" target="#b26">[27]</ref>. Corresponding parameter estimates are in <ref type="table" target="#tab_2">Table 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Results</head><p>SGPT-5.8B achieves the best average nDCG@10 both on the BEIR subset selected in <ref type="bibr" target="#b26">[27]</ref> and on the full BEIR benchmark. It outperforms the roughly same-sized cpt-text-L and the 30x larger cpt-text-XL by 8.1% and 4.2%, respectively. Yet, cpt-text models have gone through an additional unsupervised training stage <ref type="bibr" target="#b26">[27]</ref> and are fully trained. SGPT-BE-5.8B fine-tunes just 700K parameters, 0.0004% of the parameters fine-tuned for cpt-text-XL <ref type="bibr" target="#b26">[27]</ref>. See <ref type="table" target="#tab_2">Table 2</ref> for sizes. We suspect much of the difference to come from the cpt-text model's inferior last token pooling as shown in <ref type="figure">Figure  3</ref>. Further, we suspect that the benefits of the additional unsupervised contrastive pre-training stage diminish when followed by supervised contrastive fine-tuning. SGPT-BE-5.8B improves on the overall state-of-the-art, a Cross-Encoder, by 3%. It improves on the previously best sentence embeddings (Bi-Encoder) on BEIR, GTR-XXL, by 7%. However, these improvements come at a significant cost. GTR-XXL has 20% fewer parameters and its embeddings have 768 dimensions. SGPT-BE-5.8B produces embeddings with 4096 dimensions, hence requiring about 5x more storage. It took the model six days on one Nvidia A100 GPU to encode the entire BioASQ corpus with 15M  documents and an average 200 words each <ref type="bibr" target="#b42">[44]</ref>. Its comparatively low performance on BioASQ may be improved by increasing the sequence length limit beyond 300, however, requiring additional compute. For SGPT-CE-6.1B, the sequence length limit was 2048 for the combined prompt on all datasets. The high performance on TREC-COVID for SGPT models could be due to the different pre-training datasets. The SGPT pre-training dataset, The Pile <ref type="bibr" target="#b12">[13]</ref>, contains data until mid-2020. This may give the models an information advantage on Covid-19. Lastly, we highlight that on Quora SGPT-BE-5.8B-msmarco is outperformed by SGPT-BE-5.8B-nli from <ref type="table" target="#tab_6">Table 5</ref>. Given our classification of Quora as a symmetric search task in ?2, this supports our overall distinction between asymmetric and symmetric search. We advise users of our models to classify their tasks as symmetric or asymmetric and use the appropriate model. For non-classifiable embedding tasks, both may work, but we recommend experimenting with embeddings from the symmetric models in ?4.1 first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This work presented SGPT. Building on SBERT, we proposed modifications to GPT models to use them as Cross-or Bi-Encoders for semantic search.</p><p>SGPT-BE uses position-weighted mean pooling and fine-tuning of only bias tensors. At scale, it produces new state-of-the-art sentence embeddings. The model can be used for semantic search or other embedding tasks. We recommend using SGPT-BE-5.8B when compute and storage are of high availability and maximum performance is desired.</p><p>SGPT-CE extracts log probabilities of pre-trained GPT models to produce unsupervised state-ofthe-art search results. The setup presented can only be used for semantic search. Storage can be limited, but compute should be of high availability for SGPT-CE-6.1B. The prompt and max re-rank parameter can be adjusted depending on performance and latency requirements.</p><p>Future research could fine-tune a GPT Cross-Encoder on MSMARCO similar to the BM25+CE model. We suspect that this should outperform the presented non-fine-tuned SGPT-CE model as well as SGPT-BE if enough documents are re-ranked. Further, the combination of SGPT with GPT for generative search results could be interesting. Possibly, SGPT embeddings could be injected into GPT models to generate answers. Lastly, a detailed study of the disadvantages of the missing biases in large GPT models could be helpful to consider their inclusion in the training of future large language models.        <ref type="table" target="#tab_1">Table 12</ref>: Prompts searched over ordered by increasing complexity and their nDCG@10 on MS-MARCO using SGPT-CE-2.7B. The example is the shortest query-doc match from FiQA <ref type="bibr" target="#b24">[25]</ref>. The sum of log probabilities from {query} is used as the re-rank score. Overflowing tokens are truncated from the left of {doc}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Licenses</head><p>Datasets from the BEIR benchmark are licensed under various licenses available in Appendix E of their paper <ref type="bibr" target="#b42">[44]</ref>. USEB datasets <ref type="bibr" target="#b47">[49]</ref> are licensed under an Apache 2.0 license. <ref type="bibr" target="#b0">1</ref> . To the best of our knowledge, these datasets do not contain private, personally identifiable information but may contain offensive content. OpenAI models are licensed by OpenAI API to customers via a non-exclusive, non-sublicensable, non-transferable, non-assignable, revocable license. 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Computational Cost</head><p>We use the OpenAI API to evaluate Search and Embeddings endpoints. We used tokens equivalent to around 5,000 USD. For SGPT experiments, we use one node with 8x NVIDIA A100 Tensor Core GPU with 40GB memory. For SGPT-CE the evaluation on the entire BEIR suite took around two weeks for the 5.8B model. For SGPT-BE symmetric search training took 21 hours, while asymmetric training took 60 hours for the 5.8B model. Our cluster was provided by Oracle.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Given a query q, documents d 1?3 , SGPT ranks the documents with scores s 1?3 . (a) The Cross-Encoder concatenates queries and documents and encodes them together. Scores are extracted log probabilities. (b) The Bi-Encoder separately encodes queries and documents. Resulting document vectors v 1?3 can be cached and retrieved at time t c , when a new query comes in. Scores are cosine similarities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Scaling behavior across parameters and re-ranking for SGPT-CE on BEIR. Scores are rescaled nDCG@10 based on bounds defined in Appendix ?A. Dataset labels are ordered by the Max Re-rank=100 6.1B performance. The higher on the y-axis, the more bottlenecked is the Cross-Encoder by BM25's performance. Average scores include MS MARCO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Documents are searched to find matches with the same content. Document: "If you receive dividends on an investment, those are taxed."The above document is a good match for the query: "Taxon Stocks or ETF's 0.259 I # Get matching document and query with the same content\nget_document()\n{doc}\nget_ query_matching_document()\n{query}" # Get matching document and query with the same content get_document() If you receive dividends on an investment, those are taxed. get_query_matching_document() Tax on Stocks or ETF's 0.253 J Documents are searched to find matches with the same content.\nThe document "{shortdoc}" is a good search result for "{shortquery}"\nThe document "{doc}" is a good search result for "{query} Documents are searched to find matches with the same content. The document "If you receive dividends on an investment, those are taxed." is a good search result for "Tax on Stocks or ETF's" The document "If you receive dividends on an investment, those are taxed." is a good search result for "Tax on Stocks or ETF's 0.252 K Document:\n{shortdoc}\nQuery:\n{shortquery}\n Document:\n{doc}\nQuery:\n{query} Document: If you receive dividends on an investment, those are taxed. Query: Tax on Stocks or ETF's Document: If you receive dividends on an investment, those are taxed. Query: Tax on Stocks or ETF's 0.250 L An intelligent, helpful bot is given. The bot responds "Yes" if the document is a fit to the query and "No" otherwise.\n###\nDocument: {doc}\nQuery: {query}\nBot:{ Yes} An intelligent, helpful bot is given. The bot responds "Yes" if the document is a fit to the query and "No" otherwise. ### Document: If you receive dividends on an investment, those are taxed. Query: Tax on Stocks or ETF's Bot: Yes 0.112</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Re-ranking performances on BEIR<ref type="bibr" target="#b42">[44]</ref>. OpenAI Search is to be distinguished from the OpenAI Embeddings endpoint. Please refer toTable 6in the Bi-Encoder section for a benchmark with the OpenAI Embeddings endpoint. Results on the Search endpoint were produced in October 2021.</figDesc><table><row><cell cols="3">Scores are nDCG@10. L: Dataset is too large for OpenAI's endpoint.  ?: Used for prompt-tuning</cell></row><row><cell cols="3">(SGPT) or training (BM25+CE). ?: Results from [44]. Other scores are from us. Average scores do</cell></row><row><cell>not include MS MARCO.</cell><cell></cell><cell></cell></row><row><cell>Model Name</cell><cell>Ada (S)</cell><cell>Babbage (M) Curie (L) Davinci (XL)</cell></row><row><cell>Parameters</cell><cell cols="2">350M (300M) 1.3B (1.2B) 6.7B (6B) 175B (175B)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>OpenAI model parameter estimates. Based on comparing the embedding sizes from the OpenAI docs with the dimensions provided in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Documents are searched to find matches with the same content.\nThe document "{doc}" is a good search result for "{query} 0.764 0.794 quoraA Questions are searched to find matches with the same content.\nThe question "{doc}" is a good search result for "{query}</figDesc><table><row><cell>Id</cell><cell>Python</cell><cell>125M 6.1B</cell></row><row><cell>G</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>0.766</cell></row><row><cell cols="2">quoraB Below are two similar questions asking the same thing.\nThe question</cell><cell>0.751</cell></row><row><cell></cell><cell>"{doc}" is similar to "{query}</cell><cell></cell></row><row><cell cols="2">quoraC These two questions are the same: 1. {doc} 2.{query}</cell><cell>0.740</cell></row><row><cell cols="2">quoraD Question Body: {doc} Question Title:{query}</cell><cell>0.782 0.830</cell></row><row><cell cols="2">quoraE Question Body: {shortdoc} Question Title: {shortquery}\n Question</cell><cell>0.773</cell></row><row><cell></cell><cell>Body: {doc} Question Title: {query}</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>SGPT-CE symmetric search results on Quora. The sum of log probabilities from {query} is used as the re-rank score. Overflowing tokens are truncated from the left of {doc}. Top 100 documents are re-ranked. Scores are nDCG@10.</figDesc><table><row><cell>4 SGPT Bi-Encoder</cell></row><row><cell>4.1 Symmetric Search</cell></row><row><cell>4.1.1 Method</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc>provides performance on the individual USEB datasets, Quora and STS-B. STS-B scores should not be the focus of comparison due to the drawbacks highlighted in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 5 :</head><label>5</label><figDesc>Results on USEB, Quora and STS-B. Metrics are average precision for USEB, nDCG@10 for Quora and Spearman correlation for STS-B. bf=BitFit. bs=Batch Size. OOD=Out-of-domain, to contrast these numbers from in-domain numbers in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>1B cpt-text-L? BM25+CE? TAS-B? SGPT-5.8B Contriever? GTR-XXL? cpt-text-L? cpt-text-XL?</figDesc><table><row><cell>Training (?)</cell><cell cols="2">Unsupervised</cell><cell>U. + U.</cell><cell cols="3">Unsupervised + Supervised</cell><cell></cell><cell cols="2">Unsupervised + Unsupervised + Supervised</cell><cell></cell></row><row><cell>Model (?)</cell><cell>[41]</cell><cell>SGPT-CE</cell><cell>[27]</cell><cell>[44]</cell><cell>[17]</cell><cell>SGPT-BE</cell><cell>[20]</cell><cell>[29]</cell><cell cols="2">OpenAI Embeddings [27]</cell></row><row><cell cols="3">Dataset (?) BM25 SGPT-6.MS MARCO 0.228 0.290</cell><cell></cell><cell>0.413  ?</cell><cell>0.408  ?</cell><cell>0.399  ?</cell><cell></cell><cell>0.442  ?</cell><cell></cell><cell></cell></row><row><cell>TREC-COVID</cell><cell>0.688</cell><cell>0.791</cell><cell>0.427</cell><cell>0.757</cell><cell>0.481</cell><cell>0.873</cell><cell>0.596</cell><cell>0.501</cell><cell>0.562</cell><cell>0.649</cell></row><row><cell>BioASQ</cell><cell>0.488</cell><cell>0.547</cell><cell></cell><cell>0.523</cell><cell>0.383</cell><cell>0.413</cell><cell></cell><cell>0.324</cell><cell></cell><cell></cell></row><row><cell>NFCorpus</cell><cell>0.306</cell><cell>0.347</cell><cell>0.369</cell><cell>0.350</cell><cell>0.319</cell><cell>0.362</cell><cell>0.328</cell><cell>0.342</cell><cell>0.380</cell><cell>0.407</cell></row><row><cell>NQ</cell><cell>0.326</cell><cell>0.401</cell><cell></cell><cell>0.533</cell><cell>0.463</cell><cell>0.524</cell><cell>0.498</cell><cell>0.568</cell><cell></cell><cell></cell></row><row><cell>HotpotQA</cell><cell>0.602</cell><cell>0.699</cell><cell>0.543</cell><cell>0.707</cell><cell>0.584</cell><cell>0.593</cell><cell>0.638</cell><cell>0.599</cell><cell>0.648</cell><cell>0.688</cell></row><row><cell>FiQA-2018</cell><cell>0.254</cell><cell>0.401</cell><cell>0.397</cell><cell>0.347</cell><cell>0.300</cell><cell>0.372</cell><cell>0.329</cell><cell>0.467</cell><cell>0.452</cell><cell>0.512</cell></row><row><cell>Signal-1M (RT)</cell><cell>0.330</cell><cell>0.323</cell><cell></cell><cell>0.338</cell><cell>0.289</cell><cell>0.267</cell><cell></cell><cell>0.273</cell><cell></cell><cell></cell></row><row><cell>TREC-NEWS</cell><cell>0.405</cell><cell>0.466</cell><cell></cell><cell>0.431</cell><cell>0.377</cell><cell>0.481</cell><cell></cell><cell>0.346</cell><cell></cell><cell></cell></row><row><cell>Robust04</cell><cell>0.425</cell><cell>0.480</cell><cell></cell><cell>0.475</cell><cell>0.427</cell><cell>0.514</cell><cell></cell><cell>0.506</cell><cell></cell><cell></cell></row><row><cell>ArguAna</cell><cell>0.472</cell><cell>0.286</cell><cell>0.392</cell><cell>0.311</cell><cell>0.429</cell><cell>0.514</cell><cell>0.446</cell><cell>0.540</cell><cell>0.469</cell><cell>0.435</cell></row><row><cell>Touch?-2020</cell><cell>0.347</cell><cell>0.234</cell><cell>0.228</cell><cell>0.271</cell><cell>0.162</cell><cell>0.254</cell><cell>0.230</cell><cell>0.256</cell><cell>0.309</cell><cell>0.291</cell></row><row><cell>CQADupStack</cell><cell>0.326</cell><cell>0.420</cell><cell></cell><cell>0.370</cell><cell>0.314</cell><cell>0.381</cell><cell>0.345</cell><cell>0.399</cell><cell></cell><cell></cell></row><row><cell>Quora</cell><cell>0.808</cell><cell>0.794</cell><cell>0.687</cell><cell>0.825</cell><cell>0.835</cell><cell>0.846</cell><cell>0.865</cell><cell>0.892</cell><cell>0.677</cell><cell>0.638</cell></row><row><cell>DBPedia</cell><cell>0.320</cell><cell>0.370</cell><cell>0.312</cell><cell>0.409</cell><cell>0.384</cell><cell>0.399</cell><cell>0.413</cell><cell>0.408</cell><cell>0.412</cell><cell>0.432</cell></row><row><cell>SCIDOCS</cell><cell>0.165</cell><cell>0.196</cell><cell></cell><cell>0.166</cell><cell>0.149</cell><cell>0.197</cell><cell>0.165</cell><cell>0.161</cell><cell>0.177  ?</cell><cell></cell></row><row><cell>FEVER</cell><cell>0.649</cell><cell>0.725</cell><cell>0.638</cell><cell>0.819</cell><cell>0.700</cell><cell>0.783</cell><cell>0.758</cell><cell>0.740</cell><cell>0.756</cell><cell>0.775</cell></row><row><cell>Climate-FEVER</cell><cell>0.186</cell><cell>0.161</cell><cell>0.161</cell><cell>0.253</cell><cell>0.228</cell><cell>0.305</cell><cell>0.237</cell><cell>0.267</cell><cell>0.194</cell><cell>0.223</cell></row><row><cell>SciFact</cell><cell>0.611</cell><cell>0.682</cell><cell>0.712</cell><cell>0.688</cell><cell>0.643</cell><cell>0.747</cell><cell>0.677</cell><cell>0.662</cell><cell>0.744</cell><cell>0.754</cell></row><row><cell>Sub-Average</cell><cell>0.477</cell><cell>0.499</cell><cell>0.442</cell><cell>0.520</cell><cell>0.460</cell><cell>0.550</cell><cell>0.502</cell><cell>0.516</cell><cell>0.509</cell><cell>0.528</cell></row><row><cell>Average</cell><cell>0.428</cell><cell>0.462</cell><cell></cell><cell>0.476</cell><cell>0.395</cell><cell>0.490</cell><cell></cell><cell>0.458</cell><cell></cell><cell></cell></row><row><cell>Best on</cell><cell>1</cell><cell>2</cell><cell>0</cell><cell>3</cell><cell>0</cell><cell>5</cell><cell>0</cell><cell>3</cell><cell>0</cell><cell>4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc></figDesc><table /><note>Comparison of BEIR state-of-the-art models. Keep model size, latency and training time in mind when inspecting this table. Further, this table compares 2 Cross-Encoders and 8 Bi-Encoders, whose respective trade-offs should be considered. Scores are nDCG@10. ?: In-domain performance.?: Results from [44]. ?: Results from [20]. ?: Results from [29]. ?: Results from [27] except when marked with ?. Other scores are from us. Average scores do not include MS MARCO.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 7 :</head><label>7</label><figDesc>Additional SGPT Cross-Encoder scores on BEIR. Bounds are the maximum achievable score, given the first-stage BM25 results. We report additional Max Re-rank=10 scores using OpenAI's Average scores do not include MS MARCO.</figDesc><table><row><cell>search endpoint: TREC-COVID: 0.545 (Ada), 0.539 (Davinci); SciFact: 0.670 (Ada), 0.658 (Davinci).</cell></row><row><cell>Scores are nDCG@10.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Additional SGPT Bi-Encoder scores on BEIR. Scores are nDCG@10. Average scores do not include MS MARCO.</figDesc><table><row><cell>Dataset (?)</cell><cell>AskU</cell><cell>CQA</cell><cell></cell><cell>TwitterP</cell><cell></cell><cell></cell><cell></cell><cell>SciDocs</cell><cell></cell><cell></cell><cell>Avg</cell><cell>Quora</cell><cell>STS-B</cell></row><row><cell>Method (?)</cell><cell></cell><cell></cell><cell>TURL</cell><cell>PIT</cell><cell>Avg</cell><cell>Cite</cell><cell>CC</cell><cell>CR</cell><cell>CV</cell><cell>Avg</cell><cell></cell><cell></cell><cell></cell></row><row><cell>OOD Unsupervised</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BERT-base-uncased-mean</cell><cell>48.5</cell><cell>6.5</cell><cell>69.1</cell><cell>61.7</cell><cell>65.4</cell><cell>59.4</cell><cell>65.1</cell><cell>65.4</cell><cell>68.6</cell><cell>64.6</cell><cell>46.2</cell><cell>57.3</cell><cell></cell></row><row><cell>BERT-large-uncased-mean</cell><cell>49.5</cell><cell>6.1</cell><cell>63.2</cell><cell>51.0</cell><cell>57.1</cell><cell>60.7</cell><cell>65.7</cell><cell>65.1</cell><cell>68.6</cell><cell>65.0</cell><cell>44.4</cell><cell></cell><cell></cell></row><row><cell>GPT-1.3B-mean</cell><cell>50.9</cell><cell>4.9</cell><cell>56.1</cell><cell>47.5</cell><cell>51.8</cell><cell>50.2</cell><cell>56.8</cell><cell>65.1</cell><cell>59.1</cell><cell>55.1</cell><cell>40.7</cell><cell>45.4</cell><cell></cell></row><row><cell>GPT-1.3B-weightedmean</cell><cell>50.2</cell><cell>5.7</cell><cell>50.6</cell><cell>49.1</cell><cell>49.9</cell><cell>53.3</cell><cell>57.0</cell><cell>58.5</cell><cell>61.1</cell><cell>57.5</cell><cell>40.8</cell><cell></cell><cell></cell></row><row><cell>GPT-1.3B-lasttoken</cell><cell>45.7</cell><cell>4.5</cell><cell>35.8</cell><cell>34.8</cell><cell>35.3</cell><cell>49.7</cell><cell>52.0</cell><cell>53.2</cell><cell>53.7</cell><cell>52.2</cell><cell>34.4</cell><cell></cell><cell></cell></row><row><cell cols="2">OOD Unsupervised + OOD Supervised (NLI)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SBERT-large-nli-v2</cell><cell>53.9</cell><cell>11.8</cell><cell>74.6</cell><cell>70.1</cell><cell>72.3</cell><cell>67.8</cell><cell>70.6</cell><cell>71.8</cell><cell>73.0</cell><cell>70.8</cell><cell>52.2</cell><cell>79.2</cell><cell>84.1</cell></row><row><cell>SGPT-125M-mean-nli</cell><cell>53.9</cell><cell>10.9</cell><cell>73.7</cell><cell>66.6</cell><cell>70.1</cell><cell>65.9</cell><cell>68.3</cell><cell>68.6</cell><cell>70.9</cell><cell>68.4</cell><cell>50.8</cell><cell>79.1</cell><cell>79.6</cell></row><row><cell>SGPT-125M-mean-nli-bf</cell><cell>54.9</cell><cell>11.1</cell><cell>72.8</cell><cell>64.3</cell><cell>68.6</cell><cell>63.5</cell><cell>66.6</cell><cell>67.0</cell><cell>69.4</cell><cell>66.6</cell><cell>50.3</cell><cell>77.0</cell><cell>76.9</cell></row><row><cell>SGPT-125M-mean-nli-linear5</cell><cell>50.1</cell><cell>6.0</cell><cell>67.9</cell><cell>58.4</cell><cell>63.1</cell><cell>53.2</cell><cell>57.1</cell><cell>58.4</cell><cell>61.8</cell><cell>57.6</cell><cell>44.2</cell><cell></cell><cell>62.8</cell></row><row><cell>SGPT-125M-mean-nli-linearthenpool5</cell><cell>50.1</cell><cell>6.4</cell><cell>69.2</cell><cell>60.3</cell><cell>64.8</cell><cell>55.9</cell><cell>59.5</cell><cell>60.9</cell><cell>64.8</cell><cell>60.3</cell><cell>45.4</cell><cell></cell><cell>68.6</cell></row><row><cell>SGPT-125M-learntmean-nli</cell><cell>54.1</cell><cell>11.2</cell><cell>73.6</cell><cell>66.8</cell><cell>70.2</cell><cell>65.7</cell><cell>68.4</cell><cell>68.7</cell><cell>71.0</cell><cell>68.4</cell><cell>51.0</cell><cell>79.3</cell><cell>80.1</cell></row><row><cell>SGPT-1.3B-mean-nli</cell><cell>56.2</cell><cell>12.5</cell><cell>75.5</cell><cell>67.6</cell><cell>71.5</cell><cell>68.6</cell><cell>71.6</cell><cell>72.3</cell><cell>73.7</cell><cell>71.5</cell><cell>53.0</cell><cell>80.5</cell><cell>81.8</cell></row><row><cell>SGPT-1.3B-weightedmean-nli</cell><cell>56.8</cell><cell>13.1</cell><cell>75.6</cell><cell>70.1</cell><cell>72.9</cell><cell>71.1</cell><cell>74.6</cell><cell>74.3</cell><cell>76.6</cell><cell>74.2</cell><cell>54.2</cell><cell>81.5</cell><cell>83.1</cell></row><row><cell>SGPT-1.3B-weightedmean-nli-bf</cell><cell>56.2</cell><cell>12.4</cell><cell>74.5</cell><cell>69.8</cell><cell>72.1</cell><cell>68.3</cell><cell>71.8</cell><cell>72.2</cell><cell>74.0</cell><cell>71.6</cell><cell>53.1</cell><cell>81.9</cell><cell>84.0</cell></row><row><cell>SGPT-2.7B-weightedmean-nli-bf</cell><cell>56.6</cell><cell>13.2</cell><cell>75.0</cell><cell>72.0</cell><cell>73.5</cell><cell>70.5</cell><cell>73.2</cell><cell>73.1</cell><cell>75.3</cell><cell>73.0</cell><cell>54.1</cell><cell>82.0</cell><cell>85.5</cell></row><row><cell>SGPT-5.8B-weightedmean-nli-bf-bs48</cell><cell>55.9</cell><cell>15.0</cell><cell>74.8</cell><cell>74.1</cell><cell>74.4</cell><cell>73.8</cell><cell>77.1</cell><cell>76.6</cell><cell>77.5</cell><cell>76.3</cell><cell>55.4</cell><cell>83.9</cell><cell>86.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 9 :</head><label>9</label><figDesc>Additional results on USEB, Quora and STS-B. Metrics are average precision for USEB, nDCG@10 for Quora and Spearman correlation for STS-B. bf=BitFit. bs=batch size. OOD=Outof-domain, to contrast these numbers from in-domain numbers in<ref type="bibr" target="#b47">[49]</ref>. However, fragments may be in-domain due to the large pre-training data of the transformer models. CQADupstack and SciDocs differ from the same-name datasets in BEIR.</figDesc><table><row><cell>Model (?)</cell><cell cols="4">OpenAI Embeddings Search OpenAI Embeddings Similarity</cell></row><row><cell>Dataset (?)</cell><cell>Ada</cell><cell>Curie</cell><cell>Ada</cell><cell>Curie</cell></row><row><cell>MS MARCO</cell><cell>0.37935  ?</cell><cell></cell><cell></cell><cell></cell></row><row><cell>TREC-COVID</cell><cell>0.68067</cell><cell>0.56141</cell><cell>0.18800</cell><cell>0.07612</cell></row><row><cell>NFCorpus</cell><cell>0.33170</cell><cell>0.38007</cell><cell>0.18166</cell><cell>0.19961</cell></row><row><cell>NQ</cell><cell>0.42815</cell><cell></cell><cell>0.02020</cell><cell></cell></row><row><cell>HotpotQA</cell><cell>0.59393</cell><cell></cell><cell>0.12598</cell><cell></cell></row><row><cell>FiQA-2018</cell><cell>0.38412</cell><cell>0.45211</cell><cell>0.07541</cell><cell>0.05138</cell></row><row><cell cols="2">Signal-1M (RT) 0.25388</cell><cell></cell><cell>0.22923</cell><cell></cell></row><row><cell>TREC-NEWS</cell><cell>0.43899</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ArguAna</cell><cell>0.46913</cell><cell>0.46978</cell><cell>0.39647</cell><cell></cell></row><row><cell>Touch?-2020</cell><cell>0.28679</cell><cell></cell><cell></cell><cell></cell></row><row><cell>CQADupStack</cell><cell></cell><cell></cell><cell>0.10171</cell><cell></cell></row><row><cell>Quora</cell><cell>0.70572</cell><cell></cell><cell>0.82175</cell><cell>0.83107</cell></row><row><cell>DBPedia</cell><cell></cell><cell></cell><cell>0.03479</cell><cell></cell></row><row><cell>SCIDOCS</cell><cell>0.14827</cell><cell>0.17738</cell><cell>0.06275</cell><cell></cell></row><row><cell>SciFact</cell><cell>0.67255</cell><cell>0.74345</cell><cell></cell><cell>0.46676</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 10 :</head><label>10</label><figDesc>Our results using the OpenAI Embeddings Endpoint in December 2021. They match closely with the results published in<ref type="bibr" target="#b26">[27]</ref>. Scores are nDCG@10. ?: In-domain performance.</figDesc><table><row><cell>Method</cell><cell>NFCorpus</cell><cell>FiQA</cell><cell cols="3">ArguaAna SCIDOCS SciFact</cell><cell>Avg</cell></row><row><cell>Unsupervised</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BM25</cell><cell>0.30630</cell><cell>0.25407</cell><cell>0.47174</cell><cell>0.16472</cell><cell cols="2">0.61100 0.36157</cell></row><row><cell>Unsupervised + Supervised (MSMARCO)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>SBERT-base-msmarco</cell><cell>0.26316</cell><cell>0.25269</cell><cell>0.43918</cell><cell>0.13302</cell><cell cols="2">0.53685 0.32498</cell></row><row><cell>SBERT-base-msmarco-asym</cell><cell>0.26532</cell><cell>0.22916</cell><cell>0.41694</cell><cell>0.11993</cell><cell cols="2">0.49366 0.30500</cell></row><row><cell>SBERT-base-msmarco-bitfit</cell><cell>0.24934</cell><cell>0.22260</cell><cell>0.46007</cell><cell>0.12843</cell><cell cols="2">0.53443 0.31897</cell></row><row><cell>SGPT-125M-weightedmean-msmarco</cell><cell>0.21628</cell><cell>0.19757</cell><cell>0.41055</cell><cell>0.11720</cell><cell cols="2">0.54417 0.29715</cell></row><row><cell>SGPT-125M-weightedmean-msmarco-asym</cell><cell>0.20919</cell><cell>0.20865</cell><cell>0.35309</cell><cell>0.10618</cell><cell cols="2">0.52456 0.28034</cell></row><row><cell>SGPT-125M-weightedmean-msmarco-bitfit</cell><cell>0.19088</cell><cell>0.17411</cell><cell>0.45265</cell><cell>0.11244</cell><cell cols="2">0.53280 0.24381</cell></row><row><cell>SGPT-125M-weightedmean-msmarco-specb</cell><cell>0.23734</cell><cell>0.22793</cell><cell>0.41137</cell><cell>0.11542</cell><cell cols="2">0.58245 0.31490</cell></row><row><cell>SGPT-125M-weightedmean-msmarco-specb-bitfit</cell><cell>0.22327</cell><cell>0.20911</cell><cell>0.44932</cell><cell>0.11956</cell><cell cols="2">0.57703 0.31566</cell></row><row><cell>SGPT-125M-lasttoken-msmarco-specb</cell><cell>0.14277</cell><cell>0.18626</cell><cell>0.28850</cell><cell>0.11014</cell><cell cols="2">0.31202 0.20794</cell></row><row><cell>SGPT-125M-weightedmean-msmarco-specb-bitfitwte</cell><cell>0.14440</cell><cell>0.09439</cell><cell>0.15428</cell><cell>0.05666</cell><cell cols="2">0.32163 0.15427</cell></row><row><cell>SGPT-1.3B-weightedmean-msmarco-specb-bitfit</cell><cell>0.31748</cell><cell>0.30813</cell><cell>0.47963</cell><cell>0.15827</cell><cell cols="2">0.67697 0.38810</cell></row><row><cell>SGPT-2.7B-weightedmean-msmarco-specb-bitfit</cell><cell>0.32936</cell><cell>0.32469</cell><cell>0.49207</cell><cell>0.16520</cell><cell cols="2">0.71228 0.40472</cell></row><row><cell>SGPT-5.8B-weightedmean-msmarco-specb-bitfit-bs48</cell><cell>0.35330</cell><cell>0.37080</cell><cell>0.49888</cell><cell>0.19389</cell><cell cols="2">0.74195 0.43176</cell></row><row><cell>SGPT-5.8B-weightedmean-msmarco-specb-bitfit-bs256</cell><cell>0.36213</cell><cell>0.37200</cell><cell>0.51352</cell><cell>0.19719</cell><cell cols="2">0.74693 0.43835</cell></row><row><cell>Unsupervised + Unsupervised + Supervised (MSMARCO)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ada Search (Dec 2021)</cell><cell>0.33170</cell><cell>0.38412</cell><cell>0.46913</cell><cell>0.14827</cell><cell cols="2">0.67255 0.40115</cell></row><row><cell>Curie Search (Dec 2021)</cell><cell>0.38007</cell><cell>0.45211</cell><cell>0.46978</cell><cell>0.17738</cell><cell cols="2">0.74345 0.44456</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head>Table 11 :</head><label>11</label><figDesc>SGPT-BE experiments on a subset of the 5 smallest BEIR datasets by corpus size. The best checkpoint for all models is displayed. specb=special brackets. bs=batch size. bitfitwte=BitFit + Word Token Embeddings are trained. The idea was to help the model learn the special role of the brackets. It did not help. asym=Two-tower model with separate transformers for queries and documents. SGPT-125M-weightedmean-msmarco-specb performs 3% worse than SBERT-basemsmarco on average. SGPT-125M-weightedmean-msmarco-specb-bitfit performs 1% worse than SBERT-base-msmarco-bitfit on average. Interestingly, Curie beats SGPT-5.8B on this subset, but does not on the bigger subset inTable 6. Scores are nDCG@10. Documents are searched to find matches with the same content.\nThe document "{doc}" is a good search result for "{query} Documents are searched to find matches with the same content. The document "If you receive dividends on an investment, those are taxed." is a good search result for "Tax on Stocks or ETF's 0.278 H Documents are searched to find matches with the same content.\nDocument: "{doc}"\n\nThe above document is a good match for the query: "{query}</figDesc><table><row><cell cols="3">B Task and Experimental Details</cell><cell></cell></row><row><cell cols="2">B.1 Prompts</cell><cell></cell><cell></cell></row><row><cell>Id</cell><cell>Python</cell><cell>Example</cell><cell>MSMARCO</cell></row><row><cell>A</cell><cell>{doc} {query}</cell><cell>If you receive dividends on an investment, those are taxed. Tax on Stocks or</cell><cell>0.210</cell></row><row><cell></cell><cell></cell><cell>ETF's</cell><cell></cell></row><row><cell>B</cell><cell>{doc}\n{query}</cell><cell>If you receive dividends on an investment, those are taxed.</cell><cell>0.230</cell></row><row><cell></cell><cell></cell><cell>Tax on Stocks or ETF's</cell><cell></cell></row><row><cell>C</cell><cell>Document:\n{doc}\n\n</cell><cell>Document:</cell><cell>0.264</cell></row><row><cell></cell><cell>Query:\n{query}</cell><cell>If you receive dividends on an investment, those are taxed.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Query:</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Tax on Stocks or ETF's</cell><cell></cell></row><row><cell>D</cell><cell>Body:\n{doc}\n\n</cell><cell>Body:</cell><cell>0.242</cell></row><row><cell></cell><cell>Title:\n{query}</cell><cell>If you receive dividends on an investment, those are taxed.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Title:</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Tax on Stocks or ETF's</cell><cell></cell></row><row><cell>E</cell><cell>selected document:\n{doc}\n\n</cell><cell>selected document:</cell><cell>0.252</cell></row><row><cell></cell><cell>relevant query:\n{query}</cell><cell>If you receive dividends on an investment, those are taxed.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>relevant query:</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Tax on Stocks or ETF's</cell><cell></cell></row><row><cell>F</cell><cell>The selected text is:\n{doc}\n\n</cell><cell>The selected text is:</cell><cell>0.246</cell></row><row><cell></cell><cell>The relevant title is:\n{query}</cell><cell>If you receive dividends on an investment, those are taxed.</cell><cell></cell></row><row><cell></cell><cell></cell><cell>The relevant title is:</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Tax on Stocks or ETF's</cell><cell></cell></row><row><cell>G</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/UKPLab/useb</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://openai.com/api/policies/terms/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We thank Constantin Eichenberg and Samuel Weinbach for insightful discussions and valuable feedback throughout the project. We thank Robert Baldock, Marco Bellagente and Koen Oostermeijer for reading drafts of this paper. This work has been supported by OpenAI under the academic access program.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quentin</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Preetham</forename><surname>Gali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Hallahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levy-Kramer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Connor</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Nestler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kip</forename><surname>Parker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pieler</surname></persName>
		</author>
		<title level="m">GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch</title>
		<editor>Shivanshu Purohit, Tri Songz, Phil Wang, and Samuel Weinbach</editor>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishi</forename><surname>Bommasani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Drew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russ</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simran</forename><surname>Altman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sydney Von Arx</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeannette</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bohg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emma</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brunskill</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.07258</idno>
		<title level="m">On the opportunities and risks of foundation models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luiz</forename><surname>Bonifacio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Abonizio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.05144</idno>
		<title level="m">Marzieh Fadaee, and Rodrigo Nogueira. 2022. InPars: Data Augmentation for Information Retrieval using Large Language Models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabor</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Angeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.05326</idno>
		<title level="m">A large annotated corpus for learning natural language inference</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sastry</surname></persName>
			<affiliation>
				<orgName type="collaboration">Amanda Askell</orgName>
			</affiliation>
		</author>
		<idno type="arXiv">arXiv:2005.14165</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>et al. 2020. Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Isotropy in the contextual embedding space: Clusters and manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenneth</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mona</forename><surname>Diab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inigo</forename><surname>Lopez-Gazpio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucia</forename><surname>Specia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.00055</idno>
		<title level="m">Semeval-2017 task 1: Semantic textual similarity-multilingual and cross-lingual focused evaluation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Yi</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicole</forename><surname>Limtiaco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rhomni</forename><surname>St John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Constant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Guajardo-Cespedes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Tar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.11175</idno>
		<title level="m">Universal sentence encoder</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aakanksha</forename><surname>Chowdhery</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maarten</forename><surname>Bosma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gehrmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.02311</idno>
		<title level="m">PaLM: Scaling Language Modeling with Pathways</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kawin</forename><surname>Ethayarajh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.00512</idno>
		<title level="m">How contextual are contextualized word representations? comparing the geometry of BERT, ELMo, and GPT-2 embeddings</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Formal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Lassance</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Piwowarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">St?phane</forename><surname>Clinchant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.10086</idno>
		<title level="m">SPLADE v2: Sparse lexical and expansion model for information retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><surname>Biderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sid</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurence</forename><surname>Golding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Travis</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Phang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Horace</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anish</forename><surname>Thite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noa</forename><surname>Nabeshima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.00027</idno>
		<title level="m">The pile: An 800gb dataset of diverse text for language modeling</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Scaling deep contrastive learning batch size under memory limited setup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Callan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.06983</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Simcse: Simple contrastive learning of sentence embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingcheng</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08821</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Giorgi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Osvald</forename><surname>Nitski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bader</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03659</idno>
		<title level="m">Declutr: Deep contrastive learning for unsupervised textual representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficiently teaching an effective dense retriever with balanced topic aware sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Hofst?tter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng-Chieh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jheng-Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Allan</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="113" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Edward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yelong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeyuan</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Allen-Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shean</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09685</idno>
		<title level="m">Lora: Low-rank adaptation of large language models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Internetlivestats. 2022. Google Search Statistics</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.09118</idno>
		<title level="m">Towards Unsupervised Dense Information Retrieval with Contrastive Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Colbert: Efficient and effective passage search via contextualized late interaction over bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blunsom</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.07278</idno>
		<title level="m">2020. A survey on contextual embeddings</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Cutting down on prompts and parameters: Simple few-shot learning with language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Robert L Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivana</forename><surname>Bala?evi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13353</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Www&apos;18 open challenge: financial opinion mining and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Macedo</forename><surname>Maia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siegfried</forename><surname>Handschuh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr?</forename><surname>Freitas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manel</forename><surname>Zarrouk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Balahur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the The Web Conference</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1941" to="1942" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Understanding searches better than ever before</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pandu</forename><surname>Nayak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><forename type="middle">Michael</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolas</forename><surname>Tezak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.10005</idno>
		<title level="m">Text and Code Embeddings by Contrastive Pre-Training</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MS MARCO: A human generated machine reading comprehension dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CoCo@ NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmo</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuyun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><forename type="middle">Hern?ndez</forename><surname>?brego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.07899</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>et al. 2021. Large Dual Encoders Are Generalizable Retrievers</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><surname>Nogueira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14424</idno>
		<title level="m">Multi-stage document ranking with BERT</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<title level="m">OpenAI. 2022. Introducing Text and Code Embeddings in the OpenAI API</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">True few-shot learning with language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douwe</forename><surname>Kiela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">RocketQA: An optimized training approach to dense passage retrieval for open-domain question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqi</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daxiang</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08191</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Tim Salimans, and Ilya Sutskever</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katie</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename><surname>Millican</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susannah</forename><surname>Ring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Young</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.11446</idno>
		<title level="m">Scaling language models: Methods, analysis &amp; insights from training gopher</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10084</idno>
		<title level="m">Sentence-bert: Sentence embeddings using siamese bert-networks</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Schiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tilman</forename><surname>Beck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Daxenberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09821</idno>
		<title level="m">Christian Stab, and Iryna Gurevych. 2019. Classification and clustering of arguments with contextualized word embeddings</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">The probabilistic relevance framework: BM25 and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Zaragoza</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Now Publishers Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Schick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.07540</idno>
		<title level="m">Generating Datasets with Pretrained Language Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Augmented sbert: Data augmentation method for improving bi-encoders for pairwise sentence scoring tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nandan</forename><surname>Thakur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><surname>Reimers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Daxenberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08240</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nandan</forename><surname>Thakur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08663</idno>
		<title level="m">Nils Reimers, Andreas R?ckl?, Abhishek Srivastava, and Iryna Gurevych. 2021. BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">TREC-COVID: constructing a pandemic information retrieval test collection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellen</forename><surname>Voorhees</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tasmeer</forename><surname>Alam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Bedrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dina</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyle</forename><surname>Hersh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kirk</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Soboroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGIR Forum</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021" />
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Superglue: A stickier benchmark for general-purpose language understanding systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yada</forename><surname>Pruksachatkun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aran</forename><surname>Komatsuzaki</surname></persName>
		</author>
		<ptr target="https://github.com/kingoflolz/mesh-transformer-jax" />
		<title level="m">GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kexin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.06979</idno>
		<title level="m">Nils Reimers, and Iryna Gurevych. 2021. TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhabrata</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>Hassan Awadallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.06274</idno>
		<title level="m">LiST: Lite Self-training Makes Efficient Few-shot Learners</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">A broad-coverage challenge corpus for sentence understanding through inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adina</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Nangia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel R</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05426</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">HotpotQA: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher D</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1809.09600</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Ben Zaken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shauli</forename><surname>Ravfogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.10199</idno>
		<title level="m">BitFit: Simple Parameterefficient Fine-tuning for Transformer-based Masked Language-models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Bing delivers more contextualized search using quantized transformer inference on NVIDIA GPUs in Azure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingqin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cassandra</forename><surname>Oduola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
