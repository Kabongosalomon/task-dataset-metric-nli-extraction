<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Contrast and Mix: Temporal Contrastive Video Domain Adaptation with Background Mixing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aadarsh</forename><surname>Sahoo</surname></persName>
							<email>sahoo_aadarsh@</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rutav</forename><surname>Shah</surname></persName>
							<email>rutavms@</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
							<email>rpanda@ibm.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
							<email>saenko@bu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Boston University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iit</forename><surname>Kharagpur</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Contrast and Mix: Temporal Contrastive Video Domain Adaptation with Background Mixing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T19:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unsupervised domain adaptation which aims to adapt models trained on a labeled source domain to a completely unlabeled target domain has attracted much attention in recent years. While many domain adaptation techniques have been proposed for images, the problem of unsupervised domain adaptation in videos remains largely underexplored. In this paper, we introduce Contrast and Mix (CoMix), a new contrastive learning framework that aims to learn discriminative invariant feature representations for unsupervised video domain adaptation. First, unlike existing methods that rely on adversarial learning for feature alignment, we utilize temporal contrastive learning to bridge the domain gap by maximizing the similarity between encoded representations of an unlabeled video at two different speeds as well as minimizing the similarity between different videos played at different speeds. Second, we propose a novel extension to the temporal contrastive loss by using background mixing that allows additional positives per anchor, thus adapting contrastive learning to leverage action semantics shared across both domains. Moreover, we also integrate a supervised contrastive learning objective using target pseudo-labels to enhance discriminability of the latent space for video domain adaptation. Extensive experiments on several benchmark datasets demonstrate the superiority of our proposed approach over state-of-the-art methods. Project page: https://cvir.github.io/projects/comix.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Unsupervised domain adaptation (UDA), which alleviates the requirement of large amounts of annotated data by adapting a model learned on a labelled source domain to an unlabelled target domain, has drawn a great deal of attention in the last few years <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b84">85]</ref>. Much progress has been made in developing deep UDA methods by minimizing the cross-domain divergence <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b73">74]</ref>, adding adversarial domain discriminators <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b78">79]</ref>, and image-to-image translation techniques <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b54">55]</ref>. However, despite impressive results on commonly used benchmark datasets (e.g., <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b60">61]</ref>), most of the methods have been developed only for images and not for videos, where the annotation task is often more complicated requiring tedious human labor in comparison to images.</p><p>More recently, very few works have attempted deep UDA for video action recognition by directly matching segment-level features <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b44">45]</ref> or with attention weights <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b56">57]</ref>. However, <ref type="bibr" target="#b0">(1)</ref> trivially matching segment-level feature distributions by extending the image-specific approaches, without considering the rich temporal information may not alone be sufficient for video domain adaptation; <ref type="bibr" target="#b1">(2)</ref> prior methods often focus on aligning target features with source, rather than exploiting any action semantics shared across both domains (e.g., difference in background with the same action: videos in the top row of <ref type="figure" target="#fig_0">Figure 1</ref> are from the source and target domain respectively, but both capture the same action walking); (3) existing methods often rely on complex adversarial learning which is unwieldy to train, resulting in very fragile convergence. <ref type="bibr">35th</ref> Conference on Neural Information Processing Systems (NeurIPS 2021).</p><p>Meanwhile, self-supervised pretext tasks like predicting rotation and translation have recently emerged as an alternative to adversarial learning for unsupervised domain adaptation in images <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b74">75]</ref>. While these works show the promising potential of self-supervised learning in aligning source and target domains, the more recent very successful contrastive representation learning <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b55">56]</ref> has never been used to adapt video action recognition models to target domains. Motivated by this, in this paper, we explore the following natural, yet important question: whether and how contrastive learning could be exploited for the challenging and practically important task of unsupervised video domain adaptation for human action recognition? To this end, we introduce Contrast and Mix (CoMix), a simple yet effective approach based on contrastive learning to adapt video action recognition models trained on a labeled source domain to unlabelled target domains. First, we propose to represent video as a graph and then utilize temporal contrastive self-supervised learning over the graph representations as a nexus between source and target domains to align features, without requiring any additional adversarial learning, as most prior works do in video domain adaptation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b56">57]</ref>. Specifically, we maximize the similarity between encoded representations of the same video at two different speeds as well as minimize the similarity between different videos played at different speeds, leveraging the fact that changing video speed does not change an action on both domains. While minimization of contrastive self-supervised losses in both domains simultaneously helps in domain alignment, it ignores action semantics shared across them as the loss treats each domain individually. To alleviate this, we incorporate new synthetic videos into the temporal contrastive objective, which are obtained by mixing background of a video from one domain to a video from another domain, as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (bottom). Importantly, since mixing background doesn't change the temporal dynamics, we introduce pseudo-labels for the mixed videos to be same as the label of the original videos and consider additional positives per anchor (see <ref type="figure" target="#fig_1">Figure 2)</ref>, which encourages the model to generalize to new samples that may not be covered by temporal contrastive learning in hand. In other words, mixed background video of an input sample in the embedding space act as small semantic perturbations that are not imaginary, i.e., they are representative of the action semantics shared across source and target domains. Finally, rather than relying only on the supervision of source categories to learn a discriminative representation, we generate pseudo-labels for the target samples in every batch and then harness the label information using a temporal supervised contrastive term, that pushes the examples from the same class close and the examples from different classes further apart <ref type="figure" target="#fig_1">(Figure 2</ref>: right). While our modified contrastive losses are motivated by the supervised contrastive learning <ref type="bibr" target="#b30">[31]</ref>, we use pseudo labels for exploiting shared action semantics and discriminative information from target domain, instead of using true labels as an alternative to supervised cross-entropy loss (which is not present for target samples). To the best of our knowledge, ours is the first work that successfully leverages contrastive learning in an unified framework to align cross-domain features while enhancing discriminabilty of the latent space for unsupervised video domain adaptation.</p><p>To summarize, the main contributions of our work are as follows:</p><p>? We introduce Contrast and Mix (CoMix), a new contrastive learning framework to learn discriminative invariant feature representations for unsupervised video domain adaptation. Overall, CoMix is simple and easy to implement which perfectly fits into modern mini-batch end-to-end training.</p><p>? We propose a novel extension to temporal contrastive loss by using background mixing that allows additional positives per anchor, thus adapting contrastive learning to leverage action semantics shared across both domains. We also integrate a supervised contrastive learning objective using pseudo label information from the target domain to enhance discriminabilty of the latent space. ? We conduct extensive experiments on several challenging benchmarks (UCF-HMDB <ref type="bibr" target="#b8">[9]</ref>, Jester <ref type="bibr" target="#b56">[57]</ref>, and Epic-Kitchens <ref type="bibr" target="#b53">[54]</ref>) for video domain adaptation to demonstrate the superiority of our approach over state-of-the-art methods. Our experiments show that CoMix delivers a significant performance increase over the compared methods, e.g., CoMix outperforms SAVA <ref type="bibr" target="#b11">[12]</ref> (ECCV'20) by 3.6% on UCF-HMDB <ref type="bibr" target="#b8">[9]</ref> and TA 3 N [9] (ICCV'19) by 9.2% on Jester <ref type="bibr" target="#b48">[49]</ref> benchmark respectively).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Action Recognition. Much progress has been made in developing a variety of ways to recognize video actions, by either applying 2D-CNNs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b83">84]</ref> or 3D-CNNs <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b77">78]</ref>. Many successful architectures are usually based on the two-stream model <ref type="bibr" target="#b70">[71]</ref>, processing RGB frames and optical-flow in two separate CNNs with a late fusion in the upper layers <ref type="bibr" target="#b29">[30]</ref>. SlowFast network <ref type="bibr" target="#b18">[19]</ref> employs two pathways for recognizing actions by processing a video at different frame rates. Mitigating background bias in action recognition has also been presented in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b37">38]</ref>. Despite remarkable progress, these models critically depend on large labeled datasets which impose challenges for cross-domain action recognition. In contrast, our work focuses on unsupervised domain adaptation for action recognition, with labeled data in source domain, but only unlabeled data in target domain. Unsupervised Domain Adaptation. Unsupervised domain adaptation has been studied from multiple perspectives (see reviews <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b84">85]</ref>). Representative works minimize some measurement of distributional discrepancy <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b73">74]</ref> or adopt adversarial learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b78">79]</ref> to generate domain-invariant features. Leveraging image translation <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b54">55]</ref> or style transfer <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b96">97]</ref> is also another popular trend in domain adaptation. Deep self-training that focus on iteratively training the model using both labeled source data and generated target pseudo labels have been proposed in <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b95">96]</ref>. Semi-supervised domain adaptation leveraging a few labeled samples from the target domain has also been proposed for many applications <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b66">67]</ref>. A very few methods have recently attempted video domain adaptation, using adversarial learning combined with temporal attention <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b56">57]</ref>, multi-modal cues <ref type="bibr" target="#b53">[54]</ref>, and clip order prediction <ref type="bibr" target="#b11">[12]</ref>. While existing video DA methods mainly rely on adversarial learning (which is often complicated and hard to train) in some form or other, they do not take any action semantics shared across domains into consideration. Our approach on the other hand, successfully leverages temporal contrastive learning to learn domain-invariant features while exploiting shared action semantics through background mixing for video domain adaptation. Recently, self-supervised tasks like predicting rotation and translation have been used for unsupervised domain adaptation and generalization, mainly for images <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b74">75]</ref>. By contrast, we focus on the more challenging problem of domain adaptation for human action recognition, where our goal is to align domains by learning consistent features representing different speeds of unlabeled videos. We further propose a temporal supervised contrastive loss to ensure discriminabilty by considering pseudo-labeling in an unified framework for video domain adaptation. Contrastive Learning. Contrastive representation learning is becoming increasingly attractive due to its great potential to leverage large amount of unlabeled images <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b51">52,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b55">56]</ref> and videos <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b82">83]</ref>. Speed of a video is investigated for self-supervised <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b91">92]</ref> and semi-supervised learning <ref type="bibr" target="#b71">[72,</ref><ref type="bibr" target="#b99">100]</ref> unlike the problem we consider in this paper. Recent works <ref type="bibr" target="#b89">[90,</ref><ref type="bibr" target="#b92">93]</ref> utilize contrastive learning with different augmentations for learning unsupervised representations of graph data. Contrastive learning has also been recently used in supervised settings, where labels are used to guide the choice of positive and negative pairs <ref type="bibr" target="#b30">[31]</ref>. While our approach is inspired by these, we propose a novel temporal contrastive learning framework with background mixing for video domain adaptation, which to our best knowledge has not been explored in the literature. Image Mixtures. Mixup regularization <ref type="bibr" target="#b94">[95]</ref> and its variants <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b93">94]</ref> that train models on virtual examples constructed as convex combinations of pairs of images and labels have been used to improve the generalization of neural networks. Very few methods apply Mixup in domain adaptation, but mainly to stabilize the domain discriminator <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b88">89,</ref><ref type="bibr" target="#b90">91]</ref> or to smoothen the predictions <ref type="bibr" target="#b47">[48]</ref>. Several works have recently leveraged the idea of different image mixtures <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b69">70]</ref> for improving contrastive representation learning. Our proposed background mixing can be regarded as an extension of this line of research by adding background of a video from one domain to a video from another domain, to explore shared semantics while learning domain-invariant features for action recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>Unsupervised video domain adaptation aims to improve the model generalization performance by transferring knowledge from a labeled source domain to an unlabeled target domain. Formally, we have a set of labelled source videos</p><formula xml:id="formula_0">D source = {(V i{s} , y i )} N S i=1 and a set of unlabelled target videos D target = {V i{t} } N T i=1</formula><p>, with a common label space L. Given these data sets, our goal is to learn a single model for action recognition that performs well on previously unseen target domain videos.  <ref type="figure">Figure 3</ref>: An Overview of our Approach. Given labeled videos in source domain and only unlabeled videos in target domain, CoMix adopts supervised learning on source videos, jointly with temporal contrastive learning on both domains to align features. Additional crossdomain contrastive supervision is obtained using background mixing across domains and using target pseudolabels for enhancing discriminability of the latent space.</p><p>CoMix provides a more simpler yet effective approach than adversarial learning for aligning both domains.</p><p>Approach Overview. <ref type="figure">Figure 3</ref> illustrates an overview of CoMix. Our action recognition model consists of a feature encoder F with a temporal graph encoder G. Given a video, the feature encoder F first extracts clip-level features, and then a graph encoder G utilizes those features to model intrinsic temporal relations for providing a robust encoded representation for action recognition. CoMix adopts supervised learning on the source videos, as the labels are available, jointly with two novel temporal contrastive learning loss terms to align the features for domain adaptation. Specifically, we maximize the similarity of the encoded representation of the fast version of a video (represented by f clips) with that of the slow version of the same video (represented by s clips, where s &lt; f ) as well as minimize the similarity of the representations of different videos within each of the two domains. However, as temporal contrastive loss treats each domain individually, we further add two new sets of synthetic videos that contain source videos mixed with target background and vice versa, respectively for introducing the background variations among the videos while keeping the action semantics intact. Finally, we generate pseudo-labels for the target videos in every mini-batch and utilize them using another temporal supervised contrastive term. This term contrasts target videos with the same pseudo-label as positives to learn features discriminative for the target domain. We now describe each of our proposed components individually in detail in the following subsections.</p><p>Video Representation. Capturing long-range temporal structure in videos is crucial for action recognition, which in turn affects the overall generalization performance of a model when adapting across domains. Thus, we adopt a graph convolutional neural network (G) on top of a 3D convolutional neural network (F) as our video feature encoder. Specifically, for a video V with n clips, the feature extractor F maps the clips into the corresponding sequence of features, which alone do not incorporate the rich temporal structure of the video. Therefore, we use the temporal graph encoder which constructs a fully connected graph on top of the clip-level features, with learnable edge weights through a parameterized adjacency matrix, as in <ref type="bibr" target="#b85">[86]</ref>. With these graph representations, we apply a graph convolutional neural network with three layers and finally perform average pooling over all the node features to output the encoded representation of the video V. In summary, the end-to-end network G(F(.)) : V ? R c takes a sequence of clips from a video as input and outputs confidence scores (logits) over the number of classes c for recognizing actions.</p><p>Temporal Contrastive Learning. Given video representations, our goal is to leverage contrastive self-supervised learning in both domains for unsupervised domain adaptation. To this end, we use temporal speed invariance in videos as a proxy task and enforce this with a pairwise contrastive loss. Specifically, our key idea is to represent videos in two different temporal speeds (fast and slow) to obtain their encoded representations and then consider the fast and slow version representations of the same video to constitute positive pairs, while versions from different videos constitute negative pairs. Formally, let us consider a mini-batch of B videos</p><formula xml:id="formula_1">{V 1 n , V 2 n , ..., V B n } with corresponding feature representations {z 1 n , z 2 n , ..., z B n },</formula><p>where each of the videos V i n is represented using n number of sampled clips. Let f be the number of clips used to represent the fast version of the videos (forwarded through the base branch), and s be that used for the slow version (forwarded through the auxiliary branch), with s &lt; f , as shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Given positive and negative pairs, the model is trained such that it learns to maximize agreement between positive pairs, while minimizing agreement between negative pairs. This is achieved by employing a temporal contrastive loss (L tcl ) as</p><formula xml:id="formula_2">L tcl (V i f , V i s ) = ? log h(z i f , z i s ) h(z i f , z i s ) + B j=1,j =i v?{s,f } h(z i f , z j v ) (1) where, h(u, v) = exp( u v u 2 v 2 /? )</formula><p>is the exponential of cosine similarity measure and ? is the temperature hyperparameter <ref type="bibr" target="#b9">[10]</ref>. We use f = 16, and choose s from {12, 8, 4} following a random uniform distribution in every training iteration where randomness encourages the model to learn from a variety of temporal speed variations to learn robust representations. Given unlabeled videos, we maximize similarity between encoded representations of the same video at two different speeds (fast and slow) as well as minimize similarity between different videos played at different speeds.</p><p>Background Mixing. As temporal contrastive loss treats each domain individually, it ignores shared action semantics which is vital for domain alignment. Thus, we propose a new perspective of temporal contrastive loss through background mixing, specifically to alleviate the crossdomain background shift, as seen in <ref type="figure" target="#fig_0">Figure 1</ref>. The basic idea is to obtain the background frames for the videos in one domain and mix it with the frames of the videos from the other domain. More details on how we extract the backgrounds are provided in the appendix. This introduces variation in each of the domains by adding new synthetic videos with the same action semantics as earlier, but possessing background from the other domain. Given two videos V i{s} ? D source and V i{t} ? D target with corresponding background frames (single image per video) as BG i{s} and BG i{t} , we obtain the synthetic videos in both domains by a convex combination of the background with each of the frames in the videos as follows.</p><formula xml:id="formula_3">V i{s} = (1 ? ?) ? V i{s} + ? ? BG i{t} V i{t} = (1 ? ?) ? V i{t} + ? ? BG i{s}<label>(2)</label></formula><p>where, ? is sampled from the uniform distribution [0, ?], V i{s} andV i{t} correspond to the video from source domain with target background and vice versa, respectively. The main operation in our proposed background mixing is to generate a synthetic video with background from the other domain while retaining the temporal action semantics intact. Since mixing background doesn't change the motion pattern of a video which actually defines an action, we assume both the original and mixed video to be of the same action class and go beyond single instance positives in Eq. 1 by adding additional positives per anchor, as in supervised contrastive learning <ref type="bibr" target="#b30">[31]</ref> (see <ref type="figure" target="#fig_1">Figure 2</ref> for an illustrative example). The modified temporal contrastive loss with background mixing (L bgm ) is defined as below: . Also, the loss is computed for all positive pairs in the mini-batch, i.e.,</p><formula xml:id="formula_4">L bgm (V i f , V i s ) = ? 1 |P(z i f )| p?P(z i f ) log h(z i f , p) p?P(z i f ) h(z i f , p) + B j=1,j =i v?{s,f } h(z i f , z j v ) + h(z i f ,? j v ) (3) where, P(z i f ) ? {z i s ,? i s ,? i f } is</formula><formula xml:id="formula_5">(V i f , V i s ), (V i s , V i f ), (V i f ,V i s ), and (V i s ,V i f ).</formula><p>Simultaneous minimization of L bgm in both source and target domains not only learns temporal dynamics but also helps to better align the features for video domain adaptation by leveraging action semantics shared across both domains. Our background mixing is especially effective in video domain adaptation as it enforces the model to be robust to domain changes (i.e., difference in background as shown in <ref type="figure" target="#fig_0">Figure 1</ref>) while leaving the action semantics intact. Further, it can also be adopted as a data augmentation strategy for improved generalization in standard video action recognition: we leave this as an interesting future work.</p><p>Incorporating Target Pseudo Labels. While temporal contrastive loss with background mixing helps in aligning the learned representations across the two domains, we cannot fully rely on source categories to learn features discriminative for target domain. Therefore, we propose to use a supervised contrastive loss <ref type="bibr" target="#b30">[31]</ref> over pseudo-labeled target samples, an extended version of temporal contrastive loss in Eqn. 1 to enhance discriminabilty by allowing many samples per anchor to be positive, so that videos of the same pseudo-label can be attracted to each other in the embedding space. Let A be the subset of videos assigned pseudo-labels using a confidence threshold, from a mini-batch of B videos, the supervised temporal contrastive loss for incorporating target pseudo-labels (L tpl ) is defined as</p><formula xml:id="formula_6">L tpl (V i f , V i s ) = ? 1 |P(z i f )| p?P(z i f ) log h(z i f , p) p?P(z i f ) h(z i f , p) + a?A,a =i v?{s,f } h(z i f , z a v ) (4) where, P(z i f ) ? {z p s , z p f : p ? A &amp;? p =? i } \ {z i f }</formula><p>is the set of all positives for video V i f and? i represent the pseudo-label for target video V i . Note that the set of positives (P(.)) includes all the target domain samples (fast and slow) classified as the same action class as that of the anchor (z i f ) through the pseudo labels. Following <ref type="bibr" target="#b100">[101]</ref>, we leverage a temporal ensemble prediction for a given video V i from the target domain to produce robust and better-calibrated version of pseudo-labels. Specifically, we obtain the encoded (logits) representations z i f and z i s from the base and auxiliary branch respectively and then compute the pseudo-label as? i = arg max k softmax(z i f used ), where z i f used represents the mean of both logits. We consider the class index k on which the model is most confident among c classes, provided it is higher than a confidence threshold.</p><p>Optimization. Besides the losses L bgm and L tpl , we minimize the standard supervised cross-entropy loss (L ce ) on the labelled source videos as follows.</p><formula xml:id="formula_7">L ce (V i{s} , y i ) = ? c k=1 (y i ) k log(G(F(V i{s} ))) k<label>(5)</label></formula><p>Overall, the loss function for training our model involving both source and target domain data is,</p><formula xml:id="formula_8">L CoM ix = L {s} ce + ? bgm (L {s} bgm + L {t} bgm ) + ? tpl L {t} tpl<label>(6)</label></formula><p>where ? bgm and ? tpl are weights to balance the impact of individual loss terms. To reduce the number of hyper-parameters, we use the same weight ? bgm for both L {s} bgm and L {t} bgm . Notably, for the semi-supervised domain adaptation setting, we also use supervised cross-entropy loss for the few labeled target domain videos in addition to the source domain videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Datasets. We evaluate the performance of our approach using several publicly available benchmark datasets for video domain adaptation, namely UCF-HMDB <ref type="bibr" target="#b7">[8]</ref>, Jester <ref type="bibr" target="#b56">[57]</ref>, and Epic-Kitchens <ref type="bibr" target="#b53">[54]</ref>. UCF-HMDB (assembled by authors in <ref type="bibr" target="#b7">[8]</ref>) is an overlapped subset of the original UCF <ref type="bibr" target="#b72">[73]</ref> and HMDB datasets <ref type="bibr" target="#b33">[34]</ref>, containing 3, 209 videos across 12 classes. Jester (assembled by authors in <ref type="bibr" target="#b56">[57]</ref>) is a large-scale cross-domain dataset that contains videos of humans performing hand gestures <ref type="bibr" target="#b48">[49]</ref> from two domains, namely Source and Target that contain 51, 498 and 51, 415 video clips respectively across 7 classes. Epic-Kitchens (assembled by authors in <ref type="bibr" target="#b53">[54]</ref>) is a challenging egocentric dataset that consists of videos across 8 largest action classes from three domains, namely D1, D2 and D3, corresponding to P08, P01 and P22 kitchens on the full Epic-Kitchens dataset <ref type="bibr" target="#b13">[14]</ref>. We use the standard training and testing splits provided by the authors in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b53">54]</ref> to conduct our experiments on each dataset. More details about the datasets can be found in the appendix.</p><p>Baselines. We compare our approach with the following baselines. (1) source only (a lower bound) and supervised target only (an upper bound) baselines that trains the network using labeled source data and labeled target data respectively, (2) popular UDA methods based on adversarial learning (e.g., DANN <ref type="bibr" target="#b20">[21]</ref>, and ADDA <ref type="bibr" target="#b78">[79]</ref>), (3) existing video domain adaptation methods, including SAVA <ref type="bibr" target="#b11">[12]</ref>, TA 3 N [9], ABG <ref type="bibr" target="#b44">[45]</ref> and TCoN <ref type="bibr" target="#b56">[57]</ref>. We also compare with Source + Target (which simply uses all labelled data available to it to train the network) and ENT <ref type="bibr" target="#b66">[67]</ref> in semi-supervised domain adaptation experiments. We directly quote the numbers reported in published papers when possible and use source code made publicly available by the authors of TA 3 N [9] on both Jester and Epic-Kitchens.</p><p>Implementation Details. Following <ref type="bibr" target="#b11">[12]</ref>, we use I3D <ref type="bibr" target="#b3">[4]</ref> as the backbone feature encoder network, initialized with Kinetics pre-trained weights. For the temporal graph encoder, we use a 3-layer GCN similar to <ref type="bibr" target="#b85">[86]</ref>. We follow the standard 'pre-train then adapt' procedure used in prior works <ref type="bibr" target="#b78">[79,</ref><ref type="bibr" target="#b11">12]</ref> and train the model with only source data to provide a warmstart before the proposed approach is employed. The dimension of the features extracted from the I3D encoder is 1024 which is the same as the node-feature dimension of the initial layer of the GCN. The final layer of the GCN has its node-feature dimension same as the number of action classes in a dataset and uses a mean aggregation strategy to output the logits. We use a clip-length of 8-frames and train all the models end-to-end using SGD with a momentum of 0.9 and a weight decay of 1e-7. We use an initial learning rate of 0.001 for the I3D and 0.01 for the GCN in all our experiments. We use a batch size of 40 equally split over the two domains, where each batch consists of n clips from the same video, where n is 16 for the fast version (f ) and 12, 8, or 4 for the slow version (s). For inference, we use 16 uniformly sampled clips per video and use the base branch of the model to recognize the action. The temperature parameter is set to ? = 0.5. We extract backgrounds from videos using temporal median filtering <ref type="bibr" target="#b61">[62]</ref> and empirically set ? = 0.5 for background mixing. We use a pseudo-label threshold of 0.7 in all our experiments and smooth the cross-entropy loss with = 0.1, following <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b52">53]</ref>. We set ? bgm and ? tpl from {0.01, 0.1} depending on the dataset. We report the average action recognition accuracy over 3 random trials. We use 6 NVIDIA Tesla V100 GPUs for training all our models. Results on UCF-HMDB. <ref type="table" target="#tab_0">Table 1</ref> shows results of our method and other competing approaches on UCF-HMDB dataset. Our CoMix framework achieves the best average performance of 90.3%, which is about 2.2% more than the previous state-of-the-art performance on this dataset. While comparing with the recent method, SAVA <ref type="bibr" target="#b11">[12]</ref> using the same I3D backbone, CoMix obtains 4.5% and 2.7% improvement on UCF?HMDB and HMDB?UCF task respectively, without relying on frame attention or adversarial learning. These improvements clearly show that our temporal graph contrastive learning with background mixing is not only able to better leverage the temporal information but also shared action semantics, essential for effective video domain adaptation. In summary, CoMix outperforms all the existing video DA methods on UCF-HMDB, showing the efficacy of our approach in learning more transferable features for cross-domain action recognition without using any target labels.</p><p>Results on Jester and Epic-Kitchens. On the large-scale Jester dataset, our proposed approach, CoMix also outperforms other DA approaches by increasing the Source Only (no adaptation) accuracy from 51.5% to 64.7%, as shown in <ref type="table" target="#tab_1">Table 2</ref> (left). In particular, our approach achieves an absolute improvement of 9.2% over TA 3 N [9], which corroborates the fact that CoMix can well handle not only the appearance gap but also the action gap present on this dataset (e.g., for the action class "rolling hand", source domain contains videos of "rolling hand forward", while the target domain only consists of videos of "rolling hand backward").  Semi-supervised Domain Adaptation. To further study the robustness of our proposed approach, we extend the unsupervised domain adaptation to a semi-supervised setting, where one (1-shot) and three target labels (3shot) per class are available for training. <ref type="table" target="#tab_3">Table 3</ref> shows that our simple approach consistently outperforms the adversarial DA methods (DANN <ref type="bibr" target="#b20">[21]</ref>, and ADDA <ref type="bibr" target="#b78">[79]</ref>) including the semi-supervised method, ENT <ref type="bibr" target="#b66">[67]</ref>, on both UCF-HMDB and Jester datasets. Remarkably, CoMix with three target labels per class improves the performance of Source + Target baseline from 93.7% to 96.6%, which is only 0.2% lower than the supervised target upper bound (in <ref type="table" target="#tab_0">Table 1</ref>) on HMDB?UCF task (96.6% vs 96.8%). These results well demonstrate the utility of our proposed approach in many practical applications where annotating a few videos per class is typically possible and therefore worth doing given the boost it provides.</p><p>Effectiveness of Individual Components. As seen from <ref type="table" target="#tab_5">Table 4</ref>, the vanilla temporal contrastive learning (TCL) achieves an average accuracy of 85.8% on UCF-HMDB while 57.5% on Jester (1 st row), which is already better than DANN <ref type="bibr" target="#b20">[21]</ref>, and ADDA <ref type="bibr" target="#b78">[79]</ref> (ref.   Comparison with Different MixUp Strategies. We explore the effectiveness of background mixing by comparing with different MixUp strategies ( Effect of Background Pseudo-labels. We investigate the effect of pseudo-labels on background mixed videos (i.e., both videos considered to be of same action class while creating positives) by simply adding them as unlabeled videos without any modification to the contrastive objective in Eq. 1.</p><p>CoMix without background pseudo-labels decreases the performance from 90.3% to 89.0% (?1.3%: <ref type="table" target="#tab_8">Table 6</ref>), showing its effectiveness in leveraging action semantics shared across both domains.</p><p>Effect of Source Contrastive Learning. CoMix adopts contrastive learning on both source and target domains, although we already have supervised cross-entropy loss on source videos. We observe that applying contrastive learning on target domain only, by removing source contrastive objective L {s} bgm from Eq. 6, lowers down the performance from 90.3% to 88.4% (?1.9%) on UCF-HMDB <ref type="table" target="#tab_8">(Table 6</ref>). This shows the importance of training the model using the same temporal invariance objective on both domains simultaneously to achieve effective alignment across domains. Effect of Random Speed Invariance. We remove randomness in video speed from the auxiliary branch of our temporal contrastive learning framework and observe that CoMix (with 16 clips in the base branch and only 8 clips in the auxiliary branch) leads to an average top-1 accuracy of 89.6% compared to 90.3% (?0.7%: <ref type="table" target="#tab_8">Table 6</ref>), showing the importance of random speed invariance in learning robust features.</p><p>Self-Training vs Supervised Contrastive Learning. We directly use self-training that uses crossentropy loss on target pseudo labels instead of L {t} tpl and find that the average performance drops to 88.7% on UCF-HMDB, indicating the advantage of supervised contrastive objective in enhancing discriminability of the latent space by successfully leveraging label information from target domain.  <ref type="figure">Figure 5</ref>: Feature Visualizations using t-SNE. Plots show visualization of our approach with different components on UCF?HMDB task. Blue and red dots represent source and target data respectively. Features for both target and source domain become progressively discriminative and improve from left to right by adoption of our novel components within a temporal contrastive learning framework. Best viewed in color.</p><p>Representation on Baseline Methods: Additionally, in <ref type="table" target="#tab_9">Table 7</ref> we compare with domain adversarial adaptation methods DANN <ref type="bibr" target="#b20">[21]</ref> and TA3N <ref type="bibr" target="#b8">[9]</ref> including the Source only baseline with GCN feature representation on both UCF-HMDB and Jester datasets. CoMix improves the Source only accuracy by 5.2% and 10.7% respectively on UCF-HMDB and Jester datasets. Furthermore, CoMix outperforms DANN <ref type="bibr" target="#b20">[21]</ref> with the same GCN equipped as ours, on both datasets (+7.1%, +1.8%, respectively) showing its effectiveness over adversarial learning in aligning features for video domain adaptation. TA3N <ref type="bibr" target="#b8">[9]</ref> performs very poorly (62.3% and 51.7%) when equipped additionally with graph representations. We believe this is because TA3N already utilizes Temporal Relational Network <ref type="bibr" target="#b97">[98]</ref> for modeling temporal relations, which probably hinders in learning GCN features for successful domain adaptation in videos. (c) Alternatives for Graph Representation: We replace GCN using MLP/LSTM of similar complexity and notice that both alternatives are inferior to GCN on UCF-HMDB (MLP: 88.1%, LSTM: 84.3%, GCN: 90.3%), which shows the effectiveness of GCN in our contrastive learning framework for capturing the temporal dependencies, essential for video domain adaptation.</p><p>Effect of Background Extraction Method. We experiment with a different background extraction strategy <ref type="bibr" target="#b98">[99]</ref> that uses Gaussian Mixture Models (GMM) to extract the backgrounds and observe that the very simple and fast strategy based on temporal median filtering <ref type="bibr" target="#b61">[62]</ref> outperforms GMM by 2.3% on average on UCF-HMDB (UCF?HMDB: 85.3% vs 86.7%, HMDB?UCF: 90.7% vs 93.9%, Avg: 88.0% vs 90.3%). Note that our CoMix framework is agnostic to the method used for background extraction and can be incorporated with any other background extraction techniques for videos, e.g., learnable background segmentation strategies such as <ref type="bibr" target="#b86">[87,</ref><ref type="bibr" target="#b58">59]</ref>.</p><p>Feature Visualizations. We use t-SNE <ref type="bibr" target="#b45">[46]</ref> to visualize the features learned using different components of our CoMix framework. As seen from <ref type="figure">Figure 5</ref>, alignment of domains including discriminability improves as we adopt "TCL" and "BGM" to the vanilla Source only model. The best results are obtained when all the three components "TCL", "BGM" and "TPL" i.e., CoMix are added and trained using an unified framework (Eq. 6) for unsupervised video domain adaptation. Additional results and analysis including more qualitative examples are included in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we introduce a new end-to-end temporal contrastive learning framework to bridge the domain gap by learning consistent features representing two different speeds of the unlabeled videos. We also propose two novel extension to temporal contrastive loss by using background mixing and target pseudo-labels, that allows additional positive(s) per anchor, thus adapting contrastive learning to leverage cross-domain action semantics and label information from the target domain respectively in an unified framework, for learning discriminative invariant features. We demonstrate the effectiveness of our approach on three standard datasets, outperforming several competing methods. Broader Impact. Our research can help reduce burden of collecting large-scale supervised data in many real-world applications of human action recognition by transferring knowledge from auxiliary datasets. The positive impact that our work could have on society is in making technology more accessible for institutions and individuals that do not have rich resources for collecting and annotating large-scale video datasets. Negative impacts of our research are difficult to predict, however, it shares many of the pitfalls associated with standard deep learning models such as susceptibility to adversarial attacks and lack of interpretablity. Other adverse effects could be potential attrition in jobs in certain sectors of economy where fewer employees (security guards, nurses, etc.) are needed to monitor human activities as a result of wider adoption of automated video recognition systems. Acknowledgements. This work was partially supported by the ISIRD Grant EEE.</p><p>while Swiping Right, Swiping Down to be in the target domain. Different sub-actions are put into different domains in order to maximize the domain discrepancy, as stated by <ref type="bibr" target="#b56">[57]</ref>. The resulting cross-domain dataset possesses 7 action classes, namely, Push and Pull, Rolling Hand, Sliding Two Fingers, Swiping, Thumps Up and Down, Turning Hand, and Zooming In and Out. For Jester, we have only a single transfer task i.e. Jester(S)?Jester(T). The detailed composition of the action classes is shown in <ref type="table" target="#tab_10">Table 9</ref> with a class-wise distribution of videos depicted in <ref type="figure" target="#fig_6">Figure 7</ref>.</p><p>The dataset is publicly available to download at: https://20bn.com/datasets/jester. Epic Kitchens Dataset. The Epic-Kitchens <ref type="bibr" target="#b13">[14]</ref> dataset is a challenging egocentric dataset consisting of videos (action segments) capturing daily activities performed in kitchens. The three largest kitchens, namely, P01, P22, and P08 form the three domains D1, D2, and D3, respectively. Moreover, the 8 largest action classes, namely, take, put, open, wash, close, cut, pour, and mix are used to form the dataset for the domain adaptation setting, following <ref type="bibr" target="#b53">[54]</ref>. The dataset has 1543 training videos and 435 test videos from D1, 2495 training videos and 750 test videos from D2, and 3897 training videos and 974 test videos from D3. The class-wise distribution is shown in <ref type="figure" target="#fig_7">Figure 8</ref>. It can be seen that the dataset possesses high imbalance which makes it even more challenging.</p><p>The dataset is publicly available to download at: https://epic-kitchens.github.io/2021.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Temporal Graph Encoder</head><p>In this section, we provide the detailed description of the temporal graph encoder that we used for representing videos in our contrastive learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Graph Convolutional Network</head><p>The graph convolutional network (GCN) was originally proposed by <ref type="bibr" target="#b31">[32]</ref> for node classification on graph structured data. Given an input graph X ? R N ?d with N number of nodes with each node as a feature-vector of dimension d, the layer-wise propagation rule for a multi-layer GCN is:</p><formula xml:id="formula_9">H (l+1) = ?(D ? 1 2?D ? 1 2 H (l) W (l) )<label>(7)</label></formula><p>where, H (l) ? R N ?d l is the activation graph of the l th layer with node feature dimension d l ; H (0) = X.? = A + I N is the adjacency matrix of X with added self-connections through the indentity matrix I N .D ii = j? ij is the diagonal matrix used for normalization of?, and W (l) is the layer-specific trainable weight matrix. ?(.) denotes the activation function, e.g. ReLU(.).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Videos as Similarity Graphs</head><p>Motivated by the importance of capturing long-range temporal structure in videos for action recognition and hence in cross-domain adaptation, we adopt a similarity graph to represent a video in our framework, as in <ref type="bibr" target="#b85">[86]</ref>. Given a video V n = {v 1 , v 2 , ..., v n } with n clips, with the corresponding clip-level feature vector representations as Z n = {z 1 , z 2 , ..., z n }, extracted by the feature encoder F, each of dimension d. We construct a fully-connected graph X with n nodes from Z by considering the pairwise similarity or affinity between two feature vectors as:</p><formula xml:id="formula_10">F (z i , z j ) = ?(z i ) ? (z j )<label>(8)</label></formula><p>where, ?(.) and ? (.) represent two different transformation functions of the original feature vectors, defined as ?(z) = wz and ?(z) = w z. Here, the transformations are parameterized with the weights w and w of dimension d ? d each. Using such transformations helps learn the long-range correlations between the feature vectors to harness the rich temporal information of the video. We get a similarity matrix A sim of dimension n ? n by computing the affinity for all the possible pairs, using Eq. 8. The matrix is then normalized using a softmax function as:</p><formula xml:id="formula_11">A sim ij = exp(F (z i , z j )) n j=1 exp(F (z i , z j ))<label>(9)</label></formula><p>The normalized matrix A sim is now considered as the adjacency matrix for the similarity graph, allowing us to learn the edge-weights between the nodes through back-propagation, by the help of the learnable weights w and w . Hence, the resulting similarity graph convolutional network has the following propagation rule, similar to Eq. 7:</p><formula xml:id="formula_12">H (l+1) = ?(A sim(l) H (l) W (l) )<label>(10)</label></formula><p>where, H (0) = X, and A sim(l) is the affinity/adjacency matrix computed using the node features of the l th layer, similar to <ref type="bibr" target="#b85">[86]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Scalability with Graph Convolutions</head><p>The learning strategy for graph representation used in CoMix ensures that the number of learnable parameters are independent of the number of graph nodes. As described in detail above, we construct the fully connected graph with the edge weights (pairwise similarity) obtained using two different transformation functions, and on the clip-level feature vectors (where each feature vector represents a node). This strategy makes the number of trainable parameters independent of the number of nodes in a GCN layer and hence, independent of the number of clips used for a video. While fully connected graph convolutions will increase the computation with longer clip sequences, we can adopt sparse video sampling <ref type="bibr" target="#b6">[7]</ref> or techniques like <ref type="bibr" target="#b87">[88]</ref> to tradeoff computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Background Extraction Details</head><p>In this section, we provide more details about the background extraction including qualitative samples used in our temporal contrastive learning framework.</p><p>Temporal Median Filter. Temporal median filtering (TMF) is one of the most simple, intuitive, and fast methods for background generation. It has proven to be successful and commonly adopted in several recent deep learning pipelines <ref type="bibr" target="#b76">[77,</ref><ref type="bibr" target="#b46">47]</ref>. For videos, a pixel-wise temporal median filter is applied on the sequence of frames to obtain the corresponding background. The method is designed with the principle that for a given pixel location, in a sequence of frames, the most frequently repeated intensity along the temporal direction is most likely to be the background value for that pixel <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b39">40]</ref>. It does so by computing the pixel-wise median values along the temporal direction. We adopt this method for extracting backgrounds for our framework because of its simplicity and effectiveness. It must be noted that the CoMix framework is agnostic to the method used for background extraction and can be incorporated with any other background extraction techniques for videos. <ref type="figure">Figure 9</ref> shows some representative video clips randomly sampled from both the domains of the UCF-HMDB along with the corresponding background frame extracted using temporal median filtering. Note that we extract a single background frame per video from one domain and then mix it with all the frames of a video from the other domain to generate synthetic background mixed videos. The addition of a static background frame to all frames of a video does not hinder the temporal action dynamics (motion patterns) possessed by the video. We validate this hypothesis by obtaining optical flow for a given video before and after performing background mixing, and observe no significant change in them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Additional Experimental Results</head><p>Effect of Source-only Model Initialization. We follow the standard 'pre-train then adapt' procedure used in prior works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref> and train the model with only source data to provide a warmstart before our approach is trained. However, in order to understand the contribution of source-only model initialization, we trained the models with the default random initialization keeping all the other hyperparameters same. The average performance dropped to 86.4% (-3.9%) on UCF-HMDB dataset. This validates that the source-only initialization plays an important role in providing a proper warmstart to the models which leads to an effective optimization, in consistent with prior works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>.</p><p>Effect of Target Pseudo-label Threshold. In <ref type="table" target="#tab_0">Table 10</ref>, we study the sensitivity of the final performance with respect to the pseudo label threshold on the UCF-HMDB dataset and notice that the performance of CoMix is quite stable with respect to this parameter (best performance at threshold set to 0.7). The slight decrease in performance with PL = 0.9 is understandable since very few target videos are getting selected as additional positives for the supervised contrastive loss. Effect of Mixed Backgrounds. We tried a variant of background mixing in which the backgrounds from both the domains are first convexly combined to form a mixedbackground, which sort of represents a generalized background for both the domains. The obtained mixedbackground is then used for the background mixing component and is mixed with the videos from both the domains. This alternate background mixing strategy provided an average performance of 89.4% on the UCF-HMDB dataset, which is 0.9% lower than the cross-domain background mixing (i.e., adding background from one domain to the other) used in the proposed approach.</p><p>Convergence and Multiple Seeds. The convergence of the proposed approach varies with dataset and task complexity ranging from 3000 iterations for HMDB?UCF dataset to 7000 iterations for UCF?HMDB and EpicKitchens datasets. We observe that the convergence is fairly stable across different seeds and report the average performance over three runs with different random seeds. To quantify, the standard deviations in performance obtained for UCF-HMDB, Jester and EpicKitchens datasets are 0.3, 0.1 and 0.2 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Clip</head><p>Background using TMF Sample Clips from the UCF domain of UCF-HMDB dataset Sample Clips from the HMDB domain of UCF-HMDB dataset <ref type="figure">Figure 9</ref>: Background Extraction. The figure shows some representative video clips from UCF-HMDB dataset with corresponding extracted background using temporal median filtering (TMF). Best viewed in color.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Additional Feature Visualizations</head><p>In this section, we provide additional t-SNE <ref type="bibr" target="#b45">[46]</ref> plots to visualize the features learned using different components of our CoMix framework. We choose the Source only model as a vanilla method and add different components one-by-one to visualize their contributions in learning discriminative features  <ref type="figure" target="#fig_0">Figure 10</ref>: Feature Visualizations using t-SNE. Plots show visualization of our approach with different components on HMDB?UCF task from UCF-HMDB. Blue and red dots represent source and target data respectively. Features for both target and source domain become progressively discriminative and improve from left to right by adoption of our novel components within a contrastive learning framework. Best viewed in color.</p><p>for video domain adaptation. In the main paper, we have provided the plots for the UCF?HMDB task from the UCF-HMDB dataset (refer to <ref type="figure">Figure 5</ref> in main paper). Here we provide the plots for the HMDB?UCF task in <ref type="figure" target="#fig_0">Figure 10</ref>. As can be seen from <ref type="figure" target="#fig_0">Figure 10</ref>, alignment of domains including discriminability improves as we adopt "TCL" and "BGM" to the vanilla Source only model. The best results are obtained when all three components "TCL", "BGM" and "TPL" i.e., CoMix are added and trained using an unified framework (Eq. 6 in main paper) for unsupervised video domain adaptation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Background Mixing. Top row shows two representative videos from the source and target domain respectively. Both videos capture the same action "walking" with different backgrounds. Bottom row shows videos obtained after mixing target background with source video and vice versa.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Temporal Contrastive Learning with Background Mixing and Target Pseudo-labels. Temporal contrastive loss (left) contrasts a single temporally augmented positive (same video, different speed) per anchor against rest of the videos in a mini-batch as negatives. Incorporating background mixing (middle) provides additional positives per anchor possessing same action semantics with a different background alleviating background shift across domains. Incorporating target pseudo-labels (right) additionally enhances the discriminabilty by contrasting the target videos with the same pseudo-label as positives against rest of the videos as negatives.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Temporal Contrastive Loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>the set of positives for the anchor z i f , and? i s/f represent the feature representation of the corresponding background-mixed video depending on the domain to which V i belongs. Note that for anchor z i f , there are 3 positive pairs: (a) slow version of the mixed video (? i s ), (b) fast version of the mixed video (? i f ), and (c) slow version of the original video (z i s )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Class-wise distribution of videos for UCF-HMDB. The bar chart shows the distribution of videos across the 12 action classes of the UCF-HMDB dataset. Best viewed in color with zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Class-wise distribution of videos for Jester. The bar chart shows the distribution of videos across the 7 action classes of the Jester dataset for the source and the target domains. Best viewed in color with zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Class-wise distribution of videos for Epic-Kitchens. The bar chart shows distribution of videos across 8 action classes of Epic-Kitchens for three domains D1, D2, and D3. Best viewed in color with zoom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results on UCF-HMDB Dataset. CoMix establishes new state-of-the-art for unsupervised video domain adaptation on UCF-HMDB, by significantly outperforming existing methods.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="3">UCF?HMDB HMDB?UCF Average</cell></row><row><cell>DANN [21]</cell><cell>ResNet-101</cell><cell>75.3</cell><cell>76.4</cell><cell>75.8</cell></row><row><cell>JAN [44]</cell><cell>ResNet-101</cell><cell>74.7</cell><cell>79.3</cell><cell>77.0</cell></row><row><cell>AdaBN [37]</cell><cell>ResNet-101</cell><cell>75.5</cell><cell>77.4</cell><cell>76.4</cell></row><row><cell>MCD [68]</cell><cell>ResNet-101</cell><cell>74.4</cell><cell>79.3</cell><cell>76.8</cell></row><row><cell>TA 3 N [9]</cell><cell>ResNet-101</cell><cell>78.3</cell><cell>81.8</cell><cell>80.1</cell></row><row><cell>ABG [45]</cell><cell>ResNet-101</cell><cell>79.1</cell><cell>85.1</cell><cell>82.1</cell></row><row><cell>TCoN [57]</cell><cell>ResNet-101</cell><cell>87.2</cell><cell>89.1</cell><cell>88.1</cell></row><row><cell>Source Only</cell><cell>I3D</cell><cell>80.3</cell><cell>88.8</cell><cell>84.5</cell></row><row><cell>DANN [21]</cell><cell>I3D</cell><cell>80.7</cell><cell>88.0</cell><cell>84.3</cell></row><row><cell>ADDA [79]</cell><cell>I3D</cell><cell>79.1</cell><cell>88.4</cell><cell>83.7</cell></row><row><cell>TA 3 N [9]</cell><cell>I3D</cell><cell>81.4</cell><cell>90.5</cell><cell>85.9</cell></row><row><cell>SAVA [12]</cell><cell>I3D</cell><cell>82.2</cell><cell>91.2</cell><cell>86.7</cell></row><row><cell>CoMix</cell><cell>I3D</cell><cell>86.7</cell><cell>93.9</cell><cell>90.3</cell></row><row><cell>Supervised Target</cell><cell>I3D</cell><cell>95.0</cell><cell>96.8</cell><cell>95.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Results on Jester and Epic-Kitchens Datasets. CoMix outperforms TA 3 N [9] by 9.2% on the challenging Jester dataset. On Epic-Kitchens, CoMix achieves the best performance on 5 out of 6 transfer tasks including the best average performance among all compared methods.</figDesc><table><row><cell>Method</cell><cell>Backbone</cell><cell cols="7">Jester Source?Target D2?D1 D3?D1 D1?D2 D3?D2 D1?D3 D2?D3 Epic-Kitchens</cell><cell>Average</cell></row><row><cell>Source Only</cell><cell>I3D</cell><cell>51.5</cell><cell>35.4</cell><cell>34.6</cell><cell>32.8</cell><cell>35.8</cell><cell>34.1</cell><cell>39.1</cell><cell>35.3</cell></row><row><cell>DANN [21]</cell><cell>I3D</cell><cell>55.4</cell><cell>38.3</cell><cell>38.8</cell><cell>37.7</cell><cell>42.1</cell><cell>36.6</cell><cell>41.9</cell><cell>39.2</cell></row><row><cell>ADDA [79]</cell><cell>I3D</cell><cell>52.3</cell><cell>36.3</cell><cell>36.1</cell><cell>35.4</cell><cell>41.4</cell><cell>34.9</cell><cell>40.8</cell><cell>37.4</cell></row><row><cell>TA 3 N [9]</cell><cell>I3D</cell><cell>55.5</cell><cell>40.9</cell><cell>39.9</cell><cell>34.2</cell><cell>44.2</cell><cell>37.4</cell><cell>42.8</cell><cell>39.9</cell></row><row><cell>CoMix</cell><cell>I3D</cell><cell>64.7</cell><cell>38.6</cell><cell>42.3</cell><cell>42.9</cell><cell>49.2</cell><cell>40.9</cell><cell>45.2</cell><cell>43.2</cell></row><row><cell>Supervised Target</cell><cell>I3D</cell><cell>95.6</cell><cell>57.0</cell><cell>57.0</cell><cell>64.0</cell><cell>64.0</cell><cell>63.7</cell><cell>63.7</cell><cell>61.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc></figDesc><table /><note>(right) summarizes the results on Epic-Kitchens, which is another challenging dataset consisting of total 6 transfer tasks with a large imbalance across different action classes. Overall, CoMix obtains the best on 5 tasks including the best average performance of 43.2%, compared to only 35.3% and 39.9% achieved by the source only and TA 3 N [9] respectively. While the improvements achieved by our approach are encouraging on both Jester and Epic-Kitchens, the accuracy gap between CoMix and supervised target is still significant (30.9% on Jester and 18.3% on Epic-Kitchens), which highlights the great potential for improvement in future for unsupervised video domain adaptation. Comparison with MM-SADA [54]. MM-SADA[54] is another state-of-the-art approach for video domain adaptation that leverages the idea of using multi-modal (RGB and Optical flow) data to learn better domain invariant representations. The approach has two main components: adversarial learning and multi-modal supervision. While CoMix does not use optical flow features anywhere, the RGB-only version of MM-SADA still uses optical flow features for the multi-modal self-supervision. Interestingly, CoMix (43.2%) shows very competitive performance using only RGB features when compared to the above (43.9%) on the Epic-Kitchens dataset. Additionally, we train MM-SADA (RGB-only) (but perform multimodal supervision using both RGB and flow following the original paper [54]) on UCF-HMDB dataset and notice that CoMix outperforms it by a margin of 3% on an average (UCF ? HMDB: 82.2% vs 86.7%, HMDB ? UCF: 91.2% vs 93.9%, Avg: 86.7% vs 90.3%), showing its effectiveness in unsupervised video domain adaptation.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Semi-Supervised Domain Adaptation on UCF-HMDB and Jester Datasets. CoMix significantly outperforms all the compared methods in both one-shot and three-shot settings.</figDesc><table><row><cell>Method</cell><cell cols="6">UCF?HMDB HMDB?UCF Jester(S) ? Jester(T) 1-shot 3-shot 1-shot 3-shot 1-shot 3-shot</cell></row><row><cell cols="2">Source + Target 83.2</cell><cell>85.8</cell><cell>90.3</cell><cell>93.7</cell><cell>53.8</cell><cell>55.0</cell></row><row><cell>DANN [21]</cell><cell>85.4</cell><cell>86.9</cell><cell>92.1</cell><cell>93.1</cell><cell>55.1</cell><cell>59.9</cell></row><row><cell>ADDA [79]</cell><cell>83.6</cell><cell>86.3</cell><cell>91.2</cell><cell>93.0</cell><cell>59.5</cell><cell>61.3</cell></row><row><cell>ENT [67]</cell><cell>85.6</cell><cell>88.6</cell><cell>92.8</cell><cell>95.8</cell><cell>58.6</cell><cell>61.5</cell></row><row><cell>CoMix</cell><cell cols="4">88.4 93.1 95.4 96.6</cell><cell>65.3</cell><cell>69.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1</head><label>1</label><figDesc>,2), showing its effectiveness over adversarial learning in aligning features. While both background mixing (BGM) and incorporation of target pseudo-labels (TPL) individually improves the performance over TCL (+2.9%, +5.6% using BGM and +1.9%, +5.4% using TPL, respectively), addition of both of them leads to the best average performance of 90.3% on UCF-HMDB dataset and 64.7% on the Jester dataset. This corroborates the fact that both cross-domain action semantics (through BGM) and discriminabilty (through TPL) of the latent space play crucial roles in video domain adaptation in addition to the vanilla contrastive learning for aligning features.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Ablation Study on UCF-HMDB and Jester. TCL: Temporal Contrastive Learning, BGM: Background Mixing, TPL: Target Pseudo-Labels.</figDesc><table><row><cell cols="3">TCL BGM TPL U?H H?U Average Jester(S)?Jester(R)</cell></row><row><cell>83.3 88.4</cell><cell>85.8</cell><cell>57.5</cell></row><row><cell>86.2 91.2</cell><cell>88.7</cell><cell>63.1</cell></row><row><cell>83.5 91.9</cell><cell>87.7</cell><cell>62.9</cell></row><row><cell cols="2">86.7 93.9 90.3</cell><cell>64.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison with MixUp Strategies.</figDesc><table><row><cell cols="4">Background mixing outperforms other alternatives in</cell></row><row><cell cols="4">leveraging shared action semantics on UCF-HMDB.</cell></row><row><cell>Method</cell><cell cols="3">U?H H?U Average Jester(S)?Jester(R)</cell></row><row><cell>Gaussian Noise</cell><cell>84.7 90.6</cell><cell>87.6</cell><cell>54.3</cell></row><row><cell>Video MixUp</cell><cell>85.1 91.7</cell><cell>88.4</cell><cell>62.2</cell></row><row><cell>Video CutMix</cell><cell>84.6 92.1</cell><cell>88.3</cell><cell>58.6</cell></row><row><cell cols="3">Background Mixing 86.7 93.9 90.3</cell><cell>64.7</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5</head><label>5</label><figDesc></figDesc><table /><note>): (a) Gaussian Noise: adding White Gaussian Noise to videos in both domains; (b) Video MixUp [95]: directly mixing one video with another from a different domain, as in images; (c) Video CutMix [94]: randomly replacing a region of a video with another region from the other domain. The proposed way of generating synthetic videos by mixing background of a video from one domain to a video from another domain, outperforms all three alternatives on UCF-HMDB as well as on the more challenging Jester dataset. Note that while both MixUp and CutMix destroy motion pattern of original video, background mixing keeps semantic consistency without changing the temporal dynamics.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Ablation Study on Contrastive Learning.</figDesc><table><row><cell>Method</cell><cell cols="2">U?H H?U Average</cell></row><row><cell>CoMix</cell><cell cols="2">86.7 93.7 90.3</cell></row><row><cell>-w/o Background Pseudo-labels</cell><cell>85.8 92.2</cell><cell>89.0</cell></row><row><cell cols="2">-w/o Source Contrastive Learning 85.1 91.8</cell><cell>88.4</cell></row><row><cell>-w/o Random Speed Invariance</cell><cell>86.4 92.8</cell><cell>89.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Baseline Comparisons w/ GCN Representations on UCF-HMDB and Jester Datasets.</figDesc><table><row><cell cols="4">Method (w/ GCN) U?H H?U Average Jester(S)?Jester(R)</cell></row><row><cell>Source Only</cell><cell>82.5 87.7</cell><cell>85.1</cell><cell>54.0</cell></row><row><cell>DANN [21]</cell><cell>80.0 86.3</cell><cell>83.2</cell><cell>62.9</cell></row><row><cell>TA 3 N [9]</cell><cell>52.5 72.4</cell><cell>62.3</cell><cell>51.7</cell></row><row><cell>CoMix</cell><cell cols="2">86.7 93.9 90.3</cell><cell>64.7</cell></row></table><note>work lowers down the performance from 90.3% to 88.1% on UCF-HMDB dataset, which shows that graph contrastive learning is more useful in capturing the temporal dependencies, essential for video domain adaptation. (b) Effect of Graph</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 :</head><label>9</label><figDesc>Action Classes in Jester. The table shows the action class composition for each of the domains (i.e. Source and Target) in the Jester dataset and their correspondence to each other for the domain adaptation setting.</figDesc><table><row><cell>Jester</cell><cell>Jester (S)</cell><cell>Jester (T)</cell></row><row><cell>Push and Pull</cell><cell>Pushing Hand Away Pushing Two Fingers Away</cell><cell>Pulling Hand In Pulling Two Fingers In</cell></row><row><cell>Rolling Hand</cell><cell>Rolling Hand Forward</cell><cell>Rolling Hand Backward</cell></row><row><cell>Sliding Two Fingers</cell><cell>Sliding Two Fingers Left Sliding Two Fingers Up</cell><cell>Sliding Two Fingers Right Sliding Two Fingers Down</cell></row><row><cell>Swiping</cell><cell>Swiping Left Swiping Up</cell><cell>Swiping Right Swiping Down</cell></row><row><cell>Thumps Up and Down</cell><cell>Thumb Up</cell><cell>Thumb Down</cell></row><row><cell>Turning Hand</cell><cell>Turning Hand Counterclockwise</cell><cell>Turning Hand Clockwise</cell></row><row><cell>Zooming In and Out</cell><cell cols="2">Zooming Out With Full Hand Zooming Out With Two Fingers Zooming In With Two Fingers Zooming In With Full Hand</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 :</head><label>10</label><figDesc>Effect of Target Pseudo-labelThreshold. Performance on UCF-HMDB.</figDesc><table><row><cell cols="3">PL Threshold U?H H?U Average</cell></row><row><cell>0.5</cell><cell>85.6 93.5</cell><cell>89.5</cell></row><row><cell>0.6</cell><cell>85.6 93.5</cell><cell>89.5</cell></row><row><cell>0.7</cell><cell cols="2">86.7 93.9 90.3</cell></row><row><cell>0.8</cell><cell>86.4 92.5</cell><cell>89.4</cell></row><row><cell>0.9</cell><cell>85.6 90.9</cell><cell>88.2</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>The appendix consists of the following sections:</p><p>? Section A: Details of all the datasets used in our experiments. ? Section B: Description of the Temporal Graph Encoder architecture in CoMix. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Dataset Details</head><p>In this section, we provide the detailed description of the datasets we used to perform all the experiments for CoMix, namely, (1) UCF-HMDB <ref type="bibr" target="#b8">[9]</ref>, (2) Jester <ref type="bibr" target="#b56">[57]</ref>, and (3) Epic-Kitchens <ref type="bibr" target="#b53">[54]</ref>.</p><p>UCF-HMBD Dataset. The UCF-HMDB dataset (assembled by <ref type="bibr" target="#b8">[9]</ref>) is derived from the original UCF101 <ref type="bibr" target="#b72">[73]</ref> and HMDB51 <ref type="bibr" target="#b33">[34]</ref>. It is constructed by collecting all the relevant and overlapping action classes or categories from both the datasets as two domains, resulting in 2 transfer tasks (UCF?HMDB and HMDB?UCF). The dataset possesses 12 action classes, namely, Climb, Fencing, Golf, Kick_Ball, Pullup, Punch, Pushup, Ride_Bike, Ride_Horse, Shoot_Ball, Shoot_Bow, and Walk. For some of the cases, multiple action classes from the original dataset are combined to form a single action super-class for that domain. E.g., RockClimbingIndoor and RopeClimbing classes in the HMDB51 <ref type="bibr" target="#b33">[34]</ref> dataset are combined to form Climb class for the HMDB domain in the UCF-HMDB dataset. The detailed composition of the action classes is shown in <ref type="table">Table 8</ref>. The dataset contains 3, 209 videos in total with 1438 training videos and 571 validation videos from UCF, and 840 training videos and 360 validation videos from HMDB (following the splits by <ref type="bibr" target="#b8">[9]</ref>), with a class-wise distribution shown in <ref type="figure">Figure 6</ref>.</p><p>The datasets are publicly available to download at: https://www.crcv.ucf.edu/data/UCF101.php https://serre-lab.clps.brown.edu/resource/hmdb-a-large-human-motion-database/. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional Implementation Details</head><p>In this section, we provide additional implementation details including hyperparameters with a detailed overview of the model architectures used in our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Model Architectures</head><p>Feature Encoder. Following <ref type="bibr" target="#b11">[12]</ref>, we use I3D <ref type="bibr" target="#b3">[4]</ref> as our feature encoder F for all our experiments. It takes clips (set of consecutive frames) of videos, of length 8, as input and maps them to the corresponding clip-level feature vector of length 1024. The layer-wise architectural view of the I3D feature encoder backbone is shown below: Temporal Graph Encoder For the temporal graph encoder G, we use a 3-layer similarity based graph convolutional neural network, as discussed in Section B. The graph encoder takes the output of the feature encoder F as input and gives the logits as the output. The layer-wise architectural view of the temporal graph encoder is shown below: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Hyperparameters</head><p>Below, we provide the exact values of the two loss weights ? bgm and ? tpl (refer to Eq. 6 in the main paper) for each of the datasets:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Speednet: Learning the Speediness in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Ephrat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oran</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inbar</forename><surname>Mosseri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tali</forename><surname>Irani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9922" to="9931" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixmatch: A Holistic Approach to Semi-Supervised Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><forename type="middle">A</forename><surname>Raffel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5049" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Domain Generalization by Solving Jigsaw Puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fabio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio D&amp;apos;</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvia</forename><surname>Innocente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Bucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tommasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2229" to="2238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo Vadis, Action Recognition? A New Model and the Kinetics Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6299" to="6308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint Domain Alignment and Discriminative Feature Learning for Unsupervised Deep Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyuan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3296" to="3303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep Analysis of CNN-Based Spatio-Temporal Representations for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu Richard</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kandan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="6165" to="6175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep analysis of cnn-based spatio-temporal representations for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Fu Richard</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kandan</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Cohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanfu</forename><surname>Fan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6165" to="6175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain Adversarial Reinforcement Learning for Partial Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Temporal Attentive Alignment for Large-Scale Video Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min-Hung</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaekwon</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6321" to="6330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Simple Framework for Contrastive Learning of Visual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Why Can&apos;t I Dance in the Mall? Learning to Mitigate Scene Bias in Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Messou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="853" to="865" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Shuffle and Attend: Video Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="678" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">A Comprehensive Survey on Domain Adaptation for Visual Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="1" to="35" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Scaling Egocentric Vision: The Epic-Kitchens Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hazel</forename><surname>Doughty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanni</forename><forename type="middle">Maria</forename><surname>Farinella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonino</forename><surname>Furnari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evangelos</forename><surname>Kazakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Moltisanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toby</forename><surname>Perrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="720" to="736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Frustratingly Easy Semi-Supervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daum?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avishek</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2010 Workshop on Domain Adaptation for Natural Language Processing</title>
		<meeting>the 2010 Workshop on Domain Adaptation for Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="53" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Domain Stylization: A Strong, Simple Baseline for Synthetic to Real Image Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aysegul</forename><surname>Dundar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting-Chun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Zedlewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.09384</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">With a Little Help From My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debidatta</forename><surname>Dwibedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yusuf</forename><surname>Aytar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="9588" to="9597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">X3d: Expanding Architectures for Efficient Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="203" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Slowfast Networks for Video Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6202" to="6211" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A Large-Scale Study on Unsupervised Spatiotemporal Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3299" to="3309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Domain-Adversarial Training of Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fran?ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A Kernel Two-Sample Test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensho</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hirokatsu</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3154" to="3160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Momentum Contrast for Unsupervised Visual Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning Deep Representations by Mutual Information Estimation and Maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Cycada: Cycle-Consistent Adversarial Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1989" to="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Duplex Generative Adversarial Network for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lanqing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meina</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1498" to="1507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep Domain Adaptation in Action Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arshad</forename><surname>Jamal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vinay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipti</forename><surname>Namboodiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Deodhare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Video Representation Learning by Recognizing Temporal Transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Jenni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Givi</forename><surname>Meishvili</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Favaro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="425" to="442" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Large-Scale Video Classification with Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanketh</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rahul</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Supervised Contrastive Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prannay</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Teterwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Sarna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Maschinot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Cooperative Learning of Audio and Video Models From Self-Supervised Synchronization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Korbar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00230</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">HMDB: A Large Video Database for Human Motion Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hildegard</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hueihan</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Est?baliz</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Co-Regularization Based Semi-Supervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avishek</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="478" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">. I-Mix</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2010.08887</idno>
		<title level="m">A Domain-Agnostic Strategy for Contrastive Representation Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adaptive Batch Normalization for Practical Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodi</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="109" to="117" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Resound: Towards Action Recognition Without Representation Bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="513" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Temporal Shift Module for Efficient Video Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7082" to="7092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Scene background estimation based on temporal median filter with gaussian filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzheng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miaohui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hejin</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 23rd International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="132" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Domain Adaptation for Semantic Segmentation via Patch-Wise Contrastive Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ferstl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Zebedin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Leistner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.11056</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning Transferable Features with Deep Adaptation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Conditional Adversarial Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1640" to="1650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep Transfer Learning With Joint Adaptation Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Adversarial Bipartite Graph Learning for Video Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yadan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zijian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahsa</forename><surname>Baktashmotlagh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Visualizing Data Using t-SNE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Motionrec: A unified deep framework for moving object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murari</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahipal</forename><surname>Lav Kush Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Singh Saran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2734" to="2743" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Virtual Mixup Training for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangbin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.04215</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The Jester Dataset: A Large-Scale Video Dataset of Human Gestures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joanna</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ingo</forename><surname>Bax</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Instance Adaptive Self-Training for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaqi</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanghang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">AR-Net: Adaptive Frame Resolution for Efficient Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chung-Ching</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prasanna</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Karlinsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="86" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Self-Supervised Learning of Pretext-Invariant Representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6707" to="6717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">When Does Label Smoothing Help?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4694" to="4703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Multi-Modal Domain Adaptation for Fine-Grained Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Munro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dima</forename><surname>Damen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="122" to="132" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Image to Image Translation for Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zak</forename><surname>Murez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Ramamoorthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyungnam</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4500" to="4509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<title level="m">Representation Learning With Contrastive Predictive Coding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Adversarial Cross-Domain Action Recognition With Co-Attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Boxiao Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehsan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="11815" to="11822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Videomoco: Contrastive Video Representation Learning With Temporally Adversarial Examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibing</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11205" to="11214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Multi-frame Recurrent Adversarial Network for Moving Object Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akshay</forename><surname>Prashant W Patil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subrahmanyam</forename><surname>Dudhane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Murala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2302" to="2311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyi</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<title level="m">Multi-Adversarial Domain Adaptation. Thirty-second AAAI conference on artificial intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingchao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Usman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neela</forename><surname>Kaushik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dequan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Visda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06924</idno>
		<title level="m">The Visual Domain Adaptation Challenge</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Background Subtraction Techniques: A Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimo</forename><surname>Piccardi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2004 IEEE International Conference on Systems, Man and Cybernetics</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3099" to="3104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Spatiotemporal Contrastive Video Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianjian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huisheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="6964" to="6974" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Broaden Your Views for Self-Supervised Video Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adri?</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauline</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viorica</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Valko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16559</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Adapting Visual Category Models to New Domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="213" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aadarsh</forename><surname>Sahoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Select</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mix</forename><surname>Label</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.03358</idno>
		<title level="m">Learning Discriminative Invariant Feature Representations for Partial Domain Adaptation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Semi-Supervised Domain Adaptation via Minimax Entropy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Sclaroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8050" to="8058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Maximum Classifier Discrepancy for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuniaki</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohei</forename><surname>Watanabe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshitaka</forename><surname>Ushiku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatsuya</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3723" to="3732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Wasserstein Distance Guided Representation Learning for Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanru</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zechun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05438</idno>
		<title level="m">Rethinking Image Mixtures for Unsupervised Visual Representation Learning</title>
		<meeting><address><addrLine>Un-Mix</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Two-Stream Convolutional Networks for Action Recognition in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Semi-Supervised Action Recognition with Temporal Contrastive Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omprakash</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abir</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10389" to="10399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">UCF101: A Dataset of 101 Human Actions Classes From Videos in the Wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khurram</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Amir Roshan Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Deep Coral: Correlation Alignment for Deep Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.11825</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Unsupervised Domain Adaptation Through Self-Supervision. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Rethinking the Inception Architecture for Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Bsuv-net: A fully-convolutional neural network for background subtraction of unseen videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Tezcan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prakash</forename><surname>Ishwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janusz</forename><surname>Konrad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2774" to="2783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Learning Spatiotemporal Features With 3D Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lubomir</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Adversarial Discriminative Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Deep Hashing Network for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hemanth</forename><surname>Venkateswara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Eusebio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayok</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sethuraman</forename><surname>Panchanathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5018" to="5027" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Manifold Mixup: Better Representations by Interpolating Hidden States</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Beckham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Najafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6438" to="6447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Self-Supervised Video Representation Learning by Pace Prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangliu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="504" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Removing the Background by Adding the Background: Towards Background Robust Selfsupervised Video Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinpeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuting</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><forename type="middle">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pai</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xing</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11804" to="11813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Temporal Segment Networks: Towards Good Practices for Deep Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Deep Visual Domain Adaptation: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">312</biblScope>
			<biblScope unit="page" from="135" to="153" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Videos as Space-Time Region Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="399" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Background Subtraction on Depth Videos With Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xueying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangli</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobing</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Simplifying graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amauri</forename><surname>Souza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Dual Mixup Regularized Learning for Adversarial Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diana</forename><surname>Inkpen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><surname>El-Roby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="540" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<monogr>
		<title level="m" type="main">Self-Supervised Learning of Graph Neural Networks: A Unified Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaochen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.10757</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Adversarial Domain Adaptation with Domain Mixup</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minghao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingbing</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="6502" to="6509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Video Playback Rate Perception for Self-Supervised Spatio-Temporal Representation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dezhao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6548" to="6557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Graph Contrastive Learning With Augmentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Cutmix: Regularization Strategy To Train Strong Classifiers With Localizable Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongyoon</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyuk</forename><surname>Seong Joon Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsuk</forename><surname>Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoon</forename><surname>Choe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6023" to="6032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">Mixup: Beyond Empirical Risk Minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Label Propagation with Augmented Anchors: A Simple Semi-Supervised Learning baseline for Unsupervised Domain Adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yabin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="781" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Fully Convolutional Adaptation Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaofan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6810" to="6818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Temporal Relational Reasoning in Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Andonian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Improved Adaptive Gaussian Mixture Model for Background Subtraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoran</forename><surname>Zivkovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Pattern Recognition</title>
		<meeting>the 17th International Conference on Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="28" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qitong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia-Bin</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.16565</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">Learning Representational Invariances for Data-Efficient Action Recognition. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuliang</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Bian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.09713</idno>
		<title level="m">Jia-Bin Huang, and Tomas Pfister. PseudoSeg: Designing Pseudo Labels for Semantic Segmentation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b101">
	<monogr>
		<title level="m" type="main">For all the experiments, the source-only models were trained for 4000 iterations and then our framework was trained for an additional 10000 iterations, initialized with the source-only models</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
