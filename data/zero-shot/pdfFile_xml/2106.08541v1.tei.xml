<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Distilling Self-Knowledge From Contrastive Links to Classify Graph Nodes Without Passing Messages</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aiguo</forename><surname>Chen</surname></persName>
							<email>agchen@uestc.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
							<email>kyan@uestc.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Tian</surname></persName>
							<email>lingtian@uestc.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
								<address>
									<postCode>611731</postCode>
									<settlement>Chengdu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<postCode>611731</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">School of Computer Science and Engineering University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<postCode>611731</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">School of Computer Science and Engineering University of Electronic Science and Technology of China Chengdu</orgName>
								<address>
									<postCode>611731</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Distilling Self-Knowledge From Contrastive Links to Classify Graph Nodes Without Passing Messages</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:07+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Nowadays, Graph Neural Networks (GNNs) following the Message Passing paradigm become the dominant way to learn on graphic data. Models in this paradigm have to spend extra space to look up adjacent nodes with adjacency matrices and extra time to aggregate multiple messages from adjacent nodes. To address this issue, we develop a method called LinkDist that distils self-knowledge from connected node pairs into a Multi-Layer Perceptron (MLP) without the need to aggregate messages. Experiment with 8 real-world datasets shows the MLP derived from LinkDist can predict the label of a node without knowing its adjacencies but achieve comparable accuracy against GNNs in the contexts of semi-and fullsupervised node classification. Moreover, LinkDist benefits from its Non-Message Passing paradigm that we can also distil self-knowledge from arbitrarily sampled node pairs in a contrastive way to further boost the performance of LinkDist.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graphs are universal representations for many real-world data containing pairwise relationship. For example, academic papers and their citation relationship can construct a graph, where the papers are represented as nodes with their encoded abstracts as node features. If one paper cites another, their nodes are connected in that graph. Graphs like this are usually called citation networks <ref type="bibr" target="#b14">[15]</ref>. Likewise, we have co-purchase networks <ref type="bibr" target="#b13">[14]</ref>, co-authorship networks <ref type="bibr" target="#b15">[16]</ref>, protein-protein interaction networks (PPI), post-post co-comment networks <ref type="bibr" target="#b7">[8]</ref>, and so on <ref type="bibr" target="#b16">[17]</ref>.</p><p>Due to the wide application of graphs, researchers propose more and more methods to mine information from graphs. A well-investigated family of methods is Graph Neural Networks (GNNs) <ref type="bibr" target="#b17">[18]</ref>. They combine node features and the structural information of graphs to study the representations of nodes for downstream tasks, such as Node Classification where the goal is to classify nodes by assigning proper labels to them.</p><p>Many Graph Neural Networks follow the Message Passing paradigm <ref type="bibr" target="#b4">[5]</ref> which contains three steps:</p><p>1. Each node passes its representation to its adjacent nodes 2. Each node aggregates the representations it receives 3. Each node transforms the aggregated information to get its newest representation For example, a layer in Graph Convolutional Network (GCN) <ref type="bibr" target="#b11">[12]</ref> aggregates each node's received messages by computing their average and transforms the averaged messages with a linear transformation.</p><p>This paradigm applies Neural Networks to graphs and makes learning on graphs flexible and powerful. However, it still has some drawbacks.</p><p>? Predicting a single node's label has to aggregate information from multiple adjacent nodes, making the computation heavy and costing extra space to look up adjacent nodes (usually with the adjacency matrix).</p><p>? Passing messages is not easy in practice. For batching purposes, we have to group nodes by their degrees (as the number of their adjacent nodes) and invokes message passing group by group if the graph is too large to manipulate its adjacency matrix.</p><p>? Relying on information from adjacent nodes takes the graph time and effort to propagate messages onto all nodes when new nodes or new edges are inserted.</p><p>One way to rectify some of the aforementioned issues is Knowledge Distillation <ref type="bibr" target="#b5">[6]</ref>. Knowledge Distillation is to transfer the knowledge from a well-trained teacher network to a smaller student network for deployment purposes. For example, when the spaces and computing resources on devices are too limited to store adjacency matrices or aggregate multiple messages timely, we can distil the knowledge from a trained GCN into a Multi-Layer Perceptron (MLP) <ref type="bibr" target="#b1">[2]</ref> and deploy the MLP alone onto those devices. The deployed MLP can predict without the need to aggregate information from adjacent nodes, thus it can be light-weighted and fast. However, when graphs change, the MLP still has to study the updated knowledge about every single node from the GCN again. This makes Knowledge Distillation hard to apply to dynamic graphs.</p><p>However, we observe that the MLP distilled from a trained GCN can sometimes outperform the original GCN, as our experiment in Section 4 shows. It suggests that Message Passing may not be essential if we can find a way to distil the structural knowledge into MLPs. Inspired by this, we propose to distil knowledge from node pairs. Specifically, we predict the label of a node by its features with an MLP and predict the label of its adjacent node with a forked layer. In the meantime, we maximize the agreement of these two predictions by distilling their knowledge into each other. After that, we omit the forked layer and get an MLP that contains distilled structural knowledge. This method, named LinkDist, is light-weighted, fast, and easy to implement. When graphs change, LinkDist adjusts its parameters instead of propagating messages to all nodes. While addressing all the three challenges in Message Passing, the Node Classification experiment in Section 4 shows that the MLPs derived from LinkDist can predict with comparable accuracy against GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Self-Knowledge Distillation Besides the Teacher-Student scheme of Knowledge Distillation, a similar technique known as Self-Knowledge Distillation <ref type="bibr" target="#b20">[21]</ref> regularizes a model by distilling its own knowledge without teacher models. An effective way to achieve this is to induce predictions that are consistent with relevant data such as the universal label distribution of same-labelled nodes <ref type="bibr" target="#b19">[20]</ref>. Our method LinkDist works similarly. It distils its own knowledge by predicting the label of a node consistently with both the node itself and its adjacent nodes.</p><p>Contrastive Learning Contrastive Learning <ref type="bibr" target="#b12">[13]</ref> is a technique that learns to distinguish data points without labels by pulling similar points together and pushing dissimilar points away. Likewise, we propose a contrastive way to train LinkDist. While we distil self-knowledge by minimizing the difference of predictions made with two connected nodes, we also maximize the difference of predictions made with two arbitrarily sampled nodes. Besides, different from Contrastive Learning that learns without labels, we pull predictions made with connected nodes to the ground-truth labels and push predictions made with arbitrarily sampled node pairs away from them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method: LinkDist</head><p>In this section, we describe LinkDist that leverages a forked Multi-Layer Perceptron to produce predictions on nodes both from their features and from their locality. It is trained by iterating on the edge set and distilling self-knowledge from links to induce consistent predictions. We then propose a method of contrastive training to distil knowledge from links that do not exist. After training, we evaluate LinkDist in two modes. One is faster, and one is more accurate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Forked Multi-Layer Perceptron</head><p>Our objective is to learn a mapping M : x ? (z, s) that receives the features x of a node v and predicts both the node's label y and the label of its adjacent node. In graphic data, aggregating messages from adjacent nodes can help a node to determine its label. Vice versa, it is reasonable to partly recover the label of adjacent node with features of a node.</p><p>The mapping M : x ? (z, s) can be implemented as a forked Multi-Layer Perceptron with hidden layers, an output layer, and an inference layer. We feed a node v's features x into the hidden layers, encoding them into a hidden representation h. Then, the output layer and the inference layer use the hidden representation h to produce the logits z, s of label distributions respectively, where z is the approximation of v's label y and s is the possible label distribution of v's adjacent node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The Training of LinkDist</head><p>We illustrate the training of LinkDist in <ref type="figure">Figure 1</ref>. In the training phase, we iterate on the edge set E = {(v i , v j ), ? ? ?} and feed the features x i , x j of each node pair (v i , v j ) into the forked MLP, getting the approximations z i , z j of their labels y i , y j and the approximations s i , s j of their adjacent nodes' labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground-Truth Label Supervision</head><p>If the node v i is assigned with a ground-truth label y i , we supervise z i by minimizing its Cross-Entropy loss with y i . Since v i and v j are each other's adjancet nodes, the label y i is also the possible distribution of s j , so we also supervise s j with y i . Likewise, if the node v j is assigned with a ground-truth label y j , we supervise both z j and s i with y j .</p><p>In addition, the label distribution of the edge set that we iterate on is different from that of the node set. To predict labels of nodes in the node set, we assign weights y n /y e to labels when computing the Cross-Entropy loss L CE , where y n is the label distribution in the training set and y e is the label distribution in the edge set with unlabelled nodes masked.</p><p>Link Distillation For each node, we have two predictions of its label from different sources. One is from its features. Another one is from its adjacent node. By minimizing their difference L M SE measured with Mean Square Error, we can distil knowledge from both of them into each other, maximizing the agreement of predicting the label of a node by its feature information and by the structural knowledge from its adjacent node.</p><p>In total, we optimize L CE + ? ? L M SE to cast both ground-truth label supervision and link distillation. By default, we set the hyperparameter ? = 1 ? n e /e where n e is the times that labelled nodes appear in the edge set. In other words, the more labelled nodes containing supervising information we have, the less important the knowledge inferred from adjacent nodes is.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Contrastive Training With Negative Links</head><p>To leverage node pairs that are not connected, we propose a method of contrastive training as illustrated in <ref type="figure">Figure 2</ref>. We sample arbitrary pairs of nodes to construct negative links <ref type="bibr" target="#b6">[7]</ref> and feed the features</p><formula xml:id="formula_0">x i , x k of each node pair (v i , v k )</formula><p>into the forked MLP, getting the approximations z i , z k of their labels y i , y k and the approximations s i , s k of their adjacent nodes' labels.</p><p>While we supervise z i with y i and supervise z k with y k as we do in the previous section 3.2, we maximize the Cross-Entropy of s i , z k and the Cross-Entropy of s k , z i . This is because node v k is arbitrarily sampled that it is not likely to connect v i . Therefore, the approximation z i of node v i 's label is probably different from the approximation s k of labels assigned to node v k 's adjacent nodes and z k is probably different from s i . Likewise, we maximize the MSE of z i , s k and the MSE of z k , s i to distil self-knowledge from negative links.</p><p>This contrastive training method has to be adopted with the training process described in the previous section 3.2. In this work, we name the contrastively trained model CoLinkDist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Two Evaluating Modes of LinkDist</head><p>The forked MLP in LinkDist outputs the label distribution z of the node with its output layer and the label distribution s of the node's adjacent node with its inference layer. When omitting the inference layer and its output s, the forked MLP becomes an MLP, but with structural knowledge distilled from links. The MLPs derived from LinkDist and CoLinkDist, named LinkDistMLP and CoLinkDistMLP, can classify graph nodes fast without aggregating messages from their adjacent nodes, thus they can be light-weighted to deploy without adjacency matrices or other nodes' features. Moreover, the experiment in Section 4 shows the accuracy of LinkDistMLP outperforms that of vanilla MLPs by large margins, approaching the accuracy of GCNs.</p><p>When adjacency matrices and features of adjacent nodes are available, LinkDist can also utilize Message Passing to boost its accuracy. It combines the prediction from the node v i itself and the predictions from its adjacent nodes to estimate its possible labe? i with:</p><formula xml:id="formula_1">y i = z i + ? ? vj ?N (vi) s j where N (v i )</formula><p>is the set of nodes that are adjacent to node v i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment: Node Classification</head><p>In this section, we conduct the Node Classification experiment on 8 real-world datasets. The code to reproduce this work is published at https://github.com/cf020031308/LinkDist.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>To fully evaluate our method, we experiment with 8 real-world datasets including 4 citation networks, 2 co-purchase networks and 2 co-authorship networks. They are Cora, Citeseer, Pubmed <ref type="bibr" target="#b14">[15]</ref>, Cora Full <ref type="bibr" target="#b2">[3]</ref>, Amazon Photo, Amazon Computer <ref type="bibr" target="#b13">[14]</ref>, Coauthor CS and Coauthor Physics <ref type="bibr" target="#b15">[16]</ref>. These datasets are very different in many respects. We summarize them in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Semi-and Full-Supervised Among the 8 datasets, Cora, Citeseer and Pubmed are distributed with their predefined dataset splits. In detail, in each of these 3 datasets, the training set contains 20 nodes from each class, the validation set contains 500 nodes and the testing set contains 1000 nodes. On Cora, Citeseer and Pubmed, we accordingly use these splits to do the semi-supervised Node Classification experiment. On other datasets, we split nodes into training sets, validation sets and testing sets exactly like on Cora, Citeseer and Pubmed. In particular, we randomly select at most 20 nodes from each class to construct the training set, 500 nodes to construct the validation set, and 1000 nodes to construct the testing set.</p><p>In addition to this semi-supervised setting, we further split each dataset with the full-supervised setting. Specifically, we randomly select 60% of the nodes in a dataset to construct its training set. Then, we equally divide the rest nodes into a validation set and a testing set.</p><p>Transductive and Inductive In the semi-supervised Node Classification, we experiment with both the transductive setting and the inductive setting. With the transductive setting, methods use all nodes features and connections in the training phase. With the inductive setting, nodes in the two evaluation sets (the validation set and the testing set) are masked in the training phase. So are connections that have endpoints in the evaluation sets. This setting meets practical scenarios where graphs are consistently expanding with new nodes <ref type="bibr" target="#b3">[4]</ref>.</p><p>In the full-supervised Node Classification, the majority of nodes are either in the evaluation sets or connected with nodes from the evaluation sets. So we experiment with only the transductive setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines</head><p>We implement 3 methods as baselines.</p><p>Graph Convolutional Network (GCN) LinkDist working in Message Passing paradigm can utilize all the information that most GNNs use. So we compare LinkDist against GCN to examine its applicability in common situations. Due to the inductive setting, we implement GCN with its normalized adjacency matrix as (D + I) ?1 (A + I) instead of (D + I) ? 1 2 (A + I)(D + I) ? 1 2 . I is the identity matrix, A is the adjacency matrix, and D is a diagonal matrix where each diagonal entry d i is the degree of node v i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Layer Perceptron (MLP)</head><p>In Non-Message Passing mode, LinkDistMLP works just like an MLP. However, with knowledge distilled from links, LinkDistMLP should go beyond it. So we place an MLP here to act as the lower limit of the LinkDistMLP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MLP distilled from GCN (GCN2MLP)</head><p>We teach an MLP the soft label logits of all nodes outside the evaluation sets predicted by a trained GCN. The distilled MLP has an identical structure to both the MLP and the LinkDistMLP but inherits the structural knowledge from GCNs. We think it is a proper benchmark for LinkDistMLP to measure how far it is from 'the perfection'.</p><p>All implementations of MLP and GCN have 3 full-connected layers. The LinkDist has 2 hidden layers, 1 output layer and 1 inference layer, all implemented as full-connected layers. Hidden representations within all layers have the identical size 256. There are 1 Batch Normalization <ref type="bibr" target="#b9">[10]</ref>, 1 Layer Normalization [1], 1 Dropout <ref type="bibr" target="#b8">[9]</ref> with 0.5 possibilities to erase a position and 1 LeakyReLU <ref type="bibr" target="#b18">[19]</ref> activator between every two layers.</p><p>All the methods can be divided into two groups by whether a method leverages information from adjacent nodes in the evaluation phase. MLP, GCN2MLP, LinkDistMLP, and CoLinkDistMLP involve no Message Passing, whereas GCN, LinkDist, and CoLinkDist do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>We use Adam <ref type="bibr" target="#b10">[11]</ref> with a learning rate of 0.01 to optimize parameters. The GCN is trained in full batches. The MLP and the LinkDist are trained in mini-batches with the batch size set to 1024. Since the MLP and the GCN iterates on the node set while the LinkDist iterate on the edge set, we train the former two models for 200 epochs but train the LinkDist for about 200/d epochs whered is the average degree of the graph.</p><p>After every epoch of training, we evaluate the method on the two evaluation sets, producing a pair of accuracy scores. After running, we get the testing set accuracy score paired with the highest <ref type="table">Table 2</ref>: Accuracy Scores (%) in the semi-supervised and inductive Node Classification. Methods are divided into two groups by whether the method aggregates messages from adjacent nodes in the evaluation phase. We bold the highest accuracy for every dataset in each group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Cora   validation set accuracy score as the score of the evaluated method. We run every method on every dataset 10 times and record the average score in <ref type="table">Table 2</ref>, <ref type="table" target="#tab_2">Table 3, and Table 4</ref>.</p><p>From those tables, we can see that the LinkDistMLP can outperform the same-structured MLP on almost all 8 datasets by large margins. While the LinkDistMLP predicts with only the features of the central node, it still matches the GCN in the inductive setting and even exceeds the GCN in the other two settings. Moreover, utilizing Message Passing (LinkDist) and training contrastively (CoLinkDistMLP, CoLinkDist) can both consistently boost the accuracy. This indicates the success of distilling self-knowledge from node pairs and that the LinkDists are competitive methods with GNNs in the Node Classification tasks.</p><p>We also notice that the GCN2MLP behaves very well in the semi-supervised setting, especially in the inductive setting where there's still a gap between LinkDistMLPs and the GCN2MLP, as <ref type="table">Table 2</ref> and <ref type="table" target="#tab_2">Table 3</ref> show. However, in the full-supervised setting as <ref type="table" target="#tab_3">Table 4</ref> shows, the accuracy of the GCN2MLP is almost the same as that of the MLP. It is far worse than that of the GCN and the LinkDists. This is because the GCN2MLP in the semi-supervised setting gains additional knowledge from nodes that are outside of the training set, whereas in the full-supervised setting, all nodes are either in the training set or in the evaluation sets. There is no much additional knowledge to distil.</p><p>We tend to experiment with the PPI dataset <ref type="bibr" target="#b7">[8]</ref>, where the training set is formed up with several complete graphs and the trained methods are required to classify nodes on completely unseen graphs, but find that the hyperparameter ? in the LinkDist become 0 and LinkDist degenerated into a vanilla MLP. In other words, LinkDist may have problems to induce on entirely unseen graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we propose a novel way named LinkDist to classify nodes on graphs without the need of passing messages.</p><p>Semi-and full-supervised Node Classification experiments show that LinkDist can predict with comparable accuracy against GNNs even without information from adjacent nodes.</p><p>LinkDist can be very practical because it rectifies the three issues of Message Passing:</p><p>? LinkDist learns in mini-batches of edges. It requires less space than using adjacency matrices and can be implemented easily for large graphs.</p><p>? LinkDist can be deployed to devices with limited resources, predicting fast like MLPs but accurately like GNNs, without the need to store adjacent nodes and their features.</p><p>? When new nodes or new edges are inserted, LinkDist updates the predictions of existing nodes by optimizing its learnable parameters, instead of propagating information to all nodes.</p><p>While LinkDist in Message Passing mode (LinkDist and CoLinkDist) can produce very competitive accuracy with GNNs, there's still a gap between LinkDist in non-Message Passing mode (LinkDistMLP and CoLinkDistMLP) and GCN2MLP, if experimenting with the inductive setting. It shows the power of networks as simple as MLPs and points out the potential of LinkDist. In the future, we may investigate to find more effective solutions to distil knowledge into MLPs to narrow the gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>This work addresses the issues of Message Passing that are widely adopted in Graph Neural Networks. The proposed LinkDist is a general-purpose method. It may reduce the cost of mining information from graphic data in existing applications. To the best of our knowledge, it has no foreseeable negative societal impacts.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>The training of LinkDist. The features x i , x j of two connected nodes (v i , v j ) are fed into the forked MLP with hidden layers, an output layer, and an inference layer. The forked MLP produces z i , s i , z j , s j as approximations to the label of v i , the label of v i 's adjacent node, the label of v j and the label of v j 's adjacent node. They are supervised by the ground-truth labels y i , y j of v i , v j if are available. s i and s j contain dark knowledge of labelled nodes and additional knowledge of unlabelled nodes. So we distil knowledge from s i into z j and from s j into z i . Contrastive training with negative links. The features x i , x k of two arbitrarily sampled nodes (v i , v k ) are fed into the forked MLP. The forked MLP produces z i , s i , z k , s k as approximations to the label of v i , the label of v i 's adjacent node, the label of v k and the label of v k 's adjacent node. z i , z k are supervised by the ground-truth labels y i , y k of v i , v j if are available. Since the node v k is not likely to connect v i , we push s i away from z k and y k by maximizing their differences measured by Mean Square Error and Cross-Entropy respectively. Likewise, we push s k away from z i and y i .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of the 8 real-world datasets Dataset #Nodes #Features #Classes #Edges Average Degree</figDesc><table><row><cell>Cora</cell><cell>2708</cell><cell>1433</cell><cell>7</cell><cell>5278</cell><cell>3.90</cell></row><row><cell>Citesee</cell><cell>3327</cell><cell>3703</cell><cell>6</cell><cell>4552</cell><cell>2.77</cell></row><row><cell>Pubmed</cell><cell>19717</cell><cell>500</cell><cell>3</cell><cell>44324</cell><cell>4.50</cell></row><row><cell>Cora Full</cell><cell>19793</cell><cell>8710</cell><cell>70</cell><cell>63421</cell><cell>6.41</cell></row><row><cell>Amazon Photo</cell><cell>7650</cell><cell>745</cell><cell cols="2">8 119081</cell><cell>31.13</cell></row><row><cell>Amazon Computer</cell><cell>13752</cell><cell>767</cell><cell cols="2">10 245861</cell><cell>35.76</cell></row><row><cell>Coauthor CS</cell><cell>18333</cell><cell>6805</cell><cell>15</cell><cell>81894</cell><cell>8.93</cell></row><row><cell>Coauthor Physics</cell><cell>34493</cell><cell>8415</cell><cell cols="2">5 247962</cell><cell>14.38</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Accuracy Scores (%) in the semi-supervised and transductive Node Classification. Methods are divided into two groups by whether the method aggregates messages from adjacent nodes in the evaluation phase. We bold the highest accuracy for every dataset in each group.</figDesc><table><row><cell>Method</cell><cell cols="6">Cora Citesee Pubmed Cora Amazon Amazon Coauthor Coauthor</cell></row><row><cell></cell><cell></cell><cell>Full</cell><cell cols="2">Photo Computer</cell><cell cols="2">CS Physics</cell></row><row><cell>MLP</cell><cell>56.28 54.74</cell><cell>70.33 41.91</cell><cell>75.89</cell><cell>66.14</cell><cell>90.11</cell><cell>89.53</cell></row><row><cell>GCN2MLP</cell><cell>67.61 63.29</cell><cell>77.87 54.15</cell><cell>87.11</cell><cell>78.21</cell><cell>92.49</cell><cell>93.42</cell></row><row><cell>LinkDistMLP</cell><cell>80.79 70.26</cell><cell>72.41 51.78</cell><cell>88.19</cell><cell>78.23</cell><cell>89.83</cell><cell>90.72</cell></row><row><cell cols="2">CoLinkDistMLP 81.19 70.96</cell><cell>75.41 53.43</cell><cell>88.90</cell><cell>78.77</cell><cell>90.66</cell><cell>91.63</cell></row><row><cell>GCN</cell><cell>76.47 63.11</cell><cell>73.99 57.58</cell><cell>87.00</cell><cell>77.95</cell><cell>87.93</cell><cell>92.79</cell></row><row><cell>LinkDist</cell><cell>81.05 70.27</cell><cell>74.06 55.87</cell><cell>90.14</cell><cell>82.65</cell><cell>90.94</cell><cell>92.06</cell></row><row><cell>CoLinkDist</cell><cell>81.39 70.79</cell><cell>75.64 57.05</cell><cell>90.54</cell><cell>82.53</cell><cell>91.88</cell><cell>92.74</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Accuracy Scores (%) in the full-supervised and transductive Node Classification. Methods are divided into two groups by whether the method aggregates messages from adjacent nodes in the evaluation phase. We bold the highest accuracy for every dataset in each group.</figDesc><table><row><cell>Method</cell><cell cols="6">Cora Citesee Pubmed Cora Amazon Amazon Coauthor Coauthor</cell></row><row><cell></cell><cell></cell><cell>Full</cell><cell cols="2">Photo Computer</cell><cell cols="2">CS Physics</cell></row><row><cell>MLP</cell><cell>74.71 71.95</cell><cell>87.92 62.03</cell><cell>91.75</cell><cell>85.08</cell><cell>95.35</cell><cell>96.57</cell></row><row><cell>GCN2MLP</cell><cell>75.95 72.93</cell><cell>86.25 62.39</cell><cell>91.10</cell><cell>85.41</cell><cell>95.29</cell><cell>96.46</cell></row><row><cell>LinkDistMLP</cell><cell>87.58 75.25</cell><cell>88.79 69.53</cell><cell>93.83</cell><cell>89.44</cell><cell>95.68</cell><cell>96.91</cell></row><row><cell cols="2">CoLinkDistMLP 87.54 75.77</cell><cell>89.53 69.83</cell><cell>94.12</cell><cell>88.85</cell><cell>95.74</cell><cell>96.87</cell></row><row><cell>GCN</cell><cell>86.03 73.37</cell><cell>84.79 68.97</cell><cell>93.09</cell><cell>90.08</cell><cell>92.27</cell><cell>95.96</cell></row><row><cell>LinkDist</cell><cell>88.24 74.72</cell><cell>88.86 69.87</cell><cell>93.75</cell><cell>89.49</cell><cell>95.66</cell><cell>96.87</cell></row><row><cell>CoLinkDist</cell><cell>87.89 75.79</cell><cell>89.58 70.32</cell><cell>94.36</cell><cell>89.42</cell><cell>95.80</cell><cell>97.05</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Standard deviations (%) in the semi-supervised and inductive Node Classification. Methods are divided into two groups by whether the method aggregates messages from adjacent nodes in the evaluation phase. We bold the highest accuracy for every dataset in each group.</figDesc><table><row><cell>Method</cell><cell cols="7">Cora Citesee Pubmed Cora Amazon Amazon Coauthor Coauthor</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Full</cell><cell cols="2">Photo Computer</cell><cell cols="2">CS Physics</cell></row><row><cell>MLP</cell><cell>0.87</cell><cell>1.10</cell><cell>1.05 0.98</cell><cell>2.97</cell><cell>2.42</cell><cell>0.93</cell><cell>1.14</cell></row><row><cell>GCN2MLP</cell><cell>0.98</cell><cell>0.91</cell><cell>1.76 1.92</cell><cell>2.10</cell><cell>1.47</cell><cell>0.40</cell><cell>0.95</cell></row><row><cell>LinkDistMLP</cell><cell>0.90</cell><cell>0.72</cell><cell>2.03 1.81</cell><cell>1.73</cell><cell>1.95</cell><cell>1.35</cell><cell>1.05</cell></row><row><cell cols="2">CoLinkDistMLP 0.62</cell><cell>1.04</cell><cell>0.47 1.67</cell><cell>1.90</cell><cell>2.41</cell><cell>0.84</cell><cell>0.90</cell></row><row><cell>GCN</cell><cell>0.91</cell><cell>0.64</cell><cell>1.61 1.79</cell><cell>1.82</cell><cell>2.25</cell><cell>1.19</cell><cell>1.09</cell></row><row><cell>LinkDist</cell><cell>0.79</cell><cell>0.91</cell><cell>1.20 1.74</cell><cell>1.91</cell><cell>1.23</cell><cell>0.76</cell><cell>1.01</cell></row><row><cell>CoLinkDist</cell><cell>0.58</cell><cell>0.77</cell><cell>0.68 2.08</cell><cell>2.23</cell><cell>2.12</cell><cell>0.83</cell><cell>1.08</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Standard deviations (%) in the semi-supervised and transductive Node Classification. Methods are divided into two groups by whether the method aggregates messages from adjacent nodes in the evaluation phase. We bold the highest accuracy for every dataset in each group. Did you discuss whether the data you are using/curating contains personally identifiable information or offensive content? [N/A] 5. If you used crowdsourcing or conducted research with human subjects... (a) Did you include the full text of instructions given to participants and screenshots, if applicable? [N/A] (b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A] (c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]</figDesc><table><row><cell>Method</cell><cell cols="7">Cora Citesee Pubmed Cora Amazon Amazon Coauthor Coauthor</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Full</cell><cell cols="2">Photo Computer</cell><cell cols="2">CS Physics</cell></row><row><cell>MLP</cell><cell>0.87</cell><cell>1.10</cell><cell>1.05 0.98</cell><cell>2.97</cell><cell>2.42</cell><cell>0.93</cell><cell>1.14</cell></row><row><cell>GCN2MLP</cell><cell>0.83</cell><cell>1.74</cell><cell>1.68 2.14</cell><cell>1.23</cell><cell>1.73</cell><cell>0.89</cell><cell>0.79</cell></row><row><cell>LinkDistMLP</cell><cell>0.43</cell><cell>0.70</cell><cell>1.35 1.43</cell><cell>2.07</cell><cell>2.49</cell><cell>1.11</cell><cell>1.10</cell></row><row><cell cols="2">CoLinkDistMLP 0.61</cell><cell>0.79</cell><cell>0.38 1.83</cell><cell>1.88</cell><cell>2.05</cell><cell>0.68</cell><cell>1.65</cell></row><row><cell>GCN</cell><cell>1.12</cell><cell>1.24</cell><cell>1.39 1.68</cell><cell>1.53</cell><cell>2.55</cell><cell>1.79</cell><cell>0.86</cell></row><row><cell>LinkDist</cell><cell>0.85</cell><cell>0.75</cell><cell>0.85 2.05</cell><cell>1.78</cell><cell>2.51</cell><cell>1.08</cell><cell>1.06</cell></row><row><cell>CoLinkDist</cell><cell>0.46</cell><cell>0.74</cell><cell>1.02 1.43</cell><cell>1.88</cell><cell>2.52</cell><cell>0.98</cell><cell>1.42</cell></row><row><cell>(e) A Appendix</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>Standard deviations (%) in the full-supervised and transductive Node Classification. Methods are divided into two groups by whether the method aggregates messages from adjacent nodes in the evaluation phase. We bold the highest accuracy for every dataset in each group.</figDesc><table><row><cell>Method</cell><cell cols="7">Cora Citesee Pubmed Cora Amazon Amazon Coauthor Coauthor</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Full</cell><cell cols="2">Photo Computer</cell><cell cols="2">CS Physics</cell></row><row><cell>MLP</cell><cell>1.80</cell><cell>2.23</cell><cell>0.52 0.90</cell><cell>0.82</cell><cell>0.77</cell><cell>0.22</cell><cell>0.19</cell></row><row><cell>GCN2MLP</cell><cell>1.70</cell><cell>1.55</cell><cell>0.64 0.61</cell><cell>0.55</cell><cell>0.63</cell><cell>0.34</cell><cell>0.24</cell></row><row><cell>LinkDistMLP</cell><cell>1.24</cell><cell>2.00</cell><cell>0.56 0.67</cell><cell>0.41</cell><cell>0.63</cell><cell>0.25</cell><cell>0.20</cell></row><row><cell cols="2">CoLinkDistMLP 1.61</cell><cell>1.35</cell><cell>0.37 0.61</cell><cell>0.48</cell><cell>0.88</cell><cell>0.51</cell><cell>0.20</cell></row><row><cell>GCN</cell><cell>1.67</cell><cell>1.59</cell><cell>0.82 0.76</cell><cell>0.64</cell><cell>0.54</cell><cell>0.45</cell><cell>0.23</cell></row><row><cell>LinkDist</cell><cell>1.23</cell><cell>1.90</cell><cell>0.44 0.66</cell><cell>0.30</cell><cell>0.51</cell><cell>0.32</cell><cell>0.22</cell></row><row><cell>CoLinkDist</cell><cell>1.24</cell><cell>1.45</cell><cell>0.26 0.62</cell><cell>0.43</cell><cell>0.72</cell><cell>0.34</cell><cell>0.17</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Checklist</head><p>The checklist follows the references. Please read the checklist guidelines carefully for information on how to answer these questions. For each question, change the default [TODO] to [Yes] , <ref type="bibr">[No]</ref> , or [N/A] . You are strongly encouraged to include a justification to your answer, either by referencing the appropriate section of your paper or providing a brief inline description. For example:</p><p>? Did you include the license to the code and datasets? [Yes] See Section ??.</p><p>? Did you include the license to the code and datasets? <ref type="bibr">[No]</ref> The code and the data are proprietary.</p><p>? Did you include the license to the code and datasets? [N/A] Please do not modify the questions and only use the provided macros for your answers. Note that the Checklist section does not count towards the page limit. In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.</p><p>1. For all authors... We include the standard deviations in <ref type="table">Table 5</ref>, <ref type="table">Table 6</ref>, and <ref type="table">Table 7</ref> in Appendix A. (d) Did you include the total amount of compute and the type of resources used (e.g., type of GPUs, internal cluster, or cloud provider)? <ref type="bibr">[No]</ref> We think the hardwares do not matter. Even with a CPU one can reproduce our work and we use a GPU with 11GB memory.</p><p>4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets... </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>abs/1607.06450</idno>
		<title level="m">Layer normalization. CoRR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning, 5th Edition. Information science and statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>G?nnemann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fastgcn: Fast learning with graph convolutional networks via importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings. OpenReview.net</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno>abs/1704.01212</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Knowledge distillation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<idno>abs/2006.05525</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Krishnapuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rastogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016-08-13" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">; I</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
	<note>Guyon</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>abs/1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<editor>Bach, F. R. and Blei, D. M.</editor>
		<meeting>the 32nd International Conference on Machine Learning<address><addrLine>Lille, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-07-11" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
	<note>of JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio, Y. and LeCun, Y.</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>abs/1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Self-supervised learning: Generative or contrastive</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno>abs/2006.08218</idno>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image-based recommendations on styles and substitutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Targett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moffat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015-08-09" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Mag</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="93" to="106" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft academic graph: When experts are not enough</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanakia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Quant. Sci. Stud</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="396" to="413" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Deep graph library: A graph-centric, highly-performant package for graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01315</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1505.00853</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Regularizing class-wise predictions via self-knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Seattle, WA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020-06-13" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13873" to="13882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Be your own teacher: Improve the performance of convolutional neural networks via self distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ma</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
