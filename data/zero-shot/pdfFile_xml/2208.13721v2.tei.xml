<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CounTR: Transformer-based Generalised Visual Counting</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
							<email>liuchang666@sjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Coop. Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujie</forename><surname>Zhong</surname></persName>
							<email>jaszhong@hotmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Meituan Inc</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Visual Geometry Group (VGG)</orgName>
								<orgName type="institution">University of Oxford</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Coop. Medianet Innovation Center</orgName>
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CounTR: Transformer-based Generalised Visual Counting</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>LIU, ET. AL.: TRANSFORMER-BASED GENERALISED VISUAL COUNTING 1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:59+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we consider the problem of generalised visual object counting, with the goal of developing a computational model for counting the number of objects from arbitrary semantic categories, using arbitrary number of "exemplars", i.e. zero-shot or fewshot counting. To this end, we make the following four contributions: (1) We introduce a novel transformer-based architecture for generalised visual object counting, termed as Counting TRansformer (CounTR), which explicitly captures the similarity between image patches or with given "exemplars" using the attention mechanism; (2) We adopt a two-stage training regime, that first pre-trains the model with self-supervised learning, and followed by supervised fine-tuning; (3) We propose a simple, scalable pipeline for synthesizing training images with a large number of instances or that from different semantic categories, explicitly forcing the model to make use of the given "exemplars"; (4) We conduct thorough ablation studies on the large-scale counting benchmark, e.g. FSC-147, and demonstrate state-of-the-art performance on both zero and few-shot settings. Project page: https://verg-avesta.github.io/CounTR_Webpage/.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Despite all the exceptional abilities, the human visual system is particularly weak in counting objects in the image. In fact, given a visual scene with a collection of objects, one can only make a rapid, accurate, and confident judgment if the number of items is below five, with an ability termed as subitizing <ref type="bibr" target="#b15">[16]</ref>. While for scenes with an increasing number of objects, the accuracy and confidence of the judgments tend to decrease dramatically. Until at some point, counting can only be accomplished by calculating estimates or enumerating the instances, which incurs low accuracy or tremendous time cost.</p><p>In this paper, our goal is to develop a generalised visual object counting system, that augments humans' ability for recognising the number of objects in a visual scene. Specifically, generalised visual object counting refers to the problem of identifying the number of the salient objects of arbitrary semantic class in an image (i.e. open-world visual object counting) with arbitrary number of instance "exemplars" provided by the end user, to refer to the particular objects to be counted, i.e. from zero-shot to few-shot object counting. To this end, we propose a novel architecture that transforms the input image (with the few-shot annotations if any) into a density map, and the final count can be obtained by simply summing over the density map.</p><p>Specifically, we take inspiration from Lu et al. <ref type="bibr" target="#b18">[19]</ref> that self-similarity is a strong prior in visual object counting, and introduce a transformer-based architecture where the selfsimilarity prior can be explicitly captured by the built-in attention mechanisms, both among the input image patches and with the few-shot annotations (if any). We propose a two-stage training scheme, with the transformer-based image encoder being firstly pre-trained with self-supervision via masked image modeling <ref type="bibr" target="#b10">[11]</ref>, followed by supervised fine-tuning for the task at hand. We demonstrate that self-supervised pre-training can effectively learn the visual representation for counting, thus significantly improving the performance. Additionally, to tackle the long-tailed challenge in existing generalised visual object counting datasets, where the majority of images only contain a small number of objects, we propose a simple, yet scalable pipeline for synthesizing training images with a large number of instances, as a consequence, establishing reliable data sources for model training, to condition the userprovided instance exemplars.</p><p>To summarise, in this paper, we make four contributions: First, we introduce an architecture for generalised visual object counting based on transformer, termed as CounTR (pronounced as counter). It exploits the attention mechanisms to explicitly capture the similarity between image patches, or with the few-shot instance "exemplars" provided by the end user; Second, we adopt a two-stage training regime (self-supervised pre-training, followed by supervised fine-tuning) and show its effectiveness for the task of visual counting; Third, we propose a simple yet scalable pipeline for synthesizing training images with a large number of instances, and demonstrate that it can significantly improve the performance on images containing a large number of object instances; Fourth, we conduct thorough ablation studies on the large-scale counting benchmark, e.g. FSC-147 <ref type="bibr" target="#b23">[24]</ref>, and demonstrate state-of-the-art performance on both zero-shot and few-shot settings, improving the previous best approach by over 18.3% on the mean absolute error of the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Visual object counting. In the literature, object counting approaches can generally be cast into two categories: detection-based counting <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b12">13]</ref> or regression-based counting <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b28">29]</ref>. The former relies on a visual object detector that can localize object instances in an image. This method, however, requires training individual detectors for different objects, and the detection problem remains challenging if only a small number of annotations are given. The latter avoids solving the hard detection problem, instead, methods are designed to learn either a mapping from global image features to a scalar (number of objects), or a mapping from dense image features to a density map, achieving better results on counting overlapping instances. However, previous methods from both lines (detection, regression) have only been able to count objects of one particular class (e.g. cars, cells).</p><p>Class-agnostic object counting. Recently, class-agnostic few-shot counting <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30]</ref> has witnessed a rise in research interest in the community. Unlike the class-specific models that could only count objects of specific classes like cars, cells, or people, class-agnostic counting aims to count the objects in an image based on a few given "exemplar" instances, thus is also termed as few-shot counting. Generally speaking, class-agnostic few-shot counting models need to mine the commonalities between the counts of different classes of objects during training. In <ref type="bibr" target="#b18">[19]</ref>, the authors propose a generic matching network (GMN), which regresses the density map by computing the similarity between the CNN features from image and exemplar shots; FamNet <ref type="bibr" target="#b23">[24]</ref> utilizes feature correlation for prediction and uses adaptation loss to update the model's parameters at test time; SAFECount <ref type="bibr" target="#b29">[30]</ref> uses the support feature to enhance the query feature, making the extracted features more refined and then regresses to obtain density maps; In a very recent work <ref type="bibr" target="#b11">[12]</ref>, the authors exploit a pre-trained DINO <ref type="bibr" target="#b20">[21]</ref> model and a lightweight regression head to count without exemplars. In this paper, we also use transformer-based architecture, however, train it from scratch, and augment it with the ability to count the objects with any shot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>In this paper, we consider the challenging problem of generalised visual object counting, where the goal is to count the salient objects of arbitrary semantic class in an image, i.e. open-world visual object counting, with arbitrary number of "exemplars" provided by the end user,i.e. from zero-shot to few-shot object counting.</p><p>Overview. Given a training set, i.e.</p><formula xml:id="formula_0">D train = {(X 1 , S 1 , y 1 ), . . . , (X N , S N , y N )}, where X i ? R H?W ?3 denotes the input image, S i = {b i } K denotes the box coordinates (b k i ? R 4</formula><p>) for a total of K ? {0, 1, 2, 3...} given "exemplars", i.e. zero-shot or few-shot counting, y i ? R H?W ?1 refers to a binary spatial density map, with 1's at the objects' center location, indicating their existence, and 0's at other locations without the objects, the object count can thus be computed by spatially summing over the density map. Our goal here is thus to train a generalised visual object counter that can successfully operate on a test set, given zero or few exemplars, i.e. D test = {(X N+1 , S N+1 ), . . . , (X M , S M )}. Note that, the semantic categories for objects in training set (C train ) and testing set (C test ) are disjoint, i.e. C train ? C test = / 0. To achieve this goal, we introduce a novel transformer-based architecture, termed as Counting TRansformer (CounTR). Specifically, the attention mechanisms in transformer enable to explicitly compare visual features between any other spatial locations and with "exemplars", which are provided by the end user in the few-shot scenario. In Section 3.2, we further introduce a two-stage training regime, in which the model is firstly pre-trained with self-supervision via masked image reconstruction (MAE), followed by fine-tuning on the downstream counting task. To the best of our knowledge, this is the first work to show the effectiveness of self-supervised pre-training for generalised visual object counting. Additionally, in Section 3.3, we propose a novel and scalable mosaic pipeline for synthesizing training images, as a way to resolve the challenge of long-tailed distribution (i.e. images with a large number of instances tend to be less frequent) in the existing object counting dataset. In Section 3.4, we will introduce our test-time normalisation method including test-time normalisation and test-time cropping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>Here, we introduce the proposed Counting TRansformer (CounTR), as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. In specific, the input image (X i ), and user-provided "exemplars" (S k i , ?k ? {0, 1, 2, 3}) are fed as input and mapped to a density heatmap, where the object count can be obtained by token is used as key and value instead. The outputs are up-sampled in the decoder and finally, we get the corresponding density map. The object count can be obtained by summing the density map. Note that, given the different exemplars with diversity, the model should ideally understand the invariance (shape, color, scale, texture), for example, if the three given exemplars are all in the same color, the model should only count the objects of that color, otherwise, count all instances of the same semantic. simply summing over it:</p><formula xml:id="formula_1">y i = ? DECODER (? FIM (? VIT-ENC (X i ), ? CNN-ENC (S k i ))), ?k ? {0, 1, ..., K}<label>(1)</label></formula><p>In the following sections, we will detail the three building components, namely, visual encoder (? VIT-ENC (?) and ? CNN-ENC (?)), feature interaction module (i.e. FIM, ? FIM (?)), and visual decoder (? DECODER (?)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Visual Encoder</head><p>The visual encoder is composed of two components, serving for two purposes: first, an encoder based on Vision Transformer (ViT) <ref type="bibr" target="#b5">[6]</ref> for processing the input image that maps it into a high-dimensional feature map; second, compute the visual features for the "exemplars", if there is any. Specifically, as for ViT, the input image is broken into patches with a size of 16 ? 16 pixels and projected to tokens by a shared MLP. To indicate the order of each token in the sequence, positional encoding is added, ending up with M 'tokens'. They are further passed through a series of transformer encoder layers, in our model, 12 layers are used. We do not include the [CLS] token in the sequence, and the output from the ViT encoder is a sequence of D-dim vectors :</p><formula xml:id="formula_2">F VIT = ? VIT-ENC (X i ) ? R M?D<label>(2)</label></formula><p>for more details, we refer the readers to the original ViT paper. For few-shot counting, we use the exemplar encoder to extract the visual representation. It exploits a lightweight ConvNet architecture (4 convolutional layers, followed by a global average pooling), that maps the given exemplars (resized to the same resolution) into vectors,</p><formula xml:id="formula_3">F CNN = ? CNN-ENC (S k i ) ? R K?D (3)</formula><p>Note that, under the zero-shot scenario with no exemplar given, we adopt a learnable <ref type="bibr">[SPE]</ref> token as the substitute to provide cues for the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Feature Interaction Module</head><p>Here, we introduce the proposed feature interaction module (FIM), for fusing information from both encoders. Specifically, the FIM is constructed with a series of standard transformer decoder layers, where the image features act as the Query, and two different linear projections (by MLPs) of the exemplar features (or learnable special token), are treated as the Value and Key. With such design, the output from FIM remains the same dimensions as image features (F VIT ), throughout the interaction procedure:</p><formula xml:id="formula_4">F FIM = ? FIM (F VIT , W k ? F CNN , W v ? F CNN ) ? R M?D<label>(4)</label></formula><p>Conceptually, such transformer architecture perfectly reflects the self-similarity prior to the counting problem, as observed by Lu et al. <ref type="bibr" target="#b18">[19]</ref>. In particular, the self-attention mechanisms in transformer decoder enables to measure of the self-similarity between regions of the input image, while the cross-attention between Query and Value allows to compare image regions with the arbitrary given shots, incorporating users' input for more customised specification on the objects of interest, or simply learn to ignore the ConvNet branch while encountering the learnable [SPE] token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Decoder</head><p>At this stage, the outputs from the feature interaction module are further reshaped back to 2D feature maps and restored to the original resolution as the input image. We adopt a progressive up-sampling design, where the vector sequence is first reshaped to a dense feature map and then processed by a ConvNet-based decoder. Specifically, we use 4 up-sampling blocks, each of which consists of a convolution layer and a 2? bilinear interpolation. After the last up-sampling, we adopt a linear layer as the density regressor, which outputs a one-channel density heatmap:</p><formula xml:id="formula_5">y i = ? DECODER (F FIM ) ? R H?W ?1<label>(5)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Two-stage Training Scheme</head><p>In images, the visual signals are usually highly redundant, e.g. pixels within local regions are spatially coherent. Such prior is even more obvious in the counting problem, as the objects often tend to appear multiple times in a similar form. Based on such observation, we here consider exploiting the self-supervised learning to pre-train the visual encoder (? VIT-ENC (?)). Specifically, we adopt the recent idea from Masked Autoencoders (MAE), to train the model by image reconstruction with only partial observations.  Supervised Fine-tuning. After the pre-training, we initialise the image encoder with the weights of the pre-trained ViT, and fine-tune our proposed architecture on generalised object counting. In detail, our model takes the original image X i and K exemplars S i = {b i } K from D train as input and outputs the density map? i ? R H?W ?1 corresponding to the original image X i . The statistical number of salient objects in the image C i ? R can be obtained by summing the discrete density map? i . We use the mean square error per pixel to evaluate the difference between the predicted density map? i and the ground truth density map y i . The ground truth density maps are generated based on the dot annotations: L(? i , y i ) = 1 HW ? ||y i ?? i || 2 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Scalable Mosaicing</head><p>In this section, we introduce a scalable mosaic pipeline for synthesizing training images, in order to tackle the long-tailed problem (i.e. very few images contain a large number of instances) in existing counting datasets. We observe that existing datasets for generalised object counting are highly biased towards a small number of objects. For example, in the FSC-147 dataset, only 6 out of 3659 images in the train set contain more than 1000 objects. This is potentially due to the costly procedure for providing manual annotation. In the following, we elaborate on the two steps of the proposed mosaic training data generation, namely, collage and blending (as shown in <ref type="figure" target="#fig_2">Figure 2</ref>). Note that, we also notice one concurrent work <ref type="bibr" target="#b11">[12]</ref> uses a similar idea.</p><p>Collage. Here, we first crop a random-sized square area from the image and scale it to a uniform size, e.g. a quarter of the size of the original image. After repeating the region cropping multiple times, we collage the cropped regions together and update the corresponding density map. It comes in two different forms: using only one image or four different images.</p><p>If we only use one image, we can increase the number of objects contained in the image, which helps a lot with tackling the long-tail problem. If we use four different images, we can significantly improve the training images' background diversity and enhance the model's ability to distinguish between different classes of objects. To fully use these two advantages, (a) Test-time Normalisation.</p><p>(b) Test-time Cropping. <ref type="figure">Figure 3</ref>. The test-time normalisation process visualisation. In test-time normalisation, if the average sum of the exemplar positions in the density map is over 1.8, the sum of the density map will be divided by this average to become the final prediction. In test-time cropping, if at least one exemplar's side length is smaller than 10 pixels, the image will be cropped into 9 pieces and the model will process these 9 images separately. The final prediction will be the sum of the results of these 9 images.</p><p>we make the following settings. If the number of objects contained in the image is more than a threshold, we use the same image to collage; if not, we use four different images. Note that if four different images are used, we could only use the few-shot setting for inference, otherwise the model will not know which object to count. If we use the same image, the mosaiced image can be used to train the few-shot setting and zero-shot setting.</p><p>Blending. Simply cropping and collaging do not synthesize perfect images, as there remain sharp artifacts between the boundaries. To resolve these artifacts, we exploit blending at the junction of the images. In practise, we crop the image with a slightly larger size than a quarter of the original image size, such that we can leave a particular space at the border for ?-channel blending. We use a random ?-channel border width, which makes the image's composition more realistic. Note that, we only blend the original image instead of the density map, to maintain the form of dot annotation (only 0 and 1). Since there are few objects inside the blending border and the mosaic using one image is only applied to images with a very large number of objects, the error caused by blending is almost negligible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Test-time Normalisation</head><p>For few-shot counting, we have introduced a test-time normalisation strategy to calibrate the output density map in the main text. Specifically, at inference time, we exploit the prior knowledge that the object count at the exemplar position should exactly be 1.0, any prediction deviation can thus be calibrated by dividing the density map by the current predicted count at the exemplar position. We take this approach because due to the ambiguity of the bounding boxes, the model sometimes chooses the smallest self-similarity unit of an object to count, rather than the entire object, as shown in <ref type="figure">Figure 3 (a)</ref>. Therefore, if the average sum of the density map area corresponding to the bounding boxes exceeds a threshold, such as 1.8, we will exploit this test-time normalisation approach. Additionally, for images with tiny objects (one exemplar with a side length shorter than 10 pixels), we adopt a sliding window prediction, by dividing the image equally into nine pieces and scaling them to their original size, to be individually processed by our model. The total number of objects is the sum of the individual count results of the nine images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Here, we start by briefly introducing the few-shot counting benchmark, FSC-147 dataset, and the evaluation metrics; In Section 4.2, we describe the implementation details of our model and the design of the inference stage; In Section 4.3, we compare our model's performance with other counting models and demonstrate state-of-the-art performance on both zero-shot and few-shot settings; In Section 4.4, we conduct a series of ablation studies to demonstrate the effectiveness of the two-stage training and the image mosaicing; In Section 4.5, we give additional experiment results on Val-COCO, Test-COCO, and CARPK; In Section 4.6, we show the qualitative visualisation of CounTR's results on the FSC-147 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>Datasets. We experiment on FSC-147 <ref type="bibr" target="#b23">[24]</ref>, which is a multi-class few-shot object counting dataset containing 6135 images. Each image's number of counted objects varies widely, ranging from 7 to 3731, and the average is 56. The dataset also provides three randomly selected object instances annotated by bounding boxes as exemplars in each image. The training set has 89 object categories, while the validation and test sets both have 29 disjoint categories, making FSC-147 an open-set object counting dataset.</p><p>Metrics. We use two standard metrics to measure the performance of our model, namely, Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE).</p><formula xml:id="formula_6">MAE = 1 N I N I ? i=1 |C i ?C GT i |, RMSE = 1 N I N I ? i=1 (C i ?C GT i ) 2<label>(6)</label></formula><p>Here, N I is the total number of testing images, and C i and C GT i are the predicted number and ground truth of the i th image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Training Details</head><p>In this section, we aim to give the detail of our proposed two-stage training procedure, that is, first pre-train the ViT encoder with MAE <ref type="bibr" target="#b10">[11]</ref>, and then fine-tune the whole model on supervised object counting.</p><p>MAE Pre-training. As input, the image is of size 384 ? 384, which is first split into patches of size 16 ? 16, and projected into 576 vectors. Our visual encoder uses 12 transformer encoder blocks with a hidden dimension of 768, and the number of heads in the multi-head self-attention layer is 12. The decoder uses 8 transformer layers with a hidden dimension of 512. As input for pre-training ViT with MAE, we randomly drop 50% of the visual tokens, and task the model to reconstruct the masked patches with pixel-wise mean square error. During pre-training, we chose a batch size of 16 and trained on the FSC-147 for 300 epochs with a learning rate of 5 ? 10 ?6 .</p><p>Fine-tuning stage. The feature interaction module uses 2 transformer decoder layers with a hidden dimension of 512. The ConvNet encoder exploits 4 convolutional layers and a global average pooling layer to extract exemplar features with 512 dimensions. The image decoder uses 4 up-sampling layers with a hidden dimension of 256. For optimisation, we minimise the mean square error between the model's prediction and the ground truth density map, which is generated with Gaussians centering each object. We scale the loss by a factor of 60, and randomly drop 20% non-object pixels, to alleviate the sample imbalance issue. We use AdamW as the optimiser. Our model is trained on the FSC-147 training set with a learning rate of 1 ? 10 ?5 and a batch size of 8. Our model is trained and tested on NVIDIA GeForce RTX 3090.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Inference Details</head><p>At inference time, we adopt sliding windows for images of different resolutions, with the model processing a portion of an image with a fixed-size square window as used in training, and gradually moving forward with a stride of 128 pixels. The density map for overlapped regions is simply computed by averaging the predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to state-of-the-art</head><p>We evaluate the proposed CounTR model on the FSC-147 dataset and compare it against existing approaches. As shown in <ref type="table" target="#tab_0">Table 1</ref>, CounTR has demonstrated new state-of-the-art on both zero-shot and few-shot counting, outperforming the previous methods significantly, especially on the results of the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Year </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>In this section, we have conducted thorough ablation studies to demonstrate the effectiveness of the proposed ideas, as shown in <ref type="table" target="#tab_1">Table 2</ref>, we can make the following observations: (1)  <ref type="table">Table 3</ref>. Comparison with state-of-the-art on the FSC-147 subset.</p><p>CARPK. CARPK <ref type="bibr" target="#b12">[13]</ref> is a class-specific car counting benchmark with 1448 images of parking lots from a bird's view. We also fine-tuned our model on the CARPK train set and test on it with Non-Maximum Suppression (NMS). We compared our CounTR model with several detection-based object counting models and regression-based few-shot counting models. As shown in <ref type="table" target="#tab_2">Table 4</ref>, even compared with the existing class-specific counting models, i.e., the models that can only count cars, our CounTR still shows comparable performance. We show qualitative results from our few-shot counting setting in <ref type="figure">Figure 4</ref>. As we can see from the first five images from FSC-147, our model can easily count the objects' numbers and locate their position. The last image mistakenly chose the smallest self-similarity unit of spectacle lenses instead of sunglasses for counting due to the ambiguity of the bound boxes, which can be corrected by test-time normalisation. For more qualitative visualisation, please refer to <ref type="figure" target="#fig_3">Figure 5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we aim at the generalised visual object counting problem of counting the number of objects from arbitrary semantic categories using arbitrary number of "exemplars". We propose a novel transformer-based architecture for it, termed as CounTR. It is first pretrained with self-supervised learning, and followed by supervised fine-tuning. We also propose a simple, scalable pipeline for synthesizing training images that can explicitly force the model to make use of the given "exemplars". Our model achieves state-of-the-art performance on both zero-shot and few-shot settings.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Architecture detail for CounTR. The query image and exemplars are encoded by separate visual encoders. The image features are then fed into the feature interaction module as query vectors, and the exemplar features are fed as key and value vectors. When there is no instance exemplar provided, a learnable [SPE]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) Type A: using four images.(b) Type B: using one image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The mosaic pipeline for synthesizing training images. (1) stands for crop and scale, and (2) stands for collage and blending. In the following section, we will combine crop, scale, and collage as the collage stage. Type A uses four different images to improve background diversity and Type B uses only one image to increase the number of objects contained in an image. White highlights are the dot annotation density map after Gaussian filtering for visualization.Self-supervised Pre-training with MAE. In detail, we first divide the image into regular non-overlapping patches, and only sample a subset of the patches (50% in our case) as input to the ViT encoders. The computed features are further passed through a lightweight decoder, consisting of several transformer decoder layers, where the combination of learnable mask tokens and positional encoding is used as Query to reconstruct the input image from only observed patches. The training loss is simply defined as the Mean Squared Error (MSE) between the reconstructed image and the input image in pixel space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>More qualitative results of CounTR on FSC-147.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison with state-of-the-art on the FSC-147 dataset. P-GMN stands for Pretrained GMN. RepRPN-C stands for RepRPN-Counter. RCC stands for reference-less class-agnostic counting with weak supervision.</figDesc><table><row><cell></cell><cell></cell><cell>Backbone</cell><cell># Shots</cell><cell></cell><cell>Val</cell><cell></cell><cell>Test</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MAE</cell><cell>RMSE</cell><cell>MAE</cell><cell>RMSE</cell></row><row><cell>RepRPN-C [23]</cell><cell>Arxiv2022</cell><cell>ConvNets</cell><cell>0</cell><cell>31.69</cell><cell>100.31</cell><cell>28.32</cell><cell>128.76</cell></row><row><cell>RCC [12]</cell><cell>Arxiv2022</cell><cell>Pre-trained ViT</cell><cell>0</cell><cell>20.39</cell><cell>64.62</cell><cell>21.64</cell><cell>103.47</cell></row><row><cell>CounTR (ours)</cell><cell>2022</cell><cell>ViT</cell><cell>0</cell><cell>17.40</cell><cell>70.33</cell><cell>14.12</cell><cell>108.01</cell></row><row><cell>FR [14]</cell><cell>ICCV2019</cell><cell>ConvNets</cell><cell>3</cell><cell>45.45</cell><cell>112.53</cell><cell>41.64</cell><cell>141.04</cell></row><row><cell>FSOD [7]</cell><cell>CVPR2020</cell><cell>ConvNets</cell><cell>3</cell><cell>36.36</cell><cell>115.00</cell><cell>32.53</cell><cell>140.65</cell></row><row><cell>P-GMN [19]</cell><cell>ACCV2018</cell><cell>ConvNets</cell><cell>3</cell><cell>60.56</cell><cell>137.78</cell><cell>62.69</cell><cell>159.67</cell></row><row><cell>GMN [19]</cell><cell>ACCV2018</cell><cell>ConvNets</cell><cell>3</cell><cell>29.66</cell><cell>89.81</cell><cell>26.52</cell><cell>124.57</cell></row><row><cell>MAML [8]</cell><cell>ICML2017</cell><cell>ConvNets</cell><cell>3</cell><cell>25.54</cell><cell>79.44</cell><cell>24.90</cell><cell>112.68</cell></row><row><cell>FamNet [24]</cell><cell>CVPR2021</cell><cell>ConvNets</cell><cell>3</cell><cell>23.75</cell><cell>69.07</cell><cell>22.08</cell><cell>99.54</cell></row><row><cell>BMNet+ [27]</cell><cell>CVPR2022</cell><cell>ConvNets</cell><cell>3</cell><cell>15.74</cell><cell>58.53</cell><cell>14.62</cell><cell>91.83</cell></row><row><cell>CounTR (ours)</cell><cell>2022</cell><cell>ViT</cell><cell>3</cell><cell>13.13</cell><cell>49.83</cell><cell>11.95</cell><cell>91.23</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Data augmentation: While comparing the Model-A, we include image-wise data augmentation in Model-B training, including Gaussian noise, Gaussian blur, horizontal flip, color jittering, and geometric transformation. As indicated by the result, Model B slightly outperforms Model A on both validation and test set, suggesting that these augmentation methods can indeed be useful to the model to a certain extent, however, is limited. (2) Self-supervised pre-training: In Model-C, we introduce the self-supervised pre-training for warming up the ViT encoder. Compared with Model B which directly fine-tunes the ViT encoder (pretrained on ImageNet) on the FSC-147 training set, Model C has improved all results on both validation and test sets significantly. (3) Effectiveness of mosaic: With the help of the mosaic method, Model-D has shown further performance improvements, demonstrating its effectiveness for resolving the challenge from the long-tailed challenge, by introducing images with a large number of object instances, and object distractors from different semantic categories. (4) Test-time normalisation: In Model-E, we experiment with test-time normalisation for the few-shot counting scenario, where the output prediction is calibrated by the given exemplar shot. On both validation and test set, test-time normalisation has demonstrated significant performance boosts. (5) On shot number: In Model E, as the number of given shots increases, e.g. 1, 2, or 3, we observe no tiny difference in the final performance, showing the robustness of our proposed CounTR for visual object counting under any shots. Ablation study. We observe that one image in the test set (image id:7171) has significant annotation error (see supp. material), result without it has also been reported. Selfsup: refers to the proposed two-stage training regime. TT-Norm: denotes test-time normalisation4.5 Additional ExperimentsIn this section, we further evaluate the model on several other datasets, e.g., Val-COCO, Test-COCO, and CARPK.Net<ref type="bibr" target="#b23">[24]</ref>, and our model outperforms it with a large advance (15.16 MAE and 24.29 RMSE on Val-COCO and 11.87 MAE and 14.81 RMSE on Test-COCO), which demonstrates the superiority of our model.</figDesc><table><row><cell>Model</cell><cell>Augmentation Selfsup Mosaic TT-Norm. # Shots</cell><cell>Val</cell><cell>Test</cell></row><row><cell></cell><cell></cell><cell cols="2">MAE RMSE MAE RMSE</cell></row><row><cell>A0</cell><cell>0</cell><cell cols="2">24.84 86.33 21.06 130.04</cell></row><row><cell>A1</cell><cell>3</cell><cell cols="2">24.68 85.89 20.98 129.58</cell></row><row><cell>B0</cell><cell>0</cell><cell cols="2">23.80 81.53 21.14 131.27</cell></row><row><cell>B1</cell><cell>3</cell><cell cols="2">23.67 81.40 20.93 130.75</cell></row><row><cell>C0</cell><cell>0</cell><cell cols="2">18.30 72.21 16.20 114.30</cell></row><row><cell>C1</cell><cell>3</cell><cell cols="2">18.19 71.47 16.05 113.11</cell></row><row><cell>D0</cell><cell>0</cell><cell cols="2">18.07 71.84 14.71 106.87</cell></row><row><cell>D1</cell><cell>3</cell><cell cols="2">17.40 70.33 14.12 108.01</cell></row></table><note>Val-COCO and Test-COCO. Val-COCO and Test-COCO [24] are FSC-147 subsets col- lected from COCO, and they are often used as evaluation benchmarks for detection-based object counting models. Here we compared our CounTR model with several counting mod- els based on detection including Faster-RCNN [26], RetinaNet [18], and Mask-RCNN [10]. As shown in Table 3, it can be easily found that our model still has a huge improvement even compared to the best-performing Mask-RCNN [10], halving its error on both Val-COCO and Test-COCO. We also compared our model with the few-shot counting sota method Fam-</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 .</head><label>4</label><figDesc>Comparison with state-of-the-art on the CARPK dataset.</figDesc><table><row><cell>Methods</cell><cell>Year</cell><cell>Method</cell><cell>Type</cell><cell cols="2">CARPK</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>MAE</cell><cell>RMSE</cell></row><row><cell>YOLO [25]</cell><cell>CVPR2016</cell><cell>Detection</cell><cell>Generic</cell><cell>48.89</cell><cell>57.55</cell></row><row><cell>Faster-RCNN [26]</cell><cell>NIPS2015</cell><cell>Detection</cell><cell>Generic</cell><cell>47.45</cell><cell>57.39</cell></row><row><cell>S-RPN [13]</cell><cell>ICCV2017</cell><cell>Detection</cell><cell>Generic</cell><cell>24.32</cell><cell>37.62</cell></row><row><cell>RetinaNet [18]</cell><cell>ICCV2017</cell><cell>Detection</cell><cell>Generic</cell><cell>16.62</cell><cell>22.30</cell></row><row><cell>LPN [13]</cell><cell>ICCV2017</cell><cell>Detection</cell><cell>Generic</cell><cell>23.80</cell><cell>36.79</cell></row><row><cell>One Look [22]</cell><cell>ECCV2016</cell><cell>Detection</cell><cell>Specific</cell><cell>59.46</cell><cell>66.84</cell></row><row><cell>IEP Count [28]</cell><cell>TIP2018</cell><cell>Detection</cell><cell>Specific</cell><cell>51.83</cell><cell>-</cell></row><row><cell>PDEM [9]</cell><cell>CVPR2019</cell><cell>Detection</cell><cell>Specific</cell><cell>6.77</cell><cell>8.52</cell></row><row><cell>GMN [19]</cell><cell>CVPR2021</cell><cell>Regression</cell><cell>Generic</cell><cell>7.48</cell><cell>9.90</cell></row><row><cell>FamNet [24]</cell><cell>CVPR2021</cell><cell>Regression</cell><cell>Generic</cell><cell>18.19</cell><cell>33.66</cell></row><row><cell>BMNet+ [27]</cell><cell>CVPR2022</cell><cell>Regression</cell><cell>Generic</cell><cell>5.76</cell><cell>7.83</cell></row><row><cell>CounTR (ours)</cell><cell>2022</cell><cell>Regression</cell><cell>Generic</cell><cell>5.75</cell><cell>7.45</cell></row></table><note>4.6 Qualitative Results Figure 4. Qualitative results of CounTR on FSC-147. For visualisation purpose, we have overlaid the predited density map on the original image. TTN stands for test-time augmentation.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">(a) The exemplar annotation of 7171.jpg. (b) The ground truth annotation is 14. Figure 6. The ground truth annotation and exemplar annotation of 7171.jpg, and we can easily figure out the inconsistency.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement AZ is supported by EPSRC Programme Grant VisualAI EP/T028572/1, a Royal Society Research Professorship RP\R1\191132. We thank Xiaoman Zhang and Chaoyi Wu for proof-reading.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A Appendix</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Interactive object counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Counting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Arteta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On detection of multiple object instances using hough transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kholi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A neural-based crowd estimation by hybrid global learning algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siu-Yeung</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Tommy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Tat</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
	<note>Part B (Cybernetics)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative models for multi-class object layout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaitanya</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charless C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Few-shot object detection with attention-rpn and multi-relation detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi-Keung</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Wing</forename><surname>Tai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Precise detection in densely packed scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aviv</forename><surname>Eisenschtat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Hassner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Learning to count anything: Reference-less class-agnostic counting with weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Hobley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Prisacariu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.10203</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Drone-based object counting by spatially regularized regional proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng-Ru</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Liang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Winston H</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fewshot object detection via feature reweighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bingyi</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A viewpoint invariant approach for crowd counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">18th International Conference on Pattern Recognition (ICPR&apos;06)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The discrimination of visual number</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Lord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">W</forename><surname>Reese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Volkmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Journal of Psychology</title>
		<imprint>
			<date type="published" when="1949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning to count objects in images. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Kaiming He, and Piotr Doll?r. Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Class-agnostic counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erika</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Estimation of crowd density using image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aparecido Nilceu Marana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sa Velastin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lotufo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Emerging properties in self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caron</forename><surname>Mathilde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision</title>
		<meeting>the International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A large contextual dataset for classification, detection and counting of cars with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goran</forename><surname>T Nathan Mundhenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Konjevod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wesam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kofi</forename><surname>Sakla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boakye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Exemplar free class agnostic counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viresh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.14212</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning to count everything</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viresh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Udbhav</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thu</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minh</forename><surname>Hoai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards realtime object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Represent, compare, and learn: A similarity-aware framework for class-agnostic counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengxin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Guo</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Divide and count: Generic object counting by image divisions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Stahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Silvia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan C Van</forename><surname>Pintea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Microscopy cell counting and detection with fully convolutional regression networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidi</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alison</forename><surname>Noble</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer methods in biomechanics and biomedical engineering: Imaging &amp; Visualization</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Few-shot object counting with similarity-aware feature enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.08959</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
