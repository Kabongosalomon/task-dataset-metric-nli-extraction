<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MaGNET : UNIFORM SAMPLING FROM DEEP GENERA- TIVE NETWORK MANIFOLDS WITHOUT RETRAINING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2022-01-21">21 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmed</forename><forename type="middle">Imtiaz</forename><surname>Humayun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Balestriero</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Baraniuk</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Rice University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">MaGNET : UNIFORM SAMPLING FROM DEEP GENERA- TIVE NETWORK MANIFOLDS WITHOUT RETRAINING</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-21">21 Jan 2022</date>
						</imprint>
					</monogr>
					<note>Accepted for publication at ICLR 2022 1 Accepted for publication at ICLR 2022</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T04:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep Generative Networks (DGNs) are extensively employed in Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and their variants to approximate the data manifold and distribution. However, training samples are often distributed in a non-uniform fashion on the manifold, due to costs or convenience of collection. For example, the CelebA dataset contains a large fraction of smiling faces. These inconsistencies will be reproduced when sampling from the trained DGN, which is not always preferred, e.g., for fairness or data augmentation. In response, we develop MaGNET, a novel and theoretically motivated latent space sampler for any pre-trained DGN, that produces samples uniformly distributed on the learned manifold. We perform a range of experiments on various datasets and DGNs, e.g., for the state-of-the-art StyleGAN2 trained on FFHQ dataset, uniform sampling via MaGNET increases distribution precision and recall by 4.1% &amp; 3.0% and decreases gender bias by 41.2%, without requiring labels or retraining. As uniform distribution does not imply uniform semantic distribution, we also explore separately how semantic attributes of generated samples vary under MaGNET sampling. Figure 1: Random batches of StyleGAN2 (? = 0.5) samples with 1024 ? 1024 resolution, generated using standard sampling (left), uniform sampling via MaGNET on the learned pixel-space manifold (middle), and uniform sampling on the style-space manifold (right) of the same model. MaGNET sampling yields a higher number of young faces, better gender balance, and greater background/accessory variation, without the need for labels or retraining. Images are sorted by gender-age and color coded red-green (female-male) according to Microsoft Cognitive API predictions. Larger batches of images and attribute distributions are furnished in Appendix E.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deep Generative Networks (DGNs) are Deep Networks (DNs) trained to learn latent representations of datasets; such frameworks include Generative Adversarial Networks (GANs) <ref type="bibr" target="#b27">(Goodfellow et al., 2014)</ref>, Variational Autoencoders (VAEs) <ref type="bibr" target="#b38">(Kingma &amp; Welling, 2013)</ref>, flow-based models such as NICE <ref type="bibr" target="#b19">(Dinh et al., 2014)</ref>, and their variants <ref type="bibr" target="#b22">(Dziugaite et al., 2015;</ref><ref type="bibr" target="#b69">Zhao et al., 2016;</ref><ref type="bibr" target="#b21">Durugkar et al., 2016;</ref><ref type="bibr" target="#b44">Mao et al., 2017;</ref><ref type="bibr" target="#b67">Yang et al., 2019;</ref><ref type="bibr" target="#b24">Fabius &amp; van Amersfoort, 2014;</ref><ref type="bibr" target="#b63">van den Oord et al., 2017;</ref><ref type="bibr" target="#b32">Higgins et al., 2017;</ref><ref type="bibr" target="#b60">Tomczak &amp; Welling, 2017;</ref><ref type="bibr" target="#b18">Davidson et al., 2018;</ref><ref type="bibr" target="#b20">Dinh et al., 2016;</ref><ref type="bibr" target="#b28">Grathwohl et al., 2018;</ref><ref type="bibr" target="#b39">Kingma &amp; Dhariwal, 2018)</ref>. A common assumption that we will carry through our study is that the datasets of interest are not uniformly distributed in their ambient space, but rather are concentrated on, or around, manifolds of lower intrinsic dimension, e.g., the manifold of natural images <ref type="bibr" target="#b48">(Peyr?, 2009)</ref>. Different DGN training methods have been developed and refined to obtain models that approximate as closely as possible the training set distribution. This becomes an Achilles heel when the training set, regardless of its size, is not representative of the true data distribution, i.e., when the training samples have been curated based on cost or availability that result in implicit/explicit biases. In such scenarios, while the training samples will lie on the true data manifold, the density distribution of the training set will be different from the natural distribution of the data.</p><p>Deploying a DGN trained with a biased data distribution can be catastrophic, in particular, when employed for tasks such as data augmentation <ref type="bibr" target="#b53">(Sandfort et al., 2019)</ref>, controlled data generation for exploration/interpretation <ref type="bibr" target="#b58">(Thirumuruganathan et al., 2020)</ref>, or estimation of statistical quantities of the data geometry, such as the Lipschitz constant of the data manifold <ref type="bibr" target="#b29">(Gulrajani et al., 2017;</ref><ref type="bibr" target="#b54">Scaman &amp; Virmaux, 2018)</ref>. Biased data generation from DGNs due to skewed training distributions also raises serious concerns in terms of fair machine learning <ref type="bibr" target="#b33">(Hwang et al., 2020;</ref><ref type="bibr" target="#b56">Tan et al., 2020)</ref>. While ensuring semantic uniformity in samples is an extremely challenging task, we take one step in the more reachable goal of controlling the DGN sampling distribution to be uniform in terms of the sample distribution on the data manifold. To that end, we propose MaGNET (for Maximum entropy Generative NETwork), a simple and efficient modification to any DGN that adapts its latent space distribution to provably produce samples uniformly spread on the DGN learned manifold. Importantly, MaGNET can be employed on any pre-trained and differentiable DGN regardless of its training setting, reducing the requirement of fine-tuning or retraining of the DGN. This is crucial as many models, such as BigGAN <ref type="bibr" target="#b14">(Brock et al., 2019)</ref> and StyleGAN <ref type="bibr" target="#b37">(Karras et al., 2020)</ref>, have significant computational and energy requirements for training. A plug-and-play method is thus greatly preferred to ease deployment in any already built/trained deep learning pipeline.</p><p>Previously, there has been rigorous work on DGNs aimed at improving the training stability of models, deriving theoretical approximation results, understanding the role of the DGN architectures, and numerical approximations to speed-up training and deployment of trained models <ref type="bibr" target="#b44">(Mao et al., 2017;</ref><ref type="bibr" target="#b46">Miyato et al., 2018;</ref><ref type="bibr" target="#b66">Xu &amp; Durrett, 2018;</ref><ref type="bibr" target="#b68">Zhang et al., 2017;</ref><ref type="bibr" target="#b12">Biau et al., 2018;</ref><ref type="bibr" target="#b40">Kodali et al., 2017;</ref><ref type="bibr" target="#b50">Roy et al., 2018;</ref><ref type="bibr" target="#b0">Andr?s-Terr? &amp; Li?, 2019;</ref><ref type="bibr" target="#b8">Balestriero et al., 2020a;</ref><ref type="bibr" target="#b59">Tomczak &amp; Welling, 2016;</ref><ref type="bibr" target="#b11">Berg et al., 2018)</ref>. Existing methods <ref type="bibr" target="#b45">(Metz et al., 2016;</ref><ref type="bibr" target="#b57">Tanaka, 2019;</ref><ref type="bibr" target="#b15">Che et al., 2020</ref>) also try to tackle mode dropping by improving approximation of the data distribution, but this can potentially increase the bias learned implicitly by the DGNs. We are the first to consider the task of providing uniform sampling on the DGN underlying manifold, which has far-reaching consequences, ranging from producing DGNs stable to data curation and capable of handling inconsistencies such as repeated samples in the training set. We provide a first-of-its-kind provable uniform sampling on the data manifold that can be used to speed up estimation of various geometric quantities, such as estimation of the Lipschitz constant.</p><p>MaGNET applies to any (pretrained) DGN architecture (GAN, VAE, NF, etc.) using continuous piecewise affine (CPA) nonlinearities, such as the (leaky) ReLU; smooth nonlinearities can be dealt with via a first-order Taylor approximation argument. Our main contributions are as follows:</p><p>[C1] We characterize the transformation incurred by a density distribution when composed with a CPA mapping (Sec. 3.1) and derive the analytical sampling strategy that enables one to obtain a uniform distribution on a manifold that is continuous and piecewise affine (Sec 3.2).</p><p>[C2] We observe that current DGNs produce continuous piecewise affine manifolds, and we demonstrate how to leverage [C1] to produce uniform sampling on the manifold of any DGN (Sec. 3.2).</p><p>[C3] We conduct several carefully controlled experiments that validate the importance of uniform sampling and showcase the performance of MaGNET on pretrained models such as BigGAN <ref type="bibr" target="#b14">(Brock et al., 2019)</ref>, StyleGAN2 <ref type="bibr" target="#b37">(Karras et al., 2020)</ref>, progGAN <ref type="bibr" target="#b35">(Karras et al., 2017)</ref>, and NVAE <ref type="bibr" target="#b62">(Vahdat &amp; Kautz, 2020)</ref>, e.g., we show that MaGNET can be used to increase distribution precision and recall of StyleGAN2 by 4.1% &amp; 3.0% and decrease gender bias by 41.2%, without requiring labels or retraining <ref type="bibr">(Sec. 4.2 and Sec. 4.3)</ref>.</p><p>Reproducible code for the various experiments and figures will be provided upon completion of the review process. Computation and software details are provided in Appendix H, with the proofs of our results in Appendix I. Discussion on the settings in which MaGNET is desirable and on possible limitations is provided in (Sec. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Continuous Piecewise Affine (CPA) Mappings. A rich class of functions emerge from piecewise polynomials: spline operators. In short, given a partition ? of a domain R S , a spline of order k is a mapping defined by a polynomial of order k on each region ? ? ? with continuity constraints on the entire domain for the derivatives of order 0,. . . ,k ? 1. As we will focus entirely on affine splines (k = 1) we only define this case for concreteness. An affine spline S produces its output via</p><formula xml:id="formula_0">S(z) = ??? (A ? z + b ? )1 {z??} ,<label>(1)</label></formula><p>with input z and A ? , b ? the per-region slope and offset parameters respectively, with the key constraint that the entire mapping is continuous on the domain S ? C 0 (R S ). Spline operators and especially affine spline operators have been extensively used in function approximation theory <ref type="bibr" target="#b17">(Cheney &amp; Light, 2009</ref>), optimal control <ref type="bibr" target="#b23">(Egerstedt &amp; Martin, 2009</ref>), statistics <ref type="bibr" target="#b25">(Fantuzzi et al., 2002)</ref> and related fields.</p><formula xml:id="formula_1">Deep Generative Networks. A deep generative network (DGN) is a (nonlinear) operator G ? with parameters ? mapping a latent input z ? R S to an observation x ? R D by composing L intermediate layer mappings.</formula><p>The only assumption we require for our study is that the nonlinearities present in the DGN are continuous piecewise affine as is the case with (leaky-)ReLU, absolute value, max-pooling. For smooth nonlinearities, our results hold from a first-order Taylor approximation argument. Precise definitions of DGN operators can be found in <ref type="bibr" target="#b26">Goodfellow et al. (2016)</ref>. We will omit ? from the G ? operator for conciseness unless needed. It is also common to refer to z as the latent representation, and x as the generated/observed data, e.g., a time-series or image. One property of DGNs that employ nonlinearities such as (leaky-)ReLU, max-pooling, and the likes, is that the entire input-output mapping becomes a CPA spline with z as both the argument and region parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CONTINUOUS PIECEWISE AFFINE MAPPING OF A PROBABILITY DENSITY</head><p>In this section, we study the properties of a probability density that is transformed by a CPA mapping. Our goal is to derive the produced density and characterize its properties, such as how the per-region affine mappings in Eq. 1 impact the density concentration. We present some key results that serve as the backbone of our core result in the next section: how to sample uniformly from the manifold generated by DGNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DENSITY ON THE GENERATED MANIFOLD</head><p>Consider an affine spline operator S (Eq. 1) going from a space of dimension S to a space of dimension D with D ? S. The image of this mapping is a CPA manifold of dimension at most S, the exact dimension is determined by the rank of the per-region slope matrices. Formally, the image of S is given by From Eq. 2 we observe that the generated manifold i.e., surface is made of regions that are the affine transformations of the latent space partition regions ? ? ? based on the coordinate change induced by A ? and the shift induced by b ? . We visualize this in <ref type="figure" target="#fig_1">Fig. 2</ref> for a toy spline operator with 2dimensional latent space and 3-dimensional ambient/output space. In the remainder of our study we will denote for conciseness S(?) Aff(?; A ? , b ? ). When the input space is equipped with a density distribution, then this density is transformed by the mapping S and "lives" on the surface of the CPA manifold generated by S. Given a distribution p z over the latent space, we can explicitly compute the output distribution after the application of S, which leads to an intuitive result exploiting the CPA property of the generator. For this result, we require that operator S be bijective between its domain and its image. That is, each slope matrix A ? , ?? ? ? should be full rank, and there should not be any folding of the generated CPA surface that intersects with itself, i.e., S(?) ? S(? ) = {} ?? ? = ? . We now derive the key result of this section that characterizes the density distribution lying on the manifold.</p><formula xml:id="formula_2">Im(S) {S(z) : z ? R S } = ??? Aff(?; A ? , b ? ) (2) with Aff(?; A ? , b ? ) = {A ? z + b ? : z ? ?}</formula><p>Lemma 1. The volume of a region ? ? ? denoted by ?(?) is related to the volume affinely transformed region S(?) by,</p><formula xml:id="formula_3">?(S(?)) ?(?) = det(A T ? A ? ),<label>(3)</label></formula><p>where ?(S(?)) is using the measure on the S-dimensional affine subspace spanned by the CPA mapping. (Proof in Appendix I.1.)</p><p>Theorem 1. The probability density p S (x) generated by S for latent space distribution p z is given by,</p><formula xml:id="formula_4">p S (x) = ??? p z A T ? A ? ?1 A T ? (x ? b ? ) det(A T ? A ? ) 1 {x?S(?)} .<label>(4)</label></formula><p>(Proof in Appendix I.2.)</p><p>In words, the distribution obtained in the output space naturally corresponds to a piecewise affine transformation of the original latent space distribution, weighted by the change in volume of the perregion mappings from (Eq. 3). For Gaussian and Uniform p z , we use the above results to obtain the analytical form of the density covering the output manifold, we have provided proof and differential entropy derivations in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MAKING THE DENSITY ON THE MANIFOLD UNIFORM</head><p>The goal of this section is to build on Thm. 1 to provide a novel latent space distribution such that the density distribution lying on the generated manifold is uniform.</p><p>One important point that we highlight is that having a Uniform density distribution in the latent space of the affine spline is not enough to have a uniform density lying on the manifold; it would be if det(A T ? A ? ) = det(A T ? A ? ), ?? = ? , in words, the change in volume of the per region mapping is equal for all ?. This is evident from Appendix B (Eq. 8). So we propose here a novel latent space sampler with the purpose that once transformed by the affine spline i.e., the DGN, that distribution becomes uniform on the DGN manifold. We focus here on the technical aspect and provide precise motivations behind such construction in the next section that deals with practical applications. To obtain K samples uniformly distributed on the output manifold of S:</p><p>1. For K MaGNET samples, sample N K (as large as possible) iid latent vectors with U being the latent space domain of S (z 1 , . . . , z N ), with z i ? U(U ) 2. Compute the per-region slope matrices A i J S (z i ) (Eq. 1) and the change of volume</p><formula xml:id="formula_5">scalar (? 1 , . . . , ? N ) det(A T 1 A 1 ), . . . , det(A T N A N ) , where A i = A ? 1 {zi??} 3. Sample (with replacement) K latent vectors (z 1 , . . . , z K ) with probability ? (? 1 , . . . , ? N )</formula><p>We discuss the possible choices of N and K in Appendix D, where we observe that, even for stateof-the-art models like StyleGAN2, N =250,000 is sufficient to provide a stable approximation of the true latent space target distribution. In practice, A i is simply obtained through backpropagation as it is the Jacobian matrix of the DGN at z i , as in A i = J S(z i ).</p><p>The above provides a Monte-Carlo approximation that does not require knowledge of the partition ? nor the per-region slope matrices (Eq. 1). Those are computed on-demand as z i are sampled. The above procedure produces uniform samples on the manifold learned by a DGN regardless of how it has been trained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">MAGNET: MAXIMUM ENTROPY GENERATIVE NETWORK SAMPLING</head><p>The goal of this section is to first bridge current DGNs with affine splines, then leverage Thm. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">UNIFORM SAMPLING ON THE DEEP GENERATIVE NETWORK MANIFOLD</head><p>We proposed in the previous section a thorough study of affine splines and how those mappings transform a given input distribution. This now takes high relevance as per the following remark.</p><p>Remark 1. Any DGN (or part of it) that employs CPA nonlinearities (as in Sec. 2) is itself a CPA; that is, the input-output mapping can be expressed as in (Eq. 1).</p><p>This observation in the context of classifier DNs goes back to <ref type="bibr" target="#b47">Montufar et al. (2014)</ref> and has been further studied in Unser <ref type="formula" target="#formula_0">(2018)</ref>; <ref type="bibr" target="#b5">Balestriero &amp; Baraniuk (2018b)</ref>. We also shall emphasize that operators such as Batch-Normalization <ref type="bibr" target="#b34">(Ioffe &amp; Szegedy, 2015)</ref> are not continuous piecewise affine during training but become affine operators during evaluation time. For completeness, we also provide that analytical form of the per-region affine mappings A ? , b ? of Eq. 1 in the case of DGNs in Sec. C in the appendix. The key for our method is thus to combine the above with the results from Sec. 3.2 to obtain the following statement. This result follows by leveraging the analytical DGN distribution from Thm. 1 and by replacing p z with the proposed one, leading to p S (x) ? ??? 1 {x?S(?)} i.e. being uniform on the DGN manifold. By using the above one can take any (trained) DGN and produce uniform samples on the learned underlying manifold. Hence, our solution allows to produce a generative process that becomes invariant to the training set distribution. While this provides a theoretical guarantee for uniform sampling, it also highlight the main limitation of MaGNET: the uniform samples will be lying on a continuous piecewise affine manifold. That is, unless the true manifold M is also continuous, MaGNET will occasionally introduce abnormal samples that correspond to sampling from the regions of discontinuity of M. We will see in the following sections how even on high-quality image datasets, MaGNET produces very few abnormal samples, one reason being that for complicated data manifold, state-of-the-art DGNs are often built with a (class) conditioning. That is, the above continuity assumption on M lessens only to a within-class continuity assumption which is a much more realistic assumption. Sampling uniformly on the DGN manifold has many important applications that are deferred to the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">QUANTITATIVE VALIDATION: -BALL CONCENTRATION, GMM LIKELIHOOD AND FR?CHET INCEPTION DISTANCE</head><p>We now report three controlled experiments to validate the applicability of the theoretical results from Sec. 3.2 for the MaGNET sampling procedure.</p><p>First, we consider MNIST and assume that the entire data manifold is approximately covered by the training samples. Regardless of the training data distribution on the manifold (uniform or not), we can pick a datum at random, count how many generated samples (?) are within this datum -ball neighborhood and repeat this process for 10,000 training samples. If this count does not vary between training datum, then it strongly indicates that the generated samples are uniformly distributed on the manifold covered by the training data. We perform this experiment using a pretrained stateof-the-art variational autoencoder NVAE <ref type="bibr" target="#b62">(Vahdat &amp; Kautz, 2020)</ref> to compare between standard and MaGNET sampling with the number of generated samples N ranging from 1,000 to 10,000. We report the distribution of counts in <ref type="figure" target="#fig_3">Fig. 3</ref>. Again, uniform sampling is equivalent to having the same count for all training samples, i.e., a Dirac distribution in the reported histograms. We can see that MaGNET sampling approaches that distribution while standard sampling has a heavy-tail count distribution, i.e., the generated digits have different concentration at different parts of the data manifold. Another quantitative measure consists of fitting a Gaussian Mixture Model (GMM) with varying number of clusters, on the generated data, and comparing the likelihood obtained for standard and MaGNET sampling. As we know that in both cases the samples lie on the same manifold and domain, the sampling with lower likelihood will correspond to the one for which samples are spread more uniformly on the manifold. We report this in <ref type="figure">Fig. 4</ref>, further confirming the ability of MaG-NET to produce uniformly spread samples. We report the generated samples in Appendix E. Lastly, we compare the Fr?chet Inception Distance (FID) <ref type="bibr" target="#b31">(Heusel et al., 2017)</ref> between 50,000 generated samples and 70,000 training samples for StyleGAN2 trained on FFHQ. Since uniform sampling via MaGNET increases the diversity of generated samples, we see that MaGNET sampling improves the FID for truncation <ref type="bibr" target="#b36">(Karras et al., 2019;</ref><ref type="bibr" target="#b14">Brock et al., 2019)</ref>, ? = {.4, .5, .6, .7} by 2.76 points on average (see Appendix F). While for the aforementioned ? MaGNET samples alone provide an improved FID, for higher ? values, we introduce an increasing amount of MaGNET samples for FID calculation. We observe in <ref type="figure">Fig. 4</ref> that by progressively increasing the percentage of MaGNET samples, we are able to exceed the state-of-the-art FID of 2.74 for StyleGAN2 (? = 1), reaching an FID of 2.66 with ? 4% of MaGNET samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">QUALITATIVE VALIDATION: HIGH-DIMENSIONAL STATE-OF-THE-ART IMAGE</head><p>GENERATION We now turn into the qualitative evaluation of MaGNET sampling, to do so we propose extensive experiments on various state-of-the-art image DGNs. We also remind the reader that in all cases, standard and MaGNET sampling are performed on the same DGN (same weights) as discussed in Sec. 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2-Dimensional</head><p>Dataset and Colored-MNIST. The first set of controlled experiments are designed such that the training set contains inconsistencies while it is known that the original distribution is uniform on the data manifold. Such inconsistencies can occur in real datasets due to challenges related to dataset compilation. We provide illustrative examples in <ref type="figure">Fig. 5</ref>, where we demonstrate that unless uniform sampling is employed, the trained DGN reproduces the inconsistencies present in the training set, as expected. This toy dataset visualization validates our method from Sec. 3.2. Going further, we take the MNIST dataset (in this case, only digit 8 samples) and apply imbalanced  coloring based on the hue distribution provided in Appendix <ref type="figure" target="#fig_1">Fig. 12</ref>, that favors cyan color. We train a ?-VAE DGN (BVAE) on that cyan-inclined dataset, and present in <ref type="figure">Fig. 6</ref>   <ref type="bibr" target="#b35">(Karras et al., 2017)</ref>, trained on 1024 ? 1024 resolution images. In <ref type="figure" target="#fig_7">Fig. 9</ref> &amp; we provide  random samples from standard and MaGNET sampling, the latter portraying more qualitative diversity. We see that uniform manifold sampling via MaGNET recovers samples containing number of attributes that are generally underrepresented in the samples generated by vanilla progGAN. (See Appendix E) for larger batches and attribute distributions). Notice that uniform sampling not only recovers under-represented groups e.g., age &lt; 30, head-wear, and bald hair, it also increases the presence of neutral emotion and black hair. One interesting observation is that MaGNET also increases the number of samples off the true data manifold (images that are not celebrity faces), exposing regions where the manifold is not well approximated by progGAN. Conditionally Uniform Generation: ImageNet with BigGAN. We present experiments on the state-of-the-art conditional generative model BigGAN <ref type="bibr" target="#b14">(Brock et al., 2019)</ref> using MaGNET sampling. In <ref type="figure" target="#fig_5">Fig. 7</ref> we provide random samples from standard and MaGNET sampling. More experiments on different classes are presented in Appendix E. We see that uniform sampling on the learned data manifold yields a large span of backgrounds and textures, including humans, while standard sampling produces examples closer to the modes of the training dataset. This is quite understandable considering that ImageNET was curated using a large number of images scraped from the internet. MaGNET therefore could possibly be used for data exploration/model interpretation and also as a diagnostic tool to assess the quality of the learned manifold a posteriori of training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">APPLICATION: MONTE-CARLO ESTIMATION AND ATTRIBUTE REBALANCING</head><p>We conclude this section with two more practical aspects offered by MaGNET.</p><p>Reduced-Variance Monte-Carlo Estimator. The first is to speed-up (in term of number of required samples) basic Monte-Carlo estimation of arbitrary topological quantities of the generated manifold. In fact, suppose one's goal is to estimate the Lipschitz constant of a DGN. A direct estimation method would use the known bound given by max z J S (z) F <ref type="bibr" target="#b65">(Wood &amp; Zhang, 1996)</ref>. This estimation can be done by repeatedly sampling latent vectors z from the same distribution that one used for training a DGN. However this implies that the produced samples will not be uniformly distributed on the manifold in turn leading to slower convergence of the estimator. Instead, we propose to use MaGNET, and report our findings in <ref type="figure" target="#fig_6">Fig. 8</ref>. More domains of application, where MaGNET can be used for estimator variance reduction, can be found in Baggenstoss <ref type="formula" target="#formula_0">(2017)</ref>. Style-space MaGNET sampling rebalances attributes. When thinking of uniform sampling on a manifold, it might seem natural to expect fairness i.e., fair representation of different attributes such as equal representation of gender, ethnicity, hair color, etc. However, this is not necessarily true in all cases. In fact, it is trivial to show that each attribute category will be equally represented iff their support on the true data manifold is of equal volume (integrated with respect to the data manifold). Fortunately, as we mentioned in Sec. 4.3, architectures such as StyleGAN2 have explicitly built a style-space, which is a latent space in which attributes are better organized along affine subspaces and occupy nearly the same volume <ref type="bibr" target="#b36">(Karras et al., 2019)</ref> i.e., MaGNET applied on the style-space DGN should improve fairness. By applying MaGNET sampling on the style-space, we are able to reduce gender bias from 67-33% (female-male) in standard StyleGAN2 to 60-40%. This simple result demonstrates the importance of our proposed sampling and how it can be used to increase fairness for DGNs trained on biased training sets. MaGNET in the style-space also yields improvements in terms of recall and precision <ref type="bibr" target="#b52">(Sajjadi et al., 2018)</ref>. Given a reference distribution (e.g., FFHQ dataset) and a learned distribution, precision measures the fidelity of generated samples while recall measures diversity. We compare the metrics for face images generated via latents z <ref type="figure">? N (0, aI)</ref> where a ? 0.5, 1, 1.5, 2, z ? U [?2, 2], and MaGNET sampling on style-space. For 70K samples generated for each case, MaGNET sampling obtains a recall and precision of (0.822, 0.92) with a 4.12% relative increase in recall and 3.01% relative increase in precision compared to other latent mz sampling methods (metrics were averaged for 10 seeds).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSIONS, LIMITATIONS AND FUTURE WORK</head><p>We have demonstrated how the affine spline formulation of DGN offers a rich portal to obtain theoretical results to provably provide uniform sampling on the manifold learned by a DGN. This allows to become robust to possibly incorrect training set distribution that any DGN would learn to replicate after during its training. We provided various experiments using pretrained state-of-the-art generative models and demonstrated that uniform sampling on the manifold offers many benefits from data exploration to statistical estimation. Beyond the sole goal of uniform sampling on a manifold, MaGNET opens many avenues, yet MaGNET is not a "one size fits all" solution.</p><p>When not to sample uniformly. We can identify to general cases in which one should not employ uniform sampling of the DGN manifold. The first case occurs whenever the true manifold is known to be discontinuous and one imperatively needs to avoid sampling in those regions of discontinuities.</p><p>In fact, in the discontinuous case, DGN training, will adapt its weight to put zero (or near zero) density in those discontinuous regions preventing standard sampling to reach those regions <ref type="bibr" target="#b9">(Balestriero et al., 2020b)</ref>. However, MaGNET will reverse this process and introduce samples back in those regions. The second case occurs if one aims to produce samples from the exact same distribution as the training set distribution (assuming training of the DGN was successful). In fact, in this scenario one should use the exact same latent distribution at evaluation time than the one used during training. Future work. Currently, there are two main limitations of our MaGNET sampling strategy. The first one lies in the assumption that the trained DGN is able to learn a good enough approximation of the true underlying data manifold. In future work, we plan to explore how MaGNET can be used to test such an assumption. One potential directions is as follows. Train a DGN using several different sub-sampled datasets (similar to bootstrap methods) and then study if MaGNET samples populate manifolds that all coincide between the different DGNs. If training was successful, then those sampled manifolds should coincide. The second one lies in understanding the relationship between uniform sampling and uniform attribute representation. We demonstrated how uniform sampling in the style-space of StyleGAN2 ensures that relationship by construction. However, doing MaGNET sampling arbitrarily on any DGN might not always produce better attribute distributions. Hence, developing novel theoretical results to characterize those two effects is one crucial future direction to equip MaGNET with theoretical fairness guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">REPRODUCIBILITY STATEMENT</head><p>Reproducible code for the various experiments and figures will be provided upon completion of the review process. Computation and software details are provided in Appendix H, with the proofs of our results in Appendix I. Discussion on the settings in which MaGNET is desirable and on possible limitations is provided in (Sec. 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY MATERIAL</head><p>The following appendices support the main paper and are organized as follows: Appendix A addresses some details on the background of continuous piecewise affine deep networks, that were omitted in the main paper. Appendices E and H provide additional figures and training details for all the experiments that were studied in the main text, and Appendix I provides the proofs of all the theoretical results. Due to size constraints, the high-quality batch of samples are provided in the supplementary files.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A BACKGROUND ON CONTINUOUS PIECEWISE AFFINE DEEP NETWORKS</head><p>A max-affine spline operator (MASO) concatenates independent max-affine spline (MAS) functions, with each MAS formed from the point-wise maximum of R affine mappings <ref type="bibr" target="#b43">(Magnani &amp; Boyd, 2009;</ref><ref type="bibr" target="#b30">Hannah &amp; Dunson, 2013)</ref>. For our purpose each MASO will express a DN layer and is thus an operator producing a D dimensional vector from a D ?1 dimensional vector and is formally given by</p><formula xml:id="formula_6">MASO(v; {A r , b r } R r=1 ) = max r=1,...,R A r v + b r ,<label>(5)</label></formula><p>where A r ? R D ?D ?1 are the slopes and b r ? R D are the offset/bias parameters and the maximum is taken coordinate-wise. For example, a layer comprising a fully connected operator with weights W and biases b followed by a ReLU activation operator corresponds to a (single) MASO with R = 2, A 1 = W , A 2 = 0, b 1 = b , b 2 = 0. Note that a MASO is a continuous piecewiseaffine (CPA) operator <ref type="bibr" target="#b64">(Wang &amp; Sun, 2005)</ref>.</p><p>The key background result for this paper is that the layers of DNs constructed from piecewise affine operators (e.g., convolution, ReLU, and max-pooling) are MASOs <ref type="bibr" target="#b5">(Balestriero &amp; Baraniuk, 2018b;</ref><ref type="bibr">a)</ref>:</p><formula xml:id="formula_7">?R ? N * , ?{A r , b r } R r=1 s.t. MASO(v; {A r , b r } R r=1 ) = g (v), ?v ? R D ?1 ,<label>(6)</label></formula><p>making the entire DGN a composition of MASOs. The CPA spline interpretation enabled from a MASO formulation of DGNs provides a powerful global geometric interpretation of the network mapping based on a partition of its input space R S into polyhedral regions and a per-region affine transformation producing the network output. The partition regions are built up over the layers via a subdivision process and are closely related to Voronoi and power diagrams <ref type="bibr" target="#b7">(Balestriero et al., 2019)</ref>. We now propose to greatly extend such insights to carefully characterize and understand DGNs as well as provide theoretical justifications to various observed behaviors e.g. mode collapse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B UNIFORM AND GAUSSIAN MANIFOLD DISTRIBUTIONS</head><p>We now demonstrate the use of the above result by considering practical examples for which we are able to gain insights into the DGN data modeling and generation. We consider the two most common cases: (i) the latent distribution is set as z ? N(0, 1) and (ii) the latent distribution is set as z ? U(0, 1) (on the hypercube of dimension S). We obtain the following result by direct application of Thm. 1.</p><p>Corollary 1. The generated density distribution p S of the Gaussian and uniform densities are given by</p><formula xml:id="formula_8">p S (x) = ??? e ? 1 2 (x?b?) T (A + ? ) T A + ? (x?b?) (2?) S det(A T ? A ? ) 1 {x?G(?)} , (Gaussian) (7) p S (x) = ??? 1 det(A T ? A ? ) 1 {x?S(?)} . (Uniform)<label>(8)</label></formula><p>The two above formulae provide a precise description of the produced density given that the latent space density is Gaussian or Uniform. In the Gaussian case, the per-region slope matrices act upon the 2 distance by rescaling it from the coordinates of A ? and the per-region offset parameters b ? are the mean against which the input x is compared against. In the Uniform case, the change of volume (recall Eq. 3) is the only quantity that impacts the produced density. We will heavily rely on this observation for the next section where we study how to produce a uniform sampling onto the CPA manifold of an affine spline.</p><p>We derive the analytical form for the case of Gaussian and Uniform latent distribution in Appendix I.3. From the analytical derivation of the generator density distribution, we obtain its differential entropy.</p><p>Corollary 2. The differential Shannon entropy of the output distribution p G of the DGN is given by</p><formula xml:id="formula_9">E(p G ) = E(p z ) + ??? P (z ? ?) log( det(A T ? A ? ))</formula><p>. As a result, the differential entropy of the output distribution p G corresponds to the differential entropy of the latent distribution p z plus a convex combination of the per-region volume changes. It is thus possible to optimize the latent distribution p z to better fit the target distribution entropy as in <ref type="bibr" target="#b10">Ben-Yosef &amp; Weinshall (2018)</ref> and whenever the prior distribution is fixed, any gap between the latent and output distribution entropy imply the need for high change in volumes between ? and G(?).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C PER-REGION AFFINE MAPPINGS</head><p>For completeness we also provide that analytical form of the per-region affine mappings</p><formula xml:id="formula_10">A ? = L?1 i=0 diag ? L?i (?) W L?i ,<label>(9)</label></formula><formula xml:id="formula_11">b ? =b L + L?1 =1 L? ?1 i=0 diag ? L?i (?) W L?i diag ? (?) b ,<label>(10)</label></formula><p>where? (z) is the pointwise derivative of the activation function of layer based on its input W z ?1 + b , which we note as a function of z directly. For precise definitions of those operators see <ref type="bibr" target="#b6">Balestriero &amp; Baraniuk (2020)</ref>. The diag operator simply puts the given vector into a diagonal square matrix. For convolutional layers (or else) one can simply replace the corresponding W with the correct slope matrix parametrization as discussed in Sec. 2. Notice that since the employed activation functions ? , ? ? {1, . . . , L} are piecewise affine, their derivative is piecewise constant, in particular with values [? (z)] k ? {?, 1} with ? = 0 for ReLU, ? = ?1 for absolute value, and in general with ? &gt; 0 for Leaky-ReLU for k ? {1, . . . , D }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D NUMBER OF SAMPLES AND UNIFORMITY</head><p>Exact uniformity is reached when the Monte Carlo samples have covered each region of the DGN partition boundary. For large state-of-the-art models this condition requires sampling on the order of millions. However, we conducted an experiment to see how the number of samples really impacted the uniformity of the generated manifold as follows. We compute precision and recall metrics <ref type="bibr">[4]</ref> for StyleGAN2 with K generated samples obtained from N Monte Carlo samples based on our sampling strategy by varying N . We use K = 5000 and N ranging from 10, 000 to 500, 000. Based on the metrics, we identify that increasing beyond K = 250, 000 no longer impacts the metrics, showing that this number of monte carlo samples is enough to converge (approximately) to the uniform sampling in that case, see <ref type="figure">Fig. 10</ref>.</p><p>We report here the Jacobian computation times for Tensorflow 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E EXTRA FIGURES</head><p>This section contains samples from our proposed methods, more samples along with attribute data and pretrained weights are available at our project link.        </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H ARCHITECTURE, HARDWARE AND IMPLEMENTATION DETAILS</head><p>All the experiments were run on a Quadro RTX 8000 GPU, which has 48 GB of high-speed GDDR6 memory and 576 Tensor cores. For the software details we refer the reader to the provided codebase. In short, we employed TF2 (2.4 at the time of writing), all the usual Python scientific libraries such as NumPy and PyTorch. We employed the official repositories of the various models we employed with official pre-trained weights. As a note, most of the architectures can not be run on GPUs with less or equal to 12 GB of memory.</p><p>For StyleGAN2, we use the official config-e provided in the GitHub StyleGAN2 repo 1 , unless specified. We use the recommended default of ? = 0.5 as the interpolating stylespace truncation, to ensure generation quality of faces for the qualitative experiments. For BigGAN we use the BigGANdeep architecture with no truncation, available on TFHub 2 . We also use the NVAE 3 and ProgGAN 4 models and weights from their respective official implementations. For the Jacobian determinant calculation of images w.r.t latents, we first use a random orthogonal matrix to project generated images into a lower dimensional space, calculate the Jacobian of the projection w.r.t the latents and calculate the singular values of the jacobian to estimate the volume scalar. We use a projection of 256 dimensions for StyleGAN2-pixel, <ref type="bibr">ProgGAN</ref>  Proof. In the special case of an affine transform of the coordinate given by the matrix A ? R D?D the well known result from demonstrates that the change of volume is given by | det(A)| (see Theorem 7.26 in Rudin <ref type="formula" target="#formula_7">(2006)</ref>). However in our case the mapping is a rectangular matrix as we span an affine subspace in the ambient space R D making | det(A)| not defined.</p><p>First, we shall note that in the case of a Riemannian manifold (as is the produced surface from the per-region affine mapping) the volume form used in the usual change of variable formula can be defined via the square root of the determinant of the metric tensor. Now, for a surface of intrinsic dimension n embedded in Euclidean space of dimension m (in our case, the per-region affine mapping produces an affine subspace) parametrized by the mapping M : R n ? R m (in our case this mapping is simply the affine mapping M (z) = z ? z + b ? for each region) the metric tensor is given by g = DM T DM with D the jacobian/differential operator (in our case g = A T ? A ? for each region). This result is also known as Sard's theorem <ref type="bibr" target="#b55">(Spivak, 2018)</ref>. We thus obtain that the change of volume from the region ? to the affine subspace G(?) is given by det(A T A) which can also be written as follows with U SV T the svd-decomposition of the matrix A: Proof. We will be doing the change of variables z = (A T ? A ? ) ?1 A T ? (x ? b ? ) = A + ? (x ? b ? ), also notice that J G ?1 (x) = A + . First, we know that P G(z) (x ? w) = P z (z ? G ?1 (w)) = G ?1 (w) p z (z)dz which is well defined based on our full rank assumptions. We then proceed by</p><formula xml:id="formula_12">P G (x ? w) = ??? ??w p z (G ?1 (x)) det(J G ?1 (x) T J G ?1 (x))dx = ??? ??w p z (G ?1 (x)) det((A + ? ) T A + ? )dx = ??? ??w p z (G ?1 (x))( i:?i(A + ? )&gt;0 ? i (A + ? ))dx = ??? ??w p z (G ?1 (x))( i:?i(A?)&gt;0 ? i (A ? )) ?1 dx Etape 1 = ??? ??w p z (G ?1 (x)) 1 det(A T ? A ? )</formula><p>dx Let now prove the Etape 1 step by proving that ? i (A + ) = (? i (A)) ?1 where we lighten notations as A := A ? and U SV T is the svd-decomposition of A: </p><formula xml:id="formula_13">A + = (A T A) ?1 A T =((U SV T ) T (U SV T )) ?1 (U SV T ) T =(V S T U T U SV T ) ?1 (U SV T ) T =(V S T SV T ) ?1 V S T U T =V (S T S) ?1 S T U T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.3 PROOF OF COROLLARY 1</head><p>Proof. We now demonstrate the use of Thm. 1 where we consider that the latent distribution is set as z ? N(0, 1). We obtain that giving the desired result that is reminiscent of Kernel Density Estimation (KDE) <ref type="bibr" target="#b49">(Rosenblatt, 1956)</ref> and in particular adaptive KDE <ref type="bibr" target="#b13">(Breiman et al., 1977)</ref>, where a partitioning of the data manifold is performed on each cell (? in our case) different kernel parameters are used.</p><p>Proof. We now turn into the uniform latent distribution case on a bounded domain U in the DGN input space. By employing again Thm. 1, the given formula one can directly obtain that the output density is given by</p><formula xml:id="formula_14">p G (x) = ??? 1 x?? det(A T ? A ? ) ? 1 2 V ol(U )<label>(11)</label></formula><p>I.4 PROOF OF THM. 2 As we assume successful training, then regardless of the actual distribution p x , the DGN will learn the correct underlying manifold, and learn the best approximation to p x as possible onto this manifold. Now, applying MaGNET sampling i.e. Sec. 3.2 is equivalent to sampling from a distribution p mz such that after DGN mapping, that distribution is uniform on the learned manifold (see Thm. 1).</p><p>As we assumed that regardless of p x the DGN approximates correctly the true manifold, and as we then adapt the sampling distribution p mz to always obtain uniform sampling on that manifold, we see that this final sampling becomes invariant upon the data distribution (on the manifold) leading to the desired result.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>the affine transformation of region ? by the per-region parameters A ? b ? .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Visual depiction of Eq. 2 with a toy affine spline mapping S : R 2 ? R 3 . Left: latent space partition ? made of different regions shown with different colors and with boundaries shown in black. Right: affine spline image Im(S) which is a continuous piecewise affine surface composed of the latent space regions affinely transformed by the per-region affine mappings (Eq. 1). The per-region colors are the same on the left and on the right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 2 .</head><label>2</label><figDesc>Consider a training set sampled from a manifold M and a (trained) CPA DGN S. As long as M ? Im(S), sampling from S as per Sec. 3.2 produces uniform samples on M, regardless of the training set sampling. (Proof in I.4.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>number of samples within -ball radius, ? Distribution of the number of MNIST training samples with ? neighbors generated within an -ball radius. N samples are generated using standard sampling and MaGNET sampling using an NVAE model. Here is taken to be the average nearest neighbor distance for the training samples. For Vanilla NVAE, heavier tails are indicative of larger density variations on the manifold as N is increased, whereas for MaGNET the shorter tails are indicative of fewer variations in neighborhood density, i.e., uniform generation on the data manifold. 10,000 MNIST samples are used for comparison, for additional seeFig. 13andFig. 14in the Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :Figure 5 :Figure 6 :</head><label>456</label><figDesc>(Left) Average log-likelihood and 10?-bandwidth for 5 runs of a GMM trained on 10,000 samples using standard sampling (blue) and MaGNET sampling (black) for an NVAE trained on MNIST. The higher log-likelihood (given the same number of clusters) in the standard sampling case demonstrates an increased concentration around a few modes, as opposed MaGNET. (Right) FID (?) of StyleGAN2 trained on FFHQ for 50,000 generated samples and 7 runs. With an increasing percentage of uniformly generated samples to increase diversity, MaGNET reaches state-of-the-art FID of 2.66 achieved at a 4.1% mixture. From left to right, samples from a toy 2D distribution with triangular support, biased samples ob-tained for training a GAN, standard sampling showing a biased distribution learned by the GAN, and MaGNET sampling recovering uniformly distributed samples on the support of the true distribution. Note that the same number of samples are obtained for both standard and MaGNET sampling. Hue (color) distribution of samples obtained from standard and MaGNET sampling from a trained BVAE model on colored 8 digits from MNIST. Original dataset purposefully favored cyan coloring to represent training set inconsistency. MaGNET BVAE is able to approximately generate uniformly colored MNIST samples. The density drop around red color represent regions where the DGN manifold approximation was incorrect, hence uniform sampling can not recover those samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Random batch of samples generated from BigGAN (left) and MaGNET BigGAN (right), conditioned on the Samoyed class of ImageNet. While BigGAN samples contain homogeneous postures, MaGNET samples represent the true span of the data manifold learned by BigGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Lipschitz constant estimation using standard DGN sampling and MaGNET sampling (left). Each line represents the mean over 200 Monte-Carlo runs. As expected, estimation of such statistics from samples converges faster when employed on uniformly distributed samples. The standard deviation of the Monte-Carlo estimations are also provided (right), where it is clear that the uniform sampling via MaGNET reaches smaller standard deviation at earlier steps i.e., it is faster to converge. This is a key application of MaGNET: speedingup convergence of statistical estimation of quantities, such as the Lipschitz constant in this case. The x-axis represents the number of samples used for estimation in log-scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :</head><label>9</label><figDesc>Random batch of samples generated from vanilla progGAN (left) and MaGNET progGAN (right).Some samples contain fusion of attributes which are not frequently represented in the data distribution, e.g. female w/ facial hair, male w/ makeup, etc. MaGNET also generates more samples from regions of the manifold less approximated by the DGN. Larger batches in Appendix E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 10 :Figure 12 :Figure 14 :</head><label>101214</label><figDesc>Evolution of the precision/recall curves for varying number of samples N form the monte-carlo sampling against the number of samples K = 5k for StyleGAN2. Depiction of the imbalance hue distribution applied to color the MNIST digits. number of samples within 0.5 -ball radius Figure 13: Reprise of Fig. 3. Vanilla NVAE Left, MaGNET NVAE Right Number of training samples having Reprise of Fig. 3. Vanilla NVAE Left, MaGNET NVAE Right Figure 15: Random batches of 245 samples from a StyleGAN2 trained on FFHQ, generated via standard sampling (left), MaGNET sampling in the pixel-space (middle) and MaGNET sampling in the style-space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 16 :</head><label>16</label><figDesc>Random Samples from vanilla progGAN (left) and MaGNET progGAN (right) trained on the CelebA-HQ dataset. Samples are sorted by gender &amp; age and color coded by gender as visually predicted by the Microsoft Cognitive API. Samples not recognized by the API are color coded as white at the bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 18 :</head><label>18</label><figDesc>Random Samples from vanilla BigGAN (left) and MaGNET BigGAN (right) from the Siamese class of Imagenet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 20 :</head><label>20</label><figDesc>Facial Attributes of 5000 StyleGAN2 samples using vanilla sampling, MaGNET stylespace sampling and MaGNET pixel-space sampling. We see that MaGNET style-space increases uniformity in gender and age distributions whereas MaGNET pixel-space yields more variations in physical attributes and accessories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 21 :</head><label>21</label><figDesc>Facial Attributes of 5000 ProgGAN samples using standard sampling and MaGNET sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 22 :</head><label>22</label><figDesc>Random Samples from vanilla NVAE (left) and MaGNET NVAE (right) trained on the MNIST dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 23 :</head><label>23</label><figDesc>Random Samples from vanilla NVAE (left) and MaGNET NVAE (right) trained on the CIFAR dataset. F EXTRA TABLES</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>det(A T A) = det((U SV T ) T (U SV T )) = det((V S T U T )(U SV T )) = det(V S T SV T ) = det(S T S)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>=? ? i (A + ) = (? i (A)) ?1 with the above it is direct to see that det((A + ? )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>p</head><label></label><figDesc>G (x ? w) = ??? ??w 1 x?G(?) p z (G ?1 (x)) det(A T ? A ? ) S/2 det(A T ? A ? ) e ? 1 2 ((A + ? (x?b?)) T ((A + (x?b?)) dx = ??? ??w 1 x?G(?) 1 (2?) S/2 det(A T ? A ? ) e ? 1 2 (x?b?) T (A + ? ) T A + ? (x?b?) dx</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1 and Sec. 3.2 in order to effectively produce uniform samples on the manifold of current Deep Generative Networks such as BigGAN, StyleGAN. We build this affine splines-DGN bridge and propose carefully motivation for uniform sampling in Sec. 4.1 and finally present various experiments across architectures in Sec. 4.2, 4.3 and 4.4.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>the hue distributions for samples obtained via standard sampling and MaGNET sampling. We observe that MaGNET corrects the hue distribution back to uniformity. Uniform Face Generation: CelebA-HQ and Flickr-Faces-HQ with progGAN and Style-GAN2. Our first experiment concerns sampling from the StyleGAN2<ref type="bibr" target="#b37">(Karras et al., 2020)</ref> model pretrained on the Flickr-Faces-HQ (FFHQ) dataset. StyleGAN2 has two DGNs, one that maps to an intermediate latent space, termed style-space and another DGN that maps style-space vectors to the pixels-space (output of StyleGAN2). Implementation details are contained in Appendix H. We focus here on applying MaGNET onto the entire StyleGAN2 model (the composition of both DGNs), in Sec. 4.4 we discuss applying MaGNET to the style-space DGN. InFig. 1 we provide</figDesc><table /><note>random samples from the same StyleGAN2 model obtained via standard and MaGNET sampling. Upon qualitative evaluation it can be seen that the samples obtained via MaGNET (MaGNET Style- GAN2) have a significantly larger variety of age distribution, background variations and wearable accessories compared to standard sampling. For experiments with CelebA-HQ dataset, we adopt the DGN of Progressively Growing GAN (prog- GAN)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>FID obtained by StyleGAN2 (config-f) trained on FFHQ using standard and MaGNET sampling (pixel-space) for varying degrees of truncation (?). (Left) FID score obtained for 50,000 samples generated by a mixture of MaGNET and standard sampling. MaGNET samples can be used to increase diversity of the model, resulting in better FID than current state-of-the-art. (Right) FID score for varying truncation without mixing. Online Rejection Sampling algorithm for MaGNET Input: Latent space domain, U ; Generator G; N change of volume scalars {? 1 , ? 2 , ..., ? N };</figDesc><table><row><cell>Truncation ?</cell><cell>Sampling Method</cell><cell>Percent Mixture</cell><cell>FID (?)</cell><cell>Truncation ?</cell><cell>Sampling Method</cell><cell>FID (?)</cell></row><row><cell>1</cell><cell>Standard MaGNET</cell><cell>-4.1%</cell><cell>2.74 2.66</cell><cell>.5</cell><cell>Standard MaGNET</cell><cell>58.33 54.47</cell></row><row><cell>.9</cell><cell>Standard MaGNET</cell><cell>-20%</cell><cell>5.05 4.29</cell><cell>.4</cell><cell>Standard MaGNET</cell><cell>83.84 82.41</cell></row><row><cell>.8</cell><cell>Standard MaGNET</cell><cell>-33%</cell><cell>10.94 8.57</cell><cell>.3</cell><cell cols="2">Standard MaGNET 112.89 112.08</cell></row><row><cell>.7</cell><cell>Standard MaGNET</cell><cell>-100%</cell><cell>21.34 19.41</cell><cell>.2</cell><cell cols="2">Standard MaGNET 144.93 142.27</cell></row><row><cell>.6</cell><cell>Standard MaGNET</cell><cell>-100%</cell><cell>36.98 33.19</cell><cell>.1</cell><cell cols="2">Standard MaGNET 178.75 176.20</cell></row><row><cell cols="2">G ALGORITHMS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">Algorithm 1: MaGNET Sampling as described in Sec. 3.2</cell><cell></cell><cell></cell></row><row><cell cols="7">Input: Latent space domain, U ; Generator G; Number of regions to sample N ; Number of</cell></row><row><cell>samples K;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Output: MaGNET Samples, {x i } K i=1 ;</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Initialize, Z, S ? [], [] ;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">for n = 1, . . . , N do</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>z ? U(U );</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Get Slope Matrix, A = J G (z);</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Get volume scalar at z, ? z = det(A T A);</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Z.append(z);</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>S.append(? z )</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>end</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">for n = 1, . . . , K do</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">i ? Categorical(prob = softmax(S));</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>x i ? Z[i]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>end</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Algorithm 2: Output: MaGNET Sample, x;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>while True do</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Sample z ? U(U );</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Sample ? ? U[0, 1];</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Get Slope Matrix, A = J G (z);</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Get volume scalar at z, ? z = det(A T A); if ?z ?z+ N i=1 ?i ? ? then</cell><cell></cell><cell></cell><cell></cell></row><row><cell>x = G(z);</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>break;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>end end</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>and BigGAN, and 128 dimensions for NVAE. To estimate the volume scalar we use the top 30, 20, 15 singular values for StyleGAN2 MaGNET pixel, ProgGAN and BigGAN; 40 for StyleGAN2 MaGNET style, and 30 for NVAE.</figDesc><table><row><cell>I PROOFS</cell></row><row><cell>I.1 PROOF OF LEMMA 1</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/NVlabs/stylegan2 2 https://tfhub.dev/deepmind/biggan-deep-256/1 3 https://github.com/NVlabs/NVAE 4 https://github.com/tkarras/progressive growing of gans</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Perturbation theory approach to study the latent space degeneracy of variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helena</forename><surname>Andr?s-Terr?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Li?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.04862</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?on</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uniform manifold sampling (ums): Sampling the maximum entropy pdf</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baggenstoss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2455" to="2470" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Mad max: Affine spline insights into deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Balestriero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baraniuk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.06576</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A spline theory of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Balestriero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conf. Mach. Learn</title>
		<meeting>Int. Conf. Mach. Learn</meeting>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="374" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mad max: Affine spline insights into deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Balestriero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The geometry of deep networks: Power diagram subdivision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Balestriero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romain</forename><surname>Cosentino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Behnaam</forename><surname>Aazhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="15806" to="15815" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Analytical probability distributions and exact expectation-maximization for deep generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Balestriero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S?bastien</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Baraniuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Max-affine spline insights into deep generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Balestriero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastien</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Baraniuk</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.11912</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gaussian mixture generative adversarial networks for diverse datasets, and the unsupervised clustering of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matan</forename><surname>Ben-Yosef</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daphna</forename><surname>Weinshall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10356</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonard</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hasenclever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.05649</idno>
		<title level="m">Sylvester normalizing flows for variational inference</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rard</forename><surname>Biau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beno?t</forename><surname>Cadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ugo</forename><surname>Sangnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tanielian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07819</idno>
		<title level="m">Some theoretical properties of gans</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Variable kernel estimates of multivariate densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Meisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Purcell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Technometrics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="144" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large scale gan training for high fidelity natural image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICLR</title>
		<meeting>ICLR</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Your gan is secretly an energy-based model and you should use discriminator driven latent sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Tong Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Paull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06060</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Isolating sources of disentanglement in variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuechen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Roger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">K</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="2610" to="2620" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A course in approximation theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elliott</forename><forename type="middle">Ward</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Allan</forename><surname>Light</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>American Mathematical Soc</publisher>
			<biblScope unit="volume">101</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Tim R Davidson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><forename type="middle">De</forename><surname>Falorsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub M</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tomczak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00891</idno>
		<title level="m">Hyperspherical variational auto-encoders</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Nice: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Gemp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sridhar</forename><surname>Mahadevan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01673</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Generative multi-adversarial networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Training generative neural networks via maximum mean discrepancy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gintare Karolina Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghahramani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.03906</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Control theoretic splines: optimal control, statistics, and path planning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Magnus</forename><surname>Egerstedt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clyde</forename><surname>Martin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Princeton University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Otto</forename><surname>Fabius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Joost R Van Amersfoort</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6581</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">Variational recurrent auto-encoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Identification of piecewise affine models in noisy environment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesare</forename><surname>Fantuzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Simani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Beghelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Riccardo</forename><surname>Rovatti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Control</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="1472" to="1485" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<title level="m">Deep Learning</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Ricky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Betterncourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duvenaud</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.01367</idno>
		<title level="m">Ffjord: Free-form continuous dynamics for scalable reversible generative models</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
		<title level="m">Improved training of wasserstein gans</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multivariate convex regression with adaptive partitioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hannah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Dunson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3261" to="3294" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<title level="m">Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework. ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fairfacegan: Fairnessaware facial image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sunhee</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungho</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dohyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mirae</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeran</forename><surname>Byun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 31st British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1309" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4401" to="4410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8110" to="8119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Durk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveen</forename><surname>Kodali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Abernethy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07215</idno>
		<title level="m">On convergence and stability of gans</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09884</idno>
		<title level="m">Towards understanding the dynamics of generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Approximation and convergence properties of generative adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5545" to="5553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Convex piecewise-linear fitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Magnani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optim. Eng</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2794" to="2802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02163</idno>
		<title level="m">Unrolled generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">On the number of linear regions of deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Guido F Montufar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2924" to="2932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Manifold models for signals and images. Computer vision and image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Peyr?</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="249" to="260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Remarks on some nonparametric estimates of a density function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murray</forename><surname>Rosenblatt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="page" from="832" to="837" />
			<date type="published" when="1956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11063</idno>
		<title level="m">Theory and experiments on vector quantized autoencoders</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Real and Complex Analysis. Tata McGraw-hill education</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Walter Rudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Assessing generative models via precision and recall</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Sajjadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Bachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00035</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Data augmentation using generative adversarial networks (cyclegan) to improve generalizability in ct segmentation tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veit</forename><surname>Sandfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Perry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald M</forename><surname>Pickhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Scaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aladin</forename><surname>Virmaux</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10965</idno>
		<title level="m">Lipschitz regularity of deep neural networks: analysis and efficient estimation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Calculus on manifolds: a modern approach to classical theorems of advanced calculus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Spivak</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CRC press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Improving the fairness of deep generative models without retraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujun</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04842</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Tanaka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.06832</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Discriminator optimal transport. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Approximate query processing for data exploration using deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shohedul</forename><surname>Saravanan Thirumuruganathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gautam</forename><surname>Koudas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE 36th International Conference on Data Engineering (ICDE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1309" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Improving variational auto-encoders using householder flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09630</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jakub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07120</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">Vae with a vampprior. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">A representer theorem for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Unser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09210</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.03898</idno>
		<title level="m">Nvae: A deep hierarchical variational autoencoder</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6306" to="6315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Generalization of hinging hyperplanes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuning</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xusheng</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4425" to="4431" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Estimation of the lipschitz constant of a function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Global Optimization</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="91" to="103" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiacheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.10805</idno>
		<title level="m">Spherical latent spaces for stable variational autoencoders</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Diversitysensitive conditional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dingdong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianchen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">On the discriminationgeneralization tradeoff in gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.02771</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
