<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-modal Text Recognition Networks: Interactive Enhancements between Visual and Semantic Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeonghu</forename><surname>Na</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Dept. of Industrial &amp; Systems Engineering</orgName>
								<orgName type="institution">KAIST</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoonsik</forename><surname>Kim</surname></persName>
							<email>yoonsik.kim90@navercorp.com</email>
							<affiliation key="aff1">
								<orgName type="department">Clova AI Research</orgName>
								<orgName type="institution">NAVER Corp</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungrae</forename><surname>Park</surname></persName>
							<email>sungrae.park@upstage.ai</email>
							<affiliation key="aff2">
								<orgName type="department">Upstage AI Research, Upstage</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-modal Text Recognition Networks: Interactive Enhancements between Visual and Semantic Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Linguistic knowledge has brought great benefits to scene text recognition by providing semantics to refine character sequences. However, since linguistic knowledge has been applied individually on the output sequence, previous methods have not fully utilized the semantics to understand visual clues for text recognition. This paper introduces a novel method, called Multi-modAl Text Recognition Network (MATRN), that enables interactions between visual and semantic features for better recognition performances. Specifically, MATRN identifies visual and semantic feature pairs and encodes spatial information into semantic features. Based on the spatial encoding, visual and semantic features are enhanced by referring to related features in the other modality. Furthermore, MATRN stimulates combining semantic features into visual features by hiding visual clues related to the character in the training phase. Our experiments demonstrate that MATRN achieves state-of-theart performances on seven benchmarks with large margins, while naive combinations of two modalities show less-effective improvements. Further ablative studies prove the effectiveness of our proposed components. Our implementation is available at https://github.com/wp03052/MATRN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Scene text recognition (STR), a major component of optical character recognition (OCR) technology, identifies a character sequence in a given text image patch (e.g. words in a traffic sign). Applications of deep neural networks have brought great improvements in the performance of STR models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">26,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b30">29,</ref><ref type="bibr" target="#b34">33,</ref><ref type="bibr" target="#b35">34]</ref>. They typically consist of a visual feature extractor, abstracting the image patch, and a character sequence generator, responsible for character decoding. Despite wide explorations to find better visual feature extractors and character sequence generators, existing methods still suffer from challenging environments: occlusion, blurs, distortions, and other artifacts <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>To address these remaining challenges, STR methods have tried to utilize linguistic knowledge on the output character sequence. The mainstream of the approaches had been to model recursive operations learning linguistic knowledge for next character prediction. RNN <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">27]</ref> and Transformer <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b26">25,</ref><ref type="bibr" target="#b33">32]</ref> have been applied to learn the auto-regressive language model (LM). However, the auto-regressive process requires expensive repeated computations and also learns limited linguistic knowledge from the uni-directional transmission.</p><p>To compensate for the issues, Yu et al. <ref type="bibr" target="#b34">[33]</ref> propose SRN that refines an output sequence without auto-regressive operations. After identifying a seed character sequence, SRN re-estimates the character for each position at once by utilizing a Transformer encoder with a subsequent mask. Based on SRN, Fang et al. <ref type="bibr" target="#b6">[7]</ref> improve the iterative refinement stages by explicitly dividing a vision model (VM) and an LM by blocking gradient flows and employing a bi-directional LM pre-trained on unlabeled text datasets. These methods incorporating semantic knowledge of LMs provide breakthroughs in recognizing challenging examples with ambiguous visual clues. However, the character refinements without visual features might lead to wrong answers by missing existent visual clues.</p><p>For better combinations of semantics and visual clues, Bhunia et al. <ref type="bibr" target="#b2">[3]</ref> propose a multi-stage decoder referring to visual features multiple times to enhance semantic features. At each stage, a character sequence, designed as differentiable with Gumbel-softmax, is re-fined by re-assessing visual clues. Concurrently, Wang et al. <ref type="bibr" target="#b31">[30]</ref> propose VisionLAN utilizing a language-aware visual mask that occludes selected character regions for enhancing the visual clues at the training phase. They prove that combining visual clues and semantic knowledge leads to better STR performances. Inspired by them, we raise a novel question: what is the best way to model the interactions between visual and semantic features identified by VM and LM, respectively?</p><p>To answer the question, this paper introduces a simple-but-effective extension of a STR model, named Multi-modAl Text Recognition Network (MATRN), that enhances visual and semantic features by referring to features in both modalities. MATRN consists of three proposed modules applied upon visual and semantic features: (1) multi-modal feature enhancement, incorporating bimodalities to enhance each feature, (2) spatial encoding for semantics, linking two different modalities, (3) visual clue masking strategy, stimulating the crossreferences between visual and semantic features. <ref type="figure" target="#fig_0">Figure 1</ref> shows four types of visual and semantic feature fusions. MATRN is positioned in the bi-directional feature fusion ( <ref type="figure" target="#fig_0">Figure 1d</ref>) by applying multi-modal feature enhancement. To the best of our knowledge, this natural but simple extension has never been explored.</p><p>The resulting model, MATRN, is architecturally simple but effective. In addition, the visual and semantic feature fusions are not expensive because the whole process is conducted in parallel. When we evaluate simple combinations of visual and semantic features without our proposed components, the performance improvements are observed as less-effective. However, interestingly, the proposed components contribute to STR performances effectively and lead MATRNs to achieve superior performances with notable gains from the current state-of-theart. Consequently, our paper proves that semantics is helpful to capture better visual clues as well as that combining visual and semantic features reaches better STR performances.</p><p>Our contributions are threefold.  <ref type="bibr" target="#b34">[33]</ref> is placed in (a) by applying LM to refine the output of VM. ABINet <ref type="bibr" target="#b6">[7]</ref>, PIMNet <ref type="bibr" target="#b22">[22]</ref>, and JVSR <ref type="bibr" target="#b2">[3]</ref> can be aligned in (b) because their decoders refer to visual features iteratively during refining the final output sequence. VisionLAN <ref type="bibr" target="#b31">[30]</ref> combines semantic information into visual features in a similar way to (c). Our method, MATRN, is positioned in (d) by enhancing both features through the bi-directional reference.</p><formula xml:id="formula_0">(a) (b) (c) (d)</formula><p>-We explore the combinations of visual and semantic features, identified by VM and LM, and prove their benefits. To the best of our knowledge, multimodal feature enhancements with bi-directional fusions are novel components, that are natural extensions but have never been explored. -We propose a STR method, named MATRN, that contains three major components, spatial encoding for semantics, multi-modal feature enhancements, and visual clue masking strategy, for better combinations of two modalities. Thanks to the effective contributions of the proposed components, MATRN achieves state-of-the-art performances on seven STR benchmarks. -We provide empirical analyses that illustrate how our components improve STR performances as well as how MATRN contributes to the existing challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>To utilize the benefits of a bi-directional Transformer, the non-autoregressive decoder has been introduced in the STR community. The general decoding process of them <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b34">33]</ref> lies in the effective construction of a sequence processed parallelly in the decoder. Specifically, positional embeddings describing the order of the output sequence are used to align visual (or semantic) features. Although the output sequence is generated in parallel, the bi-directional Transformer has shown comparable performances with the auto-regressive approaches. ViTSTR <ref type="bibr" target="#b0">[1]</ref> mainly focused on their VM without explicitly learning the LM. Inspired by the success of ViT <ref type="bibr" target="#b5">[6]</ref>, ViTSTR [1] has adopted ViT training scheme to STR. Specifically, its composition is very simple composed of the Transformer encoder and is trained with un-overlapped patches.</p><p>In order to incorporate linguistic knowledge, PIMNet <ref type="bibr" target="#b22">[22]</ref>, SRN <ref type="bibr" target="#b34">[33]</ref> and ABINet <ref type="bibr" target="#b6">[7]</ref> have been proposed. To learn linguistic knowledge from the autoregressive model, PIMNet <ref type="bibr" target="#b22">[22]</ref> proposed step-wise predictions and similarity distance between non-autoregressive and auto-regressive models. SRN and ABINet introduced a language modality that refines the output sequence of VM. Then, the final predictions are achieved by fusing the output sequences of LMs and VMs. In SRN <ref type="bibr" target="#b34">[33]</ref>, the LM is trained along with VM where the LM learns semantic information from other words. Based on SRN, ABINet <ref type="bibr" target="#b6">[7]</ref> improves the iterative refinement stages by explicitly dividing the VM and LM. With a pretraining LM on unlabeled text datasets, it provides breakthroughs in recognizing challenging examples with ambiguous visual clues.</p><p>To interactively combine LM and VM, multi-modal recognizers are also introduced <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">30]</ref>. JVSR <ref type="bibr" target="#b2">[3]</ref> proposes a multi-stage decoder referring to visual features multiple times to enhance semantic features. Specifically, it is based on an RNN-attention decoder with multi-stages where each stage generates an output sequence and visual features are employed for updating each hidden state. Since the decoder takes a hidden state as an input, the visual feature can iteratively enhance the semantic features. Concurrently, VisionLAN <ref type="bibr" target="#b31">[30]</ref> proposes a language-aware visual mask that refers to semantic features for enhancing the visual features. Given a masking position of the word, the masking module occludes corresponding visual feature maps of the character region at the training phase. The previous multi-modal recognizers focus on one modality for final prediction and they utilize the other modality to improve their chosen one. In contrast, we explore the multiple combinations of multi-modal processes and propose MATRN which conducts both bi-directional enhancements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MATRN</head><p>Here, we describe our recognition model, MATRN, which incorporates both visual and semantic features. We will provide an overview of our method, and then describe each component in detail. <ref type="figure" target="#fig_1">Figure 2</ref> shows the overview of our model. It includes a visual feature extractor and a seed text generator to embed an image and provide an initial sequence of characters, as traditional STR models do. LM is applied to the seed text to extract semantic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Overview of MATRNs</head><p>Our contributions are focused on incorporating the visual and semantic features for better STR performances. Our method first encodes spatial positions into semantic features by utilizing an attention map identified during the seed text generation. The multi-modal feature enhancement module enriches individual visual and semantic features by incorporating multi-modalities. The enhanced features are named as multi-modal visual features, enhanced visual features with semantic knowledge, and multi-modal semantic features, enhanced semantic features with visual clues. Finally, both features are combined to provide the output sequence.</p><p>In the training phase of MATRN, the visual clue masking module hides visual features, related to a single character to stimulate the combination of semantics. Furthermore, the output sequence can be iteratively applied into the seed text for the LM, as follows <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Visual and Semantic Feature Extraction</head><p>To identify visual and semantic features, we construct three components: visual feature extractor, seed text generator, and language model. The following describes each module.</p><p>For the visual feature extractor, ResNet and Transformer units <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b34">33]</ref> are applied. The ResNet, F V.R , with 45 layers embeds an input image, X ? R H?W ?3 , into convolution visual features, V conv ? R </p><formula xml:id="formula_1">V = F V.T (F V.R (X) + P V ),<label>(1)</label></formula><p>where V ? R For the seed text generation, an attention mechanism is utilized to transcribe visual features into character sequences. Specifically, an attention map,</p><formula xml:id="formula_2">A V-S ? R T ? HW 16</formula><p>, is calculated by setting queries as text positional embeddings, P S ? R T ?D , and keys as G(V) ? R HW 16 ?D in the attention mechanism, where T is the maximum length of sequence and G(?) is a mini U-Net. Through the attention map, visual features are abstracted upon sequential features,</p><formula xml:id="formula_3">E V = A V-S V, where V ? R HW 16</formula><p>?D indicates the flattened visual features. By applying a linear layer and softmax function, a seed character sequence, Y (0) ? R T ?C , is generated, where C indicates the number of character classes. The whole process can be formalized as follows;</p><formula xml:id="formula_4">A V-S = softmax P S G(V) ? / ? D ,<label>(2)</label></formula><formula xml:id="formula_5">Y (0) = softmax A V-S VW ,<label>(3)</label></formula><p>where W ? R D?C indicates a linear transition matrix. The LM, introduced by <ref type="bibr" target="#b6">[7]</ref>, consists of four Transformer decoder blocks. It uses P S as inputs and Y (0) as the key/value of the attention layer. By processing the multiple decoder blocks, the LM identifies semantic features, S ? R T ?D ;</p><formula xml:id="formula_6">S = F LM (Y (0) ),<label>(4)</label></formula><p>where F LM indicates the LM. We initialize the LM with the weights, pre-trained on WikiText-103, provided by <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Spatial Encoding to Semantic Features</head><p>One important point of combining the visual and semantic features is how to align each piece of information of different modalities. To guide the relationship between visual and semantic features, MATRN encodes spatial positions of visual features into semantic features. We call this process spatial encoding to semantics (SES).</p><p>The key idea of SES is utilizing the attention map A V-S , used for the seed text generation, and the spatial position embedding P V , introduced in the visual feature extractor. Since A V-S provides which visual features are used to estimate a character at each position, the spatial positions for semantic features, P Align ? R T ?D , are calculated as follows;</p><formula xml:id="formula_7">P Align = A V-S P V ,<label>(5)</label></formula><p>where P V ? R HW 16 ?D is the flattened sinusoidal spatial position embedding, P V . Then, we encode the spatial information into semantic features:</p><formula xml:id="formula_8">S Align = S + P Align .<label>(6)</label></formula><p>From this encoding process, the spatially aligned semantic features, S Align , contain spatial clues of visual features are highly related. It should be noted that SES does not need any additional parameters as well as it is simple and effective for the cross-references between visual and semantic features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Multi-modal Features Enhancement</head><p>Now, we hold visual features,?, that learn visual clues for character estimations, and semantic features, S Align , that contain linguistic knowledge for a character sequence. Previous methods <ref type="bibr" target="#b34">[33,</ref><ref type="bibr" target="#b6">7]</ref> simply use a gated mechanism to seed character feature E V and semantic feature S. However, this simple fusion mechanism might not completely utilize these two features. Therefore, we propose a way in which the visual and semantic features refer to each other effectively and consequently enhance the features.</p><p>Multi-modal transformer <ref type="bibr" target="#b29">[28]</ref>, which consists of transformer layers processing multiple types of features at once, has been introduced in several domains such as visual question answering <ref type="bibr" target="#b9">[10]</ref>, vision-language navigation <ref type="bibr" target="#b3">[4]</ref>, autonomous driving <ref type="bibr" target="#b21">[21]</ref>, video retrieval <ref type="bibr" target="#b7">[8]</ref>, and many others. Inspired by them, we employ the multi-modal transformer for visual and semantic features enhancement for STR. The multi-modal transformers have multiple Transformer encoder blocks that consist of an attention layer and a feed-forward layer. At the attention layer, both visual and semantic features are processed through self-attentions.</p><p>Since the queries determine their major modality, visual features are enhanced as multi-modal visual features, V M ? R HW 16 ?D , and semantic features are updated into multi-modal semantic features, S M ? R T ?D .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Final Output Fusion</head><p>Both multi-modal features are utilized to finalize the output character sequence. While multi-modal semantic features are already aligned as a sequence, multimodal visual features are required to be re-organized to estimate characters. To align the visual features into a sequence, we apply a character generator, which has the same architecture of the seed text generator, to aggregate V M into sequential features, E V M (See ?3.2). Afterward, two sequential features, E V M and S M , are combined through a gate mechanism to identify features, F ? R T ?D , used for final character estimations:</p><formula xml:id="formula_9">G = ? E V M ; S M W gated ,<label>(7)</label></formula><formula xml:id="formula_10">F = G ? E V M + (1 ? G) ? S M ,<label>(8)</label></formula><p>where W gated ? R 2D?D is a weight, [; ] indicates concatenation, and ? is element-wise product. Finally, a linear layer and softmax function are applied on F to estimate a character sequence, Y (1) ? R T ?C .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Visual Clue Masking Strategy</head><p>To facilitate a better blend of the visual and semantic features, we propose a visual clue masking strategy, which is motivated by VisionLAN <ref type="bibr" target="#b31">[30]</ref>. This strategy selects a single character randomly and hides corresponding visual features based on the attention map, A V-S , identified in the seed text generation. By explicitly deleting influential features for the character estimation, the multi-modal FE module becomes stimulated to encode semantic knowledge into the visual features in order to compensate for the missing visual clues. <ref type="figure" target="#fig_4">Figure 3</ref> provides the conceptual description of the visual clue masking strategy. The masking process chooses a position randomly in a character sequence and finds the top-K visual features relevant to the chosen position. For example, if the fourth position is selected, the process identifies the front K visual features in the descending order of the attention scores at the fourth position. The identified visual features are replaced into v [MASK] ? R D . The visual clue masking is only applied in the training phase. To reduce the discrepancy between training and evaluating phases, we keep the identified features unchanged with probability 0.1, as like <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Training Objective</head><p>MATRN is trained as end-to-end learning with multi-task cross-entropy objectives from multi-level visual and semantic features. We denote L * is a crossentropy loss for estimated character sequences from a feature * . For the estimations, a linear layer and a softmax function are utilized. In addition, MATRN applies iterative semantic feature correction to resolve the noisy input for the LM, as follows <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b15">15]</ref>. At the iteration, the input of LM is replaced into the output of the output fusion layer (See <ref type="figure" target="#fig_1">Figure 2)</ref>. The loss of MATRN is formed as follows;</p><formula xml:id="formula_11">L = L E V + 1 M M i=1 L S (i) + L S M (i) + L E V M (i) + L F (i) ,<label>(9)</label></formula><p>where M is the number of iterations. Here, S (i) , S M (i) , E V M (i) , and F (i) indicate the semantic, multi-modal semantic, multi-modal visual, and final fused features at the i-th iteration, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>For fair comparisons, we use the same training dataset and evaluation protocol with <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b34">33]</ref>. For the training set, we use two widely-used synthetic datasets, MJSynth <ref type="bibr" target="#b10">[11]</ref> and SynthText <ref type="bibr" target="#b8">[9]</ref>. MJSynth has 9M synthetic text images and SynthText consists of 7M images including examples with special characters. Most previous works have used these two synthetic datasets together: MJSynth + SynthText <ref type="bibr" target="#b1">[2]</ref>.</p><p>For evaluation, eight widely used real-world STR benchmark datasets are used as test datasets. The datasets are categorized into two groups: "regular" and "irregular" datasets, according to the geometric layout of texts. "regular" datasets mainly contain horizontally aligned text images. IIIT5K (IIIT) <ref type="bibr" target="#b19">[19]</ref> consists of 3,000 images collected from the web. Street View Text (SVT) <ref type="bibr" target="#b11">[12]</ref> has 647 images collected from Google Street View. ICDAR2013 (IC13) <ref type="bibr" target="#b13">[13]</ref> represents images cropped from mall pictures and has two variants; 857 images (IC13 S ) and 1015 images (IC13 L ). We utilize all two variants for providing fair comparisons. We skipped the evaluation on ICDAR2003 <ref type="bibr" target="#b17">[17]</ref> because it contains duplicated images with IC13 <ref type="bibr" target="#b1">[2]</ref>.</p><p>"irregular" datasets consist of more examples of text in arbitrary shapes. IC-DAR2015 (IC15) consists of images taken from scenes and also has two versions; 1,811 images (IC15 S ) and 2,077 images (IC15 L ). Street View Text Perspective (SVTP) <ref type="bibr" target="#b20">[20]</ref> contains 645 images of which texts are captured in perspective views. CUTE80 (CUTE) <ref type="bibr" target="#b24">[24]</ref> consists of 288 images of which texts are heavily curved.</p><p>In our analyses, we measure the word prediction accuracy on each dataset. For "Total.", we evaluate the accuracy of unified evaluation datasets except for IC13 L and IC15 L . It should be noted that we follow the philosophy of Baek et al. <ref type="bibr" target="#b1">[2]</ref> which compares STR models upon the common evaluation datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>The height and width of the input image are 32 and 128 by re-sizing text images and we apply image augmentation methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b34">33]</ref> such as rotation, color jittering, and noise injection. The number of character classes is 37; 10 for digits, 26 for alphabets, and a single padding token.</p><p>We borrow the network structures of the visual feature extractor, seed text generator, and language model from ABINet <ref type="bibr" target="#b6">[7]</ref>. We set the feature dimension, D, as 512 and the maximum length of the sequence, T , as 25. For the multimodal transformer, we use 2 Transformer blocks with 8 heads and a hidden size of 512. The iteration number M is set to 3 unless otherwise specified. We fixed the number of visual features mask, K, as 10.</p><p>We adopt the code from ABINet 4 , and keep the experiment configuration. We use a pre-trained visual feature extractor and a pre-trained language model, which are provided by <ref type="bibr" target="#b6">[7]</ref>. We use 4 NVIDIA GeForce RTX 3090 GPUs to train our models with batch size of 384. We used Adam <ref type="bibr" target="#b14">[14]</ref> optimizer of initial learning rate 10 ?4 , and the learning rate is decayed to 10 ?5 after six epochs. <ref type="table" target="#tab_0">Table 1</ref> shows the existing STR methods and their performances on the eight STR benchmark datasets, including the variant versions of IC13 and IC15. In this comparison, we only consider the existing methods that are trained on MJSynth and SynthText. When comparing the existing STR methods, PREN2D, JVSR, and ABINet showed state-of-the-art performances (See underlined values in the table). When compared to them, MATRN achieves the state-of-the-art performances on all evaluation datasets except IC15 L . Specifically, our model achieved superior performance improvements on SVTP and CUTE, 1.3 percent point (pp) and 1.8pp respectively, because these datasets contain low-quality images, curved images, or proper nouns. Therefore, we found that our multi-modal fusion modules resolve the difficulties of scene text images, which cannot solve alone. JVSR <ref type="bibr" target="#b2">[3]</ref> still holds the best position on IC15 L , but MATRN shows huge performance gains For apples-to-apples comparisons, we reproduced ABINet, which is one of the state-of-the-art methods and also our baseline before adding multi-modal fusion modules. In the sanity check, we observed that all reproduced performances are aligned upon confidence intervals from the reported scores. When comparing MATRN from the reproduced ABINet, the performance improvements are statistically significant over all datasets and the gaps are 0.4pp, 1.3pp, 0.7pp, 0.7pp, 1.3pp, and 4.5pp on IIIT, SVT, IC13 S , IC15 S , SVTP, and CUTE, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison to State-of-the-Arts</head><p>Many previous works, such as SE-ASTER, SRN, ABINet, JVSR, and Vi-sionLAN, also analyzed how semantic information can be utilized for text recognition. When compared to them, MATRN shows the best performances on all but one datasets. This result implies that our incorporation method for visual and semantic features is effective compared to the existing methods that utilized semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Performance Comparison under the Comparable Resources</head><p>Since MATRN employs additional layers and modules upon ABINet, the performance gains might be considered as the effect of the additional memories and computational costs. To prove the pure benefits of the proposed methods, we evaluate large ABINets that utilize additional memories and computational costs as much as that MATRN requires. Specifically, the scale-up is conducted in two parts; adding transformer layers into VM (or LM) until the models have a similar number of parameters (Big models) and a similar inference speed (Bigger models). <ref type="table" target="#tab_1">Table 2</ref> shows the evaluation results. The Big models have similar parameters with MATRN but their speeds are faster since there is no crossreferences between visual and semantic features. By scaling up the models, the Bigger models have similar inference speeds with MATRN but hold more parameters. When comparing the performances, the Big models provide relatively </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Studies on Proposed Modules</head><p>Here, we analyze how the proposed modules contribute to the final performances. <ref type="table" target="#tab_2">Table 3</ref> shows the ablation studies that start from ABINet and add the proposed modules one by one. As can be seen, the total performances increase when adding proposed modules gradually. The application of the multi-modal transformer provides 0.4pp performance improvements based on ABINet. By applying SES on the multi-modal transformer, the total performance increases by 0.3pp. When adding the visual clue masking, the total performance finally becomes 93.5% with the 0.2pp improvement. Consequently, the simple application of a multi-modal transformer brings 0.4pp and our novel modules provide 0.5pp of performance improvements. We should note that applying a multi-modal transformer requires additional computations and parameters but the other proposed modules use quite small computations without any additional parameters. The ablation study indicates that our proposed modules for better multi-modal fusion lead to better STR performances effectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Discussion</head><p>Uni-modal vs. Multi-modal Feature Enhancement. The existing methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b31">30]</ref> focus on uni-modal FE by utilizing the other modality. To analyze the benefit of multi-modal FE, we compare the uni-modal FE that updates only a single modality utilizing the multi-modal transformer. In this experiment, we use SES for better fusion through the multi-modal transformer but do not apply the visual clue masking strategy for a fair comparison. <ref type="table">Table 4</ref> provides the comparison results. In the table, the first model is identical to ABINet with SES and the uni-modal FE models (the second and third models) update the target features through the multi-modal transformer. As can be seen, the unimodal FEs provide marginal performance improvements; 0.1pp in Total. When enhancing both modalities (the last model), the STR model enjoys two benefits of the semantic and visual FE and shows large performance improvements in Total. Given these points, we found that combining visual and semantic features improves the recognition performance, but one-way information flows are not enough to fuse two modalities. Besides, the multi-modal FE enables the two features to communicate in both directions and provides better performance.</p><p>STR Performance at Each Level of Features MATRN utilizes multi-task cross-entropy objectives, described in ?3.7. Here, we evaluate the STR performances from the multiple features; V, S, V M , S M , and F. <ref type="table">Table 5</ref> shows the results of ABINet and MATRN. Interestingly, the results of S show insufficient performances in both models by refining character sequences without consideration of visual clues. However, the semantic features are combined and lead to better performances; F (ABINet), V M , and S M (MATRN). In addition, the multi-modal features in MATRN show better performances than the final performances of ABINet and their combination shows the best.  ities. As can be seen in the examples, visual and semantic features refer to their own modality as well as interact with each other. <ref type="figure" target="#fig_6">Figure 5</ref> shows the test examples that ABINet fails but MATRN successes. As can be seen, MATRN provides robust results on "cropped characters", "heavily curved text", "ambiguous visual clues" and "low resolutions". The results show that MATRN tackles the existing challenges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis on Cross-references</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis on Previous Failure Cases</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper explores the combinations of visual and semantic features identified by VM and LM for better STR performances. Specifically, we propose MATRN that enhances visual and semantic features through cross-references between two modalities. MATRN consists of SES, that matches semantic features on 2D space that visual features are aligned in, multi-modal FE, that updates visual and semantic features together through the multi-modal transformer, and visual clue masking strategy, that stimulates the semantic references of visual features. In our experiments, naive applications of the multi-modal transformer lead to marginal improvements from the baseline. To this end, the components of MATRN effectively contribute to the multi-modal combinations and MATRN finally achieves state-of-the-art performances on seven STR benchmarks with large margins.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Four types of visual and semantic feature fusions for STR: (a) simple combination of outputs from VM and LM, (b) visual-to-semantic feature fusion, (c) semanticto-visual feature fusion, and (d) bi-directional feature fusion. SRN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>An overview of MATRN. A visual feature extractor and an LM extract visual and semantic features, respectively. By utilizing the attention map, representing relations between visual features and character positions, MATRNs encode spatial information into the semantic features and hide visual features related to a randomly selected character. Through the multi-modal feature enhancement module, visual and semantic features interact with each other and the enhanced features in the two modalities are fused to finalize the output sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>H 4 ?</head><label>4</label><figDesc>W 4 ?D . H, W are height and width of the image and D is the feature dimension. Before applying the Transformer, sinusoidal spatial position embeddings, P V ? R H 4 ? W 4 ?D , are added. Then, the Transformer, F V.T , with three layers is applied:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>H 4 ? W 4</head><label>44</label><figDesc>?D indicates visual features that are the outputs of the visual feature extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .</head><label>3</label><figDesc>Conceptual descriptions of visual clue masking strategy. Based on the attention map, representing relations between visual features and characters, influential features for a randomly selected character position are masked. In the multi-modal FE stage, semantic features are stimulated to be merged more strongly to compensate for the missing visual clues.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 Fig. 4 .</head><label>44</label><figDesc>shows attention map examples identified by the multi-modal FE of MATRN. At each attention map, the top-left and the bottom-right show the uni-modal attentions referring to their uni-modal features and the others provide the cross attentions between two different modal-Examples of self-attention maps in multi-modal FE. Attention maps on the top-right and the bottom-left indicate the cross attentions between two modalities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Examples that ABINet fails (first line) but MATRN succeeds (second line).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Recognition accuracies (%) on eight benchmark datasets, including the variant versions. The underlined values represent the best performances among the previous STR methods and the bold values indicate the best performances among all models including ours. For our implementation, we conduct repeated experiments with three different random seeds and report the averaged accuracy with standard deviation.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Regular test dataset</cell><cell cols="4">Irregular test dataset</cell></row><row><cell>Model</cell><cell cols="8">Year IIIT SVT IC13S IC13L IC15S IC15L SVTP CUTE</cell></row><row><cell>CombBest [2]</cell><cell cols="6">2019 87.9 87.5 93.6 92.3 77.6 71.8</cell><cell>79.2</cell><cell>74.0</cell></row><row><cell>ESIR [35]</cell><cell cols="2">2019 93.3 90.2</cell><cell>-</cell><cell>91.3</cell><cell>-</cell><cell>76.9</cell><cell>79.6</cell><cell>83.3</cell></row><row><cell>SE-ASTER [23]</cell><cell cols="2">2020 93.8 89.6</cell><cell>-</cell><cell cols="2">92.8 80.0</cell><cell></cell><cell>81.4</cell><cell>83.6</cell></row><row><cell>DAN [29]</cell><cell cols="2">2020 94.3 89.2</cell><cell>-</cell><cell>93.9</cell><cell>-</cell><cell>74.5</cell><cell>80.0</cell><cell>84.4</cell></row><row><cell cols="3">RobustScanner [34] 2020 95.3 88.1</cell><cell>-</cell><cell>94.8</cell><cell>-</cell><cell>77.1</cell><cell>79.5</cell><cell>90.3</cell></row><row><cell>AutoSTR [37]</cell><cell cols="2">2020 94.7 90.9</cell><cell>-</cell><cell cols="2">94.2 81.8</cell><cell>-</cell><cell>81.7</cell><cell>-</cell></row><row><cell>Yang et al. [32]</cell><cell cols="2">2020 94.7 88.9</cell><cell>-</cell><cell cols="3">93.2 79.5 77.1</cell><cell>80.9</cell><cell>85.4</cell></row><row><cell>SATRN [16]</cell><cell cols="2">2020 92.8 91.3</cell><cell>-</cell><cell>94.1</cell><cell>-</cell><cell>79.0</cell><cell>86.5</cell><cell>87.8</cell></row><row><cell>SRN [33]</cell><cell cols="3">2020 94.8 91.5 95.5</cell><cell>-</cell><cell>82.7</cell><cell>-</cell><cell>85.1</cell><cell>87.8</cell></row><row><cell>GA-SPIN [36]</cell><cell cols="2">2021 95.2 90.9</cell><cell>-</cell><cell cols="3">94.8 82.8 79.5</cell><cell>83.2</cell><cell>87.5</cell></row><row><cell>PREN2D [31]</cell><cell cols="3">2021 95.6 94.0 96.4</cell><cell>-</cell><cell>83.0</cell><cell>-</cell><cell>87.6</cell><cell>91.7</cell></row><row><cell>JVSR [3]</cell><cell cols="2">2021 95.2 92.2</cell><cell>-</cell><cell>95.5</cell><cell>-</cell><cell cols="2">84.0 85.7</cell><cell>89.7</cell></row><row><cell>VisionLAN [30]</cell><cell cols="3">2021 95.8 91.7 95.7</cell><cell>-</cell><cell>83.7</cell><cell>-</cell><cell>86.0</cell><cell>88.5</cell></row><row><cell>ABINet [7]</cell><cell cols="3">2021 96.2 93.5 97.4</cell><cell>-</cell><cell>86.0</cell><cell>-</cell><cell>89.3</cell><cell>89.2</cell></row><row><cell cols="2">ABINet (reproduced)</cell><cell cols="5">96.2 93.7 97.2 95.4 85.9 82.1</cell><cell>89.3</cell><cell>89.0</cell></row><row><cell></cell><cell></cell><cell>?0.2 ?0.4</cell><cell>?0.2</cell><cell>?0.2</cell><cell>?0.2</cell><cell>?0.1</cell><cell>?0.4</cell><cell>?0.3</cell></row><row><cell>MATRN (ours)</cell><cell></cell><cell cols="7">96.6 95.0 97.9 95.8 86.6 82.8 90.6 93.5</cell></row><row><cell></cell><cell></cell><cell>?0.1 ?0.2</cell><cell>?0.1</cell><cell>?0.1</cell><cell>?0.1</cell><cell>?0.0</cell><cell>?0.2</cell><cell>?0.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of ABINet and MATRN under the comparable resources. Param. indicates the number of model parameters (M) and Time represents the inference time (ms/image) with batch size of 1 under AMD 32 cores, RTX 3090 GPU, and SSD 2TB. The underline indicates the similar or more resource when comparing from those of MATRN and the bold represents the best performer.</figDesc><table><row><cell>Model</cell><cell cols="6">Param. Time IIIT SVT IC13S IC15S SVTP CUTE Total.</cell></row><row><cell>ABINet</cell><cell cols="3">36.7M 21.6ms 96.2 93.7 97.2 85.9</cell><cell>89.3</cell><cell>89.0</cell><cell>92.6</cell></row><row><cell>ABINet w/ VM-Big</cell><cell cols="3">46.2M 22.6ms 96.4 93.8 97.9 86.3</cell><cell>89.5</cell><cell>88.5</cell><cell>92.9</cell></row><row><cell>ABINet w/ LM-Big</cell><cell cols="3">46.2M 26.6ms 96.0 94.3 97.5 86.3</cell><cell>89.9</cell><cell>88.9</cell><cell>92.8</cell></row><row><cell cols="4">ABINet w/ VM-Bigger 90.3M 29.7ms 96.3 94.9 98.0 86.5</cell><cell>89.9</cell><cell>89.9</cell><cell>93.1</cell></row><row><cell cols="4">ABINet w/ LM-Bigger 52.5M 30.0ms 96.1 94.3 97.7 86.0</cell><cell>90.1</cell><cell>89.2</cell><cell>92.8</cell></row><row><cell>MATRN (ours)</cell><cell cols="6">44.2M 29.6ms 96.6 95.0 97.9 86.6 90.6 93.5 93.5</cell></row><row><cell></cell><cell>?0.1 ?0.2</cell><cell>?0.1</cell><cell>?0.1</cell><cell>?0.2</cell><cell>?0.6</cell><cell>?0.1</cell></row><row><cell cols="7">on the other datasets: 1.4pp on IIIT, 2.8pp on SVT, 0.3pp on IC13 L , 4.9pp on</cell></row><row><cell cols="2">SVTP, and 3.8pp on CUTE.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Performance improvements through gradual applications of our proposed modules. At the last line, all modules are applied and the model becomes MATRN.</figDesc><table><row><cell></cell><cell cols="4">Multi-modal FE Encoding Masking SES Visual Clue IIIT SVT IC13S IC15S SVTP CUTE Total.</cell></row><row><cell>ABINet?</cell><cell></cell><cell></cell><cell></cell><cell>96.2 93.7 97.2 85.9 89.3 89.0 92.6</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell></cell><cell>96.5 94.3 98.0 85.9 90.1 91.0 93.0</cell></row><row><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell>96.4 94.7 98.1 86.9 90.4 89.9 93.3</cell></row><row><cell>MATRN?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>96.6 95.0 97.9 86.6 90.6 93.5 93.5</cell></row><row><cell cols="5">small performance improvements: 0.3pp of VM-Big and 0.2pp of LM-Big in To-</cell></row><row><cell cols="5">tal. The Bigger models show better performance improvements than the Big</cell></row><row><cell cols="5">models; 0.5pp of VM-Bigger and 0.2pp of LM-Bigger in Total. However, the</cell></row><row><cell cols="5">performance gains from the scaling-up are restricted when comparing the perfor-</cell></row><row><cell cols="5">mance improvements of MATRN; 0.9pp in Total. In addition, the performances</cell></row><row><cell cols="5">of MATRN are statistically significant when comparing all large ABINets. The</cell></row><row><cell cols="5">experiments prove that the benefits of MATRN have not solely lie in the increas-</cell></row><row><cell cols="3">ing computation resources.</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .Table 5 .</head><label>45</label><figDesc>Comparisons between uni-modal and multi-model FEs. Visual FE Semantic FE IIIT SVT IC13S IC15S SVTP CUTE Total. STR Performances with each level of features from VM, LM, and their fusions. Each value indicates total STR accuracy (%). V and S represent the output features from VM and LM, respectively. V M and S M indicate the enhanced features through the cross-reference. F represents the combined features for the final output.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">96.5 93.2 97.0</cell><cell>85.9</cell><cell>89.0</cell><cell>89.2</cell><cell>92.7</cell></row><row><cell></cell><cell>?</cell><cell cols="3">96.5 93.8 97.2</cell><cell>85.8</cell><cell>89.0</cell><cell>90.3</cell><cell>92.8</cell></row><row><cell>?</cell><cell></cell><cell cols="3">96.1 93.5 97.5</cell><cell>86.1</cell><cell>89.8</cell><cell>91.3</cell><cell>92.8</cell></row><row><cell>?</cell><cell>?</cell><cell cols="5">96.4 94.7 98.1 86.9 90.4</cell><cell>89.9</cell><cell>93.3</cell></row><row><cell></cell><cell>Model</cell><cell>V</cell><cell>S</cell><cell>V M</cell><cell>S M</cell><cell>F</cell></row><row><cell></cell><cell>ABINet</cell><cell cols="2">90.9 49.5</cell><cell>-</cell><cell>-</cell><cell>92.6</cell></row><row><cell></cell><cell cols="6">MATRN 91.2 52.6 93.4 93.4 93.5</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://github.com/FangShancheng/ABINet</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vision transformer for fast and efficient scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Atienza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition -ICDAR 2021</title>
		<editor>Llad?s, J., Lopresti, D., Uchida, S.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="319" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">What is wrong with scene text recognition model comparisons? dataset and model analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2019-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint visual semantic reasoning: Multi-stage decoder for text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Bhunia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10" />
			<biblScope unit="page" from="14940" to="14949" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">History aware multimodal transformer for vision-and-language navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Guhur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
		<ptr target="https://www.aclweb.org/anthology/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-06" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Minneapolis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=YicbFdNTTy" />
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale. In: International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Read like humans: Autonomous, bidirectional and iterative language modeling for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="7098" to="7107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-modal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="214" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Iterative answer prediction with pointer-augmented multimodal transformers for textvqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9992" to="10002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Deep Learning, NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<idno type="DOI">10.1109/ICCV.2011.6126402</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2011.6126402" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almaz?n</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>De Las Heras</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.2013.221</idno>
		<ptr target="https://doi.org/10.1109/ICDAR.2013.221" />
		<title level="m">12th International Conference on Document Analysis and Recognition</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
	<note>Icdar 2013 robust reading competition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1412.6980" />
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations</title>
		<editor>Bengio, Y., LeCun, Y.</editor>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-07" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deterministic non-autoregressive neural sequence modeling by iterative refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1149</idno>
		<ptr target="https://www.aclweb.org/anthology/D18-1149" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-11" />
			<biblScope unit="page" from="1173" to="1182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On recognizing texts of arbitrary shapes with 2d self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Icdar 2003 robust reading competitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Panaretos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Young</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.2003.1227749</idno>
		<ptr target="https://doi.org/10.1109/ICDAR.2003.1227749" />
	</analytic>
	<monogr>
		<title level="m">Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="682" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="67" to="83" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scene text recognition using higher order language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
		<idno type="DOI">10.5244/C.26.127</idno>
		<ptr target="https://doi.org/http://dx.doi.org/10.5244/C.26.127" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<publisher>BMVA Press</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="127" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Recognizing text with perspective distortion in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2013.76</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2013.76" />
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multi-modal fusion transformer for end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7077" to="7087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pimnet: a parallel, iterative and mimicking network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="2046" to="2055" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Seed: Semantics enhanced encoder-decoder framework for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A robust arbitrary text detection system for natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Risnumawan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shivakumara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="8027" to="8048" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<idno type="DOI">10.1016/j.eswa.2014.07.008</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0957417414004060" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">NRTR: A no-recurrence sequence-to-sequence model for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.2019.00130</idno>
		<ptr target="https://doi.org/10.1109/ICDAR.2019.00130" />
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Document Analysis and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019-09-20" />
			<biblScope unit="page" from="781" to="786" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2016.2646371</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2016.2646371" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2298" to="2304" />
			<date type="published" when="2017-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Aster: An attentional scene text recognizer with flexible rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2018.2848939</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2018.2848939" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2035" to="2048" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Multimodal transformer for unaligned multimodal language sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Decoupled attention network for text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cai</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v34i07.6903</idno>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/6903" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="12216" to="12224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">From two to one: A new scene text recognizer with visual language modeling network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="14194" to="14203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Primitive representation learning for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021-06" />
			<biblScope unit="page" from="284" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A holistic representation guided attention network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neucom.2020.07.010</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0925231220311176" />
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">414</biblScope>
			<biblScope unit="page" from="67" to="75" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Towards accurate scene text recognition with semantic reasoning networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Robustscanner: Dynamically enhancing positional clues for robust text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<editor>Vedaldi, A., Bischof, H., Brox, T., Frahm, J.M.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="135" to="151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Esir: End-to-end scene text recognition via iterative image rectification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spin: Structurepreserving inner offset network for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zou</surname></persName>
		</author>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/16442" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021-05" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="3305" to="3314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Autostr: Efficient backbone search for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
