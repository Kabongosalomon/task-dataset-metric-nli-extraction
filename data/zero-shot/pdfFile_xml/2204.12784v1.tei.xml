<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learn from Structural Scope: Improving Aspect-Level Sentiment Analysis with Hybrid Graph Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lvxiaowei</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxuan</forename><surname>Pang</surname></persName>
							<email>pangxiaoxuan.pxx@alibaba-inc.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
								<orgName type="institution">China Digital Commerce-Tao Cai Cai-Tech Dept-Data Intelligence Growth</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwang</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Peng</surname></persName>
							<email>pengjw@zju.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learn from Structural Scope: Improving Aspect-Level Sentiment Analysis with Hybrid Graph Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T03:15+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Aspect-level sentiment analysis aims to determine the sentiment polarity towards a specific target in a sentence. The main challenge of this task is to effectively model the relation between targets and sentiments so as to filter out noisy opinion words from irrelevant targets. Most recent efforts capture relations through target-sentiment pairs or opinion spans from a word-level or phrase-level perspective. Based on the observation that targets and sentiments essentially establish relations following the grammatical hierarchy of phrase-clausesentence structure, it is hopeful to exploit comprehensive syntactic information for better guiding the learning process. Therefore, we introduce the concept of Scope, which outlines a structural text region related to a specific target. To jointly learn structural Scope and predict the sentiment polarity, we propose a hybrid graph convolutional network (HGCN) to synthesize information from constituency tree and dependency tree, exploring the potential of linking two syntax parsing methods to enrich the representation. Experimental results on four public datasets illustrate that our HGCN model outperforms current state-of-the-art baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Aspect-level sentiment analysis (ALSA) is a fine-grained classification task, aiming at identifying opinion polarities towards specific entities called targets. <ref type="figure" target="#fig_0">Figure 1</ref> shows a review commenting on the restaurant with two target terms "menu" and "food". The sentiment polarities over them are positive and negative respectively. Since multiple targets may appear in one sentence and convey different sentiments, the main challenge of ALSA task is to effectively filter out noisy or misleading opinion words from irrelevant targets and extract distraction-free text from opinion expressions accurately.</p><p>Considerable efforts have been devoted to overcoming the difficulties. Their works can be broadly divided into the following three categories. Attention-based models: Attention mechanism has been widely adopted to learn target-specific * Corresponding author Great menu, but when the food came, it was not fresh. features <ref type="bibr" target="#b5">[Wang et al., 2016;</ref><ref type="bibr" target="#b2">Ma et al., 2017]</ref> so as to help models concentrate on crucial parts of a sentence for sentiment classification. However, the attention scatters across the whole sentence and is prone to attend on noisy opinion words from other targets. Syntax-based models: Syntax-based method aims to model syntactic structures that exist in the sentence. Among them, dependency tree <ref type="bibr">[Zhang et al., 2019;</ref><ref type="bibr">Sun et al., 2019;</ref> has been mostly used to capture the structural dependencies between targets and the sentiment expressions. Nevertheless, multi-hop dependency relations between target and unrelated opinion words that are susceptible to confusing and misleading the model. Spanbased models: Span-based models <ref type="bibr" target="#b4">[Wang and Lu, 2018;</ref><ref type="bibr" target="#b1">Hu et al., 2019;</ref> apply a divide-and-conquer strategy to handle the ALSA task. Specifically, the model extracts an opinion span towards each target based on a soft or hard selection approach, and then focuses on the phrase-level features for sentiment classification. Therefore, it is difficult to generate structurally complete spans for complicated sentences due to lack of syntactic information.</p><p>To tackle aforementioned problems, we introduce a concept of structural Scope, which is a well-structured and continuous text region expressing target-specific opinion. Scope delineates the exact context of each target so as to alleviate the distraction problem caused by opinion words from other targets. Compared with opinion span, Scope is characterized by a well-defined structure, which highlights the learning of various syntactic features (e.g., phrase, clause and sentence) related to the target to ensure structural and semantic completeness. <ref type="figure" target="#fig_0">Figure 1</ref> demonstrates an example containing two Scopes with their corresponding targets in different structures. It combines a phrase-level relation (a noun phrase connecting the target "menu" and opinion word "great") and a sentence-level relation (the target "food" linking the remote opinion words "not fresh" outside of the local clause through a coreference relation). This demonstrates that the targets and sentiments establish relations following the grammatical hierarchy of phrase-clause-sentence structure, which substantially filters out noisy opinion words from irrelevant targets.</p><p>Three observations motivate us to aggregate syntactic information of constituency tree and dependency tree to learn structural Scope. First, constituency parsing decomposes a sentence into phrases and combines them hierarchically into clauses and sentences, which naturally follows the phraseclause-sentence structure of Scope and can serve as the foundation to encode structural information. Second, dependency parsing is capable of capturing long-range dependencies such as coreference relation, which can help the model obtain the sentence-level connection. Third, constituency parsing displays the entire sentence structure and relations, while dependency parsing explores relations between words. They present two complementary sides of syntactic relations in sentences. Therefore, it would be beneficial to incorporate constituency tree and dependency tree into the model to learn more comprehensive syntactic relations.</p><p>On the basis of above analysis, we present a hybrid graph convolutional network (HGCN) to jointly learn the structural Scope and determine the sentiment polarity. For encoding the constituency tree, we propose the constituency GCN module and the computation is performed in three stages. First, the representations of the constituents are composed by the representations of the words and the embeddings of constituent labels. Second, graph convolutional networks (GCNs)  are exploited to learn constituent representations. Third, a constituent-token attention mechanism is applied to decompose back the information to word representations. Regarding the dependency tree, we utilize a relational graph convolutional network (RGCN) to obtain syntactic representation with labeled edges. Finally, we incorporate such heterogeneous information to determine the Scope region through the conditional random field (CRF) <ref type="bibr" target="#b2">[Lafferty et al., 2001]</ref>, and to identify the opinion polarities as well.</p><p>The main contributions of this work include:</p><p>? We introduce the concept of Scope as a syntacticallyinformed method to capture the relations between targets and sentiments, which could generate distractionfree opinion expressions for sentiment classification.</p><p>? We propose a constituency GCN module equipped with constituent-token attention mechanism to encode structural representations from constituency tree.</p><p>? We present a new HGCN model to synthesize constituency relations and dependency relations, exploiting the complementary strengths of two syntax parsing methods to learn syntactic relations for Scope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Definition of Scope</head><p>To eliminate noisy opinion words from unrelated targets, we propose a syntactic structure guided modeling concept called Scope. In general, Scope contains a specific target and its dependent sentiment expression within a continuous and minimum text in a sentence. Specifically, we first define a constituent 1 candidate set C s to hold a group of constituents (denoted as c i ), which include a consecutive text span from target to opinion words in their leaf nodes (denoted as N ode(c i )).</p><p>Each c i can be categorized into three levels according to grammatical definitions, which are phrase level (e.g., VP, NP), clause level (e.g., SBAR) and sentence level (e.g., S). Scope is then defined as seeking the span of text corresponding to the constituent with the fewest leaf nodes in C s :</p><formula xml:id="formula_0">N ode(C s [argmin([count(N ode(c i )), c i ? C s ])])<label>(1)</label></formula><p>where count(?) calculates the number of nodes excluding extraneous components (e.g., adjunct and punctuation).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Graph Convolutional Network</head><p>We leverage graph convolutional networks over constituency tree and dependency tree to encode structural information.</p><p>GCNs are neural networks that compute the representation of a node conditioned on its neighboring nodes in a given graph. Each node is updated by aggregating the propagated message from the neighboring nodes through multilayer GCNs. Given a graph G contains sets of nodes V and edges E. Following the idea of , we allow G to have self-loops. After that, we can obtain the adjacency matrix A ? R n?n according to the G, where n denotes the size of V. The A ij in the adjacency matrix reflects the connection between the node i and node j. Specifically, A ij = 1 if node i is connected to node j and in the other scenario Aij = 0. Then GCN is able to propagate information over the paths and update node representations. In general, stacking l layers of GCN allows aggregation of information across the l-th order neighborhood. In such an operation, the representation of each node is updated with normalization factor as follow:</p><formula xml:id="formula_1">h (l+1) i = ? ? ? n j=1 c i A ij W (l) h (l) j + b (l) ? ? (2) where h (l)</formula><p>j is the hidden representation for node j that is generated by l-th layer of the GCN. W (l) and b (l) are trainable parameters of weights and bias in l-th layer, respectively. c i is the normalization factor calculated as c i = 1/d i and d i denotes the degree of node i in the graph, which is computed as</p><formula xml:id="formula_2">d i = k j=1 A ij .</formula><p>And ? is a non-linear activation function, (e.g., RELU(?) <ref type="bibr" target="#b0">[Glorot et al., 2011]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hybrid Graph Convolutional Network</head><p>Our model aims to extract the Scope relevant to the target term and predict the sentiment polarity jointly. <ref type="figure" target="#fig_1">Figure  2</ref> gives an overview of our HGCN model. In the aspectlevel sentiment classification task, a sentence with n words {w 1 , . . . , w i , w i+1 , . . . , w j , w j+1 , . . . , w n } is given, where the target {w i , . . . , w j } is a sub-sequence of the sentence.</p><formula xml:id="formula_3">= ?"#$ Constituent GCN Embedding Encoder CRF VP NP ADJP S Operations Scope punct cop nsubj amod det</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Element-wise add</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concatenation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Average-pooling</head><p>The fried rice is amazing . Then, BiLSTM <ref type="bibr">[Graves et al., 2013]</ref> or <ref type="bibr">BERT [Devlin et al., 2019]</ref> is applied as sentence encoder to integrate contextual information into hidden representations of words. For the BiLSTM encoder, we first embed each word into lowdimensional vector space by looking up an embedding matrix E emb ? R d?|V | , where d denotes the dimension of word embedding and |V | is the size of vocabulary. Then we feed the embedding of words into BiLSTM encoder for obtaining contextual hidden representations H = {h 1 , h 2 , ? ? ? , h n }, where h i ? R 2d is the hidden representations from BiLSTM. As for the BERT, we construct an aspect oriented context "[CLS] sentence [SEP] aspect [SEP]" as input to obtain contextual hidden representations of the sentence. Moreover, there is a gap between word-piece representations of BERT and wordlevel based syntactic features. Therefore, we compose the representations of the sub-word as word representations with average-pooling to tackle this mismatch. Then the contextual representations are fed into the Constituency GCN (CGCN) and Dependency GCN (DGCN), respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constituent Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentiment Classifier</head><formula xml:id="formula_4">Constituent Decoder C-T Attention Dependency GCN V K Q B I I O ! ? . "#$% ? &amp;% "#$% ? '&amp;(# "#$% ? )'&amp;#" "#$% ? *+# "#$% ? ,-.&amp;/0 "#$% I I !<label>?</label></formula><p>Eventually, we concatenate representations from DGCN and CGCN module to form syntactic representations. After that, a CRF layer is utilized to generate BIO-tags for modeling the Scope with syntactic representations and we aggregate the syntactic representations over the target terms via pooling to determine sentiment polarity jointly. In the following, we will elaborate the details of our proposed HGCN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constituency GCN (CGCN)</head><p>The constituency tree breaks a sentence into phrases, and then they can be composed hierarchically into clause and sentence level from bottom to top. Intuitively, it is reasonable to utilize constituency tree to encode structural information for modeling Scope. In order to incorporate such structural constituency syntax, we propose CGCN module with multistage architecture for obtaining constituency-aware representations. Compared with the SpanGCN model <ref type="bibr" target="#b2">[Marcheggiani and Titov, 2020]</ref>, we design the constituent-token (C-T) atten-tion to reassign the representation of words as a substitute for BiLSTM, which reduces the computational complexity and attains better performance. As shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the architecture of CGCN is composed by three sub-modules: constituent encoder, constituent GCN and constituent decoder.</p><p>Constituent Encoder. The constituency tree is composed of constituents (V c ) and words (V w ). In order to obtain representations of constituents, we first embed each constituent label using E cons ? R dc?|N | , where d c is the dimension of embedding and |N | denotes the number of constituents. Then we assign the representation of each constituent v ? V in this stage relying on the embedding of the constituent label (e.g., NP or VP) and their child nodes set S i ? (V c ? V w ). Take <ref type="figure" target="#fig_1">Figure 2</ref> as an example. The constituent "S" has three child nodes. Two of them belong to V c and the rest one belongs to V w . Its representation is obtained through averaging the embeddings of its own label, the label embedding of its two child constituents and the representations of word ".".</p><p>Constituent GCN. In this stage, it enables the interactive information passing from each constituent and ensures information of the child nodes, which can be integrated into the representation of their immediate parent node and vice versa. GCN is operated on the graph constructed by constituency tree where the nodes represent all constituents in set V c and the edges correspond to the connection between constituents and their child nodes in both directions and each constituent is connected to itself on account of the self-loop. After that, structural syntactic representation H c = {h c 1 , h c 2 , . . . , h c m } can be obtained from Equation 2 with layer normalization applied, where m refers to the number of constituents.</p><p>Constituent Decoder. After the message passing in GCN module, structural syntactic information are injected into the representation of constituents. At this point, we propose the constituent-token (C-T) attention mechanism to assign the information back to each word. In our implementation, the attention weights indicate the contribution of the word to the representation of corresponding constituent which can be computed in constituent encoder stage. Then the allocation coefficient (i.e. attention scores) can be computed as below:</p><formula xml:id="formula_5">A cons = softmax QW q ? (KW k ) T ? d + M (3)</formula><p>where the matrices Q and K are the representation of words and the representation of constituents composed by constituent encoder, respectively. W q and W k denote the trainable parameters, while d is the dimensionality of the hidden representations. In addition, M stands for a mask matrix to ensure that information can only be injected back to the words from their parent constituent. Here, the dot product is adopted to calculate relatedness between words and constituents. Then we can obtain constituency representations of words based on attention scores, which can be formulated as:</p><formula xml:id="formula_6">H cons = A cons H c<label>(4)</label></formula><p>After the above three-stage process, we obtain the constituency representations H cons = {h cons 1 , h cons 2 , . . . , h cons n }.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dependency GCN (DGCN)</head><p>The DGCN is used to capture long-range dependencies such as coreference relationships. Intuitively, different dependency relations should have distinctive impacts on adjacent nodes. However, most of the previous work fails to consider the relationship of the edges. Inspired by <ref type="bibr" target="#b3">[Schlichtkrull et al., 2018]</ref>, we utilize relational graph convolutional network (RGCN) to encode dependency structure with labeled edges.</p><p>Firstly, in order to integrate the relationship information of the edges, we embed each kind of edge label using a randomly initialized embedding matrix E rel ? R dr?|M| where d r refers to the dimension of word embedding and |M| is the number of relation types. Then we can obtain the representation of the relation between node i and node j as r ij . We consider the representation r ij as the gate-value to determine how much of the information will be updated to neighboring nodes. Thus the adjacency matrix A d ij takes the form:</p><formula xml:id="formula_7">A d ij = ? (r ij W r + b r )<label>(5)</label></formula><p>where W r and b r denote trainable parameters of weights and bias. ?(?) refers to the sigmoid function. After that, we regard the hidden representation H from BiLSTM or BERT encoder as initial node representations in DGCN. After that, we can compute dependency graph representation H deps = {h deps 1 , h deps 2 , ? ? ? , h deps n } based on Equation 2. Finally, we apply concatenation operation on CGCN and DGCN to obtain the syntactic representations H syn for Scope modeling. Next an average pooling operation is utilized to generate the sentiment representation H senti for the ALSA task. Then H senti is fed into a linear layer and mapped to probabilities of different polarities by a softmax function, i.e.,</p><formula xml:id="formula_8">p(y) = softmax W p H senti + b p<label>(6)</label></formula><p>where W p and b p are the learned weights and bias. <ref type="table" target="#tab_2">Neutral  Negative  Train Test Train Test Train Test   Rest14  2164  727  637  196  807  196  Lap14  976  337  455  167  851  128  Rest15  912  326  36  34  256  182  Rest16  1657  611  101  44  748  204   Table 1</ref>: Statistics for the four experimental datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset Positive</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Training and Testing</head><p>During the training process, we define a joint loss for selection of Scope and sentiment classification after obtaining the syntactic representation from CGCN and DGCN:</p><formula xml:id="formula_9">L(?) = L polarity + ?L scope + ? ? 2<label>(7)</label></formula><p>where ? is the coupling co-efficiency that regulates the two losses. ? represents all trainable parameters in our proposed HGCN model and ? refers to the coefficient of L 2regularization. Moreover, L polarity denotes a standard threeway cross-entropy loss defined for the ALSA task. L scope is defined as the negative conditional log-likelihood loss of the CRF layer for Scope modeling. We can formulate them as:</p><formula xml:id="formula_10">L polarity = ? i? i log p (y i ) (8) L scope = ? log P (y | u; W, b)<label>(9)</label></formula><p>where? i denotes the ground truth, i is the i-th sentiment polarity and W , b are trainable parameters of weights and bias. In addition, as for the Scope selection task, Viterbi algorithm is utilized to predict the most likely label assignment of sequence during the testing process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Experimental Setups.</head><p>Datasets. We conduct experiments of aspect-level sentiment analysis task on four benchmark datasets. As the statistics of the datasets shown in <ref type="table">Table 1</ref>, there are restaurantdomain (Rest14, Rest15, Rest16) datasets and laptop-domain (Lap14) taken from SemEval <ref type="bibr">[Pontiki et al., 2014;</ref><ref type="bibr">Pontiki et al., 2015;</ref><ref type="bibr">Pontiki et al., 2016]</ref>. In order to utilize Scope, we develop a semi-automated annotation tool and then ask 4 experienced annotators and 2 verifiers to annotate the datasets with BIO-tags of Scope. It is worth mentioning that our tool can automatically complete the preliminary annotation based on syntax and designed rules. Then annotators make fine refinements on this basis and only 26.4% of the samples require manual adjustments. There are more details in Appendix A.</p><p>Baselines. Baselines can be categorized into three groups.</p><p>? 1) Attention-based method. ATAE-LSTM  uses aspect embedding and attention mechanism in ALSA tasks. AOA <ref type="bibr" target="#b2">[Huang et al., 2018]</ref>   <ref type="table">Table 2</ref>: Accuracy (%) comparison on baselines over 10 runs with random initialization. The best result with each dataset are in bold. For our models, the upper results represent the best performance and the lower are the average performance among 10 runs. capture word-level interactions between target term and context. ? 2) Syntactic-based method. <ref type="bibr">ASGCN [Zhang et al., 2019]</ref> and <ref type="bibr">CDT [Sun et al., 2019]</ref> first apply graph convolutional network for encoding aspect-specific word representations. BiGCN [Zhang and Qian, 2020] adopts hierarchical graph structure to encode dependency and word cooccurrence information. <ref type="bibr">InterGCN [Liang et al., 2020]</ref> employs a GCN over a dependency tree to learn aspect representations. RGAT, RGAT+BERT <ref type="bibr" target="#b5">[Wang et al., 2020]</ref> utilize a relational graph attention network to encode the pruned dependency trees. DGEDT, DGEDT+BERT <ref type="bibr" target="#b4">[Tang et al., 2020]</ref> provide a dependency graph enhanced dualtransformer network to encode heterogeneous information. <ref type="bibr">DualGCN, DualGCN+BERT [Li et al., 2021]</ref> incorporate syntactic structure and semantic relevance to generate features. ? 3) Span-based method. SA-LSTM <ref type="bibr" target="#b4">[Wang and Lu, 2018]</ref> presents a segmentation attention model to distill the sentiment semantics for capturing the structural dependencies with external opinion data. MCRF-SA  proposes a hard selection approach to determine the opinions by self-critical reinforcement learning with annotated opinions.</p><p>Training Details. We utilize Stanford Parser <ref type="bibr" target="#b2">[Manning et al., 2014]</ref> for constituency and dependency parsing. As for our HGCN model, we utilize 300-d GloVe vectors <ref type="bibr" target="#b2">[Pennington et al., 2014]</ref> to initialize word embeddings. Then the embeddings are fed into the BiLSTM model, whose hidden size is set to 100-d. The size of dependency relation embeddings are set to 30, while constituency embeddings are set to 100. We use the Adam optimizer with the learning rate of 0.01. HGCN is trained for 100 epochs with batch size 32. The regularization coefficients ? is set to 1e-4 and co-efficiency ? is set to 3e-2. As for HGCN+BERT, we use the fine-tuned BERT with officially released pre-trained BERT parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results and Analysis</head><p>Main Performances. The overall results are shown in Table 2, from which several observations can be derived.</p><p>First, the HGCN model consistently outperforms all compared baselines on different datasets. Second, compared with attention-based models, our model is significantly better since it focuses directly on the text regions that are semantically associated with the corresponding target. Therefore, it can eliminate noises introduced by the attention mechanism. Third, the performance of HGCN is also better than syntax-based models. It demonstrates that utilizing constituency and dependency information simultaneously enables the model to capture more structural syntactic information as a way to enhance the representations and HGCN can also avoid multihop between target and unrelated opinion words in dependency parsing. As for span-based models, our HGCN is able to model more complete text region compared to them and ensures semantic integrity, which allows our model to have a more robust semantic basis for determining the sentiment polarity. In addition, after we integrate our model with BERT, it obtains further improvement and reaches a new state-of-theart. In a conclusion, these results can illustrate the effectiveness of our HGCN for capturing structural syntactic information in aspect-level sentiment analysis. Effect of Multiple Aspects. In the datasets, there exists the scenario where a sentence may have multiple targets. Therefore, models may be misled by the opinion words of irrelevant targets and determine the sentiment polarity incorrectly.</p><p>We conduct experiments on three typical models from different categories mentioned above. As shown in <ref type="figure" target="#fig_2">Figure 3</ref> (In Rest15, there are no sentences with 5 targets), syntax-based  method (RGAT) outperforms attention-based method (AOA) in most cases. It indicates that establishing dependencies between words can partly avoid noisy from unrelated opinion words. As for span-based method (MCRF-SA), it can be seen that the performances tend to fluctuate when the number of targets is more than 3. This is due to the informal expressions and complicated structure of opinion words in sentences. Moreover, it is obvious that our HGCN achieves better performances on four datasets with different number of targets. This confirms the previously established viewpoint that the Scope can efficiently delineate the text region of each target so as to alleviate the distraction problem caused by opinion words from irrelevant targets. In addition, it also demonstrates that our HGCN is able to learn more comprehensive syntactic information for reaching robust results.</p><p>Investigation on the Mask Matrix. In order to investigate the necessity of the mask matrix on assigning the information back to each word from constituents, we visualize the C-T attention score matrix of the CGCN w/o mask (i.e., the mask matrix M in Equation 3 is dropped) and intact CGCN. We take the sample shown in <ref type="figure" target="#fig_1">Figure 2</ref> with its constituency tree. As shown in <ref type="figure" target="#fig_3">Figure 4</ref>, constituent is able to assign more information to the words that form it based on the attention score matrix constructed by C-T attention mechanism. However, the C-T attention may cause the problem that constituents are forced to inject information back to the words that are not composing them. Though only little information can be assigned to these words, it may introduce interference into CGCN. The mask matrix is utilized to constrain the assignment policy of C-T attention for tackling the problem. Compared with CGCN w/o mask and intact CGCN in <ref type="figure" target="#fig_3">Figure 4</ref> (M denotes the part that should be masked in C-T attention), the attention score matrix produced by intact CGCN is relatively sparse. For example, only "amazing" can be assigned the information from constituent "ADJP". Therefore, the mask matrix is a necessary component in our proposed CGCN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation.</head><p>To examine the level of benefits on each component in our model, we conduct extensive ablation studies. The results of accuracy are shown in  the subtask of Scope modeling so that HGCN cannot utilizing external Scope data. However, our HGCN w/o CRF still outperforms the majority of the baselines shown in <ref type="table">Table 2</ref>. It proves that our motivation of synthesizing constituency tree and dependency tree can learn more comprehensive syntactic information. In addition, the joint method (i.e., our HGCN) is better than the HGCN w/o CRF and achieves the best performance. This confirms that the joint scheme of Scope modeling and sentiment polarity determination is effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>Traditional sentiment analysis works are based on sentencelevel or document-level while aspect-level sentiment analysis is a more fine-grained sentiment task. Therefore, it has received extensive research attentions in recent years.</p><p>In general, opinion words are not particularly far from the target term. Thus among neural networks methods, most recent works utilize various attention-based models <ref type="bibr" target="#b2">Huang et al., 2018;</ref><ref type="bibr" target="#b0">Fan et al., 2018]</ref> to discriminate sentiment polarity via words nearby the target term implicitly.</p><p>Another trends in recent researches are concerned with leveraging dependency parse tree to establish the connection between target terms and opinion words <ref type="bibr" target="#b4">[Tang et al., 2020]</ref>. Typically, GCN is utilized to obtain aspect-oriented features from syntactic trees <ref type="bibr">[Zhang et al., 2019;</ref><ref type="bibr">Sun et al., 2019;</ref><ref type="bibr">Zhang and Qian, 2020;</ref><ref type="bibr" target="#b2">Li et al., 2021;</ref><ref type="bibr" target="#b2">Liang et al., 2020;</ref><ref type="bibr" target="#b4">Tang et al., 2020]</ref> since it has the ability on addressing the graph structure representation.</p><p>Some other efforts try to extract snippet of opinion words directly from the sentence <ref type="bibr" target="#b4">[Wang and Lu, 2018;</ref>. These works focus on the explicit extraction of targetopinion pairs to determine sentiment polarity.</p><p>In order to obtain constituent representations, we thus propose the constituency GCN model and constituent-token attention in this work for incorporating constituency parse tree inspired by the graph-based models used in semantic role labeling (SRL) task <ref type="bibr" target="#b2">[Marcheggiani and Titov, 2020]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we provide a novel perspective that captures relations between targets and sentiments following the grammatical hierarchy of phrase-clause-sentence structure, aiming at filtering out noisy opinion words from irrelevant targets. Therefore, we introduce the definition of Scope, which is a structural text region related to the target term. To learn comprehensive syntactic relations for Scope, we propose a hybrid graph convolutional network to synthesize information from constituency tree and dependency tree. Experiments on extensive datasets demonstrate that our model outperforms the baselines and achieves the state-of-the-art performance.</p><p>[Zhang and Qian, 2020] Mi Zhang and Tieyun Qian. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Datasets and Annotation Procedures</head><p>Consider the sample sentence "Great food but the service was dreadful!" with " food" and "service" as targets. The Scope of target "food" can be "Great food" under definition of the Scope. As for the BIO-tags in this case, "Great" is marked as "B" and "food" is marked as "I" while other tokens in the sentence are marked as "O". To simplify the annotation procedures, we develop a semi-automated annotation tool as shown in <ref type="figure" target="#fig_4">Figure 5</ref>. Our tool can automatically complete the preliminary annotation based on syntax and designed rules. The auto-labeled Scope is set as blue blocks in <ref type="figure" target="#fig_4">Figure 5</ref>. Therefore, the annotators do not need to annotate all samples and only make fine refinements on the inappropriate preannotation results. According to our statistics, only 26.4% of the samples require manual adjustments. This demonstrates that Scope annotation procedures are easily accessible and we will open the annotation tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Case Study</head><p>In this section, we investigate the behaviour of HGCN, CDT and RGAT on case examples. As we can see from the first example in <ref type="table" target="#tab_3">Table 4</ref>, the CDT model is mistakenly identified since it is affected by an incorrect connection in the dependency tree where the edges connect "service" and "great" with 2-hops. Due to the gate mechanism, RGAT can filter out the noisy opinion word "great" by utilizing the edge labels, thus responding correctly. Our HGCN can exploit region information provided by the structural Scope to ensure the correctness of the determination. As for the second example, our model reasonably generates the Scope related to the target term "location" and identifies the sentiment polarity correctly. However, due to the edge between target term and "treasure" in dependency tree, RGAT and CDT are both misled. The third example shows that RGAT and CDT are all confused by another target "blueberry" in the sentence with its opinion word "wonderful". By comparison, our model is able to determine sentiment polarity correctly since it delineates the exact context of each target so as to filter out noisy opinion words from irrelevant targets. These examples all illustrate the effectiveness of our Scope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Error Analysis</head><p>To analyze the limitation of our model, we trace back the error cases in the test sets, and identify three categories of reasons: short-sighted error, interruption error and representation error. The short-sighted error comes from the reason that the Scope fails to attend implicit transitive descriptions related to the target term. For example, the sentence </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sentence</head><p>HGCN CDT RGAT 1. Great food but the service was dreadful! P N P P P N 2. Unfortunately, unless you live in the neighborhood, it's not in a convenient location but is more like a hidden treasure. N P P 3. My friend had a burger and i had these wonderful blueberry pancakes.</p><p>O P P P P P <ref type="table" target="#tab_3">Table 4</ref>: The words highlighted in red and blue denote the given targets and their Scope generated by HGCN. The notations P, N and O represent positive, negative and neutral sentiment, respectively.</p><p>"For someone who used to hate Indian food, Baluchi's has changed my mind." with its target term "Indian food", our Scope ignores the latter part of the sentence which contains the semantic transitions. Thus our model considers this to be an expression of negative sentiment polarity resulting in misidentification. As for the interruption error, some grammatical connection of the target term in the sentence (e.g., parenthetical text) can be interruptive, which may interfere with the constituency parsing and the modeling of Scope. Consequently, the Scope selection will suffer from missing essential information or ill-structured problems, like separate segments. Most of the other errors can be summarized as representation error. Though the model can correctly delineate the Scope, it is still unable to determine the sentiment polarity accurately, especially for the expression of negation and modality, which remains challenging for neural networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>An instance of a restaurant review with two target terms which have opposite sentiments. The Scopes are highlighted with a box and contain their corresponding target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Overview of hybrid graph convolutional network (HGCN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Accuracy versus the number of targets (# Targets).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>The C-T attention score matrix of CGCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>The interface of our semi-automated Scope annotation tool.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>utilizes two LSTMs and an interactive attention mechanism to generate representations for the aspect and sentence.MGAN [Fan  et al., 2018]  proposes a multigrained attention network to</figDesc><table><row><cell>Model</cell><cell cols="4">Rest14 Lap14 Rest15 Rest16</cell></row><row><cell cols="2">? Attention-based methods</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ATAE-LSTM</cell><cell>77.01</cell><cell>70.09</cell><cell>76.60</cell><cell>84.36</cell></row><row><cell>AOA</cell><cell>79.97</cell><cell>72.62</cell><cell>78.17</cell><cell>87.50</cell></row><row><cell>MGAN</cell><cell>79.96</cell><cell>71.81</cell><cell>78.15</cell><cell>85.29</cell></row><row><cell cols="2">? Syntactic-based methods</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ASGCN</cell><cell>80.80</cell><cell>74.61</cell><cell>79.43</cell><cell>88.02</cell></row><row><cell>CDT</cell><cell>81.74</cell><cell>74.65</cell><cell>77.94</cell><cell>83.88</cell></row><row><cell>BiGCN</cell><cell>80.83</cell><cell>74.96</cell><cell>80.33</cell><cell>88.47</cell></row><row><cell>InterGCN</cell><cell>81.26</cell><cell>75.06</cell><cell>79.54</cell><cell>88.52</cell></row><row><cell>DGEDT</cell><cell>80.89</cell><cell>73.04</cell><cell>78.32</cell><cell>86.52</cell></row><row><cell>RGAT</cell><cell>81.50</cell><cell>73.72</cell><cell>78.78</cell><cell>88.21</cell></row><row><cell>DualGCN</cell><cell>82.85</cell><cell>76.66</cell><cell>80.16</cell><cell>88.19</cell></row><row><cell cols="2">? Span-based methods</cell><cell></cell><cell></cell><cell></cell></row><row><cell>SA-LSTM</cell><cell>77.64</cell><cell>70.67</cell><cell>76.63</cell><cell>85.96</cell></row><row><cell>MCRF-SA</cell><cell>80.94</cell><cell>74.11</cell><cell>78.71</cell><cell>87.76</cell></row><row><cell>HGCN</cell><cell>84.09</cell><cell>78.64</cell><cell>82.66</cell><cell>89.84</cell></row><row><cell>(Ours)</cell><cell>(82.91)</cell><cell>(76.82)</cell><cell>(80.81)</cell><cell>(88.92)</cell></row><row><cell cols="2">? Models with BERT</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">BERT+Finetune 84.30</cell><cell>77.45</cell><cell>81.85</cell><cell>90.24</cell></row><row><cell>DGEDT+BERT</cell><cell>85.99</cell><cell>78.88</cell><cell>82.95</cell><cell>90.94</cell></row><row><cell>RGAT+BERT</cell><cell>85.91</cell><cell>79.10</cell><cell>83.15</cell><cell>91.39</cell></row><row><cell cols="2">DualGCN+BERT 86.29</cell><cell>79.51</cell><cell>83.78</cell><cell>91.43</cell></row><row><cell>HGCN+BERT</cell><cell>87.41</cell><cell>81.49</cell><cell>85.61</cell><cell>93.02</cell></row><row><cell>(Ours)</cell><cell>(86.45)</cell><cell>(79.59)</cell><cell>(83.91)</cell><cell>(91.72)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Experimental results (%) of ablation study.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>. The first observa-</cell></row><row><cell>tion is that CGCN outperforms DGCN on Rest14 and Rest15,</cell></row><row><cell>while it fails to act as well as DGCN on the Lap14 and Rest16.</cell></row><row><cell>Moreover, it is beneficial to incorporate CGCN and DGCN</cell></row><row><cell>for achieving better performances. This indicates that aggre-</cell></row><row><cell>gate syntactic information from two complementary sides of</cell></row><row><cell>syntactic relations can improve the robustness of the model.</cell></row><row><cell>More importantly, HGCN w/o CRF means that we remove</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Convolution over hierarchical syntactic and lexical graphs for aspect level sentiment analysis. In EMNLP, pages 3540-3549, 2020. [Zhang et al., 2019] Chen Zhang, Qiuchi Li, and Dawei Song. Aspect-based sentiment classification with aspectspecific graph convolutional networks.</figDesc><table><row><cell>In EMNLP-</cell></row><row><cell>IJCNLP, pages 4568-4578, 2019.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that we regard non-terminal node above the part-of-speech (POS) level as constituent whereas the terminal node is the word.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE international conference on acoustics, speech and signal processing</title>
		<editor>Graves et al., 2013] Alex Graves, Abdel-rahman Mohamed, and Geoffrey Hinton</editor>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="6645" to="6649" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to detect opinion snippet for aspect-based sentiment analysis</title>
	</analytic>
	<monogr>
		<title level="m">CoNLL</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="970" to="979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Jointly learning aspect-focused and inter-aspect relations with graph convolutional networks for aspect sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Maria Pontiki, Dimitris Galanis, John Pavlopoulos, Harris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. Semeval-2014 task 4: Aspect based sentiment analysis. In SemEval</title>
		<editor>Mohammad Al-Smadi, Mahmoud Al-Ayyoub</editor>
		<meeting><address><addrLine>Bing Qin</addrLine></address></meeting>
		<imprint>
			<publisher>Yanyan Zhao</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="486" to="495" />
		</imprint>
	</monogr>
	<note>SemEval. Pontiki et al., 2016] Maria Pontiki, Dimitrios Galanis, Haris Papageorgiou, Ion Androutsopoulos. Suresh Manandhar. Orph?e De Clercq. et al.</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Aspect-level sentiment analysis via convolution over dependency tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schlichtkrull</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Semeval-2016 task 5: Aspect based sentiment analysis. In International workshop on semantic evaluation</title>
		<editor>Kai Sun, Richong Zhang, Samuel Mensah, Yongyi Mao, and Xudong Liu</editor>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5679" to="5688" />
		</imprint>
	</monogr>
	<note>EMNLP-IJCNLP</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dependency graph enhanced dualtransformer structure for aspect-based sentiment classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6578" to="6588" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Relational graph attention network for aspect-based sentiment analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">[</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<editor>Wang et al., 2020] Kai Wang, Weizhou Shen, Yunyi Yang, Xiaojun Quan, and Rui Wang</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3229" to="3238" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Aspect sentiment classification with aspectspecific opinion spans</title>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="3561" to="3567" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
