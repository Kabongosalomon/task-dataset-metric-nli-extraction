<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Virtual Event</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 20-24, 2021. October 20-24, 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>An</surname></persName>
							<email>dong.an@cripac.ia.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institution of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Future Technology</orgName>
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institution of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Adelaide</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institution of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center for Excellence in Brain Science and Intelligence Technology (CEBSIT)</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="institution" key="instit1">Chinese Academy of Sciences</orgName>
								<orgName type="institution" key="instit2">Artificial Intelligence Research (CAS-AIR)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Center for Research on Intelligent Perception and Computing</orgName>
								<orgName type="department" key="dep2">Institution of Automation</orgName>
								<orgName type="institution">Chinese Academy of Sciences</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Center for Excellence in Brain Science and Intelligence Technology (CEBSIT)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>An</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tieniu</forename><forename type="middle">Tan</forename></persName>
						</author>
						<title level="a" type="main">Virtual Event</title>
					</analytic>
					<monogr>
						<title level="m">China Proceedings of the 29th ACM International Conference on Multimedia (MM &apos;21)</title>
						<imprint>
							<date type="published">October 20-24, 2021. October 20-24, 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3474085.3475282</idno>
					<note>2021. Neighbor-view Enhanced Model for Vision and Language Navigation. In Event, China. ACM, New York, NY, USA, 9 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T02:40+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Computing methodologies ? Knowledge representation and reasoning;</term>
					<term>Information systems ? Information sys- tems applications KEYWORDS Vision and Language Navigation, Visual context modeling, Modular attention networks * Corresponding author</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Vision and Language Navigation (VLN) requires an agent to navigate to a target location by following natural language instructions. Most of existing works represent a navigation candidate by the feature of the corresponding single view where the candidate lies in. However, an instruction may mention landmarks out of the single view as references, which might lead to failures of textualvisual matching of existing methods. In this work, we propose a multi-module Neighbor-View Enhanced Model (NvEM) to adaptively incorporate visual contexts from neighbor views for better textualvisual matching. Specifically, our NvEM utilizes a subject module and a reference module to collect contexts from neighbor views. The subject module fuses neighbor views at a global level, and the reference module fuses neighbor objects at a local level. Subjects and references are adaptively determined via attention mechanisms. Our model also includes an action module to utilize the strong orientation guidance (e.g., "turn left") in instructions. Each module predicts navigation action separately and their weighted sum is used for predicting the final action. Extensive experimental results demonstrate the effectiveness of the proposed method on the R2R and R4R benchmarks against several state-of-the-art navigators, and NvEM even beats some pre-training ones. Our code is available at https://github.com/MarSaKi/NvEM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Vision and Language Navigation (VLN) has drawn increasing interest in recent years, partly because it represents a significant step towards enabling intelligent agents to interact with the realistic world. Running in a 3D simulator <ref type="bibr" target="#b0">[1]</ref> rendered with real-world images <ref type="bibr" target="#b1">[2]</ref>, the goal of VLN is to navigate to a target location by following a detailed natural language instruction, such as "Walk around the table and exit the room. Walk down the first set of stairs. Wait there. ". There are two kinds of simulators, which render continuous navigation trajectories <ref type="bibr" target="#b12">[13]</ref> and discrete trajectories <ref type="bibr" target="#b0">[1]</ref> respectively. In this paper we focus on the discrete one, where the agent navigates on a discrete graph (see <ref type="figure">Figure 1</ref> (a)).</p><p>A variety of approaches have been proposed to address the VLN problem <ref type="bibr">[3-5, 7, 8, 15-17, 19, 24, 28, 31-34]</ref>. Most of them adopt panoramic action space <ref type="bibr" target="#b3">[4]</ref>, where the agent select a navigable candidate from its observations to transport at each step. However, the context of navigable candidates is rarely discussed in existing works, and the commonly used single-view candidates are of limited visual contexts which may hamper the matching between instructions and the visual representations of candidates. <ref type="figure">Figure 1</ref> shows such an example, where there are three candidate "archways", each of which is represented by a single-view visual perception <ref type="figure">(Figure 1  (b)</ref>). According to the instruction, only the archway "to the left of mirror" leads to the correct navigation. However, most of existing agents may fail because they cannot find the referred "mirror" in any single-view-based candidate.</p><p>Thus, we propose to enhance the textual-visual matching by fusing visual information from candidates' neighbor views as shown in <ref type="figure">Figure 1 (c)</ref>, which is rarely explored before. It is non-trivial to fuse neighbor views for visual contexts modeling, because many unmentioned visual clues exist which may interfere with the agent's decision (e.g., the lamp in <ref type="figure">Figure 1 (c)</ref>). In addition, some instructions even do not involve visual clues in neighbor views, such as "go through the doorway". To handle this challenging problem, we propose to decompose an instruction into: action-, subject-and reference-related phrases as shown in <ref type="figure">Figure 2</ref>. Generally, the action and subject are necessary, and the optional reference helps to distinguish the desired candidate from other similar ones. Based on the above mentioned three types of instruction phrases, we further design a multi-module Neighbor-view Enhanced Model (NvEM) to adaptively fuse neighbor visual contexts in order to improve the textual-visual matching between instructions and candidates' visual perceptions. Specifically, our NvEM includes a subject module, a reference module and an action module, where subjects and references are determined via attention mechanisms. On one hand, the subject module aggregates neighbor views at a global level based on spatial information. On the other hand, the reference module aggregates related objects from neighbor views at a local level. The action module makes use of the orientation guidance (i.e., "turn left") in instructions. Each module predicts navigation action separately and their weighted sum is used to predict the final action. Note that the combination weights are trainable and predicted based on the decomposed subject-, reference-and action-related phrases.</p><p>The contributions of this work are summarized as follows:</p><p>? To improve the textual-visual matching between instructions and navigable candidates, we propose to take into account the visual contexts from neighbor views for the first time.</p><p>? We propose a subject module and a reference module to adaptively fuse visual contexts from neighbor views at both global level and local level.</p><p>? Extensive experimental results demonstrate the effectiveness of the proposed method with comparisons against several existing state-of-the-art methods, and NvEM even beats some pre-training ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Vision and Language Navigation. Numerous approaches have been proposed to address the VLN problem. Most of them are based on the CNN-LSTM architecture with attention mechanisms: at each time step, the agent first grounds surrounding observations to instructions, then chooses the most matched candidate according to the grounded instructions as the next location. Early work Speaker-Follower <ref type="bibr" target="#b3">[4]</ref> develops a speaker model to synthesize new instructions for randomly sampled trajectories. Additionally, they design a panoramic action space for efficient navigation. Later on, EnvDrop <ref type="bibr" target="#b28">[28]</ref> increases the diversity of synthetic data by randomly removing objects to generate "new environments".</p><p>On the other line, Self-monitoring <ref type="bibr" target="#b17">[17]</ref> and RCM <ref type="bibr" target="#b32">[32]</ref> utilize the cross-modality co-attention mechanism to enhance the alignment of instructions and trajectories. To learn generic linguistic and visual representations for VLN, AuxRN <ref type="bibr" target="#b37">[37]</ref> designs several auxiliary self-supervised losses. Very recently, large-scale pre-training models for VLN are widely explored <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b19">19]</ref>, where they improve the agent's generation abilities dramatically by benefitting from priors of other datasets. Different types of visual clues correspond to different phrases in an instruction, OAAM <ref type="bibr" target="#b24">[24]</ref> and RelGraph <ref type="bibr" target="#b6">[7]</ref> utilize decomposed phrases to guide more accurate action prediction. OAAM <ref type="bibr" target="#b24">[24]</ref> adopts action and object specialized clues to vote action at each time step, while RelGraph <ref type="bibr" target="#b6">[7]</ref> proposes a graph network to model the intra-and inter-relationships among the contextual and visual clues. The most relevant work to ours is RelGraph <ref type="bibr" target="#b6">[7]</ref>, where we both attempt to exploit view-level and object-level features. The key difference is: we focus on enhancing each candidate's representation with its multiple neighbor views (namely inter-view), while the representation in RelGraph is limited to a single view and thus it's intra-view. Modular Attention Networks. Modular networks are widely adopted in vision and language models. It attempts to decompose sentences into multiple phrases via attention mechanisms, as different phrases usually correspond to different visual clues. MAt-tNet <ref type="bibr" target="#b35">[35]</ref> decomposes a long sentence into appearance, location and relationship three parts for referring expression. LCGN <ref type="bibr" target="#b9">[10]</ref> utilizes a multi-step textual attention mechanism to extract different object-related phrases, then modeling objects' contexts via the relations among the phrases. LGI <ref type="bibr" target="#b21">[21]</ref> utilizes a sequential query attention module to decompose the query into multiple semantic phrases, then using these phrases to interact with video clips for video grounding. To the best of our knowledge, OAAM <ref type="bibr" target="#b24">[24]</ref> is the earliest attempt to decompose instructions in VLN. They decompose instructions into "action" and "object " specialized phrases, and use these phrases to vote next action. Our work has two key differences with OAAM: (I) Different modules: our modules are (subject, reference, action) v.s. (object, action) of OAAM. (II) Our subject module and reference module fuse information from neighbor views while OAAM only use information within one single view.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARY</head><p>In VLN <ref type="bibr" target="#b0">[1]</ref>, given a natural language instruction with words, an agent navigates on a discrete graph to reach the described target by following the instruction. At each time step , the agent observes a panorama which consists of 36 discrete views. Each view is represented by an image v , , with its orientation including heading , and elevation , . Also, there are candidates at time step , and each candidate is represented by the single view where the candidate lies in, with the candidate's relative orientation to the agent. Formally, for the -th view:</p><formula xml:id="formula_0">f , = [ResNet(v , ); ( , , , )]<label>(1)</label></formula><p>where ResNet(?) represents ResNet <ref type="bibr" target="#b5">[6]</ref> pooling features, (?) is an embedding function for heading and elevation, which repeats [cos , , sin , , cos , , sin , ] 32 times following <ref type="bibr" target="#b28">[28]</ref>. The -th candidate g , is encoded in the same way.</p><p>Previous works show that data augmentation is able to significantly improve the generalization ability in unseen environments <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">28]</ref>. We make use of this strategy by adopting EnvDrop <ref type="bibr" target="#b28">[28]</ref> as baseline, which first uses a bi-directional LSTM to encode the instruction, and the encoded instruction is represented as I = {u l } =1 . Then the agent's previous context-aware stateh ?1 is used to attend on all views to get scene feature:f = SoftAttn(h ?1 , {f , } 36 =1 ). The concatenation off and the previous action embedding a ?1 is fed into the decoder LSTM to update the agent's state: h = LSTM( [f ; a ?1 ],h ?1 ). Note that the context-aware agent state is updated via the attentive instruction feature u :</p><formula xml:id="formula_1">h = Tanh(W [u ; h ]), u = SoftAttn(h ?1 , {u } =1 )<label>(2)</label></formula><p>where W is a trainable linear projection and SoftAttn(?) mentioned above denotes soft-dot attention. Finally, EnvDrop predicts navigation action by selecting the candidate with the highest probability (W is a trainable linear projection):</p><formula xml:id="formula_2">a * = arg max (g , ), (g , ) = softmax (g ? , Wh ) (3)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">METHODOLOGY</head><p>In this section, we first briefly describe the pipeline of the proposed model in training phase, then we detail the proposed Neighbor-view Enhanced Model (NvEM). Note that in this section, we omit the time step to avoid notational clutter in the exposition. <ref type="figure" target="#fig_1">Figure 3</ref> illustrates the main pipeline of our NvEM. First, action-, subject-and reference-related phrases are attended by three independent attention schemes. Then, reference module and subject module predict navigation actions via aggregating visual contexts from candidates' neighbor views. The action module predicts navigation in terms of orientation information. Lastly, the final navigation action is determined by combining these three predictions together with weights generated from phrases' embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Phrase Extractor</head><p>Considering an instruction, such as "walk through the archway to the left of the mirror...", there are three types of phrases which the agent needs to identify: the action describes the orientation of target candidate (e.g., "walk through"), the subject describes the main visual entity of the correct navigation (e.g., "the archway") and the reference which is referenced by the subject (e.g., "to the left of the mirror"). Thus, NvEM first performs three soft-attentions independently on the instruction, conditioned on the current agent state h , to attend on these three types of phrases:</p><formula xml:id="formula_3">h = Tanh(W [u ; h ]), u = SoftAttn (h ?1 , {u } =1 ) h = Tanh(W [u ; h ]), u = SoftAttn (h ?1 , {u } =1 ) h = Tanh(W [u ; h ]), u = SoftAttn (h ?1 , {u } =1 )<label>(4)</label></formula><p>where the subscripts denote the corresponding types of phrases, SoftAttn * (?) is the same with Eq (2). u * andh * denote features of corresponding phrases and context-aware agent states, and they are updated by different linear projections W * . The global contextaware agent stateh in Eq <ref type="formula" target="#formula_1">(2)</ref> is now calculated by averaging the three specialized context-aware statesh ,h andh .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Neighbor-view Enhanced Navigator</head><p>Corresponding to the attended three types of phrases in instructions, our neighbor-view enhanced navigator contains three modules: a reference module, a subject module and an action module. The reference module and subject module predict navigation actions via aggregating visual contexts from neighbor views at local and global levels, respectively. The action module predicts navigation actions according to orientation information. We provide the details below.</p><p>Reference Module. Reference usually exists as a landmark surrounding the subject to clarify similar navigation candidates. In the example "walk through the archway to the left of the mirror...", the reference is "the mirror" and it is referred with a spatial relationship to the subject (e.g., "to the left of"). This motivates us to enhance the representation of a navigation candidate by using features of local objects and their spatial relations. Formally, for the -th candidate g , given its orientation d , the objects' features {o , , } =1 extracted by <ref type="bibr" target="#b26">[26]</ref> and their orientations {d , , } =1 in the candidate's neighbor views (assume there are objects in each neighbor and denotes the -th neighbor). We then calculate the spatial relations of the neighbor objects to the -th candidate:  where (?) is the same as that in Eq <ref type="bibr" target="#b0">(1)</ref>. Then each neighbor object is represented as the concatenation of its object feature and relative spatial embedding:</p><formula xml:id="formula_4">e , , = (d , , ? d )<label>(5)</label></formula><formula xml:id="formula_5">o , , = ? ([o , , ; e , , ])<label>(6)</label></formula><p>where [:] denotes concatenation, ? (?) projects objects' features into a D dimensional space and all ?(?) * in this section represent trainable non-linear projections, with Tanh(?) as the activation function. We use the reference-related phrases u to highlight relevant objects in neighbor views:</p><formula xml:id="formula_6">A , , = softmax , ( (W ? u ) ? (W ?? , , ) ?? D ) F = ?? , A , , ?? , ,<label>(7)</label></formula><p>where W and W are trainable linear projections. The reference module predicts the confidence of the candidate g being the next navigation action using reference-related stateh and the neighbor reference enhanced candidate representation F :</p><formula xml:id="formula_7">, =h ? W F<label>(8)</label></formula><p>where W is a trainable linear projection.</p><p>Subject Module. Instruction subject describes the main visual entity of the correct navigation, such as "the archway" in "walk through the archway to the left of the mirror...". However, sometimes there are multiple candidates containing the different instances of the subject. To alleviate the ambiguity in visual side, we propose to enhance the visual representation of subject by incorporating contexts from neighbor views. Specifically, we aggregate neighbor views at a global level, with the help of the spatial affinities neighbor views to the candidate. Formally, for the -th candidate g , given its orientation d , its neighbor views' orientations {d , } =1 and ResNet features {v , } =1 (assume it has neighbor views), we first embed all neighbor views using a trainable non-linear projection:</p><formula xml:id="formula_8">v , = ? (v , )<label>(9)</label></formula><p>Then we compute the spatial affinities among the candidate and its neighbor views based on their orientations in a query-key manner <ref type="bibr" target="#b30">[30]</ref>. Then the enhanced subject visual representation F is obtained by adaptively aggregating neighbor views' embeddings:</p><formula xml:id="formula_9">A , = softmax ( (W ? d ) ? (W ? d , ) ?? D ) F = ?? A , ?? ,<label>(10)</label></formula><p>where W and W are trainable linear projections. Similar to reference module, the subject module predicts the confidence of candidate g being the next navigation action via:</p><formula xml:id="formula_10">, =h ? W F<label>(11)</label></formula><p>where W is a trainable linear projection.</p><p>Action Module. Action related phrases serve as strong guidance in navigation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">24]</ref>, such as "go forward", "turn left" and "go down". Inspired by <ref type="bibr" target="#b24">[24]</ref>, for the -th candidate g , the action module predicts the confidence of candidate g being the next navigation action, using the candidate's orientation d and action-related stateh :</p><formula xml:id="formula_11">F =? ( (d )) , =h ? W F<label>(12)</label></formula><p>where W is a trainable linear projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Adaptive Action Integrator</head><p>The action, subject and reference modules usually contribute at different degrees to the final decision. Thus, we propose to adaptively integrate predictions from these three modules. We first calculate the combination weights conditioned on action-, subject-and reference-specialized phrases u , u and u . Take the action weight w as an example:</p><formula xml:id="formula_12">w = (W ? u ) (W ? u ) + (W ? u ) + (W ? u )<label>(13)</label></formula><p>where the W , W and W are trainable linear projections. Then the final action probability of the -th candidate is calculated by weighted summation of above confidences:</p><formula xml:id="formula_13">P = softmax (w ? , + w ? , + w ? , )<label>(14)</label></formula><p>In the inference phase, the agent selects the candidate with the maximum probability as shown in Eq (3) at each step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training</head><p>We apply the Imitation Learning (IL) + Reinforcement Learning (RL) objectives to train our model following <ref type="bibr" target="#b28">[28]</ref>. In imitation learning, the agent takes the teacher action a * at each time step to learn to follow the ground-truth trajectory. In reinforcement learning, the agent samples an action a via the probability P and learns from the rewards. Formally:</p><formula xml:id="formula_14">L = T * ?? =1 ?a * log(P ) + T ?? =1 ?a log(P )A<label>(15)</label></formula><p>where is a coefficient for weighting the IL loss, T * and T are the total numbers of steps the agent takes in IL and RL respectively. A is the advantage in A2C algorithm <ref type="bibr" target="#b20">[20]</ref>. We apply the summation of two types of rewards in RL objective, goal reward and fidelity reward following <ref type="bibr" target="#b11">[12]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we first describe the commonly used VLN datasets and the evaluation metrics. Then we present implementation details of NvEM. Finally, we compare against several state-of-the-art methods and provide ablation experiments. Qualitative visualizations are also presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Datasets and Evaluation Metrics</head><p>R2R benchmark. The Room-to-Room (R2R) dataset <ref type="bibr" target="#b0">[1]</ref> consists of 10,567 panoramic view nodes in 90 real-world environments as well as 7,189 trajectories described by three natural language instructions. The dataset is split into train, validation seen, validation unseen and test unseen sets. We follow the standard metrics employed by previous works to evaluate the performance of our agent. These metrics include: the Trajectory Length (TL) which measures the average length of the agent's navigation path, the Navigation Error (NE) which is the average distance between the agent's final location and the target, the Success Rate (SR) which measures the ratio of trajectories where the agent stops at 3 meters within the target, and the Success Rate weighted by Path Length (SPL) which considers both path length and success rate. Note that the SR and SPL in unseen environments are main metrics for R2R. R4R benchmark. The Room-for-Room (R4R) dataset <ref type="bibr" target="#b11">[12]</ref> is an extended version of R2R, which has longer instructions and trajectories. The dataset is split into train, validation seen, validation unseen sets. Besides the main metrics in R2R, R4R includes additional metrics: the Coverage Weighted by Length Score (CLS) <ref type="bibr" target="#b11">[12]</ref>, the Normalized Dynamic Time Wrapping (nDTW) <ref type="bibr" target="#b10">[11]</ref> and the nDTW weighted by Success Rate (sDTW) <ref type="bibr" target="#b10">[11]</ref>. In R4R, SR and SPL measure the accuracy of navigation, while CLS, nDTW and sDTW measure the fidelity of predicted paths and ground-truth paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Implementation Details</head><p>We use ResNet-152 <ref type="bibr" target="#b5">[6]</ref> pre-trained on Places365 <ref type="bibr" target="#b36">[36]</ref> to extract view features. We apply Faster-RCNN <ref type="bibr" target="#b26">[26]</ref> pre-trained on the Visual Genome Dataset <ref type="bibr" target="#b13">[14]</ref> to obtain object labels in reference module, then encoded by Glove <ref type="bibr" target="#b23">[23]</ref>. To simplify the object vocabulary, we retain the top 100 most frequent classes mentioned in R2R training data following <ref type="bibr" target="#b6">[7]</ref>. Our method exploits neighbor views to represent a navigation candidate, thus the number of neighbors and objects are crucial for the final performance. Our default setting adopts 4 neighbor views and the top 8 detected objects in each neighbor view (we also tried other settings, please see Sec 5.4). For simplicity, the objects' positions are roughly represented by their corresponding views' orientations in Eq <ref type="bibr" target="#b4">(5)</ref>. This is reasonable as instructions usually mention approximate relative relations of objects and candidates. Our model is trained with the widely used two-stage strategy <ref type="bibr" target="#b28">[28]</ref>. At the first stage, only real training data is used. At the second stage, we pick the model with the highest SR at the first stage, and keep training it with both real data and synthetic data generated from <ref type="bibr" target="#b3">[4]</ref>. As for R4R experiment, we only apply the first stage training. We set the projection dimension D in Sec 4.3 as 512, the glove embedding dimension as 300, and train the agent using RMSprop optimizer <ref type="bibr" target="#b27">[27]</ref> with 1 ? 10 ?4 learning rate. We train the first stage for 80,000 iterations and the second stage will continue training up to 200,000 iterations. All our experiments are conducted on a Nvidia V100 Tensor Core GPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with the State-of-The-Art</head><p>We compare NvEM with several SoTA methods under the singlerun setting on both R2R and R4R benchmarks. Note that VLN mainly focus on the agent's performance on unseen splits, so the performance we report is based on the model which has the highest SR on the validation unseen split.</p><p>As shown in <ref type="table" target="#tab_1">Table 1</ref>, on the R2R benchmark our NvEM outperforms the baseline EnvDrop <ref type="bibr" target="#b28">[28]</ref> by a large margin, which obtains 7% absolute improvements in terms of SPL on both Val Unseen and Test splits. Compared to the state-of-the-art method RelGraph <ref type="bibr" target="#b6">[7]</ref>, our model obtains 2% absolute improvements on Val Unseen and Test splits. Moreover, NvEM even beats some pre-training methods, such as PRESS <ref type="bibr" target="#b16">[16]</ref>, PREVALENT <ref type="bibr" target="#b4">[5]</ref>, and VLNBert (init. OS-CAR) <ref type="bibr" target="#b7">[8]</ref>. We also note that our method does not surpass VLNBert pre-trained on PREVALENT, which uses in-domain data for pretraining. As shown later in the ablation study, the success of our model mainly benefits from the incorporating of visual contexts,  where NvEM effectively improve the textual-visual matching leading to more accurate actions. On the R4R benchmark, we observe similar phenomenon with that on R2R. As shown in <ref type="table" target="#tab_2">Table 2</ref>, NvEM not only significantly outperforms the baseline EnvDrop <ref type="bibr" target="#b28">[28]</ref> with 10% absolute improvement, but also sets the new SoTA. In particular, NvEM achieves 0.41 CLS, 0.36 nDTW and 0.20 sDTW, which are largely higher than the second best RelGraph <ref type="bibr" target="#b6">[7]</ref> by 4%, 4% and %2, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>We conduct ablation experiments over different components of NvEM on R2R dataset. Specifically, we study how the action-, subject-and reference-module contribute to navigation. Then we compare the single-view subject module against the neighbor-view one. Lastly, we study how the numbers of neighbor views and objects in the subject-and reference-module affect the performance. All our ablation models are trained from scratch by two stage training.</p><p>The importance of different modules. Our full model utilizes three modules: the action-, subject-and reference-module, which correspond to orientations, global views and local objects. To study how they affect the performance of navigation, we conduct an ablation experiment by removing corresponding module. The results are shown in <ref type="table" target="#tab_3">Table 3</ref>. In model #1, we remove the action-module, and it achieves the worst performance compared to others in terms of the main metric SPL. This indicates that orientations are strong guidance for navigation. The same phenomenon is observed in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b24">24]</ref>. In model #2, we remove the subject-module, and it performs slight better than model#1 but still far behind the full model. This indicates the global view information is important for VLN. In model #3, we remove the reference-module, and it performs slight worse than the full model. This indicates reference contains some useful information but not as important as subject. Another reason is that the local reference can also be included in global views of the subject-module.  <ref type="formula" target="#formula_0">(10)</ref>, while spa is our default setting which is based on spatial affinity. The results show that both model #2 and #3 perform better than the single view based model #1, which indicates the superior of neighbor-view based models. Moreover, we observe that language grounded neighbor-view fusion (model #2) performs worse than the spatial based one (model #3). This may be caused by the gap between language embeddings and visual embeddings. The reason may be that more neighbors contain more redundant information. Not only will they affect the aggregation for subject context, but also increase the difficulty to highlight relevant objects. An example is shown in <ref type="figure" target="#fig_2">Figure 4</ref>, intuitively, 8 neighbors have more redundant visual information than 4 neighbors one.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Qualitative Visualizations</head><p>Here we present some success and failure cases. <ref type="figure" target="#fig_3">Figure 5</ref> shows two success cases. Taking the bottom one as an example, each module attends on correct phrases, such as "go up", "stairs" and "to the left of the refrigerator" corresponding to action, subject and reference modules respectively. More visual information of the subject "stairs" from neighbor views are incorporated by our subject-module. In addition, taking advantage of the reference-module, our model could perceive the mentioned "refrigerator" in neighbor views as reference, and suppress unmentioned ones. We note that the attention scores of objects are not that high, which may mainly caused by the fact that we consider totally 32 objects for each candidate (4 neighbors and 8 objects in each neighbor). <ref type="figure">Figure 6</ref> visualizes a failure case. The model mainly focus on the "bathroom" in the subject-module, which leads to a wrong direction. This indicates that the attentive phrases of NvEM are not always correct, and thus it still could be improved by studying how to extract more accurate phrases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we present a novel multi-module Neighbor-view Enhanced Model to improve textual-visual matching via adaptively incorporating visual information from neighbor views. Our subject module aggregates neighbor views at a global level based on spatial-affinity, and our reference module aggregates neighbor objects at a local level guided by referring phrases. Extensive experiments demonstrate that NvEM effectively improves the agent's performance in unseen environments and our method sets the new state-of-the-art.</p><p>Considering the similarity between R2R task and other VLN tasks, such as dialogue navigation <ref type="bibr" target="#b22">[22,</ref><ref type="bibr" target="#b29">29]</ref>, remote object detection <ref type="bibr" target="#b25">[25]</ref> and navigation in continuous space <ref type="bibr" target="#b12">[13]</ref>, we believe the  <ref type="figure">Figure 6</ref>: Visualization of a failure case. Green arrow denotes ground-truth action, while red arrow denotes predicted action. The mask on the panorama denotes neighbor-view spatial affinities of the predicted candidate in subject module (note that neighbor views overlap with the center view). The boxes are objects with top three scores in reference module. Attentions on three types of phrases are also showed.</p><p>neighbor-view enhancement idea could also benefit agents in other embodied AI tasks. We leave them as future works to explore.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>(a) Illustration of panorama view and navigation graph in VLN tasks. The yellow node is the current location. Candidate nodes are blue and green. Note that the green one is the ground-truth. (b) Single-view-based candidates used in previous works, which contain limited visual contexts and might be insufficient for action prediction. (c) Our neighbor-view scheme enriches the visual context of each candidate, leading to better textual-visual matching. Numbers above each view denote (heading, elevation).Walk through the archway to the left of the mirror?Go down the hallway?Walk past the piano to the oval door opening? : Examples of action-, subject-and reference-related phrases in instructions. Some instructions do not contain references.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Main architecture of the proposed multi-module Neighbor-view Enhanced Model (NvEM). First, action-, subject-and reference-related phrases are attended via an attention mechanism (Section 4.2). Then, reference module and subject module predict navigation actions via aggregating visual contexts from candidates' neighbor views at local and global levels. The action module predicts navigation in terms of orientation information. (Section 4.3). Lastly, the weighted sum of all three predictions predicts the final navigation decision. (Section 4.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>A schematic diagram of 4 neighbors and 8 neighbors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Visualizations of two success cases. The mask on the panorama denotes neighbor-view spatial affinities of the predicted candidate in subject module (note that neighbor views overlap with the center view). The boxes are objects with top three scores in reference module. Attentions on three types of phrases are also showed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Walk through the archway to the left of the mirror? Walk through the archway to the left of the mirror?Phrase Query Spatial Affinity Tanh Tanh Tanh Action module Subject module Reference module</head><label></label><figDesc></figDesc><table><row><cell cols="2">Modular phrases</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Module weights</cell></row><row><cell cols="2">[ , , r ]</cell><cell></cell><cell>arch</cell><cell></cell><cell>? r</cell><cell>[w , w s , w r ]</cell></row><row><cell></cell><cell></cell><cell>lamp</cell><cell></cell><cell></cell><cell></cell></row><row><cell>a</cell><cell>r</cell><cell>Faster-RCNN</cell><cell>left of</cell><cell>r</cell><cell>Classifier</cell></row><row><cell></cell><cell></cell><cell></cell><cell>mirror</cell><cell></cell><cell></cell><cell>IL</cell><cell>RL</cell></row><row><cell></cell><cell></cell><cell></cell><cell>hallway</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell></row><row><cell></cell><cell></cell><cell>ResNet-</cell><cell></cell><cell></cell><cell>Classifier</cell><cell>softmax</cell></row><row><cell></cell><cell></cell><cell>152</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Agent?</cell><cell>,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>heading elevation</cell><cell></cell><cell></cell><cell>Classifier</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison of single-run performance with the state-of-the-art methods on R2R. ? denotes works that apply pretrained textual or visual encoders.</figDesc><table><row><cell></cell><cell></cell><cell>Val Seen</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Val Unseen</cell><cell></cell><cell></cell><cell>Test Unseen</cell></row><row><cell>Agent</cell><cell>TL</cell><cell cols="3">NE? SR? SPL?</cell><cell>TL</cell><cell cols="3">NE? SR? SPL?</cell><cell>TL</cell><cell>NE? SR? SPL?</cell></row><row><cell>Random</cell><cell cols="3">9.58 9.45 0.16</cell><cell>-</cell><cell cols="3">9.77 9.23 0.16</cell><cell>-</cell><cell cols="2">9.89 9.79 0.13</cell><cell>0.12</cell></row><row><cell>Human</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell cols="3">-11.85 1.61 0.86</cell><cell>0.76</cell></row><row><cell>PRESS [16]  ?</cell><cell cols="3">10.57 4.39 0.58</cell><cell cols="4">0.55 10.36 5.28 0.49</cell><cell cols="3">0.45 10.77 5.49 0.49</cell><cell>0.45</cell></row><row><cell>PREVALENT [5]  ?</cell><cell cols="3">10.32 3.67 0.69</cell><cell cols="4">0.65 10.19 4.71 0.58</cell><cell cols="3">0.53 10.51 5.30 0.54</cell><cell>0.51</cell></row><row><cell>VLNBert (init. OSCAR) [8] ?</cell><cell cols="3">10.79 3.11 0.71</cell><cell cols="4">0.67 11.86 4.29 0.59</cell><cell cols="3">0.53 12.34 4.59 0.57</cell><cell>0.53</cell></row><row><cell cols="4">VLNBert (init. PREVALENT) [8] ? 11.13 2.90 0.72</cell><cell cols="4">0.68 12.01 3.93 0.63</cell><cell cols="3">0.57 12.35 4.09 0.63</cell><cell>0.57</cell></row><row><cell>Seq2Seq [1]</cell><cell cols="3">11.33 6.01 0.39</cell><cell>-</cell><cell cols="3">8.39 7.81 0.22</cell><cell>-</cell><cell cols="2">8.13 7.85 0.20</cell><cell>0.18</cell></row><row><cell>Speaker-Follower [4]</cell><cell cols="3">-3.36 0.66</cell><cell>-</cell><cell cols="3">-6.62 0.35</cell><cell cols="3">-14.82 6.62 0.35</cell><cell>0.28</cell></row><row><cell>SM [17]</cell><cell cols="3">-3.22 0.67</cell><cell>0.58</cell><cell cols="3">-5.52 0.45</cell><cell cols="3">0.32 18.04 5.67 0.48</cell><cell>0.35</cell></row><row><cell>RCM+SIL [32]</cell><cell cols="3">10.65 3.53 0.67</cell><cell cols="4">-11.46 6.09 0.43</cell><cell cols="3">-11.97 6.12 0.43</cell><cell>0.38</cell></row><row><cell>Regretful [18]</cell><cell cols="3">-3.23 0.69</cell><cell>0.63</cell><cell cols="3">-5.32 0.50</cell><cell cols="3">0.41 13.69 5.69 0.48</cell><cell>0.40</cell></row><row><cell>VLNBert (no init.) [8]</cell><cell cols="3">9.78 3.92 0.62</cell><cell cols="4">0.59 10.31 5.10 0.50</cell><cell cols="3">0.46 11.15 5.45 0.51</cell><cell>0.47</cell></row><row><cell>EnvDrop [28]</cell><cell cols="3">11.00 3.99 0.62</cell><cell cols="4">0.59 10.70 5.22 0.52</cell><cell cols="3">0.48 11.66 5.23 0.51</cell><cell>0.47</cell></row><row><cell>AuxRN [37]</cell><cell cols="4">-3.33 0.70 0.67</cell><cell cols="3">-5.28 0.55</cell><cell>0.50</cell><cell></cell><cell>-5.15 0.55</cell><cell>0.51</cell></row><row><cell>RelGraph [7]</cell><cell cols="3">10.13 3.47 0.67</cell><cell>0.65</cell><cell cols="3">9.99 4.73 0.57</cell><cell cols="3">0.53 10.29 4.75 0.55</cell><cell>0.52</cell></row><row><cell>NvEM (ours)</cell><cell cols="3">11.09 3.44 0.69</cell><cell cols="7">0.65 11.83 4.27 0.60 0.55 12.98 4.37 0.58 0.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Comparison of single-run performance with the state-of-the-art methods on R4R. goal and fidelity indicate goal and fidelity reward in reinforcement learning. ? denotes our reimplemented R4R results.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Val Seen</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Val Unseen</cell><cell></cell><cell></cell></row><row><cell>Agent</cell><cell cols="11">NE? SR? SPL? CLS? nDTW? sDTW? NE? SR? SPL? CLS? nDTW? sDTW?</cell></row><row><cell>EnvDrop [28]</cell><cell cols="2">-0.52</cell><cell>0.41</cell><cell>0.53</cell><cell>-</cell><cell>0.27</cell><cell>-0.29</cell><cell>0.18</cell><cell>0.34</cell><cell>-</cell><cell>0.09</cell></row><row><cell>RCM-a (goal) [12]</cell><cell cols="2">5.11 0.56</cell><cell>0.32</cell><cell>0.40</cell><cell>-</cell><cell cols="2">-8.45 0.29</cell><cell>0.10</cell><cell>0.20</cell><cell>-</cell><cell>-</cell></row><row><cell cols="3">RCM-a (fidelity) [12] 5.37 0.53</cell><cell cols="2">0.31 0.55</cell><cell>-</cell><cell cols="2">-8.08 0.26</cell><cell>0.08</cell><cell>0.35</cell><cell>-</cell><cell>-</cell></row><row><cell>RCM-b (goal) [11]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-0.29</cell><cell>0.15</cell><cell>0.33</cell><cell>0.27</cell><cell>0.11</cell></row><row><cell>RCM-b (fidelity) [11]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-0.29</cell><cell>0.21</cell><cell>0.35</cell><cell>0.30</cell><cell>0.13</cell></row><row><cell>OAAM [24]</cell><cell cols="2">-0.56</cell><cell>0.49</cell><cell>0.54</cell><cell>-</cell><cell>0.32</cell><cell>-0.31</cell><cell>0.23</cell><cell>0.40</cell><cell>-</cell><cell>0.11</cell></row><row><cell>RelGraph [7] ?</cell><cell cols="3">5.14 0.55 0.50</cell><cell>0.51</cell><cell>0.48</cell><cell cols="2">0.35 7.55 0.35</cell><cell>0.25</cell><cell>0.37</cell><cell>0.32</cell><cell>0.18</cell></row><row><cell>NvEM (ours)</cell><cell cols="2">5.38 0.54</cell><cell>0.47</cell><cell>0.51</cell><cell>0.48</cell><cell cols="4">0.35 6.85 0.38 0.28 0.41</cell><cell>0.36</cell><cell>0.20</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Ablation experiment about importance of different modules. To answer the above questions, we test different types of subject-modules, and the results are shown inTable 4. Note that we remove the reference module to exclude the affect from reference. In table 4, single uses single-view global features to represent subject, lang uses subject-aware phrases u as query and view features as keys to ground views in Eq</figDesc><table><row><cell></cell><cell></cell><cell>Modules</cell><cell></cell><cell cols="2">Val Seen</cell><cell>Val Unseen</cell></row><row><cell cols="4">model action subject reference</cell><cell>SR</cell><cell>SPL</cell><cell>SR</cell><cell>SPL</cell></row><row><cell>1</cell><cell></cell><cell>?</cell><cell>?</cell><cell cols="2">0.623 0.584 0.486 0.440</cell></row><row><cell>2</cell><cell>?</cell><cell></cell><cell>?</cell><cell cols="2">0.580 0.523 0.504 0.446</cell></row><row><cell>3</cell><cell>?</cell><cell>?</cell><cell></cell><cell cols="2">0.666 0.636 0.579 0.539</cell></row><row><cell>4</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="2">0.686 0.645 0.601 0.549</cell></row><row><cell cols="6">Single-view subject vs neighbor-view subject. In the subject-</cell></row><row><cell cols="6">module, we aggregate neighbor view features based on their spatial</cell></row><row><cell cols="6">affinities to the candidate, which raises the following questions: is</cell></row><row><cell cols="6">neighbor-view more effective than single-view? How about aggre-</cell></row><row><cell cols="4">gating views in other manners?</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Different types of subject-modules, single denotes single-view, lang denotes aggregating views using language as query while spa denotes using spatial affinity.The analysis of number of neighbors and objects. In our default setting, the subject-module uses 4 neighbor views and the reference-module uses 8 objects in each neighbor view. However, it is naturally to consider more neighbors (e.g., 8 neighbors) and more objects. In this experiment, we test 4, 8 neighbors, and 4, 8, 12 objects in each view. The results are shown inTable 5. In model #1 and model #2, we adjust the number of objects and keep 4 neighbors. They perform worse than our default setting (model #4). The reason may be that fewer objects might not contain the objects mentioned by instructions, and more objects could be redundant. To study the number of neighbors, we keep the object number as 8. Comparing model #3 with model #4, the results show 4 neighbors is the better.</figDesc><table><row><cell></cell><cell>Subject</cell><cell cols="2">Val Seen</cell><cell cols="2">Val Unseen</cell></row><row><cell cols="2">model single lang spa</cell><cell>SR</cell><cell>SPL</cell><cell>SR</cell><cell>SPL</cell></row><row><cell>1</cell><cell>?</cell><cell cols="4">0.641 0.605 0.556 0.507</cell></row><row><cell>2</cell><cell>?</cell><cell cols="4">0.637 0.608 0.564 0.522</cell></row><row><cell>3</cell><cell cols="5">? 0.666 0.636 0.579 0.539</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Ablation experiments about the number of neighbor views and objects. Views denote the number of neighbors, and Objects denote the number of objects in a view.</figDesc><table><row><cell></cell><cell>Views</cell><cell>Objects</cell><cell cols="2">Val Seen</cell><cell>Val Unseen</cell></row><row><cell cols="3">model 4 8 4 8 12</cell><cell>SR</cell><cell>SPL</cell><cell>SR</cell><cell>SPL</cell></row><row><cell>1</cell><cell>?</cell><cell cols="4">? 0.655 0.617 0.564 0.517</cell></row><row><cell>2</cell><cell>?</cell><cell>?</cell><cell cols="3">0.667 0.638 0.572 0.532</cell></row><row><cell>3</cell><cell>?</cell><cell>?</cell><cell cols="3">0.644 0.615 0.564 0.523</cell></row><row><cell>4</cell><cell>?</cell><cell>?</cell><cell cols="3">0.686 0.645 0.601 0.549</cell></row><row><cell></cell><cell cols="2">4 neighbors:</cell><cell></cell><cell cols="2">8 neighbors:</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vision-andlanguage navigation: Interpreting visually-grounded navigation instructions in real environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niko</forename><surname>S?nderhauf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3674" to="3683" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Niessner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<title level="m">Matterport3D: Learning from RGB-D Data in Indoor Environments. International Conference on 3D Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Evolving Graphical Planner: Contextual Global Planning for Vision-and-Language Navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Narasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<imprint>
			<date type="published" when="2020-12-06" />
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
	<note>virtual</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Speaker-Follower Models for Vision-and-Language Navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cirik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3318" to="3329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards Learning a Generic Agent for Vision-and-Language Navigation via Pre-Training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weituo</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="13134" to="13143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Language and Visual Entity Relationship Graph for Agent Navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicong</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Rodriguez Opazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A Recurrent Vision-and-Language BERT for Navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yicong</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cristian</forename><surname>Rodriguez Opazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
		<idno>abs/2011.13922</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are You Looking? Grounding to Multiple Modalities in Visionand-Language Navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Fried</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6551" to="6557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language-Conditioned Graph Networks for Relational Reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronghang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="10293" to="10302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">General Evaluation for Instruction Conditioned Navigation using Dynamic Time Warping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visually Grounded Interaction and Language (ViGIL)</title>
		<meeting><address><addrLine>NeurIPS</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Workshop</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Stay on the Path: Instruction Fidelity in Vision-and-Language Navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Magalh?es</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Baldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1862" to="1872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Beyond the Nav-Graph: Vision-and-Language Navigation in Continuous Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Krantz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Wijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="104" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Embodied Vision-and-Language Navigation with Dynamic Convolutional Filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Landi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Corsini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rita</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th British Machine Vision Conference 2019, BMVC 2019</title>
		<meeting><address><addrLine>Cardiff, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Robust Navigation with Language Pretraining and Stochastic Sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaolin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1494" to="1499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Self-Monitoring Navigation Agent via Auxiliary Progress Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The Regretful Agent: Heuristic-Aided Navigation Through Progress Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuxuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ghassan</forename><surname>Alregib</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6732" to="6740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving Vision-and-Language Navigation with Image-Text Pairs from the Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Majumdar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayush</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="259" to="274" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Asynchronous Methods for Deep Reinforcement Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adri?</forename><forename type="middle">Puigdom?nech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33nd International Conference on Machine Learning, ICML 2016</title>
		<meeting>the 33nd International Conference on Machine Learning, ICML 2016</meeting>
		<imprint>
			<date type="published" when="1928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Local-Global Video-Text Interactions for Temporal Grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonghwan</forename><surname>Mun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10807" to="10816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Help, Anna! Visual Navigation with Natural Multimodal Assistance via Retrospective Curiosity-Encouraging Imitation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Khanh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hal</forename><surname>Daum?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="684" to="695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anton van den Hengel, and Qi Wu. 2020. Object-and-Action Aware Model for Visual Language Navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zizheng</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengping</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<imprint>
			<biblScope unit="page" from="303" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuankai</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="9979" to="9988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><forename type="middle">B</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">An overview of gradient descent optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<idno>abs/1609.04747</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Hao Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bansal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2610" to="2621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Vision-and-Dialog Navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Thomason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><surname>Cakmak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd Annual Conference on Robot Learning</title>
		<meeting><address><addrLine>CoRL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="394" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is All you Need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Soft Expert Reward Learning for Vision-and-Language Navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-08-23" />
			<biblScope unit="page" from="126" to="141" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IX</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiuyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinghan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan-Fang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6629" to="6638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement Learning for Planned-Ahead Vision-and-Language Navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018 -15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-09-08" />
			<biblScope unit="page" from="38" to="55" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XVI</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Environment-Agnostic Multitask Learning for Natural Language Grounded Navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vihan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Yang</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zornitsa</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sujith</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ravi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020 -16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-08-23" />
			<biblScope unit="page" from="413" to="430" />
		</imprint>
	</monogr>
	<note>Proceedings, Part XXIV</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">MAttNet: Modular Attention Network for Referring Expression Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1307" to="1315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Places: A 10 Million Image Database for Scene Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?gata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Vision-Language Navigation With Self-Supervised Auxiliary Reasoning Tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fengda</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10009" to="10019" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
