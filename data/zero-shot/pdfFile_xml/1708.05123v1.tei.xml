<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep &amp; Cross Network for Ad Click Predictions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date type="published" when="2017-08-17">17 Aug 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University Stanford</orgName>
								<address>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Fu</surname></persName>
							<email>binfu@google.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Fu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingliang</forename><surname>Wang</surname></persName>
							<email>mlwang@google.com</email>
							<affiliation key="aff3">
								<orgName type="institution">Google Inc</orgName>
								<address>
									<settlement>New York</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep &amp; Cross Network for Ad Click Predictions</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-08-17">17 Aug 2017</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Feature engineering has been the key to the success of many prediction models. However, the process is nontrivial and o en requires manual feature engineering or exhaustive searching. DNNs are able to automatically learn feature interactions; however, they generate all the interactions implicitly, and are not necessarily efficient in learning all types of cross features. In this paper, we propose the Deep &amp; Cross Network (DCN) which keeps the benefits of a DNN model, and beyond that, it introduces a novel cross network that is more efficient in learning certain bounded-degree feature interactions. In particular, DCN explicitly applies feature crossing at each layer, requires no manual feature engineering, and adds negligible extra complexity to the DNN model. Our experimental results have demonstrated its superiority over the state-of-art algorithms on the CTR prediction dataset and dense classification dataset, in terms of both model accuracy and memory usage.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Click-through rate (CTR) prediction is a large-scale problem that is essential to multi-billion dollar online advertising industry. In the advertising industry, advertisers pay publishers to display their ads on publishers' sites. One popular payment model is the cost-perclick (CPC) model, where advertisers are charged only when a click occurs. As a consequence, a publisher's revenue relies heavily on the ability to predict CTR accurately.</p><p>Identifying frequently predictive features and at the same time exploring unseen or rare cross features is the key to making good predictions. However, data for Web-scale recommender systems is mostly discrete and categorical, leading to a large and sparse feature space that is challenging for feature exploration. is has limited most large-scale systems to linear models such as logistic regression.</p><p>Linear models <ref type="bibr" target="#b2">[3]</ref> are simple, interpretable and easy to scale; however, they are limited in their expressive power. Cross features, on the other hand, have been shown to be significant in improving the models' expressiveness. Unfortunately, it o en requires manual feature engineering or exhaustive search to identify such features; moreover, generalizing to unseen feature interactions is difficult.</p><p>In this paper, we aim to avoid task-specific feature engineering by introducing a novel neural network structure -a cross network -that explicitly applies feature crossing in an automatic fashion. e cross network consists of multiple layers, where the highestdegree of interactions are provably determined by layer depth. Each layer produces higher-order interactions based on existing ones, and keeps the interactions from previous layers. We train the cross network jointly with a deep neural network (DNN) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14]</ref>. DNN has the promise to capture very complex interactions across features; however, compared to our cross network it requires nearly an order of magnitude more parameters, is unable to form cross features explicitly, and may fail to efficiently learn some types of feature interactions. Jointly training the cross and DNN components together, however, efficiently captures predictive feature interactions, and delivers state-of-the-art performance on the Criteo CTR dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>Due to the dramatic increase in size and dimensionality of datasets, a number of methods have been proposed to avoid extensive taskspecific feature engineering, mostly based on embedding techniques and neural networks.</p><p>Factorization machines (FMs) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> project sparse features onto low-dimensional dense vectors and learn feature interactions from vector inner products. Field-aware factorization machines (FFMs) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> further allow each feature to learn several vectors where each vector is associated with a field. Regre ably, the shallow structures of FMs and FFMs limit their representative power.</p><p>ere have been work extending FMs to higher orders <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref>, but one downside lies in their large number of parameters which yields undesirable computational cost. Deep neural networks (DNN) are able to learn non-trivial high-degree feature interactions due to embedding vectors and nonlinear activation functions. e recent success of the Residual Network <ref type="bibr" target="#b4">[5]</ref> has enabled training of very deep networks. Deep Crossing <ref type="bibr" target="#b14">[15]</ref> extends residual networks and achieves automatic feature learning by stacking all types of inputs. e remarkable success of deep learning has elicited theoretical analyses on its representative power. ere has been research <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> showing that DNNs are able to approximate an arbitrary function under certain smoothness assumptions to an arbitrary accuracy, given sufficiently many hidden units or hidden layers. Moreover, in practice, it has been found that DNNs work well with a feasible number of parameters. One key reason is that most functions of practical interest are not arbitrary.</p><p>Yet one remaining question is whether DNNs are indeed the most efficient ones in representing such functions of practical interest. In the Kaggle 1 competition, the manually cra ed features in many winning solutions are low-degree, in an explicit format and effective. e features learned by DNNs, on the other hand, are implicit and highly nonlinear. is has shed light on designing a model that is able to learn bounded-degree feature interactions more efficiently and explicitly than a universal DNN. e wide-and-deep <ref type="bibr" target="#b3">[4]</ref> is a model in this spirit. It takes cross features as inputs to a linear model, and jointly trains the linear model with a DNN model. However, the success of wide-and-deep hinges on a proper choice of cross features, an exponential problem for which there is yet no clear efficient method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Main Contributions</head><p>In this paper, we propose the Deep &amp; Cross Network (DCN) model that enables Web-scale automatic feature learning with both sparse and dense inputs. DCN efficiently captures effective feature interactions of bounded degrees, learns highly nonlinear interactions, requires no manual feature engineering or exhaustive searching, and has low computational cost.</p><p>e main contributions of the paper include:</p><p>? We propose a novel cross network that explicitly applies feature crossing at each layer, efficiently learns predictive cross features of bounded degrees, and requires no manual feature engineering or exhaustive searching. ? e cross network is simple yet effective. By design, the highest polynomial degree increases at each layer and is determined by layer depth. e network consists of all the cross terms of degree up to the highest, with their coefficients all different.</p><p>? e cross network is memory efficient, and easy to implement. ? Our experimental results have demonstrated that with a cross network, DCN has lower logloss than a DNN with nearly an order of magnitude fewer number of parameters. e paper is organized as follows: Section 2 describes the architecture of the Deep &amp; Cross Network. Section 3 analyzes the cross network in detail. Section 4 shows the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">DEEP &amp; CROSS NETWORK (DCN)</head><p>In this section we describe the architecture of Deep &amp; Cross Network (DCN) models. A DCN model starts with an embedding and stacking layer, followed by a cross network and a deep network in parallel. ese in turn are followed by a final combination layer which combines the outputs from the two networks. e complete DCN model is depicted in <ref type="figure" target="#fig_0">Figure 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Embedding and Stacking Layer</head><p>We consider input data with sparse and dense features. In Webscale recommender systems such as CTR prediction, the inputs are mostly categorical features, e.g. country=usa . Such features are o en encoded as one-hot vectors e.g. [0,1,0] ; however, this o en leads to excessively high-dimensional feature spaces for large vocabularies.  To reduce the dimensionality, we employ an embedding procedure to transform these binary features into dense vectors of real values (commonly called embedding vectors):</p><formula xml:id="formula_0">1 h ps://www.kaggle.com/ ? x 1 x L1 x 2 h 1 h 2 h L2 ? Cross network !""#$%"&amp;'()* x stack p = sigmoid(Wlogitxstack + blogit) p x 1 = x 0 x T 0 w c,0 + b c,0 + x 0 x 0 h1 = ReLu(Wh,</formula><formula xml:id="formula_1">x embed,i = W embed,i x i ,<label>(1)</label></formula><p>where x embed,i is the embedding vector, x i is the binary input in the i-th category, and W embed,i ? R n e ?n is the corresponding embedding matrix that will be optimized together with other parameters in the network, and n e , n are the embedding size and vocabulary size, respectively. In the end, we stack the embedding vectors, along with the normalized dense features x dense , into one vector:</p><formula xml:id="formula_2">x 0 = x T embed, 1 , . . . , x T embed,k , x T dense ,<label>(2)</label></formula><p>and feed x 0 to the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cross Network</head><p>e key idea of our novel cross network is to apply explicit feature crossing in an efficient way. e cross network is composed of cross layers, with each layer having the following formula:</p><formula xml:id="formula_3">x l +1 = x 0 x T l w l + b l + x l = f (x l , w l , b l ) + x l ,<label>(3)</label></formula><p>where x l , x l +1 ? R d are column vectors denoting the outputs from the l-th and (l + 1)-th cross layers, respectively; w l , b l ? R d are the weight and bias parameters of the l-th layer. Each cross layer adds back its input a er a feature crossing f , and the mapping function f : R d ? R d fits the residual of x l +1 ? x l . A visualization of one cross layer is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>High-degree Interaction Across Features. e special structure of the cross network causes the degree of cross features to grow with layer depth. e highest polynomial degree (in terms of = + + *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Output</head><p>Feature Crossing Bias Input input x 0 ) for an l-layer cross network is l + 1. In fact, the cross network comprises all the cross terms</p><formula xml:id="formula_4">+ + = * * * x 0 x ? x b w y</formula><formula xml:id="formula_5">x ? 1 1 x ? 2 2 . . . x ? d d of degree from 1 to l + 1. Detailed analysis is in Section 3.</formula><p>Complexity Analysis. Let L c denote the number of cross layers, and d denote the input dimension. en, the number of parameters involved in the cross network is</p><formula xml:id="formula_6">d ? L c ? 2.</formula><p>e time and space complexity of a cross network are linear in input dimension. erefore, a cross network introduces negligible complexity compared to its deep counterpart, keeping the overall complexity for DCN at the same level as that of a traditional DNN.</p><p>is efficiency benefits from the rank-one property of x 0 x T l , which enables us to generate all cross terms without computing or storing the entire matrix. e small number of parameters of the cross network has limited the model capacity. To capture highly nonlinear interactions, we introduce a deep network in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Deep Network</head><p>e deep network is a fully-connected feed-forward neural network, with each deep layer having the following formula:</p><formula xml:id="formula_7">h l +1 = f (W l h l + b l ),<label>(4)</label></formula><p>where h l ? R n l , h l +1 ? R n l +1 are the l-th and (l + 1)-th hidden layer, respectively; W l ? R n l +1 ?n l , b l ? R n l +1 are parameters for the l-th deep layer; and f (?) is the ReLU function. Complexity Analysis. For simplicity, we assume all the deep layers are of equal size. Let L d denote the number of deep layers and m denote the deep layer size. en, the number of parameters in the deep network is</p><formula xml:id="formula_8">d ? m + m + (m 2 + m) ? (L d ? 1).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Combination Layer</head><p>e combination layer concatenates the outputs from two networks and feed the concatenated vector into a standard logits layer. e following is the formula for a two-class classification problem:</p><formula xml:id="formula_9">p = ? [x T L 1 , h T L 2 ]w logits ,<label>(5)</label></formula><p>where x L 1 ? R d , h L 2 ? R m are the outputs from the cross network and deep network, respectively, w logits ? R (d +m) is the weight vector for the combination layer, and ? (x) = 1/(1 + exp(?x)). e loss function is the log loss along with a regularization term,</p><formula xml:id="formula_10">loss = ? 1 N N i =1 i log(p i ) + (1 ? i ) log(1 ? p i ) + ? l w l 2 , (6)</formula><p>where p i 's are the probabilities computed from Equation 5, i 's are the true labels, N is the total number of inputs, and ? is the L 2 regularization parameter. We jointly train both networks, as this allows each individual network to be aware of the others during the training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CROSS NETWORK ANALYSIS</head><p>In this section, we analyze the cross network of DCN for the purpose of understanding its effectiveness. We offer three perspectives: polynomial approximation, generalization to FMs, and efficient projection. For simplicity, we assume b i = 0.</p><p>Notations. Let the i-th element in w j be w</p><formula xml:id="formula_11">(i ) j . For multi-index ? = [? 1 , ? ? ? , ? d ] ? N d and x = [x 1 , ? ? ? , x d ] ? R d , we define |? | = d i =1 ? i . Terminology. e degree of a cross term (monomial) x ? 1 1 x ? 2 2 ? ? ? x ? d d</formula><p>is defined by |? |. e degree of a polynomial is defined by the highest degree of its terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Polynomial Approximation</head><p>By the Weierstrass approximation theorem <ref type="bibr" target="#b12">[13]</ref>, any function under certain smoothness assumption can be approximated by a polynomial to an arbitrary accuracy. erefore, we analyze the cross network from the perspective of polynomial approximation. In particular, the cross network approximates the polynomial class of the same degree in a way that is efficient, expressive and generalizes be er to real-world datasets.</p><p>We study in detail the approximation of a cross network to the polynomial class of the same degree. Let us denote by P n (x) the multivariate polynomial class of degree n:</p><formula xml:id="formula_12">P n (x) = ? w ? x ? 1 1 x ? 2 2 . . . x ? d d 0 ? |? | ? n, ? ? N d .<label>(7)</label></formula><p>Each polynomial in this class has O(d n ) coefficients. We show that, with only O(d) parameters, the cross network contains all the cross terms occurring in the polynomial of the same degree, with each term's coefficient distinct from each other.</p><formula xml:id="formula_13">T 3.1.</formula><p>Consider an l-layer cross network with the i + 1-th layer defined as</p><formula xml:id="formula_14">x i +1 = x 0 x T i w i + x i . Let the input to the network be x 0 = [x 1 , x 2 , . . . , x d ] T , the output be l (x 0 ) = x T</formula><p>l w l , and the parameters be w i , b i ? R d . en, the multivariate polynomial l (x 0 ) reproduces polynomials in the following class:</p><formula xml:id="formula_15">? c ? (w 0 , . . . , w l )x ? 1 1 x ? 2 2 . . . x ? d d 0 ? |? | ? l + 1, ? ? N d , where c ? = M ? i?B ? j?P ? |? | k=1 w (j k ) i k , M ? is a constant inde- pendent of w i 's, i = [i 1 , .</formula><p>. . , i |? | ] and j = [j 1 , . . . , j |? | ] are multiindices, B ? = y ? {0, 1, ? ? ? , l } |? | i &lt; j ? |? | = l , and P ? is the set of all the permutations of the indices (1, ? ? ? , 1</p><formula xml:id="formula_16">? 1 times ? ? ? d, ? ? ? , d ? d times</formula><p>). e proof of eorem 3.1 is in the Appendix. Let us give an example. Consider the coefficient c ? for x 1 x 2 x 3 with ? = (1, 1, 1, 0, . . . , 0). Up to some constant, when l = 2, c ? = i, j,k ?P ? w</p><formula xml:id="formula_17">(i ) 0 w (j) 1 w (k) 2 ; when l = 3, c ? = i, j,k ?P ? w (i ) 0 w (j) 1 w (k) 3 +w (i ) 0 w (j) 2 w (k) 3 +w (i ) 1 w (j) 2 w (k) 3 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Generalization of FMs</head><p>e cross network shares the spirit of parameter sharing as the FM model and further extends it to a deeper structure.</p><p>In a FM model, feature x i is associated with a weight vector v i , and the weight of cross term</p><formula xml:id="formula_18">x i x j is computed by v i , v j . In DCN, x i is associated with scalars {w (i ) k } l k=1</formula><p>, and the weight of x i x j is the multiplications of parameters from the sets {w</p><formula xml:id="formula_19">(i ) k } l k=0 and {w (j) k } l</formula><p>k=0 . Both models have each feature learned some parameters independent from other features, and the weight of a cross term is a certain combination of corresponding parameters.</p><p>Parameter sharing not only makes the model more efficient, but also enables the model to generalize to unseen feature interactions and be more robust to noise. For example, take datasets with sparse features. If two binary features x i and x j rarely or never co-occur in the training data, i.e., x i 0 ? x j 0, then the learned weight of x i x j would carry no meaningful information for prediction. e FM is a shallow structure and is limited to representing cross terms of degree 2. DCN, in contrast, is able to construct all the cross terms</p><formula xml:id="formula_20">x ? 1 1 x ? 2 2 . . . x ? d</formula><p>d with degree |? | bounded by some constant determined by layer depth, as claimed in eorem 3.1. erefore, the cross network extends the idea of parameter sharing from a single layer to multiple layers and high-degree cross-terms. Note that different from the higher-order FMs, the number of parameters in a cross network only grows linearly with the input dimension.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Efficient Projection</head><p>Each cross layer projects all the pairwise interactions between x 0 and x l , in an efficient manner, back to the input's dimension.</p><p>Considerx ? R d as the input to a cross layer. e cross layer first implicitly constructs d 2 pairwise interactions x ixj , and then implicitly projects them back to dimension d in a memory-efficient way. A direct approach, however, comes with a cubic cost.</p><p>Our cross layer provides an efficient solution to reduce the cost to linear in dimension d. Consider x p = x 0x T w. is is in fact equivalent to</p><formula xml:id="formula_21">x T p = x 1x1 . . . x 1xd . . . x dx1 . . . x dxd ? ? ? ? ? ? ? ? ? ? ? ? ? ? | w | 0 ... 0 0 | w | ... 0 . . . . . . . . . . . . 0 0 ... | w | ? ? ? ? ? ? ? ? ? ? ? ? ? ?<label>(8)</label></formula><p>where the row vector contains all d 2 pairwise interactions x ixj 's, the projection matrix has a block diagonal structure with w ? R d being a column vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL RESULTS</head><p>In this section, we evaluate the performance of DCN on some popular classification datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Criteo Display Ads Data</head><p>e Criteo Display Ads 2 dataset is for the purpose of predicting ads click-through rate. It has 13 integer features and 26 categorical features where each category has a high cardinality. For this dataset, an improvement of 0.001 in logloss is considered as practically significant. When considering a large user base, a small improvement in prediction accuracy can potentially lead to a large increase in a company's revenue. e data contains 11 GB user logs from a period of 7 days (?41 million records). We used the data of the first 6 days for training, and randomly split day 7 data into validation and test sets of equal size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>DCN is implemented on TensorFlow, we briefly discuss some implementation details for training with DCN. Data processing and embedding. Real-valued features are normalized by applying a log transform. For categorical features, we embed the features in dense vectors of dimension 6?(category cardinality) <ref type="bibr">1/4</ref> . Concatenating all embeddings results in a vector of dimension 1026.</p><p>Optimization. We applied mini-batch stochastic optimization with Adam optimizer <ref type="bibr" target="#b8">[9]</ref>. e batch size is set at 512. Batch normalization <ref type="bibr" target="#b5">[6]</ref> was applied to the deep network and gradient clip norm was set at 100. Regularization. We used early stopping, as we did not find L 2 regularization or dropout to be effective. Hyperparameters. We report results based on a grid search over the number of hidden layers, hidden layer size, initial learning rate and number of cross layers. e number of hidden layers ranged from 2 to 5, with hidden layer sizes from 32 to 1024. For DCN, the number of cross layers 3 is from 1 to 6. e initial learning rate 4 was tuned from 0.0001 to 0.001 with increments of 0.0001. All experiments applied early stopping at training step 150,000, beyond which overfi ing started to occur. LR. We used Sibyl [2]-a large-scale machine-learning system for distributed logistic regression. e integer features were discretized on a log scale. e cross features were selected by a sophisticated feature selection tool. All of the single features were used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Models for Comparisons</head><p>FM. We used an FM-based model with proprietary details.</p><p>W&amp;D. Different than DCN, its wide component takes as input raw sparse features, and relies on exhaustive searching and domain knowledge to select predictive cross features. We skipped the comparison as no good method is known to select cross features.</p><p>DC. Compared to DCN, DC does not form explicit cross features. It mainly relies on stacking and residual units to create implicit crossings. We applied the same embedding (stacking) layer as DCN, followed by another ReLu layer to generate input to a sequence of residual units. e number of residual units was tuned form 1 to 5, with input dimension and cross dimension from 100 to 1026.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Model Performance</head><p>In this section, we first list the best performance of different models in logloss, then we compare DCN with DNN in detail, that is, we investigate further into the effects introduced by the cross network.</p><p>Performance of different models. e best test logloss of different models are listed in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>e optimal hyperparameter se ings were 2 deep layers of size 1024 and 6 cross layers for the DCN model, 5 deep layers of size 1024 for the DNN, 5 residual units with input dimension 424 and cross dimension 537 for the DC, and 42 cross features for the LR model. at the best performance was found with the deepest cross architecture suggests that the higher-order feature interactions from the cross network are valuable. As we can see, DCN outperforms all the other models by a large amount. In particular, it outperforms the state-of-art DNN model but uses only 40% of the memory consumed in DNN. For the optimal hyperparameter se ing of each model, we also report the mean and standard deviation of the test logloss out of 10 independent runs: DCN: 0.4422 ? 9 ? 10 ?5 , DNN: 0.4430 ? 3.7 ? 10 ?4 , DC: 0.4430 ? 4.3 ? 10 ?4 . As can be seen, DCN consistently outperforms other models by a large amount.</p><p>Comparisons Between DCN and DNN. Considering that the cross network only introduces O(d) extra parameters, we compare DCN to its deep network-a traditional DNN, and present the experimental results while varying memory budget and loss tolerance.</p><p>In the following, the loss for a certain number of parameters is reported as the best validation loss among all the learning rates and model structures.</p><p>e number of parameters in the embedding layer was omi ed in our calculation as it is identical to both models. <ref type="table" target="#tab_2">Table 2</ref> reports the minimal number of parameters needed to achieve a desired logloss threshold. From <ref type="table" target="#tab_2">Table 2</ref>, we see that DCN is nearly an order of magnitude more memory efficient than a single DNN, thanks to the cross network which is able to learn bounded-degree feature interactions more efficiently.  <ref type="table" target="#tab_3">Table 3</ref> compares performance of the neural models subject to fixed memory budgets. As we can see, DCN consistently outperforms DNN. In the small-parameter regime, the number of parameters in the cross network is comparable to that in the deep network, and the clear improvement indicates that the cross network is more efficient in learning effective feature interactions. In the large-parameter regime, the DNN closes some of the gap; however, DCN still outperforms DNN by a large amount, suggesting that it can efficiently learn some types of meaningful feature interactions that even a huge DNN model cannot. We analyze DCN in finer detail by illustrating the effect from introducing a cross network to a given DNN model. We first compare the best performance of DNN with that of DCN under the same number of layers and layer size, and then for each se ing, we show how the validation logloss changes as more cross layers are added. <ref type="table" target="#tab_4">Table 4</ref> shows the differences between the DCN and DNN model in logloss. Under the same experimental se ing, the best logloss from the DCN model consistently outperforms that from a single DNN model of the same structure. at the improvement is consistent for all the hyperparameters has mitigated the randomness effect from the initialization and stochastic optimization.   <ref type="figure">Figure 3</ref> shows the improvement as we increase the number of cross layers on randomly selected se ings. For the deep networks in <ref type="figure">Figure 3</ref>, there is a clear improvement when 1 cross layer is added to the model. As more cross layers are introduced, for some se ings the logloss continues to decrease, indicating the introduced cross terms are effective in the prediction; whereas for others the logloss starts to fluctuate and even slightly increase, which indicates the higher-degree feature interactions introduced are not helpful.  <ref type="figure">Figure 3</ref>: Improvement in the validation logloss with the growth of cross layer depth. e case with 0 cross layers is equivalent to a single DNN model. In the legend, "layers" is hidden layers, "nodes" is hidden nodes. Different symbols represent different hyperparameters for the deep network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Non-CTR datasets</head><p>We show that DCN performs well on non-CTR prediction problems. We used the forest covertype (581012 samples and 54 features) and Higgs (11M samples and 28 features) datasets from the UCI repository. e datasets were randomly split into training (90%) and testing (10%) set. A grid search over the hyperparameters was performed. e number of deep layers ranged from 1 to 10 with layer size from 50 to 300. e number of cross layers ranged from 4 to 10. e number of residual units ranged from 1 to 5 with their input dimension and cross dimension from 50 to 300. For DCN, the input vector was fed to the cross network directly.</p><p>For </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION AND FUTURE DIRECTIONS</head><p>Identifying effective feature interactions has been the key to the success of many prediction models. Regre ably, the process o en requires manual feature cra ing and exhaustive searching. DNNs are popular for automatic feature learning; however, the features learned are implicit and highly nonlinear, and the network could be unnecessarily large and inefficient in learning certain features. e Deep &amp; Cross Network proposed in this paper can handle a large set of sparse and dense features, and learns explicit cross features of bounded degree jointly with traditional deep representations. e degree of cross features increases by one at each cross layer. Our experimental results have demonstrated its superiority over the state-of-art algorithms on both sparse and dense datasets, in terms of both model accuracy and memory usage.</p><p>We would like to further explore using cross layers as building blocks in other models, enable effective training for deeper cross networks, investigate the efficiency of the cross network in polynomial approximation, and be er understand its interaction with deep networks during optimization.</p><p>Appendix: Proof of eorem 3.1 P . Notations. Let i be a multi-index vector of 0's and 1's with its last entry fixed at 1. For multi-index ? = [? 1 , ? ? ? , ? d ] ? N d and x = [x 1 , ? ? ? , x d ] T , we define |? | = d i =1 ? i , and x ? = x ? 1 1 x ? 2 2 ? ? ? x ? d d . We first proof by induction that</p><formula xml:id="formula_22">l (x 0 ) = x T l w l = l +1 p=1 |i|=p l j=0 (x T 0 w j ) i j ,<label>(9)</label></formula><p>and then we rewrite the above form to obtain the desired claim.</p><p>Base case. When l = 0, 0 (x 0 ) = x T 0 w 0 . <ref type="figure">Clearly Equation 9</ref> holds. Induction step. We assume that when l = k,</p><formula xml:id="formula_23">k (x 0 ) = x T k w k = k+1 p=1 |i|=p k j=0 (x T 0 w j ) i j .</formula><p>When l = k + 1,</p><formula xml:id="formula_24">x T k+1 w k+1 = (x T k w k )(x T 0 w k+1 ) + x T k w k+1<label>(10)</label></formula><p>Because x k only contains w 0 , . . . , w k?1 , it follows that the formula of x T k w k+1 can be obtained from that of x T k w k by replacing all the w k 's occurred in x T k w k to w k+1 . en</p><formula xml:id="formula_25">x T k+1 w k+1 = k+1 p=1 |i|=p (x T 0 w k+1 ) k j=0 (x T 0 w j ) i j + k+1 p=1 |i|=p (x T 0 w k+1 ) i k k?1 j=0 (x T 0 w j ) i j = k+2 p=2 |i|=p i k =1 k+1 j=0 (x T 0 w j ) i j + k+1 p=1 |i|=p i k =0 k+1 j=0 (x T 0 w j ) i j = k+1 p=2 |i|=p k+1 j=0 (x T 0 w j ) i j + (x T 0 w k+1 ) + k+1 j=0 (x T 0 w j ) = k+2 p=1 |i|=p k+1 j=0 (x T 0 w j ) i j .<label>(11)</label></formula><p>e first equality is a result of increasing the size of i from k + 1 to k + 2.</p><p>e second equality used the fact that the last entry of i is always 1 by definition, and the same was applied to the last equality. By induction hypothesis, Equation 9 holds for all l ? Z.</p><p>Next, we compute c ? (w 0 , ? ? ? , w l ), the coefficient of x ? , by rearranging the terms in <ref type="bibr">Equation 9</ref>. Note that all the different permutations of x 1 ? ? ? x 1 ? 1 ? ? ? x d ? ? ? x d ? d are in the form of x ? . erefore, c ? is the summation of all the weights associated with each permutation occurred in Equation 9. e weight for permutation x j 1 x j 2 ? ? ? x j p is</p><formula xml:id="formula_26">i 1 , ??? ,i p w (j 1 ) i 1 w (j 2 ) i 2 ? ? ? w (j p ) i p ,</formula><p>where (i 1 , ? ? ? , i p ) belongs to the set of all the corresponding active indices for |i| = p, specifically, (i 1 , ? ? ? , i p ) ? B p =: y ? {0, 1, ? ? ? , l } p i &lt; j ? p = l . erefore, if we denote P ? to be the set of all the permutations of (1 ? ? ? 1 ? 1 ? ? ? d ? ? ? d ? d ), then we arrive at our claim</p><formula xml:id="formula_27">c ? = j 1 , ??? , j p ?P p i 1 , ??? ,i p ?B p p k=1 w (j k ) i k .<label>(12)</label></formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>e Deep &amp; Cross Network</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Visualization of a cross layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>We compare DCN with five models: the DCN model with no cross network (DNN), logistic regression (LR), Factorization Machines (FMs), Wide and Deep Model (W&amp;D), and Deep Crossing (DC). DNN. e embedding layer, the output layer, and the hyperparameter tuning process are the same as DCN. e only change from the DCN model was that there are no cross layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Best test logloss from different models. "DC" is deep crossing, "DNN" is DCN with no cross layer, "FM" is Factorization Machine based model, "LR" is logistic regression.</figDesc><table><row><cell>Model</cell><cell>DCN</cell><cell>DC</cell><cell>DNN</cell><cell>FM</cell><cell>LR</cell></row><row><cell cols="6">Logloss 0.4419 0.4425 0.4428 0.4464 0.4474</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>#parameters needed to achieve a desired logloss.</figDesc><table><row><cell>Logloss</cell><cell>0.4430</cell><cell>0.4460</cell><cell>0.4470</cell><cell>0.4480</cell></row><row><cell>DNN</cell><cell cols="4">3.2 ? 10 6 1.5 ? 10 5 1.5 ? 10 5 7.8 ? 10 4</cell></row><row><cell>DCN</cell><cell cols="4">7.9 ? 10 5 7.3 ? 10 4 3.7 ? 10 4 3.7 ? 10 4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Best logloss achieved with various memory budgets. #Params 5 ? 10 4 1 ? 10 5 4 ? 10 5 1.1 ? 10 6 2.5 ? 10 6</figDesc><table><row><cell>DNN</cell><cell>0.4480 0.4471 0.4439</cell><cell>0.4433</cell><cell>0.4431</cell></row><row><cell>DCN</cell><cell>0.4465 0.4453 0.4432</cell><cell>0.4426</cell><cell>0.4423</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Differences in the validation logloss (?10 ?2 ) between DCN and DNN. e DNN model is the DCN model with the number of cross layers set to 0. Negative values mean that the DCN outperforms DNN.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>the forest covertype data, DCN achieved the best test accuracy 0.9740 with the least memory consumption. Both DNN and DC achieved 0.9737. e optimal hyperparameter se ings were 8 cross layers of size 54 and 6 deep layers of size 292 for DCN, 7 deep layers of size 292 for DNN, and 4 residual units with input dimension 271 and cross dimension 287 for DC. For the Higgs data, DCN achieved the best test logloss 0.4494, whereas DNN achieved 0.4506. e optimal hyperparameter settings were 4 cross layers of size 28 and 4 deep layers of size 209 for DCN, and 10 deep layers of size 196 for DNN. DCN outperforms DNN with half of the memory used in DNN.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">h ps://www.kaggle.com/c/criteo-display-ad-challenge<ref type="bibr" target="#b2">3</ref> More cross layers did not lead to significant improvement, so we restrict ourselves in a small range for finer tuning.<ref type="bibr" target="#b3">4</ref> Experimentally we observe that for the Criteo dataset, a learning rate larger than 0.001 usually degrades the performance.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Higher-Order Factorization Machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akinori</forename><surname>Fujino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naonori</forename><surname>Ueda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masakazu</forename><surname>Ishihata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3351" to="3359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Sibyl: A system for large scale supervised machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Canini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">Technical Talk</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Simple and scalable response prediction for display advertising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eren</forename><surname>Manavoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Romer</forename><surname>Rosales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology (TIST)</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">61</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Heng-Tze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Levent</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremiah</forename><surname>Koc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Harmsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tushar</forename><surname>Shaked</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hrishi</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Glen</forename><surname>Aradhye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07792</idno>
		<title level="m">Mustafa Ispir, and others. 2016. Wide &amp; Deep Learning for Recommender Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<title level="m">Batch normalization: Accelerating deep network training by reducing internal covariate shi</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Field-aware factorization machines in a real-world online advertising system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchin</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Lefortier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on World Wide Web Companion. International World Wide Web Conferences Steering Commi ee</title>
		<meeting>the 26th International Conference on World Wide Web Companion. International World Wide Web Conferences Steering Commi ee</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="680" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fieldaware factorization machines for CTR prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchin</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Sheng</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Jen</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th ACM Conference on Recommender Systems</title>
		<meeting>the 10th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="43" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Factorization machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE International Conference on Data Mining. IEEE</title>
		<imprint>
			<biblScope unit="page" from="995" to="1000" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Factorization Machines with libFM</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steffen Rendle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Intell. Syst. Technol</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<date type="published" when="2012-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Principles of mathematical analysis</title>
		<imprint>
			<date type="published" when="1964" />
			<biblScope unit="volume">3</biblScope>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>Walter Rudin and others</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep Crossing: Web-Scale Modeling without Manually Cra ed Combinatorial Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Hoens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haijing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="255" to="262" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Learning polynomials with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Valiant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Residual Networks Behave Like Ensembles of Relatively Shallow Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garne</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="550" to="558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Gi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.01697</idno>
		<title level="m">Tensor machines for learning target-specific polynomial features</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
