<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hopular: Modern Hopfield Networks for Tabular Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?fl</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Gruber</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Bitto-Nemling</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Advanced Research in Artificial Intelligence (IARAI)</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Institute of Advanced Research in Artificial Intelligence (IARAI)</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Institute for Machine Learning</orgName>
								<orgName type="laboratory">ELLIS Unit Linz and LIT AI Lab</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Hopular: Modern Hopfield Networks for Tabular Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T21:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract/>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep Learning has led to tremendous success in vision and natural language processing, where it excelled on large image and text corpora <ref type="bibr" target="#b26">(LeCun et al., 2015;</ref><ref type="bibr" target="#b34">Schmidhuber, 2015)</ref>. While it yielded competitive results on large tabular datasets <ref type="bibr" target="#b2">Avati et al. (2018)</ref>; <ref type="bibr" target="#b39">Simm et al. (2018)</ref>; <ref type="bibr" target="#b51">Zhang et al. (2019b)</ref>; <ref type="bibr" target="#b27">Mayr et al. (2018)</ref>, so far it could not convince on small tabular data. However, in real-world settings, small tabular datasets with less than 10,000 samples are ubiquitous. They are found in life sciences, when building a model for a certain disease with a limited number of patients, for bio-assays in drug design, or for the effect of environmental soil contamination. The same situation appears in most industrial applications, when a company wants to predict customer behavior, to control processes, to optimize its logistics, to market new products, or to employ predictive maintenance. The omnipresence of small tabular datasets can also be witnessed at Kaggle challenges. On small-sized and medium-sized tabular datasets with less than 10,000 samples, Support Vector Machines (SVMs) <ref type="bibr" target="#b4">(Boser et al., 1992;</ref><ref type="bibr" target="#b7">Cortes &amp; Vapnik, 1995;</ref><ref type="bibr" target="#b35">Sch?lkopf &amp; Smola, 2002)</ref>, Random Forests <ref type="bibr" target="#b19">(Ho, 1995;</ref><ref type="bibr" target="#b5">Breiman, 2001</ref>) and, in particular, Gradient Boosting <ref type="bibr" target="#b15">(Friedman, 2001)</ref> typically outperform Deep Learning methods with Gradient Boosting having the edge. In real world applications, the best Recently, research on extending Deep Learning methods to tabular data has been intensified. Some approaches to tabular data are only remotely related to Deep Learning. AutoGluon-Tabular stacks small neural networks for tabular data <ref type="bibr" target="#b12">(Erickson et al., 2020)</ref>. Neural Oblivious Decision Ensembles (NODE) generalizes ensembles of oblivious decision trees by hierarchical representation learning <ref type="bibr" target="#b30">(Popov et al., 2019)</ref>. NODE is a hybrid of differentiable decision trees and neural networks. DNF-Net builds neural structures corresponding to logical Boolean formulas in disjunctive normal forms, which enable localized decisions using small subsets of the features <ref type="bibr" target="#b0">(Abutbul et al., 2020)</ref>.</p><p>However, most research focused on adapting established Deep Learning techniques to tabular data. Modifications to deep neural networks like introducing leaky gates or skip connections can improve their performance on tabular data <ref type="bibr" target="#b14">(Fiedler, 2021)</ref>. Even plain MLPs that are well-regularized work well on tabular data <ref type="bibr" target="#b21">(Kadra et al., 2021)</ref>. Different regularization coefficients to each weight improve the performance of Deep Learning architectures on tabular data <ref type="bibr" target="#b36">(Shavitt &amp; Segal, 2018)</ref>. TabularNet consists of three modules <ref type="bibr">(Du et al., 2021)</ref>. First, it uses handcrafted cell-level feature extraction with a language model for textual data. Secondly, it uses both row and column-wise pooling via bidirectional gated recurrent units. Thirdly, a graph convolutional network captures dependencies between cells of the table.</p><p>Many approaches that adapt Deep Learning methods to tabular data use attention mechanisms from transformers <ref type="bibr" target="#b42">(Vaswani et al., 2017)</ref> and BERT . The TabTransformer learns contextual embeddings of categorical features <ref type="bibr" target="#b20">(Huang et al., 2020)</ref>. However, continuous features are not covered, therefore the feature-feature interaction is limited. The FT-Transformer maps features to tokens that are fed into a transformer <ref type="bibr" target="#b16">(Gorishniy et al., 2021)</ref>. The FT-Transformer performs well on tabular data but all considered datasets have more than 10,000 samples. TabNet uses an attentive transformer for sequential attention to predict masked features <ref type="bibr" target="#b1">(Arik &amp; Pfister, 2021)</ref>. Therefore, TabNet does instance-wise feature selection, that is, can select the relevant features for each input differently. TabNet also utilizes feature masking for pre-training, which was very successful in natural language processing when pre-training the BERT model. Also semi-supervised learning has been proposed for tabular data using projections of the features and contrastive learning <ref type="bibr" target="#b8">(Darabi et al., 2021)</ref>. The contrastive loss is low if pairs of the same class have high similarity. Value Imputation and Mask Estimation (VIME) uses self-and semi-supervised learning of deep architectures for tabular data <ref type="bibr" target="#b48">(Yoon et al., 2020)</ref>. Like BERT, the network has to predict the values of the masked feature vectors, where the target is always masked. The success of BERT feature masking confirms that Deep Learning techniques must employ strong regularization to be successful on tabular data <ref type="bibr" target="#b21">(Kadra et al., 2021)</ref>. A multi-head self-attentive neural network for modeling feature-feature interactions was also used in AutoInt <ref type="bibr" target="#b41">(Song et al., 2019)</ref>. So far we mentioned work, where attention mechanisms extract feature-feature and feature-target relations. However, also inter-sample attention can be implemented, if the whole training set is given at the input. TabGNN uses a graph neural network for tabular data to model inter-sample relations <ref type="bibr" target="#b18">(Guo et al., 2021)</ref>. However, the authors focus on large tabular datasets with more than 40,000 samples. SAINT contains both self-attention and inter-sample attention and embeds both categorical and continuous features before feeding them into transformer modules <ref type="bibr" target="#b40">(Somepalli et al., 2021)</ref>. SAINT uses self-supervised pre-training with a contrastive loss to minimize the difference between original and mixed samples. Non-Parametric Transformers (NPTs) also use feature self-attention and inter-sample attention <ref type="bibr" target="#b25">(Kossen et al., 2021)</ref>. The feature self-attention identifies dependencies between features, while inter-sample attention detects relations between samples. As in previous approaches, BERT masking is used during training, where the masked feature values and the target have to be predicted.</p><p>We suggest Hopular to learn with modern Hopfield networks from tabular data. Hopular is a Deep Learning architecture, where each layer is equipped with continuous modern Hopfield networks <ref type="bibr" target="#b33">(Ramsauer et al., 2021;</ref><ref type="bibr" target="#b46">Widrich et al., 2020)</ref>. Continuous modern Hopfield networks can store two types of data: (i) the whole training set or (ii) the feature embedding vectors of the original input. Like SAINT and NPT, Hopular can detect feature-feature, feature-target, sample-sample, and sampletarget dependencies via modern Hopfield networks. Hopular's novelty is that every layer can directly access the original input as well as the whole training set via stored data in the Hopfield networks. In each layer, the stored training set enables similarity-, prototype-, or quantization-based learning methods like nearest neighbor. In each layer, the stored original input enables the identification of dependencies between the features and the target. Consequently, the current model and its prediction can be step-wise improved at every layer via direct access to both the training set and the original input. Therefore, a pass through a Hopular model is similar to standard learning algorithms, which iteratively improve the current model and its prediction by re-accessing the training set. The number of iterations is fixed by the number of layers in the Hopular architecture. As previous methods, Hopular uses a feature embedding and BERT masking, where masked features have to be predicted. Hopular is most closely related to SAINT <ref type="bibr" target="#b40">(Somepalli et al., 2021)</ref> and Non-Parametric Transformers (NPTs) <ref type="bibr" target="#b25">(Kossen et al., 2021)</ref>, but in contrast to SAINT and NPTs, the whole training set and the original input are provided via Hopfield networks at every layer and not only at the input.</p><p>Recently, it was reported that Random Forests still outperform standard Deep Learning techniques on tabular datasets with up to 10,000 samples <ref type="bibr" target="#b47">(Xu et al., 2021)</ref>. In <ref type="bibr" target="#b38">(Shwartz-Ziv &amp; Armon, 2021)</ref>, the authors show that XGBoost outperforms various Deep Learning methods that are designed for tabular data on datasets that did not appear in the original papers. Therefore, we test Hopular on exactly those datasets to see whether it performs as well as XGBoost. Furthermore, we test Hopular on UCI datasets <ref type="bibr" target="#b33">(Ramsauer et al., 2021;</ref><ref type="bibr" target="#b23">Klambauer et al., 2017;</ref><ref type="bibr" target="#b43">Wainberg et al., 2016;</ref><ref type="bibr" target="#b13">Fern?ndez-Delgado et al., 2014)</ref>. Hopular surpasses Gradient Boosting, Random Forests, and SVMs but also state-of-the-art Deep Learning approaches to tabular data like NPTs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Brief Review of Modern Hopfield Networks</head><p>We briefly review continuous modern Hopfield networks. Their main properties are that they retrieve stored patterns with only one update and that they have exponential storage capacity <ref type="bibr" target="#b33">(Ramsauer et al., 2021)</ref>.</p><p>We assume a set of patterns {x 1 , . . . , x N } ? R d that are stacked as columns to the matrix X = (x 1 , . . . , x N ) and a state pattern (query) ? ? R d that represents the current state. The largest norm of a stored pattern is M = max i x i . Continuous modern Hopfield networks with state ? have the energy</p><formula xml:id="formula_0">E = ? ? ?1 log N i=1 exp(?x T i ?) + ? ?1 log N + 1 2 ? T ? + 1 2 M 2 .<label>(1)</label></formula><p>For energy E and state ?, the update rule ? new = f (?; X, ?) = X p = X softmax(?X T ?)</p><p>(2) has been proven to converge globally to stationary points of the energy E, which are almost always local minima <ref type="bibr" target="#b33">(Ramsauer et al., 2021)</ref>. The update rule Eq. (2) is also the formula of the well-known transformer attention mechanism <ref type="bibr" target="#b42">(Vaswani et al., 2017;</ref><ref type="bibr" target="#b33">Ramsauer et al., 2021)</ref>, therefore Hopfield retrieval and transformer attention coincide.</p><p>The separation ? i of a pattern x i is defined as its minimal dot product difference to any of the other patterns:</p><formula xml:id="formula_1">? i = min j,j =i x T i x i ? x T i x j .</formula><p>A pattern is well-separated from the data if ? i ? 2 /?N + 1 /? log 2(N ? 1)N ?M 2 . If the patterns x i are well separated, the iterate Eq. (2) converges to a fixed point close to a stored pattern. If some patterns are similar to one another and, therefore, not well separated, the update rule Eq. (2) converges to a fixed point close to the mean of the similar patterns. This fixed point is a metastable state of the energy function and averages over similar patterns.</p><p>The next theorem states that the update rule Eq. (2) typically converges after one update if the patterns are well separated. Furthermore, it states that the retrieval error is exponentially small in the separation ? i (for the proof see <ref type="bibr" target="#b33">(Ramsauer et al., 2021)</ref>): Theorem 2.1. With query ?, after one update the distance of the new point f (?) to the fixed point x * i is exponentially small in the separation ? i . The precise bounds using the Jacobian J = ?f (?) /?? and its value J m in the mean value theorem are:</p><formula xml:id="formula_2">f (?) ? x * i ? J m 2 ? ? x * i ,<label>(3)</label></formula><formula xml:id="formula_3">J m 2 ? 2 ? N M 2 (N ? 1) exp(? ? (? i ? 2 max{ ? ? x i , x * i ? x i } M )) .</formula><p>(4) For given and sufficiently large ? i , we have f (?) ? x * i &lt; , that is, retrieval with one update.</p><formula xml:id="formula_4">The retrieval error f (?) ? x i of pattern x i is bounded by f (?) ? x i ? 2 (N ? 1) exp(? ? (? i ? 2 max{ ? ? x i , x * i ? x i } M )) M . (5)</formula><p>The main requirement to modern Hopfield networks to be suited for tabular data is that they can store and retrieve enough patterns. We want to store a potentially large training set in every layer of a Deep Learning architecture. We first define what we mean by storing and retrieving patterns from a modern Hopfield network. Definition 2.2 (Pattern Stored and Retrieved). We assume that around every pattern x i a sphere S i is given. We say x i is stored if there is a single fixed point x * i ? S i to which all points ? ? S i converge, and S i ? S j = ? for i = j. We say x i is retrieved for a given if iteration (update rule) Eq. (2) gives a pointx i that is at least -close to the single fixed point</p><formula xml:id="formula_5">x * i ? S i . The retrieval error is x i ? x i .</formula><p>As with classical Hopfield networks, we consider patterns on the sphere, i.e. patterns with a fixed norm. For randomly chosen patterns, the number of patterns that can be stored is exponential in the dimension d of the space of the patterns (for the proof see <ref type="bibr" target="#b33">(Ramsauer et al., 2021)</ref>): Theorem 2.3. We assume a failure probability 0 &lt; p ? 1 and randomly chosen patterns on the sphere with radius M := K ? d ? 1. We define a := 2 /d?1(1 + ln(2?K 2 p(d ? 1))), b := 2K 2 ? /5, and c := b /W0(exp(a+ln(b)), where W 0 is the upper branch of the Lambert W function <ref type="bibr">(Olver et al., 2010, (4.13)</ref>), and ensure c ? ( 2 / ? p) 4 /d?1 . Then with probability 1 ? p, the number of random patterns that can be stored is:</p><formula xml:id="formula_6">N ? ? p c d?1 4 .<label>(6)</label></formula><p>Therefore it is proven for c ? 3.1546 with ? = 1, K = 3, d = 20 and p = 0.001 (a + ln(b) &gt; 1.27) and proven for c ? 1.3718 with ? = 1, K = 1, d = 75, and p = 0.001 (a + ln(b) &lt; ?0.94).</p><p>This theorem motivates to use continuous modern Hopfield networks for tabular data, where we want to store the training set in each layer of a Deep Learning architecture. Even for hundreds of thousands of training samples, the continuous modern Hopfield network is able to store the training set if the dimension of the pattern is large enough.</p><p>3 Hopular: Modern Hopfield Networks for Tabular Data Hopular architecture. The Hopular architecture consists of an Embedding layer, several stacked Hopular blocks, and a Summarization layer as depicted in <ref type="figure">Figure 1</ref>. As Hopular operates on features as well as on targets, we more generally refer to them as attributes.  (i) The input to the Embedding Layer is an original input sample with d attributes, including a masked target. Categorical attributes are encoded as one-hot vectors, whereas continuous attributes are normalized to zero mean and unit variance. Then a mapping to an e-dimensional embedding space is applied. The index of an attribute w.r.t. the position inside the sample as well as the attribute type are conserved by separate e-dimensional learnable embeddings. All three embedding vectors are element-wise summed and serve as the final representation of an input attribute. The original input sample is then represented by the concatenation of all attribute representations. This concatenation also initializes the current prediction vector ? ? R d?e -see <ref type="figure">Figure A</ref>.3 of the Appendix.</p><p>(ii) The current prediction vector serves as input to a Hopular Block. A Hopular block consecutively applies two different Hopfield modules. Each of these Hopfield modules refines the current prediction vector by updating the current predictions for all attributes and combining it with its input via a residual connection. Thus, in addition to the target, also the features of the original input sample must be predicted during training. <ref type="figure">Figure 2</ref> illustrates the forward-pass of a single original input sample with the masked target indicated by the question mark (?). All current attribute predictions are refined. The masked target is transformed by the Hopular block to a corresponding prediction as indicated by a check mark (). Also feature representations can be masked as with BERT pre-training.</p><p>(iii) The Summarization Layer summarizes the refined current prediction vector resulting from the stacked Hopular blocks. The current prediction vector is mapped to the final prediction vector by separately mapping each current feature prediction to the corresponding final prediction as well as mapping the current target prediction to the final target prediction -see <ref type="figure">Figure A</ref>.4 of the Appendix. In the following we describe the components (I)-(II) of a Hopular Block.</p><p>(I) Hopfield Module H s . The first Hopfield module H s implements a modern Hopfield network for Deep Learning architectures similar to HopfieldLayer <ref type="bibr" target="#b33">(Ramsauer et al., 2021</ref> with the training set as fixed stored patterns. The current input ? (which is also the current prediction from the previous layer) to Hopfield module H s is interacting with the whole training data as described in Eq. <ref type="formula" target="#formula_7">(7)</ref>. This is the update rule of continuous modern Hopfield networks as given in Eq. <ref type="formula">(2)</ref> </p><formula xml:id="formula_7">H s (?) = W S W X Xsoftmax(? X T W T X W ? ?) .<label>(7)</label></formula><p>The hyperparameter ? allows to steer the type of fixed point the update rule Eq. <ref type="formula">(2)</ref>   </p><formula xml:id="formula_8">H s (?) = W G H 1 s (?) T , . . . , H M s (?) T T ,<label>(8)</label></formula><formula xml:id="formula_9">H f (?) = W F W Y Y softmax ? Y T W T Y W ? ? .<label>(9)</label></formula><p>H f may contain more than one continuous modern Hopfield network, which leads to an analog equation as Eq. <ref type="formula" target="#formula_8">(8)</ref>  Hopular's Objective and Training Method. Hopular's objective is a weighted sum of the selfsupervised loss for predicting masked features and the standard supervised target loss. In the following we explain the feature masking as well as the objective in more detail.</p><p>Feature Masking. We follow state-of-the-art Deep Learning methods like SAINT <ref type="bibr" target="#b40">(Somepalli et al., 2021)</ref> and Non-Parametric Transformers (NPTs) <ref type="bibr" target="#b25">(Kossen et al., 2021)</ref> that are tailored to tabular data and use BERT masking  of the input features. Masked input features must be predicted during training. Feature masking is an especially beneficial self-supervised approach when handling small datasets as it exerts a strong regularizing effect on the training procedure. The amount of masked features during training is determined by the masking probability, which is a hyperparameter of the model. In Hopular, both features and targets can be masked during training, while for inference only the target is masked.</p><p>Objective. Hopular's objective is a weighted sum of the masked feature loss L f and the supervised target loss L t . The overall loss L is</p><formula xml:id="formula_10">L = ? L f + (1 ? ?)L t ,<label>(10)</label></formula><p>where L t and L f are the negative logloss in case of discrete attributes and the mean squared error in case of continuous attributes with ? as a hyperparameter. In our default hyperparameter setting ? is annealed using a cosine scheduler starting at 1 with a final value of 0. Another essential hyperparameter for Hopular is ? in Eq. <ref type="formula" target="#formula_7">(7)</ref> and Eq. (9). A small ? retrieves a pattern close to the mean of the stored patterns, while a large ? retrieves the stored pattern that is closest to the initial state pattern <ref type="bibr" target="#b33">(Ramsauer et al., 2021</ref> </p><formula xml:id="formula_11">? ? ? + H s (?) 5: ? ? Reshape(? T ) 6: ? ? ? + H f (?) 7: ? ? Reshape(?) T 8: end for 9: ? ? S(?)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Since Deep Learning methods have already been successfully applied to larger tabular datasets <ref type="bibr" target="#b2">(Avati et al., 2018;</ref><ref type="bibr" target="#b39">Simm et al., 2018;</ref><ref type="bibr" target="#b51">Zhang et al., 2019b;</ref><ref type="bibr" target="#b27">Mayr et al., 2018)</ref> we want to know whether Hopular is competitive on small tabular datasets. In particular, we compare Hopular to XGBoost, CatBoost, LightGBM, and NPTs <ref type="bibr" target="#b25">(Kossen et al., 2021)</ref>. Gradient Boosting has the lead on tabular data when excluding Deep Learning methods. NPTs represent state-of-the-art Deep Learning methods for tabular data, as NPTs yielded very good results on small tabular datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Small-Sized Tabular Datasets</head><p>In these experiments, we compare Hopular to other Deep Learning methods, XGBoost, CatBoost, and LightGBM on small-sized tabular datasets.</p><p>Methods Compared. We compare Hopular, XGBoost, CatBoost, LightGBM, NPTs, and other 24 machine learning methods as described in <ref type="bibr" target="#b23">(Klambauer et al., 2017)</ref>. The compared methods include 10 Deep Learning (DL) approaches. Following <ref type="bibr" target="#b23">(Klambauer et al., 2017;</ref><ref type="bibr" target="#b43">Wainberg et al., 2016)</ref>, 17 methods are selected from their respective method group as the model with the median performance over all datasets within each method group. NPTs are used in a non-transductive setting for a fair comparison.</p><p>Hyperparameter Selection. All hyperparameters are selected on seperate validation sets. For NPTs we perform hyperparameter search as in <ref type="table" target="#tab_8">Table A</ref>.5. This includes the hyperparameters that have already been successfully used in <ref type="bibr" target="#b25">(Kossen et al., 2021)</ref> on small-and medium-sized tabular datasets. This selection also serves as a constraint on the computational resources invested for Hopular. For XGBoost, CatBoost, and LightGBM, we apply the same Bayesian hyperparameter optimization procedure as described in <ref type="bibr" target="#b38">(Shwartz-Ziv &amp; Armon, 2021)</ref>. For LightGBM we use the default hyperparameter ranges as specified by hyperopt-sklearn . Section A.3 of the Appendix describes the hyperparameter selection in more detail.</p><p>Datasets. Following <ref type="bibr" target="#b23">(Klambauer et al., 2017)</ref>, we consider UCI machine learning repository datasets with less than or equal to 1,000 samples as being small. We select 21 of these datasets and give an overview in <ref type="table" target="#tab_8">Table A.</ref>3. The datasets themselves as well as the train/test splits are taken from <ref type="bibr" target="#b13">(Fern?ndez-Delgado et al., 2014)</ref>. A detailed explanation of the dataset selection process as well as a description of the datasets can be found in Section A.2 of the Appendix. <ref type="table" target="#tab_8">Table 1</ref>: Median rank of compared methods across the datasets of the UCI machine learning repository. Methods are ranked for each dataset according to the accuracy on the respective test set. Hopular achieves the lowest median rank of 7.5, therefore is the best performing method across the considered UCI datasets. The complete list can be seen in <ref type="table" target="#tab_8">Table A</ref>.7 of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Rank Method Rank Results. <ref type="table" target="#tab_8">Table 1</ref> shows the median rank of all compared methods across the datasets of the UCI machine learning repository (see <ref type="table" target="#tab_8">Table A</ref>.7 of the Appendix for the complete list). Methods are ranked for each dataset according to the accuracy on the respective test set. 17 method groups have been compared previously <ref type="bibr" target="#b43">(Wainberg et al., 2016)</ref>, to which we add XGBoost <ref type="bibr" target="#b6">(Chen &amp; Guestrin, 2016)</ref>, CatBoost <ref type="bibr" target="#b10">(Dorogush et al., 2017;</ref><ref type="bibr" target="#b31">Prokhorenkova et al., 2018)</ref>, LightGBM <ref type="bibr" target="#b22">(Ke et al., 2017)</ref>, NPTs <ref type="bibr" target="#b25">(Kossen et al., 2021)</ref>, Self-Normalizing Networks <ref type="bibr" target="#b23">(Klambauer et al., 2017)</ref>, and our Hopular. Deep Learning methods are indicated by "(DL)" and are not grouped. Hopular has a median rank of 7.5, followed by Support Vector Machines with 9.5, while NPTs, XGBoost, CatBoost, and LightGBM have a median rank of 11, 12, 14, and 14.5 respectively. Hopular with modern Hopfield networks as memory performs better than other Deep Learning methods and in particular better than the closely-related NPTs. Across the considered UCI datasets, Hopular is the best performing method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Medium-Sized Tabular Datasets</head><p>In these experiments, we compare Hopular to other Deep Learning methods, XGBoost, CatBoost, and LightGBM on medium-sized tabular datasets. In (Shwartz-Ziv &amp; Armon, 2021), the authors show that XGBoost outperforms various Deep Learning methods that are designed for tabular data on datasets that did not appear in the original papers. We want to know whether XGBoost still has the lead on these medium-sized datasets.</p><p>Methods Compared. We compare Hopular, NPTs, XGBoost, CatBoost, and LightGBM. NPTs are used in a non-transductive setting for a fair comparison.</p><p>Hyperparameter Selection. All hyperparameters are selected on seperate validation sets. For NPTs we perform hyperparameter search as in <ref type="table" target="#tab_8">Table A</ref>.5. This includes the hyperparameters that have already been successfully used in <ref type="bibr" target="#b25">(Kossen et al., 2021)</ref> on small-and medium-sized tabular datasets. This selection also serves as a constraint on the computational resources invested for Hopular. For XGBoost, CatBoost, and LightGBM, we apply the same Bayesian hyperparameter optimization procedure as described in <ref type="bibr" target="#b38">(Shwartz-Ziv &amp; Armon, 2021)</ref>. For LightGBM we use the default hyperparameter ranges as specified by hyperopt-sklearn . Section A.3 of the Appendix describes the hyperparameter selection in more detail.</p><p>Datasets. We select the datasets and dataset splits of (Shwartz-Ziv &amp; Armon, 2021), where XGBoost performs better than Deep Learning methods that have been designed for tabular data. We extend this selection by two datasets for regression: (a) colleges was already used for other Deep Learning methods for tabular data <ref type="bibr" target="#b40">(Somepalli et al., 2021)</ref>, and (b) sulfur is publicly available and fits with its 10,082 instances well into the existing collection of medium-sized datasets. <ref type="table" target="#tab_8">Table A</ref> <ref type="bibr">.4</ref> gives an overview of the medium-sized datasets. A detailed description of the datasets can be found in Section A.2 of the Appendix. Results. <ref type="table" target="#tab_6">Table 2</ref> reports the results of Hopular, NPTs, XGBoost, CatBoost, and LightGBM on the medium-sized datasets. The evaluation procedure is from <ref type="bibr" target="#b38">(Shwartz-Ziv &amp; Armon, 2021)</ref>. Hopular is the best performing method on 3 out of the 6 datasets. The runner-up method, CatBoost, is twice the best method, whereas XGBoost once. The biggest performance difference is achieved by Hopular on the two regression datasets, where the capabilities of an external memory really shine. Directly deriving the underlying function for regression datasets may be a difficult task, especially in absence of abundant data. Hopular is able to mitigate this shortcoming by incorporating local neighbourhood information and iteratively refining its current prediction by memory lookups. Over the 6 datasets, NPTs and XGBoost have a median rank of 4.5, CatBoost and LightGBM of 2.5 and 2, respectively, and Hopular has a median rank of 1.5. On average over all 6 datasets, Hopular performs better than NPTs, XGBoost, CatBoost, and LightGBM. We also found that our method needs only a fraction of the memory compared to NPTs which can be seen in <ref type="table" target="#tab_8">Table A</ref>.8. We also added runtime estimates in <ref type="table" target="#tab_8">Table A</ref>.9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Hopular is a novel Deep Learning architecture where every layer is equipped with an external memory. This enables Hopular to mimic standard iterative learning algorithms that refine the current prediction by re-accessing the training set. We validated the usefulness of this property both on small-and medium-sized tabular datasets. Hopular is the best performing method across a broad selection of specifically challenging small-sized UCI datasets. Additionally, Hopular is the best-performing method on medium-sized tabular datasets among which CatBoost and LightGBM achieved very competitive results. This makes Hopular a strong contender to current state-of-the-art methods like Gradient Boosting and other Deep Learning methods specialized in small-and medium-sized datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Datasets</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 UCI Dataset Selection</head><p>To assess the performance of Hopular and other Deep Learning methods on small datasets, we select a subset of 21 datasets from <ref type="bibr" target="#b23">(Klambauer et al., 2017)</ref>. The sizes of these datasets range from 200 to 1,000 samples. We put the focus on smaller sizes, therefore we select 13 datasets with 500 samples or less. Additionally, we select four datasets with 500 to 750 samples and four dataset with 750 to 1,000 samples. Small datasets typically have small test sets, which introduce a high variance in their evaluations. This is especially true if they are overly small or unbalanced. Furthermore, some test sets seem to be not sampled iid from the whole population. Thus, the method evaluation may be highly dependent on the chosen train/test split and performance estimates may be skewed. Problematic datasets in <ref type="bibr" target="#b23">(Klambauer et al., 2017)</ref> are characterized by having a range of accuracy values across well established methods of greater or equal 0.5 We exclude the problematic datasets seeds, spectf, libras, dermatology, arrythmia, and conn-bench-vowel-deterding. The dataset spect is excluded as its description in <ref type="bibr" target="#b13">(Fern?ndez-Delgado et al., 2014)</ref> is in conflict with the available UCI version regarding the number of attributes and samples. The dataset heart-hungarian is excluded as the dataset description is insufficient to distinguish between categorical and continuous attributes, which is required by some methods. Since breast-cancer-wisc is practically solved (0.9859 accuracy), it is excluded as it does not allow to distinguish the performances of the compared methods. We drop heart-va, since the best reported method has only a low accuracy of 0.4. Below we give more precise descriptions of the datasets used in our small-sized experiments: conn-bench-sonar-mines-rocks or Connectionist Bench (Sonar, Mines vs. Rocks): A classification setting of 208 instances with 60 continuous features per instance. The task is to discriminate between sonar sounds from metal vs. rocks. glass or Glass Identification: A classification setting of 214 instances with 9 continuous features per instance. The task is to discriminate between 6 types of glass. statlog-heart: A classification setting of 270 instances with 6 continuous and 7 categorical features per instance. The task is to predict the presence or absence of a heart disease. breast-cancer: A classification setting of 286 instances with 9 categorical features per instance. The task is to predict the presence or absence of breast cancer. heart-cleveland or Heart Disease: A classification setting of 303 instances with 6 continuous and 7 categorical features per instance. The task is to predict the presence or absence of a heart disease. haberman-survival: A classification setting of 306 instances with 3 continuous features per instance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Small-Sized Dataset Description</head><p>The task is to predict whether patients survived longer than 5 years or not. vertebral-column2, vertebral-column3 or Vertebral Column Dataset: Two classification settings of 310 instances each with 6 continuous features per instance. The task is to classify patients into either 2 or 3 classes. primary-tumor: A classification setting of 330 instances with 17 categorical features per instance.</p><p>The task is to predict the class of primary tumors. ecoli: A classification setting of 336 instances with 5 continuous and 2 categorical features per instance. The tasks is to classify proteins into 8 classes. horse-colic: A classification setting of 368 instances with 8 continuous and 19 categorical features per instance. The task is to predict the survival or death of a horse. congressional-voting: A classification setting of 435 instances with 16 categorical features per instance. The task is to predict political affiliation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>cylinder-bands:</head><p>A classification setting of 512 instances with 20 continuous and 19 categorical features per instance. The task is to classify the band type.</p><p>credit-approval: A classification setting of 690 instances with 6 continuous and 9 categorical features per instance. The task is to determine positive or negative feedback for credit card applications.</p><p>blood-transfusion or Blood Transfusion Service Center: A classification setting of 748 instances with 4 continuous and 1 categorical feature per instance. The task is to predict whether a person donated blood or not.</p><p>statlog-german-credit: A classification setting of 1,000 instances with 23 continuous features per instance. The goal is to determine credit-worthiness of customers.</p><p>mammographic or Mammographic Mass: A classification setting of 961 instances with 1 continuous and 5 categorical features per instance. The task is to discriminate between benign and malignant mammographic masses.</p><p>led-display: A classification setting of 1,000 instances with 6 categorical features per instance. The task is to classify decimal digits from light-emiting diodes with noise.</p><p>statlog-australian-credit: A classification setting of 690 instances with 5 continuous and 9 categorical features. The task to grant customers credit-approval or not.</p><p>energy-y2 or Energy efficiency Data Set: A classification setting of 768 instances with 7 continuous features per instance. The task is to predict the cooling load for a given building.</p><p>monks-2 It is part of the Monk's Problems Data Set. A classification task for 601 instances with 6 categorical features. The task is to discriminate between two classes. Below we give more precise descriptions of the datasets used in our medium-sized experiments: shrutime: A classification setting of 10,000 instances with 2 continuous and 9 categorical features per instance. The task is to predict whether a bank account is closed or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 Medium-Sized Dataset Description</head><p>blastchar: A classification setting of 7,048 instances with 3 continuous and 17 categorical features per instance. The task is to predict customer behavior.</p><p>gesture or gesture-phase or Gesture Phase Segmentation: A classification setting of 9,873 instances with 31 continuous features per instance. The task is to classify gesture phases.</p><p>eye or eye-movements: A classification setting of 10,936 instances with 19 continuous and 3 categorical features per instance. The task is to discriminate between correct, irrelevant or relevant answers.</p><p>colleges: A regression setting of 7,064 instances with 33 continuous and 12 categorical features per instance. The task is to predict pell grant percentages for colleges in the USA.</p><p>sulfur: A regression setting of 10,082 instances with 5 continuous features per instance. The task is to predict H2S concentration in a factory module. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Hyperparameter selection process</head><p>For the hyperparameter selection process for NPTs we follow <ref type="bibr" target="#b25">(Kossen et al., 2021)</ref> and take exactly the same hyperparameter settings that were successfully used among several datasets. We use these hyperparameter settings for experiments on small-and medium-sized datasets. For small-sized datasets we additionally use these settings with an increased embedding dimension of 128. Especially for such datasets the discrimination among similar samples can be a challenging task. This problem can be mitigated by mapping to a higher-dimensional embedding space where the samples have greater distances between each other. NPTs follow a masking procedure similar to  which is realized by feature and label masking probabilities. Following the strategy in <ref type="bibr" target="#b25">(Kossen et al., 2021)</ref> we use the LAMB <ref type="bibr">(You et al., 2020)</ref> optimizer for all NPT experiments, extended by a Lookahead <ref type="bibr" target="#b50">(Zhang et al., 2019a)</ref> wrapper with fixed values. For LAMB we use ? L = (0.9, 0.999), = 1e?6 and for Lookahead ? = 0.5, k = 6. The hyperparameter settings for NPTs are shown in <ref type="table" target="#tab_8">Table A</ref>.5. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Memory footprint and runtime estimates</head><p>In table A.8 we show the memory footprint of Hopular and NPTs for all medium-sized datasets ranging from the smallest to the largest model. In all cases the whole training set is stored in the memory of module H s . Even in the full batch setting where all the data is used as model input there is no prohibitive memory increase. In contrast, NPTs have a much higher memory memory consumption in the full batch setting. There, for 3 datasets the larger models even run out of memory on an Nvidia A100 GPU. In table A.9 we perform measurements on training and inference times. We show the step time for medium-sized datasets during training. Inference times are assumed to be much lower, as no gradient computation and parameter updates need to be performed. In our first example we consider Nadaraya-Watson kernel regression <ref type="bibr" target="#b44">(Watson, 1964;</ref><ref type="bibr" target="#b28">Nadaraya, 1964;</ref><ref type="bibr" target="#b3">Benedetti, 1977;</ref><ref type="bibr" target="#b45">Weinberger &amp; Tesauro, 2007)</ref>. The training set is {(z 1 , y 1 ), . . . , (z N , y N )} with inputs z i summarized by the input matrix Z = (z 1 , . . . , z N ) and labels y i summarized in the label matrix Y = (y 1 , . . . , y N ). The kernel function is k(z i , z). The estimator g for y given z is:</p><formula xml:id="formula_12">g(z) = N i=1 y i k(z i , z) N i=1 k(z i , z) .<label>(11)</label></formula><p>By using the RBF kernel we get:</p><formula xml:id="formula_13">k(z i , z j ) = exp(? ?/2 z i ? z j 2 ) = exp(? ?/2 (z T i z i ? 2 z T i z j + z T j z j )) .<label>(12)</label></formula><p>For normalized vector z i we have z T i z i = z i 2 = 1, therefore k(z i , z j ) = exp(? ? (1 ? z T i z j )) = c exp(? z T i z j ) .</p><p>We obtain for Nadaraya-Watson kernel regression with the RBF kernel and normalized inputs:</p><formula xml:id="formula_15">g(z) = Y softmax(? Z T z) .<label>(14)</label></formula><p>Metric learning for kernel regression learns the kernel k which is the distance function <ref type="bibr" target="#b45">(Weinberger &amp; Tesauro, 2007)</ref>. A Hopular Block can do the same in Eq. 7 via learning the weight matrices W X and W ? . If we set in Eq. 14:</p><formula xml:id="formula_16">Z T = X T W T X , z = W ? ?, Y = W S W X X<label>(15)</label></formula><p>then we obtain Eq. 7, with the fixed label matrix Y .</p><p>In the second example we show how Hopular can realize a linear model with the AdaBoost Objective. The AdaBoost objective for classification with a binary target y ? {?1, +1} can be written as follows -see Eq. 3 and Eq. 4 in <ref type="bibr" target="#b37">(Shen &amp; Li, 2010)</ref>:</p><formula xml:id="formula_17">L = ln N i=1</formula><p>exp(? y i g(z i )) .</p><p>We use this objective for learning the linear model:</p><formula xml:id="formula_19">g(z i ) = ? ? T z i .<label>(17)</label></formula><p>The objective multiplied by ? ?1 with Y as the diagonal matrix of the targets {y 1 , ? ? ? , y N } becomes:</p><formula xml:id="formula_20">L = ? ?1 ln N i=1 exp(? ? y i ? T z i ) = lse(? , ? Y Z T ?) ,<label>(18)</label></formula><p>where lse is the log-sum-exponential function. The gradient of this objective is:</p><formula xml:id="formula_21">?L ?? = ? Z Y softmax(? ? Y Z T ?) .<label>(19)</label></formula><p>This is Eq. 7 with:</p><formula xml:id="formula_22">Y Z T = X T W T X , W ? = I, W S = I<label>(20)</label></formula><p>Thus, a Hopular Block can implement a gradient descent update rule for a linear classification model using the AdaBoost objective function. The current prediction ? comes from the previous layer.</p><p>These are two additional examples among the standard iterative learning algorithms which Hopular can mimic.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Source code</head><p>Source code is available at: https://github.com/ml-jku/hopular</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :Figure 2 :</head><label>12</label><figDesc>Architecture overview of Hopular. Hopular consists of three different types of layers or blocks. (I) Embedding Layer-each attribute of an original input sample is represented in an e-dimensional space. The original input sample itself is then represented by the concatenation of all of its attribute representations. (II) Hopular Block-the input representation is then refined by L consecutive Hopular blocks. This is achieved by applying the two Hopfield modules H s and H f in an alternating way. (III) Summarization Layer-lastly, this refined current prediction is summarized by an attribute-wise mapping, leading to the final prediction. A Hopular Block. The first Hopfield module stores the whole training set and identifies sample-sample relations. The second Hopfield module stores the embedded input features and extracts feature-feature and feature-target relations. The Hopfield modules refine the current prediction by combining the aggregated retrievals of the M Hopfield networks with their respective input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Layer. All attributes of an original input sample are mapped to an edimensional embedding space. The position of an attribute within a sample and the attribute type are conserved by separate e-dimensional embeddings. All three embedding vectors are summed and serve as the final representation of an input attribute. The input sample is represented by the concatenation of all its attribute representations. summarization reshape Figure A.4: Summarization Layer. The current prediction vector on the right is mapped to the final prediction vector on the left by separately mapping each current attribute prediction to its respective final prediction. This final prediction vector lives in the same space as the original input sample and is used for the computation of the respective losses.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>. Hence, the Hopfield module H s identifies sample-sample relations and can perform similarity searches like a nearest-neighbor search in the whole training data. H s can also average over training data that are similar to a mapping of the current prediction vector ?.Next, we describe Hopfield Module H s in more detail. Let d be the number of attributes, e the embedding dimension of each single attribute, h the dimension of the Hopfield embedding space, and n the number of samples in the training set. The forward-pass for module H s with one Hopfield network and current prediction vector ? ? R d?e , learned weight matrices W ? , W X ? R h?(d?e) , W S ? R (d?e)?h , the stored training set X ? R (d?e)?n , and a fixed scaling parameter ? is given as</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>converges to, hence it may further amplify the nearest-neighbor-lookup of the sample-sample Hopfield module H s . H s may contain more than one continuous modern Hopfield network. In this case, the respective results are combined and projected, serving as the modules final output. We have M separate Hopfield networks H i s , where the module output is defined as</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The second Hopfield module H f implements a modern Hopfield network for Deep Learning architectures via the layer Hopfield<ref type="bibr" target="#b33">(Ramsauer et al., 2021</ref> with the embedded features of the original input sample as stored patterns. The refined prediction vector from the previous layer is reshaped and transposed to the matrix ?, which serves as input to the Hopfield module H f . ? interacts with the embedded features of the original input sample as described in Eq. (9). Again, this is the update rule of continuous modern Hopfield networks as given in Eq. (2). Therefore, the Hopfield module H f extracts and models feature-feature and feature-target relations. Current feature and target predictions are adjusted and refined after they are associated with the original input sample feature representations.Next, we describe Hopfield Module H f in more detail. The matrix ? ? R e?d is a transposed and reshaped version of current prediction vector ? with respect to the embedding dimension e. Using the learned weight matrices W ? , W Y ? R h?e , W F ? R e?h , the embedded original input sample Y ? R e?d , and a fixed scaling parameter ? the forward-pass is</figDesc><table><row><cell>with vector H 1 s (?)</cell><cell>T , . . . , H M s (?) T</cell><cell>T</cell><cell>and a learnable weight matrix W</cell></row></table><note>G ? R (d?e)?(M ?d?e) .(II) Hopfield Module H f .</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>for H s .Hopular architecture and Modern Hopfield Networks. Deep Learning could not convince so far on small tabular datasets, on the other hand iterative learning algorithms, like Gradient Boosting methods, are the best-performing methods in this domain. Therefore, we introduce a DL architecture that is able to mimic and extend these iterative algorithms by reaccessing the whole training set and refining the current prediction in each layer. Modern Hopfield Networks directly access an external memory in a content-based fashion as depicted in Eq. (2). Hopular populates this external memory in two different ways: (a) Hopular uses the training set as an external memory, and (b) Hopular uses the embedded feature representations of the original input sample as external memory. During training, retrieval from the respective memory is learned whereas the type of fixed point of the modern Hopfield network, as described in Section 2, specifies the type of retrieved pattern. Additionally, modern Hopfield networks can retrieve patterns with only one update -see Theorem 2.1.Furthermore, their exponential storage capacity (Theorem 2.3) makes it possible to retrieve patterns from external memories with even hundreds of thousands instances. Because of these properties Hopular can mimic iterative learning algorithms e.g. such based on gradient descent, boosting, or feature selection that refine the current prediction by re-accessing the training set in contrast to other Deep Learning methods for tabular data. Both NPTs and SAINT consider feature-feature and sample-sample interactions via their respective attention mechanisms which solely use the result of the previous layer. In contrast, Hopular not only uses the result of the previous layer but also the original input sample and the whole training set. For example, our method can implement gradient boosting with a boosting step at each layer. The ability to mimic iterative learning algorithms that are known to perform specifically well on tabular data makes modern Hopfield networks a promising approach for processing tabular data. For the instantiation variant that we use for our experiments the Hopfield module H s identifies sample-sample relations and can perform similarity searches like a nearest-neighbor search in the whole training data. In the Appendix in Section A.6 we give further intuition of how Hopular can mimic iterative learning algorithms on the basis of two examples.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Results of all compared methods on the subset of medium-sized tabular datasets<ref type="bibr" target="#b38">(Shwartz-Ziv &amp; Armon, 2021)</ref>. For classification tasks (C), the accuracy is reported. For regression tasks (R), the mean squared error multiplied by a factor of 1000 is reported. The reported deviations are the corresponding standard error of the mean. All values are computed on the respective test sets, averaged over three replicates. ? 0.09 25.67 ? 0.23 30.47 ? 0.00 26.40 ? 0.09 25.64 ? 0.09 eye (C) 53.56 ? 0.48 53.21 ? 0.12 57.43 ? 0.00 56.35 ? 0.05 57.34 ? 0.28 gesture (C) 71.20 ? 0.19 67.83 ? 0.06 68.05 ? 0.00 68.86 ? 0.21 69.01 ? 0.09 blastchar (C) 80.05 ? 0.11 79.98 ? 0.11 76.78 ? 0.00 80.13 ? 0.12 79.92 ? 0.21 shrutime (C) 86.12 ? 0.09 85.62 ? 0.07 84.58 ? 0.00 86.39 ? 0.04 86.18 ? 0.02</figDesc><table><row><cell>Dataset</cell><cell>Hopular</cell><cell>NPTs</cell><cell>XGBoost</cell><cell>CatBoost</cell><cell>LightGBM</cell></row><row><cell>sulfur (R) colleges (R)</cell><cell>1.04 ? 0.02 21.18</cell><cell>1.24 ? 0.02</cell><cell>1.23 ? 0.00</cell><cell>1.06 ? 0.01</cell><cell>1.16 ? 0.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Table A.3: Overview of small-sized datasets with their number of instances, number of continuous features, and number of categorical features. All small-sized datasets are classification tasks.</figDesc><table><row><cell>Dataset</cell><cell>Size (N )</cell><cell># cont. features</cell><cell># cat. features</cell></row><row><cell>conn-bench</cell><cell>208</cell><cell>60</cell><cell>0</cell></row><row><cell>glass</cell><cell>214</cell><cell>9</cell><cell>0</cell></row><row><cell>statlog-heart</cell><cell>270</cell><cell>6</cell><cell>7</cell></row><row><cell>breast-cancer</cell><cell>286</cell><cell>0</cell><cell>9</cell></row><row><cell>heart-cleveland</cell><cell>303</cell><cell>6</cell><cell>9</cell></row><row><cell>haberman-survival</cell><cell>306</cell><cell>3</cell><cell>0</cell></row><row><cell>vertebral-column2</cell><cell>310</cell><cell>6</cell><cell>0</cell></row><row><cell>vertebral-column3</cell><cell>310</cell><cell>6</cell><cell>0</cell></row><row><cell>primary-tumor</cell><cell>330</cell><cell>0</cell><cell>17</cell></row><row><cell>ecoli</cell><cell>336</cell><cell>5</cell><cell>0</cell></row><row><cell>horse-colic</cell><cell>368</cell><cell>8</cell><cell>19</cell></row><row><cell>congressional-voting</cell><cell>435</cell><cell>0</cell><cell>16</cell></row><row><cell>cylinder-bands</cell><cell>512</cell><cell>20</cell><cell>19</cell></row><row><cell>monks-2</cell><cell>601</cell><cell>6</cell><cell>0</cell></row><row><cell>statlog-australian-credit</cell><cell>690</cell><cell>5</cell><cell>9</cell></row><row><cell>credit-approval</cell><cell>690</cell><cell>6</cell><cell>9</cell></row><row><cell>blood-transfusion</cell><cell>748</cell><cell>4</cell><cell>1</cell></row><row><cell>energy-y2</cell><cell>768</cell><cell>7</cell><cell>0</cell></row><row><cell>mammographic</cell><cell>961</cell><cell>1</cell><cell>5</cell></row><row><cell>led-display</cell><cell>1,000</cell><cell>0</cell><cell>6</cell></row><row><cell>statlog-german-credit</cell><cell>1,000</cell><cell>23</cell><cell>0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table A</head><label>A</label><figDesc></figDesc><table><row><cell cols="4">.4: Medium-sized datasets with their number of instances, number of continuous features,</cell></row><row><cell cols="4">and number of categorical features. Classification tasks are marked with (C), whereas regression tasks</cell></row><row><cell>are marked with (R).</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Size (N )</cell><cell># cont. features</cell><cell># cat. features</cell></row><row><cell>blastchar (C)</cell><cell>7,048</cell><cell>3</cell><cell>17</cell></row><row><cell>colleges (R)</cell><cell>7,064</cell><cell>33</cell><cell>12</cell></row><row><cell>gesture-phase (C)</cell><cell>9,873</cell><cell>31</cell><cell>0</cell></row><row><cell>shrutime (C)</cell><cell>10,000</cell><cell>2</cell><cell>9</cell></row><row><cell>sulfur (R)</cell><cell>10,082</cell><cell>5</cell><cell>0</cell></row><row><cell cols="2">eye-movements (C) 10,936</cell><cell>19</cell><cell>3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table A</head><label>A</label><figDesc>.5: Complete listing of all evaluated hyperparameter settings for NPTs. For all experiments a learning rate of 0.001 as well as a dropout probability of 0.1 is used. Settings marked with an asterisk (*) are not performed on conn-bench-sonar-mines-rocks due to out-of-memory issues.</figDesc><table><row><cell>dataset</cell><cell># netw.</cell><cell># att.</cell><cell>label mask.</cell><cell>feature mask.</cell><cell>learn. rate</cell><cell>emb.</cell></row><row><cell>group</cell><cell>layers</cell><cell>heads</cell><cell>prob.</cell><cell>prob.</cell><cell>scheduler</cell><cell>dim.</cell></row><row><cell></cell><cell>8</cell><cell>8</cell><cell>1.0</cell><cell>0.15</cell><cell>cosine</cell><cell>32</cell></row><row><cell></cell><cell>16</cell><cell>8</cell><cell>1.0</cell><cell>0.15</cell><cell>cosine</cell><cell>32</cell></row><row><cell></cell><cell>8</cell><cell>16</cell><cell>1.0</cell><cell>0.15</cell><cell>cosine</cell><cell>32</cell></row><row><cell>small and</cell><cell>16</cell><cell>16</cell><cell>1.0</cell><cell>0.15</cell><cell>cosine</cell><cell>32</cell></row><row><cell>medium</cell><cell>8</cell><cell>8</cell><cell>0.1</cell><cell>0.15</cell><cell>cosine</cell><cell>32</cell></row><row><cell></cell><cell>8</cell><cell>8</cell><cell>0.5</cell><cell>0.15</cell><cell>cosine</cell><cell>32</cell></row><row><cell></cell><cell>8</cell><cell>8</cell><cell>1.0</cell><cell>0.20</cell><cell>cosine</cell><cell>32</cell></row><row><cell></cell><cell>8</cell><cell>8</cell><cell>1.0</cell><cell>0.15</cell><cell>cosine cyclic</cell><cell>32</cell></row><row><cell></cell><cell>8</cell><cell>8</cell><cell>1.0</cell><cell>0.15</cell><cell>cosine</cell><cell>128</cell></row><row><cell></cell><cell>16</cell><cell>8</cell><cell>1.0</cell><cell>0.15</cell><cell>cosine</cell><cell>128 *</cell></row><row><cell></cell><cell>8</cell><cell>16</cell><cell>1.0</cell><cell>0.15</cell><cell>cosine</cell><cell>128</cell></row><row><cell>small</cell><cell>16 8</cell><cell>16 8</cell><cell>1.0 0.1</cell><cell>0.15 0.15</cell><cell>cosine cosine</cell><cell>128 * 128</cell></row><row><cell></cell><cell>8</cell><cell>8</cell><cell>0.5</cell><cell>0.15</cell><cell>cosine</cell><cell>128</cell></row><row><cell></cell><cell>8</cell><cell>8</cell><cell>1.0</cell><cell>0.20</cell><cell>cosine</cell><cell>128</cell></row><row><cell></cell><cell>8</cell><cell>8</cell><cell>1.0</cell><cell>0.15</cell><cell>cosine cyclic</cell><cell>128</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table A .</head><label>A</label><figDesc>6: Complete listing of all evaluated hyperparameter settings for Hopular. For all experiments a learning rate of 0.001 was used. The dropout probabilities p i , p h and p o refer to the embedding layer, Hopular Block and summarization layer, respectively. The three settings of the second group (medium-sized) were performed in a non-exhaustive way w.r.t. to all medium-sized datasets.For a fair comparison we upper bound Hopular's capacity by the capacity of NPTs which results in the settings shown in Table A.6. As Hopular provides an additional adjustable scaling factor for ?, we also test scaling factors of 100 and 1000 to further emphasize nearest-neighbor search. In our ? Colsample bytree: Uniform distribution [0.5, 1]? Colsample bylevel: Uniform distribution [0.5, 1]? Alpha: Log-Uniform distribution [log 0.0001, log 1]InTable A.7 we show the median rank across all 21 selected UCI datasets. Methods are ranked for each dataset according to their accuracy on the respective test set.Table A.7: Median rank of compared methods across the datasets of the UCI machine learning repository. Methods are ranked for each dataset according to the accuracy on the respective test set. Hopular achieves the lowest median rank of 7.5, therefore is the best performing method across the considered UCI datasets.</figDesc><table><row><cell cols="5">? Lambda: Log-Uniform distribution [log 1, log 4]</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">? Boosting type: Uniform choice {gbdt, dart, goss} ? Number of estimators: 1000</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>A.4 Results</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Rank Method</cell><cell></cell><cell></cell><cell>Rank</cell><cell></cell></row><row><cell cols="2">Hopular (DL)</cell><cell></cell><cell></cell><cell cols="3">7.5 Rule-Based Methods</cell><cell></cell><cell>15.0</cell><cell></cell></row><row><cell cols="3">Support Vector Machines</cell><cell></cell><cell cols="3">9.5 Other Ensembles</cell><cell></cell><cell>15.0</cell><cell></cell></row><row><cell cols="7">Logistic and Multinomial Regression 10.0 BatchNorm (DL)</cell><cell></cell><cell>15.0</cell><cell></cell></row><row><cell cols="2">Random Forest</cell><cell></cell><cell></cell><cell cols="3">11.0 Boosting Methods</cell><cell></cell><cell>15.0</cell><cell></cell></row><row><cell cols="4">Self-Normalizing Networks (DL)</cell><cell cols="5">11.0 Generalized Linear Models 15.5</cell><cell></cell></row><row><cell cols="4">Non-Parametric Transformers (DL)</cell><cell cols="3">11.0 WeightNorm (DL)</cell><cell></cell><cell>15.5</cell><cell></cell></row><row><cell cols="3">Neural Networks (DL)</cell><cell></cell><cell cols="4">11.5 Discriminant Analysis</cell><cell>16.0</cell><cell></cell></row><row><cell cols="2">XGBoost</cell><cell></cell><cell></cell><cell cols="3">12.0 Other Methods</cell><cell></cell><cell>17.5</cell><cell></cell></row><row><cell cols="4">Multivariate Adaptive Reg. Splines</cell><cell cols="3">12.0 ResNet (DL)</cell><cell></cell><cell>19.0</cell><cell></cell></row><row><cell cols="2">Decision Trees</cell><cell></cell><cell></cell><cell cols="3">13.5 LayerNorm (DL)</cell><cell></cell><cell>19.0</cell><cell></cell></row><row><cell cols="2">MSRAinit (DL)</cell><cell></cell><cell></cell><cell cols="3">14.0 Partial Least Squares</cell><cell></cell><cell>19.5</cell><cell></cell></row><row><cell cols="2">Bagging Methods</cell><cell></cell><cell></cell><cell cols="3">14.0 Bayesian Methods</cell><cell></cell><cell>20.0</cell><cell></cell></row><row><cell cols="2">CatBoost</cell><cell></cell><cell></cell><cell cols="3">14.0 Nearest Neighbour</cell><cell></cell><cell>24.0</cell><cell></cell></row><row><cell cols="2">LightGBM</cell><cell></cell><cell></cell><cell cols="3">14.5 Stacking (Wolpert)</cell><cell></cell><cell>28.0</cell><cell></cell></row><row><cell cols="3">Highway Networks (DL)</cell><cell></cell><cell>14.5</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>dataset</cell><cell># Hop.</cell><cell># Hop.</cell><cell>?-scaling</cell><cell>mask</cell><cell>replace</cell><cell>weight</cell><cell></cell><cell>dropout</cell><cell></cell></row><row><cell>group</cell><cell>blocks</cell><cell>nets</cell><cell>factor</cell><cell>prob.</cell><cell>prob.</cell><cell>decay</cell><cell>p i</cell><cell>p h</cell><cell>p o</cell></row><row><cell></cell><cell>4</cell><cell>8</cell><cell cols="2">10 {0,2,3} 0.025</cell><cell>0.175</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.01</cell></row><row><cell>small and</cell><cell>8</cell><cell>8</cell><cell cols="2">10 {0,2,3} 0.025</cell><cell>0.175</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.01</cell></row><row><cell>medium</cell><cell>4</cell><cell>16</cell><cell cols="2">10 {0,2,3} 0.025</cell><cell>0.175</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.01</cell></row><row><cell></cell><cell>8</cell><cell>16</cell><cell cols="2">10 {0,2,3} 0.025</cell><cell>0.175</cell><cell>0.1</cell><cell>0.1</cell><cell>0.1</cell><cell>0.01</cell></row><row><cell>medium</cell><cell>8</cell><cell>16</cell><cell>10 {0}</cell><cell>0.000</cell><cell>0.000</cell><cell>0.0</cell><cell>0.0</cell><cell>0.0</cell><cell>0.00</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table A .</head><label>A</label><figDesc>8: Memory footprint of Hopular and NPTs in gibibytes (GiB) for medium-sized datasets ranging from our smallest to largest model. Settings with a memory footprint of 80.00+ are not performed due to out-of-memory issues.</figDesc><table><row><cell>Dataset</cell><cell>Hopular single sample full batch</cell><cell>NPTs single sample</cell><cell>full batch</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table A</head><label>A</label><figDesc>.9: Step time of Hopular and NPTs in milliseconds (ms) during training.</figDesc><table><row><cell>Dataset</cell><cell>Hopular single sample full batch</cell><cell cols="2">NPTs single sample</cell><cell>full batch</cell></row><row><cell>blastchar (C) colleges (R) gesture-phase (C) shrutime (C) sulfur (R) eye-movements (C)</cell><cell cols="2">73.69 ? 0.02 120.15 ? 0.09 95.40 ? 0.03 1,155.47 ? 0.06 503.45 ? 0.08 824.34 ? 0.17 118.13 ? 0.13 81.74 ? 0.11 99.38 ? 0.08 61.90 ? 0.02 652.81 ? 0.04 68.18 ? 0.08 52.71 ? 0.02 629.55 ? 0.04 59.44 ? 0.08 76.94 ? 0.02 1,141.37 ? 0.03 84.21 ? 0.08</cell><cell cols="2">167.26 ? 0.25 321.32 ? 0.25 384.58 ? 0.16 182.11 ? 0.16 159.86 ? 0.28 338.53 ? 0.18</cell></row><row><cell cols="2">A.6 Hopular Intuition: Mimicking Iterative Learning</cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The ELLIS Unit Linz, the LIT AI Lab, the Institute for Machine Learning, are supported by the Federal State Upper Austria. IARAI is supported by Here Technologies. We thank the projects AI-MOTION (LIT-2018-6-YOU-212), AI-SNN (LIT-2018-6-YOU-214), DeepFlood (LIT-2019-8-YOU-213), Medical Cognitive Computing Center (MC3), INCONTROL-RL (FFG-881064), PRIMAL (FFG-873979), S3AI (FFG-872172), DL for GranularFlow (FFG-871302), AIRI FG 9-N (FWF-36284, FWF-36235), ELISE (H2020-ICT-2019-3 ID: 951847). We thank Audi.JKU Deep Learning Center, TGW LOGISTICS GROUP GMBH, Silicon Austria Labs (SAL), FILL Gesellschaft mbH, Anyline GmbH, Google, ZF Friedrichshafen AG, Robert Bosch GmbH, UCB Biopharma SRL, Merck Healthcare KGaA, Verbund AG, Software Competence Center Hagenberg GmbH, T?V Austria, Frauscher Sensonic and the NVIDIA Corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abutbul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Elidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Katzir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dnf-Net</surname></persName>
		</author>
		<ptr target="https://openreview.net/" />
		<title level="m">forum?id=73WTGs96kho. 9th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>A neural architecture for tabular data. ArXiv. ICLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Attentive interpretable tabular learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">?</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tabnet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="6679" to="6687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improving palliative care with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Avati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Downing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<idno>doi: 10.1186/ s12911-018-0677-8</idno>
	</analytic>
	<monogr>
		<title level="j">BMC Medical Informatics and Decision Making</title>
		<imprint>
			<biblScope unit="volume">122</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On the nonparametric estimation of regression functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Benedetti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="248" to="253" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A training algorithm for optimal margin classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory</title>
		<meeting>the 5th Annual ACM Workshop on Computational Learning Theory<address><addrLine>Pittsburgh, PA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM Press</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="144" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Random forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<idno type="DOI">10.1023/A:1010933404324</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="5" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">XGBoost: A scalable tree boosting system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno type="DOI">10.1145/2939672.2939785</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Contrastive Mixup: selfand semi-supervised learning for tabular domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Darabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fazeli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pazoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankararaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sarrafzadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">CatBoost: unbiased boosting with categorical features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Dorogush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gusev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kazeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">O</forename><surname>Prokhorenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vorobev</surname></persName>
		</author>
		<idno>1706.09516</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">TabularNet: A neural network architecture for understanding semantic structures of tabular data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<idno>doi: 10.1145/ 3447548.3467228</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;21</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;21<address><addrLine>New York, NY, USA, 2021</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="322" to="331" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">AutoGluon-Tabular: Robust and accurate AutoML for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Erickson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shirkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Larroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Do we need hundreds of classifiers to solve real world classification problems?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fern?ndez-Delgado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cernadas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amorim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3133" to="3181" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Simple modifications to improve tabular neural networks. ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fiedler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Greedy function approximation: A gradient boosting machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Friedman</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1013203451</idno>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1189" to="1232" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Revisiting deep learning models for tabular data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gorishniy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Rubachev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Khrulkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bootstrap your own latent -a new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">?</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Azar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Piot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Valko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21271" to="21284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Multiplex graph neural network for tabular data prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tabgnn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Random decision forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICDAR.1995.598994</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 3rd International Conference on Document Analysis and Recognition</title>
		<meeting>3rd International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="278" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cvitkovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Karnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tabtransformer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tabular data modeling using contextual embeddings</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Regularization is all you need: Simple neural nets can excel on tabular data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lindauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Grabocka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">LightGBM: A highly efficient gradient boosting decision tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><forename type="middle">;</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Fergus, R., Vishwanathan, S., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>Guyon, I</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="971" to="980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Hyperopt-sklearn: automatic hyperparameter configuration for scikit-learn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Komer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Eliasmith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML workshop on AutoML</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">50</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Self-attention between datapoints: Going beyond individual input-output pairs in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kossen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Band</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rainforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ArXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Large-scale comparison of machine learning methods for drug target prediction on chembl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steijaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ceulemans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="DOI">10.1039/C8SC00148K</idno>
	</analytic>
	<monogr>
		<title level="j">Chemical Science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="5441" to="5451" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On estimating regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Nadaraya</surname></persName>
		</author>
		<idno type="DOI">10.1137/1109020</idno>
	</analytic>
	<monogr>
		<title level="m">Theory of Probability &amp; Its Applications</title>
		<imprint>
			<date type="published" when="1964" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="141" to="142" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">NIST handbook of mathematical functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">W J</forename><surname>Olver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">W</forename><surname>Lozier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">F</forename><surname>Boisvert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">W</forename><surname>Clark</surname></persName>
		</author>
		<imprint>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>1 pap/cdr edition, 2010. ISBN 9780521192255</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Neural oblivious decision ensembles for deep learning on tabular data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Popov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Morozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Babenko</surname></persName>
		</author>
		<ptr target="https://openreview.net/" />
	</analytic>
	<monogr>
		<title level="m">forum?id=r1eiu2VtwH. 8th International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="1909" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CatBoost: unbiased boosting with categorical features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Prokhorenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gusev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vorobev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Dorogush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Bengio, S., Wallach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?fl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Holzleitner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavlovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Sandve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Greiff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kreil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandstetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Hopfield networks is all you need. ArXiv</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Hopfield networks is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?fl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Holzleitner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavlovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Sandve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Greiff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kreil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandstetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=tL89RnzIiCd" />
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep learning in neural networks: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neunet.2014.09.003</idno>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="85" to="117" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Learning with kernels -Support Vector Machines, Regularization, Optimization, and Beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Regularization learning networks: Deep learning for tabular datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shavitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Garnett</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On the dual formulation of boosting algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2010.47</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2216" to="2231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shwartz-Ziv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Armon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Data</surname></persName>
		</author>
		<idno>2106.03253</idno>
		<ptr target="https://openreview.net/" />
		<title level="m">forum?id=vdgtepS1pV. AutoML Workshop of International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Deep learning is not all you need</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Crepurposing high-throughput image assays enables biological activity prediction for drug discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arany</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Steijaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gustin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chupakhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vialard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bujinsters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Velter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vapirev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carpenter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wuyts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Moreau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ceulemans</surname></persName>
		</author>
		<idno>doi: 10.1016/j. chembiol.2018.01.015</idno>
	</analytic>
	<monogr>
		<title level="j">Cell Chemical Biology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="611" to="618" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">SAINT: Improved neural networks for tabular data via row attention and contrastive pre-training. ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Somepalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Goldblum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Schwarzschild</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">B</forename><surname>Bruss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Goldstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Automatic feature interaction learning via self-attentive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Autoint</surname></persName>
		</author>
		<idno type="DOI">10.1145/3357384</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM &apos;19</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management, CIKM &apos;19<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1161" to="1170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">;</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Vishwanathan, S., and Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
	<note>Guyon, I</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Are random forests truly the best classifiers?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wainberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alipanahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3837" to="3841" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Smooth regression analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Watson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sankhya: The Indian Journal of Statistics, Series A</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="359" to="372" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Metric learning for kernel regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tesauro</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics</title>
		<editor>Meila, M. and Shen, X.</editor>
		<meeting>the Eleventh International Conference on Artificial Intelligence and Statistics<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="612" to="619" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Modern Hopfield networks and attention for immune repertoire classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sch?fl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pavlovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Holzleitner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandstetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Sandve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Greiff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">When are deep networks really better than random forests at small sample sizes? ArXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ainsworth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kusmanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Vogelstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Extending the success of self-and semi-supervised learning to tabular domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vanderschaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vime</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M. F., and Lin, H.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="11033" to="11043" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Large batch optimization for deep learning: Training bert in 76 minutes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hseu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demmel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hsieh</surname></persName>
		</author>
		<idno>1904.00962</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Lookahead optimizer: k steps forward, 1 step back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno>1907.08610</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">3d human pose estimation via human structure-aware fully connected network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hao</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patrec.2019.05.020</idno>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<biblScope unit="page" from="404" to="410" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">This is why we independently of BERT masking always mask the corresponding sample in the training set. We use default values for masking and dropout. For the medium-sized datasets we also test two different settings of weight decay, and of dropout probabilities in the Embedding layer, Hopular block and Summarization layer. In contrast to NPTs, we always mask all labels. In our experiments the Hopfield dimension h (as described in Section 3) is fixed by the embedding size e, the number of features d and the number of Hopfield networks M such that h = d ? e/M . The LAMB (You et al., 2020) optimizer is used for all Hopular experiments, extended by a method similar to Lookahead (Zhang et al., 2019a) but without synchronization of fast and slow weights. This is analogous to the exponential moving average used in</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">default setting the weighting term ? for our objective in Eq. (10) is annealed using a cosine scheduler starting at 1 with a final value of 0</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>Since we store the training data in H s we have to make sure that the model does not just learn to retrieve the original input sample from the training set (like a database query). Grill et al., 2020). For LAMB we use ? L = (0.9, 0.999), = 1e?6 and for Lookahead ? = 0.005, k = 1. NPTs and Hopular are both trained for 10,000 epochs with early stopping</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">For all Boosting methods we thereby evaluate 1,000 different hpyerparameter settings. More precisely, the hyperparameters and their search spaces for XGBoost are defined in the following. ? Learning rate: Log-Uniform distribution</title>
		<imprint/>
	</monogr>
	<note>For XGBoost and CatBoost we use the package hyperopt and apply the same Bayesian hyperparameter optimization procedure as described in Shwartz-Ziv &amp; Armon (2021). ?7, 0</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>? Colsample Bytree</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Uniform distribution [0.2, 1</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>? Colsample Bylevel</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>Uniform distribution [0.2, 1</note>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Log-Uniform distribution [?16, 2] ? Alpha: Uniform choice {0, Log-Uniform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>? Min Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weight</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Uniform choice {0, Log-Uniform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Lambda</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Uniform choice {0, Log-Uniform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Gamma</surname></persName>
		</author>
		<imprint>
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Number</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Estimators</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">1000</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">It is important to mention that the package hyperopt defines the Log-Uniform distribution by the exponents of the respective interval boundaries -e.g. Log-Uniform</title>
		<imprint/>
	</monogr>
	<note>?7, 0] is defined on [e ?7 , e 0</note>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">The hyperparameters and their search spaces for CatBoost are defined in the following. ? Learning rate: Log-Uniform distribution</title>
		<imprint/>
	</monogr>
	<note>?5, 0</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">? L2 leaf regularization: Log-Uniform distribution</title>
		<imprint/>
	</monogr>
	<note>log 1, log 10</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">For LightGBM we use the default hyperparameter ranges as specified by hyperopt-sklearn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Komer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<title level="m">? Learning rate: Log-Uniform distribution</title>
		<imprint/>
	</monogr>
	<note>log 0.0001, log 0.5] ? 0.0001</note>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Discrete uniform distribution [2, 121] ? Gamma: Log-Uniform distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Number</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leaves</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>log 0.001, log 5] ? 0.0001</note>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>? Min Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weight</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note type="report_type">Log-Uniform distribution</note>
	<note>log 1, log 100</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Uniform distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Subsample</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>0.5, 1</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
