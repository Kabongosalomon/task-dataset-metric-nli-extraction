<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Hierarchical Transformers Are More Efficient Language Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Nawrot</surname></persName>
							<email>p.nawrot99@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Warsaw</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Szymon</forename><surname>Tworkowski</surname></persName>
							<email>szy.tworkowski@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Warsaw</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?</forename><surname>Tyrolski</surname></persName>
							<email>michal.tyrolski@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Warsaw</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
							<email>lukaszkaiser@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">OpenAI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
							<email>yuhuai@google.com</email>
							<affiliation key="aff2">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
							<email>szegedy@google.com</email>
							<affiliation key="aff2">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henryk</forename><surname>Michalewski</surname></persName>
							<email>henrykm@google.com</email>
							<affiliation key="aff2">
								<orgName type="department">Google Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Hierarchical Transformers Are More Efficient Language Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformer models yield impressive results on many NLP and sequence modeling tasks. Remarkably, Transformers can handle long sequences, which allows them to produce long coherent outputs: entire paragraphs produced by GPT-3 or well-structured images produced by DALL-E. These large language models are impressive but also very inefficient and costly, which limits their applications and accessibility. We postulate that having an explicit hierarchical architecture is the key to Transformers that efficiently handle long sequences. To verify this claim, we first study different ways to downsample and upsample activations in Transformers so as to make them hierarchical. We use the best performing upsampling and downsampling layers to create Hourglass -a hierarchical Transformer language model. Hourglass improves upon the Transformer baseline given the same amount of computation and can yield the same results as Transformers more efficiently. In particular, Hourglass sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efficiency on the widely studied enwik8 benchmark.</p><p>Matej Grci?, Ivan Grubi?i?, and Sini?a ?egvi?. 2021.</p><p>Densely connected normalizing flows.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Transformer models <ref type="bibr">(Vaswani et al., 2017)</ref> are capable of solving many sequence modeling tasks, including classical NLP tasks <ref type="bibr" target="#b10">(Devlin et al., 2019)</ref>, summarization <ref type="bibr">(Zhang et al., 2020)</ref>, language modeling <ref type="bibr" target="#b5">(Radford et al., 2019;</ref><ref type="bibr">Brown et al., 2020)</ref>, code generation <ref type="bibr">(Chen et al., 2021)</ref>, or even music generation <ref type="bibr">(Huang et al., 2018;</ref><ref type="bibr" target="#b11">Dhariwal et al., 2020)</ref> and image generation <ref type="bibr">(Parmar et al., 2018;</ref><ref type="bibr" target="#b2">Chen et al., 2020;</ref><ref type="bibr">Ramesh et al., 2021)</ref>. One compelling feature of Transformers is their ability to handle long contexts given as part of the input. This is particularly visible in tasks where the output depends on parts of the context that may not be * Equal contribution. Order determined by coin toss. close-by in the generated sequence, like in summarization, where the summary may need to refer to information scattered across the context, or in largescale image generation, where pixels belonging to the same object may be far apart in the generation order. Transformers excel at such tasks thanks to self-attention, and they are used with longer and longer contexts. Transformer-XL Hourglass <ref type="figure">Figure 1</ref>: Bits-per-character vs. training cost for baseline (orange) and hierarchical Transformers (green). We observe significant perplexity improvements on en-wik8 over the vanilla Transformer-XL baseline, see text for details.</p><p>The ability of Transformers to handle long contexts comes at a price: each self-attention layer, at least in its original form, has complexity quadratic in the length of the context. When a stack of n Transformer layers is used, both memory and time complexity is equal to O(L 2 n) where L is a sequence length and n number of decoder blocks. Due to this limitation, vanilla transformers are infeasible to train on tasks with very long input sequences, for instance, on high-resolution images. This issue has been studied extensively, and a number of techniques were introduced that modify attention mechanism without changing overall transformer architecture <ref type="bibr" target="#b5">(Child et al., 2019;</ref><ref type="bibr">Roy et al., 2020;</ref><ref type="bibr">Ren et al., 2021)</ref>. These sparse attention mechanisms reduce the complexity of self-attention arXiv:2110.13711v2 <ref type="bibr">[cs.</ref>LG] 16 Apr 2022 but still force the model to operate on the sequence of the same length as the input.</p><p>For generative Transformer models, operating at the original scale of the input sequence is necessary, at least in the early and final layers, as the input must be processed at first and generated at the end <ref type="bibr">(Section 4.3)</ref>. But forcing the models to operate at this granularity throughout the layer stack has both fundamental and practical shortcomings:</p><p>? Fundamentally, we aim for the models to create high-level representations of words, entities, or even whole events -which occur at a very different granularity than single letters that the model receives on input.</p><p>? On the practical side, even layers with linear complexity can be slow and memory-intensive when processing very long sequences.</p><p>To alleviate these issues, we propose to change the Transformer architecture to first shorten the internal sequence of activations when going deeper in the layer stack and then expand it back before generation. We merge tokens into groups using a shortening operation (Section 2.1) and so reduce the overall sequence length, and then up-sample them again combining with the sequence from earlier layers (Section 2.3), The first part is analogous to the Funnel-Transformer architecture <ref type="bibr" target="#b8">(Dai et al., 2020)</ref>, and the whole architecture takes inspiration from U-Nets <ref type="bibr">(Ronneberger et al., 2015)</ref>. In contrast to both these architectures, the model we present is autoregressive, which is harder to ensure in hierarchical models than in vanilla Transformers.</p><p>The resulting model -which we call Hourglassis an autoregressive Transformer language model that operates on shortened sequences. It yields significant performance improvements for different attention types <ref type="figure" target="#fig_1">(Fig. 6,7)</ref>. We tested Hourglass with Transformer-XL <ref type="bibr" target="#b9">(Dai et al., 2019)</ref> and <ref type="bibr">Reformer (Kitaev et al., 2020)</ref> blocks on enwik8 dataset. In both cases, it is not only better in terms of perplexity, but it is faster and uses less memory during training. We also propose a regularization technique for hierarchical Transformers called shorten factor dropout which improves perplexity upon baselines trained with fixed shorten factor (see Section 4.1). Finally, Hourglass achieves the new stateof-the-art among Transformer models for image generation of ImageNet32 (see Tab. 3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Standard self-attention mechanism uses full tokenlevel sequence representations. In the Hourglass, we bring efficiency to the model by utilizing shortening, which allows us to use the Transformer layers on inputs with significantly smaller lengths. A high-level overview of our proposed model architecture is shown in figures 2 and 3.</p><p>Attention type in the vanilla layers and shortened layers is a configurable parameter. By default we use relative attention defined in Transformer-XL <ref type="bibr" target="#b9">(Dai et al., 2019)</ref>. Any attention module can be used -we show significant efficiency gains when applying Hourglass also for LSH <ref type="bibr">(Kitaev et al., 2020)</ref> attention (see Section 3.2 and <ref type="figure" target="#fig_1">Fig. 7</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Methods of shortening the input sequence</head><p>Shortening can be defined as any function S that accepts a tensor x of shape (l, d) and returns a tensor x of shape ( l k , d), where k is a hyperparameter called shorten factor.</p><p>A simple shortening method is 1D average pooling with stride k and pool size k, applied along the sequence dimension l. Another way of shortening is what we will further call linear pooling (l and d denote sequence length and d model ):</p><p>Algorithm 2 LinearPooling</p><formula xml:id="formula_0">x ? Reshape(x, ( l k , k ? d)) x ? LinearP rojection(x )</formula><p>Shortening can be also performed by attention, as was introduced in <ref type="bibr" target="#b8">(Dai et al., 2020)</ref></p><formula xml:id="formula_1">: x = S(x)+ Attention(Q = S(x), K = V = x)</formula><p>where S is shortening function, originally S = AvgP ool. Directly after this attention operation, a positionwise feed-forward with a residual is performed, so that these two layers form a Transformer block <ref type="bibr">(Vaswani et al., 2017)</ref>. In this work we also try S = LinearP ool and find it more effective on image tasks (see Tab. 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Shortening and autoregressive property</head><p>Information leaks Shortening interferes with the standard causal masking used in Transformer decoders. Namely, in any shortened representation by a factor of k each shortened token contributes to predicting up to the next k tokens in the finest scale, that is if e is the shortened sequence and x is the sequence on the finest scale, e 0 is not only used to generate x 0 ; in fact, the same embedding is used to generate tokens x 0 , ..., x k?1 . Therefore, we need to guarantee that e 0 and any other e i cannot access information about tokens they will implicitly predict. To ensure that, we apply another shift right by k ? 1 tokens, directly before any shortening by a factor of k <ref type="figure">(Fig. 4)</ref>. The shift is the smallest that does not cause an information leak (see <ref type="figure">Fig. 5</ref> for an example of a shifting that leads to a leak). We included a more detailed analysis of this fact in the Appendix (Section A.2).</p><p>Reduced expressivity Let us consider an Hourglass model with shortening by a factor of k and no transformer blocks operating on the finest scale (that is, a model without vanilla layers).</p><p>In this situation P (x) = n?1 i=0 P (x i |e 0 , ..., e i k ) = n?1 i=0 P (x i |x 0 , ..., x i k ?k?1 ) because for predicting x i we combine the processing done on shortened representations e with token-independent operations. This means token x i is generated independently from the tokens x i k ?k , ..., x i?1 . This situation is detrimental to the model's capabilities, though including at least one vanilla layer solves this issue. In the Appendix we provide a detailed example illustrating this problem (Section A.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Upsampling methods</head><p>Upsampling is a crucial part of the Hourglass architecture since we need to convert shortened representations back to the full token-level sequence in order to perform language modeling. A method proposed in <ref type="bibr" target="#b8">(Dai et al., 2020</ref>) is repeating each shortened vector shorten factor times. This method is computationally efficient, but it does not distinguish tokens with respect to position inside the group.</p><p>Another method is linear upsampling which works analogously to linear pooling -it projects vectors of shape ( l k , d) to ( l k , k ? d) and then reshapes to l vectors, each of dimension d. This method is fast and allows to project shortened embeddings differently for each position in the group. This happens because the (k ? d) ? d projection matrix can be thought of as k separate d ? d matrices, one per each position.</p><p>We also investigated a method which we further call attention upsampling. It is similar to attention pooling <ref type="bibr" target="#b8">(Dai et al., 2020)</ref> and to the aggregation layer from <ref type="bibr">(Subramanian et al., 2020)</ref>. It works as follows: x = U (x, x ) + Attention(Q = U (x, x ), K = V = x ) where x are embeddings from just before the shortening, x are final shortened embeddings and U is an arbitrary upsampling function. After the attention operation there is also a residual with a feed-forward layer.</p><p>Linear upsampling learns a fixed pattern that is the same for each shortened token. Attention upsampling has the advantage of being contentbased -each token can extract relevant information from the shortened embeddings. We set U (x, x ) = x + LinearU psampling(x ) which allows to explicitly inject group-level information into the attention queries. We experimentally show that variants of attention upsampling lead to the best results for our model across different datasets (see Tab. 7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 HourglassLM</head><p>procedure</p><formula xml:id="formula_2">HOURGLASS(x, [k, ...s_f actors]) x ? P reV anillaLayers(x) x ? Shortening(Shif tRight(x, k?1), k) if EMPTY(s_f actors) then x ? ShortenedLayers(x ) else x ? HOURGLASS(x , s_f actors) end if x ? x + U psampling(x, x , k) x ? P ostV anillaLayers(x) return x Figure 3:</formula><p>The architecture starts with pre vanilla layers -a stack of Transformer blocks operating on the full token-level sequence. After them we insert shortening layer where k is the shorten factor parameter ( <ref type="figure">Fig. 4)</ref>. The sequence is shifted right before shortening to prevent information leak ( <ref type="figure">Fig. 5</ref>). Then we recursively insert another Hourglass block operating on k times smaller scale. On the final level of shortening, we apply shortened layers -Transformer blocks operating on the smallest scale. Upsampling layer brings the resulting activations x back to the original resolution. After upsampling and residual, the activations are processed by token-level post vanilla layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>In this section, we present experimental results of Hourglass. We start with a quick analysis of time and memory complexity of the approach (Section 3.1). Then we investigate the efficiency gains of applying Hourglass to Transformers with different attention types (Section 3.2). Finally, we use Hourglass with relative attention parametrization from Transformer-XL <ref type="bibr" target="#b9">(Dai et al., 2019)</ref>, evaluate it on three language modeling tasks, and compare the results with other models. (Sections 3.3, 3.4)</p><p>To show cross-domain generalization of our method, we train our model on one dataset related to Natural Language Processing and two from the Computer Vision field.</p><p>To ensure consistency in presenting configurations of our model, we introduce a notation describing hierarchy of our architecture: (N 1 @f 1 , . . . , N k @f k ) where each entry (N j @f j ) means N j layers shortened by factor f j .</p><p>Our  <ref type="figure">Figure 4</ref>: An overview of our shortening approach. Different colors denote token positions. Initially, we shift right by one, which is a standard step in TransformerLM. Then, just before performing shortening, we additionally shift the tokens right by shorten factor ? 1 to preserve the autoregressive property of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Shortening</head><p>ShiftRight(sf-2) Upsampling <ref type="figure">Figure 5</ref>: An example of information leak. If the shift right factor is too small, after upsampling the knowledge from the next tokens leaks to previous ones violating autoregressiveness and making decoding impossible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Computational cost analysis</head><p>In vanilla Transformers, the number of parameters can indicate the computation required to train the model. This is not true for Hourglass -for instance, it can have 128 layers operating on a sequence shortened by 32 and still fit into the memory of a single GPU. A weak correlation between true Hourglass' computational cost and its number of parameters can be observed in <ref type="table">Table 1</ref>.</p><p>Hourglass achieves the biggest speedup with the standard O(l 2 ) attention. In that case, a single shortening by a shorten factor k reduces the complexity to O( l 2 k 2 ) so by a factor of k 2 . For more recent linear-time attention mechanisms <ref type="bibr">(Katharopoulos et al., 2020;</ref><ref type="bibr" target="#b6">Choromanski et al., 2021)</ref> the reduction would be smaller -but still by a factor of k. Feed-forward layers also have linear complexity so shortening reduces it by a factor of k.</p><p>In <ref type="table">Table 1</ref> we show an empirical efficiency comparison between Hourglass and Transformer-XL. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Impact of Hourglass</head><p>To demonstrate the efficiency of Hourglass, we measured how computational cost decreases and perplexity improves, purely adding the technique to Transformer-XL <ref type="bibr" target="#b9">(Dai et al., 2019)</ref> and Reformer (Kitaev et al., 2020) backbones (results depicted in <ref type="figure" target="#fig_1">Figures 6 and 7</ref>, respectively). In both cases, models are implemented under the same codebase and the only difference between Hourglass and its corresponding baseline is the usage of shortening and upsampling layers. We show that by incorporating a single shortening of the input, we can train larger models with the same memory requirements and training speed and achieve better perplexity than baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Enwik8</head><p>Enwik8 (Mahoney, 2011) is a byte-level language modeling benchmark containing the first 100M bytes of unprocessed English Wikipedia text, split into 90M train, 5M valid, and 5M test sets.</p><p>Similarly to <ref type="bibr" target="#b9">(Dai et al., 2019)</ref> and <ref type="bibr" target="#b0">(Beltagy et al., 2020)</ref>, we evaluate our model on the test set, splitting it into overlapping sequences of size l = 4096 with a step size of 128 and calculate the test loss only over the last 128 tokens. With a (4@1, 8@3, 4@1) hierarchy, d model = 768, d f f = 3072 and 8 heads, we reach 0.98 test bitsper-character.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Image Generation</head><p>We use datasets introduced in (van den Oord et al., 2016a) which are downsampled versions of the popular ImageNet. In the autoregressive image generation setup, they consist of respectively 32?32?3 and 64 ? 64 ? 3 tokens, corresponding to RGB channels, per image. As the only preprocessing step we flatten the images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1">ImageNet32</head><p>For our main result the following hierarchy is used: (3@1, 24@3, 3@1   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2">ImageNet64</head><p>The sequence length that our model can handle is limited mainly by the computational complexity of used attention module. We replace relative attention in vanilla layers by LSH attention <ref type="bibr">(Kitaev et al., 2020)</ref>, which allows us to handle 12288-long sequences. To achieve relative attention parametrization, the LSH attention is combined with rotary positional embeddings <ref type="bibr">(Su et al., 2021)</ref>. In shortened layers, standard relative attention is used. For LSH attention, we set chunk length to 128 and use 2 hashes, which results in small memory complexity in our full-size layers. In this setup, we reach a score of 3.443 bpd with a (3@1, 12@3, 3@1) architecture. All attention layers had d model = 768, d f f = 3072 and 8 heads. No dropout was used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3">CIFAR-10</head><p>CIFAR-10 (Krizhevsky, 2009) is an image dataset consisting of 60000 images of size 32x32. We use this dataset primarily for our ablations (Section 4).</p><p>Due to the relatively small number of examples compared to ImageNet, models reach convergence after 100k steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Ablations</head><p>In this section, we start by introducing a training technique called shorten factor dropout (Section 4.1), and then analyze Hourglass's components de-ImageNet32 BPD PixelCNN (van den <ref type="bibr">Oord et al., 2016b)</ref> 3.83 Image Transformer <ref type="bibr">(Parmar et al., 2018)</ref> 3.77 Axial Transformer <ref type="bibr">(Ho et al., 2019)</ref> 3.76 Hourglass 3.74 VDM <ref type="bibr">(Kingma et al., 2021)</ref> 3.72 DenseFlow <ref type="bibr">(Grci? et al., 2021)</ref> 3.63 ImageNet64 BPD Reformer <ref type="bibr">(Kitaev et al., 2020)</ref> 3.65 Performer <ref type="bibr" target="#b6">(Choromanski et al., 2021)</ref> 3.64 Hourglass 3.44 Sparse Transformer <ref type="bibr" target="#b5">(Child et al., 2019)</ref> 3.44 Routing Transformer <ref type="bibr">(Roy et al., 2020)</ref> 3.43 Combiner <ref type="bibr">(Ren et al., 2021)</ref> 3.42 VDM <ref type="formula">(2021)</ref> 3.40 DenseFlow <ref type="formula">(2021)</ref> 3.35 scribed above. We show that shortened layers behave similarly to full token-level layers in terms of scalability (Section 4.2). Then we study the effect of different distributions of (pre, post) vanilla layers on Hourglass' accuracy (Section 4.3). We further analyze the performance of various upsampling and downsampling methods (Sections 4.4 and 4.5). Finally, we discuss different shorten factors and multi-stage shortening in Section 4.6. We conduct the ablations on both text and image generation to show applicability across different domains. We report bits per character (BPC) on the enwik8 validation (dev) set evaluated without context (sequence length 2048) and bits per dim (BPD) on the CIFAR-10 test set. For the exact hyperparameter setup refer to the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Shorten factor dropout</head><p>Different shorten factors can be used for the same model when using parameterless pooling methods. We propose a training procedure where the shorten factor is randomly sampled with uniform distribution from a predefined set in each step. We observe that such a training regime improves validation loss compared to a baseline trained with a single, fixed shorten factor. For example, a model trained with shorten factor randomly sampled from {2, 3} performs better when evaluated with any of these shorten factors, compared to models trained with a corresponding fixed shorten factor (Tab. 4).</p><p>We hypothesise that such a technique promotes a more uniform distribution of information over the sequence of tokens. It may be essential for fixed-size pooling techniques as they do not ac-count for variable length constituents like words. By spreading information uniformly, we prevent a situation where we lose content by shortening three information-dense tokens or lose available capacity by merging three low information ones.</p><p>Shorten factor dropout is not limited to our architecture and can be applied to any model that utilizes shortening, particularly <ref type="bibr" target="#b8">(Dai et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hierarchy</head><p>Train  </p><formula xml:id="formula_3">k Val k = 2 Val k = 3 2@1 8@k 2@1 {2,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scaling shortened layers</head><p>In this study, we show that layers operating on the shortened sequence contribute significantly to Hourglass's accuracy. In <ref type="table">Table 5</ref> we measure the impact of scaling the depth of the shortened part of the model with a fixed number of vanilla layers. We also check if scaling laws of Transformers, described in (Kaplan et al., 2020), hold by comparing a regression line fitted to various Hourglass configurations and one fitted to Transformer-XL baseline. We observe in <ref type="figure">Figure 1</ref> that the slopes are very similar, which indicates that the laws hold.</p><p>Number of shortened layers enwik8 CIFAR-10 Baseline (n = 1) 1.164 3.28 n = 4 1.134 3.16 n = 8 1.111 3.07 n = 16 1.096 3.03 <ref type="table">Table 5</ref>: Impact of increasing the number of shortened layers on perplexity. Vanilla layers: (1, 1) for CIFAR-10 and (2, 2) for enwik8, shorten factor 3 used in both.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Impact of vanilla layers</head><p>We observe a significant contribution to Hourglass' performance with increasing the number of vanilla layers. One reason is that we perform more computations as in vanilla layers we process the sequence in token-level -no shortening is applied. We also see that the distribution of vanilla layers before shortening and after shortening does impact the training (see Tab. 6), and equal distribution leads to the best perplexity.  <ref type="table">Table 6</ref>: Impact of the distribution of vanilla layers on enwik8 (BPC) and CIFAR-10 score (BPD). We see that equal distribution of layers before and after shortening leads to better results on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Upsampling method</head><p>In    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Pooling method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Shortening strategies</head><p>While the analysis above gives a clear indication of what methods to choose for shortening and upsampling, we are still left with the question of which shorten factors to use and whether to do singlestage or multi-stage shortening.</p><p>Consistently, it is beneficial to do at least one shortening and by a factor of at least 3, while keeping 2-3 vanilla layers. Beyond that, a number of different configurations can yield similar results. In <ref type="table">Table 1</ref> we present the different hierarchical configurations that we tested on enwik8 and plotted in <ref type="figure">Figure 1</ref>. It can be seen that configurations with similar computation costs perform similarly. The sequence length used in these experiments is 2048 -we hypothesise that more hierarchy may be beneficial with even longer sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Shortening in Transformers Shortening in our work is inspired by Funnel-Transformer <ref type="bibr" target="#b8">(Dai et al., 2020)</ref>. The key difference is that they train an encoder model for text classification, where our work is entirely focused on language modeling, which provides additional challenges we had to solve regarding shortening in the autoregressive setup (Section 2.2). Another difference is that they use repeat upsampling method while we use attention. There are also a few works related to character-level modeling which use shortening, namely <ref type="bibr" target="#b7">(Clark et al., 2021)</ref> and <ref type="bibr">(Tay et al., 2021)</ref>. However, the authors of these works focused mainly on shortening sequence in encoder part of the transformer, whereas we focused on applying shortening in decoder.</p><p>The idea of shortening is also discussed in <ref type="bibr">(Subramanian et al., 2020)</ref>. However, proposed architectures either focus on downsampling or upsampling, while Hourglass is a U-Net-like architecture and is symmetric in these terms. Their models use transformer layers on the finest scales when postprocessing final representations. We do these also, in the beginning, to preprocess tokens on the finest scale, and we have found it essential to the score (Section 4.3). Our attention upsampling method is similar to their aggregation layer in the bottom-up model, however we use one upsampling for each scale change while they combine different scales to create one global upsampling.</p><p>Relative positional encoding Our work is primarily built on the backbone of Transformer-XL <ref type="bibr" target="#b9">(Dai et al., 2019</ref>) -we use the same relative attention parametrization. Instead of the segmentlevel recurrence mechanism, we use shortening to make our model more efficient and feasible to train on longer sequences. Another relative attention parametrization is RoFormer <ref type="bibr">(Su et al., 2021)</ref> where rotary positional embeddings are introduced.</p><p>We find this work particularly relevant because the method is compatible with any attention type, including efficient attention, and can be combined with our model (Section 3.4.2).</p><p>Sparse Attention A well-known approach addressing the memory bottleneck is utilizing sparsity patterns in the attention matrix -Routing <ref type="bibr">(Roy et al., 2020)</ref> and Sparse Transformer <ref type="bibr" target="#b5">(Child et al., 2019)</ref> are examples of such methods. Our solution is different in the sense that it uses full attention -just with shortened sequence length. Combiner (Ren et al., 2021) makes a step further and provides full attention capabilities with similar computational complexity to Routing and Sparse transformers by leveraging structured factorization. This work, similarly to papers mentioned above on efficient transformers, concentrates on speeding up the attention component, while the most important feature of the Hourglass architecture is that it can use any attention module as a drop-in.</p><p>Image generation on downsampled ImageNet VDM <ref type="bibr">(Kingma et al., 2021)</ref> and <ref type="bibr">DenseFlow (Grci? et al., 2021)</ref> are recently proposed state-of-the-art methods for density estimation on this dataset. The difference between these methods and Transformerbased methods <ref type="bibr">(Parmar et al., 2018;</ref><ref type="bibr">Ho et al., 2019)</ref> including this work is that the former, unlike Transformers, are non-autoregressive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we show how hierarchy can improve the efficiency of Transformers in a language modeling setup. Our proposed architecture, Hourglass, significantly outperforms the baseline both in terms of perplexity reached at a given computation cost <ref type="figure">(Figure 1</ref>) and empirical metrics like running memory ( <ref type="figure">Figure 6</ref>). Hourglass achieves state-of-the-art results among autoregressive models on the Ima-geNet32 generation task and competitive results on other image generation and language modeling tasks.</p><p>Hourglass can be used with any attention type, which opens many directions for future research related to Transformers capable of processing longer sequences. Another line of future work might be related to advances in the shortening mechanism itself, for example, involving a dynamic pooling operation that could explicitly handle the problem of fixed-size groups in multi-stage shortening. We also leave open the problem of choosing the best hi-erarchy for a task. We conjecture that experiments with much longer contexts will provide better guidance for this choice and will benefit even more from the Hourglass architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>Some experiments were performed using the Entropy cluster funded by NVIDIA, Intel, the Polish National Science Center grant UMO-2017/26/E/ST6/00622, and ERC Starting Grant TOTAL. The work of Henryk Michalewski was supported by the Polish National Science Center grant UMO-2018/29/B/ST6/02959. The authors would like to thank Marek Cygan and Kamil Wilczek for their help with cluster setup, and Grzegorz Grudzi?ski, Dawid Jamka and Sebastian Jaszczur for helpful discussions. This article describes a Team Programming Project completed at the University of Warsaw in the academic year 20/21. We are grateful to Janusz Jab?onowski, the head of Team Programming Projects, for his support and openmindedness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Autoregressive shortening</head><p>In Section 2.2 we address two problems of shortening in an autoregressive setup: information leaks and reduced expressivity. Here we study these issues in more detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Motivation behind using vanilla layers</head><p>At first sight, it may be tempting to create hierarchical models that directly shorten the input to maximize the efficiency gains. In this section, we explain why vanilla layers are crucial for modeling at least some sequences, especially due to autoregressivity.</p><p>Consider a sequence modeling task where the input is a random sequence with repeats, such as A#AC#CD#DB#B. The sequence consists of chunks L#L where L is a random uniform letter and # is a special symbol. A vanilla Transformer language model can achieve 66% sequence accuracy on this task -it cannot predict the token at the beginning of the chunk, but it can predict the last token of the chunk by simply copying the token at 2 positions earlier, which is possible using a vanilla self-attention layer.</p><p>It is however not easy to learn this task in a shortening setup when there are no vanilla layers operating on the finest scale -this is the situation defined in Reduced expressivity subsection of Section 2.2. Assume shorten factor is k = 3 and the input is A#AB#BC#C. To avoid information leak, we shift the input sequence right by 1, and then by k?1 = 2 directly before shortening. Then the sequence is 000A#AB#B. Our shortened embeddings are as follows: e 0 = S(emb 0 , emb 0 , emb 0 ), e 1 = S(emb A , emb # , emb A ) where emb is input embedding matrix and S is a shortening function.  Because no vanilla layers are used, for prediction we can use only shortened embeddings and input token embeddings shifted right by 1 from the residual connection. Note that to predict the A token at position 3 we can use only embedding of emb # and e 0 -both of these contain no information so we are unable to predict this token better than randomly (see <ref type="table" target="#tab_15">Table 9</ref>). An analogous situation occurs for prediction of any tokens at positions divisible by 3, which makes the model unable to achieve more than 50% accuracy when the task has vocabulary size of at least 2.</p><p>This issue can be solved by adding at least one vanilla layer to the model, so that it can attend within the neighborhood of k previous tokens. For this particular problem, it is sufficient to use local attention with context size k in vanilla layers which is significantly more efficient than full attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Information leaks -analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.1 Definition of autoregressive model</head><p>Formally, given a target sequence, x = x 1 , ..., x n , an autoregressive model (e.g. transformer decoder) models the sequence as P (x) = n i=1 P (x i |x 1 , ..., x i?1 ) and</p><formula xml:id="formula_4">? i P (x i |x 1 , ..., x n ) = P (x i |x 1 , ..., x i?1 )</formula><p>namely x i token depends only on previous tokens, never on itself nor next ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Definition of information leak</head><p>We say that a leak was caused by function F n : A n ?? A n transforming sequence of input tokens (x 1 , x 2 , ..., x n ) into another sequence (u 1 , ..., u n ) = F ((x 1 , ..., x n )) when ? i&lt;j&lt;n P (x i |x 1 , ..., x i?1 , x j ) = P (x i |x 1 , ..., x i?1 ), namely there exists j ? i that token x i depends on x j which violates the autoregressive property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.3 Model representation</head><p>Let R k : A n ?? A n be a shift right function which reindexes tokens by shifting each of them right by k positions:</p><formula xml:id="formula_5">R k ((x 1 , x 2 , .</formula><p>..x n )) = (0, ..., 0 k , x 1 , ..., x n?k ) S k : A * ?? A * shortening function with factor k which takes on input x 1 , ..., x n sequence and returns s 1 , ..., s m where m = n k , U k upsampling function which works in similar way but upsamples U k <ref type="figure">((u 1 , .</ref>.., u m )) = u 1 , ..., u n .</p><p>Between them there is also applied D decoder function, D = D 1 ? ? ? ? ? D l , where each D i is a function representing decoder block. Due to causal attention masking in the decoder block, there is no risk of information leak caused by function D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.4 Leak description</head><p>Because of mentioned attention mask, we will omit the flow of information between tokens caused by the influence of attention mechanism because this mask keeps the autoregressive property. Now, let (x 1 , ..., x n ) be an input sequence and (u 1 , ..., u n ) = U (D(S k (T s <ref type="figure">((x 1 , ..</ref>., x n ))))) = F . In order to preserve autoregressive property, it is obligatory that no leak occurs.</p><p>We will show that shift by any value 0 &lt; s &lt; k ? 1 where k is the shorten factor will cause a leak. To start with, consider input sequence (x 1 , ..., x n ) and perform operation F . R s ((x 1 , ..., x n )) = (0, ..., 0 s , x 1 , ..., x n?s ) = r. Assuming that n is divisible by s, we have S k (r) = (v 1 , ..., v n k ) = v where each v i consists of information obtained in (r (i?1)?k+1 , ..., r ik ). Now let see that operation D preserves autoregressive property, let d = D(t). Now, U (d) = (u 1 , ..., u n ) and each u i depends on d i?1 k +1 . Now consider s ? k ? 2 and let (u 1 , ..., u n ) = F ((x 1 , ..., x n )) will be a result of our Transformer part. Let take u 1 which depends on d 1 and d 1 depends on (r 1 , ..., r k ) = (0, ..., 0, x 1 , ..., x k?s ). For that reason d 1 depends on x 1 , x 2 , ..., x k?s , so we have P (x 1 |x k?s ) = P (x 1 ) which violates the autoregressive property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experimental setup B.1 Common parameters</head><p>Here we list common hyperparameters used for all experiments mentioned in the paper. We use Adam optimizer with ? 1 = 0.9, ? 2 = 0.98 and = 1e?9.</p><p>Weight decay and gradient clipping is not used. In terms of model details, we decided to use a Pre-Norm architecture and FastGelu activation in feed-forward layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Enwik8</head><p>We use d model = 512, d f f = 2048 and 8 attention heads. Models in ablation study are trained for 200k steps with cosine learning rate schedule, setting cycle length for 200k steps and linear warmup of 4000 steps.</p><p>For the main result achieving 0.98 bpc with 4@1, 8@3, 4@1 hierarchy, we set d model = 768, d f f = 3072 and n heads = 8 which results in 146M parameters. It is trained for a total number of 350k steps with one cycle of cosine schedule. Linear warmup of 20k steps is used.</p><p>At the beginning of our work on this paper, we have performed grid search over following hyperparameters for enwik8: All next experiments were conducted using these parameters without additional searching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Downsampled ImageNet -common parameters</head><p>For ImageNet32 and ImageNet64 experiments we use inverse square root learning rate decay from <ref type="bibr">(Vaswani et al., 2017)</ref>, setting warmup steps to 8000 in both experiments. Total batch size is 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 ImageNet32</head><p>In this dataset, we operate on input sequence length of 3072. We use d model = 512, d ff = 2048, 8 attention heads and 0.01 dropout rate. We perform 400k training steps with linear warmup and inverse square root decay and then we train for additional 70k steps with cosine learning rate decay, starting from the learning rate from the previous schedule at 400k and decreasing it to 0 at 470k steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 ImageNet64</head><p>As an input we receive a sequence of 12288 tokens representing 64 ? 64 ? 3 images. We set d model = 768, d ff = 3072, 8 attention heads and dropout equal to 0. We perform 300k steps with linear warmup and inverse square root decay.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 CIFAR-10</head><p>All the ablation studies are run for 100k training steps, unless otherwise specified. Input sequence has length 3072 and model parameters are as follows: d model = 512, d ff = 2048, 8 attention heads and dropout equal to 0. Total batch size is 8. Cosine learning rate decay with linear warmup of 5000 steps and 100k cycle length is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Environment setup C.1 Hardware</head><p>Experiments are conducted on several setups.</p><p>? Ablation Study and short training sessions were computed on nodes consisting of 4x Titan V with 12GB memory each, 64GB RAM, Intel Xeon E5-2660 v4 CPU</p><p>? longer trainings were completed on 8x RTX 2080 Ti with 11GB memory each, 128GB RAM and Intel Xeon E5-2660 v4 CPU.</p><p>? Few longest trainings were conducted on 8?8 TPUv3 units, each with 16GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Software</head><p>All experiments were performed on Linux operating system using Trax library version 1.3.9 along with all its dependencies from this particular release date. Additionally, to run shorten factor dropout experiments we modified the Transformer-XL codebase in PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Reproducibility</head><p>To ensure the reproducibility of this work and to support open science principles, we made our code publicly available at github.com/google/trax. In this repository, we also provide Google Colab notebooks where the evaluation of our main Enwik8 and ImageNet32/64 results can be reproduced. 23</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Randomness</head><p>Seeds in all experiments were chosen randomly, however each experiment contains history which allows retrieving all randomly set parameters for reproductions. For each ablation described in the ablation study section, we rerun the baseline 3 times to calculate standard deviation. All other experiments are run only once due to costs and since the variance we noticed was minimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Experiment representation</head><p>Each experiment is represented by a configuration file that unambiguously determines the whole setup -all hyperparameters and training details like specific optimizers, data preprocessing functions, or batch size per device.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Hourglass -a high-level architecture overview. The arrows denote residual connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 7 :</head><label>7</label><figDesc>Comparison between Reformer baseline and Hourglass, both with LSH attention, on Enwik8 valid set w.r.t. cost of one training step in seconds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 8 :</head><label>8</label><figDesc>Examples of our model completions, where bottom half of each image was generated by our model, prompted by the upper half.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>?</head><label></label><figDesc>batch size: {8, 16, 32}, finally chosen 8 ? dropout: {0.05, 0.1, 0.15, 0.20}, finally chosen 0.15 ? learning rate: {1e?4, 2e?4, 3e?4, 4e?4, 5e?4}, finally chosen 4e?4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>). We use d model = 512, d ff = 2048, 8 attention heads and 0.01 dropout rate.</figDesc><table><row><cell>Enwik8</cell><cell>#Param</cell><cell>BPC</cell></row><row><cell>Transformer-XL (2019) 24L</cell><cell>277M</cell><cell>0.99</cell></row><row><cell>Hourglass</cell><cell>146M</cell><cell>0.98</cell></row><row><cell>Adaptive-Span (2019) 24L</cell><cell>209M</cell><cell>0.98</cell></row><row><cell>Transformer-LS (2021)</cell><cell>110M</cell><cell>0.97</cell></row><row><cell>Feedback Transformer (2021)</cell><cell>77M</cell><cell>0.96</cell></row><row><cell>Expire-Span (2021) 24L</cell><cell>277M</cell><cell>0.95</cell></row></table><note>With this configuration we achieve 3.741 bits/dim, yielding the new state-of-the-art among autoregres- sive (Transformer-based) models on this dataset, compared to the previous state-of-the-art of 3.758 bpd by (Ho et al., 2019).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Enwik8 Results. We report bits-per-character (BPC) on the test set and number of model parameters. Hourglass applied to Transformer-XL significantly outperforms its baseline. Our technique could be also used with other more performant attention methods which we leave for future work.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Bits per Dimension (BPD) on downsampled imagenet. Autoregressive models are separated by a horizontal line from non-autoregressive ones. On Ima- geNet32, our model yields new state-of-the-art for au- toregressive models.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc></figDesc><table /><note>Comparison between models trained with shorten factor dropout (Train k = {2, 3}, Section 4.1) and fixed shorten factor baselines on enwik8.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 7</head><label>7</label><figDesc></figDesc><table><row><cell cols="3">we investigate different possibilities of</cell></row><row><cell cols="3">choosing the upsampling method. For attention-</cell></row><row><cell cols="3">free methods, linear upsampling performs better</cell></row><row><cell cols="3">on images, while repeat upsampling works well for</cell></row><row><cell cols="3">text. Attention upsampling works well regardless</cell></row><row><cell cols="3">of the function U and has the lowest perplexity.</cell></row><row><cell>Upsampling method</cell><cell>enwik8</cell><cell>CIFAR-10</cell></row><row><cell>Repeat</cell><cell>1.148</cell><cell>3.062</cell></row><row><cell>Linear</cell><cell>1.163</cell><cell>3.020</cell></row><row><cell>U (x, x ) = x</cell><cell>1.145</cell><cell>2.967</cell></row><row><cell>U (x, x ) = x + Linear(x )</cell><cell>1.132</cell><cell>3.012</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc></figDesc><table /><note>Upsampling method ablation -baseline config- urations are (2@1, 24@4, 2@1) and (1@1, 8@3, 1@1) for enwik8 and CIFAR-10, respectively.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8</head><label>8</label><figDesc></figDesc><table><row><cell cols="3">presents impact of pooling method on</cell></row><row><cell cols="3">both enwik8 (BPC) and CIFAR-10 (BPD). Atten-</cell></row><row><cell cols="3">tion pooling reaches the lowest perplexity for both</cell></row><row><cell cols="3">datasets. Average pooling performs well on text</cell></row><row><cell cols="3">among attention-free methods, while linear pool-</cell></row><row><cell cols="3">ing works better for images. Both of these methods</cell></row><row><cell cols="3">perform significantly worse for the other modality.</cell></row><row><cell cols="3">Attention pooling demonstrates small differences</cell></row><row><cell cols="3">with respect to chosen shortening function S (Sec-</cell></row><row><cell cols="3">tion 2.1), still preserving the preference towards</cell></row><row><cell cols="3">linear pooling on images and average pooling on</cell></row><row><cell>text.</cell><cell></cell><cell></cell></row><row><cell>Pooling method</cell><cell>enwik8</cell><cell>CIFAR-10</cell></row><row><cell>AvgPool</cell><cell>1.129</cell><cell>3.116</cell></row><row><cell>Attention, S = AvgP ool</cell><cell>1.124</cell><cell>3.012</cell></row><row><cell>Attention, S = LinearP ool</cell><cell>1.142</cell><cell>2.998</cell></row><row><cell>LinearPool</cell><cell>1.159</cell><cell>2.998</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 8 :</head><label>8</label><figDesc>Ablation of pooling methods. Attention pooling achieves the best perplexity on both datasets.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 9</head><label>9</label><figDesc></figDesc><table><row><cell>: Example input sequence which is difficult to</cell></row><row><cell>model without vanilla layers. The model can use only</cell></row><row><cell>input embeddings shifted by one from the residual and</cell></row><row><cell>shortened embeddings (shorten factor is 3) to predict</cell></row><row><cell>the target sequence. Note that it is impossible to pre-</cell></row><row><cell>dict tokens at positions divisible by 3 using only that</cell></row><row><cell>information.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://github.com/google/trax/blob/master/trax/models/research/examples/hourglass_enwik8.ipynb 3 https://github.com/google/trax/blob/master/trax/models/research/examples/hourglass_downsampled_imagenet.ipynb</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<imprint>
			<pubPlace>Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc-Candlish, Alec Radford, Ilya Sutskever</pubPlace>
		</imprint>
	</monogr>
	<note>and Dario Amodei. 2020. Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Generative pretraining from pixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="1691" to="1703" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerry</forename><surname>Tworek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiming</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Henrique</forename><surname>Ponde De Oliveira Pinto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Ray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raul</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Petrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heidy</forename><surname>Khlaaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brooke</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Pavlov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alethea</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Bavarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<title level="m">Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis</title>
		<editor>Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N</editor>
		<meeting><address><addrLine>Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang</addrLine></address></meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Leike</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedant</forename><surname>Achiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Morikawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miles</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mira</forename><surname>Brundage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katie</forename><surname>Murati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bob</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Mcgrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mccandlish</surname></persName>
		</author>
		<title level="m">Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucy</forename><surname>Colwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note>Rethinking attention with performers</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Canine: Pre-training an efficient tokenization-free encoder for language representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">H</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Garrette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulia</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Wieting</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Funnel-transformer: Filtering out sequential redundancy for efficient language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guokun</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Transformer-xl: Attentive language models beyond a fixed</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note>length context</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Payne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<title level="m">Jukebox: A generative model for music</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Addressing some limitations of transformers with feedback memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
