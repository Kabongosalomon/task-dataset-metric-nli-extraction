<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SUBSPACE DIFFUSION GENERATIVE MODELS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Jing</surname></persName>
							<email>bjing@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
							<email>gcorso@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renato</forename><surname>Berlinghieri</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">SUBSPACE DIFFUSION GENERATIVE MODELS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:53+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Score-based models generate samples by mapping noise to data (and vice versa) via a high-dimensional diffusion process. We question whether it is necessary to run this entire process at high dimensionality and incur all the inconveniences thereof. Instead, we restrict the diffusion via projections onto subspaces as the data distribution evolves toward noise. When applied to state-of-the-art models, our framework simultaneously improves sample quality-reaching an FID of 2.17 on unconditional CIFAR-10-and reduces the computational cost of inference for the same number of denoising steps. Our framework is fully compatible with continuous-time diffusion and retains its flexible capabilities, including exact log-likelihoods and controllable generation. Code is available at https://github.com/bjing2016/subspace-diffusion.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Score-based models are a class of generative models that learn the score of the data distribution as it evolves under a diffusion process in order to generate data via the reverse process <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b5">6]</ref>. These models-also known as diffusion models-can generate high-quality and diverse samples, evaluate exact log-likelihoods, and are easily adapted to conditional and controlled generation tasks <ref type="bibr" target="#b19">[20]</ref>. On the CIFAR-10 image dataset, they have recently achieved state-of-the-art performance in sample generation and likelihood evaluation <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>Despite these strengths, in this work we focus on and aim to address a drawback in the current formulation of score-based models: the forward diffusion occurs in the full ambient space of the data distribution, destroying its structure but retaining its high dimensionality. It does not seem parsimonious to represent increasingly noisy latent variables-which approach zero mutual information with the original data-in a space with such high dimensionality. The practical implications of this high latent dimensionality are twofold:</p><p>High-dimensional extrapolation. The network must learn the score function over the entire support of the high-dimensional latent variable, even in areas very far (relative to the scale of the data) from the data manifold. Due to the curse of dimensionality, much of this support may never be visited during training, and the accuracy of the score model in these regions is called into question by the uncertain extrapolation abilities of neural networks <ref type="bibr" target="#b22">[23]</ref>. Learning to match a lower-dimensional score function may lead to refined training coverage and further improved performance.</p><p>Computational cost. Hundreds or even thousands of evaluations of the high-dimensional score model are required to generate an image, making inference with score-based models much slower than with GANs or VAEs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>. A number of recent works aim to address this challenge by reducing the number of steps required for inference <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b1">2]</ref>. However, these methods generally trade off inference runtime with sample quality. Moreover, the dimensionality of the score function-and thereby the computational cost of a single score evaluation-is an independent and equally important factor to the overall runtime, but this factor has received less attention in existing works. The starting data distribution x 0 (0) lies near a subspace (light blue line). As the data evolves, the distribution of the orthogonal component x ? 1 (t) approaches a Gaussian faster than the subspace component x 1 (t). At time t 1 we project onto the subspace and restrict the remaining diffusion to the subspace. To generate data, we use the full and subspace score models to reverse the full and subspace diffusion steps, and sample x ? 1 (t 1 ) from a Gaussian to reverse the projection step. Top right: The diffusion of the subspace component x 1 (t) is unaffected by the projection step and restriction to the subspace; while the orthogonal component is diffused until t 1 and discarded afterwards. Bottom: CIFAR-10 images corresponding to points along the trajectory, where the subspaces correspond to lower-resolution images and projection is equivalent to downsampling.</p><p>Subspace diffusion models aim to address these challenges. In many real-world domains, target data lie near a linear subspace, such that under isotropic forward diffusion, the components of the data orthogonal to the subspace become Gaussian significantly before the components in the subspace. We propose to use a full-dimensional network to model the score only at lower noise levels, when all components are sufficiently non-Gaussian. At higher noise levels, we use smaller networks to model in the subspace only those components of the score which remain non-Gaussian. As this reduces both the number and domain of queries to the full-dimensional network, subspace diffusion addresses both of our motivating concerns. Moreover, in contrast to many prior works, subspace diffusion remains fully compatible with the underlying continuous diffusion framework <ref type="bibr" target="#b19">[20]</ref>, and therefore preserves all the capabilities available to continuous score-based models, such as likelihood evaluation, probability flow sampling, and controllable generation.</p><p>While subspace diffusion can be formulated in fully general terms, in this work we focus on generative modeling of natural images. Because the global structure of images is dominated by lowfrequency visual components-i.e., adjacent pixels values are highly correlated-images lie close to subspaces corresponding to lower-resolution versions of the same image. Learning score models over these subspaces has the advantage of remaining compatible with the translation equivariance of convolutional neural networks, and therefore requires no architectural modifications to the score model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contributions.</head><p>We propose and formulate the diffusion process, training procedure, and sampling procedure in subspaces; to our knowledge, this represents the first investigation of dimensionality reduction in a score-based model framework. We develop a method, the orthogonal Fisher divergence, for choosing among candidate subspaces and the parameters of the subspace diffusion. Experimentally, we train and evaluate lower-dimensional subspace models in conjunction with state-of-the-art pretrained full-dimensional models from <ref type="bibr" target="#b19">[20]</ref>. We improve over those models in sample quality and runtime, achieving an FID of 2.17 and a IS of 9.99 on CIFAR-10 generation. Additionally, we demonstrate probability flow sampling and likelihood evaluation on CIFAR-10, high-dimensional sampling on 256 ? 256 CelebA-HQ, and inpainting on LSUN Church.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND AND RELATED WORK</head><p>Score-based models. In score-based models, one considers the data distribution x(0) ? R d to be the starting distribution for a continuous diffusion process, defined by an Ito stochastic differential equation</p><formula xml:id="formula_0">(SDE) dx = f (x, t) dt + G(x, t) dw t ? (0, T )<label>(1)</label></formula><p>known as the forward process, which transforms x(0) into (approximately) a simple Gaussian x(T ). By convention, we typically set T = 1. A neural network is then trained to model the score ? x log p(x, t) conditioned on t. Solving the reverse stochastic differential equation</p><formula xml:id="formula_1">dx = f (x, t) dt ? G(x, t)G(x, t) T ? x log p(x, t) dt + G(t) dw<label>(2)</label></formula><p>starting with samples from the simple Gaussian distribution x(T ) yields samples from the data distribution x(0) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b0">1]</ref>. Score-based models were originally formulated separately in terms of denoising score matching at multiple noise scales <ref type="bibr" target="#b18">[19]</ref>; and of reversing a discrete-time Markov chain of diffusion steps <ref type="bibr" target="#b5">[6]</ref>. Due to the latter formulation (associated with the term diffusion model), x(t) for t &gt; 0 are often referred to as latents of x(0), and the simple Gaussian x as the prior. The two views are unified by the observation that the variational approximation to the reverse Markov chain matches the score of the diffused data <ref type="bibr" target="#b19">[20]</ref>.</p><p>The score model s ? (x, t) can be trained via denoising score matching <ref type="bibr" target="#b18">[19]</ref> using the perturbation kernels p(x(t) | x(0)), which are analytically determined by f (x, t), G(t) at each time t. The learned score can be readily adjusted with fixed terms for controlled generation tasks in the same manner as energy-based models <ref type="bibr" target="#b4">[5]</ref>. Finally, the reverse stochastic differential equation produces the same marginals x as the ordinary differential equation (ODE)</p><formula xml:id="formula_2">dx = f (x, t) dt ? 1 2 G(x, t)G(x, t) T ? x log p(x, t) dt<label>(3)</label></formula><p>which enables evaluation of exact log-likelihoods, but empirically results in degraded quality when used for sampling <ref type="bibr" target="#b19">[20]</ref>.</p><p>Accelerating score-based models. Due to the fine discretization required to solve (2) to high accuracy, score-based models suffer from slow inference. Several recent works aim to address this. Denoising diffusion implicit models (DDIM) <ref type="bibr" target="#b17">[18]</ref> can be viewed as solving the equivalent ODE with a reduced number of steps. Progressive distillation <ref type="bibr" target="#b15">[16]</ref> proposes a student-teacher framework for learning sampling networks requiring logarithmically fewer steps. <ref type="bibr" target="#b7">[8]</ref> derives an adaptive step-size solver for the reverse SDE. Other works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b1">2]</ref> focus on reducing the number of steps in the discrete-time Markov chain formulation. However, these approaches generally result in degraded sample quality compared to the best continuous-time models.</p><p>Taking a different approach, latent score-based generative models (LSGM) <ref type="bibr" target="#b20">[21]</ref> use a score-based model as the prior of a deep VAE, 1 resulting in more Gaussian scores, improved sample quality, and fewer model evaluations. In a similar vein, critically-damped Langevin diffusion (CLD-SGM) <ref type="bibr" target="#b3">[4]</ref> augments the data dimensions with momentum dimensions and diffuses only in momentum space, resulting in more Gaussian scores and fewer evaluations for comparable quality. However, both these methods significantly modify the original formulation of score-based models, such that exact likelihood evaluation and controllable generation become considerably more difficult. <ref type="bibr" target="#b1">2</ref> Unlike these previous works, subspace diffusion simultaneously improves sample quality and inference runtime while also preserving all the capabilities of the original formulation. Furthermore, subspace diffusion can be potentially combined with step-size methods, as reducing the dimensionality of the diffusion versus reducing its length represent orthogonal approaches to accelerating sampling. Compared with LSGM and CLD-SGM, subspace diffusion also has the advantage of being compatible with existing trained score models, incurring only the overhead required to train the smaller subspace score models.</p><p>Cascading generative models. Subspace diffusion bears some similarity to cascading generative models consisting of one low-dimensional model followed by one or more super-resolution models <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>. Cascading score-based models have yielded strong results on high-resolution classconditional ImageNet generation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7]</ref>. These models formulate each super-resolution step as a full diffusion process conditioned on the lower-resolution image. Subspace diffusion, on the other hand, models a single diffusion process punctuated by projection steps. This leads to a more general theoretical framework that is useful even in domains where the concept of super-resolution does not apply. Chaining conditional diffusion processes also complicates the application of other capabilities of score-based models-for example, evaluating log-likelihoods would require marginalizing over the intermediate lower-resolution images. Our subspace diffusion framework is a modification of a single diffusion and does not incur these difficulties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">SUBSPACE DIFFUSION</head><p>A concrete formulation of a score-based model requires a choice of forward diffusion process, specified by f (x, t), G(x, t). Almost always, these are chosen to be isotropic, i.e., of the form</p><formula xml:id="formula_3">f (x, t) = f (t) x G(x, t) = g(t) I d<label>(4)</label></formula><p>where d is the data dimensionality. For example, the variance exploding (VE) SDE has f (t) = 0 and g(t) = d? 2 /dt where ? 2 (t) is the variance of the perturbation kernel at time t <ref type="bibr" target="#b19">[20]</ref>. The sole exception is the Langevin diffusion in CLD-SGM <ref type="bibr" target="#b3">[4]</ref>, but this requires new forms of scorematching and specialised SDE solvers for numerical stability. We aim to keep the simplicity and convenience of form (4) while addressing its limitations discussed in Section 1. We thus propose that at every point in time, the diffusion is restricted to some subspace, but is otherwise isotropic in that subspace. Specifically, the forward diffusion begins in the full space, but is projected and restricted to increasingly smaller subspaces as time goes on. Any isotropic forward diffusion can therefore be converted into a subspace diffusion.</p><p>For any diffusion with the form (4), define the corresponding subspace diffusion as follows. Divide (0, T ) into K + 1 subintervals, (t 0 , t 1 ), . . . , (t K , t K+1 ) where for notational convenience t 0 = 0, t K+1 = T . Then define:</p><formula xml:id="formula_4">G(x, t) = g(t)U k U T k<label>(5)</label></formula><p>for each interval t k &lt; t &lt; t k+1 , where U k ? R d?n k is the matrix whose n k ? d orthonormal columns span a subspace of R d . We refer to this subspace as the kth subspace and to the columns of U k as its basis. For notational convenience, U 0 = I d . We choose n k such that d = n 0 &gt; n 1 &gt; . . . &gt; n K . We also require the kth subspace to be a subspace of the jth subspace for any j &lt; k, which can be written as U j U T j U k = U k . Together, these definitions state that diffusion is coupled or constrained to occur in progressively smaller subspaces defined by U k in the interval (t k , t k+1 ).</p><formula xml:id="formula_5">Turning to f (x, t), define f (x, t) = f (t) x + K k=1 ?(t ? t k )(U k U T k ? I d ) x<label>(6)</label></formula><p>where ? is the Dirac delta. This states that at time t k , x is projected onto the kth subspace. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the high-level idea of subspace diffusion, along with some of its properties discussed in more detail below.</p><p>Notation. For the rest of the exposition, we define:</p><p>? U k|j = U T j U k ? R nj ?n k for j ? k defines the kth subspace written in the basis of the jth subspace. In particular, U k|0 = U k and U k|k = I n k .</p><p>? P k|j = U k|j U T k|j ? R nj ?nj for j ? k is the projection operator onto the kth subspace, written in the basis of the jth subspace.</p><p>? P ? k|j = I nj ? P k|j ? R nj ?nj for j &lt; k is the projection operator onto the complement of the kth subspace, written in the basis of the jth subspace.</p><formula xml:id="formula_6">? x k = U T k x ? R n k is the component of x in the kth subspace, written in that basis. In particular, x 0 = x. ? x ? k|j = P ? k|j x j ? R nj for j &lt; k</formula><p>is the component of x j orthogonal to the kth subspace, written in the basis of the jth subspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SCORE MATCHING</head><p>To generate data, we need to learn the score ? x log p(x, t) as usual. However, for times t k &lt; t &lt; t k+1 , the support of p(x, t) is only in the kth subspace. This means that if we learn a separate score model s k (x, t) ? ? x log p(x, t) for each interval t ? (t k , t k+1 ), then the model s k only needs to have dimensionality n k . In particular, we use models smaller than n 0 = d for all times t &gt; t 1 .</p><p>To learn these lower-dimensional models, we leverage the fact that the subspace components x k of the data diffuse under an SDE with the same f (t), g(t) as the full data, independent of the orthogonal components. This is due to the fact that the original diffusion is isotropic. To see this, consider (for simplicity) the case K = 1, i.e., we only use one proper subspace. Then since dx 1 = U T 1 dx,</p><formula xml:id="formula_7">dx 1 = f (t)U T 1 x dt + ?(t ? t 1 )U T 1 (U 1 U T 1 ? I d )x dt + g(t) U T 1 1 t&lt;t1 I d + 1 t&gt;t1 U 1 U T 1 dw<label>(7)</label></formula><p>However, because U T 1 U 1 = I d , the above simplifies as</p><formula xml:id="formula_8">dx 1 = f (t)U T 1 x dt + g(t)U T 1 dw = f (t)x 1 dt + g(t) dw 1<label>(8)</label></formula><p>where, because the columns of U 1 are orthonormal, dw 1 := U T 1 dw is a Brownian diffusion in R n1 . As a result, the perturbation kernels p(x 1 (t) | x 1 (0)) in the subspace have the same form as in the full space. This allows us to train a model to match the scores ? x1 log p(x 1 , t) via precisely the same procedure as in <ref type="bibr" target="#b19">[20]</ref>, except we treat x 1 (0) as the original undiffused data. These scores are related to the full-dimensional scores ? x log p(x, t) via U 1 , but since x = U 1 x 1 for times t &gt; t 1 , we can directly work with data points x 1 and score models ? x1 log p(x 1 , t) with no loss of information for times t &gt; t 1 . Thus, in the general case, we train K + 1 different score models s k (x k , t) ? ? x k log p(x k , t), where we consider x k to have diffused under the original f (t), g(t) for the full time scale (0, T ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SAMPLING</head><p>To generate a sample, we use each score model s k (x k , t) in the corresponding interval (t k , t k+1 ) to solve the reverse diffusion of x k . However, we cannot use the score to reverse the projection steps at the boundaries times t k . Thus, to impute</p><formula xml:id="formula_9">x k?1 (t k ) from x k (t k ), we sample x ? k|k?1 (t k )</formula><p>by injecting isotropic Gaussian noise orthogonal to the kth subspace. The variance ? ? k|k?1 of the injected noise is chosen to match the marginal variance of x ? k|k?1 at time t k , which is the sum of the original variance of x ? k|k?1 in the data and the variance of the perturbation kernel:</p><formula xml:id="formula_10">? ? k|k?1 (t k ) := ? 2 (t k ) n k?1 ? n k E x ? k|k?1 (0) 2 2 + ? 2 (t k )<label>(9)</label></formula><p>where ?(t) and ? 2 (t) are the scale and variance of the perturbation kernels.</p><p>Sampling x ? k|k?1 in this manner assumes that (at time t k ) it is independent of x k and roughly an isotropic Gaussian. The final sample quality will depend on the validity of this assumption. Intuitively, however, we specifically choose subspaces and times such that the original magnitude of x ? k|k?1 (the first term in <ref type="formula" target="#formula_10">(9)</ref>) is very small compared to the diffusion noise (the second term), which is indeed isotropic and independent of the data. We also find that a few conditional Langevin dynamics steps with s k (x k , t k+1 ) to correct for the approximations of noise injection help sampling quality. The complete sampling procedure is outlined in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: Unconditional sampling with subspace diffusion</head><p>Input: subspaces U k , projection times t k , score models s k (x k , t), k = 0 . . . K Output: approximate sample x 0 from p(x 0 , 0) = p data (x)</p><formula xml:id="formula_11">x K ? sample from prior p(x K , T ) ? R n K ; for k ? K to 0 do x k ? solve reverse SDE with s k (x k , t) from t k+1 to t k starting from x k ; if k &gt; 0 then x ? k|k?1 ? sample from N (0, ? ? k|k?1 (t k ) I) ? R n k?1 ; x ? k|k?1 ? P ? k|k?1 x ? k|k?1 ; x k?1 ? U k|k?1 x k + x ? k|k?1 ; for i ? 1 to n do // n is a hyperpameter x k?1 ? LangevinStep(x k?1 , t k ) ;</formula><p>So far we have presented subspace diffusion as an explicit modification to the forward diffusion involving projection and confined diffusion, which best matches how we implement unconditional sample generation. However, an alternate view is more suitable for controlled generation, where a full-dimensional score model is required; or in ODE-based likelihood evaluation or probability flow sampling, where the adaptive, non-monotonic evaluations make working with discrete projection steps inconvenient. In these settings, we regard subspace diffusion at time t ? (t k , t k+1 ) as explicitly modeling the score component in kth subspace with s k (x k , t), and implicitly modeling all orthogonal components with Gaussians. Specifically, for t ? (t k , t k+1 ) we decompose x as</p><formula xml:id="formula_12">x = k?1 j=0 U j x ? j+1|j + U k x k<label>(10)</label></formula><p>where the sum corresponds to the components that are "Gaussianized" out by each projection step. We thus model each x ? j+1|j impliclty as isotropic Gaussian with variance ? ? j+1|j , and model x k explicitly with score model s k (x k , t), giving the full score:</p><formula xml:id="formula_13">? x log p(x, t) ? U k s k (U T k x, t) ? k?1 j=0 P j|0 ? P j+1|0 x ? ? j+1|j (t)<label>(11)</label></formula><p>where, for clarity, we write all components in terms of x in the original basis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">IMAGE SUBSPACES</head><p>We now restrict our attention to generative modeling of natural images. Motivated by the observation that adjacent pixels tend to be similar in color, we choose subspaces that correspond to images where adjacent groups of pixels are equal in color-i.e., downsampled versions of the image. Henceforth, we refer to such downsampling subspaces in terms of their resolution (e.g., the 16 ? 16 subspace), refer to projection onto subspaces at times t k as downsampling, and to the reverse action as upsampling. <ref type="bibr" target="#b2">3</ref> To more precisely formulate these subspaces, suppose we have a full-resolution image X ? R (n?n?3) . In particular, we will work with n that are integer powers of 2. Then we define a downsampling operator D : which states that X (t) k+1 is simply X (t) k after mean-pooling 2 ? 2 patches, multiplied by 2. We can use D to implicitly define U k :</p><formula xml:id="formula_14">R (n?n?3) ? R (n/2?n/2?3) such that if X k+1 = DX k , then X k+1 [a, b, c] = 1 2 (i,j)?{0,1} 2 X k [2a + i, 2b + j, c]<label>(12)</label></formula><formula xml:id="formula_15">U T k x = D k x or U T k x = DU T k?1 x<label>(13)</label></formula><p>where here we consider x to be the column vector representation of the array X. The choice of D corresponds to orthonormal U k , as each column of U k has 2 2k nonzero entries, each with magnitude 1/2 k . Thus, all of the general results from the preceding section apply. In particular, we can consider the same forward diffusion process defined by f (t), g(t) to be occurring for each downsampled image D k x, such that the subspace score models s k (x k , t) correspond to the same score model trained over a lower-resolution version of the same dataset.</p><p>It is natural to consider whether there may exist more optimal subspaces for natural images. In <ref type="table">Table 1</ref> we compare the downsampling subspaces to the optimal subspaces of equivalent dimensionality 4 found by principle components analysis (PCA) in terms of root mean square distance (RMSD) of the data from the subspace. Generally, the downsampling subspaces can be seen to be suboptimal. However, if we were to use the optimal PCA subspaces, the coordinates would not take the form of an image-structured latent with translation equivariance, and thus would be incompatible with the convolutional neural networks in the score model. Therefore, a more appropriate comparison is with the subspace found via PCA of the distribution of all patches of pixels of the appropriate size, which we call Patch-PCA (see Appendix A for details). These subspaces offer only minor improvements over the downsampling subspaces, so we did not explore them further.</p><p>It is also possible that for any given dimensionality n &lt; d, the n-dimensional substructure that best approximates the data distribution is a nonlinear manifold rather than a subspace. However, leveraging such manifolds to reduce the dimensionality of diffusion would require substantial modifications to the present framework. While potentially promising, we leave such extensions to nonlinear manifolds to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">ORTHOGONAL FISHER DIVERGENCE</head><p>We now propose a principled manner to choose among the candidate subspaces for a given image dataset, as well as the downsampling times t k .</p><p>For any fixed choice of proper subspaces U 1 . . . U k , the optimal values of each t k must balance two factors: smaller t k reduces the number of reverse diffusion steps occurring at higher dimensionality n k?1 , whereas larger t k makes the Gaussian approximation of the orthogonal components x ? k|k?1 more accurate when we sample at time t k . This suggests that we should choose the minimum times that keep the error of the Gaussian approximation below some tolerance threshold. However, we  <ref type="bibr" target="#b19">[20]</ref>. Similar plots can be generated for other models. Given a divergence threshold, the optimal downsampling times t k for any subspace sequence are the times at which the corresponding divergences attain that threshold. For example, on CIFAR-10 with a target D F = 3 ? 10 ?3 and the sequence 32 ? 16 ? 8, the downsampling times are t 1 = 0.516, t 2 = 0.558. In this case, the intermediate 16 ? 16 subspace would be used for only 4.2% of the diffusion. As the plot shows, this imbalance would characterise any sequence of more than one proper subspace.</p><p>cannot quantify the true error as we do not have access to the underlying distribution of x ? k|k?1 . Thus, we instead examine how much the learned full-dimensional score model s 0 (x, t) diverges from the Gaussian approximation on x ? k|k?1 as t is varied. To quantify this divergence, for any j &lt; k we introduce the orthogonal Fisher divergence of U k|j as:</p><formula xml:id="formula_16">D F (U k|j ; t) = ? ? k|j (t) n j ? n k E x(t) ? ? P ? k|j U T j s 0 (x, t) + x ? k|j ? ? k|j (t) 2 ? ? (14)</formula><p>The first term is the component of the score orthogonal to U k|j , and the second term is the score of the Gaussian approximation of x ? k|j . The divergence is normalized by the (approximate) expected norm of the Gaussian score, which enables values for different t, j, k to be compared. The expectation over x(t) can then be approximated using the training data. The divergence D(U k|k?1 ; t) then corresponds to the error that would be introduced by the upsampling step at time t.</p><p>Given a sequence of subspaces, the divergence threshold becomes the sole hyperparameter of the sampling process, as we can compute <ref type="bibr" target="#b13">(14)</ref> to determine the upsampling times t k for any threshold. Once the t k are known, we can estimate the runtime improvement over the full-dimensional score model. Thus, we can choose the subspaces sequence to minimize the estimated runtime. Additionally, it is more convenient to consider D F (U k|0 ; t) as opposed to D(U k|k?1 ; t), which corresponds to assuming that at time t k , x ? k|k?1 is sampled with variance ? ? k|0 rather than ? ? k|k?1 . <ref type="bibr" target="#b4">5</ref> The benefit of this approximation is that we can speak of the divergence purely as a property of the subspace, independent of the preceding subspace (if any). Thus, we can simultaneously plot the orthogonal Fisher divergence for each downsampling subspace, as illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>. The choice of intervals for any subspace sequence and divergence threshold can then be directly read off the plot.</p><p>As <ref type="figure" target="#fig_1">Figure 2</ref> shows, for standard image datasets there appears to be little utility to using more than one proper subspace, as the diffusion in intermediate dimensions would be very brief. On the other hand, training additional models is computationally expensive and adds to the sum of the model sizes required for inference. Thus, our experiments focus on subspace diffusions consisting of only one proper downsampling subspace. In particular, for CIFAR-10, we consider the 8 ? 8 and 16 ? 16 subspaces separately, while for CelebA-HQ and LSUN Church we consider only the 64 ? 64 subspace, which offers the best potential runtime improvement. <ref type="bibr" target="#b4">5</ref>. The difference is minimal as the variance of the perturbation kernel dominates either term for reasonable divergence thresholds.</p><p>Model FID ? DDIM <ref type="bibr" target="#b17">[18]</ref> 4.04 FastDPM <ref type="bibr" target="#b9">[10]</ref> 2.86 Bilateral DPM <ref type="bibr" target="#b10">[11]</ref> 2.38 Analytic DPM <ref type="bibr" target="#b1">[2]</ref> 3.04 Prog. Distillation <ref type="bibr" target="#b15">[16]</ref> 2.57 CLD-SGM <ref type="bibr" target="#b3">[4]</ref> 2.23 LSGM <ref type="bibr" target="#b20">[21]</ref> 2.10 Adaptive solver <ref type="bibr" target="#b7">[8]</ref> 2  Fisher divergence threshold </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>We demonstrate the utility and versatility of our method by improving upon and accelerating the capabilities of state-of-the-art continuous score-based models. Specifically, we take the pretrained models on CIFAR-10, CelebA-256-HQ, and LSUN Church from <ref type="bibr" target="#b19">[20]</ref> as full-dimensional score models, train additional subspace score models of the same architecture, and use them together in the subspace diffusion framework. All lower-dimensional models are trained with the same hyperparameters and training procedure as the original model (see Appendix B). During inference, we use the unmodified reverse SDE solvers and the same number and spacing of denoising steps. We investigate results for a range of divergence thresholds, corresponding to different durations of diffusion in the subspace.</p><p>For all experiments, further results and additional samples may be found in Appendix C and Appendix D, respectively.</p><p>Unconditional sampling We evaluate subspace diffusion on unconditional CIFAR-10 generation with the Inception score (IS) and distance (FID) as metrics. We examine both the NCSN++ and DDPM++ models from <ref type="bibr" target="#b19">[20]</ref>, which correspond to different forward diffusion processes, as well as the deep versions of these models, for a total of 4 full-dimensional models. For each model, we separately construct subspace diffusion with 8 ? 8 and 16 ? 16 subspaces. As in <ref type="bibr" target="#b19">[20]</ref>, we choose the best checkpoint by FID.</p><p>In <ref type="figure" target="#fig_2">Figure 3</ref>, we show the performance of the NCSN++ subspace diffusion models for different choices of the Fisher divergence threshold D F . In all cases, the models display U-shaped performance curves as the threshold is varied. When the threshold is small, most of the diffusion is done at full dimensionality, and the performance is close to that of the full model alone. As the threshold increases and more diffusion is done in the subspace, the models improve over the full model until reaching the best performances between D F = 1 ? 10 ?3 and D F = 3 ? 10 ?3 . This improvement offers support for the hypothesis, discussed in the introduction, that restricting the dimensionality (or support) of the score to be matched can help the subspace model learn and extrapolate more ac-  <ref type="figure" target="#fig_1">Figure 2</ref>, there is little deterioration in quality for images generated with up to 60% of the diffusion in the subspace.</p><p>curately than the full-dimensional model. Finally, for large thresholds the performance deteriorates as the Gaussian approximation of the orthogonal component becomes too inaccurate. <ref type="table" target="#tab_2">Table 2</ref> compares the performance of the best subspace diffusion models with the original fulldimensional models from <ref type="bibr" target="#b19">[20]</ref> and with prior methods for accelerating score-based models. Subspace diffusion and LSGM <ref type="bibr" target="#b20">[21]</ref> are the only methods where the improved runtime does not come at the cost of decreased performance (relative to <ref type="bibr" target="#b19">[20]</ref>). The runtime improvement over the fulldimensional baseline varies with the choice of divergence threshold; for those leading to the best sample qualities, the improvements are typically around 30%. Since the concept of subspace diffusion is orthogonal to the techniques used by most previous work (see <ref type="bibr">Section 2)</ref>, it can potentially be used in combination with them for further runtime improvement.</p><p>Next, we show the applicability of our method to higher-resolution datasets by generating samples on CelebA-HQ-256 with NCSN++ subspace diffusion. As discussed in Section 3.4, we use only the 64 ? 64 subspaces and perform no hyperparameter tuning or checkpoint selection. In <ref type="figure" target="#fig_3">Figure 4</ref>, we show random samples from CelebA-HQ for different amounts of diffusion in the subspace, along with the corresponding Fisher divergence. Qualitatively, we can restrict up to 60-70% of the diffusion to the subspace without significant loss of quality.   <ref type="bibr" target="#b19">[20]</ref>. In <ref type="table" target="#tab_4">Table 3</ref>, we show results for these tasks on CIFAR-10 for subspace diffusion in combination with the DDPM++ (deep) model. We use the alternate subspace score formulation (11) with the original ODE solvers, and use the last checkpoint of each training run. Subspace diffusion has little to no impact on the log-likelihoods obtained and slightly hurts sample quality.</p><p>Inpainting Subspace diffusion can also be used for controllable generation tasks, an example of which is image inpainting. Indeed, by using the alternate formulation (11), the subspace model ap-pears as a full-dimensional model and integrates seamlessly with the existing inpainting procedures described in <ref type="bibr" target="#b19">[20]</ref>. In <ref type="figure" target="#fig_3">Figure 4</ref>, we show inpainting results on LSUN Church with 64 ? 64 subspace diffusion in conjunction with the pretrained NCSN++ model. As with the unconditional samples, quality does not visibly deteriorate with up to 60% of the diffusion occurring in the subspace.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We presented a novel method for more efficient generative modeling with score-based models. Subspace diffusion models restrict part of the diffusion to lower-dimensional subspaces such that the score of the projected distribution is faster to compute and easier to learn. Empirically on image datasets, our method provides inference speed-ups while preserving or improving the performance and capabilities of state-of-the-art models. Potential avenues of future work include applying subspace diffusion to other data domains and combining it with step-size based methods for accelerating inference. More generally, we hope that our work opens up further research on dimensionality reduction in diffusion processes, particularly to nonlinear manifolds and/or learned substructures, as a means of both simplifying and improving score-based generative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PATCH-PCA</head><p>We investigate the optimality of the downsampling subspaces in comparison with the best possible subspaces that produce an image-structured latent. Recall that the downsampling subspaces are defined as follows: suppose we have a full-resolution image X ? R (n?n?3) , with n an integer power of 2. Then the downsampled image X ? R (n/2?n/2?3) satisfies</p><formula xml:id="formula_18">X [a, b] = 1 2 (i,j)?{0,1} 2 X[2a + i, 2b + j]<label>(15)</label></formula><p>where each element X[i, j] is an RGB color in R 3 . For output pixel X [a, b], this is an operation over the 2 ? 2 patch of pixels X[2a + i, 2b + j] | (i, j) ? {0, 1} 2 , which can be regarded as an element of a 12-dimensional vector space. That is,</p><formula xml:id="formula_19">X [a, b] = f (X[2a + i, 2b + j] | (i, j) ? {0, 1} 2 ) f : R 12 ? R 3<label>(16)</label></formula><p>for f independent of a, b. The downsampling subspace corresponds to taking twice the mean of the input patch, but we can generalize to arbitrary linear functions and consider <ref type="bibr" target="#b15">(16)</ref> with any linear f to define an image-structured subspace. The key aspect of this definition is that each basis element of the subspace corresponds to a spatially localized set of input features, and that the transformation operates identically for all spatial locations in the original image.</p><p>To find the optimal n/2 ? n/2 image-structured subspace we run PCA over the 12-dimensional distribution of patches X[2a + i, 2b + j] | (i, j) ? {0, 1} 2 for all possible values of (a, b), and over all images (or as large a subset as is computationally feasible). We then project each patch onto the top three principal components to form the smaller image. This definition and procedure can be naturally extended to smaller subspaces by considering the input patches of 4     </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Visual schematic of subspace diffusion with one projection step. Top left:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Orthogonal Fisher divergence plots computed with respect to the pretrained NCSN++ full-dimensional score models from</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>CIFAR-10 sample quality from NCSN++ subspace diffusion (shallow and deep models) with different subspaces and divergence thresholds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Random high resolution samples with 64 ? 64 subspace diffusion. Top: Inpainting on the 256 ? 256 LSUN Church dataset. Bottom: Unconditional generation of samples for CelebA-HQ-256. From right to left, the fraction of the diffusion in the subspace increases in intervals of 20%, with the corresponding orthogonal Fisher divergence shown. As expected from the divergence analysis in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>4 :Figure 5 :</head><label>45</label><figDesc>NCSN++ subspace diffusion results on CIFAR-10 unconditional generation. Runtimes are reported as percentages of the respective full diffusion model. D ADDITIONAL SAMPLES Random samples from CIFAR-10 using the NCSN++ deep 16 ? 16 subspace diffusion. Each row shows samples with an extra 10% of the diffusion on the full-dimensional space (from 0% at the top to 100% at the bottom). High quality samples start appearing with 50-60% of the diffusion at full dimensionality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Random samples from CelebA-HQ using the NCSN++ 64x64 subspace diffusion. Each row shows samples with an extra 10% of the diffusion on the full-dimensional space (from 0% at the top to 100% at the bottom). High quality samples start appearing with 30-40% of the diffusion at full dimensionality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :60%D F = 8 x 10 - 3 40%D F = 2 x 10 - 5 20%D F = 4 x 10 - 6 0%D F = 4 x 10 - 6 Figure 8 :</head><label>735668</label><figDesc>Random samples from LSUN Church using the NCSN++ 64 ? 64 subspace diffusion. Each row shows samples with an extra 10% of the diffusion on the full-dimensional space (from 0% at the top to 100% at the bottom). High quality samples start appearing with 40% of the diffusion at full dimensionality. Random samples of inpainting procedure from LSUN Church using the NCSN++ 64 ? 64 subspace diffusion, with different proportions of subspace diffusion (reported at the top along with the corresponding Fisher divergence threshold).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>CIFAR-10 sample quality for 50k images. Left: the best performance of previous methods to accelerate score-based models. Right: the original full diffusion from<ref type="bibr" target="#b19">[20]</ref> and the respective best subspace diffusion (all 16 ? 16), with the corresponding divergence threshold, downsampling time t 1 , and empirical runtime relative to the full model.</figDesc><table><row><cell>FID</cell><cell>2.2 2.3 2.4 2.5 2.6</cell><cell>deep full deep subspace 16 deep subspace 8</cell><cell>2.3 2.4 2.5 2.6 2.7 2.8</cell><cell>shallow full shallow subspace 16 shallow subspace 8</cell></row><row><cell></cell><cell cols="2">0.0001 0.0003 0.001 0.003 0.01 0.03</cell><cell cols="2">0.0001 0.0003 0.001 0.003 0.01 0.03</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>ODE sampling and NLL evaluation on CIFAR-10 from DDPM++ (deep) with subspace diffusion.</figDesc><table><row><cell>ODE sampling and likelihood Subspace dif-</cell></row><row><cell>fusion retains the flexible capabilities of the</cell></row><row><cell>continuous-time SDE framework. In particular, the</cell></row><row><cell>corresponding probability flow ODE (3) can be used</cell></row><row><cell>to evaluate exact log-likelihoods and generate sam-</cell></row><row><cell>ples, as described in</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>? 4, 8 ? 8 pixels as vector spaces of dimensionality 48, 192, etc.</figDesc><table><row><cell cols="2">C DETAILED RESULTS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell>Subspace</cell><cell>Threshold</cell><cell>t 1</cell><cell>Runtime</cell><cell>FID ?</cell><cell>IS ?</cell></row><row><cell></cell><cell>None</cell><cell>-</cell><cell>-</cell><cell>100%</cell><cell>2.38</cell><cell>9.93</cell></row><row><cell></cell><cell></cell><cell>1 ? 10 ?4</cell><cell>0.64</cell><cell>75%</cell><cell>2.45</cell><cell>9.81</cell></row><row><cell></cell><cell></cell><cell>3 ? 10 ?4</cell><cell>0.60</cell><cell>72%</cell><cell>2.37</cell><cell>9.87</cell></row><row><cell>NCSN++ shallow (VE)</cell><cell>16 ? 32</cell><cell>1 ? 10 ?3 3 ? 10 ?3 1 ? 10 ?2 3 ? 10 ?2</cell><cell>0.56 0.52 0.47 0.42</cell><cell>69% 66% 63% 59%</cell><cell>2.31 2.29 2.46 2.67</cell><cell>9.95 9.99 9.96 9.93</cell></row><row><cell></cell><cell></cell><cell>1 ? 10 ?4</cell><cell>0.69</cell><cell>72%</cell><cell>2.41</cell><cell>9.90</cell></row><row><cell></cell><cell></cell><cell>3 ? 10 ?4</cell><cell>0.64</cell><cell>68%</cell><cell>2.39</cell><cell>9.83</cell></row><row><cell></cell><cell>8 ? 32</cell><cell>1 ? 10 ?3 3 ? 10 ?3</cell><cell>0.60 0.56</cell><cell>64% 60%</cell><cell>2.29 2.35</cell><cell>9.92 10.08</cell></row><row><cell></cell><cell></cell><cell>1 ? 10 ?2</cell><cell>0.51</cell><cell>56%</cell><cell>2.74</cell><cell>10.09</cell></row><row><cell></cell><cell></cell><cell>3 ? 10 ?2</cell><cell>0.46</cell><cell>52%</cell><cell>3.42</cell><cell>10.10</cell></row><row><cell></cell><cell>None</cell><cell>-</cell><cell>-</cell><cell>100%</cell><cell>2.20</cell><cell>9.89</cell></row><row><cell></cell><cell></cell><cell>1 ? 10 ?4</cell><cell>0.64</cell><cell>75%</cell><cell>2.25</cell><cell>9.86</cell></row><row><cell></cell><cell></cell><cell>3 ? 10 ?4</cell><cell>0.60</cell><cell>73%</cell><cell>2.19</cell><cell>9.93</cell></row><row><cell>NCSN++ deep (VE)</cell><cell>16 ? 32</cell><cell>1 ? 10 ?3 3 ? 10 ?3 1 ? 10 ?2 3 ? 10 ?2</cell><cell>0.56 0.52 0.47 0.42</cell><cell>69% 67% 63% 60%</cell><cell>2.17 2.23 2.31 2.51</cell><cell>9.94 9.91 9.90 9.82</cell></row><row><cell></cell><cell></cell><cell>1 ? 10 ?4</cell><cell>0.69</cell><cell>72%</cell><cell>2.22</cell><cell>9.85</cell></row><row><cell></cell><cell></cell><cell>3 ? 10 ?4</cell><cell>0.64</cell><cell>68%</cell><cell>2.24</cell><cell>9.87</cell></row><row><cell></cell><cell>8 ? 32</cell><cell>1 ? 10 ?3 3 ? 10 ?3</cell><cell>0.60 0.56</cell><cell>64% 60%</cell><cell>2.21 2.39</cell><cell>9.92 10.08</cell></row><row><cell></cell><cell></cell><cell>1 ? 10 ?2</cell><cell>0.51</cell><cell>56%</cell><cell>3.05</cell><cell>10.01</cell></row><row><cell></cell><cell></cell><cell>3 ? 10 ?2</cell><cell>0.46</cell><cell>51%</cell><cell>3.51</cell><cell>9.96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">. Note, however, that the hierarchical prior has greater dimensionality than the data. 2. In CLD-SGM, one must marginalize over the momentum variables; and in LSGM one must marginalize over the latent variable of VAE.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">. It is via this choice of subspace that subspace diffusion superficially resembles the cascading models discussed in Section 2.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">. That is, an N ? N subspace has dimensionality 3N 2 .</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head><p>We thank Yilun Du, Xiang Fu, Jason Yim, Shangyuan Tong, Yilun Xu, Felix Faltings, and Saro Passaro for helpful feedback and discussions. Bowen Jing acknowledges support from the Department of Energy Computational Science Graduate Fellowship.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B HYPERPARAMETERS</head><p>As mentioned in the main text, we did not tune any hyperparameters for training and directly used the default settings from <ref type="bibr" target="#b19">[20]</ref>, including checkpoint intervals. The sole exception was that we used reduced batch sizes due to different hardware constraints. We report FID and IS for the SDE sampler on CIFAR-10 using the best training checkpoint, as in <ref type="bibr" target="#b19">[20]</ref>. All other results are obtained using the last training checkpoint.</p><p>During inference, the only hyperparameter tuned was the number of conditional Langevin steps. We tried 0, 1, 2, 5, or 10 steps using the last training checkpoint of the 8 ? 8 NCSN++ model and chose the value leading to the best FID averaged across the cutoff times. We then used 2 steps for all experiments with the SDE sampler. The Langevin signal-to-noise ratio was fixed to 0.22 for NCSN++ and 0.01 for DDPM++ based on the best settings found in <ref type="bibr" target="#b19">[20]</ref>. All other inference hyperparameters were fixed to their default values.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reverse-time diffusion equation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anderson</forename><surname>Brian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic Processes and their Applications</title>
		<imprint>
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Analytic-dpm: an analytic estimate of the optimal reverse variance in diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Score-based generative modeling with critically-damped langevin diffusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Dockhorn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Implicit generation and generalization in energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Cascaded diffusion models for high fidelity image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salimans</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Gotta go fast when generating data with score-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexia</forename><surname>Jolicoeur-Martineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Pich?-Taillefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tal</forename><surname>Kachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Mitliagkas</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Variational diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">On fast sampling of diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Ping</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Invertible Neural Networks, Normalizing Flows, and Explicit Likelihood Models</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bddm: Bilateral denoising diffusion models for fast and high-quality speech synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Max</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generating high fidelity images with subscale pixel networks and multidimensional upscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved denoising diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generating diverse high-fidelity images with vq-vae-2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Razavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Image super-resolution via iterative refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chitwan</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Norouzi</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Progressive distillation for fast sampling of diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Noise estimation for generative diffusion models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>San-Roman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliya</forename><surname>Nachmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Denoising diffusion implicit models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenlin</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Score-based generative modeling through stochastic differential equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Score-based generative modeling in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karsten</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Learning to efficiently sample from diffusion probabilistic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Chan</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">ArXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">How neural networks extrapolate: From feedforward to graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken-Ichi</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
