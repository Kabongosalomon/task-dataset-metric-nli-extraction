<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ST-UNet: A Spatio-Temporal U-Network for Graph-structured Time Series Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoteng</forename><surname>Yin</surname></persName>
							<email>htyin@pku.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
							<email>zhanxing.zhu@pku.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Mathematical Sciences</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Center for Data Science (AAIS)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ST-UNet: A Spatio-Temporal U-Network for Graph-structured Time Series Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Center for Data Science, Peking University Beijing Institute of Big Data Research</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>spatio-temporal graph</term>
					<term>multi-scale framework</term>
					<term>U-network</term>
					<term>graph convolution</term>
					<term>dilated recurrent skip-connections</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The spatio-temporal graph learning is becoming an increasingly important object of graph study. Many application domains involve highly dynamic graphs where temporal information is crucial, e.g. traffic networks and financial transaction graphs. Despite the constant progress made on learning structured data, there is still a lack of effective means to extract dynamic complex features from spatio-temporal structures. Particularly, conventional models such as convolutional networks or recurrent neural networks are incapable of revealing the temporal patterns in short or long terms and exploring the spatial properties in local or global scope from spatiotemporal graphs simultaneously. To tackle this problem, we design a novel multi-scale architecture, Spatio-Temporal U-Net (ST-UNet), for graph-structured time series modeling. In this U-shaped network, a paired sampling operation is proposed in spacetime domain accordingly: the pooling (ST-Pool) coarsens the input graph in spatial from its deterministic partition while abstracts multi-resolution temporal dependencies through dilated recurrent skip connections; based on previous settings in the downsampling, the unpooling (ST-Unpool) restores the original structure of spatio-temporal graphs and resumes regular intervals within graph sequences. Experiments on spatio-temporal prediction tasks demonstrate that our model effectively captures comprehensive features in multiple scales and achieves substantial improvements over mainstream methods on several real-world datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>With the latest success of extending deep learning approaches from regular grids to structured data, graph representation learning has become an active research area nowadays. Many real-world data such as social relations, biological molecules and sensor networks are naturally with a graph form. Recently, there has been a surge of interests in exploring and analyzing the representation of graphs for tasks like node classification and link prediction <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11]</ref>. However, among those studies, the dynamic graph has received relatively less attention than the static graph that consists of fixed node values or labels. The spatio-temporal graph is one of typical dynamic * Both authors contributed equally to this research. ? Corresponding author. graphs, with varying input for each node along time axis, e.g. traffic sensor streaming and human action sequences. In this work, we systematically study the dynamic graph in spacetime domain, with an aim to develop a principled and effective method to interpret the spatio-temporal graph and to forecast future values or labels of certain nodes thereof, or to predict the whole graph in the next few time steps.</p><p>In the field of spatio-temporal data, videos are a well-studied example, whose successive frames consistently share spatial and temporal structures. By leveraging different types of neural networks, a hybrid framework is constructed to exploit such spatiotemporal regularities within video frames, for instance, applications in weather radar echoes <ref type="bibr" target="#b21">[22]</ref> and in traffic heatmaps <ref type="bibr" target="#b25">[26]</ref>. In this case, each frame in the video firstly passes through convolution neural networks (CNN) for visual feature extraction, and then followed by recurrent neural networks (RNN) for sequence learning. Even though images can be regarded as special cases of graphs, widely used deep learning models still face significant challenges in applying to spatio-temporal graphs. First, graph-structured data are generated from non-Euclidean domain, which may not align in regular grids as required by existing models. Second, compared to grid-like data, there is no spatial locality or order information among nodes of a graph. Due to such irregularities, standard operations (for example, convolution and pooling) are not directly applicable to graph domain.</p><p>To bridge the above gap, <ref type="bibr" target="#b0">[1]</ref> proposes graph convolutional networks (GCNs) redefining the notion of the convolution and generalizing it to arbitrary graphs based on spectral graph theory. The introduction of GCNs boosts the latest rapid development of graph study. Moreover, it has been successfully adopted in a variety of applications where the dynamic graph is strongly associated. For instance, in action recognition, human action sequences can be assembled as a spatio-temporal graph, where body joints are constituted as a series of skeleton graph changing along time axis. Correspondingly, <ref type="bibr" target="#b12">[13]</ref> designs a GCN-based model to capture the spatial patterns of skeleton sequences as well as the temporal dynamics contained therein. In traffic forecasting, each sensor station streams the traffic status of a certain road within a traffic network. In this sensor graph, the spatial edges are weighted by the pair-wised distance between stations in the network while the temporal ones are connected by the same sensors between adjacent time frames. Recent studies have investigated the feasibility of combining GCNs with RNN <ref type="bibr" target="#b13">[14]</ref> or CNN <ref type="bibr" target="#b22">[23]</ref> for traffic prediction on road networks. GCN-based models obtain considerable improvements compared to traditional ones that typically ignore the spatio-temporal correlations and lack in the capability for handling structured sequences.</p><p>In order to accurately understand local and global properties of dynamic graphs, it is necessary to process the data through multiple scales. The spatio-temporal graph particularly requires such scale-spanning analysis since its particularity and complexity in spacetime domain. However, most mainstream methods have overlooked such principle, partially because of the difficulties of extending existing operations like the pooling to graph data. Nevertheless, multi-scale modeling of the dynamic graph has the similarity with the pixel-wise prediction task, as an image pixel corresponding to a graph node. U-shaped networks with U-Net <ref type="bibr" target="#b17">[18]</ref> as the representative achieve state-of-the-art performance on pixel-level prediction, whose architecture has high representational capacity of both the local distributed and the global hidden information within the input. Thus, it is particularly appealing to apply such U-shaped design to modeling dynamic graphs.</p><p>In this paper, we propose a novel multi-scale framework, Spatio-Temporal U-Net (ST-UNet), to model and predict graph-structured time series. To precisely capture the spatio-temporal correlations in dynamic graphs, we firstly generalize the U-shaped architecture from images to spatio-temporal graphs. ST-UNet employs multi-granularity graph convolution for extracting both generalized and localized spatial features, and adds dilated recurrent skip-connections for capturing multi-resolution temporal dependencies. Under the settings of ST-UNet, we define two essential operations of the framework accordingly: the spatio-temporal pooling (ST-Pool) operation samples nodes to form a smaller graph from the output of deterministic graph partition <ref type="bibr" target="#b14">[15]</ref> and abstracts time series at multiple temporal resolutions through skip connections between recurrent units. Consequently, the unpooling (ST-Unpool) as a paired operation restores the original structure and temporal dependency of dynamic graphs based on previous settings in the downsampling. To better localize the representation from the input, higher-level features retrieved from the pooling part are concatenated with the upsampled output. Overall, with contributions of hierarchical U-shaped design, ST-UNet is able to effectively derive multi-scale features and precisely learn representations from the spatio-temporal graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Following spectral-based formulation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b16">17]</ref>, the graph convolution operator ' * ' is introduced as the multiplication of a graph signal ? R with a kernel (?) = ( ), where is a vector of Fourier coefficients, as *</p><formula xml:id="formula_0">= ( ) = ( ? ) = (?)<label>(1)</label></formula><p>where ? R ? is the graph Fourier basis, which is a matrix of eigenvectors of the normalized graph Laplacian = ? ? 1 2 ? 1 2 = ? ? R ? ( is an identity matrix and ? R ? is the diagonal degree matrix of adjacency matrix with = ? ); while ? ? R ? is the diagonal matrix of eigenvalues of <ref type="bibr" target="#b19">[20]</ref>. In order to localize the filter, the kernel can be restricted to a truncated expansion of Chebyshev polynomials (?) to ? 1 order with the rescaled? = 2?/ max ? as (?) = ?1 =0 (?), where ( ?) ? R is a vector of Chebyshev coefficients <ref type="bibr" target="#b7">[8]</ref>. Hence, the graph convolution can then be expressed as,</p><formula xml:id="formula_1">* = ( ) = ?1 ?? =0 (?) ,<label>(2)</label></formula><p>where (?) ? R ? is the Chebyshev polynomial of order evaluated at the rescaled Laplacian?= 2 / max ? . Apart from convolutional operations on graphs, there are also several recent studies focusing on structured sequence learning. Structured RNN <ref type="bibr" target="#b9">[10]</ref> attempts to fit the spatio-temporal graph into a mixture of recurrent neural networks by associating each node and edge to a certain type of the networks. Based on the framework of convLSTM <ref type="bibr" target="#b21">[22]</ref>, graph convolutional recurrent network (GCRN) <ref type="bibr" target="#b18">[19]</ref> is firstly proposed modeling structured sequences by replacing regular 2D convolution with spectral-based graph convolution. And it has set a trend of GCN-embedded designs for the follow-up studies <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b22">23]</ref>. The fast pooling operation on graph signals was firstly introduced by <ref type="bibr" target="#b3">[4]</ref>. Recently, graph U-Net <ref type="bibr" target="#b4">[5]</ref> brings pooling and upsampling operations to graph data. However, the applicable scope of these operations is bounded by the static graph. Especially, graph U-Net introduces extra training parameters for node selection during the pooling procedure. Furthermore, its pooling operation does not keep the original structure of the input graph that may raise an issue for those tasks whose local spatial relations are critical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we start with the definition of the spatio-temporal graph and the problem formulation of prediction tasks on it. The special design of U-shaped network is elaborated in the following with essential operations of pooling and upsampling defined on the spatio-temporal graph. Base on the above advances, a multiscale architecture, Spatio-Temporal U-Net, is introduced for graphstructured time series modeling eventually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Spatio-temporal Graph Modeling</head><p>Suppose spatio-temporal data are gathered through a structured spatial region consisting of nodes. Inside each node, there are measurements which vary over time. Thus, observation at any time can be represented by a feature vector ? R . Moreover, data collected over the whole region are able to be expressed in terms of a feature matrix = { 1 , 2 , . . . , } ? R ? . As time goes by, a chronological sequence of matrices 1 , 2 , . . . , is accumulated, which can be further formalized as the spatio-temporal graph defined as follows.</p><p>Definition 3.1 (Spatio-temporal Graph). A spatio-temporal graph is an attributed graph with a time-variable feature matrix X. It is defined as G = (V, E, , X) where V is the set of |V | = vertices, E is the set of edges, and ? R ? is an adjacency matrix recording the weighted connectedness between two vertices. Contrary to the static graph, node attributes of the spatio-temporal one evolve over time as X = { 1 , 2 , . . . , } ? R ? ? , where is the length of time steps and is the dimension of features in each node.  <ref type="figure">Figure 1</ref>: An illustration of the proposed Spatio-Temporal U-Net architecture. ST-UNet employs graph convolutional gated recurrent units (GCGRU) as its backbone. In this example, the proposed framework contains three GCGRU layers formed as a U-shaped structure with one ST-Pool and one ST-Unpool applied in one side respectively. Spatio-temporal features obtained from the input are downsampled into multi-resolution representations through a ST-Pooling operation. As subgraph (a) represents, the input graph at each time step is equally coarsened into nearly a quarter of its original size at the level 2 combining with feature pooling regarding the channel dimension. Meanwhile, the temporal dependency of the input sequence is dilated to 2 with skip-connections crossing every other recurrent unit, as shown in subgraph (b). The ST-Unpooling, as a reverse operation, restores the spatio-temporal graph into its original structure with upsampling in spatial features and resumes regular dependencies of time series concurrently. To assemble a more precise output with better localized representations, high-level features of the pooling side are fused with the upsampled output through a skip connection at the same level. The final output can be utilized for predicting node attributes or the entire graph in the next few time steps.</p><p>In practice, due to structural properties of the data, spatio-temporal graph modeling can be formulated as the prediction task of graphstructured time series. The objective of this task is to accurately predict future attributes of nodes in a given spatio-temporal graph based on historical records, which is formally described below. Definition 3.2 (Spatio-temporal Prediction). Spatio-temporal prediction aims to forecast the most likely future length-sequence of node attributes in a graph given the previous observations:</p><formula xml:id="formula_2">+1 , . . . ,?+ = arg max +1 , ..., + +1 , . . . , + | ? +1 , . . . , ; G<label>(3)</label></formula><p>where ? X is an observation of node attributes linked by a weighted graph G at time step .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Pooling Operation on Spatio-temporal Graphs</head><p>The spatio-temporal graph can be decomposed into two domains: graph-structured data in spatial while time series in temporal. As a result, it inherits the characteristics of structural complexity from graphs and dynamic complexity from sequences. Therefore, we discuss the downsampling approaches applied from the spatial and the temporal perspective respectively in this section. Lastly, a unified pooling operation is defined in spacetime domain.</p><p>Spatial Graph Pooling. Pooling layers play a vital role in CNNs since its function of achieving feature reduction. It generally follows the convolutional layer to progressively reduce spatial resolution of feature maps and enlarge receptive fields, thereby controlling parameter overfitting and achieving better generalization. However, the standard pooling operation is not directly applicable to graphstructured data, since it requires distinct neighborhoods which are obviously not accessible from graphs. Besides the local pooling, there are operations imposed on the input generally that could bypass the requirement of locality information, such as global pooling and -max pooling. But these pooling approaches also bring issues of limited flexibility and inconsistent selection <ref type="bibr" target="#b4">[5]</ref>.</p><p>It is indispensable for the pooling in extracting multilevel abstraction of graphs. Similar to the pooling operation used in <ref type="bibr" target="#b3">[4]</ref>, we use the global path growing algorithm (GPA) <ref type="bibr" target="#b14">[15]</ref> instead to perform graph partitions by solving the maximum weight matching problem (noted as 'MaxWeightMatching'). GPA is consist of the greedy algorithm and the improved path growing algorithm (PGA') <ref type="bibr">[? ]</ref>. Given a graph = ( , , ) with nodes at time step , PGA' finds an approximate solution to the problem with a subset of edges ? satisfying: 1) there are no two members of sharing an endpoint; 2) its total weights are the largest. Subsequently, it generates the partition through gradually removing edges in and merging nodes connected thereof, as the Algorithm 1 describes. At each level, it reduces the size of a graph by the factor of two, producing a coarser graph corresponding to observing the data domain at a different resolution:</p><formula xml:id="formula_3">? = ( , ), ? G (4) where ? = ( ? , ? , ? )</formula><p>is a partitioned graph with ? ? /2 nodes at the level which controls reduction scale of the input. ? is a set of super nodes, each element ? of which contains a disjoint subset of . We use (?) to denote mapping relations between nodes in and ? . Formally, after the graph convolutional layer, we can acquire the convolved feature matrix of a coarser graph ? through the graph partition algorithm as</p><formula xml:id="formula_4">? = ( * ),<label>(5)</label></formula><p>where ? R ? is a graph signal matrix with attributes in each node of</p><formula xml:id="formula_5">while ? = { ? 1 , ? 2 , . . . , ? | ? ? R ? } is a length-? feature matrix</formula><p>with channels in each node of ? and is the number of nodes contained in each super node ? ? ? . Finally, we employ the maximum or mean feature activation over nodes in partitioned regions to obtain pooled features in each ? of ? regarding the channel dimension as</p><formula xml:id="formula_6">= gPooling( ? ),<label>(6)</label></formula><p>where ? R ? ? is the output of spatial graph pooling with -channel features on ? nodes. <ref type="figure">Figure 1 (a)</ref> shows an example of the spatial graph pooling. Since graph partition is calculated in advance, it makes the operation very efficient without introducing extra training parameters. Moreover, the scope of spatial graph pooling can be adjusted through the level which offers a precise control. In order to address the inconsistency issue in node selection, the deterministic result of graph partition in PGA' is equally applied to the graph at each time step.</p><p>Temporal Downsampling. Recurrent neural networks and its variants have shown impressive stability and capability of tackling sequence learning problems. Conventional recurrent models such as long short-term memory (LSTM) <ref type="bibr" target="#b8">[9]</ref> and gated recurrent units (GRU) <ref type="bibr" target="#b2">[3]</ref> are initially designed for regular sequences with fixed time intervals, which significantly limits their capacity for capturing complex data dependencies. Recently, several studies have explored how to expand the scope of recurrent units in RNNs to more sophisticated data like the spatio-temporal one. Based on fully-connected LSTM (FC-LSTM), <ref type="bibr" target="#b21">[22]</ref> develops a modified recurrent network with embedded convolutional layers (convLSTM) to forecast spatio-temporal sequences. Inside each recurrent unit, convolutional operations with kernels are substituted for multiplications by dense matrices, which enables the network for handling image sequences. Afterwards, <ref type="bibr" target="#b18">[19]</ref> extends this approach by replacing the standard convolution by the graph convolution for structured sequence modeling. Following the similar scheme, we leverage the GRU model and GCN layers as Graph Convolutional Gated Recurrent Units (GCGRU) to discover temporal patterns from graph-structured time series:</p><formula xml:id="formula_7">= ( * + * ? ?1 ) = ( * + * ? ?1 ) ? ? = tanh ( ? * + ? * ( ? ? ?1 )) ? = ? ? ?1 + (1 ? ) ? ? ?<label>(7)</label></formula><p>where ? is the Hadamard product and stands for non-linear activation functions. In this setting, and represent the gate of update and reset at time step ; while ? ? and ? denote the current memory content and final memory at current time step respectively. Both ( ?) ? R ? ? ? and ( ?) ? R ? ? ? ? are parameters of the size-graph convolutional kernel. We use the notion ' ( ?) * ' to describe the graph convolution between the graph signal and ? ? filters which are the functions of the graph Laplacian parameterized by -localized Chebyshev coefficients as Eq. (2) notes. By stacking several graph convolutional recurrent layers, the adopted backbone GCGRU can be used as a seq2seq model for graph-structured sequence learning.</p><p>The above architecture may be enough to model structured sequences by exploiting local stationarity and spatio-temporal correlations. But it still suffers from the restriction of interpreting temporal dynamic through determinate periods. In terms of multitimescale modeling, many attempts have been made to extend recurrent networks to various time scope, including phased LSTM <ref type="bibr" target="#b15">[16]</ref> and clockwork RNNs <ref type="bibr" target="#b11">[12]</ref>. Inspired by jumping design between recurrent units in <ref type="bibr" target="#b1">[2]</ref>, we insert the skip connection between gated recurrent units to learn graph-structured sequences in multilevel temporal dependencies. It also generates a dilation between successive cells, which is equivalent to abstract temporal features over a different resolution. Denote as the GCGRU cell in layer at time . The dilated skip connection can be expressed as</p><formula xml:id="formula_8">= , ? ,<label>(8)</label></formula><p>where is the input to layer at time ; denotes the skip length, also referred to the dilation of layer ; and (?) represents the GRU cell and output operations. <ref type="figure">Figure 1 (b)</ref> provides a diagram of the proposed temporal downsampling implemented by the dilated recurrent skip-connections. Such hierarchical design of dilation brings in multiple temporal scales for recurrent units at different layers. It also contributes to broadening the range of temporal dependency as the regular jump connection does but with fewer parameters and high efficiency.</p><p>In summary, based on the proposals made in pooling on spatiotemporal data, we define spatio-temporal pooling (ST-Pool) as the operation performing downsampling on a spatio-temporal graph by aggregating convolved features over non-overlapped partitions regarding the channel dimension on its spatial projection while dilating dynamic dependencies over recurrent units aligned in the same layer on its temporal projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Spatio-temporal Unpooling Operation</head><p>As the inverse operation of downsampling, the unpooling is crucial in the U-shaped network for recovering pooled features to their original resolution through upsampling. There are several approaches defined on grid-like data that could satisfy this aim, such as transposed convolution <ref type="bibr" target="#b24">[25]</ref> and unpooling layers <ref type="bibr" target="#b23">[24]</ref>. However, these operations are not directly applicable to spatio-temporal domain due to specialty and compositionality of its data. To this end, we propose spatio-temporal unpooling (ST-Unpool) accordingly: to restore primary structure of the input, the operation utilizes the reversed mapping ? (?) to place back merged nodes and edges from ? to ; to resume regular temporal dependencies between recurrent units, the output of each time step in a skip-connected layer are fed into a vanilla recurrent layer without further temporal dilation. Meanwhile, we provide three strategies for upsampling node attributes from a coarser graph, namely, direct copy, ordered deconv and weighted deconv. As the name suggests, the first approach directly copies features of a super node to each node it contains; while ordered deconvolution assigns parameterized features to each merged node based on its degree order. On top of ordered deconvolution, the weighted one concatenates structural information of merged nodes in a sub-graph as an embedded feature vector to upsampled features. All three methods of upsampling have been tested and compared in Section 4.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Architecture of Spatio-Temporal U-Net</head><p>Based on spatio-temporal pooling and unpooling operations proposed above, we develop a U-shaped multi-scale architecture, Spatio-Temporal U-Net, to address the challenge of analyzing and predicting graph-structured sequences. Following the classic U-shaped design, it contains two parts in symmetry: downsampling and upsampling. In the contracting part, it firstly applies graph convolution to aggregate information from each node's neighborhoods, and then follows by the ST-Pool layer to encode convolved features into multiple spatio-temporal resolution. In the expansive part, it utilizes the ST-Unpool layer for upsampling the reduced features to their original dimensions, with the concatenation of corresponding high-level features retrieved from the downsampling. In the end, one graph convolution layer is attached to propagate the information through multiple spatial scales for the final prediction. The illustration of proposed architecture presents in <ref type="figure">Figure 1</ref>. We now can summarize the main characteristics of ST-UNet in three aspects,</p><p>? To the best of our knowledge, it is the first time that a multi-scale network with U-shaped design is applied to learn and model spatio-temporal structures from graph-structured time series. ? A novel pair of operators in spatio-temporal pooling and unpooling are firstly proposed for extracting and fusing multilevel features in spacetime domain. ? The proposed framework ST-UNet achieves the balance between accuracy and efficiency with considerable scalability through multi-scale feature extraction and fusion as shown in the experiment below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTAL STUDIES</head><p>In this section, we present the evaluation of our model proposed in <ref type="bibr">Section 3.4</ref>. Several mainstream models are tested and analyzed on spatio-temporal prediction tasks. Experiments show that ST-UNet consistently outperforms other models and achieves state-of-the-art performance regarding prediction accuracy. We also perform the ablation study to validate the effectiveness of spatio-temporal pooling and unpooling operations. Comparison between GCN-based models suggests that ST-UNet has the superiority in balancing efficiency and scalability on the large-scale dataset. For a fair comparison, we execute grid search strategy to determine the best hyper-parameters on validations for all test models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Spatio-temporal Sequence Modeling on Moving-MNIST</head><p>In order to investigate the ability of node-level prediction, we compare ST-UNet with its plain version GCGRU on a synthetic dataset, moving-MNIST constructed by <ref type="bibr" target="#b21">[22]</ref>. It consists of 20-frame sequences (first 10 frames as input and the last for prediction), each of which contains two handwritten digits whose location is bouncing inside a 64 ? 64 patch. <ref type="bibr" target="#b0">1</ref> Following the default setup in [19], image frames are converted into spatio-temporal graphs. The adjacency matrix is constructed based on distances between each pixel node and its equal neighbors of a k-nearest-neighbor graph in four directions (up, down, left and right). Kernel size of graph convolution is set to 3 for both models. The visualized outcome of moving sequence prediction in <ref type="figure" target="#fig_2">Figure 2</ref> indicates that, thanks to hierarchical feature fusion in spacetime domain, the U-shaped network can learn better representation and obtain superior performance than the model purely based on GCNs in the node-level. It suggests the transferability of such multi-scale designs from regular grids to non-Euclidean domain as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph-structured Time-series Modeling on Traffic Prediction</head><p>Experimental Setup. For traffic prediction task, we conduct experiments on two real-world public datasets: METR-LA released by <ref type="bibr" target="#b13">[14]</ref> includes traffic information gathered by 207 loop detectors of Los Angeles County in 4 months, ranging from March 1st to June 30th of 2012; PeMS (M/L) generated by <ref type="bibr" target="#b22">[23]</ref> contains traffic status collected from monitoring stations deployed over California state highway system in the weekdays of May and June of 2012, containing 228 and 1026 stations respectively. Both datasets aggregate traffic records into a 5-min interval with an adjacency matrix describing the sensor topology of traffic networks. We use the same experimental settings of previous studies on these two datasets, including data preprocessing, dataset split, and other related configurations.</p><p>The following mainstream methods are selected as the baseline: 1). Historical Average (HA); 2). Linear Support Vector Regression (LSVR); 3). Auto-Regressive Integrated Moving Average (ARIMA); 4). Feedforward Neural Network (FNN); 5). Fully-Connected LSTM (FC-LSTM) <ref type="bibr" target="#b20">[21]</ref>; 6). Spatio-Temporal Graph Convolutional Networks (STGCN) <ref type="bibr" target="#b22">[23]</ref>; 7). Diffusion Convolutional Recurrent Neural Network (DCRNN) <ref type="bibr" target="#b13">[14]</ref>.</p><p>This task requires using observed traffic time series in the window of one hour to forecast future status in the next 15, 30, and 60 minutes. Thus, three standard metrics of sequence prediction are adopted to measure the performance of different methods, namely, Mean Absolute Errors (MAE), Mean Absolute Percentage Errors (MAPE), and Root Mean Squared Errors (RMSE).</p><p>ST-UNet Settings. All ST-UNet models use the kernel size = 3 for the graph convolution. Both spatial pooling level and temporal dilation are set at 2 with 'direct copy' employed as the upsampling approach. We train our models by using Adam optimizer to minimize the mean of 1 and 2 loss for 80 epochs with the batch size as 50. The schedule sampling and layer normalization are utilized in training for better generalization. The initial learning rate is 10 ?2 with a decay rate of 0.7 after every 8 epochs. The hidden size of recurrent units in our model is 96 for METR-LA dataset; while it is assigned to 64 for the rest.</p><p>Results Analysis. <ref type="table" target="#tab_0">Table 1</ref> demonstrates the numerical results of spatio-temporal traffic prediction on datasets METR-LA and PeMS-M. We observe the following phenomenon in both datasets: 1) graph convolution based models, including STGCN, DCRNN and ST-UNet generally outperform other baselines, which emphasizes the importance of including graph topology for traffic prediction. 2) RNN-based models tend to act better for the long-term prediction, suggesting their advantages in capturing temporal dependency. 3) regarding the adopted metrics, ST-UNet achieves the best performance for all three forecasting windows, which validates the effectiveness of multi-scale designs in spatio-temporal sequence modeling. 4) traditional approaches such as LSVR and ARIMA mostly perform worse than deep learning models, due to their limited capacities for handling complex non-linear data. In addition, historical average is a reflection of traffic status in a long-term, which is invariant to the short-term impact.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study of ST-Pool &amp; ST-Unpool</head><p>As the above two tasks reveal, ST-UNet steadily outperforms mainstream models for spatio-temporal prediction. But it may be argued that performance gains are actually due to the deeper architecture or benefit from multilevel abstraction in spatial or temporal alone. Therefore, we initiate an ablation study to investigate the contribution of spatio-temporal pooling and unpooling operations in our model. We conduct the experiment with ST-UNet in four styles: the plain version by removing all ST-Pool and ST-Unpool operations; T-UNet only with pooling and upooling in temporal; S-UNet only with pooling and unpooling in spatial; and the full version. To the aim of pure comparison, we only test these variants without additional training tricks. The numerical outcome in <ref type="table" target="#tab_1">Table 2</ref> confirms that the proposed operations are valid for model enhancement in both spatial and temporal dimension. Moreover, thanks to the multi-scale feature integration through U-shaped network, applying pooling and unpooling operations in space and time coherently results in further improvement and better generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison Study of Upsampling Approaches in ST-Unpool</head><p>As we discussed in Section 3.3, there are three methods for upsampling spatial features in the unpooling part. We carry out the experiment to examine the relation between these methods and the performance of corresponding models. Comparison of three upsampling approaches in terms of the mean square error is summarized in <ref type="table" target="#tab_2">Table 3</ref>. The method of direct copy generally performs better than the other two, especially in relatively long terms. It suggests that the simple mechanism may be more steady and robust in this case. Furthermore, local properties within a super node such as degree orders and connectedness may not contain enough information to support complex feature reconstruction, due to the isomorphism   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Scalability and Efficiency Study on Large-scale Graph Data</head><p>To test the scalability and efficiency of ST-UNet, we experiment our model and other GCN-based ones on a large dataset PeMS-L which contains over one thousand sensor nodes in a single graph. We list the comparison of prediction accuracy for four major models in <ref type="table" target="#tab_3">Table 4</ref>. Apparently, conventional graph convolution based approaches, including GCGRU and DCRNN face great challenges in handling such large-scale graphs. We use the symbol '?' to mark the model whose batch size is forced to reduce a half since its graphical memory consumption exceeded the capacity over a standard GPU card. <ref type="bibr" target="#b1">2</ref> By means of its fully convolutional structures, STGCN is able to process such large dataset at once. With the help of exploring -temporal correlations in a global view, it behaves well in short-and-mid term prediction but suffering from overfitting in long periods. On the other hand, DCRNN maintains a higher standard on long-term forecasting but with the cost of massive computational demands. For instance, the model normally takes more than 10 minutes to train one epoch with the batch size of 16 on PeMS-L. By contrast, ST-UNet confers better outcome in less half of the time that DCRNN need. It has reached the balance between time efficiency and prediction accuracy through spatial and temporal pooling operations applied. It also has advantages in extracting spatial features and temporal dependencies with fewer parameters and in multilevel abstraction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we propose a universal multi-scale architecture ST-UNet to learn and predict graph-structured time series, integrating multi-granularity graph convolution and dilated recurrent skipconnections through the U-shaped network design. Experiments show that our model consistently outperforms other state-of-theart methods on several real-world datasets, indicating its great potentials on extracting comprehensive spatio-temporal features through scale-spanning sequence modeling. The ablation study validates the efficiency improvement obtained from the proposed pooling and unpooling operations in spacetime domain. Moreover, ST-UNet also achieves the balance between efficiency and capacity with considerable flexibility. These features are quite promising and practical for structured sequence modeling in the future research development and industrial applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 : 3 := ??; 4 deterministically 11 : 23 repeat 24 :</head><label>134112324</label><figDesc>Graph Partition Algorithm (gPartition) Input: graph = ( , ), : ? R ?0 ; pooling level Output: graph ? = ( ? , ? ) with adjusted ? I. Edge Selection 1 A subset of edges, := ?; 2 while ? ? do choose ? with deg( ) &gt; 0; 5 while deg( ) &gt; 0 do 6 let = ( , ) be the heaviest edge adjacent to ; = ? MaxWeightMatching ( ); 12 extend to a maximal matching; 13 end 14 return ? II. Graph Coarsening 15 while ? ? do 16 remove selected in from ; 17 merge connected by as a super node ? ? ? ; 18 adjust adjacent edges ? with related weights ? of ? ; Part I &amp; II with ? at step 21 as input;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Qualitative results for moving MNIST. First row is the ground truth, second and third are the predictions of ST-UNet( = 2, = 3) and GCGRU( = 3) respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Performance comparison of different models on METR-LA and PeMS-M datasets.</figDesc><table><row><cell>Model</cell><cell>MAE</cell><cell cols="3">METR-LA (15/ 30/ 60 min) MAPE (%)</cell><cell>RMSE</cell><cell>MAE</cell><cell>PeMS-M (15/ 30/ 60 min) MAPE (%)</cell><cell>RMSE</cell></row><row><cell>HA</cell><cell>4.16</cell><cell></cell><cell>13.0</cell><cell></cell><cell>7.80</cell><cell>4.01</cell><cell>10.61</cell><cell>7.20</cell></row><row><cell>LSVR</cell><cell cols="3">2.97/ 3.64/ 4.67 7.68/ 9.9/ 13.63</cell><cell cols="2">5.89/ 7.35/ 9.13</cell><cell cols="2">2.50/ 3.63/ 4.54</cell><cell>5.81/ 8.88/ 11.50</cell><cell>4.55/ 6.67/ 8.28</cell></row><row><cell>ARIMA</cell><cell cols="2">3.99/ 5.15/ 6.90</cell><cell cols="3">9.6/ 12.7/ 17.4 8.21/ 10.45/ 13.23</cell><cell cols="2">5.55/ 5.86/ 6.83 12.92/ 13.94/ 17.34 9.00/ 9.13/ 11.48</cell></row><row><cell>FNN</cell><cell cols="2">3.99/ 4.23/ 4.49</cell><cell>9.9/ 12.9/ 14.0</cell><cell cols="2">7.94/ 8.17/ 8.69</cell><cell cols="2">2.39/ 3.41/ 4.88</cell><cell>5.53/ 8.16/ 12.12</cell><cell>4.40/ 6.40/ 8.84</cell></row><row><cell>FC-LSTM</cell><cell cols="2">3.44/ 3.77/ 4.37</cell><cell>9.6/ 10.9/ 13.2</cell><cell cols="2">6.30/ 7.23/ 8.69</cell><cell cols="2">3.67/ 3.87/ 4.19</cell><cell>9.09/ 9.57/ 10.55</cell><cell>6.58/ 7.03/ 7.79</cell></row><row><cell>STGCN</cell><cell cols="2">2.87/ 3.48/ 4.45</cell><cell>7.4/ 9.4/ 11.8</cell><cell cols="2">5.54/ 6.84/ 8.41</cell><cell cols="2">2.25/ 3.03/ 4.02</cell><cell>5.26/ 7.33/ 9.85</cell><cell>4.04/ 5.70/ 7.64</cell></row><row><cell>DCRNN</cell><cell cols="2">2.77/ 3.15/ 3.60</cell><cell>7.3/ 8.8/ 10.5</cell><cell cols="2">5.38/ 6.45/ 7.59</cell><cell cols="2">2.25/ 2.98/ 3.83</cell><cell>5.30/ 7.39/ 9.85</cell><cell>4.04/ 5.58/ 7.19</cell></row><row><cell>ST-UNet</cell><cell cols="3">2.72/ 3.12/ 3.55 6.9/ 8.4/ 10.0</cell><cell cols="2">5.13/ 6.16/ 7.40</cell><cell cols="2">2.15/ 2.81/ 3.38</cell><cell>5.06/ 6.79/ 8.33</cell><cell>4.03/ 5.42/ 6.68</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>? 0.037 3.004 ? 0.010 3.020 ? 0.017 2.980? 0.011 MAPE(%) 7.258 ? 0.031 7.294 ? 0.046 7.274 ? 0.050 7.124? 0.037 RMSE 5.688 ? 0.044 5.556 ? 0.051 5.544 ? 0.010 5.452? 0.019 60 min MAE 3.866 ? 0.094 3.850 ? 0.061 3.802 ? 0.052 3.756? 0.031 MAPE(%) 9.328 ? 0.098 9.228 ? 0.089 9.200 ? 0.148 8.844? 0.082 RMSE 7.210 ? 0.108 7.036 ? 0.149 7.052 ? 0.129 6.716? 0.025</figDesc><table><row><cell></cell><cell cols="5">Comparison of ST-UNet variants with or without</cell></row><row><cell cols="6">ST-Pool &amp; ST-Unpool operations in terms of prediction ac-</cell></row><row><cell cols="3">curacy on PeMS-M.</cell><cell></cell><cell></cell></row><row><cell></cell><cell>Models</cell><cell>GCGRU</cell><cell>T-UNet</cell><cell>S-UNet</cell><cell>ST-UNet</cell></row><row><cell>15 min</cell><cell cols="5">MAE MAPE(%) 5.316 ? 0.031 5.302 ? 0.019 5.300 ? 0.014 5.244? 0.028 2.292 ? 0.007 2.266 ? 0.005 2.272 ? 0.004 2.248? 0.004 RMSE 4.086 ? 0.010 4.014 ? 0.010 4.000 ? 0.006 3.994? 0.005</cell></row><row><cell>30 min</cell><cell>MAE</cell><cell>3.050</cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Comparison of different upsampling approaches in ST-Unpool in terms of MSE on PeMS-M (The notion ' ?' indicates that the test model may not converge eventually).</figDesc><table><row><cell>Models</cell><cell cols="3">Direct-Copy Ordered-Deconv Weighted-Deconv</cell></row><row><cell>15min</cell><cell>3.994 ? 0.005</cell><cell>3.980? 0.009</cell><cell>4.603 ? 0.074  ?</cell></row><row><cell>30min</cell><cell>5.452? 0.019</cell><cell>5.468 ? 0.015</cell><cell>5.658 ? 0.192  ?</cell></row><row><cell>60min</cell><cell>6.716? 0.025</cell><cell>6.956 ? 0.074</cell><cell>7.425 ? 0.488  ?</cell></row><row><cell cols="4">of its node elements and significant structural differences among</cell></row><row><cell>other nodes.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of GCN-based models in terms of prediction accuracy on the large-scale dataset PeMS-L. .43/ 4.08 5.76/ 8.45/ 10.28 4.40/ 6.25/ 7.62 STGCN 2.37/ 3.27/ 4.36 5.56/ 7.98/ 11.59 4.32/ 6.21/ 8.31 DCRNN ? 2.41/ 3.28/ 4.32 5.61/ 8.18/ 11.33 4.22/ 5.87/ 7.58 ST-UNet 2.34/ 3.02/ 3.66 5.54/ 7.56/ 9.52 4.32/ 5.81/ 7.14 spatio</figDesc><table><row><cell>Models</cell><cell>MAE</cell><cell>PeMS-L (15/ 30/ 60 min) MAPE (%)</cell><cell>RMSE</cell></row><row><cell>HA</cell><cell>4.60</cell><cell>12.50</cell><cell>8.05</cell></row><row><cell>GCGRU ?</cell><cell>2.48/ 3</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">To make it feasible for all test models, the image frame in moving-MNIST is downsampled to 32 ? 32 in the experiment of this section.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">All experiments are compiled and tested on a CentOS cluster (CPU: Intel(R) Xeon(R) CPU E5-2620 v4 @ 2.10GHz, GPU: NVIDIA GeForce GTX 1080).</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Dilated recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Witbrock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas S</forename><surname>Hasegawa-Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="77" to="87" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3844" to="3852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=HJePRoAct7" />
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Large-scale learnable graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1416" to="1424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Wavelets on graphs via spectral graph theory. Applied and Computational Harmonic Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Hammond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R?mi</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gribonval</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="129" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structural-RNN: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5308" to="5317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3511</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">A clockwork rnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Spatio-Temporal graph convolution for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaolong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Diffusion convolutional recurrent neural network: Data-driven traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Engineering algorithms for approximate weighted matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jens</forename><surname>Maue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Sanders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Experimental and Efficient Algorithms</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="242" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Phased lstm: Accelerating recurrent network training for long or event-based sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Chii</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3882" to="3890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning convolutional neural networks for graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Konstantin</forename><surname>Kutzkov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical Image Computing and Computer-assisted Intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Structured sequence modeling with graph convolutional recurrent networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjoo</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07659</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David I Shuman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1211.0053</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhourong</forename><surname>Shi Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dit-Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wai-Kin</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang-Chun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Spatio-temporal graph convolutional networks: A deep learning framework for traffic forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoteng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanxing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 27th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3634" to="3640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Adaptive deconvolutional networks for mid and high level feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Predicting citywide crowd flows using deep spatio-temporal residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dekang</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuwen</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">259</biblScope>
			<biblScope unit="page" from="147" to="166" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
