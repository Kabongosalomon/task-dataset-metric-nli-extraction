<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Pose for Action -Action for Pose</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Umar</forename><surname>Iqbal</surname></persName>
							<email>uiqbal@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Group</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Garbade</surname></persName>
							<email>garbade@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Group</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
							<email>gall@iai.uni-bonn.de</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Computer Vision Group</orgName>
								<orgName type="institution">University of Bonn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Pose for Action -Action for Pose</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:55+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this work we propose to utilize information about human actions to improve pose estimation in monocular videos. To this end, we present a pictorial structure model that exploits high-level information about activities to incorporate higher-order part dependencies by modeling action specific appearance models and pose priors. However, instead of using an additional expensive action recognition framework, the action priors are efficiently estimated by our pose estimation framework. This is achieved by starting with a uniform action prior and updating the action prior during pose estimation. We also show that learning the right amount of appearance sharing among action classes improves the pose estimation. We demonstrate the effectiveness of the proposed method on two challenging datasets for pose estimation and action recognition with over 80,000 test images. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Human pose estimation from RGB images or videos is a challenging problem in computer vision, especially for realistic and unconstrained data taken from the Internet. Popular approaches for pose estimation <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b37">[38]</ref> adopt the pictorial structure (PS) model, which resembles the human skeleton and allows for efficient inference in case of tree structures <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>. Even if they are trained discriminatively, PS models struggle to cope with the large variation of human pose and appearance. This problem can be addressed by conditioning the PS model on additional observations from the image. For instance, <ref type="bibr" target="#b17">[18]</ref> detects poselets, which are examples of consistent appearance and body part configurations, and condition the PS model on these.</p><p>Instead of conditioning the PS model on predicted configurations of body parts from an image, we propose to condition the PS model on high-level information like activity. Intuitively, the information about the activity of a person can provide a strong cue about the pose and vice versa the activity can be estimated from pose. There have been only few works <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b41">[42]</ref> that couple action recognition and pose estimation to improve pose estimation. In <ref type="bibr" target="#b38">[39]</ref>, action class confidences are used to initialize an optimization scheme for estimating the parameters of a subject-specific 3D human model in indoor multi-view scenarios. In <ref type="bibr" target="#b41">[42]</ref>, a database of 3D poses is used to learn a cross-modality regression forest that predicts the 3D poses from a sequence of 2D poses, which are estimated by <ref type="bibr" target="#b37">[38]</ref>. In addition, the action is detected and the 3D poses corresponding to the predicted action are used to refine the pose. However, both approaches cannot be applied to unconstrained monocular videos. While <ref type="bibr" target="#b38">[39]</ref> requires a subject-specific model and several views, <ref type="bibr" target="#b41">[42]</ref> requires 3D pose data for training. More recently, <ref type="bibr" target="#b14">[15]</ref> proposed an approach to jointly estimate action classes and refine human poses. The approach decomposes the human poses estimated at each video frame into subparts and tracks these sub-parts across time according to the parameters learned for each action. The action class and joint locations corresponding to the best part-tracks are selected as estimates for the action class and poses. The estimation of activities, however, comes at high computational cost since the videos are pre-processed by several approaches, one for pose estimation <ref type="bibr" target="#b15">[16]</ref> and two for extracting action related features <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b33">[34]</ref>.</p><p>In this work, we present a framework for pose estimation that infers and integrates activities with a very small computational overhead compared to an approach that estimates the pose only. This is achieved by an action conditioned pictorial structure (ACPS) model for 2D human pose estimation that incorporates priors over activities. The framework of the approach is illustrated in <ref type="figure">Figure 1</ref>. We first infer the poses for each frame with a uniform distribution over actions. While the binaries of the ACPS are modeled by Gaussian mixture models, which depend on the prior distribution over the action classes, the unaries of the ACPS model are estimated by action conditioned regression forests. To this end, we modify the approach <ref type="bibr" target="#b4">[5]</ref>, which consists of two layers of random forests, on two counts. Firstly, we replace the first layer by a convolutional network and use convolutional channel features to train the second layer, which consists of regression forests. Secondly, we condition the regression forests on a distribution over actions and learn the sharing among action classes. In our experiments, we show that these modifications increase the pose estimation accuracy by more than 40% compared to <ref type="bibr" target="#b4">[5]</ref>. After the poses are inferred with a uniform distribution over actions, we update the action prior and the ACPS model based on the inferred poses to obtain the final pose estimates. Since the update procedure is very efficient, we avoid the computational expensive overhead of <ref type="bibr" target="#b14">[15]</ref>.</p><p>We evaluate our approach on the challenging J-HMDB <ref type="bibr" target="#b12">[13]</ref> and Penn-Action <ref type="bibr" target="#b43">[44]</ref> datasets, which consist of videos collected from the Internet and contain large amount of scale and appearance variations. In our experiments, we provide a detailed analysis of the impact of conditioning unaries and binaries on a distribution over actions and the benefit of appearance sharing among action classes. We demonstrate the effectiveness of the proposed approach for pose estimation and action recognition on both datasets.  <ref type="figure">Fig. 1</ref>: Overview of the proposed framework. We propose an action conditioned pictorial structure model for human pose estimation <ref type="bibr" target="#b1">(2)</ref>. Both the unaries ? and the binaries ? of the model are conditioned on the distribution of action classes p A . While the pairwise terms are modeled by Gaussians conditioned on p A , the unaries are learned by a regression forest conditioned on p A (1). Given an input video, we do not have any prior knowledge about the action and use a uniform prior p A . We then predict the pose for each frame independently <ref type="bibr" target="#b2">(3)</ref>. Based on the estimated poses, the probabilities of the action classes p A are estimated for the entire video <ref type="bibr" target="#b3">(4)</ref>. Pose estimation is repeated with the updated action prior p A to obtain better pose estimates <ref type="bibr" target="#b4">(5)</ref>.</p><p>Compared to <ref type="bibr" target="#b14">[15]</ref>, the pose estimation accuracy is improved by over 30%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>State-of-the-art approaches for pose estimation are mostly based on neural networks <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b34">[35]</ref> or on the pictorial structure framework <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b42">[43]</ref>.</p><p>Several approaches have been proposed to improve the accuracy of PS models for human pose estimation. For instance, joint dependencies can be modeled not only by the PS model, but also by a mid-level image representation such as poselets <ref type="bibr" target="#b17">[18]</ref>, exemplars <ref type="bibr" target="#b22">[23]</ref> or data dependent probabilities learned by a neural network <ref type="bibr" target="#b0">[1]</ref>. Pose estimation in videos can be improved by taking temporal information or motion cues into account <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b44">[45]</ref>. In <ref type="bibr" target="#b15">[16]</ref> several pose hypotheses are generated for each video frame and a smooth configuration of poses over time is selected from all hypotheses. Instead of complete articulated pose, <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b1">[2]</ref> track individual body parts and regularize the trajectories of the body parts through the location of neighboring parts. Similar in spirit, the approach in <ref type="bibr" target="#b42">[43]</ref> jointly tracks symmetric body parts in order to better incorporate spatio-temporal constraints, and also to avoid double-counting. Optical flow information has also been used to enhance detected poses at each video frame by analyzing body motion in adjacent frames <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b44">[45]</ref>.</p><p>Recent approaches for human pose estimation use different CNN architectures to directly obtain the heatmaps of body parts <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b34">[35]</ref>. <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b34">[35]</ref> and <ref type="bibr" target="#b20">[21]</ref> use fully convolutional neural network architectures, where <ref type="bibr" target="#b34">[35]</ref> proposes a multi-staged architecture that sequentially refines the output in each stage. For pose estimation in videos, <ref type="bibr" target="#b16">[17]</ref> combines the heatmaps of body parts from multiple frames with optical flow information to leverage the temporal information in videos. More recently, <ref type="bibr" target="#b9">[10]</ref> proposes a convolutional recurrent neural network architecture that takes as input a set of video frames and sequentially estimates the body part locations in each frame, while also using the information of estimated body parts in previous frames.</p><p>As done in this work, a few works also combine both PS model and CNNs <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b28">[29]</ref>. In contrast to the approaches that use temporal information in videos for pose refinement, we utilize the detected poses in each video frame to extract high-level information about the activity and use it to refine the poses.</p><p>Action recognition based on 3D human poses has been investigated in many works <ref type="bibr" target="#b40">[41]</ref>. With the progress in the area of 2D human pose estimation in recent years, 2D poses have also gained an increased attention for action recognition <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b36">[37]</ref>. However, utilizing action recognition to aid human pose estimation is not well studied, in particular not in the context of 2D human pose estimation. There are only a few works <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b39">[40]</ref>, <ref type="bibr" target="#b41">[42]</ref> that showed the benefit of it. The approaches in <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr" target="#b41">[42]</ref> rely on strong assumption. The approach <ref type="bibr" target="#b38">[39]</ref> assumes that a person-specific 3D model is given and considers pose estimation in the context of multiple synchronized camera views. The approach <ref type="bibr" target="#b41">[42]</ref> focuses on 3D pose estimation from monocular videos and assumes that 3D pose data is available for all actions. The approach <ref type="bibr" target="#b30">[31]</ref> adopts a mixture of PS models, and learns a model for each action class. For a given image, each model is weighted by the confidence scores of an additional action recognition system and the pose with the maximum weight is taken. A similar approach is adopted in <ref type="bibr" target="#b39">[40]</ref> to model object-pose relations. These approaches, however, do not scale with the number of action classes since each model needs to be evaluated.</p><p>The closest to our work is the recent approach of [15] that jointly estimates the action classes and refines human poses. The approach first estimates human poses at each video frame and decomposes them into sub-parts. These subparts are then tracked across video frames based on action specific spatio-temporal constraints. Finally, the action labels and joint locations are inferred from the part tracks that maximize a defined objective function. While the approach shows promising results, it does not re-estimate the parts but only re-combines them over frames i. e., only the temporal constraints are influenced by an activity. Moreover, it relies on two additional activity recognition approaches based on optical flow and appearance features to obtain good action recognition accuracy that results in a very large computational overhead as compared to an approach that estimates activities using only the pose information. In this work, we show that additional action recognition approaches are not required, but predict the activities directly from a sequence of poses. In contrast to <ref type="bibr" target="#b14">[15]</ref>, we condition the pose model itself on activities and re-estimate the entire pose per frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. OVERVIEW</head><p>Our method exploits the fact that the information about the activity of a subject provides a cue about pose and appearance of the subject, and vice versa. In this work we utilize the high-level information about a person's activity to leverage the performance of pose estimation, where the activity information is obtained from previously inferred poses. To this end, we propose an action conditioned pictorial structure (PS) that incorporates action specific appearance and kinematic models. If we have only a uniform prior over the action classes, the model is a standard PS model, which we will briefly discuss in Section IV. <ref type="figure">Figure 1</ref> depicts an overview of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PICTORIAL STRUCTURE</head><p>We adopt the joint representation <ref type="bibr" target="#b4">[5]</ref> of the PS model <ref type="bibr" target="#b7">[8]</ref>, where the vector x j ? X represents the 2D location of the j th joint in image I, and X = {x j } j?J is the set of all body joints. The structure of a human body is represented by a kinematic tree with nodes of the tree being the joints j and edges E being the kinematic constraints between a joint j and its unique parent joint p as illustrated in <ref type="figure">Figure 1</ref>. The pose configuration in a single image is then inferred by maximizing the following posterior distribution:</p><formula xml:id="formula_0">p(X |I) ? j?J ? j (x j |I) j,p?E ? jp (x j , x p )<label>(1)</label></formula><p>where the unary potentials ? j (x j |I) represent the likelihood of the j th joint at location x j . The binary potentials ? jp (x j , x p ) define the deformation cost for the joint-parent pair (j, p), and are often modeled by Gaussian distributions for an exact and efficient solution using a distance transform <ref type="bibr" target="#b7">[8]</ref>. We describe the unary and binary terms in Section IV-A and Section IV-B, respectively. In Section V-A, we then discuss how these can be adapted to build an action conditioned PS model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Unary Potentials</head><p>Random regression forests have been proven to be robust for the task of human pose estimation <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b26">[27]</ref>. A regression forest F consists of a set of randomized regression trees, where each tree T is composed of split and leaf nodes. Each split node represents a weak classifier which passes an input image patch P to a subsequent left or right node until a leaf node L T is reached. As in <ref type="bibr" target="#b4">[5]</ref>, we use a separate regression forest for each body joint. Each tree is trained with a set of randomly sampled images from the training data. The patches around the annotated joint locations are considered as foreground and all others as background. Each patch consists of a joint label c ? {0, j}, a set of image features F P , and its 2D offset d P from the joint center. During training, a splitting function is learned for each split node by randomly selecting and maximizing a goodness measure for regression or classification. At the leaves the class probabilities p(c|L T ) and the distribution of offset vectors p(d|L T ) are stored.</p><p>During testing, patches are densely extracted from the input image I and are passed through the trained trees. Each patch centered at location y ends in a leaf node L T (P (y)) for each tree T ? F. The unary potentials ? j for the joint j at location x j are then given by</p><formula xml:id="formula_1">? j (x j |I) = y?? 1 |F| T ?F p(c = j|L T (P (y))) ? p(x ? y|L T (P (y)) . (2)</formula><p>In <ref type="bibr" target="#b4">[5]</ref> a two layer approach is proposed. The first layer consists of classification forests that classify image patches according to the body parts using a combination of color features, HOG features, and the output of a skin color detector. The second layer consists of regression forests that predict the joint locations using the features of the first layer and the output of the first layer as features. For both layers, the split nodes compare feature values at different pixel locations within a patch of size 24 ? 24 pixels.</p><p>We propose to replace the first layer by a convolutional network and extract convolutional channel features (CCF) <ref type="bibr" target="#b35">[36]</ref> from the intermediate layers of the network to train the regression forests of the second layer. In <ref type="bibr" target="#b35">[36]</ref> several pre-trained network architectures have been evaluated for pedestrian detection using boosting as classifier. The study shows that the "conv3-3" layer of the VGG-16 net <ref type="bibr" target="#b24">[25]</ref> trained on the ImageNet (ILSVRC-2012) dataset performs very well even without fine tuning, but it is indicated that the optimal layer depends on the task. Instead of pre-selecting a layer, we use regression forests to select the features based on the layers "conv2-2", "conv3-3", "conv4-3", and "conv5-3". An example of the CCF extracted from an image is shown in <ref type="figure" target="#fig_0">Figure 2</ref>. Since these layers are of lower dimensions than the original image, we upsample them using linear interpolation to make their dimensions equivalent to the input image. This results in a 1408 (128+256+512+512) dimensional feature representation for each pixel. As split nodes in the regression forests, we use axis-aligned split functions. For an efficient feature extraction at multiple image scales, we use patchwork as proposed in <ref type="bibr" target="#b10">[11]</ref> to perform the forward pass of the convolutional network only once.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Binary Potentials</head><p>Binary potentials ? jp (x j , x p ) are modeled as a Gaussian mixture model for each joint j with respect to its parent joint p in the kinematic tree. As in <ref type="bibr" target="#b4">[5]</ref>, we obtain the relative offsets between child and parent joints from the training data and cluster them into k = 1, . . . , K clusters using kmeans clustering. Each cluster k takes the form of a weighted Gaussian distribution as</p><formula xml:id="formula_2">? jp (x j , x p ) = w k jp ? exp ? 1 2 d jp ? ? k jp T ? k jp ?1 d jp ? ? k jp )<label>(3)</label></formula><p>with mean ? k jp and covariance ? k jp , where d jp = (x j ? x p ). The weights w k jp are set according to the cluster frequency p(k|j, p) ? with a normalization constant ? = 0.1 <ref type="bibr" target="#b4">[5]</ref>.</p><p>For inference, we select the best cluster k for each joint by computing the max-marginals for the root node and backtrack the best pose configuration from the maximum of the max-marginals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. ACTION CONDITIONED POSE ESTIMATION</head><p>As illustrated in <ref type="figure">Figure 1</ref>, our goal is to estimate the pose X conditioned by the distribution p A for a set of action classes a ? A. To this end, we introduce in Section V-A a pictorial structure model that is conditioned on p A . Since we do not assume any prior knowledge of the action, we estimate the pose first with the uniform distribution p A (a) = 1/|A|, ?a ? A. The estimated poses for N frames are then used to estimate the probabilities of the action classes p A (a|X n=1...N ), ?a ? A as described in Section V-B. Finally, the poses X n are updated based on the distribution p A .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Action Conditioned Pictorial Structure</head><p>In order to integrate the distribution p A of the action classes obtained from the action classifier into (1), we make the unaries and binaries dependent on p A :</p><formula xml:id="formula_3">p(X |p A , I) ? j?J ? j (x j |p A , I) ? j,p?E ? jp (x j , x p |p A ). (4)</formula><p>While the unary terms are discussed in Section V-A.1, the binaries ? jp (x j , x p |p A ) are represented by Gaussians as in <ref type="bibr" target="#b2">(3)</ref>. However, instead of computing mean and covariance from all training poses with equal weights, we weight each training pose based on its action class label and p A (a). In our experiments, we will also investigate the case where p A (a) is simplified to  <ref type="figure">Fig. 3</ref>: Example patches centered at the wrist of the left hand side. We can see a large amount of appearance variation for a single body part. However, for several activities, in particular sports such as golf and pull-up, this variation is relatively small within the action classes. Nonetheless, a few classes also share appearance with each other e.g., golf and baseball or activities such as run and kick ball. This clearly shows the importance of class specific appearance models with a right amount of appearance sharing across action classes for efficient human pose estimation.</p><formula xml:id="formula_4">p A (a) = 1 if a = argmax a p A (a |X n=1...N ) 0 otherwise.<label>(5)</label></formula><p>1) Conditional Joint Regressors: <ref type="figure">Figure 3</ref> shows examples of patches of the wrist extracted from images of different action classes. We can see a large amount of appearance variation across different classes regardless of the fact that all patches belong to the same body joint. However, it can also be seen that within individual activities this variation is relatively small. We exploit this observation and propose action specific unary potentials for each joint j. To this end we adopt conditional regression forests <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b26">[27]</ref> that have been proven to be robust for facial landmark detection in <ref type="bibr" target="#b3">[4]</ref> and 3D human pose estimation in <ref type="bibr" target="#b26">[27]</ref>. While <ref type="bibr" target="#b3">[4]</ref> trains a separate regression forest for each head pose and selects a specific regression forest conditioned on the output of a face pose detector, <ref type="bibr" target="#b26">[27]</ref> proposes partially conditioned regression forests, where a forest is jointly trained for a set of discrete states of a human attribute like human orientation or height and the conditioning only happens at the leaf nodes. Since the continuous attributes are discretized, interpolation between the discrete states is achieved by sharing the votes.</p><p>In this work we resort to partially conditional forests due to its significantly reduced training time and memory requirements. During training we augment each patch P with its action class label a. Instead of p(c|L T ) and p(d|L T ), the leaf nodes model the conditional probabilities p(c|a, L T ) and p(d|a, L T ). Given the distribution over action classes p A , we obtain the conditional unary potentials:</p><formula xml:id="formula_5">? j (x j |p A , I) = y?? 1 |F| T ?F a?A p A (a)</formula><p>? p(c = j|a, L T (P (y))).p(x ? y|a, L T (P (y)) = a?A ? j (x j |a, I)p A (a).</p><p>Since the terms ? j (x j |a, I) need to be computed only once for an image I, ? j (x j |p A , I) can be efficiently computed after an update of p A .</p><p>2) Appearance Sharing Across Actions: Different actions sometimes share body pose configurations and appearance of parts as shown in <ref type="figure">Figure 3</ref>. We therefore propose to learn the sharing among action classes within a conditional regression forest. To this end, we replace the term ? j (x j |a, I) in (6) by a weighted combination of the action classes:</p><formula xml:id="formula_7">? sharing j (x j |a, I) = a ?A ? a (a )? j (x j |a , I)<label>(7)</label></formula><p>where the weights ? a (a ) represent the amount of sharing between action class a and a . To learn the weights ? a for each class a ? A, we apply the trained conditional regression forests to a set of validation images scaled to a constant body size and maximize the response of (7) at the true joint locations and minimize it at non-joint locations. Formally, this can be stated as</p><formula xml:id="formula_8">? a = argmax ? na j a ?A ?(a )? * j x gt j,na |a , I na ? max x?X neg j,na a ?A ?(a )? * j (x|a , I na ) ? ? ? 2 (8)</formula><p>subject to a ?A ?(a ) = 1 and ?(a ) ? 0. I na denotes the n th scaled validation image of action class a, x gt j,na is the annotated joint position for joint j in image I na , and X neg j,na is a set of image locations which are more than 5 pixels away from x gt j,na . The set of negative samples is obtained by computing ? * j (x|a , I na ) and taking the 10 strongest modes, which do not correspond to x gt j,na , for each image. For optimization, we use the smoothed unaries</p><formula xml:id="formula_9">? * j (x|a, I) = y?? exp ? x ? y 2 ? 2 ? j (y|a, I) (9)</formula><p>with ? = 3 and replace max by the softmax function to make (8) differentiable. The last term in <ref type="formula">(8)</ref> is a regularizer that prefers sharing, i. e., ? 2 attains its minimum value for uniform weights. In our experiments, we use ? = 0.4 as weight for the regularizer. We optimize the objective function by constrained local optimization using uniform weights for initialization ?(a ) = 1/|A|. In our experiments, we observed that similar weights are obtained when the optimization is initialized by ?(a) = 1 and ?(a ) = 0 for a = a, indicating that the results are not sensitive to the initialization. In (8), we learn the weights ? a for each action class but we could also optimize for each joint independently.</p><p>In our experiments, however, we observed that this resulted in over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Action Classification</head><p>For pose-based action recognition, we use the bag-of-word approach proposed in <ref type="bibr" target="#b12">[13]</ref>. From the estimated joint positions X n=1...N , a set of features called NTraj+ is computed that encodes spatial and directional joint information. Additionally, differences between successive frames are computed to encode the dynamics of the joint movements. Since we use a body model with 13 joints, we compute the locations of missing joints (neck and belly) in order to obtain the same 15 joints as in <ref type="bibr" target="#b12">[13]</ref>. We approximate the neck position as the mean of the face and the center of shoulder joints. The belly position is approximated by the mean of the shoulder and hip joints.</p><p>For each of the 3, 223 descriptor types, a codebook is generated by running k-means 8 times on all training samples and choosing the codebook with minimum compactness. These codebooks are used to extract a histogram for each descriptor type and video. For classification, we use an SVM classifier in a multi-channel setup. To this end, for each descriptor type t, we compute a distance matrix D t that contains the ? 2 -distance between the histograms (h t i , h t j ) of all video pairs (v i , v j ). We then obtain the kernel matrix that we use for the multi-class classification as follows</p><formula xml:id="formula_10">K(v i , v j ) = exp ? 1 L L t=1 D t (h t i , h t j ) ? t<label>(10)</label></formula><p>where L is the number of descriptor types and ? t is the mean of the distance matrix D t . For classification we use a one-vs-all approach with C = 100 for the SVM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS</head><p>In order to evaluate the proposed method, we follow the same protocol as proposed in <ref type="bibr" target="#b14">[15]</ref>. In particular, we evaluate the proposed method on two challenging datasets, namely sub-J-HMDB <ref type="bibr" target="#b12">[13]</ref> and the Penn-Action dataset <ref type="bibr" target="#b43">[44]</ref>. Both datasets provide annotated 2D poses and activity labels for each video. They consist of videos collected from the Internet and contain large amount of scale and appearance variations, low resolution frames, occlusions and foreshortened body poses. This makes them very challenging for human pose estimation. While sub-J-HMDB <ref type="bibr" target="#b12">[13]</ref> comprises videos from 12 action categories with fully visible persons, the Penn-Action dataset comprises videos from 15 action categories with a large amount of body part truncations. As in <ref type="bibr" target="#b14">[15]</ref>, we discard the activity class "playing guitar" since it does not contain any fully visible person. For testing on sub-J-HMDB, we follow the 3-fold cross validation protocol proposed by <ref type="bibr" target="#b12">[13]</ref>. On average for each split, this includes 229 videos for training and 87 videos for testing with 8, 124 and 3, 076 frames, respectively. The Penn-Action dataset consists of 1, 212 videos for training and 1, 020 for testing with 85, 325 and 74, 308 frames, respectively. To evaluate the performance of pose estimation, we use the APK (Average Precision of Keypoints) metric <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b37">[38]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Implementation Details</head><p>For the Penn-Action dataset, we split the training images half and half into a training set and a validation set. Since the dataset sub-J-HMDB is smaller, we create a validation set by mirroring the training images. The training images are scaled such that the mean upper body size is 40 pixels. Each forest   <ref type="bibr" target="#b4">[5]</ref> with the proposed convolutional channel features (CCF). APK with threshold 0.1 on split-1 of sub-J-HMDB.</p><p>consists of 20 trees, where 10 trees are trained on the training and 10 on the validation set, with a maximum depth of 20 and a minimum of 20 patches per leaf. We train each tree with 50, 000 positive and 50, 000 negative patches extracted from 5, 000 randomly selected images and generate 40, 000 split functions at each node. For the binary potentials, we use k = 24 mixtures per part. For learning the appearance sharing among action classes (Section V-A.2) and training the action classifier (Section V-B), we use the 10 trees trained on the training set and apply them to the validation set. The action classifier and the sharing are then learned on the validation set.</p><p>For pose estimation, we create an image pyramid and perform inference at each scale independently. We then select the final pose from the scale with the highest posterior (4).</p><p>In our experiments, we use 4 scales with scale factor 0.8. The evaluation of 260 trees (20 trees for each of the 13 joints) including feature extraction takes roughly 15 seconds on average. <ref type="bibr" target="#b1">2</ref> Inference with the PS model for all 4 scales takes around 1 second. The action recognition with feature computation takes only 0.18 seconds per image and it does not increase the time for pose estimation substantially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Pose Estimation</head><p>We first evaluate the impact of the convolutional channel features (CCF) for pose estimation on split-1 of sub-J-HMDB. The results in <ref type="table" target="#tab_2">Table I</ref> show that the CCF outperform the combination of color features, HOG features, and the output of a skin color detector, which is used in <ref type="bibr" target="#b4">[5]</ref>.</p><p>In <ref type="table" target="#tab_2">Table IIa</ref> we evaluate the proposed ACPS model under different settings on split-1 of sub-J-HMDB when using CCF features for joint regressors. We start with the first step of our framework where neither the unaries nor the binaries depend on the action classes. This is equivalent to the standard PS model described in Section IV, which achieves an average joint estimation accuracy of 51.5%. Given the estimated poses, the pose-based action recognition approach described in Section V-B achieves an action recognition accuracy of 66.3% for split-1.</p><p>Having estimated the action priors p A , we first evaluate action conditioned binary potentials while keeping the unary potentials as in the standard PS model. As described in Section V-A, we can use in our model the probabilities p A or replace them by the distribution <ref type="bibr" target="#b4">(5)</ref>, which considers only the classified action class. The first setting is denoted by "Cond. (p A )" and the second by "Cond. (5)". It can be seen that the conditional binaries based on (5) already outperform the baseline by improving the accuracy from 51.5% to 53.8%. However, taking the priors from all classes slightly decreases the performance. This shows that conditioning the binary potentials on the most probable class is a better choice than using priors from all classes.</p><p>Secondly, we analyze how action conditioned unary potentials affect pose estimation. For the unaries, we have the same options "Cond. (p A )" and "Cond. (5)" as for the binaries. In addition, we can use appearance sharing as described in Section V-A.2, which is denoted by "AS". For all three binaries, the conditional unaries based on (5) decrease the performance. Since the conditional unaries based on (5) are specifically designed for each action class, they do not generalize well in case of a misclassified action class.. However, adding appearance sharing to the conditional unaries boost the performance for both conditioned on (5) and p A . Adding appearance sharing outperforms all other unaries without appearance sharing, i. e., conditional unaries, independent unaries and the unaries conditioned on p A . For all unaries, the binaries conditioned on (5) outperform the other binaries. This shows that appearance sharing and binaries conditioned on the most probable class performs best, which gives an improvement of the baseline from 51.5% to 55.3%.</p><p>In <ref type="table" target="#tab_2">Table IIb</ref>, we also evaluate the proposed ACPS when using the weaker features from <ref type="bibr" target="#b4">[5]</ref>. Although the accuracies as compared to CCF features are lower, the benefit of the proposed method remains the same. For the rest of this paper, we will use CCF for all our experiments.</p><p>In <ref type="table" target="#tab_2">Table III</ref> we compare the proposed action conditioned PS model with other state-of-the-art approaches on all three splits of sub-J-HMDB. In particular, we provide a comparison with <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b37">[38]</ref>. The accuracies for the approaches <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b37">[38]</ref> are taken from <ref type="bibr" target="#b14">[15]</ref> where the APK threshold 0.2 is used. We also evaluated the convolutional network based approach <ref type="bibr" target="#b0">[1]</ref> using the publicly available source code trained on sub-J-HMDB. Our approach outperfroms the other methods by a margin, and notably improves wrist localization by more than 5% as compared to the baseline. <ref type="table" target="#tab_2">Table IV</ref> compares the proposed ACPS with the state-ofthe-art on the Penn-Action dataset. The accuracies for the approaches <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b37">[38]</ref> are taken from <ref type="bibr" target="#b14">[15]</ref>. We can see that the proposed method improves the baseline from 75.5% to 81.1%, while improving the elbow and wrist localization accuracy by more than 7% and 10%, respectively. The proposed method also significantly outperforms other approaches Only the approach [10] achieves a higher accuracy than our method. <ref type="bibr" target="#b9">[10]</ref>, however, uses a better multi-staged CNN architecture as baseline compared to our network for computing CCF features. Since the gain of ACPS compared to our baseline even increases when better features are used as shown in <ref type="table" target="#tab_2">Table IIa &amp; Table IIb</ref>, we expect at least a similar performance gain when we use the baseline architecture from <ref type="bibr" target="#b9">[10]</ref> for ACPS.      </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Action Recognition</head><p>In <ref type="table" target="#tab_2">Table VI</ref>, we compare the action recognition accuracy obtained by our approach with state-of-the-approaches for action recognition. On sub-J-HMDB, the obtained accuracy using only pose as feature is comparable to the other approaches. Only the recent work <ref type="bibr" target="#b2">[3]</ref> which combines pose, CNN, and motion features achieves a better action recognition accuracy. However, if we combine our pose-based action recognition with Fisher vector encoding of improved dense trajectories <ref type="bibr" target="#b32">[33]</ref> using late fusion, we outperform other methods that also combine pose and appearance. The results are similar on the Penn-Action dataset.</p><p>In <ref type="table">Table V</ref>, we report the effect of different action recognition approaches on pose estimation. We report the pose estimation accuracy for split-1 of sub-J-HMDB when the action classes are not inferred by our framework, but estimated using improved dense trajectories with Fisher vector encoding (IDT-FV) <ref type="bibr" target="#b32">[33]</ref> or the fusion of our posebased method and IDT-FV. Although the action recognition rate is higher when pose and IDT-FV are combined, the pose estimation accuracy is not improved. If the action classes are not predicted but are provided (GT), the accuracy improves slightly for sub-J-HMDB and from 64.8% to 68.1% for the Penn-Action dataset. We also experimented with several iterations in our framework, but the improvements compared to the achieved accuracy of 51.6% were not more than 0.1% on all three splits of sub-J-HMDB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In this paper, we have demonstrated that action recognition can be efficiently utilized to improve human pose estimation on realistic data. To this end, we presented a pictorial Method sub-J-HMDB Penn-Action Appearance features only Dense <ref type="bibr" target="#b12">[13]</ref> 46.0% -IDT-FV <ref type="bibr" target="#b32">[33]</ref> 60.9% 92.0% Pose features only Pose <ref type="bibr" target="#b12">[13]</ref> 54.1% -Pose (Ours) 61.5% 79.0% Pose + Appearance features MST <ref type="bibr" target="#b33">[34]</ref> 45.3% 74.0% Pose + Dense <ref type="bibr" target="#b12">[13]</ref> 52.9% -AOG <ref type="bibr" target="#b14">[15]</ref> 61.2% 85.5% P-CNN <ref type="bibr" target="#b2">[3]</ref> 66.8% -Pose (Ours) + IDT-FV 74.6% 92.9% TABLE VI: Comparison of action recognition accuracy with the state-of-the-art approaches on sub-J-HMDB and Penn-Action datasets.</p><p>structure model that incorporates high-level activity information by conditioning the unaries and binaries on a prior distribution over action labels. Although the action priors can be estimated by an accurate, but expensive action recognition system, we have shown that the action priors can also be efficiently estimated during pose estimation without substantially increasing the computational time of the pose estimation. In our experiments, we thoroughly analyzed various combinations of unaries and binaries and showed that learning the right amount of appearance sharing among action classes improves the pose estimation accuracy. While we expect further improvements by using a more sophisticated CNN architecture as baseline and by including a temporal model, the proposed method has already shown its effectiveness on two challenging datasets for pose estimation and action recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 :</head><label>2</label><figDesc>Example of convolutional channel features extracted using VGG-16 net<ref type="bibr" target="#b24">[25]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Features</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>V: Analysis of pose estimation accuracy with respect to action recognition accuracy. The values in the parentheses are the corresponding action recognition accuracies. (APK threshold: 0.1)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE I :</head><label>I</label><figDesc>Comparison of the features used in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE II :</head><label>II</label><figDesc>Analysis of the proposed framework under different settings. Cond. (5) denotes if the action class probabilities p A are replaced by (5). (a) using CCF features. (b) using features from [5]. (APK threshold: 0.1)</figDesc><table><row><cell>Method</cell><cell>Head</cell><cell>Sho</cell><cell>Elb</cell><cell>Wri</cell><cell>Hip</cell><cell>Knee</cell><cell>Ank</cell><cell>Avg thr.=0.2</cell><cell>Avg thr.=0.1</cell></row><row><cell>Cond. (5) + AS U. &amp; Cond. (5) B. + CCF</cell><cell>90.3</cell><cell>76.9</cell><cell>59.3</cell><cell>55.0</cell><cell>85.9</cell><cell>76.4</cell><cell>73.0</cell><cell>73.8</cell><cell>51.6</cell></row><row><cell>Cond. (p A ) + AS U. &amp; Cond. (5) B. + CCF</cell><cell>90.1</cell><cell>76.7</cell><cell>59.2</cell><cell>54.7</cell><cell>85.6</cell><cell>76.2</cell><cell>72.9</cell><cell>73.6</cell><cell>51.2</cell></row><row><cell>Indep. U. &amp; Indep. B. + CCF</cell><cell>88.1</cell><cell>76.3</cell><cell>57.0</cell><cell>49.2</cell><cell>85.0</cell><cell>75.4</cell><cell>71.7</cell><cell>71.8</cell><cell>48.7</cell></row><row><cell></cell><cell></cell><cell cols="3">State-of-the-art approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Indep. U. &amp; Indep. B. [5]</cell><cell>65.6</cell><cell>56.4</cell><cell>39.1</cell><cell>31.1</cell><cell>65.2</cell><cell>62.8</cell><cell>60.9</cell><cell>54.4</cell><cell>34.4</cell></row><row><cell>Yang &amp; Ramanan [38]</cell><cell>73.8</cell><cell>57.5</cell><cell>30.7</cell><cell>22.1</cell><cell>69.9</cell><cell>58.2</cell><cell>48.9</cell><cell>51.6</cell><cell>-</cell></row><row><cell>Park &amp; Ramanan [16]</cell><cell>79.0</cell><cell>60.3</cell><cell>28.7</cell><cell>16.0</cell><cell>74.8</cell><cell>59.2</cell><cell>49.3</cell><cell>52.5</cell><cell>-</cell></row><row><cell>Cherian et al. [2]</cell><cell>47.4</cell><cell>18.2</cell><cell>0.08</cell><cell>0.07</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>16.4</cell><cell>-</cell></row><row><cell>Nie et al. [15]</cell><cell>80.3</cell><cell>63.5</cell><cell>32.5</cell><cell>21.6</cell><cell>76.3</cell><cell>62.7</cell><cell>53.1</cell><cell>55.7</cell><cell>-</cell></row><row><cell>Chen &amp; Yuille [1]</cell><cell>78.7</cell><cell>68.4</cell><cell>48.3</cell><cell>39.7</cell><cell>76.3</cell><cell>66.3</cell><cell>60.3</cell><cell>62.6</cell><cell>42.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE III :</head><label>III</label><figDesc>Comparison with the state-of-the-art on sub-J-HMDB using APK threshold 0.2. In the last column, the average accuracy for the threshold 0.1 is given.</figDesc><table><row><cell>Method</cell><cell>Head</cell><cell>Sho</cell><cell>Elb</cell><cell>Wri</cell><cell>Hip</cell><cell>Knee</cell><cell>Ank</cell><cell>Avg thr.=0.2</cell><cell>Avg thr.=0.1</cell></row><row><cell>Cond. (5) + AS U. &amp; Cond. (5) B. + CCF</cell><cell>89.1</cell><cell>86.4</cell><cell>73.9</cell><cell>73.0</cell><cell>85.3</cell><cell>79.9</cell><cell>80.3</cell><cell>81.1</cell><cell>64.8</cell></row><row><cell>Indep. U. &amp; Indep. B. + CCF</cell><cell>84.5</cell><cell>81.3</cell><cell>66.2</cell><cell>62.6</cell><cell>82.4</cell><cell>75.1</cell><cell>76.5</cell><cell>75.5</cell><cell>57.3</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">State-of-the-art approaches</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Yang &amp; Ramanan [38]</cell><cell>57.9</cell><cell>51.3</cell><cell>30.1</cell><cell>21.4</cell><cell>52.6</cell><cell>49.7</cell><cell>46.2</cell><cell>44.2</cell><cell>-</cell></row><row><cell>Park &amp; Ramanan [16]</cell><cell>62.8</cell><cell>52.0</cell><cell>32.3</cell><cell>23.3</cell><cell>53.3</cell><cell>50.2</cell><cell>43.0</cell><cell>45.3</cell><cell>-</cell></row><row><cell>Nie et al. [15]</cell><cell>64.2</cell><cell>55.4</cell><cell>33.8</cell><cell>24.4</cell><cell>56.4</cell><cell>54.1</cell><cell>48.0</cell><cell>48.0</cell><cell>-</cell></row><row><cell>Gkioxari et al. [10]</cell><cell>95.6</cell><cell>93.8</cell><cell>90.4</cell><cell>90.7</cell><cell>91.8</cell><cell>90.8</cell><cell>91.5</cell><cell>91.8</cell><cell>-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>TABLE IV :</head><label>IV</label><figDesc>Comparison with the state-of-the-art in terms of joint localization error on the Penn-Action dataset.</figDesc><table><row><cell></cell><cell>Indep. U. + Indep. B. + CCF</cell><cell></cell><cell cols="2">Cond. (5)+AS U. &amp; Cond. (5) B. + CCF</cell><cell></cell></row><row><cell></cell><cell></cell><cell>Pose</cell><cell>IDT-FV [33]</cell><cell>Pose+IDT-FV</cell><cell>GT</cell></row><row><cell>sub-J-HMDB (split-1)</cell><cell>51.5</cell><cell>55.3 (56.2%)</cell><cell>52.6 (66.3%)</cell><cell>55.3 (76.4%)</cell><cell>55.9 (100%)</cell></row><row><cell>Penn-Action</cell><cell>57.3</cell><cell>64.8 (79.0%)</cell><cell>-</cell><cell>-</cell><cell>68.1 (100%)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>TABLE</head><label></label><figDesc></figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The models and source code are available at http://pages.iai. uni-bonn.de/iqbal_umar/action4pose/.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Measured on a 3.4GHz Intel processor using only one core with NVidia GeForce GTX 780 GPU. The image size for all videos in sub-J-HMDB is 320 ? 240 pixels.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements:</head><p>The work was partially supported by the ERC Starting Grant ARCA (677650).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mixing Body-Part Sequences for Human Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">P-cnn: Pose-based cnn features for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ch?ron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real-time facial feature detection using conditional regression forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Body parts dependent joint regressors for human pose estimation in still images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dantone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Detecting actions, poses, and objects with relational phraselets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Object detection with discriminatively trained part-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pictorial structures for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Felzenszwalb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pose from flow and flow from pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Chained predictions using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Densenet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.1869</idno>
		<title level="m">Implementing efficient convnet descriptor pyramids</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multi-person pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards understanding action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint action recognition and pose estimation from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">N-best maximal decoders for part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Poselet conditioned pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fine-grained activity recognition with holistic and pose based features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GCPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An efficient convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tracking human pose by tracking symmetric parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Adaptive pose priors for pictorial structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sapp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Action recognition in cluttered dynamic scenes using pose-specific part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional regression forests for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Iterative action and pose recognition using global-and-pose features and action-specific models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ukita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Action Recognition by Dense Trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kl?ser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cross-view action modeling, learning, and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Convolutional channel features for pedestrian, face and edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Recognizing human actions from still images with latent poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Coupled action recognition and pose estimation from multiple views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>IJCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Recognizing human-object interactions in still images by modeling the mutual context of objects and human poses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A survey on human motion analysis from depth data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Time-of-Flight and Depth Imaging. Sensors, Algorithms, and Applications</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unconstrained monocular 3d human pose estimation by action detection and cross-modality regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Estimating human pose with flowing puppets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zuffi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
