<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PointNu-Net: Simultaneous Multi-tissue Histology Nuclei Segmentation and Classification in the Clinical Wild</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Yao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaizhu</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Hussain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curran</forename><surname>Jude</surname></persName>
						</author>
						<title level="a" type="main">PointNu-Net: Simultaneous Multi-tissue Histology Nuclei Segmentation and Classification in the Clinical Wild</title>
					</analytic>
					<monogr>
						<title level="j" type="main">IEEE TRANSACTIONS ON MEDICAL IMAGING</title>
						<imprint>
							<biblScope unit="volume">XX</biblScope>
							<biblScope unit="page">1</biblScope>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Pathology</term>
					<term>Object segmentation</term>
					<term>Artificial neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Automatic nuclei segmentation and classification plays a vital role in digital pathology. However, previous works are mostly built on data with limited diversity and small sizes, making the results questionable or misleading in actual downstream tasks. In this paper, we aim to build a reliable and robust method capable of dealing with data from the 'the clinical wild'. Specifically, we study and design a new method to simultaneously detect, segment, and classify nuclei from Haematoxylin and Eosin (H&amp;E) stained histopathology data, and evaluate our approach using the recent largest dataset: PanNuke. We address the detection and classification of each nuclei as a novel semantic keypoint estimation problem to determine the center point of each nuclei. Next, the corresponding class-agnostic masks for nuclei center points are obtained using dynamic instance segmentation. By decoupling two simultaneous challenging tasks, our method can benefit from class-aware detection and class-agnostic segmentation, thus leading to a significant performance boost. We demonstrate the superior performance of our proposed approach for nuclei segmentation and classification across 19 different tissue types, delivering new benchmark results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Fig. 1. Difference among (a)</ref> <p>bottom-up methods, (b) top-down methods, and (c) our method. Bottom-up methods first predict semantic segmentation masks and then group the pixels into object instances. Top-down methods first generate dense region proposals and conduct non-maximum suppression to obtain region-of-interests (ROIs), then classify and segment each ROI independently. Different from previous works, our method detects and classifies each nuclei via the estimation of semantic keypoint heatmap, and then class-agnostic masks are generated dynamically for each detected center point of nuclei. recurrence and outcome in some cases, e.g., in stage III colon cancer <ref type="bibr" target="#b4">[5]</ref>.</p><p>Convolutional neural network (CNN) has exhibited superior performance in many applications of computer vision. It has been introduced as a novel tool to process the digital pathology images and identify morphological patterns in computational pathology <ref type="bibr" target="#b5">[6]</ref>. In general, on histology nuclei segmentation and classification, the present CNN based approaches can be categorized into either bottom-up or top-down methods. The bottom-up structure is adopted by most existing methods <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b10">[11]</ref> which first generate high-resolution semantic segmentation masks and then group the pixels into an arbitrary number of object instances, as shown in <ref type="figure">Figure 1</ref> (a). Relying on complicated pixel grouping post-processing to extract object instances, the performance of bottom-up approaches is highly dependent on segmentation results and grouping methods. Meanwhile, top-down methods, e.g., Mask-RCNN <ref type="bibr" target="#b11">[12]</ref>, first locate class-agnostic objects in prior bounding boxes to generate region proposals and then segment and classify object instances within region-of-interests (ROIs) (see <ref type="figure">Figure 1 (b)</ref>). Though Mask-RCNN can well separate touching nucleus, it can only segment the instances within bounding boxes with a very low resolution (i.e., <ref type="bibr">28 ? 28)</ref>. Furthermore, if bounding boxes are predicted less accurately and smaller than the actual instances, top-down methods may lead to poor segmentation.</p><p>In addition to the drawbacks as mentioned above, more seriously, most of these present works are typically built and evaluated on small datasets with limited diversity <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b8">[9]</ref>, mainly because the annotation of digital pathology images requires a large amount of time and effort from domain experts. As a result, the validity of the above proposed methods may be questionable <ref type="bibr" target="#b12">[13]</ref> and may cause misleading results in downstream analysis. In fact, there are a number of even more difficult challenges in 'the clinical wild'. First, the appearance of nuclei is obviously affected by manual operations such as slightly out-of-focus, imbalanced H&amp;E staining and blur boundaries; folding tissue and hollow structure may also cause inconsistency of digital pathology images. Second, significantly morphological changes can be observed across the real images. The nuclei of a single cell type can show varied shape and size due to different diseases. Third, the density of the malignant cells in neoplastic tissues is usually much higher than the cell density in normal tissues, requiring that the analysis methods should have good generalization capabilities to process mutually occluded and overlapping nuclei under such extremely high cellularity and nuclei pleomorphism scenario. All these challenges were not properly manifested in the current small data and are consequently not addressed effectively for nuclei images in 'the clinical wild'.</p><p>Aiming to alleviate the problems in bottom-up and top-down methods as well as building a reliable and robust method capable of dealing with challenges from the 'the clinical wild', in this paper, we study and design a new method to simultaneously detect, segment, and classify the nuclei from Haematoxylin and Eosin (H&amp;E) stained histopathology data 1 . Specifically, different from both the bottom-up and top-down methods, we address the nuclei segmentation and classification problem from a new perspective and propose a novel keypoint-aware network which directly outputs instance segmentation and classification in histology images, as shown in <ref type="figure">Figure 1</ref> (c). In particular, we develop a novel framework to segment and classify touching or overlapping nuclei by considering the problem as two simultaneous prediction problems. The network exploits keypoint heatmap regression to predict the center point of each instance (which inspires the model name PointNu-Net) that can detect and classify nuclei effectively; for each detected center point of instance, a highresolution binary segmentation mask is then predicted using dynamic convolution.</p><p>To our best knowledge, this is a brand new idea which was first applied on nuclei segmentation and classification. With the novel design, our method enjoys many benefits: compared with bottom-up methods, better detection results can be obtained by keypoint estimation without complex pixel grouping postprocessing; compared with top-down methods, we utilize center points to represent nucleus instead of bounding boxes which can separate touching objects more precisely, and output high-resolution masks directly via dynamic instance <ref type="bibr" target="#b0">1</ref> Codes available at: https://github.com/Kaiseem/PointNu-Net. segmentation. Therefore, overlapping and clustered nucleus can be separated as keypoints, leading to much better nuclei detection performance. In addition, our method can directly output instance segmentation and classification without any post-processing, enabling a very fast inference.</p><p>To validate our method, we first present comparative results on various datasets including two famous multi-tissue histology image datasets. Our novel methods demonstrates state-of-the-art performance compared to other recently proposed methods on nuclei segmentation and classification tasks. Importantly, we also conduct evaluations on the PanNuke data, the recently released largest pan-cancer histology dataset containing over 20,000 WSIs for nuclei segmentation and classification <ref type="bibr" target="#b13">[14]</ref>, which is statistically similar to 'the clinical wild' and seldom investigated by previous methods. In such 'clinical wild', our proposed method leads to much robust and promising performance, which is substantially and consistently higher than all the comparison models for all the 19 different tissue types.</p><p>The rest of this paper is organized as follows. We first conduct an overview of related works. Then we detail the proposed PointNu-Net, including network architecture, training scheme, and inference strategy. Finally, we evaluate the proposed method on three public available datasets qualitatively and quantitatively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>In this section we give an overview of the related work. We first review bottom-up and top-down nuclei segmentation and classification methods. Then we review some recentlydeveloped keypoint-based object detection approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Bottom-up Nuclei Segmentation And Classification</head><p>Most histology nuclei segmentation and classification approaches are designed bottom-up. Kumar et al. <ref type="bibr" target="#b6">[7]</ref> predicted three-class segmentation masks containing inside, outside, and edges of nuclei. Then pixels were grouped into instances using inside pixel seeded region growth. DIST <ref type="bibr" target="#b7">[8]</ref> formulated the inside nuclei segmentation problem as a regression of distance maps. CIA-Net <ref type="bibr" target="#b9">[10]</ref> utilized additional contour supervision to obtain segmentation with more accurate edges of nucleus. Triple U-Net <ref type="bibr" target="#b10">[11]</ref> designed the parallel feature aggregation network to fuse features from Hematoxylin and RGB images progressively. Thus it learned a more precise nuclei boundaries. Hover-Net <ref type="bibr" target="#b8">[9]</ref> was the first to achieve simultaneous nuclei segmentation and classification. It outputs the instance center by regressing vertical and horizontal distances of nuclear pixels to their centres of mass, and then predicted an additional nuclei type map to reach nuclei classification.</p><p>Although these approaches achieved encouraging results, the detection performance of the bottom-up methods highly depends on the segmentation results and pixel grouping methods, thus limiting their generalization in complex scenarios, e.g., in 'the clinical wild'. In contrast, we propose to regress a keypoint heatmap independently from nuclei segmentation. Thus there is no need of pixel grouping to separate nucleus, which enables better generalization in the complex scenarios. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Top-down Nuclei Segmentation And Classification</head><p>Top-down methods, e.g., Mask-RCNN <ref type="bibr" target="#b11">[12]</ref>, have made rapid progress in natural image segmentation. Despite their potentials in handling touching and clustered nucleus, they were seldomly used in nuclei segmentation and classification. One major limitation for top-down methods is the difficulty in segmenting precise nuclei boundaries. This may be caused by several reasons. 1). Since the prior anchors were usually designed densely, one candidate region proposal may correspond to many overlapped bounding boxes. Thus, non-maximum suppression (NMS) is often performed to filter the bounding boxes with low scores. This may result in poor segmentation when the bounding boxes cannot cover the whole nuclei. 2). Region proposal methods often have a fixed resolution of output segmentation masks (e.g., 28?28). The predicted masks are then resampled to the corresponding bounding boxes' sizes, which may also introduce quantization errors.</p><p>On the contrary, since our method utilizes the peak of keypoint heatmap to represent object instances, one candidate proposal tends to be one point only. Thus, NMS can be optional but not necessary. In addition, our method predicts highresolution masks for all instances via dynamic convolution, where the predicted masks have the same size as the input images. This can boost the performance not only in detection but also in segmentation compared with top-down methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Keypoint-based Object Detection</head><p>Unlike anchor-based detectors which regress objects by fitting the anchor boxes, keypoint-based object detectors directly regress object locations by utilizing features at certain pre-defined object keypoint. There are mainly two types of keypoint-based object detectors, i.e., group-based and group free, in the literature. Group-based detectors predict multiple keypoints for each object and group them to get bounding boxes. CornerNet <ref type="bibr" target="#b14">[15]</ref> detected top-left and bottom-right corners of the object and then matched corners of the same object by computing the distance of points in the feature space.</p><p>Duan et al. <ref type="bibr" target="#b15">[16]</ref> added a center detection branch to improve the performance by center point validation. RepPoints <ref type="bibr" target="#b16">[17]</ref> took advantage of deformable convolutional networks to get sets of points to represent objects. CentripetalNet <ref type="bibr" target="#b17">[18]</ref> improved CornerNet by predicting the centripetal shift in order to pair corner keypoints from the same object. On the other hand, group-free detectors directly predicted the center keypoints of objects so as to avoid the complex grouping process. CenterNet <ref type="bibr" target="#b18">[19]</ref> located objects by the center points and regressed the corresponding size. SaccadeNet <ref type="bibr" target="#b19">[20]</ref> improved center point detection by extracting more informative corner features during training.</p><p>Since cell nucleus tend to cluster with high density, we utilize the group-free keypoint-based object detector to detect the center point of nucleus efficiently and effectively. Different from the above mentioned methods, our proposed method decouples instance segmentation and classification into classaware keypoint regression and class-agnostic mask prediction. This allows us to directly predict semantic instance masks in one shot without the need of pixel grouping post-processing, thus enabling both more precise and faster detection. To the best of our knowledge, our proposed method is the first approach that utilizes keypoint-based detection into nuclei instance segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED METHOD</head><p>The main idea of PointNu-Net is to rephrase the nuclei instance segmentation as two simultaneous prediction problems. Concretely, our system represents one nuclei by a single point at their instance segmentation mask center. The corresponding instance segmentation is then regressed directly using dynamic convolution at the center location. As shown in <ref type="figure" target="#fig_0">Figure 2</ref>, our method first outputs three predictions, i.e., keypoint heatmap, kernel prediction, and feature prediction. The local peaks are then filtered from the keypoint heatmap to locate center points of nucleus. Next, the kernel vectors are selected from the kernel prediction according to the center point position. At last, the instance masks of all the identified center points  are produced via dynamically convoluting the corresponding kernel vectors onto the feature prediction.</p><p>Our network architecture consists of image feature extractor, feature fusion module, and three task-specific prediction branches, as shown in <ref type="figure">Figure 3</ref>. In the following, we will first describe the prediction branches including the keypoint heatmap regression for nuclei detection and classification as well as the dynamic convolution for nuclei segmentation. Afterwards, details of the employed CNN for feature extraction and fusion will be discussed. Finally, we introduce the inference process of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Detection And Classification By Keypoints</head><p>Given an input image of width W and height H, our aim is to regress a low-resolution keypoint heatmap?</p><formula xml:id="formula_0">? [0, 1] W R ? H R ?C ,</formula><p>where R is the output downsampling factor and C is the number of keypoint types. The default output downsampling factor of R = 4 is applied following the literature <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. Keypoint types include C = 1 for nuclei detection, or C &gt; 1 for nuclei detection and classification.</p><p>A prediction? x,y,c = 1 corresponds to a detected keypoint, whilst? x,y,c = 0 is background.</p><p>We train the keypoint heatmap prediction branch by following <ref type="bibr" target="#b18">[19]</ref>, as shown in <ref type="figure">Figure 3</ref>. We render all ground truth keypoints in a heatmap? ? [0, 1]</p><formula xml:id="formula_1">W R ? H R ?C using an un- normalized Gaussian kernel, exp (? (x?px) 2 +(y?py) 2 2? 2 p )</formula><p>, where ? p is an object size-adaptive standard deviation <ref type="bibr" target="#b14">[15]</ref>. If two Gaussian of the same class overlap, we take the element-wise maximum <ref type="bibr" target="#b20">[21]</ref>. The training objective is a penalty-reduced pixel-wise logistic regression with the focal loss <ref type="bibr" target="#b22">[23]</ref>:</p><formula xml:id="formula_2">L keypoint = ?1 N k pos xyc ? ? ? (1 ?? xyc ) ? log (? xyc ) if Y xyc = 1 (1 ? Y xyc ) ? (? xyc ) ? log (1 ?? xyc ) otherwise</formula><p>where ? and ? are the hyper-parameters of the focal loss, N k pos is the number of keypoints in image I, Y is the ground truth heatmap, and? is the prediction. Normalization by N k pos is chosen to normalize all positive focal loss instances to 1. We use ? = 2 and ? = 4 in our experiments, following <ref type="bibr" target="#b14">[15]</ref> and <ref type="bibr" target="#b18">[19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Instance Segmentation By Dynamic Convolution</head><p>We regress the full-size binary instance maskM x,y ? R W ?H for each point at (x, y) on the keypoint heatmap and a 3D tensor with W ?H R 2 channels is generated. Ideally, a one-to-one correspondence can be established between the semantic keypoint heatmap and the class-agnostic mask. However, it may result in unnecessary computing redundancy and out of memory problem to directly predict such a tensor.</p><p>To overcome this problem, we take advantages of dynamic convolution <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref> and divide the mask branch into a feature branch and a kernel branch. For each point in the heatmap, we engage dynamic convolution to produce the final instance mask prediction, which can be written as:M x,y = K x,y * F , where F ? R W ?H?E is the predicted feature, K x,y ? R 1?1?E is the convolution kernel generated by kernel branch, E is the length of kernel size, and * is the convolution operation.</p><p>So far, the whole instance mask M ? R W ?H? W ?H R 2 is no longer required to compute. Moreover, only the kernels corresponding to the positives are selected to compute the final prediction and calculate the training loss. Inspired by Wang et al. <ref type="bibr" target="#b25">[26]</ref>, we add locations in the neighborhood of the center as positives where the corresponding heatmap is larger than a threshold ? (e.g., heatmap &gt; ? ). The training objective of feature branch and kernel branch is a Soft Dice loss <ref type="bibr" target="#b26">[27]</ref> </p><formula xml:id="formula_3">L mask = 1 N m pos xyk 1 (max c?C (Y k xyc )&gt;? ) (1 ? DICE(M x,y , M k )),</formula><p>where N m pos is the number of instance masks, DICE is Dice coefficient,M x,y is the predicted soft mask, k is the k th instance in image I, and M k is the ground truth mask corresponding to k th instance. 1 is the indicator function, being</p><formula xml:id="formula_4">1 if max c?C (Y xyc ) &gt; ? and 0 otherwise.</formula><p>The overall objective is:</p><formula xml:id="formula_5">L total = L keypoint + ? mask L mask .<label>(1)</label></formula><p>We set ? mask = 1 and ? = 0.5 empirically in all our experiments unless specified otherwise. We apply a single network to predict semantic keypoint? , dynamic convolution kernel K and feature F simultaneously with three separated branches (shown in <ref type="figure">Figure 3</ref>), and generate class-agnostic instance masksM dynamically.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Feature Extraction And Fusion</head><p>Standard feature extraction, like ResNet <ref type="bibr" target="#b27">[28]</ref> and VG-GNet <ref type="bibr" target="#b28">[29]</ref>, cannot extract a strong and representative set of features for small objects, since they encode the input image as a high-level low-resolution representation in a series manner. However, the size of nucleus varies among the tissues, and many nucleus are very small (less than 8?8 pixels). This may result in unsatisfactory detection and segmentation for small nucleus. To overcome this problem, we take High Resolution Net (HRNet) <ref type="bibr" target="#b29">[30]</ref> as our backbone which can maintain highresolution representations as well as extracting low-resolution high-level features through the whole process. As shown in <ref type="figure">Figure 3</ref>, we omit HRNet structure for clarity, and Conv2-5 denotes multi-scale features. Different scales of features are upsampled to the same size as 'Conv2' using bilinear interpolation and concatenated together.</p><p>In order to better integrate the multi-scale features from the backbone, we design a simple module called Joint Pyramid Fusion Module (JPFM) to make sure each branch can make full use of the maintained high-resolution representations from the backbone, as shown in <ref type="figure">Figure 4</ref>. Since the multi-scale features have been fused repeatedly in HRNet, there is limited gain using the fusion module with a large receptive field. In contrast, we engage a dense design of dilated convolution (d = 1, 2, 4, 8) to extract task-specific representation for each branch. We do not use a shared fusion block for all the branches since the importance of different scale for different branch is not the same. Instead, we use the unshared JPFM for each branch, and we will discuss it in the later section with detailed experiments.</p><p>Finally, Conv Blocks are stacked in each branch to generate final prediction, while Upsample Blocks are used in feature branch to output high-resolution features. Followed by <ref type="bibr" target="#b24">[25]</ref>, a normalized coordinates is added at the first Conv Blocks of the kernel branch to embed position information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. From Points To Instance Segmentation</head><p>At inference time, as shown in <ref type="figure" target="#fig_0">Figure 2</ref>, we first extract the peaks in the heatmap for each category independently. We detect all responses whose value is greater or equal to its 8connected neighbors and a confidence threshold of 0.4 is used to filter out predictions with low confidence. Then the top 100 scoring local peaks and their corresponding kernels are selected, and dynamic convolution is performed between the predicted feature and selected kernels. Finally, a threshold of 0.5 is used to convert predicted soft masks to binary masks. Matrix NMS operation <ref type="bibr" target="#b24">[25]</ref> is alternatively used to get rid of overlapped predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Datasets</head><p>PanNuke is so far the largest publicly available nuclei segmentation and classification dataset <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Images with 19 different tissues were obtained from The Cancer Genome Atlas (TCGA), where 256 ? 256 pre-extract patches were collected from more than 20,000 Whole Slide Images. PanNuke is split into 3 randomized training, validation, and testing folds, and we follow <ref type="bibr" target="#b13">[14]</ref> to reproduce the results. This dataset is semi-automatically annotated and quality controlled by clinical pathologists, which is statistically similar to 'the clinical wild' and with minimal selection bias. Kumar is a common nuclei segmentation dataset <ref type="bibr" target="#b6">[7]</ref> consisting of 30 images from seven different tissues. They are divided into a training set of 16 images (4 breast, 4 liver, 4 kidney and 4 prostate) and a testing set of 14 images (2 breast, 2 liver, 2 kidney and 2 prostate, 2 bladder, 2 colon, 2 stomach) with the same protocol used in <ref type="bibr" target="#b6">[7]</ref>- <ref type="bibr" target="#b8">[9]</ref>. CoNSeP <ref type="bibr" target="#b8">[9]</ref> containing 41 images with different cell types. Four classes are considered: epithelial, inflammatory, spindleshaped, and miscellaneous. Training and test set partition follows previous works, where the training set contains 27 images, and the test set contains 14 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Implementation And Training Details</head><p>We implemented our framework with the open source software library PyTorch 1.8.0 on a workstation equipped with one NVIDIA GeForce RTX 3090 GPU. For Kumar dataset, we used stain normalization <ref type="bibr" target="#b31">[32]</ref> to reduce the color differences  between the stained images, and no stain normalization was performed for CoNSeP and PanNuke since the color difference between training and testing data is not large. The training objective function consists of the keypoint regression loss L keypoint and instance segmentation loss L mask , and the weights for each loss are both set to 1. AdamW <ref type="bibr" target="#b32">[33]</ref> has been used as an optimizer to minimize objective function with the mini-batch of 8 and weight decay of 0.0001. All models are trained for 100 epochs with an initial learning rate of 0.0001, which is then divided by 10 at the 80 th and again at the 90 th epoch. Various data augmentation techniques were employed including random cropping, flipping, color jittering, blurring and elastic transformation by following HoVer-Net <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results And Comparative Analysis</head><p>To quantify the instance segmentation performance of each method, we used panoptic quality (PQ), followed by Graham et al. <ref type="bibr" target="#b8">[9]</ref>. Panoptic quality was further tear apart into Detection Quality (DQ) and Segmentation Quality (SQ) components for interpretability. We used multi-class PQ (mPQ) to evaluate the performance of segmentation and classification, while binary PQ (bPQ) was used to evaluate the performance of segmentation. Specifically, mPQ was calculated independently for each positive class, and bPQ assumes that all nucleus belong to one class 2 . For Kumar and CoNSeP, we also report DICE and Aggregated Jaccard Index (AJI) <ref type="bibr" target="#b6">[7]</ref> in direct comparison with previous works on small datasets.</p><p>On large dataset PanNuke, the proposed method was compared against several deep learning based methods, e.g., DIST <ref type="bibr" target="#b7">[8]</ref>, Mask-RCNN <ref type="bibr" target="#b11">[12]</ref>, Micro-Net <ref type="bibr" target="#b33">[34]</ref> and HoVer-Net <ref type="bibr" target="#b8">[9]</ref>. As shown in <ref type="table">Table I</ref>, the proposed method achieves state-of-the-art performance not only on classification but also on segmentation. In particular, PointNu-Net outperformed the best method HoVer-Net, by 0.0328 and 0.0213 on the average mPQ and bPQ across tissues respectively. To take insight into how PointNu-Net performed for different types of nucleus, we reported mPQ and bPQ for all 19 tissue types separately. We observed that for most tissue types, PointNu-Net achieved the best performance against all the existing methods. We also reported the average PQ for each type of nucleus in <ref type="table">Table II</ref>. In some cases, distinguishing between neoplastic and nonneoplastic nuclei proved to be challenging, yet an improvement of 0.027 and 0.086 on the average PQ on the both nuclei can be observed in our method. In addition, when facing the complicated scenarios, e.g., out-of-focus, blur boundaries, and imbalanced staining, previous methods, e.g., HoVer-Net,  might perform poorly since there is no confidence mechanism to filter the poor instance results. In contrast, our method outputted instances with high confidence, resulting in more robust performance in 'the clinical wild'.</p><p>On small datasets Kumar and CoNSeP, we evaluated the performance of instance segmentation with the above mentioned methods. We also made comparisons with some additional methods. <ref type="bibr" target="#b2">3</ref> As observed from the results in <ref type="table">Table III</ref>, our method generated the highest detection quality (DQ) on Kumar and CoNSeP, as PointNu-Net had better detection performance. On the other hand, our method resulted in marginally lower segmentation quality (SQ) and did not exhibit significant advantages on segmentation. This can be explained by the fact that our method only outputs the instances with high confidence, whilst the instance with low confidence may be ignored. This can be also reflected on DICE which measures the semantic segmentation results only. In addition, since AJI may over-penalize the overlapping region according to Graham et al. <ref type="bibr" target="#b8">[9]</ref>, panoptic quality (PQ) is more fair to evaluate each method. As a consequence, PointNu-Net had a relatively low AJI (since our method allows overlapped outputs), while it has comparable results on Kumar dataset, w.r.t. bPQ. The superior performance of the proposed method can be observed in the last column of <ref type="table">Table III</ref>, i.e., multi-classes panoptic quality (mPQ). The keypoint-aware framework demonstrated superior performance of detection and classification of nuclei, making an improvement of 0.02 mPQ on CoNSeP over the best result given by HoVer-Net. It is noted that no mPQ could be available on Kumar since no classification annotation is conducted in the dataset. Visualization of nuclei segmentation results can be found in <ref type="figure">Figure 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Speed-accuracy Trade-off</head><p>Although it is rarely mentioned in the previous works, the speed of processing images also matters due to the extra large size image in WSIs. In order to investigate the tradeoff between the efficiency and the accuracy quantitatively, we performed segmentation and classification on datasets CoNSeP over 10 runs and calculated the average inference time on a 1, 000 ? 1, 000 image, as shown in <ref type="table" target="#tab_5">Table IV</ref>. As a region proposal based method that will generate redundant and overlapped region-of-interests, the top-down method Mask-RCNN required larger space to store intermediate results and longer time to post-process, resulting in almost 52 seconds processing time per image. The bottom-up method HoVer-Net took about 5.3 seconds per image thanks to its parallel separation of instances. The post-processing of HoVer-Net contained complex operations using CPU, which is a bottleneck for faster inference time. PointNu-Net detected each instance using heatmap peaks and segmented instances dynamically without any post-processing, which takes advantages of parallel calculation on GPU. As a consequence, our default version of PointNu-Net spent 3.29 seconds inference time to achieve the highest mPQ on CoNSeP, approximately 1.6 ? faster than HoVer-Net. On the other hand, the inference time declined about 10% without non-maximum suppression, just leading to a slight decrease of performance. Meanwhile, because almost 95% inference time were costed on GPU, using a smaller model can significantly speed up the model. Here we propose PointNu-Net-M and PointNu-Net-S, which used HRNet-w32 and HRNet-w18 as the backbone, respectively. Both lighter versions of PointNu-Net share the same training protocol and architectures as the default version, except using 4 instead of 7 stacked convolutional layers on the keypoint heatmap branch and kernel branch. Eventually, we can halve the inference time compared with the default PointNu-Net, while achieving acceptable performance.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>We conducted ablation studies to gain full understanding of the main components in PointNu-Net. In order to get convincing results, we evaluate all the experiments on the large dataset PanNuke.</p><p>1) Keypoint Heatmap Branch: We investigated the importance of the keypoint heatmap strategy and focal loss. First, we replaced the prediction of the keypoint heatmap with that of center points to detect and classify each instance, and used binary cross entropy (BCE) loss as the loss function. We refereed to this combination as the vanilla strategy because cross entropy loss is the most commonly used loss function for classification. Then, on the basis of vanilla, we introduced Gaussian kernel strategy on each instance center to produce the keypoint heatmap, where the Gaussian kernel size was directly proportional to the object size. Finally, we introduced focal loss as the loss function, which is our default setting of PointNu-Net.</p><p>As shown in <ref type="table">Table V</ref>, both the keypoint heatmap and the focal loss improved the performance of the vanilla setting. Specifically, the keypoint heatmap strategy boosted mPQ by 0.044 and bPQ by 0.062. The intuition behind the keypoint heatmap is that it made the ground truth smooth nearby each object center point and therefore reduced the difficulty of center point prediction. The focal loss further contributed an improvement of 0.049 and 0.050 in terms of mPQ and bPQ. This is benefited from that focal loss enforces the model to mine the hard examples, as it can reduce the weight of simple negative samples in training. These results indicated the effectiveness of the keypoint-aware prediction.</p><p>2) Backbone And Neck Selection: ResNet backbone and Feature Pyramid Networks (FPN) <ref type="bibr" target="#b35">[36]</ref> are commonly considered as the default multi scale feature extractor in many computer vision tasks due to their simple but efficient architecture. However, as discussed before, a network in series may not be a good selection for local feature extraction. In order to validate this, we performed several experiments using different backbones and neck combination. As shown in <ref type="table">Table VI</ref>  combination of ResNet101 and FPN achieved 0.486 and 0.673 in terms of mPQ and bPQ, which was already higher than the previous work HoVer-Net. However, when we used ResNeXt-101-DCN which is considered more powerful in natural image processing, the performance became even worse. This might be the reason that better ability in catching global features may not benefit our task. Instead, HRNet-w64 can better utilize local features for classifying and detecting high density instances, as it keeps the high-resolution features along feature extraction. However, since HRNet-w64 has merged multi-scale features in parallel, the effect of FPN was very limited. Hence, we introduced Joint Pyramid Fusion Module (JPFM) to make full use of the features from HRNet-w64. For feature fusion, we have two options: to construct a shared JPFM to merge the features for all branches or to learn the feature in each branch separately, which are termed as shared and unshared JPFM respectively. Experimental results demonstrated that a shared JPFM outperformed the default neck FPN, while the unshared JPFM further boosted the network's ability since different scale information may not have the same importance for the three separated branches.</p><p>To better understanding the unshared JPFM, we visualized the feature activation map in the three branches, as shown in <ref type="figure" target="#fig_4">Figure 8</ref>. As observed, keypoint heatmap and feature branches had more active feature map in d = 1, which means that the low-level high-resolution features are important to detection and classification. In contrast, the feature map in kernel branch activated strongly in d = 8, reflecting the crucial role of highlevel low-resolution features in kernel generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>In this paper, we have presented a keypoint-aware PointNu-Net for nuclei segmentation and classification within multitissue histology images, which detects and classifies nucleus by keypoint heatmap regression and segments nuclei simultaneously via dynamic convolution. We have proposed a more reasonable feature aggregation module for HRNet, called Joint Pyramid Fusion Module, which allows the network to take full use of the merged multi-scale features. In addition, we have conducted the speed-accuracy trade-off experiments and proposed light-weight versions of PointNu-Net for faster inference, which requires no image post-processing and NMS operation. Extensive experiments on one large dataset PanNuke and two small datasets Kumar and CoNSeP have demonstrated the superiority of our proposed network. Quantitative experiments have shown that our proposed method has achieved state-ofthe-art nuclei segmentation and classification performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Overview of PointNu-net. PointNu-net first produces semantic keypoint heatmap to locate centers of nuclei and then generates highresolution class-agnostic masks corresponding to the center position via dynamic convolution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Network architecture. (a) Conv block, (b) Upsample Block. The extracted multi-scale features are up-sampled to the same scale and concatenated before delivering to each branch. Keypoint heatmap branch predicts the center point of each nuclei as the detection and classification results. Then the outputs from kernel branch and feature branch are taken together and dynamic convolution is operated to get the instance segmentation results. Joint Pyramid Fusion Module. Concatenated multi-scale features are first processed by four parallel convolutional layers with dense designed dilations (d = 1, 2, 4, 8), then the output features are fused by concatenation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Examples of PointNu-Net segmentation and classification results across 19 tissues on PanNuke.In each pair, left is the ground truth overlaid, while the right is the PointNu-Net prediction overlaid. A different color represents a different class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>A selection of visual fields from PanNuke which are complicated scenarios in 'the clinical wild'.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Visualization of feature activation maps of all convolution layers in the JPFM in keypoint heatmap branch, kernel branch, and feature branch. d denotes the dilation of convolution in the JPFM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>HoVer-Net PointNu-Net Ground Truth</head><label></label><figDesc>COMPARATIVE EXPERIMENTS OF NUCLEI SEGMENTATION AND CLASSIFICATION ON THE PANNUKE. WE ALSO PROVIDE THE STANDARD DEVIATION (STD) ACROSS THESE SPLITS IN THE FINAL ROW.</figDesc><table><row><cell></cell><cell></cell><cell>DIST</cell><cell></cell><cell cols="2">Mask-RCNN</cell><cell cols="2">Micro-Net</cell><cell cols="2">HoVer-Net</cell><cell>PointNu-Net</cell></row><row><cell></cell><cell></cell><cell>mPQ</cell><cell>bPQ</cell><cell>mPQ</cell><cell>bPQ</cell><cell>mPQ</cell><cell>bPQ</cell><cell>mPQ</cell><cell>bPQ</cell><cell>mPQ</cell><cell>bPQ</cell></row><row><cell cols="2">Adrenal Gland</cell><cell cols="2">0.3442 0.5603</cell><cell>0.347</cell><cell>0.5546</cell><cell>0.4153</cell><cell>0.644</cell><cell cols="2">0.4812 0.6962</cell><cell>0.5115 0.7134</cell></row><row><cell>Bile Duct</cell><cell></cell><cell cols="2">0.3614 0.5384</cell><cell cols="2">0.3536 0.5567</cell><cell cols="2">0.4124 0.6232</cell><cell cols="2">0.4714 0.6696</cell><cell>0.4868 0.6814</cell></row><row><cell>Bladder</cell><cell></cell><cell cols="2">0.4463 0.5625</cell><cell cols="2">0.5065 0.6049</cell><cell cols="2">0.5357 0.6488</cell><cell cols="2">0.5792 0.7031</cell><cell>0.6065 0.7226</cell></row><row><cell>Breast</cell><cell></cell><cell cols="2">0.3790 0.5466</cell><cell cols="2">0.3882 0.5574</cell><cell cols="2">0.4407 0.6029</cell><cell cols="2">0.4902 0.6470</cell><cell>0.5147 0.6709</cell></row><row><cell>Cervix</cell><cell></cell><cell cols="2">0.3371 0.5309</cell><cell cols="2">0.3402 0.5483</cell><cell cols="2">0.3795 0.6101</cell><cell cols="2">0.4438 0.6652</cell><cell>0.5014 0.6899</cell></row><row><cell>Colon</cell><cell></cell><cell cols="2">0.2989 0.4508</cell><cell cols="2">0.3122 0.4603</cell><cell cols="2">0.3414 0.4972</cell><cell cols="2">0.4095 0.5575</cell><cell>0.4509 0.5945</cell></row><row><cell>Esophagus</cell><cell></cell><cell cols="2">0.3942 0.5295</cell><cell cols="2">0.4311 0.5691</cell><cell cols="2">0.4668 0.6011</cell><cell cols="2">0.5085 0.6427</cell><cell>0.5504 0.6766</cell></row><row><cell cols="2">Head &amp; Neck</cell><cell cols="2">0.3177 0.4764</cell><cell cols="2">0.3946 0.5457</cell><cell cols="2">0.3668 0.5242</cell><cell cols="2">0.4530 0.6331</cell><cell>0.4838 0.6546</cell></row><row><cell>Kidney</cell><cell></cell><cell cols="2">0.3339 0.5727</cell><cell cols="2">0.3553 0.5092</cell><cell cols="2">0.4165 0.6321</cell><cell cols="2">0.4424 0.6836</cell><cell>0.5066 0.6912</cell></row><row><cell>Liver</cell><cell></cell><cell cols="2">0.3441 0.5818</cell><cell cols="2">0.4103 0.6085</cell><cell cols="2">0.4365 0.6666</cell><cell cols="2">0.4974 0.7248</cell><cell>0.5174 0.7314</cell></row><row><cell>Lung</cell><cell></cell><cell cols="2">0.2809 0.4978</cell><cell cols="2">0.3182 0.5134</cell><cell>0.337</cell><cell>0.5588</cell><cell cols="2">0.4004 0.6302</cell><cell>0.4048 0.6352</cell></row><row><cell>Ovarian</cell><cell></cell><cell cols="2">0.3789 0.5289</cell><cell cols="2">0.4337 0.5784</cell><cell cols="2">0.4387 0.6013</cell><cell cols="2">0.4863 0.6309</cell><cell>0.5484 0.6863</cell></row><row><cell>Pancreatic</cell><cell></cell><cell cols="2">0.3395 0.5343</cell><cell cols="2">0.3624 0.5460</cell><cell cols="2">0.4041 0.6074</cell><cell cols="2">0.4600 0.6491</cell><cell>0.4804 0.6791</cell></row><row><cell>Prostate</cell><cell></cell><cell cols="2">0.3810 0.5442</cell><cell cols="2">0.3959 0.5789</cell><cell cols="2">0.4341 0.6049</cell><cell cols="2">0.5101 0.6615</cell><cell>0.5127 0.6854</cell></row><row><cell>Skin</cell><cell></cell><cell cols="2">0.2627 0.5080</cell><cell cols="2">0.2665 0.5021</cell><cell cols="2">0.3223 0.5817</cell><cell cols="2">0.3429 0.6234</cell><cell>0.4011 0.6494</cell></row><row><cell>Stomach</cell><cell></cell><cell cols="2">0.3369 0.5553</cell><cell cols="2">0.3684 0.5976</cell><cell cols="2">0.3872 0.6293</cell><cell cols="2">0.4726 0.6886</cell><cell>0.4517 0.7010</cell></row><row><cell>Testis</cell><cell></cell><cell cols="2">0.3278 0.5548</cell><cell cols="2">0.3512 0.5420</cell><cell cols="2">0.4088 0.6300</cell><cell cols="2">0.4754 0.6890</cell><cell>0.5334 0.7058</cell></row><row><cell>Thyroid</cell><cell></cell><cell cols="2">0.2574 0.5596</cell><cell cols="2">0.3037 0.5712</cell><cell cols="2">0.3712 0.6555</cell><cell cols="2">0.4315 0.6983</cell><cell>0.4508 0.7076</cell></row><row><cell>Uterus</cell><cell></cell><cell cols="2">0.3487 0.5246</cell><cell cols="2">0.3683 0.5589</cell><cell cols="2">0.3965 0.5821</cell><cell cols="2">0.4393 0.6393</cell><cell>0.4846 0.6634</cell></row><row><cell cols="4">Average across tissues 0.3406 0.5346</cell><cell cols="2">0.3688 0.5528</cell><cell cols="2">0.4059 0.6053</cell><cell cols="2">0.4629 0.6596</cell><cell>0.4957 0.6808</cell></row><row><cell cols="2">STD across splits</cell><cell cols="2">0.0156 0.0097</cell><cell cols="2">0.0046 0.0076</cell><cell cols="2">0.0082 0.0050</cell><cell cols="2">0.0076 0.0036</cell><cell>0.0084 0.0030</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>TABLE I</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>Neo</cell><cell cols="2">Non-Neo Epi Inflam</cell><cell>Conn</cell><cell>Dead</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>DIST</cell><cell>0.439</cell><cell>0.290</cell><cell>0.343</cell><cell cols="2">0.275 0.000</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Mask-RCNN 0.472</cell><cell>0.403</cell><cell>0.290</cell><cell cols="2">0.300 0.069</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Micro-Net</cell><cell>0.504</cell><cell>0.442</cell><cell>0.333</cell><cell cols="2">0.334 0.051</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>HoVer-Net</cell><cell>0.551</cell><cell>0.491</cell><cell>0.417</cell><cell cols="2">0.388 0.139</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>PointNu-Net</cell><cell>0.578</cell><cell>0.577</cell><cell>0.433</cell><cell cols="2">0.409 0.154</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>TABLE II</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">AVERAGE PQ ACROSS THREE DATASET SPLITS ON PANNUKE FOR</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">EACH NUCLEAR CATEGORY</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>For each dataset, we displayed the 6 models from left to right. The different colours of the nuclear boundaries denote separate instances.TABLE III COMPARATIVE EXPERIMENTS ON SMALL DATASETS KUMAR AND CONSEP. BPQ REFLECTS THE PERFORMANCE ON NUCLEI SEGMENTATION, WHILE MPQ HIGHLIGHTS THE PERFORMANCE ON NUCLEI SEGMENTATION AND CLASSIFICATION.</figDesc><table><row><cell>Ground Truth</cell><cell cols="2">PointNu-Net</cell><cell></cell><cell>HoVer-Net</cell><cell></cell><cell>DIST</cell><cell cols="2">Mask-RCNN</cell><cell></cell><cell cols="2">Micro-Net</cell><cell>U-Net</cell></row><row><cell>Kumar</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Ground Truth</cell><cell cols="2">PointNu-Net</cell><cell></cell><cell>HoVer-Net</cell><cell></cell><cell>DIST</cell><cell cols="2">Mask-RCNN</cell><cell></cell><cell cols="2">Micro-Net</cell><cell>U-Net</cell></row><row><cell>CoNSeP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>Kumar</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>CoNSeP</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2">DICE AJI</cell><cell>DQ</cell><cell>SQ</cell><cell>bPQ</cell><cell cols="2">DICE AJI</cell><cell>DQ</cell><cell>SQ</cell><cell>bPQ</cell><cell>mPQ</cell></row><row><cell>U-Net [35]</cell><cell></cell><cell>0.758</cell><cell cols="4">0.556 0.691 0.690 0.478</cell><cell>0.724</cell><cell cols="4">0.482 0.488 0.671 0.328 -</cell></row><row><cell>DIST [8]</cell><cell></cell><cell>0.789</cell><cell cols="4">0.559 0.601 0.732 0.443</cell><cell>0.804</cell><cell cols="4">0.502 0.544 0.728 0.398 0.372</cell></row><row><cell cols="3">Mask-RCNN [12] 0.760</cell><cell cols="4">0.546 0.704 0.720 0.509</cell><cell>0.740</cell><cell cols="4">0.474 0.619 0.740 0.460 0.450</cell></row><row><cell cols="2">Micro-Net [34]</cell><cell>0.797</cell><cell cols="4">0.560 0.692 0.747 0.519</cell><cell>0.794</cell><cell cols="4">0.527 0.600 0.745 0.449 0.430</cell></row><row><cell cols="2">CIA-Net [10]</cell><cell>0.818</cell><cell cols="4">0.620 0.754 0.762 0.577</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="2">HoVer-Net [9]</cell><cell>0.826</cell><cell cols="3">0.618 0.770 0.773</cell><cell>0.597</cell><cell>0.853</cell><cell cols="4">0.571 0.702 0.778 0.547 0.516</cell></row><row><cell cols="2">Triple U-Net [11]</cell><cell>0.837</cell><cell cols="2">0.621 -</cell><cell>-</cell><cell>0.601</cell><cell>0.843</cell><cell>0.579</cell><cell>-</cell><cell>-</cell><cell>0.562</cell><cell>-</cell></row><row><cell>PointNu-Net</cell><cell></cell><cell>0.814</cell><cell cols="2">0.606 0.784</cell><cell>0.768</cell><cell>0.603</cell><cell>0.822</cell><cell>0.560</cell><cell cols="3">0.714 0.762 0.545</cell><cell>0.536</cell></row></table><note>Fig. 7. Examples of visual nuclei segmentation results on Kumar and CoNSeP.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE IV SPEED</head><label>IV</label><figDesc>-ACCURACY TRADE-OFF ON THE DATASET CONSEP. ? DENOTES MODEL INFERENCE WITHOUT MATRIX NMS OPERATION.</figDesc><table><row><cell></cell><cell>mPQ</cell><cell>bPQ</cell></row><row><cell>Center point + BCE loss (vanilla)</cell><cell cols="2">0.403 0.569</cell></row><row><cell>Keypoint heatmap + BCE loss</cell><cell cols="2">0.447 0.631</cell></row><row><cell cols="3">Keypoint heatmap + Focal loss (ours) 0.496 0.681</cell></row><row><cell>TABLE V</cell><cell></cell></row><row><cell cols="3">EFFECTIVENESS OF KEYPOINT-AWARE MODEL.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>TABLE VI ABLATION STUDY ON BACKBONE AND NECK SELECTION.</figDesc><table><row><cell>Backbone</cell><cell>Neck</cell><cell>mPQ</cell><cell>bPQ</cell></row><row><cell>ResNet-101</cell><cell>FPN</cell><cell cols="2">0.486 0.673</cell></row><row><cell cols="2">ResNext-101-DCN FPN</cell><cell cols="2">0.480 0.667</cell></row><row><cell>HRNet-w64</cell><cell>FPN</cell><cell cols="2">0.490 0.675</cell></row><row><cell>HRNet-w64</cell><cell>Shared JPFM</cell><cell cols="2">0.492 0.677</cell></row><row><cell>HRNet-w64</cell><cell cols="3">Unshared JPFM 0.496 0.681</cell></row><row><cell>d=1</cell><cell>d=2</cell><cell>d=4</cell><cell>d=8</cell></row><row><cell>Keypoint</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Heatmap</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Branch</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Kernel</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Branch</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Feature</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Branch</cell><cell></cell><cell></cell><cell></cell></row><row><cell>, the</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Evaluation code: https://github.com/TIA-Lab/PanNuke-metrics</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Due to their architecture limitation, some methods fail to perform classification including U-Net<ref type="bibr" target="#b34">[35]</ref>, CIA-Net<ref type="bibr" target="#b9">[10]</ref> and Triple U-Net<ref type="bibr" target="#b10">[11]</ref> and hence we leave their results as "-".</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The nuclear envelope environment and its cancer connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-H</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Factor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Cancer</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="196" to="209" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Nuclear shape and orientation features from H&amp;E images predict survival in early-stage estrogen receptor-positive breast cancers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Romo-Bucheli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Janowczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ganesan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gilmore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rimm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41374-018-0095-7</idno>
	</analytic>
	<monogr>
		<title level="j">Laboratory Investigation</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="1438" to="1448" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A bottom-up approach for tumour differentiation in whole slide images of lung adenocarcinoma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Alsubaie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E A</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Snead</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.2293316</idno>
	</analytic>
	<monogr>
		<title level="m">Medical Imaging: Digital Pathology</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">10581</biblScope>
			<biblScope unit="page" from="104" to="113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gadepalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kohlberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Timofeev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Q</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02442</idno>
		<title level="m">Detecting cancer metastases on gigapixel pathology images</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A machine learning-based prognostic predictor for stage iii colon cancer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Owen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Artificial intelligence in digital pathology-new tools for diagnosis and precision oncology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Schalper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">L</forename><surname>Rimm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Velcheti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madabhushi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature reviews Clinical oncology</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="703" to="715" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A dataset and a technique for generalized nuclear segmentation for computational pathology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Verma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vahadane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sethi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1550" to="1560" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>cited By 181</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Segmentation of Nuclei in Histopathology Images by Deep Regression of the Distance Map</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Naylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>La?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Reyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Walter</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMI.2018.2865709</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="448" to="459" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Hover-Net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">D</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E A</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Azam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2019.101563</idno>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page">101563</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cia-net: Robust nuclei instance segmentation with contour-aware information aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">F</forename><surname>Onder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tsougenis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Heng</surname></persName>
		</author>
		<idno type="DOI">https:/link.springer.com/chapter/10.1007/978-3-030-20351-1_53</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Information Processing in Medical Imaging</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="682" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Triple U-net: Hematoxylin-aware nuclei segmentation with progressive dense feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Han</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.media.2020.101786</idno>
	</analytic>
	<monogr>
		<title level="j">Medical Image Analysis</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page">101786</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06870</idno>
		<title level="m">Mask R-CNN</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eisenmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reinke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Onogur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stankovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bogunovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Carass</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feldmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">F</forename><surname>Frangi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Full</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Ginneken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Honauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kozubek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Landman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>M?rz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Maier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Maier-Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">H</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>M?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Neher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Niessen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">C</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sirinukunwattana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Speidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Taha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Van Der Sommen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jannin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kopp-Schneider</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.02051</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Is the winner really the best? A critical analysis of common research practice in biomedical image analysis competitions</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gamper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Koohbanani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jahanifar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Khurram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Azam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hewitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.10778</idno>
		<title level="m">PanNuke Dataset Extension, Insights and Baselines</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Cornernet: Detecting objects as paired keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Law</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Centernet: Keypoint triplets for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="6569" to="6578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reppoints: Point set representation for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9657" to="9666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Centripetalnet: Pursuing high-quality keypoint pairs for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Objects as points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kr?henb?hl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.07850</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Saccadenet: A fast and accurate object detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Focal Loss for Dense Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<idno type="DOI">https:/doi.ieeecomputersociety.org/10.1109/ICCV.2017.324</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="2999" to="3007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Conditional convolutions for instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.05664</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Solov2: Dynamic and fast instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NeurIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NeurIPS)</meeting>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SOLO: Segmenting objects by locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">V-Net: Fully Convolutional Neural Networks for Volumetric Medical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ahmadi</surname></persName>
		</author>
		<idno type="DOI">10.1109/3DV.2016.79</idno>
	</analytic>
	<monogr>
		<title level="m">2016 Fourth International Conference on 3D Vision (3DV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="565" to="571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition. arxiv</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep high-resolution representation learning for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">PanNuke: an open pan-cancer histology dataset for nuclei instance segmentation and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gamper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Koohbanani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Benet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khuram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rajpoot</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-23937-4_2</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="11" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A method for normalizing histology slides for quantitative analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Macenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Niethammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Marron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Woosley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Symposium on Biomedical Imaging: From Nano to Macro</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1107" to="1110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Micro-net: A unified model for segmentation of various objects in microscopy images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E A</forename><surname>Raza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shaban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Pelengaris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Rajpoot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="page" from="160" to="173" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Doll?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2117" to="2125" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
