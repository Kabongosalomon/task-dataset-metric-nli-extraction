<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEARNING ENERGY-BASED MODELS BY DIFFUSION RECOVERY LIKELIHOOD</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
							<email>ruiqigao@ucla.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
							<email>yangsong@cs.stanford.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
							<email>pooleb@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><forename type="middle">Nian</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Brain</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">UCLA</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">UCLA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LEARNING ENERGY-BASED MODELS BY DIFFUSION RECOVERY LIKELIHOOD</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Published as a conference paper at ICLR 2021</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:10+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While energy-based models (EBMs) exhibit a number of desirable properties, training and sampling on high-dimensional datasets remains challenging. Inspired by recent progress on diffusion probabilistic models, we present a diffusion recovery likelihood method to tractably learn and sample from a sequence of EBMs trained on increasingly noisy versions of a dataset. Each EBM is trained with recovery likelihood, which maximizes the conditional probability of the data at a certain noise level given their noisy versions at a higher noise level. Optimizing recovery likelihood is more tractable than marginal likelihood, as sampling from the conditional distributions is much easier than sampling from the marginal distributions. After training, synthesized images can be generated by the sampling process that initializes from Gaussian white noise distribution and progressively samples the conditional distributions at decreasingly lower noise levels. Our method generates high fidelity samples on various image datasets. On unconditional CIFAR-10 our method achieves FID 9.58 and inception score 8.30, superior to the majority of GANs. Moreover, we demonstrate that unlike previous work on EBMs, our long-run MCMC samples from the conditional distributions do not diverge and still represent realistic images, allowing us to accurately estimate the normalized density of data even for high-dimensional datasets. Our implementation is available at https</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>EBMs <ref type="bibr" target="#b28">(LeCun et al., 2006;</ref><ref type="bibr" target="#b35">Ngiam et al., 2011;</ref><ref type="bibr" target="#b22">Kim &amp; Bengio, 2016;</ref><ref type="bibr" target="#b55">Zhao et al., 2016;</ref><ref type="bibr" target="#b9">Goyal et al., 2017;</ref><ref type="bibr" target="#b51">Xie et al., 2016b;</ref><ref type="bibr" target="#b5">Finn et al., 2016;</ref><ref type="bibr" target="#b6">Gao et al., 2018;</ref><ref type="bibr" target="#b26">Kumar et al., 2019;</ref><ref type="bibr" target="#b37">Nijkamp et al., 2019b;</ref><ref type="bibr" target="#b4">Du &amp; Mordatch, 2019;</ref><ref type="bibr" target="#b10">Grathwohl et al., 2019;</ref><ref type="bibr" target="#b3">Desjardins et al., 2011;</ref><ref type="bibr" target="#b7">Gao et al., 2020;</ref><ref type="bibr" target="#b1">Che et al., 2020;</ref><ref type="bibr" target="#b11">Grathwohl et al., 2020;</ref><ref type="bibr" target="#b39">Qiu et al., 2019;</ref><ref type="bibr" target="#b40">Rhodes et al., 2020)</ref> are an appealing class of probabilistic models, which can be viewed as generative versions of discriminators <ref type="bibr" target="#b29">Lee et al., 2018;</ref><ref type="bibr" target="#b11">Grathwohl et al., 2020</ref>), yet can be learned from unlabeled data. Despite a number of desirable properties, two challenges remain for training EBMs on highdimensional datasets. First, learning EBMs by maximum likelihood requires Markov Chain Monte Carlo (MCMC) to generate samples from the model, which can be extremely expensive. Second, as pointed out in <ref type="bibr" target="#b36">Nijkamp et al. (2019a)</ref>, the energy potentials learned with non-convergent MCMC do not have a valid steady-state, in the sense that samples from long-run Markov chains can differ greatly from observed samples, making it difficult to evaluate the learned energy potentials.</p><p>Another line of work, originating from <ref type="bibr" target="#b44">Sohl-Dickstein et al. (2015)</ref>, is to learn from a diffused version of the data, which are obtained from the original data via a diffusion process that sequentially adds Gaussian white noise. From such diffusion data, one can learn the conditional model of the data at a certain noise level given their noisy versions at the higher noise level of the diffusion process. After learning the sequence of conditional models that invert the diffusion process, one can then generate synthesized images from Gaussian white noise images by ancestral sampling. Building on Published as a conference paper at ICLR 2021 Figure 1: Generated samples on LSUN 128 2 church outdoor (left), LSUN 128 2 bedroom (center) and CelebA 64 2 (right). <ref type="bibr" target="#b44">Sohl-Dickstein et al. (2015)</ref>, <ref type="bibr" target="#b17">Ho et al. (2020)</ref> further developed the method, obtaining strong image synthesis results.</p><p>Inspired by <ref type="bibr" target="#b44">Sohl-Dickstein et al. (2015)</ref> and <ref type="bibr" target="#b17">Ho et al. (2020)</ref>, we propose a diffusion recovery likelihood method to tackle the challenge of training EBMs directly on a dataset by instead learning a sequence of EBMs for the marginal distributions of the diffusion process. The sequence of marginal EBMs are learned with recovery likelihoods that are defined as the conditional distributions that invert the diffusion process. Compared to standard maximum likelihood estimation (MLE) of EBMs, learning marginal EBMs by diffusion recovery likelihood only requires sampling from the conditional distributions, which is much easier than sampling from the marginal distributions. After learning the marginal EBMs, we can generate synthesized images by a sequence of conditional samples initialized from the Gaussian white noise distribution. Unlike <ref type="bibr" target="#b17">Ho et al. (2020)</ref> that approximates the reverse process by normal distributions, in our case the conditional distributions are derived from the marginal EBMs, which are more flexible. The framework of recovery likelihood was originally proposed in <ref type="bibr" target="#b0">Bengio et al. (2013)</ref>. In our work, we adapt it to learning the sequence of marginal EBMs from the diffusion data.</p><p>Our work is also related to the denoising score matching method of <ref type="bibr" target="#b49">Vincent (2011)</ref>, which was further developed by <ref type="bibr" target="#b45">Song &amp; Ermon (2019;</ref> for learning from diffusion data. The training objective used for diffusion probabilisitic models is a weighted version of the denoising score matching objective, as revealed by <ref type="bibr" target="#b17">Ho et al. (2020)</ref>. These methods learn the score functions (the gradients of the energy functions) directly, instead of using the gradients of learned energy functions as in EBMs. On the other hand, <ref type="bibr" target="#b43">Saremi et al. (2018)</ref> parametrizes the score function as the gradient of a MLP energy function, and <ref type="bibr" target="#b42">Saremi &amp; Hyvarinen (2019)</ref> further unifies denoising score matching and neural empirical Bayes.</p><p>We demonstrate the efficacy of diffusion recovery likelihood on CIFAR-10, CelebA and LSUN datasets. The generated samples are of high fidelity and comparable to GAN-based methods. On CIFAR-10, we achieve FID 9.58 and inception score 8.30, exceeding existing methods of learning explicit EBMs to a large extent. We also demonstrate that diffusion recovery likelihood outperforms denoising score matching from diffusion data if we naively take the gradients of explicit energy functions as the score functions. More interestingly, by using a thousand diffusion time steps, we demonstrate that even very long MCMC chains from the sequence of conditional distributions produce samples that represent realistic images. With the faithful long-run MCMC samples from the conditional distributions, we can accurately estimate the marginal partition function at zero noise level by importance sampling, and thus evaluate the normalized density of data under the EBM. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Let x ? p data (x) denote a training example, and p ? (x) denote a model's probability density function that aims to approximates p data (x). An energy-based model (EBM) is defined as:</p><formula xml:id="formula_0">p ? (x) = 1 Z ? exp(f ? (x)),<label>(1)</label></formula><p>where Z ? = exp(f ? (x))dx is the partition function, which is analytically intractable for highdimensional x. For images, we parameterize f ? (x) with a convolutional neural network with a scalar output.</p><p>The energy-based model in equation 1 can, in principle, be learned through MLE. Specifically, suppose we observe samples x i ? p data (x) for i = 1, 2, ..., n. The log-likelihood function is</p><formula xml:id="formula_1">L(?) = 1 n n i=1 log p ? (x i ) . = E x?p data [log p ? (x)].<label>(2)</label></formula><p>In MLE, we seek to maximize the log-likelihood function, where the gradient approximately follows <ref type="bibr" target="#b51">(Xie et al., 2016b</ref>)</p><formula xml:id="formula_2">? ? ?? D KL (p data p ? ) = E x?p data ? ?? f ? (x) ? E x?p ? ? ?? f ? (x) .<label>(3)</label></formula><p>The expectations can be approximated by averaging over the observed samples and the synthesized samples drawn from the model distribution p ? (x) respectively. Generating synthesized samples from p ? (x) can be done with Markov Chain Monte Carlo (MCMC) such as Langevin dynamics (or Hamiltonian Monte Carlo <ref type="bibr" target="#b8">(Girolami &amp; Calderhead, 2011)</ref>), which iterates where ? indexes the time, ? is the step size, and ? ? N (0, I). The difficulty lies in the fact that for highdimensional and multi-modal distributions, MCMC sampling can take a long time to converge, and the sampling chains may have difficulty traversing modes. As demonstrated in <ref type="figure" target="#fig_1">Figure 2</ref>, training EBMs with synthesized samples from non-convergent MCMC results in malformed energy landscapes <ref type="bibr" target="#b37">(Nijkamp et al., 2019b)</ref>, even if the samples from the model look reasonable.</p><formula xml:id="formula_3">x ? +1 = x ? + ? 2 2 ? x f ? (x ? ) + ? ? ,<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RECOVERY LIKELIHOOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">FROM MARGINAL TO CONDITIONAL</head><p>Given the difficulty of sampling from the marginal density p ? (x), following <ref type="bibr" target="#b0">Bengio et al. (2013)</ref>, we use the recovery likelihood defined by the density of the observed sample conditional on a noisy sample perturbed by isotropic Gaussian noise. Specifically, letx = x + ? be the noisy observation of x, where ? N (0, I). Suppose p ? (x) is defined by the EBM in equation 1, then the conditional EBM can be derived as</p><formula xml:id="formula_4">p ? (x|x) = 1 Z ? (x) exp f ? (x) ? 1 2? 2 x ? x 2 ,<label>(5)</label></formula><p>whereZ ? (x) = exp f ? (x) ? 1 2? 2 x ? x 2 dx is the partition function of this conditional EBM. See Appendix A.1 for the derivation. Compared to p ? (x) (equation 1), the extra quadratic term 1 2? 2 x ? x 2 in p ? (x|x) constrains the energy landscape to be localized aroundx, making the latter less multi-modal and easier to sample from. As we will show later, when ? is small, p ? (x|x) is approximately a single mode Gaussian distribution, which greatly reduces the burden of MCMC.</p><p>A more general formulation isx = ax + ? , where a is a positive constant. In that case, we can let y = ax, and treat y as the observed sample. Assume p ? (y) = 1 Z ? exp(f ? (y)), then by change of variable, the density function of x can be derived as g ? (x) = ap ? (ax).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MAXIMIZING RECOVERY LIKELIHOOD</head><p>With the conditional EBM, assume we have observed samples x i ? p data (x) and the corresponding perturbed samplesx i = x i + ? i for i = 1, ..., n. We define the recovery log-likelihood function as</p><formula xml:id="formula_5">J (?) = 1 n n i=1 log p ? (x i |x i ).<label>(6)</label></formula><p>The term recovery indicates that we attempt to recover the clean sample x i from the noisy sampl? x i . Thus, instead of maximizing L(?) in equation 2, we can maximize J (?), whose distributions are easier to sample from. Specifically, we generate synthesized samples by K steps of Langevin dynamics that iterates</p><formula xml:id="formula_6">x ? +1 = x ? + ? 2 2 (? x f ? (x ? ) + 1 ? 2 (x ? x ? )) + ? ? .<label>(7)</label></formula><p>The model is then updated following the same learning gradients as MLE (equation 3), because the quadratic term ? 1 2? 2 x ? x 2 is not related to ?. Following the classical analysis of MLE, we can show that the point estimate given by maximizing recovery likelihood is an unbiased estimator of the true parameters, which means that given enough data, a rich enough model and exact synthesis, maximizing the recovery likelihood learns ? such that p data (x) = p ? (x). See Appendix A.2 for a theoretical explanation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">NORMAL APPROXIMATION TO CONDITIONAL</head><p>When the variance of perturbed noise ? 2 is small, p ? (x|x) can be approximated by a normal distribution via a first order Taylor expansion atx. Specifically, the negative conditional energy is</p><formula xml:id="formula_7">?E ? (x|x) = f ? (x) ? 1 2? 2 x ? x 2 (8) . = f ? (x) + ? x f ? (x), x ?x ? 1 2? 2 x ? x 2 (9) = ? 1 2? 2 x ? (x + ? 2 ? x f ? (x)) 2 + c,<label>(10)</label></formula><p>where c include terms irrelevant of x (see Appendix A.3 for a detailed derivation). In the above approximation, we do not perform second order Taylor expansion because ? 2 is small, and x ? x 2 /2? 2 will dominate all the second order terms from Taylor expansion. Thus we can approximate p ? (x|x) by a Gaussian approximation p ? (x|x):</p><formula xml:id="formula_8">p ? (x|x) = N x;x + ? 2 ? x f ? (x), ? 2 .</formula><p>(11) We can sample from this distribution using:</p><formula xml:id="formula_9">x gen =x + ? 2 ? x f ? (x) + ? ,<label>(12)</label></formula><p>where ? N (0, I). This resembles a single step of Langevin dynamics, except that ? is replaced by ? 2? in Langevin dynamics. This normal approximation has two traits: (1) it verifies the fact that the conditional density p ? (x|x) can be generally easier to sample from when ? is small; (2) it provides hints of choosing the step size of Langevin dynamics, as discussed in section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">CONNECTION TO VARIATIONAL INFERENCE AND SCORE MATCHING</head><p>The normal approximation to the conditional distribution leads to a natural connection to diffusion probabilistic models <ref type="bibr" target="#b44">(Sohl-Dickstein et al., 2015;</ref><ref type="bibr" target="#b17">Ho et al., 2020)</ref> and denoising score matching <ref type="bibr" target="#b49">(Vincent, 2011;</ref><ref type="bibr" target="#b45">Song &amp; Ermon, 2019;</ref><ref type="bibr" target="#b43">Saremi et al., 2018;</ref><ref type="bibr" target="#b42">Saremi &amp; Hyvarinen, 2019)</ref>. Specifically, in diffusion probabilistic models, instead of modeling p ? (x) as an energy-based model, it recruits variational inference and directly models the conditional density as</p><formula xml:id="formula_10">p ? (x|x) ? N x + ? 2 s ? (x), ? 2 ,<label>(13)</label></formula><p>which is in agreement with the normal approximation (equation 11), with s ? (x) = ? x f ? (x). On the other hand, the training objective of denoising score matching is to minimize</p><formula xml:id="formula_11">1 2? 2 E p(x,x) [ x ? (x + ? 2 s ? (x)) 2 ],<label>(14)</label></formula><p>where s ? (x) is the score of the density ofx. This objective is in agreement with the objective of maximizing log-likelihood of the normal approximation (equation 10), except that for normal approximation, ? x f ? (?) is the score of density of x, instead ofx. However, the difference between the scores of density of x andx is of O(? 2 ), which is negligible when ? is sufficiently small (see Appendix A.4 for details). We can further show that the learning gradient of maximizing log-likelihood of the normal approximation is approximately the same as the learning gradient of maximizing the original recovery log-likelihood with one step of Langevin dynamics (see Appendix A.5). It indicates that the training process of maximizing recovery likelihood agrees with the one of diffusion probabilistic models and denoising score matching when ? is small.</p><p>As the normal approximation is accurate only when ? is small, it requires many time steps in the diffusion process for this approximation to work well, which is also reported in <ref type="bibr" target="#b17">Ho et al. (2020)</ref> and <ref type="bibr" target="#b46">Song &amp; Ermon (2020)</ref>. In contrast, the diffusion recovery likelihood framework can be more flexible in choosing the number of time steps and the magnitude of ?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">DIFFUSION RECOVERY LIKELIHOOD</head><p>As we discuss, sampling from p ? (x|x) becomes simple only when ? is small. In the extreme case when ? ? ?, p ? (x|x) converges to the marginal distribution p ? (x), which is again highly multimodal and difficult to sample from. To keep ? small and meanwhile equip the model with the ability to generate new samples initialized from white noise, inspired by Sohl-Dickstein et al. <ref type="formula" target="#formula_0">(2015)</ref> and <ref type="bibr" target="#b17">Ho et al. (2020)</ref>, we propose to learn a sequence of recovery likelihoods, on gradually perturbed observed data based on a diffusion process. Specifically, assume a sequence of perturbed observations x 0 , x 1 , ..., x T such that</p><formula xml:id="formula_12">x 0 ? p data (x); x t+1 = 1 ? ? 2 t+1 x t + ? t+1 t+1 , t = 0, 1, ...T ? 1.<label>(15)</label></formula><p>The scaling factor 1 ? ? 2 t+1 ensures that the sequence is a spherical interpolation between the observed sample and Gaussian white noise. Let y t = 1 ? ? 2 t+1 x t , and we assume a sequence of conditional EBMs</p><formula xml:id="formula_13">p ? (y t |x t+1 ) = 1 Z ?,t (x t+1 ) exp f ? (y t , t) ? 1 2? 2 t+1 x t+1 ? y t 2 , t = 0, 1, ..., T ? 1,<label>(16)</label></formula><p>where f ? (y t , t) is defined by a neural network conditioned on t.</p><p>We follow the learning algorithm in section 3.2. A question is how to determine the step size schedule ? t of Langevin dynamics. Inspired by the sampling procedure of the normal approximation (equation 12), we set the step size ? t = b? t , where b &lt; 1 is a tuned hyperparameter. This schedule turns out to work well in practice. Thus the K steps of Langevin dynamics iterates</p><formula xml:id="formula_14">y ? +1 t = y ? t + b 2 ? 2 t 2 (? y f ? (y ? t , t) + 1 ? 2 Algorithm 1 Training repeat Sample t ? Unif({0, ..., T ? 1}). Sample pairs (y t , x t+1 ). Set synthesized sample y ? t = x t+1 . for ? ? 1 to K do</formula><p>Update y ? t according to equation 17. end for Update ? following the gradients</p><formula xml:id="formula_15">? ?? f ? (y t , t) ? ? ?? f ? (y ? t , t). until converged. Algorithm 2 Progressive sampling Sample x T ? N (0, I). for t ? T ? 1 to 0 do y t = x t+1 . for ? ? 1 to K do Update y t according to equation 17. end for x t = y t / 1 ? ? 2 t+1 . end for return x 0 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>To show that diffusion recovery likelihood is flexible for diffusion process of various magnitudes of noise, we test the method under two settings: (1) T = 6, with K = 30 steps of Langevin dynamic per time step; (2) T = 1000, with sampling from normal approximation. (2) resembles the noise schedule of <ref type="bibr" target="#b17">Ho et al. (2020)</ref> and the magnitude of noise added at each time step is much smaller compared to (1). For both settings, we set ? 2 t to increase linearly. The network structure of f ? (x, t) is based on Wide ResNet <ref type="bibr" target="#b54">(Zagoruyko &amp; Komodakis, 2016)</ref> and we remove weight normalization. t is encoded by Transformer sinusoidal position embedding as in <ref type="bibr" target="#b17">(Ho et al., 2020)</ref>. For (1), we find that including another scaling factor c t to the step size ? t helps. Architecture and training details are in Appendix B. Henceforth we simply refer the two settings as T6 and T1k. <ref type="figure" target="#fig_2">Figures 1 and 4</ref> display uncurated samples generated from learned models on CIFAR-10, CelebA 64 ? 64, LSUN 64 ? 64 and 128 ? 128 datasets under T6 setting. The samples are of high fidelity and comparable to GAN-based methods. Appendix C.5 provides more generated samples. Tables 1 and 3 summarize the quantitative evaluations on CIFAR-10 and CelebA datasets, in terms of Frechet Inception Distance (FID) <ref type="bibr" target="#b15">(Heusel et al., 2017)</ref> and inception scores <ref type="bibr" target="#b41">(Salimans et al., 2016)</ref>. On CIFAR-10, our model achieves FID 9.58 and inception score 8.30, which outperforms existing methods of learning explicit energy-based models to a large extent, and is superior to a majority of GAN-based methods. On CelebA, our model obtains results comparable with the state-of-the-art GAN-based methods, and outperforms score-based methods <ref type="bibr" target="#b45">(Song &amp; Ermon, 2019;</ref>. Note that the score-based methods <ref type="bibr" target="#b45">(Song &amp; Ermon, 2019;</ref> and diffusion probabilistic models <ref type="bibr" target="#b17">(Ho et al., 2020)</ref> directly parametrize and learn the score of data distribution, whereas our goal is to learn explicit energy-based models.   <ref type="bibr" target="#b32">(Miyato et al., 2018)</ref> 21.7 8.22 ? .05 <ref type="bibr">SNGAN-DDLS (Che et al., 2020)</ref> 15.42 9.09 ? .10 StyleGAN2-ADA <ref type="bibr" target="#b21">(Karras et al., 2020)</ref> 3.26 9.74 ? .05</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">IMAGE GENERATION</head><p>Score-based NCSN <ref type="bibr" target="#b45">(Song &amp; Ermon, 2019)</ref> 25.32 8.87 ? .12 NCSN-v2 <ref type="bibr" target="#b46">(Song &amp; Ermon, 2020)</ref> 10.87 8.40 ? .07 DDPM <ref type="bibr" target="#b17">(Ho et al., 2020)</ref> 3.17 9.46 ? .11</p><p>Explicit EBM-conditional</p><p>CoopNets (Xie et al., 2019) -7.30 EBM-IG <ref type="bibr" target="#b4">(Du &amp; Mordatch, 2019)</ref> 37.9 8.30 JEM <ref type="bibr" target="#b10">(Grathwohl et al., 2019)</ref> 38.4 8.76</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Explicit EBM</head><p>Muli-grid <ref type="bibr" target="#b6">(Gao et al., 2018)</ref> 40.01 6.56 CoopNets <ref type="bibr" target="#b50">(Xie et al., 2016a)</ref> 33.61 6.55 EBM-SR <ref type="bibr" target="#b37">(Nijkamp et al., 2019b)</ref> -6.21 EBM-IG <ref type="bibr" target="#b4">(Du &amp; Mordatch, 2019)</ref> 38.2 6.78 Ours (T6) 9.58 8.30 ? .11 <ref type="table">Table 2</ref>: Ablation of training objectives, time steps T and sampling steps K on CIFAR-10. K = 0 indicates that we sample from the normal approximation.</p><p>Setting / Objective FID? Inception? T = 1, K = 180 32.12 6.72 ? 0.12 T = 1000, K = 0 22.58 7.71 ? 0.08 T = 1000, K = 0 (DSM) 21.76 7.76 ? 0.11 T = 6, K = 10 --T = 6, K = 30 9.58 8.30 ? 0.11 T = 6, K = 50 9.36 8.46 ? 0.13  <ref type="bibr" target="#b46">(Song &amp; Ermon, 2020)</ref> 10.23</p><p>EBM-SR <ref type="bibr" target="#b37">(Nijkamp et al., 2019b)</ref> 23.02 EBM-Triangle <ref type="bibr" target="#b14">(Han et al., 2020)</ref> 24.70 Ours (T6) 5.98   Interpolation. As shown in <ref type="figure" target="#fig_3">Figure 5</ref>, our model is capable of smooth interpolation between two generated samples. Specifically, for two samples x</p><p>0 and x for every sampling step at every time step. More interpolation results can be found in Appendix C.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image inpainting.</head><p>A promising application of energy-based models is to use the learned model as a prior model for image processing, such as image inpainting, denoising and super-resolution <ref type="bibr" target="#b6">(Gao et al., 2018;</ref><ref type="bibr" target="#b4">Du &amp; Mordatch, 2019;</ref><ref type="bibr" target="#b45">Song &amp; Ermon, 2019)</ref>. In <ref type="figure" target="#fig_4">Figure 6</ref>, we demonstrate that the learned models by maximizing recovery likelihoods are capable of realistic and semantically meaningful image inpainting. Specifically, given a masked image and the corresponding mask, we first obtain a sequence of perturbed masked images at different noise levels. The inpainting can be easily achieved by running Langevin dynamics progressively on the masked pixels while keeping the observed pixels fixed at decreasingly lower noise levels. Additional image inpainting results can be found in Appendix C.4.</p><p>Ablation study. <ref type="table">Table 2</ref> summarizes the results of ablation study on CIFAR-10. We investigate the effect of changing the numbers of time steps T and sampling steps K. First, to show that it is beneficial to learn by diffusion recovery likelihood, we compare against a baseline approach (T = 1, K = 180) where we use only one time step, so that the recovery likelihood becomes marginal likelihood. The approach is adopted by <ref type="bibr" target="#b37">Nijkamp et al. (2019b)</ref> and <ref type="bibr" target="#b4">Du &amp; Mordatch (2019)</ref>. For fair comparison, we equip the baseline method the same budget of MCMC sampling as our T6 setting (i.e., 180 sampling steps). Our method outperforms this baseline method by a large margin. Also the models are trained more efficiently as the number of sampling steps per iteration is reduced and amortized by time steps.</p><p>Next, we report the sample quality of setting T1k. We test two training objectives for this setting: (1) maximizing recovery likelihoods (T = 1000, K = 0) and (2) maximizing the approximated normal distributions (T=1000, K=0 (DSM)). As mentioned in section 3.4, (2) is equivalent to the training objectives of denoising score matching <ref type="bibr" target="#b45">(Song &amp; Ermon, 2019;</ref>) and diffusion probabilistic model <ref type="bibr" target="#b17">(Ho et al., 2020)</ref>, except that the score functions are taken as the gradients of explicit energy functions. In practice, for a direct comparison, (2) follows the same implementation as in <ref type="bibr" target="#b17">Ho et al. (2020)</ref>, except that the score function is parametrized as the gradients of the explicit energy function used in our method.</p><p>(1) and (2) achieve similar sample quality in terms of quantitative metrics, where (2) results in a slightly better FID score yet a slightly worse inception score. This verifies the fact that the training objectives of (1) and (2) are consistent. Both (1) and (2) performs worse than setting T6. A possible explanation is that the sampling error may accumulate with many time steps, so that a more flexible schedule of time steps accompanied with certain amount of sampling steps is preferred.</p><p>Last, we examine the influence of varying the number of sampling steps while fixing the number of time steps. The training becomes unstable when the number of sampling steps are not enough (T = 6, K = 10), and more sampling steps lead to better sample quality. However, since K = 50 does not gain significant improvement versus K = 30, yet of much higher computational cost, we keep K = 30 for image generation on all datasets. See Appendix C.1 for a plot of FID scores over iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">LONG-RUN CHAIN ANALYSIS</head><p>Besides achieving high quality generation, a perhaps equally important aspect of learning EBMs is to obtain a faithful energy potential. A principle way to check the validity of the learned potential is to perform long-run sampling chains and see if the samples still remain realistic. However, as pointed out in <ref type="bibr" target="#b36">Nijkamp et al. (2019a)</ref>, almost all existing methods of learning EBMs fail in getting realistic long-run chain samples. In this subsection, we demonstrate that by composing a thousand diffusion time steps (T1k setting), we can form steady long-run MCMC chains for the conditional distributions.</p><p>First we prepare a faithful sampler for conducting long-run sampling. Specifically, after training the model under T 1k setting by maximizing diffusion recovery likelihood, for each time step, we first sample from the normal approximation and count it as one sampling step, and then use Hamiltonian Monte Carlo (HMC) <ref type="bibr" target="#b34">(Neal et al., 2011)</ref> with 2 leapfrog steps to perform the consecutive sampling steps. To obtain a reasonable schedule of sampling step size, for each time step we adaptively adjust the step size of HMC to make the average acceptance rate range in [0.6, 0.9], which is computed over 1000 chains for 100 steps. <ref type="figure" target="#fig_5">Figure 7</ref> displays the adjusted step size (left) and acceptance rate (center) over time step. The adjusted step size increases logarithmically. With this step size schedule, we generate long-run chains from the learned sequence of conditional distributions. As shown in <ref type="figure" target="#fig_6">Figure 8</ref>, images remain realistic for even 100k sampling steps in total (i.e., 100 sampling steps per time step), resulting in FID 24.89. This score is close to the one computed on samples generated by 1k steps (i.e., sampled from normal approximation), which is 25.12. As a further check, we recruit a No-U-Turn Sampler (Hoffman &amp; Gelman, 2014) with the same step size schedule as HMC to perform long-run sampling, where the samples also remain realistic. See Appendix C.2 for details. More interestingly, given the faithful long-run MCMC samples from the conditional distributions, we can estimate the log ratio of the partition functions of the marginal distributions, and further estimate the partition function of p ? (y 0 ). The strategy is based on annealed importance sampling <ref type="bibr" target="#b33">(Neal, 2001)</ref>. See Appendix A.6 for the implementation details. The right subfigure of <ref type="figure" target="#fig_5">Figure 7</ref> depicts the estimated log partition function of p ? (y 0 ) over the number of MCMC samples used. To verify the estimation strategy and again check the long-run chain samples, we conduct multiple runs using samples generated with different numbers of HMC steps and display the estimation curves. All the curves saturate to values close to each other at the end, indicating the stability of long-run chain samples and the effectiveness of the estimation strategy. With the estimated partition function, by change of variable, we can estimate the normalized density of data as g ? (x 0 ) = 1 ? ? 2 1 p ? ( 1 ? ? 2 1 x 0 ). We report test bits per dimension on CIFAR-10 in <ref type="table" target="#tab_2">Table 4</ref>. Note that the result should be taken with a grain of salt, because the partition function is estimated by samples and as shown in Appendix A.6, it is a stochastic lower bound of the true value, that will converge to the true value when the number of samples grows large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We propose to learn EBMs by diffusion recovery likelihood, a variant of MLE applied to diffusion processes. We achieve high quality image synthesis, and with a thousand noise levels, we obtain faithful long-run MCMC samples that indicate the validity of the learned energy potentials. Since this method can learn EBMs efficiently with small budget of MCMC, we are also interested in scaling it up to higher resolution images and investigating this method in other data modalities in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENT</head><p>The work was done while Ruiqi Gao and Yang Song were interns at Google Brain during the summer of 2020. The work of Ying Nian Wu is supported by NSF DMS-2015577. We thank Alexander A. Alemi, Jonathan Ho, Tim Salimans and Kevin Murphy for their insightful discussions during the course of this project.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A EXTENDED DERIVATIONS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 DERIVATION OF EQUATION 5</head><p>Letx = x + ? , where ? N (0, I). Given the marginal distribution of</p><formula xml:id="formula_18">p ? (x) = 1 Z ? exp(f ? (x)),<label>(18)</label></formula><p>We can derive the conditional distribution of x givenx as</p><formula xml:id="formula_19">p ? (x|x) = p ? (x)p(x|x)/p(x) (19) = 1 Z ? exp(f ? (x)) 1 (2?? 2 ) n 2 exp(? 1 2? 2 x ? x 2 )/p(x) (20) = 1 Z ? (x) exp f ? (x) ? 1 2? 2 x ? x 2 ,<label>(21)</label></formula><p>where we absorb all the terms that are irrelevant of x asZ ? (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 THEORETICAL UNDERSTANDING</head><p>In this subsection, we analyze the asymptotic behavior of maximizing the recovery log-likelihood.</p><p>For model class {p ? (x), ??}, suppose there exists ? * such that p data = p ? * . According to the classical theory of MLE, let? 0 be the point estimate by MLE. Then we have? is an unbiased estimator of ? * with asymptotic normality:</p><formula xml:id="formula_20">? n(? 0 ? ? * ) ? N (0, I 0 (? * ) ?1 ),<label>(22)</label></formula><formula xml:id="formula_21">where I 0 (?) = E x?p ? [?? 2 ? log p ? (x)]</formula><p>is the Fisher information, and n is the number of observed samples.</p><p>Let? be the point estimate given by maximizing recovery log-likelihood, we can derive a result in parallel to that of MLE:</p><formula xml:id="formula_22">? n(? ? ? * ) ? N (0, I(? * ) ?1 ),<label>(23)</label></formula><formula xml:id="formula_23">where I(?) = E p ? (x,x) [?? 2 ? log p ? (x|x)].</formula><p>The relationship between I 0 (?) and I(?) is that</p><formula xml:id="formula_24">I 0 (?) = I(?) + E p ? (x,x) [?? 2 ? log p ? (x)].<label>(24)</label></formula><p>Thus there is loss of information, but? is still an unbiased estimator of ? * with asymptotic normality.</p><formula xml:id="formula_25">A.3 DETAILED DERIVATION OF NORMAL APPROXIMATION ?E ? (x|x) = f ? (x) ? 1 2? 2 x ? x 2<label>(25)</label></formula><formula xml:id="formula_26">. = f ? (x) + ? x f ? (x), x ?x ? 1 2? 2 x ? x 2 (26) = ? 1 2? 2 x 2 ? 2 x, x + x 2 + ? x f ? (x), x ? ? x f ? (x),x + f ? (x) (27) = ? 1 2? 2 x 2 ? 2 x + ? 2 ? x f ? (x), x ? 1 2? 2 x 2 ? ? x f ? (x),x + f ? (x) (28) = ? 1 2? 2 x ? (x + ? 2 ? x f ? (x)) 2 + c,<label>(29)</label></formula><p>A.4 DIFFERENCE BETWEEN THE SCORES OF p(x) AND p(x)</p><p>For notation clarity, withx = x + , we let p be the distribution ofx, and p be the distribution of x.</p><p>Then for a smooth testing function with vanishing tails,</p><formula xml:id="formula_27">E[h(x)] = E[h(x + )] (30) . = E[h(x) + h (x) + h (x) 2 /2]<label>(31)</label></formula><formula xml:id="formula_28">= E[h(x)] + E[h (x)]? 2 /2.<label>(32)</label></formula><p>Integral by parts,</p><formula xml:id="formula_29">E[h (x)] = h (x)p(x)dx = ? h (x)p (x)dx = p (x)h(x)dx.<label>(33)</label></formula><p>Thus we have the heat equation</p><formula xml:id="formula_30">p(x) = p(x) + p (x)? 2 /2.<label>(34)</label></formula><p>The score</p><formula xml:id="formula_31">? x logp(x) = ? x log p(x) + ? x log(1 + p (x)/p(x)? 2 /2) (35) . = ? x log p(x) + ? x [p (x)/p(x)]? 2 /2.<label>(36)</label></formula><p>Thus the difference between the score of p and p is of the order ? 2 , which is negligible when ? 2 is small.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 LEARNING GRADIENTS OF NORMAL APPROXIMATION AND ORIGINAL RECOVERY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LIKELIHOOD</head><p>In this subsection we demonstrate that the learning gradient of maximizing likelihood of the normal approximation is approximately the same as the gradient of maximizing the original recovery likelihood with one step of Langevin sampling. Specifically, the gradient of the normal approximation of recovery log-likelihood for an observed x obs is</p><formula xml:id="formula_32">? ? 1 2? 2 x obs ? (x + ? 2 f ? (x)) 2 = ? ? f ? (x)(x obs ? (x + ? 2 f ? (x)).<label>(37)</label></formula><p>On the other hand, to maximize the original recovery likelihood, suppose we sample x syn ? p ? (x|x), then the gradient ascent of the original recovery log-likelihood is</p><formula xml:id="formula_33">? ? f ? (x obs ) ? E[? ? f ? (x syn )] = h ? (x obs ) ? E[h ? (x syn )],<label>(38)</label></formula><p>where h ? (x) = ? ? f ? (x). Approximately, if we perform one step of Langevin dynamics fromx to obtain x syn , i.e., x syn =x + ? 2 f ? (x) + ? 2?e, and assume f ? (x) is locally linear in x, then</p><formula xml:id="formula_34">? ? f ? (x obs ) ? E[? ? f ? (x init )] (39) = h ? (x obs ) ? E[h ? (x + ? 2 f ? (x) + ?e)] (40) . = h ? (x) + h ? (x)(x obs ?x) ? E[h ? (x) + h ? (x)(? 2 f ? (x) + ?e)] (41) = h ? (x)(x obs ? (x + ? 2 f ? (x)) (42) = ? ? f ? (x)(x obs ? (x + ? 2 f ? (x)).<label>(43)</label></formula><p>Comparing equations 37 and 43, we see that the two gradients agree with each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 ESTIMATING THE PARTITION FUNCTION</head><p>We can utilize the sequence of learned distributions of y t (= 1 ? ? 2 t+1 x t ) to estimate the partition function. Specifically, the marginal distribution of y t is</p><formula xml:id="formula_35">p ? (y t ) = 1 Z ?,t exp (f ? (y t , t))<label>(44)</label></formula><p>We can estimate the ratio of the partition functions at two consecutive time steps using importance sampling</p><formula xml:id="formula_36">Z ?,t Z ?,t+1 = E p ? (yt+1) [exp(f ? (y, t) ? f ? (y, t + 1))]<label>(45)</label></formula><formula xml:id="formula_37">. = 1 M M i=1 [exp(f ? (y t+1,i , t) ? f ? (y t+1,i , t + 1))] ,<label>(46)</label></formula><p>where y t+1,i are samples generated by progressive sampling. Starting from t = T , where p T (x) follows Gaussian distribution, we can compute log Z ?,t along the reverse path of the diffusion process, until we reach t = 0:</p><formula xml:id="formula_38">Z ?,0 = Z ?,T T ?1 t=0 Z ?,t Z ?,t+1 .<label>(47)</label></formula><p>In practice, since the ratio given by MCMC samples can vary across many orders of magnitude, it is more meaningful to estimate</p><formula xml:id="formula_39">log Z ?,0 = log Z ?,T + T ?1 t=0 log Z ?,t Z ?,t+1 .<label>(48)</label></formula><p>Unfortunately, although equation 46 is an unbiased estimator of Z ?,t /Z ?,t+1 , the logarithm of this estimator is generally a stochastic lower bound of log(Z ?,t /Z ?,t+1 ) (Grosse et al., 2016). However, as we show below, this bound will gradually converge to an unbiased estimator of log(Z ?,t /Z ?,t+1 ), as the number of samples becomes large. Specifically, let A be the estimator in equation 46, ? be the true value of Z ?,t /Z ?,t+1 . We have E[A] = ?, then by second order Taylor expansion,</p><formula xml:id="formula_40">E[log A] . = E log ? + 1 ? (A ? ?) ? 1 2? 2 (A ? ?) 2 (49) = log ? ? 1 2? 2 Var(A).<label>(50)</label></formula><p>By law of large number, Var(A) ? 0 as M ? ?, and thus E[log A] ? log ?. This is also consistent with the estimation curves in the right subfigure of <ref type="figure" target="#fig_5">Figure 7</ref>: since Var(A) ? 0, the estimation curve increases from below as the number of samples becomes larger. When the curve becomes stable, it indicates the convergence.</p><p>Evaluation metrics. We use FID and inception scores as quantitative evaluation metrics of sample quality. On all the datasets, we calculate FID and inception scores on 50,000 samples using the original code from <ref type="bibr" target="#b41">Salimans et al. (2016)</ref> and <ref type="bibr" target="#b15">Heusel et al. (2017)</ref>.   <ref type="figure">Figure 9</ref> demonstrates FID scores computed on 2,500 samples every 15,000 iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 LONG-RUN CHAIN SAMPLING WITH NUTS</head><p>As a further check, we use a No-U-Turn Sampler (Hoffman &amp; Gelman, 2014) to perform the longrun chain sampling, with the same step size schedule obtained for HMC sampler. <ref type="figure">Figure 10</ref> displays samples with different number of sampling steps. The samples remain realistic after 100k sampling steps in total and the FID score remains stable.       </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Illustration of diffusion recovery likelihood on 2D checkerboard example. Top: progressively generated samples. Bottom: estimated marginal densities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Comparison of learningEBMs by diffusion recovery likelihood (Ours) versus marginal likelihood (Short-run).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Generated samples on unconditional CIFAR-10 (left) and LSUN 64 2 church outdoor (center) and LSUN 64 2 bedroom (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Interpolation results between the leftmost and rightmost generated samples. For top to bottom: LSUN church outdoor 128 2 , LSUN bedroom 128 2 and CelebA 64 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Image inpainting on LSUN church outdoor 128 2 (left) and CelebA 64 2 (right). With each block, the top row are mask images while the bottom row are inpainted images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Left: Adjusted step size of HMC over time step. Center: Acceptance rate over time step. Right: Estimated log partition function over number of samples with different number of sampling steps per time step. The x axis is plotted in log scale.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Long-run chain samples from model-T1k with different total amount of HMC steps. From left to right: 1k steps, 10k steps and 100k steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 :Figure 10 :</head><label>910</label><figDesc>FIDs for different number of Langevin steps. (a) 1k steps, FID=24.78 (b) 10k steps, FID=23.89 (c) 100k steps, FID=25.08 Long run chain samples with different total number of NUTS steps. C.3 ADDITIONAL INTERPOLATION RESULTS Figures 11, 12 and 13 display more examples of interpolation between two generated samples on CelebA 64 2 , LSUN church outdoor 128 2 and LSUN bedroom 128 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 11 :</head><label>11</label><figDesc>Interpolation results between the leftmost and rightmost generated samples on CelebA 64 ? 64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Interpolation results between the leftmost and rightmost generated samples on LSUN church outdoor 128 ? 128.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Interpolation results between the leftmost and rightmost generated samples on LSUN bedroom 128 ? 128. C.4 ADDITIONAL IMAGE INPAINTING RESULTS Figures 14 and 15 show additional examples of image inpainting on CelebA 64 2 and LSUN church outdoor 128 2 . C.5 ADDITIONAL UNCURATED SAMPLES Figures 16, 17, 18, 19, 20 and 21 show uncurated samples from the learned models under T6 setting on CIFAR-10, CelebA 64 2 , LSUN church outdoor 128 2 , LSUN bedroom 128 2 , LSUN church outdoor 64 2 and LSUN bedroom 64 2 datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 :</head><label>14</label><figDesc>Image inpainting results on CelebA 64 ? 64. Top: masked images, bottom: inpainted images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 :</head><label>15</label><figDesc>Image inpainting results on LSUN church outdoor 128 ? 128. Top: masked images, bottom: inpainted images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 17 :</head><label>17</label><figDesc>Generated samples on CelebA 64 ? 64.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>FID and inception scores on CIFAR-10.</figDesc><table><row><cell>Model</cell><cell>FID? Inception?</cell></row><row><cell>GAN-based</cell><cell></cell></row><row><cell>WGAN-GP (Gulrajani et al., 2017)</cell><cell>36.4 7.86 ? .07</cell></row><row><cell>SNGAN</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 3 :</head><label>3</label><figDesc>FID scores on CelebA 64 2 .</figDesc><table><row><cell>Model</cell><cell>FID?</cell></row><row><cell cols="2">QA-GAN (Parimala &amp; Channappayya, 2019) 6.42</cell></row><row><cell>COCO-GAN (Lin et al., 2019)</cell><cell>4.0</cell></row><row><cell>NVAE (Vahdat &amp; Kautz, 2020)</cell><cell>14.74</cell></row><row><cell>NCSN (Song &amp; Ermon, 2019)</cell><cell>25.30</cell></row><row><cell>NCSN-v2</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="2">: Test bits per dimension on CIFAR-</cell></row><row><cell cols="2">10.  ? indicates that we estimate the bit per</cell></row><row><cell cols="2">dimension with the approximated log partition</cell></row><row><cell cols="2">function instead of analytically computing it.</cell></row><row><cell>See section 4.2.</cell><cell></cell></row><row><cell>Model</cell><cell>BPD?</cell></row><row><cell>DDPM (Ho et al., 2020)</cell><cell>3.70</cell></row><row><cell>Glow (Kingma &amp; Dhariwal, 2018)</cell><cell>3.35</cell></row><row><cell>Flow++ (Ho et al., 2019)</cell><cell>3.08</cell></row><row><cell cols="2">GPixelCNN (Van den Oord et al., 2016) 3.03</cell></row><row><cell cols="2">Sparse Transformer (Child et al., 2019) 2.80</cell></row><row><cell>DistAug (Jun et al., 2020)</cell><cell>2.56</cell></row><row><cell>Ours  ? (T1k)</cell><cell>3.18</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Model architectures of various solutions. N is a hyperparameter that we sweep over.</figDesc><table><row><cell>(a) Resolution 32 ? 32</cell><cell cols="2">(b) Resolution 64 ? 64</cell><cell>(c) Resolution 128 ? 128</cell></row><row><cell>3 ? 3 Conv2D, 128</cell><cell cols="2">3 ? 3 Conv2D, 128</cell><cell>3 ? 3 Conv2D, 128</cell></row><row><cell>N ResBlocks, 128</cell><cell cols="2">N ResBlocks, 128</cell><cell>N ResBlocks, 128</cell></row><row><cell>Downsample 2 ? 2</cell><cell cols="2">Downsample 2 ? 2</cell><cell>Downsample 2 ? 2</cell></row><row><cell>N ResBlocks, 256</cell><cell cols="2">N ResBlocks, 256</cell><cell>N ResBlocks, 256</cell></row><row><cell>Downsample 2 ? 2</cell><cell cols="2">Downsample 2 ? 2</cell><cell>Downsample 2 ? 2</cell></row><row><cell>N ResBlocks, 256</cell><cell cols="2">N ResBlocks, 256</cell><cell>N ResBlocks, 256</cell></row><row><cell>Downsample 2 ? 2</cell><cell cols="2">Downsample 2 ? 2</cell><cell>Downsample 2 ? 2</cell></row><row><cell>N ResBlocks, 256</cell><cell cols="2">N ResBlocks, 256</cell><cell>N ResBlocks, 256</cell></row><row><cell>ReLU, global sum</cell><cell cols="2">Downsample 2 ? 2</cell><cell>Downsample 2 ? 2</cell></row><row><cell>Dense 1</cell><cell cols="2">N ResBlocks, 512</cell><cell>N ResBlocks, 512</cell></row><row><cell></cell><cell cols="2">ReLU, global sum</cell><cell>Downsample 2 ? 2</cell></row><row><cell></cell><cell cols="2">Dense 1</cell><cell>N ResBlocks, 512</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ReLU, global sum</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Dense 1</cell></row><row><cell cols="2">(d) Time embedding (temb)</cell><cell cols="2">(e) ResBlock</cell></row><row><cell cols="2">sinusoidal embedding</cell><cell cols="2">leakyReLU, 3 ? 3 Conv2D</cell></row><row><cell cols="2">Dense, leakyReLU</cell><cell cols="2">+ Dense(leakyReLU(temb))</cell></row><row><cell>Dense</cell><cell></cell><cell cols="2">leakyReLU, 3 ? 3 Conv2D</cell></row><row><cell></cell><cell></cell><cell>+ input</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>Hyperparameters of various datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="4">N ?1 in Adam Batch size Training iterations</cell></row><row><cell>CIFAR-10</cell><cell>8</cell><cell>0.9</cell><cell>256</cell><cell>240k</cell></row><row><cell>CelebA</cell><cell>6</cell><cell>0.5</cell><cell>128</cell><cell>880k</cell></row><row><cell>LSUN church outdoor 64 2</cell><cell>2</cell><cell>0.9</cell><cell>128</cell><cell>960k</cell></row><row><cell>LSUN bedroom 64 2</cell><cell>2</cell><cell>0.9</cell><cell>128</cell><cell>760k</cell></row><row><cell>LSUN church outdoor 128 2</cell><cell>2</cell><cell>0.5</cell><cell>64</cell><cell>840k</cell></row><row><cell>LSUN bedroom 128 2</cell><cell>5</cell><cell>0.5</cell><cell>64</cell><cell>580k</cell></row><row><cell cols="3">C ADDITIONAL EXPERIMENTAL RESULTS</cell><cell></cell><cell></cell></row><row><cell>C.1 FID SCORES OVER ITERATIONS</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t (x t+1 ? y ? t )) + b? t ? .(17)Algorithm 1 summarizes the training procedure. After training, we initialize the MCMC sampling from Gaussian white noise, and the synthesized sample at each time step serves to initialize the MCMC that samples from the model of the previous time step. See algorithm 2. To show the efficacy of our method, Figures 3 and 2 display several 2D toy examples learned by diffusion recovery likelihood.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B EXPERIMENTAL DETAILS</head><p>Model architecture. Our network structure is based on Wide ResNet <ref type="bibr" target="#b54">(Zagoruyko &amp; Komodakis, 2016)</ref>. <ref type="table">Table 5</ref> lists the detailed network structures of various resolutions. The number of ResBlocks at every level N is a hyperparameter that we sweep over. The values of N for various datasets are listed in <ref type="table">Table 6</ref>. Each ResBlock consists of two Conv2D layers. For the second Conv2D layer, we use zero initialization for the weights, and add a trainable channel-wise scaling parameter to the output. We remove the weight normalization, and use leaky ReLU (slope = 0.2) as the activation function in ResBlocks. Spectral normalization <ref type="bibr" target="#b32">(Miyato et al., 2018)</ref> is used to regularize parameters in Conv2D layer, ResBlocks and Dense layer. For encoding time step t, we follow the scheme in <ref type="bibr" target="#b17">(Ho et al., 2020)</ref>. Specifically, the time step t is first transformed into sinusoidal embedding, and then two Dense layers is added. The time embedding is added after the first Conv2D layer of each ResBlock.</p><p>Training. We use Adam (Kingma &amp; Ba, 2014) optimizer for all the experiments. We find that for high resolution images, using a smaller ? 1 in Adam help stabilize training. We use learning rate 0.0001 for all the experiments. For the values of ? 1 , batch sizes and the number of training iterations for various datasets, see <ref type="table">Table 6</ref>.</p><p>Datasets. We use the following datasets in our experiments: CIFAR-10 <ref type="bibr" target="#b25">(Krizhevsky et al., 2009)</ref>, CelebA <ref type="bibr" target="#b31">(Liu et al., 2018)</ref> and LSUN <ref type="bibr" target="#b53">(Yu et al., 2015)</ref>. CIFAR-10 is of resolution 32 ? 32, and contains 50, 000 training images and 10, 000 test images. CelebA contains 202,599 face images, of which 162,770 are training images and 19,962 are test images. For processing, we first clip each image to 178 ? 178 and then resize it to 64 ? 64. For LSUN, we use church outdoor and bedroom categories, which contains 126,227 and 3,033,042 training images respectively. Both categories contain 300 test images. For processing, we first crop each image to a square image of the smaller size among the height and weight, and then we resize it to 64 ? 64 or 128 ? 128. For resizing, we set antialias to True. We apply horizontal random flip as data augmentation for all datasets during training.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generalized denoising auto-encoders as generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="899" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Your gan is secretly an energy-based model and you should use discriminator driven latent sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruixiang</forename><surname>Tong Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Paull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.06060</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">On tracking the partition function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2501" to="2509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Implicit generation and generalization in energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yilun</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Mordatch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.08689</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">A connection between generative adversarial networks, inverse reinforcement learning, and energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Christiano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03852</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning generative convnets via multi-grid modeling and sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junpei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="9155" to="9164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flow contrastive estimation of energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7518" to="7528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Riemann manifold langevin and hamiltonian monte carlo methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Girolami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Calderhead</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="214" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Variational walkback: Learning a transition operator as a stochastic recurrent net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh Goyal Alias Parth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4392" to="4402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Your classifier is secretly an energy based model and you should treat it like one</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.03263</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cutting out the middle-man: Training and evaluating energy-based models without sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Grathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuan-Chieh</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05616</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Measuring the reliability of mcmc inference with bidirectional monte carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siddharth</forename><surname>Roger B Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel M</forename><surname>Ancha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2451" to="2459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint training of variational auto-encoder and latent energy-based model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7978" to="7987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Gans trained by a two time-scale update rule converge to a local nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6626" to="6637" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Flow++: Improving flowbased generative models with variational dequantization and architecture design</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aravind</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.00275</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11239</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">Denoising diffusion probabilistic models. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The no-u-turn sampler: adaptively setting path lengths in hamiltonian monte carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1593" to="1623" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Introspective classification with convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Lazarow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="823" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distribution augmentation for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning and Systems 2020</title>
		<meeting>Machine Learning and Systems 2020</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="10563" to="10576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Training generative adversarial networks with limited data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Janne</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.06676</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Deep directed generative models with energy-based probability estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesup</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03439</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="10215" to="10224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>Citeseer</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Maximum entropy generators for energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rithesh</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.08508</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Introspective neural networks for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Lazarow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2774" to="2783" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A tutorial on energy-based learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Predicting structured data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">0</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Wasserstein introspective neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwonjoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3702" to="3711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coco-gan: generation by parts via conditional coordinating</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chieh</forename><surname>Hubert Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Che</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Sheng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwann-Tzong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4512" to="4521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Large-scale celebfaces attributes (celeba) dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018-08" />
			<biblScope unit="volume">15</biblScope>
		</imprint>
	</monogr>
	<note>Retrieved</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Annealed importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and computing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="125" to="139" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning deep energy models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1105" to="1112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">On the anatomy of mcmcbased maximum likelihood learning of energy-based models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitch</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12370</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">On learning non-convergent shortrun mcmc toward energy-based model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Nijkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitch</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.09770</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Quality aware generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kancharla Parimala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Channappayya</forename><surname>Sumohana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2948" to="2958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Unbiased contrastive divergence algorithm for training energy-based latent variable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixuan</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingsong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Telescoping density-ratio estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Rhodes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">U</forename><surname>Gutmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Neural empirical bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Saremi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saeed</forename><surname>Saremi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Mehrjou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?lkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aapo</forename><surname>Hyv?rinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.08306</idno>
		<title level="m">Deep energy estimator networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Deep unsupervised learning using nonequilibrium thermodynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niru</forename><surname>Maheswaranathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Ganguli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.03585</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Generative modeling by estimating gradients of the data distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11918" to="11930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Improved techniques for training score-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09011</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Nvae: A deep hierarchical variational autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arash</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A connection between score matching and denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1661" to="1674" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruiqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09408</idno>
		<title level="m">Cooperative training of descriptor and generator networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A theory of generative convnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingnian</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2635" to="2644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Cooperative training of fast thinking initializer and slow thinking solver for multi-modal conditional learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwen</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolin</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song-Chun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying Nian</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1902.02812</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinda</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lsun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<title level="m">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
