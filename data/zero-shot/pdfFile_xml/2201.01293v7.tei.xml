<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A TRANSFORMER-BASED SIAMESE NETWORK FOR CHANGE DETECTION</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wele</forename><surname>Gedara</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaminda</forename><surname>Bandara</surname></persName>
							<email>wbandar1@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vishal</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
							<email>vpatel36@jhu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<region>Maryland</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A TRANSFORMER-BASED SIAMESE NETWORK FOR CHANGE DETECTION</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T12:41+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Change detection</term>
					<term>transformer Siamese network</term>
					<term>attention mechanism</term>
					<term>multilayer perceptron</term>
					<term>remote sensing</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper presents a transformer-based Siamese network architecture (abbreviated by ChangeFormer) for Change Detection (CD) from a pair of co-registered remote sensing images. Different from recent CD frameworks, which are based on fully convolutional networks (ConvNets), the proposed method unifies hierarchically structured transformer encoder with Multi-Layer Perception (MLP) decoder in a Siamese network architecture to efficiently render multi-scale long-range details required for accurate CD. Experiments on two CD datasets show that the proposed end-to-end trainable ChangeFormer architecture achieves better CD performance than previous counterparts. Our code and pre-trained models are available at github.com/wgcban/ChangeFormer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Change Detection (CD) aims to detect relevant changes from a pair of co-registered images acquired at distinct times <ref type="bibr" target="#b0">[1]</ref>. The definition of change may usually vary depending on the application. The changes in man-made facilities (e.g., buildings, vehicles, etc.), vegetation changes, and environmental changes (e.g., polar ice cap melting, deforestation, damages caused by disasters) are usually regarded as relevant changes. A better CD model is the one that can recognize these relevant changes while avoiding complex irrelevant changes caused by seasonal variations, building shadows, atmospheric variations, and changes in illumination conditions.</p><p>The existing state-of-the-art (SOTA) CD methods are mainly based on deep convolutional networks (ConvNets) due to their ability to extract powerful discriminative features. Since it is essential to capture long-range contextual information within the spatial and temporal scope to identify relevant changes in multi-temporal images, the latest CD studies have been focused on increasing the receptive field of the CD model. As a result, CD models with stacked convolution layers, dilated convolutions, and attention mechanisms <ref type="bibr" target="#b1">[2]</ref> (channel and spatial attention) have been proposed <ref type="bibr" target="#b2">[3]</ref>. Even though the attention-based methods are effective in capturing global details, they struggle to relate long-range details in space-time because they use attention to re-weight the bi-temporal features obtained through ConvNets in the channel and spatial dimension.</p><p>The recent success of Transformers (i.e., non-local selfattention) in Natural Language Processing (NLP) has led researchers in applying transformers in various computer vision tasks. Following the transformer design in NLP, different architectures have been proposed for various computer vision tasks, including image classification and image segmentation such as Vision Transformer (ViT), SEgmentation TRansformer (SETR), Vision Transformer using Shifted Windows (Swin), Twins <ref type="bibr" target="#b3">[4]</ref> and SegFormer <ref type="bibr" target="#b4">[5]</ref>. These Transformer networks have comparatively larger effective receptive field (ERF) than deep ConvNets -providing much stronger context modeling ability between any pair of pixels in images than ConvNets.</p><p>Although Transformer networks have a larger receptive field and stronger context shaping ability, very few works have been done on transformers for CD. In a more recent work <ref type="bibr" target="#b5">[6]</ref>, a transformer architecture is applied in conjunction with a ConvNet encoder (ResNet18) to enhance the feature representation while keeping the overall ConvNet-based feature extraction process in place. In this paper, we show that this dependency on ConvNets is not necessary, and a hierarchical transformer encoder with a lightweight MLP decoder can work very well for CD tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">METHOD</head><p>The proposed ChangeFormer network consists of three main modules as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>: a hierarchical transformer encoder in a Siamese network to extract coarse and fine features of bi-temporal image, four feature difference modules to compute feature differences at multiple scales, and a lightweight MLP decoder to fuse these multi-level feature differences and predict the CD mask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Hierarchical Transformer Encoder</head><p>Given an input bi-temporal image, the hierarchical transformer encoder generates ConvNet-like multi-level features  with high-resolution coarse features and low-resolution finegrained features required for the CD. Concretely, given a pre-change or post-change images of resolution H ? W ? 3, the transformer encoder outputs feature maps F i with a reso-</p><formula xml:id="formula_0">lution H 2 i+1 ? W 2 i+1 ?C i , where i = {1, 2, 3, 4}</formula><p>and C i+1 &gt; C i which will be further processed through the difference modules followed by MLP decoder to obtain the change map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Transformer Block</head><p>The main building block of the transformer encoder is selfattention module. In the original work <ref type="bibr" target="#b6">[7]</ref>, self-attention is estimated as:</p><formula xml:id="formula_1">Attention(Q, K, V ) = Softmax QK T ? d head V,<label>(1)</label></formula><p>where Q, K, and V denote Query, Key, and Value, respectively, and have the same dimensions of HW ? C. However, the computational complexity of eqn. <ref type="formula" target="#formula_1">(1)</ref> is O((HW ) 2 ) which prohibits its application on high-resolution images. To reduce the computational complexity of eqn. (1), we adopt the Sequence Reduction process introduced in [8] which utilizes reduction ratio R to reduce the length of the sequence HW as follows:</p><formula xml:id="formula_2">S = Reshape HW R , C ? R S,<label>(2)</label></formula><formula xml:id="formula_3">S = Linear (C ? R, C)?,<label>(3)</label></formula><p>where S denotes the sequence to be reduced i.e., Q, K, and V, Reshape(h, w) denotes tensor reshaping operation to the one with shape of (h, w), and Linear(C in , C out ) denotes a linear-layer with C in input channels and C out output channels. This results in a new set of Q, K, and V of size HW R , C , hence reduces the computational complexity of eqn. <ref type="formula" target="#formula_1">(1)</ref> to O((HW ) 2 /R).</p><p>To provide positional information for transformers, we utilize two MLP layers along with a 3 ? 3 depth-wise convolutions as follows:</p><formula xml:id="formula_4">F out = MLP(GELU(Conv2D 3?3 (MLP(F in )))) + F in , (4)</formula><p>where F in are the features from self-attention, and GELU denotes Gaussian Error Linear Unit activation. Our positional encoding scheme differs from the fixed positional encoding utilized in previous transformer networks like ViT <ref type="bibr" target="#b8">[9]</ref> which allows our ChangeFormer to take test images that are different in resolution from the ones used during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Downsampling Block</head><p>Given an input patch F i from the i-th transformer layer of resolution</p><formula xml:id="formula_5">H 2 i+1 ? W 2 i+1 ? C i , downsampling layer shrink it to obtain F i+1 of resolution H 2 i+2 ? W 2 i+2 ? C i+1</formula><p>which will be the input to the (i + 1)-th Transformer layer. To achieve this, we utilize a 3 ? 3 Conv2D layer with kernel size K = 7, stride S = 4, and padding P = 3 for the initial downsampling, and K = 3, S = 2, and P = 1 for the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">Difference Module</head><p>We utilize four Difference Modules to compute the difference of multi-level features of pre-change and post-change images from the hierarchical transformer encoder as shown in <ref type="figure" target="#fig_0">Fig. 1</ref>. More precisely, our Difference Module consists of Conv2D, ReLU, BatchNorm2d (BN) as follows:</p><formula xml:id="formula_6">F i diff = BN(ReLU(Conv2D 3?3 (Cat(F i pre , F i post )))),<label>(5)</label></formula><p>where F i pre and F i post denote the feature maps of pre-change and post-change images from the i-th hierarchical layer, and Cat denotes the tensor concatenation. Instead of computing the absolute difference of F i pre and F i post as in <ref type="bibr" target="#b5">[6]</ref>, the proposed difference module learn the optimal distance metric at each scale during training -resulting in better CD performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">MLP Decoder</head><p>We utilize a simple decoder with MLP layers that aggregates the multi-level feature difference maps to predict the change map. The proposed MLP decoder consists of three main steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">MLP &amp; Upsampling</head><p>We first process each multi-scale feature difference map through an MLP layer to unify the channel dimension and then upsample each one to the size of H/4 ? W/4 as follows: <ref type="formula">(7)</ref> where C ebd denotes the embedding dimension.</p><formula xml:id="formula_7">F i diff = Linear(C i , C ebd )(F i diff )?i,<label>(6)</label></formula><formula xml:id="formula_8">F i diff = Upsample((H/4, W/4), "bilinear")(F i diff ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Concatenation &amp; Fusion</head><p>The upsampled feature difference maps are then concatenated and fused through an MLP layer as follows:</p><formula xml:id="formula_9">F = Linear(4C ebd , C ebd )(Cat(F 1 diff ,F 2 diff ,F 3 diff ,F 4 diff )).<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Upsampling &amp; Classification.</head><p>We upsample the fused feature map F to the size of H ? W by utilizing a 2D transposed convolution layer with S = 4 and K = 3. Finally, the upsampled fused feature map is processed through another MLP layer to predict the change mask CM with a resolution of H ? W ? N cls , where N cls (=2) is the number of classes i.e., change and no-change. This process can be formulated as follows:</p><formula xml:id="formula_10">F = ConvTranspose2D(S = 4, K = 3)(F), (9) CM = Linear(C ebd , N cls )(F).<label>(10)</label></formula><p>3. EXPERIMENTAL SETUP</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Datasets</head><p>We use two publically available CD datasets for our experiments, namely LEVIR-CD <ref type="bibr" target="#b9">[10]</ref> and DSIFN-CD <ref type="bibr" target="#b10">[11]</ref>. The LEVIR-CD is a building CD dataset that contains RS image pairs of resolution 1024 ? 1024. From these images, </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Implementation Details</head><p>We implemented our model in PyTorch and trained using an NVIDIA Quadro RTX 8000 GPU. We randomly initialize the network. During training, we applied data augmentation through random flip, random re-scale (0.8-1.2), random crop, Gaussian blur, and random color jittering. We trained the models using the Cross-Entropy (CE) Loss and AdamW optimizer with weight decay equal to 0.01 and beta values equal to (0.9, 0.999). The learning rate is initially set to 0.0001 and linearly decays to 0 until trained for 200 epochs. We use a batch size of 16 to train the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Performance Metrics</head><p>To compare the performance of our model with SOTA methods, we report F1 and Intersection over Union (IoU) scores with regard to the change-class as the primary quantitative indices. Additionally, we report precision and recall of the change category and overall accuracy (OA).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">RESULTS AND DISCUSSION</head><p>In this section, we compare the CD performance of our ChangeFormer with existing SOTA methods:</p><p>? FC-EF <ref type="bibr" target="#b11">[12]</ref>: concatenates bi-temporal images and processes them through a ConvNet to detect changes. ? FC-Siam-Di <ref type="bibr" target="#b11">[12]</ref>: is a feature-difference method, which extracts multi-level features of bi-temporal images from a Siamese ConvNet, and their difference is used to detect changes. ? FC-Siam-Conc <ref type="bibr" target="#b11">[12]</ref>: is a feature-concatenation method, which extracts multi-level features of bi-temporal images from a Siamese ConvNet, and feature concatenation is used to detect changes. ? DTCDSCN <ref type="bibr" target="#b12">[13]</ref>: is an attention-based method, which utilizes a dual attention module (DAM) to exploit the inter-dependencies between channels and spatial positions of ConvNet features to detect changes. ? STANet <ref type="bibr" target="#b13">[14]</ref>: is an another Siamese-based spatialtemporal attention network for CD. ? IFNet <ref type="bibr" target="#b14">[15]</ref>: is a multi-scale feature concatenation method, which fuses multi-level deep features of bitemporal images with image difference features by  means of attention modules for change map reconstruction. ? SNUNet <ref type="bibr" target="#b15">[16]</ref>: is a multi-level feature concatenation method, in which a densely connected (NestedUNet) Siamese network is used for change detection. ? BIT <ref type="bibr" target="#b5">[6]</ref>: is a transformer-based method, which uses a transformer encoder-decoder network to enhance the context-information of ConvNet features via semantic tokens followed by feature differencing to obtain the change map. <ref type="table" target="#tab_2">Table 1</ref> presents the results of different CD methods on the test-sets of LEVIR-CD <ref type="bibr" target="#b9">[10]</ref> and DSIFN-CD <ref type="bibr" target="#b10">[11]</ref>. As can be seen from the table, the proposed ChangeFormer network achieves better CD performance in terms of F1, IoU, and OA metrics. In particular, our ChangeFormer improves previous SOTA in F1/IoU/OA by 1.2/2.2/0.1% and 20.0/44.3/6.4% for LEVIR-CD and DSIFN-CD, respectively. In addition, <ref type="figure" target="#fig_1">Fig. 2</ref> compares the visual quality of different SOTA methods on test images from LEVIR-CD and DSIFN-CD. As highlighted in red, our ChangeFormer captures much finer details compared to the other SOTA methods. These quantitative and qualitative comparisons show the superiority of our proposed CD method over the existing SOTA methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>In this paper, we proposed a transformer-based Siamese network for CD. By utilizing a hierarchical transformer encoder in a Siamese architecture with a simple MLP decoder, our method outperforms several other recent CD methods that employ very large ConvNets like ResNet18 and U-Net as the backbone. We also show better performance in terms of IoU, F1 score, and overall accuracy than recent ConvNet-based (FC-EF, FC-Siam-DI, and FC-Siam-Conc), attention-based (DTCDSCN, STANet, and IFNet), and ConvNet+Transformer-based (BIT) methods. Hence, this study shows that it is unnecessary to depend on deep-ConvNets, and a hierarchical transformer in a Siamese network with a lightweight decoder can work very well for CD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGMENT</head><p>This work was supported by NSF CAREER award 2045489.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>The proposed ChangeFormer network for CD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Qualitative results of different CD methods on LEVIR-CD<ref type="bibr" target="#b9">[10]</ref> and DSIFN-CD<ref type="bibr" target="#b10">[11]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>we crop non-overlapping patches of size 256 ? 256 and randomly split them into three parts to make train/val/test sets of samples 7120/1024/2048. The DSIFN dataset is an general CD dataset that contains the changes in different landcover objects. For experiments, we create non-overlapping patches of size 256 ? 256 from the 512 ? 512 images while utilizing the authors' default train/val/test sets. This results in 14400/1360/192 samples for training/val/test, respectively, for the DSIFN dataset.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>The average quantitative results of different CD methods on LEVIR-CD<ref type="bibr" target="#b9">[10]</ref> and DSIFN-CD<ref type="bibr" target="#b10">[11]</ref>.* All values are reported in percentage (%). Color convention: best, 2nd-best, and 3rd-best.</figDesc><table><row><cell>Method</cell><cell></cell><cell cols="3">LEVIR-CD [10]</cell><cell></cell><cell></cell><cell></cell><cell>DSIFN-CD [11]</cell></row><row><cell></cell><cell cols="3">Precision Recall</cell><cell>F1</cell><cell>IoU</cell><cell>OA</cell><cell cols="2">Precision Recall</cell><cell>F1</cell><cell>IoU</cell><cell>OA</cell></row><row><cell>FC-EF [12]</cell><cell>86.91</cell><cell cols="5">80.17 83.40 71.53 98.39</cell><cell>72.61</cell><cell>52.73 61.09 43.98 88.59</cell></row><row><cell>FC-Siam-Di [12]</cell><cell>89.53</cell><cell cols="5">83.31 86.31 75.92 98.67</cell><cell>59.67</cell><cell>65.71 62.54 45.50 86.63</cell></row><row><cell>FC-Siam-Conc [12]</cell><cell>91.99</cell><cell cols="5">76.77 83.69 71.96 98.49</cell><cell>66.45</cell><cell>54.21 59.71 42.56 87.57</cell></row><row><cell>DTCDSCN [13]</cell><cell>88.53</cell><cell cols="5">86.83 87.67 78.05 98.77</cell><cell>53.87</cell><cell>77.99 63.72 46.76 84.91</cell></row><row><cell>STANet [14]</cell><cell>83.81</cell><cell cols="5">91.00 87.26 77.40 98.66</cell><cell>67.71</cell><cell>61.68 64.56 47.66 88.49</cell></row><row><cell>IFNet [15]</cell><cell>94.02</cell><cell cols="5">82.93 88.13 78.77 98.87</cell><cell>67.86</cell><cell>53.94 60.10 42.96 87.83</cell></row><row><cell>SNUNet [16]</cell><cell>89.18</cell><cell cols="5">87.17 88.16 78.83 98.82</cell><cell>60.60</cell><cell>72.89 66.18 49.45 87.34</cell></row><row><cell>BIT [6]</cell><cell>89.24</cell><cell cols="5">89.37 89.31 80.68 98.92</cell><cell>68.36</cell><cell>70.18 69.26 52.97 89.41</cell></row><row><cell>ChangeFormer (ours)</cell><cell>92.05</cell><cell cols="5">88.80 90.40 82.48 99.04</cell><cell>88.48</cell><cell>84.94 86.67 76.48 95.56</cell></row><row><cell cols="3">*FC-EF [10] Pre-Change Post-Change Img. Img.</cell><cell cols="2">FC-Siam-Di [10]</cell><cell cols="2">FC-Siam-Conc [10]</cell><cell>DTCDSCN [11]</cell><cell>BIT [4]</cell><cell>ChangeFormer (ours)</cell><cell>GT</cell></row><row><cell>LEVIR-CD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Text Text Text Text Text Text Text</cell></row><row><cell>DSIFN-CD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Revisiting consistency regularization for semisupervised change detection in remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaminda</forename><surname>Wele Gedara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bandara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vishal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.08454</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Spin road mapper: Extracting roads from aerial images via spatial and interaction space graph reasoning for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wele</forename><surname>Gedara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaminda</forename><surname>Bandara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeya Maria Jose</forename><surname>Valanarasu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.07701</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A deeply supervised attention metric-based network and an open aerial image dataset for remote sensing change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoping</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangpei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muzammal</forename><surname>Naseer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Munawar</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fahad</forename><surname>Syed Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mubarak</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01169</idno>
		<title level="m">Transformers in vision: A survey</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Segformer: Simple and efficient design for semantic segmentation with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.15203</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Remote sensing image change detection with transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zipeng</forename><surname>Hao Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwei</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Pyramid vision transformer: A versatile backbone for dense prediction without convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Enze</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deng-Ping</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaitao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ding</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.12122</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A spatial-temporal attention-based method and a new dataset for remote sensing image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwei</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1662</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A deeply supervised image fusion network for change detection in high resolution bi-temporal remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deodato</forename><surname>Tapete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangcun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangchao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="183" to="200" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fully convolutional siamese networks for change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bertr</forename><forename type="middle">Le</forename><surname>Rodrigo Caye Daudt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Saux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boulch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 25th IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4063" to="4067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Building change detection for remote sensing images using a dual-task constrained deep siamese convolutional network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zongqian</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaomeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xue</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="811" to="815" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A spatial-temporal attention-based method and a new dataset for remote sensing image change detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwei</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1662</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A deeply supervised image fusion network for change detection in high resolution bi-temporal remote sensing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deodato</forename><surname>Tapete</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangcun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boyi</forename><surname>Shangguan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangchao</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ISPRS Journal of Photogrammetry and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="183" to="200" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Snunet-cd: A densely connected siamese network for change detection of vhr images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinyuan</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
