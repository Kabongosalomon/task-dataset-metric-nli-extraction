<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SCAM! Transferring humans between images with Semantic Cross Attention Modulation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Dufour</surname></persName>
							<email>nicolas.dufour@enpc.fr</email>
							<affiliation key="aff0">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="laboratory">LIGM</orgName>
								<orgName type="institution" key="instit1">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>Marne-la-Vall?e</addrLine>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">LIX, CNRS, Ecole Polytechnique</orgName>
								<address>
									<settlement>Paris</settlement>
									<region>IP</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SCAM! Transferring humans between images with Semantic Cross Attention Modulation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Project page: https://imagine.enpc.fr/~dufourn/scam</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Semantic Generation</term>
					<term>Semantic Editing</term>
					<term>Generative Adver- sarial Networks</term>
					<term>Subject Transfer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>2[0000?0002?1903?5110] , David Picard 1[0000?0002?6296?4222] , and Vicky Kalogeiton 2[0000?0002?7368?6993]</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract.</head><p>A large body of recent work targets semantically conditioned image generation. Most such methods focus on the narrower task of pose transfer and ignore the more challenging task of subject transfer that consists in not only transferring the pose but also the appearance and background. In this work, we introduce SCAM (Semantic Cross Attention Modulation), a system that encodes rich and diverse information in each semantic region of the image (including foreground and background), thus achieving precise generation with emphasis on fine details. This is enabled by the Semantic Attention Transformer Encoder that extracts multiple latent vectors for each semantic region, and the corresponding generator that exploits these multiple latents by using semantic cross attention modulation. It is trained only using a reconstruction setup, while subject transfer is performed at test time. Our analysis shows that our proposed architecture is successful at encoding the diversity of appearance in each semantic region. Extensive experiments on the iDesigner, CelebAMask-HD and ADE20K datasets show that SCAM outperforms competing approaches; moreover, it sets the new state of the art on subject transfer.</p><p>Keywords: Semantic Generation, Semantic Editing, Generative Adversarial Networks, Subject Transfer</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Being able to perform subject transfer between two images is a key challenge for many applications, from post-processing in game or art industries to software addressing the needs of the public. For instance, in film industries, one could replace a stunt performer by the main actors, thus alleviating the need of finding a look-alike performer; hence, increasing the filmmakers freedom. Similarly, it could enable finishing a film when an actor is indisposed.</p><p>Given a source and a target subject, the idea of subject transfer is for the source subject to replace the target subject in the target image seamlessly. The target image should keep the same background, the same interactions between arXiv:2210.04883v1 <ref type="bibr">[</ref>  <ref type="figure">Fig. 1</ref>: Subject transfer with the proposed SCAM. We first encode the desired subject with the encoder E and get the subject latent codes. Then, we encode the background and the semantic mask for the pose and background reference. Finally, the generator G synthesizes an image, where the subject is transferred with the desired background and pose. Pictures taken from the Internet 3 . subject and objects, and the same spatial configuration, to account for possible occlusions. <ref type="figure">Figure 1</ref> illustrates this. Note, in contrast to faces, buildings, or landscapes, human bodies are malleable with high morphological diversity, thus casting the task hard to model.</p><p>Most methods focus either on pose transfer [51, <ref type="bibr" target="#b1">2,</ref><ref type="bibr">45]</ref>, where the pose changes, or on style transfer <ref type="bibr">[56,</ref><ref type="bibr">32]</ref>, where the pose remains fixed but the subject's styling changes. These are limited and cannot be used out of the box for our task, as they are: <ref type="bibr" target="#b0">(1)</ref> restrictive; they only work on uniform backgrounds, failing in complex ones <ref type="bibr">(PISE [51]</ref>, <ref type="bibr">SEAN [56]</ref>, <ref type="bibr">[45]</ref>), and (2) expensive; they require hard training <ref type="bibr">[45]</ref> or training one model per human (Everybody Dance Now <ref type="bibr" target="#b1">[2]</ref>). Instead, subject transfer changes both the pose and the style/identity of the subject. Thus, a successful system is decoupled in both pose and style transfer and performs both tasks simultaneously.</p><p>Semantic editing is a related task, consisting in controlling the output of a generative network by a segmentation mask. Indeed, subject transfer can be performed with semantic editing by using the mask of the target subject with the style of the source. However, modern methods cannot handle complex layout and rich structure (like full-bodies) with in the wild backgrounds. For instance, SPADE [32] fails to control each region style independently, while <ref type="bibr">SEAN [56]</ref> fails to handle complex, detailed scenes, such as multiple background objects.</p><p>To this end, we propose the Semantic Cross Attention Modulation system (SCAM), a semantic editing model that accounts for all aforementioned challenges relevant to subject transfer. SCAM captures fine image details inside the semantic region by having multiple latents per semantic regions. This enables capturing unsupervised semantic information inside the semantic labels, which allows for better handling coarse semantic labels, such as background. Our model can generate more complex backgrounds than previous methods and outperforms <ref type="bibr">SEAN [56]</ref> both on the subject transfer and semantic reconstruction tasks.</p><p>We propose three architectural contributions: First, we propose the Semantic Cross Attention (SCA) that performs attention between a set of latents (each linked to a semantic region) and an image feature map. SCA constrains the attention, such as the latents only attend the regions on the image feature map that correspond to the relevant semantic label. Secondly, we introduce the SAT operation and encoder (Semantic Attention Transformer) that relies on cross attention to decide which information to gather in the image and for which latent, thus allowing for richer information to be encoded. Third, we propose the SCAM-Generator (after which SCAM is named) that modulates the feature maps using the SCAM-Operation, which allows every pixel to attend to the semantically meaningful latents. Note, the whole architecture is trained using a reconstruction setup only, and subject transfer is performed at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Image to Image synthesis with GANs. GANs <ref type="bibr" target="#b9">[10]</ref> generate images by processing a random vector sampled from a predefined distribution with a dedicated network <ref type="bibr" target="#b2">[3,</ref><ref type="bibr">33,</ref><ref type="bibr" target="#b17">18]</ref>. A major improvement is StyleGAN <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> that allows to modulate the feature map at each resolution according to a given style vector. Typically, unconditional GANs allow for minimal control over the generator's output. For more flexibility, Pix2Pix <ref type="bibr" target="#b13">[14,</ref><ref type="bibr">46]</ref> trains a generator coupled to an encoder, allowing for output control with multiple modalities (sketches, keypoints). One of its drawbacks is the need for data pairs, which can be hard to collect (drawings). To tackle this, CycleGAN [55] uses unpaired data by enforcing cycle consistency across domains. However, acquiring paired data is feasible when leveraging external models, such as semantic masks from segmentation models. In our case, we do not have access to ground-truth images where the subject has been transferred as it would require both the subject and the reference to have exactly the same pose and occlusion. We circumvent this by training on a reconstruction proxy task and performing subject transfer at test time. Semantic Image Generation. Even if Pix2Pix <ref type="bibr" target="#b13">[14,</ref><ref type="bibr">46]</ref> manage to control the output image with a segmentation mask, it suffers from a semantic information washing-up. SPADE <ref type="bibr">[32]</ref> propose to fix this problem by introducing layer-wise semantic conditioning. CLADE[39] propose a more efficient version of SPADE to reduce runtime complexity. Other approaches such as <ref type="bibr">[47,</ref><ref type="bibr">27,</ref><ref type="bibr">25,</ref><ref type="bibr">37,</ref><ref type="bibr">41,</ref><ref type="bibr">39,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11]</ref> propose improvements over spade. However, these approaches work well generating images from semantic information, but they do not focus on the case Although this allows better control of the output and richer representation, it still has two problems: (1) it is limited when handling coarse labels with diverse objects, (2) it creates a single vector per semantic region. In our approach, SCAM, we solve this by introducing the SAT-Encoder which can extract rich representation from images and is able to output multiple and diverse latents per semantic region. In turn, the SEAN-Generator modulates the output by both the semantic mask and the extracted style. This, however, modulates each pixel of a semantic region with the same style vector. Instead, our SCAM-Generator, uses attention to leverage different tokens to interact with, leading to different modulations per pixel, and hence enabling the emergence of unsupervised semantic structure in the semantic regions. Other approaches propose to use diffusion process approaches [31] to perform this editing process. However, these diffusion approaches are very expensive to sample from. Attention in Computer Vision. Despite their remarkable success <ref type="bibr" target="#b3">[4,</ref><ref type="bibr">34,</ref><ref type="bibr">35,</ref><ref type="bibr" target="#b0">1]</ref>, transformers suffer from a quadratic complexity problem, which makes it hard to use. To tackle this, most vision methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr">42,</ref><ref type="bibr">28]</ref> subdivide images into patches, resulting in losing information. Instead, the recent Perceiver <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b14">15]</ref> tackles this complexity issue by replacing self attention by cross attention. The image pixels are attended by a significatively smaller set of learned tokens.</p><p>Attention in GANS. It [50, <ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr">53,</ref><ref type="bibr">22</ref>,49] has shown great progress over the past years. GANsformer <ref type="bibr" target="#b12">[13]</ref> leverages cross attention to exploit multiple style codes between style vectors and feature maps. These architectures use attention for unconditional generation; however, they do not focus on subject transfer. Instead, our proposed SCA improves upon GANsformer's duplex attention for semantically constrained generation by assigning latents to semantic regions. Pose Transfer. Using keypoints for pose transfer [30,57,40,23] typically results in coarse representation of bodies. To tackle this, some methods use semantic masks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr">52,</ref><ref type="bibr">51]</ref>. These, however, focus only on pose transfer, which does not alter the background. Instead, we aim at subject transfer, where preserving the background is crucial. Most methods are limited to simple backgrounds. <ref type="bibr" target="#b1">[2]</ref> overfit a GAN to a video and regenerate the background; hence, it cannot be used for dynamic scenes.</p><p>[45] address this by adapting the weights of the generator, but this ties the subject to the background, not allowing for subject transfer.</p><p>Here, we focus on subject transfer that changes both pose and background.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>Our goal is to perform semantic editing with a focus on subject transfer. We propose the SCAM method (Semantic Cross Attention Modulation, <ref type="figure">Figure 2</ref>). It relies on SCA (Semantic Cross Attention), i.e. a novel mechanism that masks the attention according to segmentation masks, thus encoding semantically meaningful latent variables (Section 3.1). SCAM consists of: (a) SAT-Encoder (Semantic Attention Transformer) that relies on cross attention to decide which information to gather in the image and for which latent (Section 3.2); and (b) SCAM-Generator (Semantic Cross Attention Modulation) that captures rich semantic information in an unsupervised way (Section 3.3).</p><p>Notation. Let X ? R n?C be the feature map with n the number of pixels, and C the number of channels. Let Z ? R m?d be a set of m latents of dimension d and s the number of semantic labels. Each semantic label is attributed k latents, such that m = k ? s. Each semantic label mask is assigned k copies in S?{0; 1} n?m . ?(.) is the softmax operation.</p><p>Motivation. Since many regions of the image have visually diverse content (e.g., the background), we propose to encode this varied information in several complementary latents. Motivated by the findings of Gansformers <ref type="bibr" target="#b12">[13]</ref>, we use attention to introduce both a constraint on which part of the image a latent can get information from and a competing mechanism between latents attending the same region so as to specialize them. Reciprocally, using duplex attention, we introduce the same strategy by limiting the latents that the feature map can attend to and introduce a competition between parts of a semantic region that can attend the same latent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Semantic Cross Attention (SCA)</head><p>Definition. The goal of SCA is two-fold depending on what is the query and what is the key. Either it allows to give the feature map information from a semantically restricted set of latents or, respectively, it allows a set of latents to retrieve information in a semantically restricted region of the feature map. It is defined as:</p><formula xml:id="formula_0">SCA(I1, I2, I3) = ? QK T ? I3 + ? (1 ? I3) ? din V ,<label>(1)</label></formula><p>where I 1 , I 2 , I 3 the inputs, with I 1 attending I 2 , and I 3 the mask that forces tokens from I 1 to attend only specific tokens from I 2 4 , Q=W Q I 1 , K=W K I 2 and V =W V I 2 the queries, keys and values, and d in the internal attention dimension.</p><p>We use three types of SCA. (a) SCA with pixels X attending latents Z: SCA(X, Z, S), where W Q ?R n?din and W K , W V ?R m?din . The idea is to force the pixels from a semantic region to attend latents that are associated with the same label. (b) SCA with latents Z attending pixels X:</p><formula xml:id="formula_1">SCA(Z, X, S), where W Q ?R m?din , W K , W V ?R n?din .</formula><p>The idea is to semantically mask attention values to enforce latents to attend semantically corresponding pixels. (c) SCA with latents Z attending themselves:</p><formula xml:id="formula_2">SCA(Z, Z, M ), where W Q , W K , W V ?R n?din . We denote M ? N m?m this mask, with M latents (i, j)=1</formula><p>if the semantic label of latent i is the same as the one of latent j; 0 otherwise. The idea is to let the latents only attend latents that share the same semantic label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SAT-Encoder</head><p>Following <ref type="bibr" target="#b15">[16]</ref>, our SAT-Encoder relies on cross attention. It consists of L E consecutive layers of SAT-Blocks, where the input of the i+1-th one is the output of the i-th one <ref type="figure">(Figure 2</ref> (left)). Given a set of learned queries Z 0 (i.e., parameters updated with gradient descent using back-propagation), it outputs latents Z L E that have encoded the input image. This allows to create multiple latents per semantic regions resulting in specialized latents for different part of each semantic region of the image. The encoder is also flexible enough to easily assign a different number of latent for each semantic region, allowing to optimize the representation power given to each semantic region. At each layer, the latent code retrieves information from the image feature maps at different scales. The SAT-Block is composed of three components: two SAT-Operations and a strided convolution. SAT-Operations are transformer-like [44] operations, replacing self-attention by our proposed SCA. They are defined as:</p><formula xml:id="formula_3">SAT(I 1 , I 2 , S) = LN(f (LN(SCA(I 1 , I 2 , S) + I 1 )) + I 1 ),<label>(2)</label></formula><p>with LN the layer norm and f a simple 2-layer feed forward network. The first SAT-Operation, SAT(X, Z, S), let the latents retrieve information from the image feature map (case (b) from 3.1). The second SAT-Operation, SAT(Z, Z, M ), is refining the latents using SCA in a self attention setup where the latents attend themselves, keeping the semantic restriction (case (c) from 3.1). The strided convolution encode the previous layer image feature map, reducing its spatial dimension. Implementation details and reference code about SAT-Blocks and SAT-Operation are in the supplementary material</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">SCAM-Generator</head><p>Definition and Architecture. SCAM-Generator takes as an input the latent codes Z ? 0 (= Z L E encoder's output) and the segmentation mask S and outputs the generated image Y ? L G . It consists of L G SCAM-Blocks <ref type="figure">(Figure 2 (right)</ref>). The input latent of the generator is given by the encoder's output, whereas the input latents of each block within the generator are the output latents of previous blocks. Similarly, the input feature maps of each block are the feature map outputs at each resolution, while for the first features, we encode the segmentation with a convolutional layer following <ref type="bibr">[32,</ref><ref type="bibr">56]</ref>. The SCAM-Block has a progressive growing architecture similar to the one of StyleGAN2 <ref type="bibr" target="#b19">[20]</ref> and consists of 3 SCAM-Operations (See <ref type="figure">Figure 2</ref> (right)). 2 SCAM-Operations process the generator feature-map, with an upscaling operation between the two, while a parallel SCAM-operation retrieves information from the feature maps and generate the image in the RGB space. Implementation details and reference code are in the supplementary. SCAM-Operation. It aims at exchanging information between pixels/features and latents of the same semantic label and is depicted in <ref type="figure">Figure 3</ref>. Each SCAM-Operation has inputs: (1) the set of input latents Z ? in ,(2) the input feature map X ? in , and (3) the segmentation mask S. Its outputs are: (1) the output latents Z ? out , and (2) the output feature map X ? out . SCAM-Operation consists of three parts: (a) the latent SAT, (b) the feature SCA, (c) the Modulation operation. (a) The latent SAT. It uses a SAT operation to update the current latents Z ? in based on the current feature map: Z ? out :=SAT(Z ? in , X ? in , S). This allows for latent refinement while enforcing the latents semantic constraint thanks to the SCA operation inside SAT. (b) The feature SCA performs latent to image attention: it incorporates the latent information to the pixels/features using SCA. Given X ? in , the output la- tents from the SAT-Operation Z ? out and the mask S, it outputs X ? SCA :</p><formula xml:id="formula_4">X ? SCA = SCA(X ? in , Z ? out , S) .<label>(3)</label></formula><p>(c) The Modulation operation takes as input the X ? in maps and the X ? SCA from SCA, and outputs features that are passed through a convolution layer g(.)</p><p>to produce the final output image features X ? out . It is defined as:</p><formula xml:id="formula_5">X ? out = g (?(X ? SCA ) ? IN(X ? in ) + ?(X ? SCA ) + N ) .<label>(4)</label></formula><p>It consists of the ?(.) and ?(.) operators that determine the scale and bias factor for the modulation operation. We use Instance Normalization (IN) <ref type="bibr">[43]</ref> to perform the feature map normalization. Following <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, we also add noise N ?R n?C , with N ?N (0, ? 2 I n?C ), and ? learned parameter. This encourages the modulation to account for stochastic variation.</p><p>Discussion. SCAM-Operation outputs both the modulated feature map and the updated latents. Thus, the information from the previous maps is propagated to the latents and to the feature maps. This brings several advantages: first, it preserves the semantic constraint; second, it provides finer refinement within the semantic mask by attending to multiple latents; and third it allows each pixel to choose which latent to use for modulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training losses</head><p>We train SCAM with GAN and reconstruction losses. We denote by D the discriminator, G the generator and E the encoder. For the discriminator, we follow <ref type="bibr" target="#b13">[14,</ref><ref type="bibr">46,</ref><ref type="bibr">46,</ref><ref type="bibr">32]</ref> and use PatchGAN, as it discriminates for patches of the given image instead of the global image. For the GAN loss, we use Hinge GAN loss [26]: L EG,GAN , L D,GAN . For the reconstruction loss, we use the perceptual loss L Perc as in [56] and the L 1 between the input and the reconstructed input.</p><p>The final losses are L EG = L EG,GAN + ? perc L Perc + ? 1 L 1 and L D =L D,GAN with ? perc and ? 1 hyperparameters. We use ? perc =? 1 =10 in our experiments. Training details are in the supplementary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Subject transfer</head><p>Once SCAM is trained for reconstruction, at test time we perform subject transfer. Given two images X A , X B with their respective segmentation masks (S A , S B ), we retrieve their latent codes as Z A and Z B using the SAT-Encoder.</p><p>To transfer the subject from X B to the context of X A , we create Z mix , where the style codes related to the background come from Z A and the remainder codes</p><formula xml:id="formula_6">come from Z B . Then, we retrieve Y ? mix =G(Z mix , S B ). See Figure 1.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We now present experimental results for SCAM. More results and ethical impacts are discussed in supplementary. Metrics. We use PSNR, reconstruction FID (R-FID), and swap FID (S-FID). R-FID is computed as the FID between the train set and the reconstructed test set, while S-FID is between the test set and a set of subject transfer images computed on the test set. We introduce REIDAcc and REIDSim, computed in the latent space of a re-identification network <ref type="bibr" target="#b8">[9]</ref>. REIDSim computes the average cosine similarity between the subject image and the subject transfer image. REIDAcc accounts for the proportion of images where the cosine similarity of the transferred subject is higher with the subject than with the background.   , clearly indicating that our method is also better at transferring. We observe that even for a dataset that has precise labelling, our approach still outperforms competing approaches.</p><formula xml:id="formula_7">Method iDesigner CelebAMask-HQ ADE20K PSNR ? R-FID ? S-FID ? REIDSim ? REIDAcc ? PSNR ? R-FID ? S-FID ? PSNR ? R-FID ? SPADE [32] [CVPR19</formula><formula xml:id="formula_8">SAT L1 LPerc LGAN PSNR ? +? R-FID ? ?? S-FID ? ?? PSNR ? +? R-FID ? ?? S-FID ? ?? i ? ? ? ? ? ? 21.</formula><p>Results on ADE20K are shown in <ref type="table" target="#tab_3">Table 1</ref> (right), where we observe that SCAM outperforms all methods. SCAM has the best PSNR of 20.0 whereas second to best SEAN has a PSNR of 14.6. SCAM also beats SEAN-CLADE by 11.2 R-FID points (27.5 vs 38.7). We cannot evaluate S-FID on this dataset since it is hard to select what is the main subject in the image, and not all images share the same semantic labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablations</head><p>Here, we perform several ablations to validate the effectiveness of all components of SCAM. We denote by ? experiments that do not converge.</p><p>Ablations of SAT-Encoder and SCAM-Generator. We examine the effectiveness of our encoder and generator by modifying either the SAT-Encoder or the SCAM-Generator with baseline versions and report the results on iDesigner in <ref type="table" target="#tab_6">Table 2</ref>. The first row (i) corresponds to our SCAM. We benchmark some variants of our SAT-Encoder: Conv denotes whether we use convolutions in SAT or not. SA denotes whether we use self-attention SAT block or not. The results show that overall convolutions in the SAT-Encoder provide a big encoding advantage. This is especially true for the complex iDesigner dataset. Indeed (i) outperforms (ii, iv, vi) by a high margin on iDesigner e.g., by -13.1 R-FID and -7.0 S-FID for (ii), which validates our use of multiple resolutions in the encoder. We also examine a variation of the generator by removing the SAT block in the SCAM block. Having SAT in SCAM leads to better results; such as -2.4 in R-FID, and -0.6 in S-FID for (v) on iDesigner. Similar results can be observed in (vi and vii).</p><p>In (x), we use the same encoder as in SEAN and the SCAM generator. To manage to have multiple latents per semantic latents, we split the SEAN encoding in 8 smaller latents for each semantic latent. We observe here that our SAT-Encoder is better than the SEAN encoder at extracting information from images. Indeed, we obtain better R-FID and S-FID (-14,4/-5.6) with our encoder than the SEAN encoder. Ablation of Losses. <ref type="table" target="#tab_6">Table 2</ref> (i,viii,ix) ablates the three L 1 , L Perc , L GAN losses used in SCAM. The full combination (i) reaches the best results. Interestingly, removing L Perc (ix) results in the best PSNR point (22.9db) while having among the worst R-FID and S-FID (43.2, 91.3, respectively). This is expected, as removing the perceptual loss makes the generator rely only on the L 1 loss, and may artificially increase PSNR at the cost of realism. Number k of latents per semantic label. We've studied the impact of the number of latents k per semantic label on the performance of the model. We tested k ? <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16</ref>, 32] on iDesigner <ref type="bibr">[36]</ref>. We find that the R-FID increase with the number of latents where the S-FID decreases. Indeed for k = 4 we have R-FID of 15.9 and a S-FID of 27.3. For k = 32, we have R-FID of 9.7 and a S-FID of 30.3. We settle for k = 8 in our experiments since it offers the best trade-off between S-FID and R-FID. We also privilege a smaller k since the time complexity of our model is O(k). Visualization of the attention matrix. To investigate how using multiple latents per region is handled by SCAM, we visualize in <ref type="figure" target="#fig_0">Figure 4</ref> the last SCAM layer attention matrix. We colour each pixel according to the corresponding latent with the highest attention value. Overall, we observe that for each semantic region, the latents attend to different subregions, capturing semantic information without supervision. The first example (a) shows that even without specialized segmentation labels, SCAM specializes some latents to reconstruct the complex face pattern (eyes, mouth, and hair) and others for the different body parts (dress and shoes). The second row displays an interesting case: SCAM is capable of assigning different latents to the humans in the background even if they are not labelled as such.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative results on reconstruction</head><p>Overall, we observe that the reconstruction quality of SCAM is superior to competing approaches. <ref type="figure" target="#fig_1">Figure 5</ref>  and SCAM (last six columns). The first two rows are samples from iDesigner, the third is from CelebAMask-HQ and the lastrow from ADE20K. Subject reconstruction on iDesigner. SCAM reconstructs more structure than SEAN, both in the background and the human. INADE, CLADE and SPADE tend to generate images that doesn't match the style of the original image. For the background, we observe that the curtains and window frame of the second row are well-reconstructed by SCAM, in contrast to SEAN that includes colors but no other frame-cues. This highlights the rich generation capabilities of SCAM-Generator, which manage to generate complex backgrounds where competing approaches fail. For the subject, SCAM results in finer reconstructions compared to other approaches. For instance, in the first row SCAM reconstructs coherent clothes, while SEAN generate a blurred out version of the clothes. Reconstruction on CelebAMask-HQ. Overall, SCAM generates crisper and more realistic results than competing approaches. For instance, in the third row, SEAN fails entirely to reconstruct the background by producing an averaged texture. Our method does a better job at this, figuring out a better positioning for the logo and capturing better colors and shapes. On the subject, we also observe that SEAN fails to capture small details such as eye colors. Reconstruction on ADE20K. SCAM has a more reliable reconstruction than competing approaches. In the fourth row, we observe that SCAM is the only approach that manage to reconstruct some texture on the chimney wall. We also observe that overall, SCAM has more detail in the reconstructed object whereas approaches such as SEAN tend to have more averaged textures. and SCAM (last six columns). The first three rows are samples from iDesigner, the fourth is from CelebAMask-HQ and the last one is on ADE20K. Overall, amongst all methods, SCAM is the one to successfully preserve all components of subject transfer: subject appearance, background appearance and pose. Subject transfer on iDesigner. SCAM leads to superior subjects (e.g., washed out colors in SEAN vs coherent structured clothing in SCAM in the first row) and background reconstruction (i.e., global background structure, positioning of people on the sides and colours) than SEAN. For instance, in the first row, SEAN completely fails to reconstruct the background, while SEAN-CLADE reconstructs some texture but lacks detail. Similarly, in the second row, SCAM captures the model in the background whereas other approaches miss it; note also the precise people reconstruction in the left part of the catwalk for SCAM compared to competing approaches. In the third row, we have a very hard case, where the subject has a completely different pose than the pose reference. While other approaches fail to rotate the subject, SCAM does succeed in doing, even if the quality of the generated image is low. Overall, subjects have better appearance in SCAM than in other methods, like details in clothes, shoes, or faces. Subject transfer on CelebAMask-HQ. The fourth row shows that SCAM recovers more details in the transferred image, such as the colour of the skin or facial expression. Notably, SCAM does capture the bicolor separation of the hair, while SEAN, SEAN-CLADE and INADE display an averaged hair color. Subject transfer on ADE20K. In the fifth row, we consider the house as the subject we want to transfer. We can see that SCAM does transfer the hut like appearance of the subject, whereas competing approaches fail to do so. We also observe that most approaches have difficulties with the person generation, only SCAM generates a human that is coherent with the background reference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Qualitative results on subject transfer</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">User Study on iDesigner</head><p>We perform an user study on iDesigner. We compare 3 models one against each other: INADE, SEAN and SCAM. We have asked 38 different people to select among 20 reconstruction images which method in their opinion had the best reconstruction. We denote the percentage of best pick for each method as R-UP (Reconstruction User Preference). Similarly, we have the same people asked to select among 20 subject transfer images which method performed the best subject transfer in their opinion. Participant were asked to take into account image quality and quality of transfer. We denote the percentage of best pick for each method as ST-UP (Subject Transfer User Preference). We observe that our method, SCAM, outperforms competing methods by a high margin. Indeed, 98.4% of users chose SCAM as the best image reconstruction technique, 1.5% picked SEAN and 0.1% for INADE. As for subject transfer 92.8% of users preferred SCAM, 5.0% chose SCAM and 2.2% INADE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We introduced SCAM that performs semantic editing and in particular subject transfer in images. The architecture contributions of SCAM are: first, the semantic cross attention (SCA) mechanism performing attention between features and a set of latents under the constraint that they only attend to semantically meaningful regions; second, the Semantic Attention Transformer Encoder (SAT) retrieving information based on a semantic attention mask; third, the Semantic Cross Attention Modulation Generator (SCAM) performing semantic-based generation. SCAM sets the new state of the art by leveraging multiple latents per semantic region and by providing a finer encoding of the latent vectors both at encoding and decoding stages. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Materials: SCAM! Transferring humans between images with Semantic Cross Attention Modulation</head><p>In this supplementary material, we first discuss the societal and environmental impact of SCAM (Section A). Then, we give more details of SCAM and additional experimental analysis (Sections B-C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Societal impact</head><p>This work could allow for multiple applications such as editing images, where we can easily exchange someone by someone else. This can have a negative impact if used with bad intentions. Indeed, one could use our method to create fake news. As of today, deep fakes can still be detected, by either humans or algorithms. However, as research progress in these fields, detection is going to get more complicated and regulation will have to control this.</p><p>Our method could also be very useful to create movies. It could allow changing an actor by another one. This could be very helpful for finishing a movie when an actor is impaired. It could also reduce the producing cost when using superstars, where an actor could accept for his image to be used, while not acting physically himself. This however could pose some legal problems since it is easy to create content without permission of the subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Environmental impact</head><p>For this project, we used 42.3 thousands GPUs hours. The experiments were done on a GPU cluster. The GPUs used are Nvidia V100-32g. Our experiments used 80% of the GPUs maximum power of 250Wh, which amounts to 10.6 MWh of energy used for the whole project. The cluster we used is the Jean Zay cluster, situated in France. France heavily relies on nuclear energy, having a greener energy than average, with 50-80g CO2 for each kWh produced. Considering only the CO2 for the production of the electricity used, this results in 528-846kg of CO2 emitted for this project. Training a single SCAM model takes around 50 GPUs hours, which amounts to 10kWh and 500-800 g of CO2 emitted. As a comparison, the world average per capita CO2 emission is 4.7 ton/year.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Future Work</head><p>To improve upon this method, we see three main directions. First, this project is intended for images. For it to work well on videos, we would need to add some temporal information. Adding temporal consistency and smoothing should help have more visually pleasing outputs. Second, another improvement would be to directly train on transferred images. In this project, we train on the reconstructed images, and then we infer subject transfer. We conjecture that training on the transferred images would yield better results. Third, we observe that the subject transfer task is complicated by the quality of the segmentation masks and the disparity between the pose reference subject segmentation and the style reference subject. To prevent this, we could learn a network to map the pose reference subject segmentation to the wanted pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B About SCAM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Implementation details</head><p>We train all models for 50k steps, with a batch size of 32 on 4 Nvidia V100 GPUs. We use AdamW [29] with a learning rate of 0.0001 for the generator and the encoder, and 0.0004 for the discriminator with ?=(0.9, 0.999). We set k=8 latents per label of dimension d=256. Each time we use attention on a feature map, we first encode the image using 2D sinusoidal positional encodings, as attention acts on sets that do not include positional information. Note that adding positional encoding to the latents is not useful because latents can be seen as a set and not a sequence. Moreover, since the latents are going to be initialized from a learned vector, this learned vector can incorporate the positional information, if needed, and this information will propagate throughout the architecture. In the discriminator, we use GradNorm [48] to stabilize the training. For the SAT-Encoder, we have L E = 6 SAT-Blocks. In the SAT-Generator, we have L G = 7 SCAM-Blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Details on SAT</head><p>Here are the detailed implementations for SAT-Operation (Cross (See Algo-rithm1) and Self (See Algorithm 2)) and for the SAT Block (See Algorithm 3). For even finer details (Hyperparameters, practical details) see the provided Pytorch code.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: SAT-Cross at layer i</head><formula xml:id="formula_9">Data: Zi ? R m?d ;Xi ? R n?C i ;S ? ? n?m Result:Zi ? R m?d Z ? i ? SCA(Zi, Xi, S); Z ? i ? LN (Z ? i + Zi); Zi ? F F N (Z ? i ); Zi ? LN (Zi + Z ? i );</formula><p>Algorithm 2: SAT-Self at layer i</p><formula xml:id="formula_10">Data:Zi ? R m?d ;M ? ? m?m Result: Zi+1 ? R m?d Z ?? i ? SCA(Zi,Zi, M ); Z ?? i ? LN (Z ?? i +Zi); Zi+1 ? F F N (Z ?? i ); Zi+1 ? LN (Zi+1 + Z ?? i );</formula><p>Algorithm 3: SAT-Block at layer i Algorithm 4: SCAM-Block at layer j</p><formula xml:id="formula_11">Data: Xi ? R H i ?W i ?C i ;Zi ? R m?d ;S ? ? H i ?W i ?m Result: Zi+1 ? R m?d ; Xi+1 ? R H i+1 ?W i+1 ?C i+ X f</formula><formula xml:id="formula_12">Data: X ? j ? R H j ?W j ?C i ;Y ? j ? R H j ?W j ?3 ; Z ? j ? R m?d ;S ? ? H j ?W j ?m Result: X ? j+1 ? R H j+1 ?W j+1 ?C j+1 ;Y ? j+1 ? R H j+1 ?W j+1 ?3 ; Z ? j ? R m?d X ? j , Z ? j ? SCAM (X ? j , Z ? j , S); X ? j ? Upsample(X ? j , upsize = 2); X ? j+1 , Z ? j+1 ? SCAM (X ? j , Z ? j , S); X ? j,RGB ,? SCAM (X ? j+1 , Z ? j+1 , S); Y ? j ? Upsample(Y ? j , upsize = 2); Y ? j+1 ? Y ? j + X ? j,RGB</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Losses</head><p>Here, we complement Section 3.4 in the main paper and describe with more details the losses used for training SCAM. We denote by G the SCAM-Generator, E the SAT-Encoder and D the PatchGAN discriminator.</p><p>For the GAN loss, we use Hinge GAN loss [26] as follows:</p><formula xml:id="formula_13">L D,GAN = E [max(0, 1 ? D(X, S))] + E [max(0, D(G(E(X, S), S), S) + 1)]<label>(5)</label></formula><formula xml:id="formula_14">L F,GAN = E [D(G(E(X, S), S), S)] .<label>(6)</label></formula><p>For the reconstruction loss, we use the perceptual loss as in <ref type="bibr">[56]</ref>. This uses a pretrained VGG network, and tries to make the intermediate feature maps between the input and the reconstructed input as close as possible. It is defined as:</p><formula xml:id="formula_15">L VGG = E L i=1 ?F i (X) ? F i (G(E(X, S)))? 1 ,<label>(7)</label></formula><p>with L the number of VGG hidden layers and F i the i th layer feature map of the VGG feature map. We also use a L 1 loss between the input and the reconstruction.</p><formula xml:id="formula_16">L 1 = E [?X ? G(E(X, S)))? 1 ] ,<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Metrics</head><p>Here we give more details on the metrics we introduce: REIDSim and REIDAcc. Let's call I S the subject images, I B the background images and I ST the subject transfer images. Note that the subject images/background images couples are fixed to be able to compare on the same Now if REID is the REID network introduced in [9], we now have X S = REID(I S ), X B = REID(I B ) and X B = REID(I B ) the respective embeddings. Now we can compute the REID metric:</p><formula xml:id="formula_17">REIDSim(I S , I ST ) = 1 n n k=1 (X S k ) T X ST k ?X S k ??X ST k ?<label>(9)</label></formula><p>REIDAcc(I S , I B ,</p><formula xml:id="formula_18">I ST ) = 1 n n k=1 ? (X S k ) T X ST k ?X S k ??X ST k ? &gt; (X B k ) T X ST k ?X B k ??X ST k ?<label>(10)</label></formula><p>With n the number of subject transfer images we want to evaluate.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Model characteristics</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Additional experimental analysis</head><p>In this section, we display complementary experiments on the task of pose transfer. We also showcase some qualitative ablations of SCAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Pose Transfer</head><p>Here, we illustrate qualitative results for the pose transfer task for SCAM on the iDesigner [36] dataset. Given two images, one being the style image X Style and another of X P ose being the pose reference. S P ose the associated segmentation mask. The goal of pose transfer is to generate an image matching the style of X Style with the semantics of X P ose . We extract the latent codes Z Style with the SAT-Encoder. We then can generate X P T = G(Z Style , S P ose ) the generated image that share the style of X Style and the pose of X P ose . In <ref type="figure" target="#fig_6">Figure 7</ref>, we observe how SPADE, SEAN, SEAN++ and SCAM perform pose transfer. In this figure, we observe similar trends that we observe with subject transfer.</p><p>SCAM captures more details than the competing methods. This is particularly visible on the backgrounds of images a) b) and d), where the backgrounds on SCAM display more coherent outputs with the style reference image than other methods.</p><p>Similarly, we can observe that SCAM presents better subject reconstruction like in a), where given the segmentation map, SCAM generates a coherent subject. The skirt gets converted into pants because of the segmentation masks, but the texture remains the one of the style image. For SEAN and SEAN-CLADE, we do not see it generating coherent clothes that match the style of the style image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Ablations</head><p>In <ref type="figure" target="#fig_7">Figure 8</ref> we showcase some qualitative ablations to complement the main paper quantitative ablations. This additional visual ablation help us understand what each block of SCAM brings perceptually. We observe that when removing the convolutions (fourth column) in the SAT encoder, we end-up with a very simple texture that lacks details. This is particularly true in row c), where the dress texture is very repetitive and simplistic. This confirms our hypothesis that using information at multiple resolutions gives higher quality encodings.</p><p>If we remove self attention in SAT (fifth column), we observe that the reconstruction lack details in the coarser label, such as the background. For example, in row d), we can observe that the 2 person in the background features the same cloths colour where in SCAM we can see 2 different colours. Allowing latents from the same region interacting together in the encoder enables refining such details.</p><p>Removing the SAT in SCAM (sixth column) yields similar results. For instance, we observe in row b) that the background is less sharp than in SCAM. The background person seems to be floating, whereas SCAM has a more coherent interaction between the floor and the background person. Refining the latents in the SCAM-Generator also yields improves the semantic knowledge of SCAM. When removing the L 1 loss (seventh column) we have outputs similar to the ones in SCAM (third column). However, there are some colourized artefacts, like in row a) (No L1). The L 1 loss allows removing artefacts that arise from the perceptual loss.</p><p>When removing the perceptual loss (eighth column), we see some salt and pepper noise appearing in the output, like in row b). These artefacts are linked to the L 1 loss and are stabilized by the perceptual loss.</p><p>When replacing the SAT encoder with the SCAM encoder (eighth column), we study the benefits of our encoder compared to the SEAN encoder. We observe that the SEAN Encoder fails to capture details in the background, and hence it limits the generator to only simplistic texture generation. This is particularly the case in row b) and d) where the method totally misses the person in the background</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 User Study</head><p>We provide in <ref type="figure">Figure 9</ref> and 10 the samples we used for the user study and the score for each individual question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Additional visual results</head><p>We provide in <ref type="figure" target="#fig_8">Figure 11</ref> and 12 additional results on CelebAMask-HQ and ADE20K. <ref type="figure">Fig. 9</ref>: User study on reconstruction. We provide here the samples that were used to evaluate the reconstruction quality on SCAM, SEAN and INADE in a user study. Note that we provide here the segmentation mask for reference but it was not provided to users. <ref type="figure">Fig. 10</ref>: User study on subject transfer. We provide here the samples that were used to evaluate the subject quality on SCAM, SEAN and INADE in a user study. Note that we provide here the segmentation mask for reference but it was not provided to users.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 4 :</head><label>4</label><figDesc>(a) SCAM specializes latents on complex patterns (shirt, eyes, shoes); (b) it learns semantic information on its own inside the semantic labels (background people).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 5 :</head><label>5</label><figDesc>Reconstructions on iDesigner [36], CelebAMask-HQ[21] and ADE20K[54].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 6 :</head><label>6</label><figDesc>Subject Transfer on the test set of iDesigner [36], CelebAMask-HQ[21] and ADE20K[54]. Note the hard case in row 3, where only SCAM rotates the subject. For ADE20K, we consider the house as the subject in the 5th row.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5</head><label>5</label><figDesc>displays the subject and background images and the segmentation mask of the pose (first three columns). Then we show the subject transfer for SPADE [32], CLADE [39], SEAN [56], SEAN-CLADE [39], INADE [38]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>21. Lee, C.H., Liu, Z., Wu, L., Luo, P.: Maskgan: Towards diverse and interactive facial image manipulation. In: CVPR (2020) 22. Lee, K., Chang, H., Jiang, L., Zhang, H., Tu, Z., Liu, C.: Vitgan: Training gans with vision transformers. ArXiv (2021) 23. Li, K., Zhang, J., Liu, Y., Lai, Y.K., Dai, Q.: Pona: Pose-guided non-local attention for human pose transfer. In: IEEE Transactions on Image Processing (2020) 24. Li, P., Xu, Y., Wei, Y., Yang, Y.: Self-correction for human parsing. In: IEEE TPAMI (2020) 25. Li, Y., Li, Y., Lu, J., Shechtman, E., Lee, Y.J., Singh, K.K.: Collaging class-specific gans for semantic image synthesis. In: ICCV (2021) 26. Lim, J.H., Ye, J.C.: Geometric gan. In: arXiv (2017) 27. Liu, X., Yin, G., Shao, J., Wang, X., Li, h.: Learning to predict layout-to-image conditional convolutions for semantic image synthesis. In: NeurIPS (2019) 28. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV (2021) 29. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv (2017) 30. Ma, L., Jia, X., Sun, Q., Schiele, B., Tuytelaars, T., Gool, L.V.: Pose guided person image generation. In: NeurIPS (2017) 31. Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.Y., Ermon, S.: SDEdit: Guided image synthesis and editing with stochastic differential equations. In: ICLR (2022) 32. Park, T., Liu, M.Y., Wang, T.C., Zhu, J.Y.: Semantic image synthesis with spatially-adaptive normalization. In: CVPR (2019) 33. Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with deep convolutional generative adversarial networks. In: ICLR (2016) 34. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I.: Improving language understanding by generative pre-training. In: ArXiv (2018) 35. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.: Language models are unsupervised multitask learners. In: ArXiv (2019) 36. R.J. Lehman, H.H.: idesigner 2019. In: FGVC6 (2019) 37. Sch?nfeld, E., Sushko, V., Zhang, D., Gall, J., Schiele, B., Khoreva, A.: You only need adversarial supervision for semantic image synthesis. In: ICLR (2021) 38. Tan, Z., Chai, M., Chen, D., Liao, J., Chu, Q., Liu, B., Hua, G., Yu, N.: Diverse semantic image synthesis via probability distribution modeling. In: CVPR (2021) 39. Tan, Z., Chen, D., Chu, Q., Chai, M., Liao, J., He, M., Yuan, L., Hua, G., Yu, N.: Efficient semantic image synthesis via class-adaptive normalization. In: IEEE TPAMI (2021) 40. Tang, H., Bai, S., Zhang, L., Torr, P.H., Sebe, N.: Xinggan for person image generation. In: ECCV (2020) 41. Tang, H., Xu, D., Yan, Y., Torr, P.H., Sebe, N.: Local class-specific and global image-level generative adversarial networks for semantic-guided scene generation. In: CVPR (2020) 42. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jegou, H.: Training data-efficient image transformers &amp; distillation through attention. In: Proc. ICML (2021) 43. Ulyanov, D., Vedaldi, A., Lempitsky, V.: Instance normalization: The missing ingredient for fast stylization. In: CVPR (2020) 44. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.u., Polosukhin, I.: Attention is all you need. In: NeurIPS (2017) 45. Wang, T.C., Liu, M.Y., Tao, A., Liu, G., Kautz, J., Catanzaro, B.: Few-shot videoto-video synthesis. In: NeurIPS (2019) 46. Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro, B.: Highresolution image synthesis and semantic manipulation with conditional gans. In: CVPR (2018) 47. Wang, Y., Qi, L., Chen, Y.C., Zhang, X., Jia, J.: Image synthesis via semantic composition. In: ICCV (2021) 48. Wu, Y.L., Shuai, H.H., Tam, Z.R., Chiu, H.Y.: Gradient normalization for generative adversarial networks. In: arXiv (2021) 49. Zhang, B., Gu, S., Zhang, B., Bao, J., Chen, D., Wen, F., Wang, Y., Guo, B.: Styleswin: Transformer-based gan for high-resolution image generation. ArXiv (2021) 50. Zhang, H., Goodfellow, I., Metaxas, D., Odena, A.: Self-attention generative adversarial networks. In: Proc. ICML (2019) 51. Zhang, J., Li, K., Lai, Y.K., Yang, J.: Pise: Person image synthesis and editingwith decoupled gan. In: CVPR (2021) 52. Zhang, J., Liu, X., Li, K.: Human pose transfer by adaptive hierarchical deformation. In: CGF (2020) 53. Zhao, L., Zhang, Z., Chen, T., Metaxas, D., Zhang, H.: Improved transformer for high-resolution gans. In: NeurIPS. vol. 34 (2021) 54. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene parsing through ade20k dataset. In: CVPR (2017) 55. Zhu, J.Y., Park, T., Isola, P., Efros, A.A.: Unpaired image-to-image translation using cycle-consistent adversarial networks. In: ICCV (2017) 56. Zhu, P., Abdal, R., Qin, Y., Wonka, P.: Sean: Image synthesis with semantic regionadaptive normalization. In: CVPR (2020) 57. Zhu, Z., Huang, T., Shi, B., Yu, M., Wang, B., Bai, X.: Progressive pose attention transfer for person image generation. In: CVPR (2019) 58. Zhu, Z., Xu, Z., You, A., Bai, X.: Semantically multi-modal image synthesis. In: CVPR (2020)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>?</head><label></label><figDesc>lat i Flatten(Xi, dim = [0, 1]); Si = Downsample(S, shape = (Hi, Wi, m)); S f lat i ? Flatten(S, dim = [0, 1]); Zi ? SAT-Cross(Zi, X f lat i , S f lat i ); Zi+1 ? SAT-Self(Zi,Zi, M ); Xi+1 ? g(Xi) ; /* g a strided convolution */ B.3 Details on SCAM Here are the detailed implementations for the SCAM Block (See Algorithm 4). For even finer details (Hyperparameters, practical details) see the provided Pytorch code.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 :</head><label>7</label><figDesc>Pose Transfer in idesigner. We compare SCAM to competing methods on the pose transfer task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 8 :</head><label>8</label><figDesc>Qualitative ablations on idesigner. We perform the same ablations as in the quantitative ablations section of the main paper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 11 :</head><label>11</label><figDesc>Reconstruction on CelebAMask-HQ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 12 :</head><label>12</label><figDesc>Reconstruction on ADE20K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>cs.CV] 10 Oct 2022</figDesc><table><row><cell>Subject reference</cell><cell>Combined</cell></row><row><cell></cell><cell>latent codes</cell></row><row><cell></cell><cell>Pose reference</cell><cell>Subject transfer output</cell></row><row><cell>Background reference</cell><cell>(semantic mask)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Comparison on iDesigner [36] and CelebAMask-HQ [21] and ADE20K[54].Implementation details. We train all models for 50k steps, with batch size of 32 on 4 Nvidia V100 GPUs. We set k=8 latents per label of dim d=256. We generate images of resolution 256px.4.1 Datasets and metricsiDesigner[36]  contains images from designer fashion shows, including 50 different designers with 50k train and 10k test samples. We segment human parts with [24] and then merge the labels to end up with: face, body and background labels. CelebAMask-HQ [21] contains celebrity faces from CelebA-HQ<ref type="bibr" target="#b17">[18]</ref> with 28k train and 2k test images labelled with 19 semantic labels of high quality. ADE20K [54] contains diverse scenes images with 20k train and 2k test images. The images are labelled with 150 semantic labels of high quality.</figDesc><table><row><cell>]</cell><cell>10.4</cell><cell>66.7</cell><cell>67.5</cell><cell>0.67</cell><cell>0.26</cell><cell>10.9</cell><cell>38.2</cell><cell>38.3</cell><cell>10.7</cell><cell>59.7</cell></row><row><cell>CLADE[39][TPAMI21]</cell><cell>11.3</cell><cell>45.4</cell><cell>46.1</cell><cell>0.68</cell><cell>0.29</cell><cell>10.8</cell><cell>41.8</cell><cell>42.0</cell><cell>10.4</cell><cell>53.7</cell></row><row><cell cols="2">SEAN-CLADE [39] [TPAMI21] 15.3</cell><cell>48.4</cell><cell>56.1</cell><cell>0.75</cell><cell>0.31</cell><cell>16.2</cell><cell>19.8</cell><cell>24.3</cell><cell>14.0</cell><cell>38.7</cell></row><row><cell>INADE[38] [CVPR21]</cell><cell>12.0</cell><cell>33.0</cell><cell>33.9</cell><cell>0.72</cell><cell>0.34</cell><cell>12.24</cell><cell>22.7</cell><cell>23.4</cell><cell>11.3</cell><cell>48.6</cell></row><row><cell>SEAN [56] [CVPR20]</cell><cell>14.9</cell><cell>53.5</cell><cell>58.7</cell><cell>0.74</cell><cell>0.30</cell><cell>16.2</cell><cell>18.9</cell><cell>22.8</cell><cell>14.6</cell><cell>47.6</cell></row><row><cell>SCAM (Ours)</cell><cell>21.4</cell><cell>13.2</cell><cell>26.9</cell><cell>0.81</cell><cell>0.56</cell><cell>21.9</cell><cell>15.5</cell><cell>19.8</cell><cell>20.0</cell><cell>27.5</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Overall, SCAM outperforms competing approaches for all metrics. Specifically, for PSNR it outperforms SEAN-CLADE by approximately +5.9dB, whereas it reaches 13.2 R-FID vs 33.0 for INADE. These major boosts show that our reconstructed images better preserve the details of the initial images, meaning the representation power of SCAM is higher than that of other approaches. The difference is also notable for S-FID, with INADE reaching 33.9 vs 26.9 for SCAM, showing that our method</figDesc><table><row><cell>Encoder Generator</cell><cell>Losses</cell><cell>iDesigner</cell><cell>CelebAMask-HQ</cell></row><row><cell>Conv SA</cell><cell></cell><cell></cell><cell></cell></row></table><note>4.2 Comparison to the state of the art We first compare our proposed SCAM to INADE [38], SEAN [56], CLADE [39], SEAN-CLADE [39] and SPADE [32]. We reproduce all the methods and provide code for our implementations in the supplementary material. Results on iDesigner are shown in Table 1 (left).</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on iDesigner [36] and CelebAMask-HQ[21]</figDesc><table /><note>perform better subject transfer on datasets with coarse semantic labels than other approaches. We also observe a superiority of SCAM on REIDSim (+0.06 compared to INADE) and REIDAcc (+0.22 compared to INADE). Overall REI- DAcc is a hard metric. This can be explained by the fact that the subject transfer image shares the same semantic information with the background image. Results on CelebAMask-HQ are shown in Table 1 (center), where we observe that SCAM outperforms all methods. For instance, for PSNR, SCAM outper- forms SEAN by +5.7dB. SCAM also improves over SEAN by 3.4 R-FID points (15.5 vs 19.8). For subject transfer, SCAM outperforms SEAN by a S-FID de- crease of almost 3 points (19.8 vs 22.8)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>displays the input images, their masks (first two columns) and the reconstructions by SPADE [32], CLADE [39], SEAN [56], SEAN-CLADE [39], INADE [38]</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>Thanks to the SCAM-Block, SCAM results in 2x fewer number of parameters compared to SEAN (i.e., 95M vs 265M). SEAN is bigger than our model, as it comprises a big FFN for each style code at each block. Instead, with the SCAM-Block, we have parameter sharing for every token in SCAM blocks, removing this constraint. Moreover, SCAM trains faster and can infer values 50% faster than SEAN. Measurements are made on a single NVIDIA RTX 3090.</figDesc><table><row><cell>Method</cell><cell cols="3"># parameters ? Training speed ? Inference speed ?</cell></row><row><cell>SPADE</cell><cell>109M</cell><cell>95ms</cell><cell>39ms</cell></row><row><cell>CLADE</cell><cell>84M</cell><cell>86ms</cell><cell>38ms</cell></row><row><cell>SEAN-CLADE</cell><cell>240M</cell><cell>384ms</cell><cell>83ms</cell></row><row><cell>SEAN</cell><cell>265M</cell><cell>280ms</cell><cell>92ms</cell></row><row><cell>INADE</cell><cell>85M</cell><cell>179ms</cell><cell>50ms</cell></row><row><cell>SCAM</cell><cell>95M</cell><cell>180ms</cell><cell>61ms</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 :</head><label>3</label><figDesc>Comparison of model characteristics. Speeds are given in ms/samples. We evaluate on CelebAMask-HQ[18] 20 semantic labels. SCAM has k = 8.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Kate Moss picture by JB Villareal/Shoot Digital, Natalia Vodianova picture by Karl Prouse/Catwalking.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">The attention values requiring masking are filled with ?? before the softmax. (In practice ? = ? 10 9 )</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments: We would like to thank Dimitrios Papadopoulos, Monika Wysoczanska, Philippe Chiberre and Thibaut Issenhuth for proofreading and Simon Ebel for helping with the video. This work was granted access to the HPC resources of IDRIS under the allocation 2021-AD011012630 made by GENCI and was supported by a DIM RFSI grant and ANR project TOSAI ANR-20-IADJ-0009.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccandlish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Everybody dance now</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ginosar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>NAACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Soft-gated warping-gan for pose-guided person image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diversifying semantic image synthesis and editing via class-and layer-wise vaes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Endo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kanamori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Taming transformers for high-resolution image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Unsupervised pre-training for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative adversarial nets</title>
		<imprint>
			<publisher>NeurIPS</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Mask-guided portrait editing with conditional gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Clothflow: A flow-based model for clothed person generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Scott</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Hudson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zoran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>H?naff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<title level="m">Perceiver io: A general architecture for structured inputs &amp; outputs. In: arXiv</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perceiver: General perception with iterative attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jaegle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gimeno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Transgan: Two transformers can make one strong gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Progressive growing of GANs for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">A style-based generator architecture for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Analyzing and improving the image quality of stylegan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hellsten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
