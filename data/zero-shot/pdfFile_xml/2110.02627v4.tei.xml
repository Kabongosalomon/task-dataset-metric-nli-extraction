<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">MovingFashion: a Benchmark for the Video-to-Shop Challenge</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Godi</surname></persName>
							<email>marco.godi@univr.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Verona</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Joppi</surname></persName>
							<email>christian.joppi@univr.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Verona</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geri</forename><surname>Skenderi</surname></persName>
							<email>geri.skenderi@univr.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Verona</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Cristani</surname></persName>
							<email>marco.cristani@univr@humatics.it</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Verona</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Humatics Srl</orgName>
								<address>
									<settlement>Verona</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">MovingFashion: a Benchmark for the Video-to-Shop Challenge</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:37+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Retrieving clothes that are worn in social media videos (Instagram, TikTok) is the latest frontier of e-fashion, referred to as "video-to-shop" in the computer vision literature. In this paper, we present MovingFashion, the first publicly available dataset to cope with this challenge. Mov-ingFashion is composed of 14855 social videos, each one of them associated with e-commerce "shop" images where the corresponding clothing items are clearly portrayed. In addition, we present a novel baseline for this scenario, dubbed SEAM Match-RCNN. The model is trained by image-tovideo domain adaptation, allowing the use of video sequences where only their association with a shop image is given, eliminating the need for millions of annotated bounding boxes. SEAM Match-RCNN builds an embedding, where an attention-based weighted sum of few frames (10) of a social video is enough to individuate the correct product within the first 5 retrieved items in a 14K+ shop element gallery with an accuracy of 80%. This provides the best performance on MovingFashion, comparing exhaustively against the related state-of-the-art approaches and alternative baselines 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>One of the most recent challenges in e-fashion is the so-called video-to-shop <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>, whose aim is to match a social video (Instagram, TikTok) containing one or more given clothing item(s), against an image gallery, potentially an e-commerce database ( <ref type="figure">Fig. 2a,b</ref>). Individuating the outfit of a celebrity or social influencer can turn videos into priceless commercials, in a market where over a billion * indicates equal contribution <ref type="bibr" target="#b0">1</ref> The code for SEAM Match-RCNN and the MovingFashion dataset are available here: https://github.com/HumaticsLAB/ SEAM-Match-RCNN hours of video are uploaded and viewed on a daily basis <ref type="bibr" target="#b1">[2]</ref>.Video-to-shop derives from the street-to-shop problem, where the probe data is a single image <ref type="bibr" target="#b5">[6]</ref>. On one hand, video-to-shop allows an increase of the available information by adding additional frames as probes. On the other hand, as shown in <ref type="figure">Fig. 2b</ref>, this information could be noisy due to challenging illumination, drastic zooming, human poses, missing data and multiple people (dis)appearing in the video. Another issue is that a video-to-shop system needs training data with millions of bounding box annotations, linking each box with a shop item <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>.</p><p>Our first contribution is MovingFashion, the very first publicly available video-to-shop dataset, composed by ?15K different video sequences, each one related with at least one shop image. The videos of MovingFashion are obtained from the fashion e-shop Net-A-Porter (10132 videos) and the social media platforms Instagram and TikTok (4723 videos), and contain hundreds of frames per shop item, partitioned into a Regular and Hard setup.</p><p>Our second contribution is the SElf-Attention Multiframe (SEAM) Match-RCNN, a video-to-shop baseline which individuates products and extracts features in a "street" video sequence by adopting a feature collection and aggregation mechanism, and then matching the products over a "shop" image gallery. SEAM Match-RCNN extends the popular Match-RCNN <ref type="bibr" target="#b3">[4]</ref>, state-of-the-art in the street-to-shop challenge, by applying image-to-video domain adaptation with the use of a novel Multi-frame Matching Head.</p><p>Technically, a pretraining on the image domain of the Match-RCNN enables it to provide initial pseudo-labels for a video sequence, individuating bounding boxes matching a particular product. The training on the target domain exploits our Multi-frame Matching Head, that aggregates features by means of a non-local block <ref type="bibr" target="#b11">[12]</ref> between different frames, which in turn applies a temporal self-attention mechanism <ref type="bibr" target="#b2">[3]</ref> and a scoring function. In this way an aggre- gation based on the attention score is used to create a single descriptor for a clothing item. In practice, SEAM Match-RCNN allows to train on video data where only the pairs &lt;"street" video,"shop" image&gt; are available, without annotated ground-truth bounding boxes. This policy permits to alleviate an intense annotation effort, which in the case of MovingFashion would have required drawing ?18M bounding boxes. In the experiments, SEAM Match-RCNN gives the best performances on MovingFashion, against multiple baselines and state-of-the-art techniques. Actually, few frames (10) of a social video are enough to individuate the correct product within the first 5 retrieved items with an accuracy of 80%, making SEAM Match-RCNN a proof of concept for a potential product in e-fashion. Finally, SEAM Match-RCNN gives explainable results: thanks to self-attention visualization, we understand that the initial quarter of a social video carries the most information for guessing the correct product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video-to-Shop. The first street-to-shop approaches employed single street images <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b8">9]</ref>; "street" video queries followed afterwards, paving the way for video-to-shop methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>. AsymNet <ref type="bibr" target="#b0">[1]</ref> aggregates frames by exploiting temporal continuity; it combines an LSTM and a binary tree, with each component requiring a separate training procedure. On the contrary, our SEAM Match-RCNN uses self-attention to learn a descriptor from a bunch of heterogeneous images, where temporal continuity is not required. DPRNet <ref type="bibr" target="#b14">[15]</ref> manages the video-to-shop problem by treating it as street-to-shop, with a network that detects and tracks garments in the video, selecting automatically the frame with the highest quality (in terms of occlusions, blurring etc). That detection is finally fed into an imageto-image retrieval module. SEAM Match-RCNN does not perform this kind of tracking, which could be prohibitive on social videos that have strong heterogeneous variations on few frames.</p><p>Video-to-shop approaches shares similarities with video person Re-ID <ref type="bibr" target="#b7">[8]</ref>, where the goal is to match a video snippet of a person's silhouette against a gallery of image identities taken from a different camera. State-of-the-art approaches are VKD <ref type="bibr" target="#b9">[10]</ref>, NVAN <ref type="bibr" target="#b7">[8]</ref> and MGH <ref type="bibr" target="#b13">[14]</ref>. VKD proposes to learn using diverse views of the same target with a teacherstudent framework, where the teacher educates a student who observes fewer views. NVAN is based on a non-local block self-attention module, embedded into the backbone  CNN at multiple feature levels to incorporate both spatial and temporal characteristics of the pedestrian videos into the representation. Multi-Granular Hypergraph (MGH) is a novel graph-based framework which uses graph networks to cope with this problem. Video-to-shop datasets. Unfortunately, no video-toshop datasets are publicly available. The above quoted <ref type="bibr" target="#b0">[1]</ref> and <ref type="bibr" target="#b14">[15]</ref> use proprietary datasets, which have been not made open to the scientific community. We compare these datasets and their reported characteristics with our Moving-Fashion dataset in <ref type="table" target="#tab_0">Table 1</ref>. It is visible that the datasets from AsymNet and DPRNet have a moderate number of sequences (526 and 818, respectively), while MovingFashion contains almost thirty times that amount (15K). In order to create more query data, DPRNet and AsymNet sample multiple sequences from the videos (generating 26K and 5K sub-trajectories, respectively). AsymNet contains 39K exact street-shop pairs and 85K diverse shop items, so one may infer shop distractors are present (shop items not present in the street set) but no details are provided on this. DPRNet has 21K Shop items, with no mentions about the exact pairs. MovingFashion has a single item associated with a unique shop image for each video, for a total of 15K unique (video) street-shop pairs. The DeepFashion2 dataset (DF2) <ref type="bibr" target="#b3">[4]</ref> presents a particular scenario: DF2 is made for the street-to-shop challenge, but some shop items are related to more than one street image (coming from different sources), creating 11K pairings. This provides us with another experimental setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MovingFashion</head><p>MovingFashion has 5.854M annotated frames, organized into 15045 video-shop matching pairs, i.e., each video is associated with a distinct shop image. In particular, there are 14.8K unique videos, among which some se-</p><formula xml:id="formula_0">#1 r a l u g e R n o i h s a F g n i v o M #1 #28 #47 #63 #103 #132 Shop Shop d r a H n o i h s a F g n i v o M #75 #194 #247 #1 #127 #190 a.</formula><p>b. <ref type="figure">Figure 2</ref>. MovingFashion dataset samples. The top row contains a "Regular" sequence, the bottom row a "Hard" sequence.</p><p>quences (190 videos) have more than one associated shop item (e.g., a t-shirt and trousers). The length of the videos is detailed in <ref type="figure" target="#fig_0">Fig. 1b</ref>, while the frame rate amounts to about 30FPS. Shop items are divided in classes, following the DeepFashion2 <ref type="bibr" target="#b3">[4]</ref> taxonomy. The list of classes and the number of occurrences for each class in the dataset is reported in <ref type="figure" target="#fig_0">Fig. 1a</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Data sources</head><p>MovingFashion is formed by two subsets: Regular and Hard. Regular MovingFashion: Regular MovingFashion consists of 10132 videos downloaded from the e-commerce website Net-A-Porter 2 : in the street video a single person is wearing the shop item in an indoor scenario (which can vary), and the corresponding shop image consists in the shop item captured over a plain background. This is the canonical shop image we have used in our experiments. Additionally, we have collected: a front shop image captured in the same background of the street video and worn by the same model in a frontal pose; a rear view image and a detail of the fabric. These last three were not used in the experiments. An example of Regular MovingFashion is showed in <ref type="figure">Fig. 2a</ref> (more in the supplementary material). Hard MovingFashion: Hard MovingFashion consists of 4723 videos from the social platforms Instagram and Tik-Tok. In this case, shop images have been obtained either by downloading images associated to the video as multiple images of the Instagram post or as part of the video itself (the spatial layout of some raw videos was organized in two halves, one being a still picture of the "shop" item, the other with the "street" video). Hard MovingFashion represents the hardest challenge, since all of the critical conditions listed in the introduction are present here, as also visible in <ref type="figure">Fig. 2b.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Collection and cleaning</head><p>All of the videos in Net-A-Porter have been designed to promote a clothing item, which made the data collec-tion process simpler. Cleaning was necessary only to remove classes not compliant with the taxonomy of Deep-Fashion2 <ref type="bibr" target="#b3">[4]</ref>. In contrast, Instagram and TikTok videos required a lot of work, starting with the search for the street videos and their shop counterparts using the available API, up to the careful scraping of hashtags and profiles. Other minor but time-consuming issues (fully/partially duplicate videos, wrongly associated shop items) are discussed in the supplementary material.</p><p>After collection and cleaning, we split the data into a training and a testing partition, taking care of applying the same split for each single class. We perform a 90/10 train/test split. Bounding boxes are extracted using a clothing detector. We then utilize the training data to train our SEAM Match-RCNN following the unsupervised procedure shown in Sec. 4.4. Since video sequences contain more than one item, to evaluate SEAM Match-RCNN and all the comparative approaches we create a tracklet containing the correct item for each street video sequence. That is done by selecting the tracklet that matches most with the shop item, following the training tracking procedure detailed in Sec. 4.4. A tracklet is a set of consecutive detections which refer to the same object. We manually check each one of these to ensure that at least 50% of the detections in the tracklet actually contain the shop item. For the detections which are too noisy (i.e. they do not focus on any precise clothing item), we dropped the entire sequence, in order to speed up the collection procedure. Fortunately, this happened on a minority of sequences (?150 videos), indicating a general success of the tracking procedure. The remaining tracklets are kept as noisy annotations in JSON format. All of the comparative approaches shown in Sec. 5 use these ground truth tracklets for training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">SEAM Match-RCNN</head><p>SEAM Match-RCNN takes as input a sequence of street images i 1 ...i N , and compares it with the gallery of K shop images providing a list of matching scores as output. Once the model has learned, the retrieval happens by means of three procedures: 1) Tracklet creation; 2) Feature aggregation; 3) Video-to-shop matching. Going through these steps will allow us to present the structure of the network, detailed in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Tracklet creation</head><p>On the input video sequence we need to locate a set of consecutive detections which refer to the same object, dubbed here tracklet. Since multiple objects might be on the video, multiple tracklets are expected. The module that deals with this is the Match-RCNN, which is composed of three functions:  tures c i,t,k with i indicating the i-th tracklet, t indicating the frame, k the k-th detection in that frame;</p><formula xml:id="formula_1">2. A 256-d feature extractor f i,t,k = f (c i,t,k ) ? R 256</formula><p>which performs embedding of the convolutional features;</p><formula xml:id="formula_2">3. A matching score function m(f i,t,k , f i,t ? ,k ? ) ? [0, 1],</formula><p>comparing different embeddings.</p><p>f and m together form the Single-frame Matching Head.</p><p>The tracklet extraction procedure is performed in an iterative fashion, following a two-step process:</p><p>1. Determining the pivot bounding box: This represents the most confident detection f i,t best ,k best in the sequence and acts as the central reference based on which the tracklet will be built. 2. Performing propagation based on the pivot: By comparing the embedding of the pivot f i,t best ,k best with all of the detections in every frame, the tracklet i can be built. In particular, a detection joins the tracklet if its matching score (matching function m of the Single-Frame Matching Head) is above a certain threshold, to avoid considering frames where the item is not visible.</p><p>Once the tracklet i is built, its detections are removed, and another tracklet focusing on a different item can be built.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Feature aggregation</head><p>The next step is aggregating the information of a tracklet and condensing it into a single multi-frame descriptor. The module that deals with the feature aggregation procedure is the Multi-frame Matching Head and it is composed of the following functions and modules:</p><formula xml:id="formula_3">1. A 256-d feature extractorf i,t =f (c i,t ) ? R 256 oper-</formula><p>ating on the bounding box at time t of the tracklet i, i.e., c i,t . <ref type="bibr" target="#b11">[12]</ref> module which applies selfattention, enriching {f i,t } t with information coming from all the other bounding boxes related to the object tracklet i. 3. An attention module g : R N ?256 ? R N that for each detection in a tracklet computes an importance score w t such that t w t = 1. 4. An aggregation module, which fuses {f i,t } t into a joint descriptorf i by a weighted average over the attention scores {w t }:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">A non-local block</head><formula xml:id="formula_4">h(x) = g(N LB(x)) ? x, x ? R N ?256 . 5. A matching score functionm(f j ,f i ) ? [0, 1], which</formula><p>compares the aggregated descriptor for item k and street sequence i (h({f i,t } t ) asf i ) with the the shop descriptor of image j.</p><p>The aggregation procedure starts with the feature extractorf , which creates the initial descriptors for each box in a sequence. Then, self-attention is computed by the non-local block module and afterwards the attention module calculates the attention weights for each descriptor. The aggregation module puts all of the above together, producing the single multi-frame descriptorf i . Note that we discard temporal continuity by design. Social network videos usually have dramatic zooms, very fast pose dynamics and occlusions; moreover, elaborated videos may have shot changes which can fragment temporal continuity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Video-to-shop matching</head><p>Following the feature aggregation procedure described above, we obtain a single multi-frame descriptorf i of the street tracklet i. In this final procedure, the matching score functionm of the Multi-frame Matching Head is used to match the aggregated multi-frame description with the single shop descriptor of image j,f j (which can be considered as a tracklet composed by a single frame), under the assumption that a single item is portrayed in the shop image.</p><p>We use the matching functionm on all the images in the shop gallery, producing in this way a list of matching scores between the street tracklet and all the images in the shop gallery, sorted in descending order, creating thus a ranking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Model Training</head><p>To avoid the need of bounding boxes annotations, a time-consuming procedure especially for videos, SEAM Match-RCNN is trained by domain adaptation, through two phases: pretraining on the source image domain and training on the target video domain.</p><p>Pretraining on Source domain. The Match-RCNN part of SEAM Match-RCNN (detector and Single-frame Matching Head) is pretrained on an image street-to-shop dataset (e.g. DeepFashion2). The purpose of this phase is to train a model that is able to estimate bounding boxes and matching scores in the target domain (even with noisy predictions due to the domain gap). Such predictions are used to generate tracklets and pseudo-labels to train the Multi-frame Matching Head.</p><p>Training on Target domain. The training procedure estimates the parameters for the Multi-frame Matching Head of the SEAM Match-RCNN, whose structure is detailed in Sec. 4.2, and fine-tunes the Single-frame Matching Head, while the detector's weights are frozen. The weights of the features extractorf and matching score functionm are initialized copying those of f and m from the pretrained Single-frame Matching Head. Conversely, the attention modules of h are randomly initialized. During training, image and street video sequence pairs (thanks to the Moving-Fashion pairing annotations) are sampled, which are leveraged in the tracking procedure (Sec. 4.1): the pivot selection is done by selecting the detection that matches the shop product the most in the whole video if the matching score inferred from the matching function m of the Single-frame Matching Head is over a certain threshold. The propagation step remains the same as in Sec. 4.1. With this training tracking procedure a tracklet is built such that, with a certain confidence, it contains the correct shop item due to the pivot selection starting from the ground truth shop image. This is considered as a positive match during training (i.e. we set 1 as a pseudo-label for the tracklet). For what concerns the Single-frame Matching Head fine-tuning, each detection that composes the tracklet is considered as positive match as well. The tracklet is then passed as input to the Multi-frame Matching head, which computes a singular multi-frame descriptorf i thanks to the aggregation procedure described in Sec. 4.2. In the end, this singular multi-frame descriptorf i is compared with the corresponding shop descriptorf j (obtained by using the feature extractorf j = f (c j )) utilizing the matching score functionm. This produces a matching score in the range [0, 1].</p><p>Training is done by Stochastic Gradient Descent using cross-entropy loss for the classification of street videos and shop images as positive/negative matches. Positive pairings are built using the aforementioned procedure. All of the other combinations between tracklets and shop image descriptors are considered negative pairings (i.e. pseudo-label of 0) for the Single-frame Matching Head and the Multiframe Matching Head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>For the retrieval performance evaluation, we follow the testing protocol of DeepFashion2 <ref type="bibr" target="#b3">[4]</ref> for evaluating a street image probe against a shop image gallery, with some modifications in order to cope with videos. In DeepFashion2, a street image generates multiple detections: each street detection can generate a proper matching with some shop image, if it overlaps by a threshold with the corresponding ground truth street bounding box and if its item class is correct, otherwise the matching score is 0.</p><p>On MovingFashion, we compute detections on every street image and we build tracklets using the tracking procedure detailed in Sec. 4.1. Then, we compute the average IoU between each street tracklet and the ground truth tracklet. The one with the highest average IoU is chosen and used as a query. In order to guarantee fairness in experiments, all baselines and comparative methods have been pretrained on two different street-to-shop datasets: DeepFashion2 and Exact Street2Shop <ref type="bibr" target="#b5">[6]</ref>; the former has 873K probe-gallery pairs, while the latter 39K pairs only. Detailed results are reported for the first case, since performances were higher, while in the second case we show the main retrieval results, where our SEAM Match-RCNN remains the best performing approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experiments on MovingFashion</head><p>We compare our technique with three types of approaches:</p><p>Multi-frame baselines. They are extensions of singleframe techniques to multi-frame. The Max Confidence <ref type="bibr" target="#b3">[4]</ref> keeps the most confident detection in a tracklet and uses it for Single-frame Matching, employing a Match-RCNN. The Max Matching is inspired from <ref type="bibr" target="#b0">[1]</ref> and considers the max matching score between the tracklet's street frames and each shop image. These two baselines are actually working with a single image, which is selected by looking at the entire pool of frames in a tracklet. They are also useful to validate the performance boost that comes when using multiple frames instead of single ones.</p><p>The Average Distance is inspired by <ref type="bibr" target="#b0">[1]</ref> and consists in averaging single-image matching scores of the tracklet street frames and each shop image. The SEAM Match-RCNN w/o N LB,g is obtained by averaging descriptors (and not matching scores) together by average pooling, removing in practice the NLB self-attention block and the at-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method MovingFashion</head><p>Regular-MovingFashion Hard- <ref type="table" target="#tab_0">MovingFashion  T-1  T-5  T-10  T-20  T-1  T-5  T-10  T-20  T-1  T-5  T-</ref>  <ref type="table">Table 3</ref>. Top-K accuracy on MovingFashion, pretraining on S2S <ref type="bibr" target="#b5">[6]</ref> tention scoring function g from the SEAM Match-RCNN (see the scheme in <ref type="figure" target="#fig_1">Fig. 3</ref>). Finally, SEAM Match-RCNN w/o N LB keeps the attention score, without the self-attention. These last three are proper multi-frame baselines, in the sense that they merge information coming from multiple frames.</p><p>Video Re-Identification approaches. Video Re-Id approaches share similarities with Video-to-shop, in that they look for the best way to aggregate multi-frame information to match a person in a disjoint multi-camera setting. In practice, we consider each shop clothing item the equivalent of a Person Re-Identification Identity. The main differences between video-to-shop and Person Re-ID are that in our case less information is available in terms of pixels, since face and hair need to be discarded, focusing only on the clothing. Here we consider the SoA approaches of NVAN <ref type="bibr" target="#b7">[8]</ref>, VKD <ref type="bibr" target="#b9">[10]</ref> and MGH <ref type="bibr" target="#b13">[14]</ref> 3 .</p><p>Video-to-shop approaches. We consider the Asym-Net [1] approach 4 , and its modifications AsymNet[AVG] and AsymNet[MAX], in which the aggregations are made respectively by the average and the max of the similarity score nodes' outputs instead of using the fusion nodes binary tree. Asymnet exploits temporal continuity, yet it does not reach our results.</p><p>We set the sequence length to T = 10 for both training and testing, picking the frames using the Restricted Random Sampling strategy <ref type="bibr" target="#b6">[7]</ref>, thus ensuring coverage of the <ref type="bibr" target="#b2">3</ref> At the moment of writing, the MGH approach is state-of-the-art in the MARS Video Person Re-Identification dataset, followed closely by VKD and NVAN. <ref type="bibr" target="#b3">4</ref> The code is available at https://github.com/kyusbok/ Video2ShopExactMatching. entire sequence length. To analyze variability in the results, we analyze the testing samples by sub-sampling them into pool of 800, 20 times, averaging the rankings and computing standard deviations. <ref type="table">Table 2</ref> reports the results. Three facts become apparent: 1) As expected, single-frame approaches (Max Confidence, Max Matching) are definitely inferior (&lt;15% on average) than multi-frame approaches; 2) The considered re-identification approaches, apart from top-1 scores, are inferior to genuine video-to-shop methods; 3) Our SEAM Match-RCNN surpasses all the competitors, including AsymNet, which gives a better aggregation than the AVG-distance in its [AVG] version and the MAX-distance in its [MAX] version. By looking at the ablative versions of SEAM Match-RCNN, one can note that the self-attention gives the strongest performance boost, followed by the attention layer. Their cooperation, i.e., the complete SEAM Match-RCNN, reaches the highest score.</p><p>By looking at the results within the Regular and Hard MovingFashion partitions, it is quite easy to note the general decrease in performance when it comes to the hard partition. To understand the performance qualitatively, <ref type="figure">Fig. 6</ref> shows retrieval results from Regular <ref type="figure">(Fig. 6a)</ref> and Hard <ref type="figure">(Fig. 6b</ref>). Actually, even if Regular is apparently harder due to many shop alternatives which differ by fine grained results (see the flared jeans), the dramatic changes of poses and backgrounds of the Hard partition play a stronger role.</p><p>Failure cases arise when the original video has discriminant parts of the clothing item covered for most of the sequence, for instance the logo of the light blue sweatshirt <ref type="figure" target="#fig_3">(Fig. 4a</ref>). In this case, self-attention overlooks such important details. Complex visual patterns like the hard-rock band logo <ref type="figure" target="#fig_3">(Fig. 4b</ref>), seem to be not well characterized, meaning that the best match is attributed considering the shape of the logo rather than its content (the "Metallica" logo has the same shape of the probe logo).</p><p>The results w.r.t the single clothing classes of Moving-Fashion are reported in <ref type="table" target="#tab_5">Table 4</ref>, where it is possible to observe our advantage in all but three classes. Interestingly, we found that the simpler the clothing in terms of    texture, the lower the retrieval performance. This is reasonable, since texture adds discriminative details, and this is why classes with simpler texture like vest, sling, shorts and trousers performed worse. We computed textureness by gray-level co-occurrence matrix contrast; quantitatively speaking, textureness and top-1 accuracy in retrieval are found to be correlated (Spearman ? = 0.72, p ? value ? 0.05).</p><p>Another experiment regards the length of the sequences. <ref type="figure" target="#fig_4">Fig. 5</ref> reports, with the associated error bars, the performance of SEAM Match-RCNN when increasing the number of frames from 1 to 20. As expected, the curves for both partitions, at both the top-1 and top-20 are increasing, with the "Hard" partition showing a plateau after 10 frames, while the "Regular" partition seem to benefit systematically. The reason could be that "Hard" sequences are dramatically noisy, and adding more frames will augment the clutter we need to deal with, while the "Regular" ones benefit because of the fine grained details which characterize the partition. Comparative performances when varying the sequence's length against other approaches are in Tab. 5. Notably, Asymnet <ref type="bibr" target="#b0">[1]</ref> does not reach our results even when doubling the number of input frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Experiments on unrelated sets of images</head><p>MovingFashion has street videos which depict clothing items in a variety of scenarios: indoor, outdoor, etc. We are interested in bringing this variety to the extreme, an-   <ref type="table">Table 5</ref>. Top-1 accuracy on MovingFashion, with different number of frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method T-1 T-5 T-10 T-20</head><p>Max-Confidence .19(.014) .44(.020) .54(.020) .66(.017) Max Matching <ref type="bibr" target="#b0">[1]</ref> .14(.015) .45(.020) .61(.019) .75(.017) NVAN (2019) <ref type="bibr" target="#b7">[8]</ref> .22(.019) .37(.019) .43(.018) .49(.019) VKD (2020) <ref type="bibr" target="#b9">[10]</ref> .21(.014) .27(.017) .33(.017) .38(.018) MGH (2020) <ref type="bibr" target="#b13">[14]</ref> .  swering the following question: how does SEAM Match-RCNN behave when the street video sequence is formed by a few totally unrelated frames? In order to perform these experiments, we build Multi-DeepFashion2 from DeepFash-ion2 using the pairings between shop images and street sequences composed of multiple corresponding street images. The total pairings amount to 11K, each one composed of an image sequence (6 frames on average) sampled from different sources, along with the corresponding shop image. Results are in Tab. 6. Please note that, in order to be consistent with the 10-frames street sequence length we generate random repetitions for all the approaches given the smaller set of diverse images. The numbers indicate a decrease in general performance (less distinctive frames, more shop items); even in this case, we perform better than AsymNet.</p><formula xml:id="formula_5">#1 #2 #3 #4 ? #20 #1 #2 #3 #4 #20 ? a.</formula><p>b. <ref type="figure">Figure 6</ref>. Qualitative retrieval results of SEAM Match-RCNN for the MovingFashion dataset. On the left, we show 3 frames sampled from the 10 frames used for aggregation. On the right the shop images retrieved with the corresponding rank. The correct matches are with a green border, otherwise red.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experiments on the attention mechanism</head><p>The ablation studies of <ref type="table">Table 2</ref> clearly show that the attention layers play a crucial role for the SEAM Match-RCNN performance. Here we explain their role qualitatively and quantitatively. In <ref type="figure" target="#fig_5">Fig. 7</ref> we report the attention values obtained after the application of the attention layer g to the output of the self-attention layer N LB of Sec. 4.2, i.e., g(N LB(x)). On row a), one can note that the attention is high when the heart logo is visible (0.31, 0.23 in the first two frames) and it goes down when it vanishes, despite the light blue shirt (last frame) being very similar area-wise. This means that the mechanism considers the heart logo as important for retrieval. On the second row b), the effect of an occlusion in the attention score (last frame). On the third row c), a white top with a logo gives a stable attention score (around 0.28). We manually cover the logo in the third frame, causing a clear decrease in the attention, uniformly increasing the ones highlighting the logo.</p><p>Finally, driven by best practices in social video editing <ref type="bibr" target="#b4">[5]</ref>, which state that a video message has to deliver its main content in the first 6 seconds to trigger the observers' attention, we calculate the attention every 5 percentiles on all the Moving fashion sequences, producing the curves in <ref type="figure">Fig. 8a</ref>) (on the whole MovingFashion dataset) and on the separate partitions <ref type="figure">Fig. 8b</ref>. Surprisingly, the data confirms this rule, showing a clear ( <ref type="figure">Fig. 8a)</ref> peak around the first quartile (definitely within 6 seconds), then a decrease and a later increase with a local maximum on the fourth quartile. The same holds for the two separate partitions <ref type="figure">(Fig. 8b)</ref>), with less emphasis on the "Hard partition". The reason lies in the nature of the Net-A-Porter videos, which in many cases show the entire clothing item in the beginning of the sequence, with the model that moves subsequently, zooming up to critical detail (the belt for the shorts) towards the end (second peak). On the "Hard" partition, the attention for the clothing items is higher in the beginning, since the actors present their outfit and then exhibit their performance (dancing, gymnastics etc.), concluding in both the cases with uninteresting details clothing wise.    <ref type="figure">Figure 8</ref>. Mean attention score every 5 percentiles of the video length. For each video we sampled 21 equally spaced frames. On the left we report the average attention (y-axis) and frame-timing information (x-axis labels) for the whole MovingFashion dataset. On the right for the Regular and Hard subsets. We show error bands for the standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>Our SEAM Match-RCNN, trained on the new Mov-ingFashion dataset, provides a strong baseline that shows video-to-shop matching can be performed on videos in the wild like TikToks, possibly unveiling fashion trends directly from social platforms and consequently attracting big fashion players.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. MovingFashion Additional Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Image and video collection</head><p>In this section we give further details on the process of data collection and annotation of Moving Fashion.</p><p>Regarding the data collected from the Net-A-Porter website, the data labeling was a long, yet linear process, the only issue being the removal of classes not in the DeepFashion2 taxonomy, in particular shoes (deserving of a specific fashion taxonomy) and jewelry (due to the lack of a shared and widely accepted aesthetical taxonomy). For the remaining classes, the association to the specific DF2 taxonomy was direct.</p><p>Plenty more work was required for the data downloaded from Instagram. In order to to download the data, the Instaloader 5 tool was employed. We manually selected a list of hashtags and profiles with a lot of content, i.e. a lot of videos paired with fashion products for sale. Through the use of the tool, we downloaded posts containing videos only based on the previously mentioned hashtags and profiles. The layout of these videos was standard for the vast majority of them: the frame was divided vertically in two parts, one with just a still picture of the shop product and one with the video itself.</p><p>We manually annotated these videos by following these steps:</p><p>? We checked that the product actually appears in the video, since in some cases the item never appears or appears very briefly in the frame; sometimes the item is in a different color than the one in the shop image.</p><p>? We drew a bounding box around the area of the shop item(s), taking care to include as few other items as possible.</p><p>? We drew another bounding box around the area of the video.</p><p>Using these annotations we crop the street videos and shop images. This results in pairings, where in some cases we have more than one shop item associated with a street video. Next, we dealt with duplicates of shop products. In some cases the same product is showcased in multiple videos by different users, but fortunately, the shop image used in such videos is the same. We leveraged this fact to perform a duplicate search for all the shop images. Products that were found to be duplicates were merged, creating pairings where for one shop product multiple videos are associated. To perform this search, for each product we searched for duplicates using a pre-existing tool 6 that employs Perceptual Hash. However we found out that in order to have a very high recall, this process also includes a lot of false positives. To perform a more thorough search, we tried an Image Registration technique using the RANSAC algorithm between each shop image and the duplicate candidates found using the tool. We tried to estimate a Similarity Transform, to account for translations and scaling (as is the case for these images). We then put a threshold on average pixel difference to separate between duplicates and non duplicates. Since no Python libraries that implement RANSAC are available, it was performed using a custom script.</p><p>To make sure that MovingFashion respects the privacy of social media users, we have rendered any face in the videos blurred using a publicly available, face blurring tool 7 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Tracklet generation</head><p>As described in the paper, for all data, noisy tracklet annotations are available. In order to create them:</p><p>? Our SEAM Match-RCNN is trained on the data using only video-image pairing annotations. This results in a model where the Single-frame Matching Head can be effectively used for precisely tracking each item.</p><p>? We use the trained model to build a set of tracklets for each video.</p><p>? We manually go over each video and select the tracklets that contain the paired shop item, merging them if they are disjointed (this happens when an item is occluded completely or disappears from the frame and two separate tracklets are built).</p><p>The resulting tracklets are then saved. While for our approach, no tracklet annotations are used during training, they are used for all the comparative approaches. They are considered as equivalent to ours (the detector and the tracker are the same). It can be argued that they are actually better than ours as they are produced after the last epoch of training, while for our approach we start with a tracker that has not been trained yet. For the Person Re-ID approaches, the annotations are used to crop out part of the image according to the extracted bounding box. For detection based approaches, the bounding boxes are used as ground truth bounding boxes. The testing tracklets are used by all approaches for evaluation. During the SEAM Match-RCNN evaluation, they are used to select the tracklet among the ones produced automatically by the tracking procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. SEAM Match-RCNN computational complexity</head><p>In this Section we discuss the computational complexity of our proposed SEAM Match-RCNN. In particular we  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Single-frame Matching Head</head><p>Let T F be the time taken for computing features by using the f function and T M the time taken for computing matching between two feature vectors using m.</p><p>Given a street image and a shop image, the cost of computing a matching between them, assuming that the detection from the street image has already been chosen in some way (for example by comparing it with a ground truth bounding box) is 2?T F +T M (features computed for both street and shop are compared).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Multi-frame Matching Head</head><p>When extending to Multi-frame matching, the cost of tracking has to be taken into consideration. Obviously the time taken for feature computation increases linearly with the number of frames sampled from the video.</p><p>Asf andm are structured in the same way as f and m, we can assert that T F and T M also apply to them. Given a street video sequence from which we sample T frames, the  cost of building all the possible tracklets (using the tracking procedure, Sec 4-1 of the main paper) is related to the number of detections in each frame K (to simplify notation we assume that there are exactly K detections in each frame). First of all, Single-frame Matching Head features are computed, the time cost is T F ? K ? T . As a reminder, the tracking procedure consists of iteratively repeating the choice of pivot and propagation. The choice of the pivot is performed by choosing the most confident detection, so its cost is negligible as it is already included in the detection. The propagation consists of doing comparisons between the pivot features and all of the detection features in a frame. For a Single-frame the time necessary for the propagation step is T M ? K (a matching for each detection). This procedure is repeated for all frames resulting in T M ? K ? T . This results in a single tracklet, that is excluded from the set of detections for the following iterations. As the iteration is repeated until there are no more detections, we can assume that repeating the propagation K times results in a final cost of T M ? K 2 ? T . For the whole tracking procedure, the total time is</p><formula xml:id="formula_6">(T F ? K ? T ) + (T M ? K 2 ? T ).</formula><p>After tracklets are built, we can assume that the correct tracklet is chosen, for example by using the Intersection over Union with the ground truth tracklets (analogous to selecting the correct bounding box in the Match-RCNN). Given a sequence of detection of length T (length of the video sequence), the cost of computing Multi-frame Matching features is again T F ? T . Then self-attetion with the Non-Local Block is performed, resulting in a time cost of T 2 ? T SA (T SA is the cost of computing self-attention between a pair of element in the sequence, usually a simple operation like a dot product). The attention score is then computed for each frame, with a cost of T ? T A (T A is the cost of computing the attention score, in our case a simple linear layer). Finally a weighted average pooling is performed and matching is computed between the aggregated descriptor and the shop feature vector (T F + T M ). The final cost for aggregation is (T F ? T ) + (T 2 ? T SA) + (T ? T A) + (T F + T M ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Discussion</head><p>It is expected that the extension from Single-frame to Multi-frame will come with an increased cost, in relation to the number of frames. The tracking procedure is a necessary step for any possible Multi-frame approach, as detections from each frame need to be grouped in some way. The matching component increases quadratically with the maximum number of detections in each frame and linearly with the number of frames sampled from the sequence.</p><p>The aggregation has a term that increases quadratically with the number of frames. For both of these, we have to take into consideration that we usually work with 10 samples and there are rarely many different people and clothing items in a video, so even with a quadratic complexity, the total effective time is relatively small. In our experiments, we never go over 2 seconds for the whole procedure, with the majority of the videos taking about 1 second to process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional experiments</head><p>In <ref type="table">Table 7</ref>, we show the results of Single-frame baselines built on top of the Match-RCNN (the main building block of our SEAM Match-RCNN). In particular, SFM-1qrt uses the frame at the first quartile of all the available frames of that sequence, SFM-median uses the median frame and so on. SFM stands for Single-frame match and is a short term for Match-RCNN.</p><p>The correspondent baselines are shown, adopting the Deep Kronecker-Product Matching (KPM) <ref type="bibr" target="#b10">[11]</ref> and the Easy Positive Triplet Mining approach (EPHN) <ref type="bibr" target="#b12">[13]</ref>. The rationale of this choice was to focus on Single-frame Re-Identification approaches and compare them to the Match-RCNN. This was done to enlarge the spectrum of possible comparative approaches, which have open-source code available. The idea of considering Re-ID approaches against street-to-shop techniques was also presented in the DPRNet paper <ref type="bibr" target="#b14">[15]</ref>.</p><p>The inferiority of these baselines with respect of the Multi-frame of <ref type="table">Table 2</ref> in the main paper, and in particular with SEAM Match-RCNN, is evident and fully understandable.</p><p>Notably, in almost all of the MovingFashion partitions (apart the regular one with EPHN), the ?-1qrt baseline gives the higher results, which seems to be in accord with the best practices in social media video editing, that is, that videos have to deliver their main message within approximately 6 seconds <ref type="bibr" target="#b4">[5]</ref>.</p><p>As additional Multi-frame approaches, <ref type="table" target="#tab_10">Table 8</ref> shows Max Confidence, Max Matching and Average Matching scores when considering the KPM <ref type="bibr" target="#b10">[11]</ref> and the EPHN <ref type="bibr" target="#b12">[13]</ref> as Single-frame method ingredients, in the same way that Match-RCNN was used to calculate Max Confidence, Max Matching and Average Matching from <ref type="table">Table 2</ref> of the main paper.</p><p>Even in this case, SEAM Match-RCNN gives the best performance, showing an overall superiority of Match-RCNN as a Single-frame tool to aggregate visual clothing information.</p><p>The same applies when it comes to MultiDeepFashion2 where we investigate only Multi-frame policies (Max Confidence, Matching, Avg Matching and Descriptor), since Single-frame policies do not have much sense, as the Single-frames are not part of a single sequence. Even in this case, SEAM Match-RCNN is the best alternative ( <ref type="table" target="#tab_11">Table 9</ref>).</p><p>As additional qualitative results, on <ref type="figure">Fig. 9</ref> results of SEAM Match-RCNN for the Hard-MovingFashion dataset are shown. Two types of considerations can be drawn: the first one is the variability of the videos, which here can be appreciated with more examples. Second, the retrieval results on the right display that SEAM Match-RCNN is capable of finding similar images, among a shop gallery that in some cases contains highly similar items (see for example the light gray trousers).</p><p>On <ref type="figure" target="#fig_0">Fig. 10</ref> results of SEAM Match-RCNN for the Regular-MovingFashion dataset are shown. Here, on street frames which exhibit more regularities, the shop items are vice versa more insidious than the TikTok ones, since they exhibit a lower variability, see for example the black female dresses of row 6. The same rationale holds for the white shirts and the black paints.</p><p>Finally, on <ref type="figure" target="#fig_0">Fig. 11</ref> retrieval results on MultiDeepFash-ion2 are shown. Looking at the retrieval results, one can notice that shop items are way less regular/neutral than the ones on the MovingFashion (which anyway represent a more genuine excerpt of an e-commerce website): at the same time, street frames are often zoomed captures of the object of interest, in general offering a retrieval challenge different than the one on MovingFashion. The strong results obtained by SEAM Match-RCNN prove its versatility in working on a broader set of scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Future perspectives</head><p>With SEAM Match-RCNN we showed how the contribution of multiple frames can boost the retrieval accuracy by 33% on MultiDeepFashion2 w.r.t Single-frame approaches and by 69% on the MovingFashion dataset. We also obtained new, state-of-the-art results on all of the benchmarks. Still, much progress has to be made in order to present a new product to the market: looking at the results, the probability of finding the correct shop match within the top 20 ranked shop images is 87% on TikTok/Instagram videos. In order to connect all the dots available within the data, one has to exploit all of the details of the clothing items shown in some of the frames, something which we are currently not able to perform (in fact, we are discarding them with low attention), because they cannot be mapped to the general layout of the clothing item. Therefore, we should probably consider 3D atlases and have a common reference there.</p><p>This setup can be attractive for many scenarios, for example: 1) a casual user can match a video snippet of a nice outfit he/she has captured with a gallery of products (e.g. Zalando, Amazon, etc.); 2) a fast fashion company can measure the similarity of clothing items contained in a viral video, or fashion show, with the items of its catalogue, deciding which item to promote the most; 3) Youtube videos can be automatically processed by video sharing platforms to build valuable statistics of popular outfits and discover emerging trends. <ref type="figure">Figure 9</ref>. Qualitative retrieval results of SEAM Match-RCNN for the Hard-MovingFashion dataset. On the left, we show 3 frames sampled from the 10 frames used for aggregation. On the right the shop images retrieved starting from the closest match (left). The correct matches are represented with a green border. <ref type="figure" target="#fig_0">Figure 10</ref>. Qualitative retrieval results of SEAM Match-RCNN for the Regular-MovingFashion dataset. On the left, we show 3 frames sampled from the 10 frames used for aggregation. On the right the shop images retrieved starting from the closest match (left). The correct matches are represented with a green border. <ref type="figure" target="#fig_0">Figure 11</ref>. Qualitative retrieval results of SEAM Match-RCNN for the MultiDeepFashion2 dataset. On the left, we show 3 frames sampled from the 10 frames used for aggregation. On the right the shop images retrieved starting from the closest match (left). The correct matches are represented with a green border.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>MovingFashion statistics; a) Cardinality of each clothing item class; b) Histogram of the number of frames for the street sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Architecture of our SEAM Match-RCNN system. Images are first processed by the Match-RCNN to extract bounding boxes and convolutional features. After tracking a clothing item across frames, its features are further processed by the Multi-frame Matching Head producing a final matching score between the street video sequence and the shop image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Failure cases results of SEAM Match-RCNN for the MovingFashion dataset. On the left, we show 3 frames sampled from the 10 frames used for aggregation. On the right the shop images retrieved with the corresponding rank. The correct matches are with a green border.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Plot of the SEAM Match-RCNN retrieval accuracy (yaxis) using different numbers of frames (x-axis) for aggregation. Error bars represent standard deviation of the accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative observations on the attention behaviour. On the left, for each video sequence we show the detection bounding boxes and the computed attention score. On the right the paired shop item.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>0.43 0.52 0.63 0.21 0.44 0.53 0.64 0.16 0.41 0.52 0.62 SFM-1qrt 0.25 0.53 0.66 0.77 0.29 0.58 0.71 0.82 0.15 0.37 0.51 0.63 SFM-Median 0.23 0.48 0.61 0.75 0.26 0.53 0.66 0.79 0.17 0.33 0.47 0.65 SFM-3qrt 0.21 0.47 0.60 0.72 0.24 0.53 0.66 0.77 0.13 0.29 0.42 0.57 SFM-Last 0.11 0.31 0.41 0.53 0.14 0.35 0.46 0.58 0.05 0.18 0.27 0.36 EPHN-First (2020) [13] 0.15 0.34 0.44 0.53 0.16 0.36 0.46 0.55 0.11 0.27 0.37 0.47 EPHN-1qrt 0.24 0.45 0.55 0.65 0.28 0.51 0.62 0.72 0.13 0.24 0.32 0.42 EPHN-Median 0.27 0.49 0.58 0.66 0.32 0.57 0.67 0.74 0.10 0.24 0.32 0.42 EPHN-3qrt 0.24 0.47 0.55 0.65 0.29 0.55 0.64 0.74 0.09 0.21 0.29 0.40 EPHN-Last 0.17 0.33 0.41 0.49 0.20 0.39 0.47 0.56 0.07 0.15 0.19 0.27 KPM-First (2019) [11] 0.19 0.40 0.51 0.61 0.22 0.45 0.56 0.67 0.09 0.26 0.33 0.45 KPM-1qrt 0.27 0.48 0.60 0.71 0.32 0.56 0.69 0.80 0.12 0.24 0.33 0.45 KPM-Median 0.24 0.48 0.59 0.69 0.27 0.55 0.67 0.78 0.12 0.25 0.35 0.43 KPM-3qrt 0.23 0.46 0.56 0.69 0.27 0.53 0.65 0.76 0.10 0.22 0.28 0.39 KPM-Last 0.16 0.35 0.45 0.55 0.20 0.41 0.53 0.65 0.05 0.14 0.19 0.23 SEAM Match-RCNN 0.49 0.80 0.89 0.94 0.55 0.86 0.94 0.97 0.30 0.62 0.76 0.87 Table 7. SEAM Match-RCNN retrieval results on MovingFashion compared with Single-frame approaches. Note: T-K means Top-K Accuracy. 0.59 0.72 0.83 0.31 0.63 0.76 0.86 0.21 0.46 0.60 0.71 Max Matching 0.26 0.60 0.74 0.85 0.29 0.65 0.79 0.89 0.17 0.44 0.58 0.74 Average Match-RCNN [1] 0.39 0.73 0.84 0.91 0.43 0.79 0.88 0.94 0.24 0.56 0.70 0.81 Average Descriptor 0.37 0.72 0.86 0.93 0.42 0.78 0.90 0.95 0.21 0.57 0.75 0.85 EPHN-MaxConf (2020) [13] 0.22 0.43 0.55 0.65 0.26 0.50 0.61 0.71 0.10 0.22 0.34 0.44 EPHN-MaxMatching 0.35 0.59 0.67 0.74 0.42 0.68 0.76 0.81 0.14 0.32 0.41 0.52 EPHN-AvgMatching 0.31 0.55 0.64 0.73 0.37 0.64 0.73 0.81 0.11 0.28 0.37 0.48 EPHN-AvgDescriptor 0.22 0.43 0.52 0.61 0.26 0.49 0.58 0.68 0.10 0.24 0.33 0.43 KPM-MaxConf (2019) [11] 0.25 0.47 0.57 0.68 0.30 0.54 0.65 0.77 0.11 0.25 0.32 0.43 KPM-MaxMatching 0.30 0.54 0.66 0.75 0.36 0.61 0.73 0.82 0.13 0.32 0.42 0.53 KPM-AvgMatching 0.34 0.58 0.68 0.77 0.40 0.68 0.78 0.86 0.15 0.28 0.38 0.48 KPM-AvgDescriptor 0.34 0.58 0.69 0.77 0.40 0.68 0.78 0.86 0.15 0.28 0.38 0.48 SEAM Match-RCNN 0.49 0.80 0.89 0.94 0.55 0.86 0.94 0.97 0.30 0.62 0.76 0.87</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparison of Video-2-shop datasets. n.a. stands for not available.</figDesc><table><row><cell>Dataset</cell><cell>#Videos</cell><cell>#Traject.</cell><cell>#FramesXVideo [Avg.]</cell><cell>#Shops</cell><cell>[W, H]</cell><cell cols="2">#Pairs Wild</cell><cell cols="3">Occlusion Crowd Available</cell></row><row><cell>AsymNet [1]</cell><cell>526</cell><cell>26k</cell><cell>n.a.</cell><cell>85k</cell><cell>n.a.</cell><cell>39k</cell><cell>n.a.</cell><cell>n.a.</cell><cell>n.a.</cell><cell>?</cell></row><row><cell>DPRNet [15]</cell><cell>818</cell><cell>5k</cell><cell>n.a.</cell><cell>21k</cell><cell>n.a.</cell><cell>n.a.</cell><cell>n.a.</cell><cell>n.a.</cell><cell>n.a.</cell><cell>?</cell></row><row><cell>MovingFashion</cell><cell>15k</cell><cell>15k</cell><cell>390</cell><cell>14k</cell><cell>[631? 12 , 770? 21]</cell><cell>15k</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc></figDesc><table /><note>Top-1 retrieval accuracy (and standard deviation) on MovingFashion for the 14 different item classes.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 .</head><label>6</label><figDesc></figDesc><table /><note>Video-to-Shop retrieval results on MultiDeepFashion2. Note: T-K means Top-K Accuracy.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 .</head><label>8</label><figDesc>SEAM Match-RCNN retrieval results on MovingFashion compared with Multi-frame approaches. Note: T-K means Top-K Accuracy. focus on the difference between the Single-frame Matching Head and the Multi-frame Matching Head.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 .</head><label>9</label><figDesc></figDesc><table><row><cell>SEAM Match-RCNN retrieval results on MultiDeep-</cell></row><row><cell>Fashion2 compared with Multi-frame approaches. Note: T-K</cell></row><row><cell>means Top-K Accuracy.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://www.net-a-porter.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://instaloader.github.io/ 6 https://github.com/umbertogriffo/ fast-near-duplicate-image-search</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://github.com/ORB-HD/deface</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>This work is partially supported by the Italian MIUR through PRIN 2017 -Project Grant 20172BH297: I-MALL -improving the customer experience in stores by intelligent computer vision, and by the project of the Italian Ministry of Education, Universities and Research (MIUR) "Dipartimenti di Eccellenza 2018-2022". Thanks also to Giovanni Masotto for the collaboration in creating MovingFashion.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video2shop: Exact matching clothes in videos to online shopping images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhi-Qi</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4048" to="4056" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The youtube marketing communication effect on cognitive, affective and behavioural attitudes among generation z consumers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodney</forename><surname>Duffett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sustainability</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">5075</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Revisiting temporal modeling for video-based person reid</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiyang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ram</forename><surname>Nevatia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.02104</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deepfashion2: A versatile benchmark for detection, pose estimation, segmentation and re-identification of clothing images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuying</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruimao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5337" to="5345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Facebook video ads: Best practices for</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Golling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Where to buy it: Matching street clothing photos in online shops</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xufeng</forename><surname>Hadi Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3343" to="3351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Diversity regularized spatiotemporal attention for videobased person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Slawomir</forename><surname>Bak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="369" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatially and temporally efficient non-local attention network for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chih-Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Chiang Frank</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shao-Yi</forename><surname>Chien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deepfashion: Powering robust clothes recognition and retrieval with rich annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1096" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Robust re-identification by multiple views knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelo</forename><surname>Porrello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Bergamini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Calderara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Person re-identification with deep kronecker-product matching and group-shuffling random walk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yantao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Non-local neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7794" to="7803" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Improved embeddings with easy positive triplet mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hong</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abby</forename><surname>Stylianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Pless</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Winter Conference on Applications of Computer Vision</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2474" to="2482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning multi-granular hypergraphs for video-based person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ling</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Dress like an internet celebrity: Fashion retrieval in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongrui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the International Joint Conferences on Artificial Intelligence</title>
		<meeting>the International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<biblScope unit="volume">07</biblScope>
			<biblScope unit="page" from="1054" to="1060" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
