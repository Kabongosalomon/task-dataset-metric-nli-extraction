<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SberQuAD -Russian Reading Comprehension Dataset: Description and Analysis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Efimov</surname></persName>
							<email>pavel.vl.efimov@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Saint Petersburg State University</orgName>
								<address>
									<settlement>Saint Petersburg</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Chertok</surname></persName>
							<email>achertok@sberbank.ru</email>
							<affiliation key="aff1">
								<orgName type="department">Sberbank</orgName>
								<address>
									<settlement>Moscow</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonid</forename><surname>Boytsov</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pavel</forename><surname>Braslavski</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Ural Federal University</orgName>
								<address>
									<settlement>Yekaterinburg</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">JetBrains Research</orgName>
								<address>
									<settlement>Saint Petersburg</settlement>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SberQuAD -Russian Reading Comprehension Dataset: Description and Analysis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>reading comprehension</term>
					<term>question answering</term>
					<term>Russian language resources</term>
					<term>evaluation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>SberQuAD-a large scale analog of Stanford SQuAD in the Russian language-is a valuable resource that has not been properly presented to the scientific community. We fill this gap by providing a description, a thorough analysis, and baseline experimental results.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>On September 14, 2017 a data science department of Sberbank 1 -the largest financial institution in Russiaannounced a question answering (QA) challenge with substantial monetary prizes. For this competition Sberbank provided a new large Russian QA dataset containing about 50K training examples, 15K development, and 25K testing examples (see ? 3. for a detailed description). It was created similarly to the Stanford Question Answering Dataset (SQuAD) <ref type="bibr" target="#b22">(Rajpurkar et al., 2016)</ref>, which is reflected in its name SberQuAD (Sberbank Question Answering Dataset). The competitions had two tasks: retrieval of answer-bearing paragraphs and a reading comprehension (RC) task, which is the focus of this study. Despite high participation-during a 1.5-month competition 120 participants made 1,348 submissions-the dataset and the contest were neither properly documented nor presented to the scientific community: We were able to find only two studies using SberQuAD <ref type="bibr" target="#b17">(Kuratov and Arkhipov, 2019;</ref><ref type="bibr" target="#b27">Soboleva and Vorontsov, 2019)</ref>. Given the importance of the RC task, the scarcity of non-English resources, and the amount of effort went into creation of SberQuAD, it is important to fill the gap. We in turn provide a historical overview, a description, an in-depth analysis, and baseline experimental results for SberQuAD (using methods previously applied to SQuAD). We believe this is an important contribution to research in multilingual QA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>QA tasks for unstructured data are typically divided into open-domain <ref type="bibr" target="#b21">(Prager, 2006)</ref> and story comprehension tasks <ref type="bibr" target="#b14">(Hirschman and Gaizauskas, 2001)</ref>. In the open-domain setting, to answer a question the system first needs to guess which documents may contain answers. The modern history of open-domain QA starts from TREC challenges organized by NIST in 2000s <ref type="bibr" target="#b7">(Dang et al., 2007)</ref> and extended by CLEF to a multilingual setting <ref type="bibr" target="#b10">(Giampiccolo et al., 2008)</ref>. Notably,</p><p>in 2011 the open-domain system IBM Watson outstripped two human champions in the QA contest Jeopardy! <ref type="bibr" target="#b9">(Ferrucci et al., 2010)</ref>. The story comprehension-commonly referred to as reading comprehension (RC)-is a more restricted task, where the system needs to answer questions for a given document. This task has recently become quite popular with the introduction of a large scale RC dataset named SQuAD <ref type="bibr" target="#b22">(Rajpurkar et al., 2016)</ref>, which was created by crowd workers. The dataset contains more than 100K questions posed to paragraphs from popular Wikipedia articles. An answer to each question should be a valid and relevant paragraph phrase, i.e., a contiguous sequence of paragraph words including but not limited to named entities and noun phrases. The second version of SQuAD (SQuAD 2.0) contains a number of unanswerable questions <ref type="bibr" target="#b23">(Rajpurkar et al., 2018</ref>). This makes the task more difficult as the system needs to figure out when an answer does not exist. Wide adoption of SQuAD led to emergence of many datasets. TriviaQA <ref type="bibr" target="#b16">(Joshi et al., 2017)</ref> consists of 96K trivia game questions and answers found online accompanied by answer-bearing documents. Natural Questions dataset <ref type="bibr" target="#b18">(Kwiatkowski et al., 2019)</ref> is approximately three times larger than SQuAD. In that, unlike SQuAD, questions are sampled from Google search log rather than generated by crowd workers. MS MARCO <ref type="bibr" target="#b1">(Bajaj et al., 2016)</ref> contains 1M questions from a Bing search log along with free-form answers. For both MS MARCO and Natural Questions answers are produced by in-house annotators. QuAC <ref type="bibr" target="#b5">(Choi et al., 2018)</ref> and CoQA <ref type="bibr" target="#b24">(Reddy et al., 2019)</ref> contain questions and answers in information-seeking dialogues. For a more detailed discussion we address the reader to a recent survey <ref type="bibr" target="#b34">(Zhang et al., 2019)</ref>. Majority of RC dataset are in English. Few exceptions are Chinese datasets WebQA <ref type="bibr" target="#b19">(Li et al., 2016)</ref> and DuReader <ref type="bibr" target="#b12">(He et al., 2017)</ref>, as well as Bulgarian <ref type="bibr" target="#b11">(Hardalov et al., 2019)</ref> and Tibetian <ref type="bibr" target="#b29">(Sun and Xia, 2019)</ref>  Q11870 When did the term "computer science" appear? Q28900 ??? ??????? ??????????? ???? ??????? Q28900 Who was the first to use this term? Q30330 ??????? ? ????? ???????? ????????? ????? ??????????? ??????? ?????????, ????????? ? ????????????? Q30330 Starting with wich university were computer science programs created? <ref type="figure">Figure 1</ref>: A sample SberQuAD entry (both the original and the translation): answers are underlined and colored. The word which in Q30330 is misspelled on purpose to reflect the fact that the original has a misspelling.</p><p>into 10 languages. A number of studies scrutinize existing datasets to evaluate the difficulty of the task as well as robustness of the models. <ref type="bibr" target="#b3">Chen et al. (2016)</ref> sample 100 passage-questionanswer triples from CNN/Daily Mail dataset <ref type="bibr" target="#b13">(Hermann et al., 2015)</ref>, classify them manually according to difficulty into several categories, and compare performance of several models for different levels of question complexity. <ref type="bibr" target="#b15">Jia and Liang (2017)</ref> generate semi-automatically distracting sentences into paragraphs to investigate the robustness of neural reading comprehension models. <ref type="bibr" target="#b30">Talmor and Berant (2019)</ref> study how well neural network models transfer among different RC datasets. <ref type="bibr" target="#b32">Wadhwa et al. (2018)</ref> perform quantitative and qualitative analysis of four neural models on SQuAD. This work is close to ours, though our focus is on exploring the SberQuAD dataset, rather than models. <ref type="bibr" target="#b28">Sugawara et al. (2017)</ref> evaluate dataset difficulty from a human perspective. They compute readability scores and the number of skills, such as reasoning and co-reference capability, required to answer questions for six datasets. They show that SQuAD is an easy-to-answer but hard-toread dataset, which requires few skills to answer questions. <ref type="bibr" target="#b25">Rondeau and Hazen (2018)</ref> argue that human explanations are not reliable and instead estimate question complexity based on performance of several models: The fewer models can answer a question, the higher is its complexity. They find that complexity correlates with several features including the presence of named entities and the density of question words in the answer, but not with readability scores. In particular, if the answer is an named entity the question is easy in 72% of the cases as opposed to 44% of the cases when the answer is not an entity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>In this paper, we focus on task B of the Sberbank 2017 competition and the corresponding RC dataset, namely, SberQuAD. Details of the competition can be found on the competition website 2 . The original dataset comes in 2 https://github.com/sberbank-ai/ data-science-journey-2017 CSV format: A variant in SQuAD JSON format can be downloaded from the DeepPavlov QA project page. 3 A lively discussion of approaches and their effectiveness can be found in the OpenDataSciene community slack 4 . The list of top-10 best performing teams can be found in the video of the celebration ceremony. 5 As we learned from a private communication with the dataset developers, they generally followed the procedure described in the SQuAD paper <ref type="bibr" target="#b22">(Rajpurkar et al., 2016)</ref>. First they selected Wikipedia pages, split them into paragraphs, and presented paragraphs to crowd workers. For each paragraph, a crowd worker had to come up with questions that can be answered using solely the content of the paragraph. In that, an answer must have been a paragraph span, i.e., a contiguous sequence of paragraph words. The tasks were posted on Toloka 6 crowdsourcing platform. SberQuAD contains 50,364 paragraph-question-answer triples in the training set which are publicly available. A development set, which was available only during the competition, contains about 15K triples and the hold-out test containing 25K triples is kept private. There are two differences between SQuAD and SberQuAD formats. First, SberQuAD does not tell us which Wikipedia pages a paragraph belongs to. Second, each answer is represented by a string without respective starting position in the paragraph. Examples and basic statistics. <ref type="figure">Figure 1</ref> shows a sample SberQuAD paragraph with three questions: Gold-truth answers are underlined in text. Generally, the format of the question and the answers mimics that of SQuAD v1.1. Note, however, the following peculiarities: Question Q30330 contains a spelling error; Question Q28900 references prior question Q11870 and cannot, thus, be answered on its own (likely both questions were created by the same crowd worker). Basic dataset statistics is summarized in <ref type="table">Table 1</ref>: SberQuAD has about twice as fewer questions compared to SQuAD whereas the average lengths of paragraphs, questions, and answers are very similar for both datasets. Distribution of question/answer length is presented in <ref type="figure">Figure 2</ref>. There are 275 questions (0.55%) having at least 200 characters and 374 answers (0.74%) that are longer than 100 characters. Questions are substantially longer than answers. Anecdotally, very long answers and very short questions are frequently errors. For example, for question Q61603 the answer field contains a copy of the whole paragraph, while question Q76754 consists of a single word 'thermodynamics'. Because the original SberQuAD development set is not available, the original training set of SberQuAD was partitioned into a (new) training (45,328) and testing (5,036) sets by the DeepPavlov team. This is the partition that we use in our experiments: We train models on the training set and evaluate them on the testing set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of questions.</head><p>Most questions in the dataset start with either a question word or preposition: ten most common</p><formula xml:id="formula_0">starting words are ??? (what), ? (in), ??? (how), ??? (who), ????? (what adj), ????? (when), ????? (what adj), ??? (where), ??????? (how many), ?? (on).</formula><p>These starting words correspond to 62.4% of all questions.</p><p>In about 4% of the cases, an interrogative word is not among the first three words of the question, though. Manual inspection showed that in most cases these entries are declarative statements, sometimes followed by a question mark, e.g. Q15968 'famous Belgian poets?', or ungrammatical questions.</p><p>To get a better understanding of question types, we inspected questions' most common lemmatized starting bigrams (Table 11) and trigrams <ref type="table" target="#tab_3">(Table 12</ref>). In Russian, an interrogative word is often preceded by a preposition, which results in a high variability of starting n-grams: As one can see from the   . When an answer contained more than one entity, we highlighted the entity representing a key answer concept (e.g., the head noun phrase). For example 26-year-old Comtesse Sophie d'Houdetot (Q56395) was marked as PERSON. This statistics is summarized in <ref type="table" target="#tab_3">Table 2</ref>. The first column shows distribution of NEs in answers according to manual annotation. The second column shows precision/recall of the NER tool applied to sentences containing answers. The last column of the <ref type="table" target="#tab_3">Table 2</ref> reports a fraction of answers (identified by DPNER) that are NE (as opposed only a part of the answer being a NE). In total, DPNER found NEs in almost 43% of answers in the dataset. Following <ref type="bibr" target="#b22">(Rajpurkar et al., 2016)</ref>, we complemented our analysis of answers with syntactic parsing. To this end we applied the rule-based constituency parser AOT 10 to answers without detected NE. When AOT produced multiple parses, we picked the parse with the longest span. AOT parser supports a long list of phrase types (57 in total), we grouped them into conventional high-level types, which are shown in <ref type="table" target="#tab_4">Table 3</ref> 11 . Not surprisingly, noun phrases are most frequent answer types, followed by prepositional phrases. Verb phrases represent a non-negligible share of answers (7.1%), which is quite different from a traditional QA setting where answers are predominantly noun phrases <ref type="bibr" target="#b21">(Prager, 2006)</ref>. 10 http://aot.ru 11 <ref type="table" target="#tab_4">Table 3</ref> provides data for the testing set, but the distribution for the training set is quite similar. Question/paragraph similarity. We further estimate similarity between questions and paragraph sentences containing the answer: The more similar is the question to its answer's context, the simpler is the task of locating the answer. In contrast to SQuAD we refrain from syntactic parsing and rely on simpler approaches. First, we compared questions with complete paragraphs. To this end, we calculated the length of the the longest contiguous matching subsequence (LCMS) between a question and a paragraph using the difflib library. <ref type="bibr">12</ref> The last row in <ref type="table">Table 1</ref> shows that despite similar paragraph and question lengths in both SQuAD and SberQuAD, the SberQuAD questions are more similar to the paragraph text.</p><p>Second, we estimated similarity between a question and the sentence containing the answer. To ensure the accuracy of estimation, we evaluated several available tools for sentence splitting on a random sample of 100 SberQuAD paragraphs, which were manually split into 590 sentences. DeepPavlov tokenizer 13 outperformed other tools in terms of quality (P/R = 0.93/0.94) and efficiency, so we applied it to the whole train subset. Subsequently, we lemmatized the data using mystem 14 and calculated the Jaccard coefficient between a question and the sentence containing the answer. The distribution of the scores is presented in <ref type="figure" target="#fig_0">Figure 3</ref>. The mean value of the Jaccard coefficient is 0.28 (median is 0.23).</p><p>Our analysis shows that there is a substantial lexical overlap between questions and paragraph sentences containing the answer, which may indicate a heavier use of the copy-andpaste approach by crowd workers recruited for </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Employed Models</head><p>We used the following models:</p><p>? Two baselines provided by SberQuAD organizers;</p><p>? Four models, which have strong performance among models not relying on transformers. They were used in a study similar to ours <ref type="bibr" target="#b32">(Wadhwa et al., 2018</ref>);</p><p>? BERT model provided by the DeepPavlov library, which employs large pre-trained transformers <ref type="bibr" target="#b8">(Devlin et al., 2018;</ref><ref type="bibr" target="#b31">Vaswani et al., 2017)</ref>.</p><p>Preprocessing and training. We tokenized text using spaCy 16 . To initialize the embedding layer for BiDAF, DocQA, DrQA, and R-Net we use Russian case-sensitive fastText embeddings trained on Common Crawl and Wikipedia 17 . This initialization is used for both questions and paragraphs. For BiDAF and DocQA about 10% of answer strings in both training and testing sets require a correction of positions, which can be nearly always achieved automatically by ignoring punctuation (12 answers required a manual intervention). Models were trained on GPU nVidia Tesla V100 16Gb. We used default implementation settings, which are listed in   <ref type="bibr" target="#b33">Wang et al. (2017)</ref>, is a multi-layer end-toend neural network that uses a gated attention mechanism to give different levels of importance to different paragraph parts. It also uses self-matching attention for the context to aggregate evidence from the entire paragraph to refine the query-aware context representation. We use a model implementation by HKUST 19 . To increase efficiency, the implementation adopts scaled multiplicative attention instead of additive attention and uses variational dropout.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bi-Directional Attention Flow (BiDAF):</head><p>The model proposed by <ref type="bibr" target="#b26">Seo et al. (2016)</ref> takes inputs of different granularity (character, word and phrase) to obtain a query-aware context representation without previous summarization using memory-less context-to-query (C2Q) and query-to-context (Q2C) attention. We use original implementation by AI2 20 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Paragraph Reading Comprehension (DocQA):</head><p>This model, proposed by <ref type="bibr" target="#b6">Clark and Gardner (2017)</ref>, aims to answer questions based on entire documents (multiple paragraphs). If considering the given paragraph as the document, it also shows good results on SQuAD. It uses the bi-directional attention mechanism from the BiDAF and a layer of residual self-attention. We also use original implementation by AI2 21 .  cluding RC <ref type="bibr" target="#b8">(Devlin et al., 2018)</ref>. The Russian QA model is obtained by a transfer from the multilingual BERT (mBERT) with subsequent fine-tuning on the Russian Wikipedia and SberQuAD <ref type="bibr" target="#b17">(Kuratov and Arkhipov, 2019)</ref>.</p><p>Evaluation. Similar to SQuAD, SberQuAD evaluation employs two metrics to assess model performance -1) the percentage of system's answers that exactly match (EM) any of the gold standard answers and 2) the maximum overlap between the system response and ground truth answer at the token level expressed via F1 (averaged over all questions). Both metrics ignore punctuation and capitalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis of Model Performance</head><p>Main experimental results are shown in <ref type="table">Table 7</ref>. It can be seen that all the models perform worse on the Russian dataset SberQuAD than on SQuAD. In that, there is a bigger difference in exact matching scores compared to F1. For example, for BERT the F1 score drops from 91.8 to 84.8 whereas the exact match score drops from 85.1 to 66.6. The relative performance of models is consistent for both datasets, although there is a greater variability among four neural "pre-BERT" models. One explanation for lower scores is that SberQuAD has always only one correct answer, whereas SQuAD can have multiple answer variants (1.7 on the development set). Furthermore, SberQuAD contains many fewer answers that are named entities than SQuAD (13.8% vs. 52.4%), which-as we discuss below-maybe another reason for lower scores. Another plausible reason is a poorer quality of annotations: We have found a number of deficiencies including but not limited to misspellings in questions and answers. <ref type="figure" target="#fig_1">Figure 4</ref> shows the relationship between the F1 score and the question-answer similarity expressed as the Jaccard coefficient. Note that 64% of question-sentence pairs fall into first three bins. As expected, a higher value of the Jaccard coefficient corresponds to higher F1 scores (with the exception of 14 questions where Jaccard is above 0.9). 24 Furthermore, in the case of the high similarity there is only a small difference among model performance. These observations support the hypothesis that it is easier to answer questions when there is a substantial lexical overlap between a question and a paragraph sentence containing the answer. Longer questions are easier to answer too: According to <ref type="figure" target="#fig_2">Figure 5</ref>, the F1 score increases nearly monotonically with the question length. Presumably, longer questions provide more context for identifying correct answers. In contrast, dependency on the answer length is not monotonic: the F1 score first increases and achieves the maximum for 2-4 words. A one-word ground truth constitutes a harder task: missing a single correct word results in a null F1 score, whereas returning a two-word answer containing the single correct word results in only F 1 = 0.67. F1 score also decreases substantially for answers above average length. It can be explained by the fact that models are trained on the dataset where shorter answers prevail, see <ref type="table">Table 1</ref> and <ref type="figure">Figure 2</ref>. Models' average-length answers get low scores in case of longer ground truth. For example, a 4-word answer fully overlapping with a 8-word ground truth answer gets again only F 1 = 0.67. Following our analysis of the dataset, we break down model scores by the answer types. Tables 4 and 6 summarize performance of the models depending on the answers containing named entities of diffirent types.  to a high variability of contexts might contain numerals both as digits and words. Answers containing other NEs also show degraded performance -probably, again due to their higher diversity and lower counts. The scores are significantly higher when an answer is exactly a NE. This is in line with previous studies that showed that answers containing NEs are easier to answer, see for example <ref type="bibr" target="#b25">(Rondeau and Hazen, 2018)</ref>. For about 48% of the answers in the testing set that don't contain NEs we were able to derive their syntactic phrase type, see <ref type="table" target="#tab_4">Table 3</ref>. Among them, non-factoid verb phrases stand out as most difficult ones (all models perform worse on such questions). <ref type="bibr">25</ref> In contrast, answers expressed as prepositional phrases are easier to answer compared to both noun and verb phrases. Noun phrases-most common syntactic units among answers-are second-easiest structure among others to answer. However, with exception for BERT, F1 scores for noun phrases are lower than average. The models behave remarkably differently on questions with and without detected misspellings, see <ref type="table" target="#tab_10">Table 8</ref>. DrQA seems to be most sensible to misspellings: The difference in F1 is almost 8% (scores are lower for misspelled questions). DocQA has most stable behavior: The difference in F1 scores is about 2%. Questions with interrogative ??-particle represent around 1% in the whole dataset. Although score averages for such small sets are not very reliable, the decrease in performance on these questions is quite sharp and consistent for all models: It ranges from 8.5% in F1 points for DocQA to 18.7% for BiDAF. We hypothesize that these questions are substantially different from other questions and are poorly represented in the training set. Due to high variability of starting question n-grams (see <ref type="table" target="#tab_2">Tables 11 and 12)</ref>, we cannot make reliable statements for all but most frequent ones. For these-we can concludethat model performance is mostly above average. There are a few exceptions: Notably, some variants of the definition 25 Adverbial phrases appears to be even harder, but they are too few to make reliable conclusions.   questions what/who is are especially hard for BiDAF. More concrete when-questions appear to be an easier task for all models. In the case of trigrams the number of questions of each type is much smaller (recall that the testing set contains around 5,000 questions). Nevertheless, the scores for most frequent questions in which year are much better than the average scores. Finally, we sampled 100 questions where all models achieved zero F1 score (i.e., they returned a span with no overlap with a ground truth answer). We manually grouped the sampled questions into the following categories:</p><p>? An entire paragraph or its significant part can be seen as an answer to a broad/general question.</p><p>? An answer is incomplete, because it contains only a part of an acceptable longer answer. For example for Q31929 'Who did notice an enemy airplane?' only the word pilots is marked as ground truth in the context:  ? Some questions require reasoning and co-reference resolution.</p><p>? A small fraction of questions uses synonyms and paraphrases that are not directly borrowed from the paragraph.</p><p>? A relatively large fraction of 'difficult' questions contains misspellings and imply yes/no answers.</p><p>The categorization of the sample is summarized in <ref type="table" target="#tab_13">Table 10</ref>. One can see from the table that most potential causes of degraded performance can be attributed to poor data quality: Only 25% of cases can be explained by a need to deal with linguistic phenomena such as co-reference resolution, reasoning, and paraphrase detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>In this study, we conducted an in-depth analysis of the Russian reading comprehension dataset SberQuAD, which was created in 2017 but was neither properly documented nor presented to the scientific community. SberQuAD creators generally followed a procedure described by the SQuAD authors, which resulted in similarly high lexical overlap between questions and sentences with answers. Our analysis demonstrates that models perform better when such overlap is high. Despite the similarities between datasets, all the models perform worse on SberQuAD than on SQuAD, which can be attributed to having only a single answer variant and fewer answers that are named entities. Furthermore, SberQuAD annotations might have been of poorer quality, but it is hard to quantify. We believe that the provided analysis constitutes an important contribution to research in multilingual QA. It facilitates further studies by evaluating off-the-shelf models for reading comprehension task in Russian and identifying shortcomings related to dataset creation. The latter can serve as a guidance for improving/extension of the dataset in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Jaccard similarity distribution between questions and answer containing sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Model performance depending on Jaccard similarity between a question and the sentence containing an answer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Model performance depending on question length (# of words).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ones. Recently, Artetxe et al. (2019) experimented with cross-language transfer learning and prepared XQuAD dataset containing 240 paragraphs and 1,190 question-answer pairs from SQuAD v1.1 translated P6418 ?????? Computer science (???????????? ?????) ???????? ? 1959 ???? ? ??????? ??????? Communications of the ACM, ? ??????? ??? ???? (Louis Fein) ??????? ?? ???????? Graduate School in Computer Sciences (?????? ????? ? ??????? ???????????) . . . ?????? ??? ?????, ?????????? ????????? ??????? ???????? ? ?????? ?????????? ???????: ???????????? ????? ?? ???????? ????????, ????????? ? ????????????, ??????? ? ???????????? ?????? ? 1962. P6418 The term "computer science" appears in a 1959 article in Communications of the ACM, in which Louis Fein argues for the creation of a Graduate School in Computer Science . . . Louis Fein's efforts, and those of others such as numerical analyst George Forsythe, were rewarded: universities went on to create such departments, starting with Purdue in 1962. Q11870 ????? ??????? ??? ???????? ?????? Computer science ( ???????????? ????? )?</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>http://docs.deeppavlov.ai/en/master/ features/models/squad.html DeepPavlov is an open NLP framework maintained by the Neural Networks and Deep Learning Lab of Moscow Institute of Physics and Technology (MIPT), see<ref type="bibr" target="#b2">(Burtsev et al., 2018)</ref>.</figDesc><table><row><cell></cell><cell>SberQuAD</cell><cell>SQuAD 1.1</cell></row><row><cell></cell><cell>train</cell><cell>train/dev</cell></row><row><cell># questions</cell><cell cols="2">50,364 87,599 / 10,570</cell></row><row><cell># unique paragraphs</cell><cell cols="2">9,080 18,896 / 2,067</cell></row><row><cell cols="2">Number of tokens</cell><cell></cell></row><row><cell>avg. paragraph length</cell><cell>101.7</cell><cell>116.6 / 122.8</cell></row><row><cell>avg. question length</cell><cell>8.7</cell><cell>10.1 / 10.2</cell></row><row><cell>avg. answer length</cell><cell>3.7</cell><cell>3.16 / 2.9</cell></row><row><cell>avg. answer position</cell><cell>40.5</cell><cell>50.9 / 52.9</cell></row><row><cell cols="2">Number of characters</cell><cell></cell></row><row><cell>avg. paragraph length</cell><cell>753.9</cell><cell>735.8 / 774.3</cell></row><row><cell>avg. question length</cell><cell>64.4</cell><cell>59.6 / 60.0</cell></row><row><cell>avg. answer length</cell><cell>25.9</cell><cell>20.2 / 18.7</cell></row><row><cell>avg. answer position</cell><cell>305.2</cell><cell>319.9 / 330.5</cell></row><row><cell>question-paragraph LCMS</cell><cell>32.7</cell><cell>19.5 / 19.8</cell></row><row><cell cols="3">Table 1: SberQuAD statistics in the # of characters and</cell></row><row><cell cols="3">tokens. LCMS stands for the longest contiguous matching</cell></row><row><cell>subsequence.</cell><cell></cell><cell></cell></row></table><note>34 https://opendatascience.slack.com/, hashtag #sberbank_contest, September-November 2017.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 11 ,</head><label>11</label><figDesc>ten most frequent bigrams account only for about They include variations of time-related questions (in which year/century/period), location questions (in which city/country), as well as causality questions (what does X lead to/what does X depend on). Analysis of answers. While manually examining the dataset, we encountered misspelled questions. To estimate the proportion of questions with misspellings, we verified all questions using Yandex spellchecking API 7 . The automatic speller identified 2,646 and 287 misspelled questions in training and testing sets, respectively. According to a manual assessment of 200 randomly selected questions, the spellchecker has precision 0.62. 8 Manual inspection suggests that most false positives are due to either spelling/inflectional variants or rare words being replaced with more frequent ones (apparently based on language model scores). We also found 385 and 51 questions in training and testing sets, respectively, containing Russian interrogative particle ?? (whether/if ). This form implies a yes/no question, which is generally not possible to answer in the RC setting by selecting a valid and relevant paragraph phrase. For this reason, most answers for these yes-no questions are fragments supporting or refuting the question statement. In addition, we found 15 answers in the training set, where the correct answer 'yes' (Russian ??) can be found as a paragraph word substring, but not as a valid/relevant phrase. Following<ref type="bibr" target="#b22">(Rajpurkar et al., 2016)</ref>, we analyzed answers presented in the dataset by their type. To this end, we employed a NER tool from DeepPavlov library, DPNER hereafter. 9 DPNER is a multilingual BERT model trained on OntoNotes corpus annotated with 19 entity types and transferred to Russian (for a discussion of zero-shot transfer see a paper by<ref type="bibr" target="#b20">Pires et al. (2019)</ref>). To evaluate DPNER on SberQuAD data, we randomly sampled 1,000 answers and manually tagged containing named entities (NE) using the</figDesc><table><row><cell>Figure 2: Question/answer length histograms (in chars)</cell></row><row><cell>19% of all questions. Judging by bigram statistics, defini-</cell></row><row><cell>tion (what do you call/what is X) and time-related questions</cell></row><row><cell>(when) are among most popular ones. Trigram statistics</cell></row><row><cell>(Table 12) permits a more precise inference about most com-</cell></row><row><cell>mon question types:</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell cols="3">: Named entities in a manually annotated sample</cell></row><row><cell cols="3">of 1,000 answers (Manual); answers containing automati-</cell></row><row><cell cols="3">cally detected NEs (DPNER); detection quality on manually</cell></row><row><cell cols="3">annotated sample (P/R); automatically detected NEs that</cell></row><row><cell cols="3">exactly match answers' boundaries (Exact).</cell></row><row><cell cols="3">Type % test R-Net BiDAF DocQA DrQA BERT</cell></row><row><cell>NP</cell><cell>24.0 77.5 70.3</cell><cell>78.2 73.5 84.5</cell></row><row><cell>PP</cell><cell>10.5 83.1 78.6</cell><cell>84.9 81.4 89.1</cell></row><row><cell>VP</cell><cell>7.1 61.9 54.0</cell><cell>62.7 55.5 71.6</cell></row><row><cell>ADJP</cell><cell>5.9 73.0 65.3</cell><cell>75.5 67.2 80.5</cell></row><row><cell>ADVP</cell><cell>0.3 67.9 45.3</cell><cell>70.7 51.2 76.6</cell></row><row><cell>non-R</cell><cell>0.3 91.7 88.2</cell><cell>98.2 92.9 95.1</cell></row><row><cell>None</cell><cell>9.1 75.7 69.0</cell><cell>77.1 70.1 83.0</cell></row><row><cell>Test set</cell><cell>77.8 72.2</cell><cell>79.5 75.0 84.8</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table /><note>Distribution of answers by constituent types (NP - noun phrase, PP -prepositional phrase, VP -verb phrase, ADJP -adjective phrase, ADVP -adverb phrase, non-R - words in non-Russian characters; None -not recognized).following tags: DATE, NUMBER, PERSON, LOCATION, ORGANIZATION, and OTHER (artwork, TV show, etc.)</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell>:</cell><cell></cell><cell></cell></row><row><cell>Model Optim. Batch</cell><cell># epochs</cell><cell>Init. LR</cell></row><row><cell cols="3">R-Net Adadelta 32 40 (60K steps) 0.5</cell></row><row><cell>BiDAF Adadelta 60</cell><cell>12</cell><cell>0.5</cell></row><row><cell>DocQA Adadelta 45</cell><cell>26</cell><cell>1</cell></row><row><cell>DrQA Adamax 128</cell><cell>40</cell><cell>N/A</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Training parameters. LR stands for learning rate.Baselines. Contest organizers made two baselines 18 available. Simple baseline: The model returns a sentence with the maximum word overlap with the question. ML baseline generates features for all word spans in the sentence returned by the simple baseline. The feature set includes TF-IDF scores, span length, distance to the beginning/end of the sentence, as well as POS tags. The model uses gradient boosting to predict F1 score. At the testing stage the model selects a candidate span with maximum predicted score.</figDesc><table><row><cell>crawl-vectors.html</cell></row><row><cell>18 https://github.com/sberbank-ai/</cell></row><row><cell>data-science-journey-2017/tree/master/</cell></row><row><cell>problem_B/</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell>represents answers</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Answer quality for misspelled questions.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 9 :</head><label>9</label><figDesc>Yes/no (??-particle) questions.</figDesc><table><row><cell>Category</cell><cell>%</cell></row><row><cell>Incomplete answer</cell><cell>29</cell></row><row><cell>Vague question</cell><cell>19</cell></row><row><cell>Incorrect answer</cell><cell>14</cell></row><row><cell>Broad question</cell><cell>12</cell></row><row><cell cols="2">Co-reference resolution 12</cell></row><row><cell>Reasoning</cell><cell>10</cell></row><row><cell>Misspellings</cell><cell>6</cell></row><row><cell>No answer</cell><cell>3</cell></row><row><cell>Yes/no</cell><cell>3</cell></row><row><cell>Paraphrase</cell><cell>3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 :</head><label>10</label><figDesc>Qualitative analysis of 100 difficult questions (questions can be assigned to more than one category).</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>Table 11: Model F1 scores depending on questions' leading bigrams (bigrams are lemmatized).</figDesc><table><row><cell>Bigram</cell><cell cols="7">% test R-Net BiDAF DocQA DrQA BERT</cell></row><row><cell>? ????? / in what</cell><cell></cell><cell>8.62</cell><cell>84.2</cell><cell>82.7</cell><cell>85.8</cell><cell>84.6</cell><cell>87.7</cell></row><row><cell>??? ?????????? / how is X called</cell><cell></cell><cell>2.46</cell><cell>84.5</cell><cell>74.7</cell><cell>81.9</cell><cell>78.7</cell><cell>89.8</cell></row><row><cell>??? ???? / who was</cell><cell></cell><cell>1.21</cell><cell>81.8</cell><cell>71.0</cell><cell>83.2</cell><cell>78.3</cell><cell>89.2</cell></row><row><cell>?? ????? / on what</cell><cell></cell><cell>1.21</cell><cell>75.9</cell><cell>72.7</cell><cell>76.7</cell><cell>78.0</cell><cell>80.2</cell></row><row><cell>??? ????? / what is</cell><cell></cell><cell>1.15</cell><cell>71.6</cell><cell>67.6</cell><cell>74.4</cell><cell>70.6</cell><cell>77.0</cell></row><row><cell>? ????? / with what</cell><cell></cell><cell>1.01</cell><cell>76.6</cell><cell>78.3</cell><cell>79.4</cell><cell>78.4</cell><cell>89.9</cell></row><row><cell>??? ??? / what for</cell><cell></cell><cell>0.91</cell><cell>81.6</cell><cell>79.8</cell><cell>82.5</cell><cell>78.1</cell><cell>86.9</cell></row><row><cell>? ??? / to what</cell><cell></cell><cell>0.77</cell><cell>90.9</cell><cell>82.2</cell><cell>86.7</cell><cell>88.1</cell><cell>90.2</cell></row><row><cell>??? ???????? / what is</cell><cell></cell><cell>0.69</cell><cell>84.6</cell><cell>88.0</cell><cell>93.5</cell><cell>87.2</cell><cell>93.2</cell></row><row><cell>????? ???? / when was</cell><cell></cell><cell>0.68</cell><cell>79.2</cell><cell>82.2</cell><cell>84.0</cell><cell>86.9</cell><cell>92.5</cell></row><row><cell>Test set</cell><cell></cell><cell></cell><cell>77.8</cell><cell>72.2</cell><cell>79.5</cell><cell>75.0</cell><cell>84.8</cell></row><row><cell>Trigram</cell><cell></cell><cell cols="6">% test R-Net BiDAF DocQA DrQA BERT</cell></row><row><cell>? ????? ??? / in which year</cell><cell></cell><cell>4.39</cell><cell>89.4</cell><cell>88.8</cell><cell>90.4</cell><cell>89.9</cell><cell>91.0</cell></row><row><cell>? ????? ????? / in which city</cell><cell></cell><cell>0.32</cell><cell>87.0</cell><cell>88.5</cell><cell>87.0</cell><cell>83.8</cell><cell>92.6</cell></row><row><cell>??? ???????????? ???? / what is</cell><cell></cell><cell>0.30</cell><cell>58.5</cell><cell>46.3</cell><cell>51.8</cell><cell>52.3</cell><cell>58.5</cell></row><row><cell cols="2">??? ??????????? ? / what does happen to</cell><cell>0.28</cell><cell>64.6</cell><cell>58.6</cell><cell>78.2</cell><cell>64.1</cell><cell>86.8</cell></row><row><cell>? ????? ??? / starting from which year</cell><cell></cell><cell>0.28</cell><cell>93.9</cell><cell>93.9</cell><cell>93.9</cell><cell>93.9</cell><cell>93.9</cell></row><row><cell>? ????? ??? / in which century</cell><cell></cell><cell>0.26</cell><cell>87.7</cell><cell>89.2</cell><cell>90.8</cell><cell>86.9</cell><cell>90.8</cell></row><row><cell>? ????? ?????? / in which period</cell><cell></cell><cell>0.26</cell><cell>86.8</cell><cell>83.9</cell><cell>88.1</cell><cell>82.1</cell><cell>86.8</cell></row><row><cell>? ??? ????????? / what does X lead to</cell><cell></cell><cell>0.24</cell><cell>83.6</cell><cell>72.0</cell><cell>75.9</cell><cell>79.7</cell><cell>70.8</cell></row><row><cell cols="2">?? ??? ???????? / what does X depend on</cell><cell>0.20</cell><cell>78.8</cell><cell>73.6</cell><cell>79.2</cell><cell>84.0</cell><cell>92.5</cell></row><row><cell>? ????? ?????? / in which country</cell><cell></cell><cell>0.18</cell><cell>97.8</cell><cell>97.8</cell><cell>91.3</cell><cell>94.4</cell><cell>100.0</cell></row><row><cell>Test set</cell><cell></cell><cell></cell><cell>77.8</cell><cell>72.2</cell><cell>79.5</cell><cell>75.0</cell><cell>84.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">On July 15, during a reconnaissance east to Zolotaya</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Lipa, pilots of the 2nd Siberian Corps Air Squadron</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Lieutenant Pokrovsky and Cornet Plonsky noticed an</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">enemy airplane.</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">? Vague questions are related to the corresponding para-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">graph but seem to be a result of a misinterpretation of</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">the context by a crowdsource worker. For example, in</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">Q70465 'What are the disadvantages of TNT compar-</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">ing to dynamite and other explosives?' the ground truth</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">answer 'a detonator needs to be used' is not mentioned</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">as a disadvantage in the paragraph. A couple of these</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">questions use paronyms of concepts mentioned in the</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">paragraph. For example, Q46229 asks about 'discrete</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="5">policy', while the paragraph mentions 'discretionary</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">policy'.</cell><cell></cell><cell></cell></row></table><note>? No answer in the paragraph and incorrect answer con- stitute more straightforward error cases.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 12 :</head><label>12</label><figDesc>Model F1 scores depending on questions' leading trigrams (trigrams are lemmatized)</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://youtu.be/J5HOjC4Xn_Y?t=29830 6 https://toloka.yandex.com</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">https://yandex.ru/dev/speller/ (in Russian) 8 This score is significantly lower than scores obtained for Yandex spellchecker by DeepPavlov (P = 83.09; R = 59.86), see http://docs.deeppavlov.ai/en/master/ features/models/spelling_correction.html 9 http://docs.deeppavlov.ai/en/master/ features/models/ner.html</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">https://docs.python.org/3/library/ difflib.html 13 https://github.com/deepmipt/ru_sentence_ tokenizer 14 https://yandex.ru/dev/mystem/ (in Russian)15  Note that in the interface for crowdsourcing SQuAD questions, prompts at each screen reminded the workers to formulate questions in their own words; in addition, the copy-paste functionality for the paragraph was purposefully disabled.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="16">https://github.com/buriy/spacy-ru 17 https://fasttext.cc/docs/en/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="24">Among these 14 questions the majority are long sentences from the paragraph with a single word (answer) substituted by a question word; there is an exact copy with just a question mark at the end; one question has the answer erroneously attached after the very question.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Peter Romov, Vladimir Suvorov, and Ekaterina Artemova (Chernyak) for providing us with details about SberQuAD preparation. We also thank Natasha Murashkina for initial data processing.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">On the cross-lingual transferability of monolingual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogatama</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1910.11856</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bajaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcnamara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mitra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Stoica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09268</idno>
		<title level="m">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deeppavlov: Open-source library for dialogue systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burtsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seliverstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Airapetyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arkhipov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Baymurzina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bushkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gureenkova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khakhulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuratov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kuznetsov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL 2018, System Demonstrations</title>
		<meeting>ACL 2018, System Demonstrations</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="122" to="127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A thorough examination of the CNN/Daily Mail reading comprehension task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bolton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.02858</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00051</idno>
		<title level="m">Reading wikipedia to answer open-domain questions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">QuAC: Question Answering in Context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Simple and effective multi-paragraph reading comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gardner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10723</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Overview of the TREC 2007 Question Answering Track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Sixteenth Text REtrieval Conference</title>
		<meeting>The Sixteenth Text REtrieval Conference</meeting>
		<imprint>
			<publisher>TREC</publisher>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building Watson: An Overview of the DeepQA Project</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Ferrucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chu-Carroll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gondek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalyanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lally</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Prager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Schlaefer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Welty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="59" to="79" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Overview of the CLEF 2007 Multilingual Question Answering Track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Giampiccolo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Forner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Herrera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pe?as</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ayache</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Forascu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jijkoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Osenova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sacaleanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sutcliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Multilingual and Multimodal Information Retrieval</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="200" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Beyond English-Only Reading Comprehension: Experiments in Zero-Shot Multilingual Transfer for Bulgarian</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hardalov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Koychev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Nakov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.01519</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>She</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.05073</idno>
		<title level="m">DuReader: a Chinese machine reading comprehension dataset from real-world applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Teaching machines to read and comprehend</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Hermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kocisky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Grefenstette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1701" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Natural language question answering: the view from here</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hirschman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Gaizauskas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="275" to="300" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adversarial examples for evaluating reading comprehension systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07328</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">TriviaQA: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adaptation of deep bidirectional multilingual transformers for russian language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kuratov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arkhipov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.07213</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kwiatkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Palomaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Redfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="453" to="466" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Dataset and neural recurrent sequence labeling model for open-domain factoid question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06275</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">How multilingual is multilingual bert?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Schlinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garrette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019</title>
		<meeting>the 57th Conference of the Association for Computational Linguistics, ACL 2019<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2019-07-28" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4996" to="5001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Open-domain question-answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Prager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="231" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Squad: 100,000+ questions for machine comprehension of text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lopyrev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.05250</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.03822</idno>
		<title level="m">Know what you don&apos;t know: Unanswerable questions for squad</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">CoQA: A Conversational Question Answering Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="249" to="266" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Systematic Error Analysis of the Stanford Question Answering Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-A</forename><surname>Rondeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Hazen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Machine Reading for Question Answering</title>
		<meeting>the Workshop on Machine Reading for Question Answering</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Bidirectional attention flow for machine comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kembhavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soboleva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vorontsov</surname></persName>
		</author>
		<title level="m">Three-stage question answering system with sentence ranking</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Evaluation metrics for machine reading comprehension: Prerequisite skills and readability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sugawara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kido</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yokono</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A hybrid network model for Tibetan question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="52769" to="52777" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">MultiQA: An Empirical Investigation of Generalization and Transfer in Reading Comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Talmor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Berant</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.13453</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Comparative analysis of neural qa models on squad</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">R</forename><surname>Chandu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Nyberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.06972</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Gated self-matching networks for reading comprehension and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Machine reading comprehension: a literature review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename></persName>
		</author>
		<idno>abs/1907.01686</idno>
		<imprint>
			<date type="published" when="2019" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
