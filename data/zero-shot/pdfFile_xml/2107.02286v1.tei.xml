<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Injecting Knowledge Base Information into End-to-End Joint Entity and Relation Extraction and Coreference Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Severine</forename><surname>Verlinden</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ghent University -imec</orgName>
								<address>
									<settlement>IDLab Ghent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klim</forename><surname>Zaporojets</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ghent University -imec</orgName>
								<address>
									<settlement>IDLab Ghent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ghent University -imec</orgName>
								<address>
									<settlement>IDLab Ghent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ghent University -imec</orgName>
								<address>
									<settlement>IDLab Ghent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Ghent University -imec</orgName>
								<address>
									<settlement>IDLab Ghent</settlement>
									<country key="BE">Belgium</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Injecting Knowledge Base Information into End-to-End Joint Entity and Relation Extraction and Coreference Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T15:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We consider a joint information extraction (IE) model, solving named entity recognition, coreference resolution and relation extraction jointly over the whole document. In particular, we study how to inject information from a knowledge base (KB) in such IE model, based on unsupervised entity linking. The used KB entity representations are learned from either (i) hyperlinked text documents (Wikipedia), or (ii) a knowledge graph (Wikidata), and appear complementary in raising IE performance. Representations of corresponding entity linking (EL) candidates are added to text span representations of the input document, and we experiment with (i) taking a weighted average of the EL candidate representations based on their prior (in Wikipedia), and (ii) using an attention scheme over the EL candidate list. Results demonstrate an increase of up to 5% F1-score for the evaluated IE tasks on two datasets. Despite a strong performance of the prior-based model, our quantitative and qualitative analysis reveals the advantage of using the attention-based approach.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Information extraction (IE) comprises several subtasks, e.g., named entity recognition (NER), coreference resolution (coref), relation extraction (RE). State-of-the-art results mainly report performance on single tasks, usually solving them on a sentence level (especially NER, RE). However, in practice, IE system decisions should be consistent on the document level, e.g., when processing news articles to automatically link entities (aside from potentially learning, e.g., new relations). Yet, the challenge of solving the tasks jointly on a document level has not received as much attention and remains hard <ref type="bibr" target="#b1">(Durrett and Klein, 2014;</ref><ref type="bibr" target="#b26">Yao et al., 2019;</ref><ref type="bibr" target="#b27">Zaporojets et al., 2021)</ref>. * Equal contribution On the other hand, it is well established that IE models benefit from incorporating background information of knowledge bases (KBs). Still, so far this has been shown from the perspective of solving individual tasks such as relation classification or entity typing (e.g., <ref type="bibr" target="#b17">Peters et al. (2019)</ref>; <ref type="bibr">Liu et al. (2020)</ref>). Integrating KBs in joint models, realizing and analyzing the more complex end-to-end setting, has been left unexplored.</p><p>In terms of the nature of KBs adopted in IE, current approaches use either (i) structured knowledge graphs comprising (subj,rel,obj) triples, e.g., Wikidata <ref type="bibr" target="#b25">(Yang and Mitchell, 2017;</ref><ref type="bibr" target="#b4">Han et al., 2018;</ref><ref type="bibr" target="#b28">Zhang et al., 2019)</ref>, or (ii) textual descriptions, usually in hyperlinked documents, e.g., Wikipedia <ref type="bibr" target="#b13">(Martins et al., 2019;</ref><ref type="bibr" target="#b22">Yamada et al., 2020)</ref>. It has not been established to what extent KB-text and KB-graph entity representations complement each other in boosting IE performance.</p><p>We address both research gaps of (a) integrating KB information into a joint end-to-end IE model for solving named entity recognition, coreference resolution and relation extraction, and (b) analyzing what KB representation is more beneficial for IE, either KB-graph trained on Wikidata, or KBtext trained directly on Wikipedia. We particularly contribute: (i) a first span-based end-to-end architecture incorporating KB knowledge in a joint entity-centric setting, exploiting unsupervised entity linking (EL) to select KB entity candidates, (ii) exploration of prior-and attention-based mechanisms to combine the EL candidate representations into the model, (iii) assessment of the complementarity of KB-graph and KB-text representations, and (iv) consistent gains of up to 5% F1-score when incorporating KB knowledge in 3 document-level IE tasks evaluated on 2 different datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Figure 1 illustrates our model architecture. Input document tokens are represented using concatenated GloVe <ref type="bibr" target="#b16">(Pennington et al., 2014)</ref> and character embeddings <ref type="bibr" target="#b12">(Ma and Hovy, 2016)</ref> and pushed through a BiLSTM to obtain contextualized token representations, which are combined into spans. Similar to ; <ref type="bibr" target="#b27">Zaporojets et al. (2021)</ref>, a span pruner limits the number of spans for downstream modules. The KB module ( ?2.2) combines span representations with KB entity representations ( ?2.1), trained either on Wikidata (KBgraph) or Wikipedia (KB-text). The KB-enriched span representations then serve as input for joint predictions on downstream IE tasks ( ?2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Entity Representations</head><p>We experiment with 3 possible entity representations: KB-text, KB-graph, and concatenating both. KB-text: We follow <ref type="bibr" target="#b24">Yamada et al. (2016)</ref> to obtain the entity representations using a skip-gram architecture <ref type="bibr">(Mikolov et al., 2013a,b)</ref>, training to jointly predict (i) the linked entities (through Wikipedia hyperlinks) given the target entity, and (ii) the neighboring words for a given entity hyperlink. KB-graph: We adopt <ref type="bibr" target="#b6">Joulin et al. (2017)</ref> to train the entity embeddings directly on Wikidata triples (subj,rel,obj) by optimizing a linear classifier to predict the obj entity from the subj entity and the relation type rel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">KB module</head><p>For a span s i from token l to r, we obtain the representation g i as input to the KB module by concatenating the respective hidden LSTM states h l and h r , and an embedding ? r?l for the corresponding span width r ? l:</p><formula xml:id="formula_0">g i = [h l ; h r ; ? r?l ].</formula><p>(1)</p><p>We look up a given span s i in a dictionary built from Wikipedia, to determine its candidate entities set 1 C i , as well as the prior probability p ij for each c ij ? C i , as per <ref type="bibr">Yamada et al. (2016, ?3)</ref>.</p><p>To combine the KB candidates c ij , we either use (i) a uniform average (Uniform), (ii) the prior weights p ij (Prior), (iii) an attention scheme (Attention), or (iv) attention with prior information (AttPrior). The unnormalized attention scores for Attention and AttPrior are:</p><formula xml:id="formula_1">? Attention (s i , c ij , K) = F A ([g i ; ? K (c ij )]) (2) ? AttPrior (s i , c ij , K) = F AP [g i ; ? K (c ij ); p ij ] (3)</formula><p>where K ? {KB-text, KB-graph, both} refers to the entity representations from ?2.1, ? K returns such representation for c ij , and F * is a feed-forward neural network (FFNN). The KB representation for span s i is a weighted average of its candidates C i :</p><formula xml:id="formula_2">e K i = c ij ?C i ? ij ? ? K (c ij )<label>(4)</label></formula><p>where weights ? ij either are uniform (1/ |C i |), the prior p ij , or softmax-normalized attention scores (softmax over ? from eq. (2) or eq. (3)). The concatenation [g i ; e K i ] forms the KB-enriched representation for span s i , as input for IE modules ( ?2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Joint IE model</head><p>The joint IE model comprises 3 modules ( <ref type="figure" target="#fig_0">Fig. 1)</ref>  </p><formula xml:id="formula_3">i if ? NER (s i ) l &gt; 0.</formula><p>Coref module: We use the coreference scheme proposed by <ref type="bibr" target="#b7">Lee et al. (2017)</ref>, using a FFNN to produce scores ? coref (s i , s j ): at inference time, the highest scoring antecedent of span s j is then chosen (potentially s j itself). Indeed, to allow for singletons we accept self-references (s j , s j ) if NER predicts the span s j to be an entity. RE module: Similar to <ref type="bibr" target="#b9">Luan et al. ( , 2018</ref>, we use a FFNN to produce scores</p><formula xml:id="formula_4">? RE (s i , s j ) ? R |L R | for each pair of spans (s i , s j ), with L R the set of relation types. We accept relation l ? L R for pair (s i , s j ) if ? RE (s i , s j ) l &gt; 0.</formula><p>IE unification: Above modules make span level predictions. We obtain entity-centric predictions using the coref clusters, by assigning the union of predicted entity/relation types within a coref cluster to all its members, as do <ref type="bibr" target="#b27">Zaporojets et al. (2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experimental Setup</head><p>We evaluate our proposed models 2 on entity-centric multi-task datasets, summarized in <ref type="table" target="#tab_1">Table 1</ref>: DWIE <ref type="bibr" target="#b27">(Zaporojets et al., 2021)</ref> and DocRED <ref type="bibr" target="#b26">(Yao et al., 2019)</ref>. We report on coreference resolution (coref), NER and relation extraction (RE). For coref, we report the average of 3 common F1 scores, as implemented by Pradhan et al. <ref type="formula" target="#formula_2">(2014)</ref>: MUC <ref type="bibr" target="#b20">(Vilain et al., 1995)</ref>, B 3 (Bagga and Baldwin, 1998) and CEAF e <ref type="bibr" target="#b11">(Luo, 2005</ref> Our experiments address 2 main questions (see <ref type="figure" target="#fig_0">Fig. 1</ref>): (Q1) Which type of KB representation is most helpful for IE (KB-text, KB-graph, or both; see ?2.1)? (Q2) Which weighting scheme to use for ? (Uniform, Prior, Attention, AttPrior; see ?2.2)?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We summarize the comparison of various model choices for both DWIE and DocRED datasets in <ref type="table" target="#tab_4">Table 2</ref>. First, looking into (Q1), we note that including background information from KB-graph and KB-text significantly boosts performance compared to the Baseline without any KB. Additionally, our model outperforms the results from <ref type="bibr" target="#b27">Zaporojets et al. (2021)</ref> (not listed in the table) by about 2 percentage points F1, using the same input (GloVe) representations. Furthermore, we observe a general improvement in results when combining both representations, suggesting that a (hyper)text corpus (Wikipedia) and a knowledge graph (Wikidata) embed complementary information for raising IE performance.</p><p>Deeper analysis reveals that adding KB representations mainly benefits performance for "rare" entity types: e.g., limiting the test set to entity types that occur ?50 times in the training set for DWIE, compared to Baseline, F1 for NER goes up by +13.9 for KB-both with AttPrior, while the benefit gradually decreases for more frequently occurring entity types. For RE, we note that overall we also see a clear performance gain from adding KB information (e.g., +5.1% F1 for both KB sources with AttProp compared to Baseline for DWIE), yet the boost is not as clear for relations with fewer training instances. (The latter makes sense, since we inject KB representations of entities rather than explicitly also for relations; we leave studying adding relation embedding information for future work.)</p><p>Second, for (Q2), we note that the AttPrior scheme is the overall winner among the different EL candidate weigthing schemes. We observed that in terms of ranking EL candidates, Prior performs quite well on DWIE -for 86.5% of entity mentions it assigns the highest score to the correct EL candidate, while Attention and AttPrior achieve it for 46.2%, resp. 77.2% of the mentions -which basically confirms that DWIE has a similar entity distribution as Wikipedia. 3 Yet, it seems necessary to include alternative candidates, and   the attention-based schemes thus can correct EL mistakes of Prior, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. This correction leads to a resulting boost for the IE tasks as reported in <ref type="table" target="#tab_4">Table 2</ref>. E.g., we found that for DWIE, looking at clusters with entity mentions for which Prior makes wrong EL predictions, the AttPrior weighting scheme retrieves +3.7% more of the gold standard annotated named entities (as opposed to just +0.6% in the clusters with correct Prior EL candidates). Perfecting the EL prediction would potentially boost IE performance even more.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>As stated earlier, we studied how to integrate (i) knowledge base information into IE, and particularly (ii) end-to-end IE combining multiple tasks (NER, relation extraction, coreference resolution), while (iii) taking an entity-centric perspective, i.e., focus on making consistent decisions on the document level. For (i), integrating KB into IE has been applied for individual tasks: relation classification <ref type="bibr" target="#b18">(Poerner et al., 2020;</ref><ref type="bibr" target="#b28">Zhang et al., 2019;</ref><ref type="bibr" target="#b25">Yang and Mitchell, 2017)</ref>, entity typing <ref type="bibr" target="#b17">(Peters et al., 2019)</ref> and NER <ref type="bibr" target="#b22">(Yamada et al., 2020)</ref>. For (ii), recently span-based architectures <ref type="bibr" target="#b7">(Lee et al., 2017;</ref><ref type="bibr" target="#b2">Fei et al., 2020)</ref> have been proposed. Our work unifies the KB integration concept into such span-based IE system, in particular an entity-centric one (as per (iii)), building on <ref type="bibr" target="#b5">Jia et al. (2019)</ref>; <ref type="bibr" target="#b27">Zaporojets et al. (2021)</ref>. For the KB integration approach, we exploit entity representations trained on a hypertext corpus, as in <ref type="bibr" target="#b24">(Yamada et al., 2016;</ref><ref type="bibr" target="#b3">Ganea and Hofmann, 2017;</ref><ref type="bibr" target="#b22">Yamada et al., 2020)</ref> or learnt from a knowledge graph <ref type="bibr" target="#b25">(Yang and Mitchell, 2017;</ref><ref type="bibr" target="#b4">Han et al., 2018;</ref><ref type="bibr" target="#b28">Zhang et al., 2019)</ref>. Our results show that both offer complementary value for IE. Similarly to our work, <ref type="bibr" target="#b23">Yamada and Shindo (2019)</ref> also explore using an attention-weighted combination of entity representations, but they use it to build a full document representation (with mentions having the entities as candidates) for a text classification task. In contrast, our span-based attention model is able to "inject" knowledge in each of the mentions separately, for more fine-grained downstream IE tasks that are mention-dependent, e.g., coreference resolution, relation extraction and NER.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose an end-to-end model for joint IE (NER + relation extraction + coreference resolution) incorporating entity representations from a background knowledge base (KB), using a span-based system. We find that representations built from a knowledge graph and a hypertext corpus are complementary in boosting IE performance. To combine candidate entity representations for text spans, we explore various weighting schemes: an attention-based combination is successful in combining prior frequency information from a hypertext corpus with contextual information to identify the relevant entity, and achieves highest IE performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Joint information extraction (IE) model with addition of a knowledge base (KB) module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Illustration of EL candidate weighting: the ? weights for top candidates for "Red Planet" from the example sentence at the top. Attention-based weighting (Attention, AttPrior) correctly identify the "Mars" entity, while the Wikipedia-based Prior fails, as most of Wikipedia's "Red Planet" links refer to the film.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Named Entity Recognition (NER) input document . . . BiLSTM Span pruner KB module Coreference (coref) module Relation Extraction (RE)</head><label></label><figDesc></figDesc><table><row><cell>IE</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>uni -cation</cell><cell></cell><cell cols="2">KB module</cell><cell></cell></row><row><cell>joint IE model</cell><cell>Entity Linking (EL) dictionary E1, E2, E3</cell><cell>KB represen-E1 tations</cell><cell>entities candidate text span</cell><cell>sentation text span + weighted KB repre-</cell></row><row><cell></cell><cell>E5, E6</cell><cell>E2</cell><cell>E1</cell><cell>weighted combination</cell></row><row><cell></cell><cell>E7, E8, E9</cell><cell>E3</cell><cell>E2</cell><cell>+</cell></row><row><cell></cell><cell>. . .</cell><cell>. . .</cell><cell>E3</cell><cell>?</cell></row><row><cell></cell><cell></cell><cell>and/or</cell><cell></cell><cell>Q2: How to combine candidate</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell></cell><cell>entity representations?</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Q1: Which KB representations</cell></row><row><cell>GloVe + character embeddings for tokens</cell><cell></cell><cell></cell><cell></cell><cell>are most helpful for IE?</cell></row><row><cell>arXiv:2107.02286v1 [cs.CL] 5 Jul 2021</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>using the same KB-enriched representations [g i ; e K i ], Dataset statistics.and using a weighted combination of the 3 module losses to minimize during training. Note that NER and RE are framed as multi-label classification. NER module: We use a FFNN on each span s i to produce scores ? NER (s i ) ? R |L E | , with L E the set of possible entity types. At inference, we accept type l ? L E for span s</figDesc><table><row><cell>Dataset</cell><cell cols="3"># Entity # Entity # Relations clusters types</cell><cell># Relation types</cell></row><row><cell>DWIE</cell><cell>23,130</cell><cell>311</cell><cell>21,749</cell><cell>65</cell></row><row><cell cols="2">DocRED 98,610</cell><cell>6</cell><cell>50,503</cell><cell>96</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>0?0.2 71.7?0.5 47.0?1.4 81.9?0.3 68.5?0.3 23.5?0.6 Uniform 90.7?0.2 73.5?0.5 48.5?1.1 82.9?0.1 70.7?0.2 24.5?0.3 KB-text Attention 90.7?0.3 73.4?0.8 49.0?0.4 83.4?0.1 71.2?0.1 24.5?0.3 AttPrior 90.7?0.3 73.7?0.6 49.6?0.8 83.2?0.2 71.3?0.2 24.8?0.4</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>DWIE</cell><cell></cell><cell></cell><cell>DocRED</cell><cell></cell></row><row><cell>KB Source</cell><cell>Setup</cell><cell>Coref</cell><cell>NER</cell><cell>RE</cell><cell>Coref</cell><cell>NER</cell><cell>RE</cell></row><row><cell>-</cell><cell cols="4">Baseline 90.Prior 90.7?0.2 73.8?0.5 49.4?0.4</cell><cell cols="3">82.9?0.2 70.9?0.3 25.3?0.4</cell></row><row><cell></cell><cell cols="4">Uniform 91.0?0.3 73.6?0.4 48.0?1.2</cell><cell cols="3">83.3?0.2 71.1?0.2 24.9?0.2</cell></row><row><cell>KB-graph</cell><cell cols="4">Attention 91.2?0.3 73.9?0.5 50.1?1.1</cell><cell cols="3">83.7?0.1 71.6?0.1 25.0?0.4</cell></row><row><cell></cell><cell cols="4">AttPrior 91.3?0.2 74.6?0.3 50.5?1.0</cell><cell cols="3">83.5?0.3 71.5?0.2 25.1?0.2</cell></row><row><cell></cell><cell>Prior</cell><cell cols="3">90.8?0.3 73.6?0.6 49.6?1.1</cell><cell cols="3">83.4?0.1 71.1?0.1 25.2?0.2</cell></row><row><cell>both</cell><cell cols="4">Uniform 91.1?0.1 74.1?0.5 49.3?0.5</cell><cell cols="3">83.5?0.1 71.3?0.2 24.8?0.1</cell></row><row><cell cols="5">(KB-graph + Attention 91.2?0.3 74.3?0.6 51.3?1.3</cell><cell cols="3">83.5?0.2 71.5?0.1 24.8?0.3</cell></row><row><cell>KB-text)</cell><cell cols="4">AttPrior 91.5?0.2 75.0?0.4 52.1?1.2</cell><cell cols="3">83.6?0.2 71.8?0.3 25.7?0.7</cell></row><row><cell></cell><cell>Prior</cell><cell cols="3">90.8?0.1 73.8?0.2 49.8?1.2</cell><cell cols="3">83.2?0.1 71.2?0.1 25.1?0.3</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Main results of the experiments in F1 scores grouped by the background KB source. We report Avg. F1 scores of MUC, B 3 and CEAF e for Coref, and hard F1 metrics for NER and RE. Bold font indicates the best results for each of the different KB source types. Additionally, the best overall results are underlined.</figDesc><table><row><cell>NASA's Mars rover, "Curiosity" will [...] continue</cell></row><row><cell>exploring the surface of the Red Planet.</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We limit this to the 16 most frequent ones.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Code and models available at https://github. com/klimzaporojets/e2e-kb-ie.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">DWIE is a news article corpus.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Part of the research leading to these results has received funding from (i) the European Union's Horizon 2020 research and innovation programme under grant agreement no. 761488 for the CPN project, 4 and (ii) the Flemish Government under the programme "Onderzoeksprogramma Artifici?le Intelligentie (AI) Vlaanderen".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Algorithms for scoring coreference chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit</forename><surname>Bagga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Breck</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1998 International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference (LREC 1998)</title>
		<meeting>the 1998 International Conference on Language Resources and Evaluation Workshop on Linguistics Coreference (LREC 1998)</meeting>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="page" from="563" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A joint model for entity analysis: Coreference, typing, and linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Durrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="477" to="490" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>TACL</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Boundaries and edges rethinking: An end-to-end neural model for overlapping entity relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yafeng</forename><surname>Hao Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donghong</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ipm.2020.102311</idno>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">102311</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep joint entity disambiguation with local neural attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugen</forename><surname>Octavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Ganea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hofmann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1277</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2619" to="2629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural knowledge acquisition via mutual attention between knowledge graph and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Artificial Intelligence (AAAI 2018)</title>
		<meeting>the 2018 Conference on Artificial Intelligence (AAAI 2018)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Document-level n-ary relation extraction with multiscale representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cliff</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1370</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3693" to="3704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Fast linear model for knowledge graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10881</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d17-1018</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Haotang Deng, and Ping Wang. 2020. K-BERT: Enabling language representation with knowledge graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiruo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Artificial Intelligence (AAAI 2020)</title>
		<meeting>the 2020 Conference on Artificial Intelligence (AAAI 2020)</meeting>
		<imprint>
			<biblScope unit="page" from="2901" to="2908" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/d18-1360</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3219" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A general framework for information extraction using dynamic span graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amy</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3036" to="3046" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">On coreference resolution performance metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2005 Conference on Empirical Methods in Natural Language Processing (EMNLP 2005)</title>
		<meeting>the 2005 Conference on Empirical Methods in Natural Language Processing (EMNLP 2005)</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Annual Meeting of the Association for Computational Linguistics (ACL 2016)</title>
		<meeting>the 2016 Annual Meeting of the Association for Computational Linguistics (ACL 2016)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Joint learning of named entity recognition and entity linking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><forename type="middle">Henrique</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zita</forename><surname>Marinho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andr? Ft</forename><surname>Martins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Annual Meeting of the Association for Computational Linguistics: Student Research Workshop (ACL 2019, SRW)</title>
		<meeting>the 2019 Annual Meeting of the Association for Computational Linguistics: Student Research Workshop (ACL 2019, SRW)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page">190</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 International Conference on Learning Representations</title>
		<meeting>the 2013 International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
	<note>ICLR 2013</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 International Conference on Neural Information Processing Systems (NIPS 2013)</title>
		<meeting>the 2013 International Conference on Neural Information Processing Systems (NIPS 2013)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Logan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vidur</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1005</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">E-BERT: Efficient-yet-effective entity embeddings for BERT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nina</forename><surname>Poerner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulli</forename><surname>Waltinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hinrich</forename><surname>Sch?tze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="803" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Scoring coreference partitions of predicted mentions: A reference implementation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoqiang</forename><surname>Sameer Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marta</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Strube</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/p14-2006</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Annual Meeting of the Association for Computational Linguistics (ACL 2014)</title>
		<meeting>the 2014 Annual Meeting of the Association for Computational Linguistics (ACL 2014)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="30" to="35" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A modeltheoretic coreference scoring scheme</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Vilain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Burger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Aberdeen</surname></persName>
		</author>
		<idno type="DOI">10.3115/1072399.1072405</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1995 conference on Message understanding (MUC6, 1995)</title>
		<meeting>the 1995 conference on Message understanding (MUC6, 1995)</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="45" to="52" />
		</imprint>
	</monogr>
	<note>Dennis Connolly, and Lynette Hirschman</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Entity, relation, and event extraction with contextualized span representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulme</forename><surname>Wennberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1585</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5788" to="5793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Luke: Deep contextualized entity representations with entityaware self-attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akari</forename><surname>Asai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuji</forename><surname>Matsumoto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.523</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6442" to="6454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural attentive bag-of-entities model for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Computational Natural Language Learning</title>
		<meeting>the 2019 Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="563" to="573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint learning of the embedding of words and entities for named entity disambiguation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ikuya</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroyuki</forename><surname>Shindo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hideaki</forename><surname>Takeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshiyasu</forename><surname>Takefuji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/k16-1025</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 2016 SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 2016 SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="250" to="259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Leveraging knowledge bases in LSTMs for improving machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-1132</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Annual Meeting of the Association for Computational Linguistics (ACL 2017)</title>
		<meeting>the 2017 Annual Meeting of the Association for Computational Linguistics (ACL 2017)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1436" to="1446" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">DocRED: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deming</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lixin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Annual Meeting of the Association for Computational Linguistics (ACL 2019)</title>
		<meeting>the 2019 Annual Meeting of the Association for Computational Linguistics (ACL 2019)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="764" to="777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">DWIE: An entity-centric dataset for multi-task document-level information extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klim</forename><surname>Zaporojets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Develder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Demeester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">102563</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Long-tail relation extraction via knowledge graph embeddings and graph convolution networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ningyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shumin</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhanlin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huajun</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1306</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019)</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019)</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3016" to="3025" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
