<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
				<date>October 10-14, 2022. 2022. October 10-14, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Artificial Intelligence</orgName>
								<orgName type="department" key="dep2">School of Informatics</orgName>
								<orgName type="laboratory">Media Analytics and Computing Lab</orgName>
								<orgName type="institution">Xiamen University</orgName>
								<address>
									<postCode>361005</postCode>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohai</forename><surname>Xu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhang</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">DAMO Academy</orgName>
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>Ma</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guohai</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><forename type="middle">Ji</forename></persName>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Institute of Artificial Intelligence</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">X-CLIP: End-to-End Multi-grained Contrastive Learning for Video-Text Retrieval</title>
					</analytic>
					<monogr>
						<title level="m">Proceedings of the 30th ACM International Conference on Multimedia (MM &apos;22)</title>
						<meeting>the 30th ACM International Conference on Multimedia (MM &apos;22) <address><addrLine>Lisboa, Portugal; Lisboa, Portugal</addrLine></address>
						</meeting>
						<imprint>
							<date type="published">October 10-14, 2022. 2022. October 10-14, 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3503161.3547910</idno>
					<note>CCS CONCEPTS ? Information systems ? Video search; Novelty in informa-tion retrieval. *Work is done during internship at Alibaba Group. ?Corresponding Author.. ACM, New York, NY, USA, 13 pages. https://</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T08:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Video-text retrieval has been a crucial and fundamental task in multi-modal research. The development of video-text retrieval has been considerably promoted by large-scale multi-modal contrastive pre-training, which primarily focuses on coarse-grained or finegrained contrast. However, cross-grained contrast, which is the contrast between coarse-grained representations and fine-grained representations, has rarely been explored in prior research. Compared with fine-grained or coarse-grained contrasts, cross-grained contrast calculate the correlation between coarse-grained features and each fine-grained feature, and is able to filter out the unnecessary fine-grained features guided by the coarse-grained feature during similarity calculation, thus improving the accuracy of retrieval. To this end, this paper presents a novel multi-grained contrastive model, namely X-CLIP, for video-text retrieval. However, another challenge lies in the similarity aggregation problem, which aims to aggregate fine-grained and cross-grained similarity matrices to instance-level similarity. To address this challenge, we propose the Attention Over Similarity Matrix (AOSM) module to make the model focus on the contrast between essential frames and words, thus lowering the impact of unnecessary frames and words on retrieval results. With multi-grained contrast and the proposed AOSM module, X-CLIP achieves outstanding performance on five widely-used video-text retrieval datasets, including MSR-VTT (49.3 R@1), MSVD (50.4 R@1), LSMDC (26.1 R@1), DiDeMo (47.8 R@1) and ActivityNet (46.2 R@1). It outperforms the previous state-of-the-art by +6.3%, +6.6%, +11.1%, +6.7%, +3.8% relative improvements on these benchmarks, demonstrating the superiority of multi-grained contrast and AOSM. Code is available at https://github.com/xuguohai/X-CLIP.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Video-text retrieval (VTR) is a multi-modal task, which aims to find the most relevant video/text based on the text/video query. With the explosive growth of videos on the Internet, VTR has attracted increasing interests and served as an important role in people's daily life. Recent years have witnessed the rapid development of VTR, which is supported by a series of pre-training multi-modal models <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b43">44]</ref>, innovative retrieval methods <ref type="bibr">[3, 5, 13-15, 24, 30, 34, 35, 38, 41, 54, 58, 61, 63, 66]</ref> and video-text benchmarks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b55">56]</ref>. Recently, with great success in large-scale contrastive languageimage pre-training, VTR has also achieved great progress. Specifically, with 400M image-text pairs for training, CLIP <ref type="bibr" target="#b43">[44]</ref> can embed the images and sentences into the shared semantic space for similarity calculation. Furthermore, CLIP4Clip <ref type="bibr" target="#b37">[38]</ref> transfers the imagetext knowledge of CLIP to the VTR task, resulting in significant performance improvements on several video-text retrieval datasets. However, CLIP and CLIP4Clip embed the whole sentence and image/video into textual and visual representations, thus lacking the ability to capture fine-grained interactions. To this end, some previous works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b58">59]</ref> propose fine-grained contrastive frameworks, which consider the contrast between each word of the sentence and each frame of the video. Moreover, TACo <ref type="bibr" target="#b56">[57]</ref> introduces tokenlevel and sentence-level loss to consider both fine-grained and coarse-grained contrast. Although they have shown promising advances on the VTR task, cross-modality semantic contrast still needs to be systematically explored.</p><p>As shown in <ref type="figure">Fig. 1</ref>, a video is composed of multiple frames, and a sentence consists of several words. Video and sentence are usually redundant, which may contain some unnecessary frames or unimportant words. Concretely, given a specific video or sentence query, unnecessary frames or unimportant words refer to the candidates with low relevance to the query (i.e., light-colored frames and words in <ref type="figure">Fig. 1</ref>). However, most current works mainly focus on coarse-grained contrast <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b43">44]</ref>, fine-grained contrast <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b58">59]</ref> or both <ref type="bibr" target="#b56">[57]</ref>, which are inefficient in filtering out these unnecessary frames and words. Specifically, coarse-grained contrast calculates the similarity between video-level and sentence-level features, and A man is driving a car. man car A A is driving a</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Coarse-grained Contrast</head><p>Fine-grained Contrast Cross-grained Contrast <ref type="figure">Figure 1</ref>: X-CLIP aims for improving video-text retrieval performance via multi-grained contrastive learning, including fine-grained (frame-word), coarse-grained (video-sentence) and cross-grained (video-word, sentence-frame) contrast. The transparency of words and frames represents the degree of relevance to query.</p><p>fine-grained contrast calculates the similarity between frame-level and word-level features. To this end, we ask: How to effectively filter out unnecessary information during retrieval? To answer this question, we propose the cross-grained contrast, which calculates the similarity score between the coarse-grained features and each fine-grained feature. As shown in <ref type="figure">Fig. 1</ref>, with the help of the coarsegrained feature, unimportant fine-grained features will be filtered out and important fine-grained features will be up-weighted. However, challenges in cross-grained contrast arise from aggregating similarity matrices to instance-level similarity scores. A naive and easy method is to use Mean-Max strategy <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b58">59]</ref> to calculate the instance-level similarity score after obtaining the similarity matrix. However, the conventional Mean-Max strategy is not conducive to filtering out the unnecessary information in videos and sentences during retrieval. On one hand, Mean applies the same weight to all frames and words, so the contrast between unnecessary frames and unimportant words may harm the retrieval performance. On the other hand, Max only considers the most important frame and word, ignoring other critical frames and words. Based on the above analysis, in this paper, we propose an endto-end multi-grained contrast model, namely X-CLIP, for videotext retrieval. Specifically, X-CLIP first adopts modality-specific encoders to generate multi-grained visual and textual representations and then considers multi-grained contrast of features (i.e., videosentence, video-word, sentence-frame, and frame-word) to obtain multi-grained similarity scores, vectors, and matrices. To effectively filter out the unnecessary information and obtain meaningful instance-level similarity scores, the AOSM module of X-CLIP conducts the attention mechanism over the similarity vectors/matrices. Different from the conventional Mean-Max strategy, our proposed AOSM module dynamically considers the importance of each frame in the video and each word in the sentence, so the adverse effects of unimportant words and unnecessary frames on retrieval performance are reduced.</p><p>To validate the effectiveness of our proposed X-CLIP, we conduct extensive experiments on five widely-used video-text retrieval benchmarks and achieve significantly better performance than previous approaches. Specifically, our X-CLIP achieves 49.3 R@1 on MSR-VTT (i.e., 6.3% relative improvement, 2.9% absolute improvement over the previous state-of-the-art approach). Besides, our proposed X-CLIP achieves 50.4 R@1, 26.1 R@1, 47.8 R@1, 46.2 R@1 on the MSVD, LSMDC, DiDeMo and ActivityNet datasets, respectively, which outperforms the previous SOTA method by +6.6% (+3.1%), +11.1% (+2.6%), +6.7% (+3.0%), +3.8% (+1.7%) on relative (absolute) improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS 2.1 Vision-Language Pre-Training</head><p>With the success of self-supervised pre-training such as BERT <ref type="bibr" target="#b11">[12]</ref> in NLP, vision-language pre-training on large-scale unlabeled crossmodal data has attracted growing attention <ref type="bibr">[23, 31-33, 37, 44, 50, 51, 55, 60]</ref>. One line of work such as LXMERT <ref type="bibr" target="#b50">[51]</ref>, OSCAR <ref type="bibr" target="#b32">[33]</ref> and ALBEF <ref type="bibr" target="#b30">[31]</ref> focuses on pre-training on enormous image-text pairs data, and obtains significant improvement in a variety of vision-andlanguage tasks. To better cope with the image-text retrieval tasks, contrastive language-image pre-training methods such as CLIP <ref type="bibr" target="#b43">[44]</ref>, ALIGN <ref type="bibr" target="#b22">[23]</ref> and WenLan <ref type="bibr" target="#b18">[19]</ref> have been proposed, by leveraging billion-scale image-text pairs data from the web with a dual-stream Transformer. Due to the great advantage of CLIP for visual representation learning, some recent work such as CLIP4Clip <ref type="bibr" target="#b37">[38]</ref> has also begun to transfer the knowledge of CLIP to video-text retrieval tasks and obtained new state-of-the-art results. The other line of work such as VideoBERT <ref type="bibr" target="#b49">[50]</ref>, HERO <ref type="bibr" target="#b31">[32]</ref> and Frozen in Time <ref type="bibr" target="#b3">[4]</ref> directly collects video-text pairs data for video-language pre-training, by further considering the temporal information in videos. However, the scale of the video-language pre-training dataset is much smaller than image-text pre-training since the process of video-text dataset collection is much more expensive. In this work, we follow the line of CLIP4Clip <ref type="bibr" target="#b37">[38]</ref>, which enhances video-text retrieval by borrowing the ability of visual representation learning from contrastive image-text pre-training. Different from CLIP4Clip <ref type="bibr" target="#b37">[38]</ref>, we design a multi-grained video-text alignment function to better align the video-text semantics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Video-Text Retrieval</head><p>Video-text retrieval is a popular but challenging task, which involves cross-modal fusion of multiple modalities and additional understanding of temporal information in videos. Traditional videotext retrieval methods tend to design task-specific or modalityspecific fusion strategies for cross-modal learning from offline extracted video and text features <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b61">62]</ref>, including face recognition/object recognition/audio processing. However, they are limited by the pre-extracted single modal features, since these features are not properly learnt for the target downstream tasks. Recently, the paradigm of end-to-end video-text retrieval by training models directly from raw video/text has gained large popularity. For example, MIL-NCE <ref type="bibr" target="#b39">[40]</ref> adopts Multiple Instance Learning and Noise Contrastive Estimation for end-to-end video representation learning, which addresses visually misaligned narrations from uncurated videos. ClipBERT <ref type="bibr" target="#b29">[30]</ref> proposes to sparsely sample video clips for end-to-end training to obtain clip-level predictions, while Frozen in Time <ref type="bibr" target="#b3">[4]</ref> uniformly samples video frames and conducts end-to-end training on both image-text and videotext pairs data. CLIP4Clip <ref type="bibr" target="#b37">[38]</ref> transfers the knowledge of CLIP to end-to-end video-text retrieval and investigates three similarity calculation approaches for video-sentence contrastive learning. However, cross-grained (i.e., video-word and sentence-frame) contrast is also critical, which has rarely been explored in previous works. We propose the first work of multi-grained contrastive learning for endto-end video-text retrieval, by considering all the video-sentence, video-word, sentence-frame, and frame-word contrasts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Multi-Grained Contrastive Learning</head><p>Recently, contrastive learning <ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b17">18]</ref> has been a popular topic in deep learning community. CLIP <ref type="bibr" target="#b43">[44]</ref> implements the idea of contrastive learning based on a large number of image-text pairs, achieving outstanding performance on several multi-modal downstream tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b63">64,</ref><ref type="bibr" target="#b64">65]</ref>. To achieve fine-grained contrastive learning, FILIP <ref type="bibr" target="#b58">[59]</ref> contrasts the patch in the image with the word in the sentence, achieving fine-grained semantic alignment. TACo <ref type="bibr" target="#b56">[57]</ref> proposes token-level and sentence-level losses to include both fine-grained and coarse-grained contrasts. Although contrastive learning has been widely used in multi-modal pretraining, cross-grained contrast has rarely been explored in previous works, which is also critical for semantic alignment. Therefore, we propose a multi-grained contrastive learning method for video-text retrieval, which aims to achieve multi-grained semantic alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we elaborate each component of our proposed X-CLIP, whose architecture is shown in <ref type="figure">Fig. 2</ref>. Specifically, we first introduce how to extract the multi-grained visual and textual representations in Sec. 3.1. We then explain the multi-grained contrastive learning based on these feature representations in Sec. 3.2, which aims to obtain multi-grained contrast scores, vectors, and matrices. We also introduce how to aggregate the similarity vectors/matrices to the instance-level similarity score in Sec. 3.3. Finally, we describe the similarity calculation and objective function for video-text retrieval in Sec. 3.4 and 3.5, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature Representation</head><p>3.1.1 Frame-level Representation. For a video??V, we first sample video frames using the sampling rate of 1 frame per second (FPS). Frame encoder is used to process these frames to obtain frame-level features, which is a standard vision transformer (ViT) with 12 layers. Following the previous work <ref type="bibr" target="#b37">[38]</ref>, we initialize our frame encoder with the public CLIP <ref type="bibr" target="#b43">[44]</ref> checkpoints. The architecture of ViT is the same as the transformer <ref type="bibr" target="#b51">[52]</ref> encoder in natural language processing (NLP), except ViT introduces a visual tokenization process to convert video frames into discrete token sequences. The discrete token sequence, which is prepended with a [CLS] token, is then fed into the Transformer of ViT. The [CLS] tokens from the last layer are extracted as the frame-level features?( , ) ?V .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Visual</head><p>Representation. However,?( , ) ?V are extracted from separate frames, without considering the interaction among frames. Therefore, we further propose a temporal encoder with temporal position embedding P, which is a set of predefined parameters, to model the temporal relationship. To be specific, the temporal encoder is also a standard transformer with 3 layers, which can be formulated as:</p><formula xml:id="formula_0">V = V + P ,<label>(1)</label></formula><p>where V = [ ( ,1) , ( ,2) , ( ,3) , ..., <ref type="bibr">( , )</ref> ] is the final frame-level (fine-grained) visual features for the video?, is the number of frames in the video?. To obtain video-level (coarse-grained) visual feature ? ? R , all frame-level features of the video are averaged, which can be formulate as:</p><formula xml:id="formula_1">? = 1 ?? ( , ) .<label>(2)</label></formula><p>3.1.3 Textual Representation. Given a sentence, we directly use the text encoder of CLIP to generate the textual representation, which is also initialized by the public checkpoints of CLIP <ref type="bibr" target="#b43">[44]</ref>. Specifically, it is a transformer encoder, which consists of multihead self-attention and feed-forward networks. The transformer consists of 12 layers and 8 attention heads. The dimension of the query, key, and value features is 512. The tokenizer used in the experiment is lower-cased byte pair encoding (BPE) <ref type="bibr" target="#b47">[48]</ref> with a 49,152 vocab size. Before being fed into the text encoder, the textual token sequence is padded with [BOS] and [EOS] at the beginning and end, respectively. The sentence-level (coarse-grained) textual feature ? ? R and word-level (fine-grained) textual features T = [ ( ,1) , ( ,2) , ( ,3) , ..., <ref type="bibr">( , )</ref> ] are the outputs of the [EOS] token and corresponding word tokens from the final layer of text encoder, where is the length of the sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multi-Grained Contrastive Learning</head><p>Previous VTR works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38]</ref> focus on fine-grained and coarsegrained contrastive learning, which include video-sentence and frame-word contrasts. However, as explained in Sec. 1, cross-grained (i.e., video-word and sentence-frame) contrast is explicit to filter out the unnecessary information in the video and sentence. Therefore, different from previous works <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b58">59]</ref>, which only focus singlegrained contrast, X-CLIP is a multi-grained contrastive framework for VTR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Video-Sentence Contrast. Given the video-level representation ? ? R</head><p>and sentence-level representation ? ? R , we use matrix multiplication to evaluate the similarity between video and sentence, which can be formulated as:</p><formula xml:id="formula_2">? = ( ? ) ? ( ? ),<label>(3)</label></formula><p>where ? ? R 1 is the video-sentence similarity score. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Video-Word</head><p>Contrast. For the given video-level representation ? ? R and word-level representation vector T ? R ? , we use matrix multiplication to calculate the similarity between the video representation and each word representation, which can be represented as follows:</p><formula xml:id="formula_3">? = (T ? ) ? ,<label>(4)</label></formula><p>where ? ? R 1? is the similarity vector between video and each word in the sentence, is the length of the sentence.  <ref type="figure">Figure 2</ref>: Illustration of the proposed X-CLIP model. The input sentences are processed by the text encoder to generate coarsegrained and fine-grained textual representations. The input video is sampled into ordinal frames and these frames are fed into the frame encoder to generate frame-level representations. The frame-level representations are then fed into the temporal encoder to capture the temporal relationships. The outputs of the temporal encoder are fine-grained visual representations, and the coarse-grained visual representation is obtained by averaging all these fine-grained features. Based on these representations, we calculate the video-sentence, video-word, sentence-frame, and frame-word similarity score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Sentence-Frame</head><p>Contrast. Similar to Video-Word Contrast, we can calculate the similarity between the sentence representation ? ? R and each frame representationV ? R ? based on matrix multiplication, which can be formulated as follows:</p><formula xml:id="formula_4">? =V ? ,<label>(5)</label></formula><p>where ? ? R ?1 is the similarity vector between the sentence and each frame of a video, is the number of frames in the video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Frame-Word</head><p>Contrast. The fine-grained similarity matrix between word representations and frame representations can be also obtained using the matrix multiplication:</p><formula xml:id="formula_5">? =VT ? ,<label>(6)</label></formula><p>where ? ? R ? is the fine-grained similarity matrix, and are the number of frames and words, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Attention Over Similarity Matrix (AOSM)</head><p>To obtain the instance-level similarity, we fuse the similarity vector/matrix in Eq. 4, Eq. 5 and Eq. 6. As discussed in Sec. 1, Mean-Max strategies <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b58">59]</ref> ignore the importance of different frames and words. To address this issue, we propose the Attention Over Similarity Matrix (AOSM) module, where scores in similarity vectors/matrices will be given different weights during aggregation. Specifically, given the similarity vectors ? ? R 1? and ? ? R ?1 , we first use Softmax to obtain the weights for the similarity vector, where scores for the fine-grained features related to the query will be given high weights. Then, we aggregate these similarity scores based on the obtained weights, which can be formulated as follows:</p><formula xml:id="formula_6">? ? = ?? =1 ( ? (1, ) / ) =1 ( ? (1, ) / ) ? (1, ) ,<label>(7)</label></formula><formula xml:id="formula_7">? ? = ?? =1 ( ? ( ,1) / ) =1 ( ? ( ,1) / ) ? ( ,1) ,<label>(8)</label></formula><p>where is the temperature parameter of Softmax.</p><p>Since the fine-grained similarity matrix ? ? R ? contains the similarity scores of frames and words, we perform attention operations on the matrix twice. The first attention aims to get finegrained video-level and sentence-level similarity vectors, which can be formulated as follows:</p><formula xml:id="formula_8">= ?? =1 ( ? ( , * ) / ) =1 ( ? ( , * ) / ) ? ( , * ) ,<label>(9)</label></formula><formula xml:id="formula_9">= ?? =1 ( ? ( * , ) / ) =1 ( ? ( * , ) / ) ? ( * , ) ,<label>(10)</label></formula><p>where * represents all content in the dimension, ? R 1? and ? R ?1 are the video-level and sentence-level similarity vector, respectively. Specifically, ? R 1? shows the similarity score between the video and words in the sentence.</p><p>? R ?1 represents the similarity score between the sentence and frames in the video.</p><p>To obtain fine-grained instance-level similarity scores, we conduct the second attention operation on the video-level vector ? R 1? and sentence-level similarity vector ? R ?1 , which can be represented as follows:</p><formula xml:id="formula_10">? = ?? =1 ( (1, ) / ) =1 ( (1, ) / ) (1, ) ,<label>(11)</label></formula><formula xml:id="formula_11">? = ?? =1 ( ( ,1) / ) =1 ( ( ,1) / ) ( ,1) ,<label>(12)</label></formula><p>where ? ? R 1 and ? ? R 1 are the instance-level similarities.</p><p>We use the average value as the fine-grained similarity score:</p><formula xml:id="formula_12">? ? = ( ? + ? )/2.<label>(13)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Similarity Calculation</head><p>The similarity score ( , ) measures the semantic similarity between the two instances. Different from the previous work <ref type="bibr" target="#b37">[38]</ref> that only consider the coarse-grained contrast, our proposed X-CLIP adopt multi-grained contrast during retrieval. Therefore, the final similarity score ( , ) of X-CLIP contains multi-grained contrastive similarity scores, which can be represented as follows:</p><formula xml:id="formula_13">( , ) = ( ? + ? ? + ? ? + ? ? )/4.<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Objective Function</head><p>During training, given a batch of video-text pairs, the model will generate a ? similarity matrix. We adopt the symmetric InfoNCE loss over the similarity matrix to optimize the retrieval model, which can be formulated as:  <ref type="bibr" target="#b37">[38]</ref>, the text encoder and frame encoder of X-CLIP are initialized by the public CLIP checkpoints. We use the Adam optimizer <ref type="bibr" target="#b26">[27]</ref> to optimize the X-CLIP and decay the learning rate using a cosine schedule strategy <ref type="bibr" target="#b35">[36]</ref>. Since the parameters of the text encoder and frame encoder are initialized from the public CLIP checkpoints, we adopt different learning rates for different modules. Specifically, the initial learning rate for text encoder and frame encoder is 1e-7, and the initial learning rate for other modules is 1e-4. We set the max token length, max frame length, batch size, and the training epoch to 32, 12, 300, and 3 for MSR-VTT, MSVD, and LSMDC datasets. Since videos and captions in DiDeMo and ActivityNet are longer and more complex, we set the max token length, max frame length, and the training epoch to 64, 64, and 20. Due to the limitation of GPU memory, we also reduce the batch size of DiDeMo and ActivityNet to 64. We conduct ablation, quantitative and qualitative experiments on the MSR-VTT dataset, it is more popular and competitive compared with other datasets. The base model of X-CLIP is ViT-B/32 if not specified. In order to enhance the expression ability of the model, we adopt linear embedding during calculating the video-sentence and frame-word similarity scores, which are initialized with the identity matrices. Besides, we also use the FC layers which are initialized with the identity matrices on similarity scores to enhance the modeling ability of the model.</p><formula xml:id="formula_14">L 2 = ? 1 ?? =1 log exp ( , ) =1 exp ( , ) ,<label>(15)</label></formula><formula xml:id="formula_15">L 2 = ? 1 ?? =1 log exp ( , ) =1 exp ( , ) ,<label>(16)</label></formula><formula xml:id="formula_16">L = L 2 + L 2 .<label>(17</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Evaluation Protocols.</head><p>To evaluate the retrieval performance of our proposed model, we use recall at Rank K (R@K, higher is better), median rank (MdR, lower is better), and mean rank (MnR, lower is better) as retrieval metrics, which are widely used in previous retrieval works <ref type="bibr">[</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Performance Comparison</head><p>We compare X-CLIP against the previous works on MSR-VTT, MSVD, LSMDC, DiDeMo, and ActivityNet. X-CLIP achieves the SOTA results on all five datasets with significant improvements. For the MSR-VTT dataset, the performance comparison is shown in Tab. 1. By analyzing the table, we gain the following observations:</p><p>? Benefiting from the large-scale image-text pre-training, both CLIP4Clip and our model X-CLIP can obtain significant gains in performance compared with all the baselines. The consistent improvements verify that it is important to adopt end-to-end finetuning to realize the full potential of the image-text pretrained model on video-text retrieval. ? Compared with the strongest competitor (i.e., CLIP4Clip-seqTransf), X-CLIP obtains 49.3 R@1 (6.3% relative improvement, 2.9% absolute improvement) in the text-to-video retrieval task and 48.9 R@1 (7.7% relative improvement, 3.5% absolute improvement) in the video-to-text retrieval task by employing CLIP(ViT-B/16) as pre-trained model. This can be attributed to that our proposed cross-grained contrast and the AOSM module are critical to reducing the bad effects of unnecessary frames and unimportant words. ? Compared to all the other state-of-the-arts, our model with ViT-B/16 achieves the best performance in all metrics. Surprisingly,    that, in all variants of CLIP4Clip, we only report the performance of CLIP4Clip-MeanP and CLIP4Clip-seqTranf, because they perform better than the other two variants in consideration of experience in the previous work <ref type="bibr" target="#b37">[38]</ref> and performance comparison in Tab. 1. By analyzing these tables, we can observe that X-CLIP also achieves significant improvement on these datasets for text-to-video and video-to-text retrieval tasks. Specifically, for the text-to-video retrieval task, X-CLIP outperforms the CLIP4Clip with ViT-B/16 on R@1 by +6.6% (+3.1%), +11.1% (+2.6%), +6.7% (+3.0%), +3.8% (+1.7%) relative (absolute) improvement on aforesaid four datasets respectively. For the video-to-text retrieval task, X-CLIP obtains +5.7% (+3.6%), +12.9% (+3.0%), +1.3% (+0.6%), +5.2% (+2.3%) relative (absolute) improvement on R@1. This demonstrates that our proposed X-CLIP can achieve consistent performance improvement on several video-text retrieval datasets. More experimental results are in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To fully examine the impact of different contrastive modules, we conduct an ablation study to compare different variants of X-CLIP. As shown in Tab. 6, we gain two important observations: ? With the number of contrastive modules increasing, the retrieval performance tends to be higher. When X-CLIP is equipped with all contrastive modules, the best retrieval performance can be achieved. This may be because each contrastive module plays a different role in the retrieval task and different contrast modules can promote each other to achieve better retrieval results. ? Our proposed cross-grained contrast can assist fine-grained contrast or coarse-grained contrast to achieve better performance in the retrieval task. Specifically, X-CLIP with the sentence-video contrast module (i.e., Exp1) only achieves 43.0 R@1 in the textto-video retrieval task. However, when X-CLIP is additionally equipped with cross-grained contrast modules (i.e., Exp8 and Exp9), the performance gets obvious absolute improvements of 2.4% and 1.0% respectively. Similarly, when X-CLIP is only equipped with fine-grained and coarse-grained contrast modules (i.e., Exp10), it achieves 44.8 R@1 in the text-to-video task. However, when it is additionally equipped with cross-grained contrast modules (i.e., Exp13 and Exp14), 1.0% and 0.7% absolute improvement of R@1 can be achieved. Therefore, we conclude that the performance improvement of cross-grained contrast modules in the retrieval task does not conflict with that of coarse-grained and fine-grained contrast modules.</p><p>To justify the effectiveness of the proposed AOSM module, we compare our method with the conventional Mean-Max and other variants (i.e., Max-Max, Max-Mean and Mean-Mean). As shown in Tab. 7, we observe that the Mean-Mean strategy performs worst. This may be because the Mean-Mean strategy, which applies the same weight to all similarity scores during aggregating, can not eliminate the adverse effects of unnecessary frames and unimportant words on the retrieval results. The Max-Mean, Mean-Max and Max-Max strategies perform better than the Mean-Mean strategy. This can be attributed to that these strategies adopt the highest similarity during aggregation, so contrast scores between unnecessary frames and unimportant words will be filtered out. However, since these strategies adopt the top-1 similarity score, some important similarity scores will also be ignored. To address this issue, we propose the AOSM module, where all similarity scores will be applied with different weights during aggregation. From Tab. 7, we observe that compared with other strategies, our proposed attention mechanism achieves better performance.</p><p>To explore the impact of the temporal encoder module in X-CLIP, we also conduct an ablative study to compare the X-CLIP with and without the temporal encoder. As shown in Tab 8, based on either ViT-B/32 or ViT/16, X-CLIP with temporal encoder consistently outperforms X-CLIP without temporal encoder. This may be because the temporal encoder is used to model the temporal relation of different frames in a video. Therefore, X-CLIP without temporal encoder can not understand and perceive the information that requires a combination of multiple frames, e.g., action. Based on the above analysis, we conclude that temporal modeling is also a key to improving the performance of retrieval tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Effect of Temperature Parameter</head><p>To explore the effect of different in the AOSM module, we also designed a group of experiments by setting different temperature parameters in Softmax. From Tab. 9, we observe that the retrieval performance first improves before reaching the saturation point (i.e., = 0.01), and then begins to decline slightly. The main reason may be that when is large, too many noisy similarity scores are considered. On the contrary, if the is small, some important similarity scores may be ignored. Besides, our proposed attention mechanism with different consistently performs better than the Mean-Mean strategy, and the attention mechanism with the optimal outperforms other strategies in all evaluation protocols. This justifies that our proposed attention mechanism helps to strengthen the influence of important similarity scores and weaken the influence of noisy similarity scores, thus achieving better retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Qualitative Analysis</head><p>To qualitatively validate the effectiveness of our proposed X-CLIP, we show some typical video-to-text and text-to-video retrieval examples in <ref type="figure" target="#fig_0">Fig. 3</ref> and <ref type="figure" target="#fig_2">Fig. 4</ref>, respectively. From these retrieval results, we find that X-CLIP could accurately understand the content of sentences and videos. Meanwhile, it is robust for X-CLIP to comprehend complex and similar sentences and videos, which is mainly attributed to the multi-grained contrast of our proposed model. To be specific, as shown in the first example in <ref type="figure" target="#fig_0">Fig.3</ref>, although the top-3 retrieved sentences are similar, our proposed X-CLIP can still choose the correct sentence by understanding the details of    sentences and videos. Similarly, as shown in the first example in <ref type="figure" target="#fig_2">Fig.4</ref>, all top-3 retrieved videos describe the same cartoon, while "squid" does not appear in the second and third videos. Due to the multi-grained contrast, X-CLIP performs well in visual and textual content understanding, so it can retrieve the correct video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this paper, we present X-CLIP, a novel end-to-end multi-grained contrastive model for video-text retrieval, which first encodes the Query: cartoons of a sponge a squid and starfish Top1: (30.79) Top2: <ref type="bibr">(29.37)</ref> Top3: <ref type="bibr">(29.21)</ref> Query: a woman bakes and decorates a cake  sentences and videos into coarse-grained and fine-grained representations, and conducts fine-grained, coarse-grained, and crossgrained contrasts over these representations. The multi-grained contrast and the AOSM module of X-CLIP help to reduce the negative effects of unnecessary frames and unimportant words during retrieval. Significant performance gains on five popular video-text retrieval datasets demonstrate the effectiveness and superiority of our proposed model.  <ref type="figure">Figure 6</ref>: Overview of the new X-CLIP, which uses a Transformer to model multi-grained features rather than the AOSM module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">APPENDIX 6.1 More Performance Comparison</head><p>To verify the effectiveness of our method, we display the detailed comparison between our proposed X-CLIP and all variants of CLIP4Clip on different backbones (i.e., ViT-B/32 and ViT-B/16). As shown in Tab. 10 -Tab. 13, our proposed X-CLIP outperforms all variants of CLIP4Clip. Notably, X-CLIP with a weak backbone (i.e., ViT-B/32) even achieves comparable performance to CLIP4Clip with a strong backbone (i.e., ViT-B/16). This may be because our proposed crossgrained contrast is conducive to removing the noise information in the videos and sentences and capturing the important information. The outstanding performance again proves the importance and effectiveness of multi-grained contrast and the AOSM module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Effect of training dataset size on contrastive modules</head><p>To gain deep insight into our four contrastive modules, we conduct the experiment to validate the X-CLIP with a single contrast module on the training datasets of different sizes. As illustrated in <ref type="figure" target="#fig_3">Fig. 5</ref>, when the training data is sufficient (i.e., 9k), the videoto-text and text-to-video retrieval performance of four variants is similar. When the size of the training dataset is reduced to 3k, the performance differences of different variants begin to appear and the word-frame contrastive module performs worse than other modules. Furthermore, when the size of the training dataset is reduced to 0.1k, other contrastive modules perform better than the word-frame contrastive module by a significant margin. The main reason can be that compared with other modules, the word-frame contrastive module is more complex, so it is difficult to optimize this module on a small amount of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Effect of the AOSM module and Transformer modeling</head><p>To demonstrate the superiority and effectiveness of our proposed AOSM module, we also try to use a Transformer module to model the relationship of multi-grained features, which introduces more computation and parameters. The architecture of the new model is shown in <ref type="figure">Fig. 6</ref>. As shown in Tab. 14, our proposed AOSM module performs better than the Transf module. The performance gain can result from two aspects: 1) The new Transformer architecture introduces too many parameters, which makes it hard to be optimized with the limited amount of data. Our proposed AOSM is a well-designed module, where the importance of each frame and word is explicitly calculated. Thus, noise information in the video and sentence can be removed in X-CLIP. Besides, compared with Transformer, our proposed AOSM module contains fewer parameters, so it is easy to optimize the AOSM module.</p><p>2) The similarity scores of Transformer are obtained by a Linear layer, while the similarity scores of our proposed AOSM are obtained by the dot product. Notably, the dot product is the conventional approach for similarity calculation in the CLIP. However, Linear is a new approach, which does not carry any prior knowledge. Therefore, the prior knowledge of CLIP has little gain in Transformer, but our X-CLIP retains this prior knowledge well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Top1: a woman?Figure 3 :</head><label>3</label><figDesc>plays instruments in a field. (31.03) Top2: women of a foreign nation comb their hair and perform in traditional costumes. (26.60) Top3: woman playing instruments in a field for a music video. (26.30) ? Top1: A police officer drives his white car onto a grassy field and then back on to the street. (32.50) Top2: A car is in a wreck. (28.03) Top3: A car is racing on road. (27.85) ? Top1: A cartoon character prepares to ride a bicycle. (34.30) Top2: Cartoon of a squid on a bike looking up at a treehouse. (29.64) Top3: A video game character rides around on a motorcycle. (27.27) Top-3 video-to-text retrieval results on MSR-VTT. The number in parentheses is the similarity score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Top-3 text-to-video retrieval results on MSR-VTT. The number in parentheses is the similarity score.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FujianFigure 5 :</head><label>5</label><figDesc>Province of China (No.2021J01002). This work was supported by Alibaba Group through Alibaba Research Intern Program. Retrieval performance of models with different contrastive modules in different sizes of the training set on the MSR-VTT dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>9K' split, where 9,000 videos and 180,000 captions are used for training and the rest are used for testing. MSVD [7] contains 1,970 videos, the duration of which vary from 1 to 62 seconds. Each video is annotated with 40 English captions. We use 1,200, 100, 670 videos for training, validating, and testing. LSMDC [45] is a dataset that contains 118,081 videos and captions. The duration of each video ranges from 2 to 30 seconds. We adopt 109,673, 7,408, and 1,000 videos for training, validating, and testing. DiDeMo [2] contains 10,000 videos and 40,000 captions. Following previous works [4, 30, 35], all captions of a video are concatenated together during video-paragraph retrieval. .2 Experimental Settings 4.2.1 Implementation Details. We conduct the experiments on 4 NVIDIA Tesla V100 32GB GPUs using the PyTorch library. Following the previous work</figDesc><table><row><cell>)</cell></row><row><cell>4 EXPERIMENTS</cell></row><row><cell>4.1 Datasets</cell></row></table><note>MSR-VTT [56] is a popular video-text retrieval dataset, which contains 10,000 videos and 200,000 captions. The length of videos in this dataset ranges from 10 to 32 seconds. In this paper, we adopt the widely-used 'Training-ActivityNet [6] contains 20,000 YouTube videos, which are anno- tated temporally. Following previous works [15, 38, 49], all captions of a video are also concatenated together during video-paragraph retrieval for fair comparison.4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Retrieval performance comparison to SOTAs on the MSR-VTT dataset.</figDesc><table><row><cell>Text-to-Video Retrieval</cell><cell>Video-to-Text Retrieval</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Retrieval performance comparison on MSVD.</figDesc><table><row><cell></cell><cell cols="3">Text-to-Video</cell><cell cols="2">Video-to-Text</cell><cell></cell></row><row><cell>Model</cell><cell cols="6">R@1? R@5? MnR? R@1? R@5? MnR?</cell></row><row><cell>Multi Cues [41]</cell><cell>20.3</cell><cell>47.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CE [35]</cell><cell>19.8</cell><cell>49.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>SSB [42]</cell><cell>28.4</cell><cell>60.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>NoiseE [1]</cell><cell>20.3</cell><cell>49.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CLIP-straight [43]</cell><cell>37.0</cell><cell>64.1</cell><cell>-</cell><cell>59.9</cell><cell>85.2</cell><cell>-</cell></row><row><cell>Frozen [4]</cell><cell>33.7</cell><cell>64.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TT-CE+ [11]</cell><cell>25.4</cell><cell>56.9</cell><cell>-</cell><cell>27.1</cell><cell>55.3</cell><cell>-</cell></row><row><cell>CLIP4Clip-MeanP (ViT-B/32) [38]</cell><cell>46.2</cell><cell>76.1</cell><cell>10.0</cell><cell>56.6</cell><cell>79.7</cell><cell>7.6</cell></row><row><cell>CLIP4Clip-seqTransf (ViT-B/32) [38]</cell><cell>45.2</cell><cell>75.5</cell><cell>10.3</cell><cell>62.0</cell><cell>87.3</cell><cell>4.3</cell></row><row><cell>CLIP4Clip-MeanP (ViT-B/16) [38]</cell><cell>47.3</cell><cell>77.7</cell><cell>9.1</cell><cell>62.9</cell><cell>87.2</cell><cell>4.2</cell></row><row><cell>CLIP4Clip-seqTransf (ViT-B/16) [38]</cell><cell>47.2</cell><cell>77.7</cell><cell>9.1</cell><cell>63.2</cell><cell>87.2</cell><cell>4.2</cell></row><row><cell>X-CLIP (ViT-B/32)</cell><cell>47.1</cell><cell>77.8</cell><cell>9.5</cell><cell>60.9</cell><cell>87.8</cell><cell>4.7</cell></row><row><cell>X-CLIP (ViT-B/16)</cell><cell>50.4</cell><cell>80.6</cell><cell>8.4</cell><cell>66.8</cell><cell>90.4</cell><cell>4.2</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Retrieval performance comparison on LSMDC.</figDesc><table><row><cell></cell><cell cols="3">Text-to-Video</cell><cell cols="3">Video-to-Text</cell></row><row><cell>Model</cell><cell cols="6">R@1? R@5? MnR? R@1? R@5? MnR?</cell></row><row><cell>CT-SAN [62]</cell><cell>5.1</cell><cell>16.3</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>JSFusion [61]</cell><cell>9.1</cell><cell>21.2</cell><cell>-</cell><cell>12.3</cell><cell>28.6</cell><cell>-</cell></row><row><cell>CE [35]</cell><cell>11.2</cell><cell>26.9</cell><cell>96.8</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>MMT [15]</cell><cell>12.9</cell><cell>29.9</cell><cell>75.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>NoiseE [1]</cell><cell>6.4</cell><cell>19.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CLIP-straight [43]</cell><cell>11.3</cell><cell>22.7</cell><cell>-</cell><cell>6.8</cell><cell>16.4</cell><cell>-</cell></row><row><cell>MDMMT [14]</cell><cell>18.8</cell><cell>38.5</cell><cell>58.0</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Frozen [4]</cell><cell>15.0</cell><cell>30.8</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HiT [34]</cell><cell>14.0</cell><cell>31.2</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TT-CE+ [11]</cell><cell>17.2</cell><cell>36.5</cell><cell>-</cell><cell>17.5</cell><cell>36.0</cell><cell>-</cell></row><row><cell>CLIP4Clip-MeanP (ViT-B/32) [38]</cell><cell>20.7</cell><cell>38.9</cell><cell>65.3</cell><cell>20.6</cell><cell>39.4</cell><cell>56.7</cell></row><row><cell>CLIP4Clip-seqTransf (ViT-B/32) [38]</cell><cell>22.6</cell><cell>41.0</cell><cell>61.0</cell><cell>20.8</cell><cell>39.0</cell><cell>54.2</cell></row><row><cell>CLIP4Clip-MeanP (ViT-B/16) [38]</cell><cell>23.5</cell><cell>43.2</cell><cell>54.8</cell><cell>22.6</cell><cell>50.5</cell><cell>50.3</cell></row><row><cell>CLIP4Clip-seqTransf (ViT-B/16) [38]</cell><cell>23.5</cell><cell>45.2</cell><cell>51.6</cell><cell>23.2</cell><cell>42.4</cell><cell>47.4</cell></row><row><cell>X-CLIP (ViT-B/32)</cell><cell>23.3</cell><cell>43.0</cell><cell>56.0</cell><cell>22.5</cell><cell>42.2</cell><cell>50.7</cell></row><row><cell>X-CLIP (ViT-B/16)</cell><cell>26.1</cell><cell>48.4</cell><cell>46.7</cell><cell>26.9</cell><cell>46.2</cell><cell>41.9</cell></row><row><cell cols="7">our model with the ViT-B/32 can even achieve comparable per-</cell></row><row><cell cols="7">formance to CLIP4Clip with ViT-B/16, which again demonstrates</cell></row><row><cell cols="7">the effectiveness and superiority of multi-grained contrast and</cell></row><row><cell>the AOSM module.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note>We also further validate the generalization of X-CLIP on MSVD, LSMDC, DiDeMo and ActivityNet in Tab. 2 -5. It is worth noting</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Retrieval performance comparison on DiDeMo.</figDesc><table><row><cell>Text-to-Video</cell><cell>Video-to-Text</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Retrieval performance comparison on ActivityNet.</figDesc><table><row><cell></cell><cell cols="3">Text-to-Video</cell><cell cols="3">Video-to-Text</cell></row><row><cell>Model</cell><cell cols="6">R@1? R@5? MnR? R@1? R@5? MnR?</cell></row><row><cell>FSE [63]</cell><cell>18.2</cell><cell>44.8</cell><cell>-</cell><cell>16.7</cell><cell>43.1</cell><cell>-</cell></row><row><cell>CE [35]</cell><cell>18.2</cell><cell>47.7</cell><cell>23.1</cell><cell>17.7</cell><cell>46.6</cell><cell>24.4</cell></row><row><cell>HSE [63]</cell><cell>20.5</cell><cell>49.3</cell><cell>-</cell><cell>18.7</cell><cell>48.1</cell><cell>-</cell></row><row><cell>MMT [15]</cell><cell>28.7</cell><cell>61.4</cell><cell>16.0</cell><cell>28.9</cell><cell>61.1</cell><cell>17.1</cell></row><row><cell>SSB [42]</cell><cell>29.2</cell><cell>61.6</cell><cell>-</cell><cell>28.7</cell><cell>60.8</cell><cell>-</cell></row><row><cell>HiT [34]</cell><cell>29.6</cell><cell>60.7</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>ClipBERT [30]</cell><cell>21.3</cell><cell>49.0</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>TT-CE+ [11]</cell><cell>23.5</cell><cell>57.2</cell><cell>-</cell><cell>23.0</cell><cell>56.1</cell><cell>-</cell></row><row><cell>CLIP4Clip-MeanP (ViT-B/32) [38]</cell><cell>40.5</cell><cell>72.4</cell><cell>7.4</cell><cell>42.5</cell><cell>74.1</cell><cell>6.6</cell></row><row><cell>CLIP4Clip-seqTransf (ViT-B/32) [38]</cell><cell>40.5</cell><cell>72.4</cell><cell>7.5</cell><cell>41.4</cell><cell>73.7</cell><cell>6.7</cell></row><row><cell>CLIP4Clip-MeanP (ViT-B/16) [38]</cell><cell>44.0</cell><cell>73.9</cell><cell>7.0</cell><cell>44.1</cell><cell>74.0</cell><cell>6.5</cell></row><row><cell>CLIP4Clip-seqTransf (ViT-B/16) [38]</cell><cell>44.5</cell><cell>75.2</cell><cell>6.4</cell><cell>44.1</cell><cell>75.2</cell><cell>6.4</cell></row><row><cell>X-CLIP (ViT-B/32)</cell><cell>44.3</cell><cell>74.1</cell><cell>7.9</cell><cell>43.9</cell><cell>73.9</cell><cell>7.6</cell></row><row><cell>X-CLIP (ViT-B/16)</cell><cell>46.2</cell><cell>75.5</cell><cell>6.8</cell><cell>46.4</cell><cell>75.9</cell><cell>6.4</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Retrieval performance with different contrastive granularity on the MSR-VTT dataset.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Contrastive Module</cell><cell></cell><cell></cell><cell cols="2">Text-to-Video</cell><cell></cell><cell></cell><cell cols="2">Video-to-Text</cell><cell></cell></row><row><cell>ID</cell><cell cols="12">Sent-Video Sent-Frame Word-Video Word-Frame R@1? R@5? R@10? MnR? R@1? R@5? R@10? MnR?</cell></row><row><cell>Exp1</cell><cell>?</cell><cell></cell><cell></cell><cell></cell><cell>43.0</cell><cell>70.7</cell><cell>81.6</cell><cell>16.3</cell><cell>43.0</cell><cell>70.2</cell><cell>81.2</cell><cell>11.5</cell></row><row><cell>Exp2</cell><cell></cell><cell>?</cell><cell></cell><cell></cell><cell>42.7</cell><cell>69.6</cell><cell>81.3</cell><cell>13.9</cell><cell>43.1</cell><cell>70.7</cell><cell>82.1</cell><cell>9.9</cell></row><row><cell>Exp3</cell><cell></cell><cell></cell><cell>?</cell><cell></cell><cell>42.8</cell><cell>69.9</cell><cell>80.1</cell><cell>17.0</cell><cell>43.2</cell><cell>70.1</cell><cell>80.5</cell><cell>13.8</cell></row><row><cell>Exp4</cell><cell></cell><cell></cell><cell></cell><cell>?</cell><cell>42.7</cell><cell>69.5</cell><cell>81.3</cell><cell>14.4</cell><cell>42.8</cell><cell>70.8</cell><cell>81.7</cell><cell>10.6</cell></row><row><cell>Exp5</cell><cell></cell><cell>?</cell><cell>?</cell><cell></cell><cell>44.6</cell><cell>72.8</cell><cell>82.4</cell><cell>13.9</cell><cell>45.7</cell><cell>73.2</cell><cell>82.3</cell><cell>9.1</cell></row><row><cell>Exp6</cell><cell></cell><cell></cell><cell>?</cell><cell>?</cell><cell>45.6</cell><cell>72.0</cell><cell>82.0</cell><cell>13.6</cell><cell>44.8</cell><cell>72.5</cell><cell>81.7</cell><cell>9.6</cell></row><row><cell>Exp7</cell><cell></cell><cell>?</cell><cell></cell><cell>?</cell><cell>44.1</cell><cell>70.2</cell><cell>81.3</cell><cell>14.3</cell><cell>44.4</cell><cell>71.6</cell><cell>82.8</cell><cell>9.7</cell></row><row><cell>Exp8</cell><cell>?</cell><cell>?</cell><cell></cell><cell></cell><cell>45.4</cell><cell>72.2</cell><cell>81.6</cell><cell>13.4</cell><cell>45.4</cell><cell>72.8</cell><cell>82.7</cell><cell>9.2</cell></row><row><cell>Exp9</cell><cell>?</cell><cell></cell><cell>?</cell><cell></cell><cell>44.0</cell><cell>70.3</cell><cell>82.5</cell><cell>13.9</cell><cell>43.6</cell><cell>70.9</cell><cell>81.8</cell><cell>11.3</cell></row><row><cell>Exp10</cell><cell>?</cell><cell></cell><cell></cell><cell>?</cell><cell>44.8</cell><cell>72.6</cell><cell>83.0</cell><cell>13.6</cell><cell>45.3</cell><cell>73.0</cell><cell>83.8</cell><cell>9.5</cell></row><row><cell>Exp11</cell><cell></cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>45.7</cell><cell>72.7</cell><cell>82.5</cell><cell>13.2</cell><cell>45.6</cell><cell>72.8</cell><cell>82.9</cell><cell>9.2</cell></row><row><cell>Exp12</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell></cell><cell>45.7</cell><cell>72.7</cell><cell>82.5</cell><cell>13.2</cell><cell>45.6</cell><cell>72.8</cell><cell>82.9</cell><cell>9.2</cell></row><row><cell>Exp13</cell><cell>?</cell><cell>?</cell><cell></cell><cell>?</cell><cell>45.8</cell><cell>73.2</cell><cell>82.7</cell><cell>13.2</cell><cell>46.5</cell><cell>72.6</cell><cell>83.8</cell><cell>9.7</cell></row><row><cell>Exp14</cell><cell>?</cell><cell></cell><cell>?</cell><cell>?</cell><cell>45.5</cell><cell>72.8</cell><cell>82.9</cell><cell>13.5</cell><cell>46.4</cell><cell>72.5</cell><cell>83.7</cell><cell>9.6</cell></row><row><cell>Exp15</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell>46.1</cell><cell>73.0</cell><cell>83.1</cell><cell>13.2</cell><cell>46.8</cell><cell>73.3</cell><cell>84.0</cell><cell>9.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Retrieval performance with different fusion methods for similarity matrices on the MSR-VTT dataset.</figDesc><table><row><cell></cell><cell cols="3">Text-to-Video</cell><cell cols="3">Video-to-Text</cell></row><row><cell>Method</cell><cell cols="6">R@1? R@5? MnR? R@1? R@5? MnR?</cell></row><row><cell>Max-Max</cell><cell>44.0</cell><cell>72.6</cell><cell>13.5</cell><cell>44.4</cell><cell>72.5</cell><cell>9.2</cell></row><row><cell>Mean-Mean</cell><cell>43.2</cell><cell>71.2</cell><cell>14.8</cell><cell>42.5</cell><cell>70.2</cell><cell>11.4</cell></row><row><cell>Mean-Max</cell><cell>44.4</cell><cell>71.1</cell><cell>14.9</cell><cell>44.2</cell><cell>71.7</cell><cell>10.2</cell></row><row><cell>Max-Mean</cell><cell>44.9</cell><cell>71.3</cell><cell>13.5</cell><cell>43.8</cell><cell>71.8</cell><cell>9.4</cell></row><row><cell>Attention</cell><cell>46.1</cell><cell>73.0</cell><cell>13.2</cell><cell>46.8</cell><cell>73.3</cell><cell>9.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Ablation study of temporal encoder on the MSR-VTT dataset. TE is short for temporal encoder.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Text-to-Video</cell><cell cols="2">Video-to-Text</cell><cell></cell></row><row><cell cols="8">Base Model TE R@1? R@5? MnR? R@1? R@5? MnR?</cell></row><row><cell>ViT-B/32</cell><cell>?</cell><cell>45.2 46.1</cell><cell>72.9 73.0</cell><cell>13.8 13.2</cell><cell>45.6 46.8</cell><cell>73.9 73.3</cell><cell>9.2 9.1</cell></row><row><cell>ViT-B/16</cell><cell>?</cell><cell>48.3 49.3</cell><cell>75.3 75.8</cell><cell>13.4 12.2</cell><cell>47.6 48.9</cell><cell>76.1 76.8</cell><cell>9.0 8.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Retrieval performance with different temprature parameters in Softmax on the MSR-VTT dataset.</figDesc><table><row><cell></cell><cell cols="3">Text-to-Video</cell><cell cols="3">Video-to-Text</cell></row><row><cell></cell><cell cols="6">R@1? R@5? MnR? R@1? R@5? MnR?</cell></row><row><cell>1</cell><cell>43.9</cell><cell>71.6</cell><cell>14.5</cell><cell>43.5</cell><cell>71.3</cell><cell>11.3</cell></row><row><cell>0.1</cell><cell>45.2</cell><cell>72.2</cell><cell>14.0</cell><cell>45.3</cell><cell>73.1</cell><cell>9.3</cell></row><row><cell>0.01</cell><cell>46.1</cell><cell>73.0</cell><cell>13.2</cell><cell>46.8</cell><cell>73.3</cell><cell>9.1</cell></row><row><cell>0.001</cell><cell>45.6</cell><cell>72.2</cell><cell>13.7</cell><cell>43.6</cell><cell>72.5</cell><cell>9.4</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For clarity and simplicity, we have omitted the frame (word) index and video (sentence) index of visual (textual) representations.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Noise estimation using density estimation for self-supervised multimodal learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Amrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Ben-Ari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rotman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.03186</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Localizing moments in video with natural language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><forename type="middle">Anne</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5803" to="5812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Lu?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15691</idno>
		<title level="m">Vivit: A video vision transformer</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Frozen in time: A joint video and image encoder for end-to-end retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Bain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?l</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.00650</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gedas</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.05095</idno>
		<title level="m">Is Space-Time Attention All You Need for Video Understanding? arXiv preprint</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Activitynet: A large-scale video benchmark for human activity understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Fabian Caba Heilbron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernard</forename><surname>Escorcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">Carlos</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Niebles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ieee conference on computer vision and pattern recognition</title>
		<meeting>the ieee conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="961" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Collecting highly parallel data for paraphrase evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>William B Dolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies</title>
		<meeting>the 49th annual meeting of the association for computational linguistics: human language technologies</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="190" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning. PMLR</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1597" to="1607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Ross Girshick, and Kaiming He. 2020. Improved baselines with momentum contrastive learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">An empirical study of training self-supervised vision transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="9640" to="9649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Teachtext: Crossmodal generalized distillation for text-video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioana</forename><surname>Croitoru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simion-Vlad</forename><surname>Bogolin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hailin</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11583" to="11593" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dual encoding for zero-example video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xirong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoxi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9346" to="9355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mdmmt: Multidomain multimodal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Dzabraev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Kalashnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stepan</forename><surname>Komkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleksandr</forename><surname>Petiushko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3354" to="3363" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multimodal transformer for video retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valentin</forename><surname>Gabeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karteek</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2020: 16th European Conference</title>
		<meeting><address><addrLine>Glasgow, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020-08-23" />
			<biblScope unit="page" from="214" to="229" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV 16</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improving Video Retrieval by Adaptive Margin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yajuan</forename><surname>L?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1359" to="1368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.00833</idno>
		<title level="m">PixelFolder: An Efficient Progressive Pixel Synthesis Network for Image Generation</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">WenLan: Bridging vision and language by large-scale multi-modal pre-training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqi</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangzhen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoyu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoxing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyuan</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baogui</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.06561</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Tgif-qa: Toward spatio-temporal reasoning in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunseok</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjin</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving image captioning by leveraging intra-and inter-layer global representation in transformer network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuhai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1655" to="1663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Knowing What to Learn: A Metric-oriented Focal Mechanism for Image Captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1109/TIP.2022.3183434</idno>
		<ptr target="https://doi.org/10.1109/TIP.2022.3183434" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Scaling up visual and visionlanguage representation learning with noisy text supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinfei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zarana</forename><surname>Parekh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun-Hsuan</forename><surname>Sung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning. PMLR</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4904" to="4916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hierarchical Cross-Modal Graph Consistency Learning for Video-Text Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weike</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengcheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jieming</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3404835.3462974</idno>
		<ptr target="https://doi.org/10.1145/3404835.3462974" />
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada</title>
		<editor>Fernando Diaz, Chirag Shah, Torsten Suel, Pablo Castells, Rosie Jones, and Tetsuya Sakai</editor>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-07-11" />
			<biblScope unit="page" from="1114" to="1124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Relevance-guided supervision for openqa with colbert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="929" to="944" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Colbert: Efficient and effective passage search via contextualized late interaction over bert</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.69809</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">ICLR 2015. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hierarchical conditional relation networks for video question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vuong</forename><surname>Thao Minh Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetha</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Truyen</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9972" to="9981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stacked cross attention for image-text matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="201" to="216" />
		</imprint>
	</monogr>
	<note>Gang Hua, Houdong Hu, and Xiaodong He</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Less is more: Clipbert for video-and-language learning via sparse sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luowei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamara</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7331" to="7341" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Align before fuse: Vision and language representation learning with momentum distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramprasaath</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akhilesh</forename><surname>Gotmare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shafiq</forename><surname>Joty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiming</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven Chu Hong</forename><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Licheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingjing</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00200</idno>
		<title level="m">Hierarchical encoder for video+ language omni-representation pre-training</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Oscar: Object-semantics aligned pre-training for vision-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengchuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houdong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="121" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengsheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiru</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenkui</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongyuan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.15049</idno>
		<title level="m">Hit: Hierarchical transformer with momentum contrast for videotext retrieval</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Albanie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arsha</forename><surname>Nagrani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.13487</idno>
		<title level="m">Use what you have: Video retrieval using representations from collaborative experts</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Sgdr: Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Clip4clip: An empirical study of clip for end to end video clip retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaishao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.08860</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Knowing what it is: Semantic-enhanced Dual Attention Transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiwei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.1109/TMM.2022.3164787</idno>
		<ptr target="https://doi.org/10.1109/TMM.2022.3164787" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">End-to-end learning of visual representations from uncurated instructional videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Miech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Smaira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9879" to="9889" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning joint embedding with multimodal cues for crossmodal video-text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Niluthpol Chowdhury Mithun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amit K Roy-Chowdhury</forename><surname>Metze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM on International Conference on Multimedia Retrieval</title>
		<meeting>the 2018 ACM on International Conference on Multimedia Retrieval</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="19" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandela</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Yao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuki</forename><surname>Asano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Metze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02824</idno>
		<title level="m">Joao Henriques, and Andrea Vedaldi. 2020. Support-set bottlenecks for video-text representation learning</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A straightforward framework for video retrieval using clip</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jes?s Andr?s</forename><surname>Portillo-Quintero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jos?</forename><forename type="middle">Carlos</forename><surname>Ortiz-Bayliss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Terashima-Mar?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mexican Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pamela</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.00020</idno>
		<title level="m">Learning transferable visual models from natural language supervision</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">The long-short story of movie description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German conference on pattern recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="209" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rouditchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angie</forename><surname>Boggust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harwath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhiraj</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kartik</forename><surname>Audhkhasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hilde</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rogerio</forename><surname>Feris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09199</idno>
		<title level="m">Avlnet: Learning audio-visual language representations from instructional videos</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Colbertv2: Effective and efficient retrieval via lightweight late interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keshav</forename><surname>Santhanam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omar</forename><surname>Khattab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Saad-Falcon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01488</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.07909</idno>
		<title level="m">Neural machine translation of rare words with subword units</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabien</forename><surname>Baradel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05743</idno>
		<title level="m">Learning video representations using contrastive bidirectional transformer</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Videobert: A joint model for video and language representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7464" to="7473" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Bansal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07490</idno>
		<title level="m">Lxmert: Learning cross-modality encoder representations from transformers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhashini</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huijuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raymond</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4729</idno>
		<title level="m">Translating videos to natural language using deep recurrent neural networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">T2VLAD: Global-Local Sequence Alignment for Text-Video Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<ptr target="https://openaccess.thecvf.com/content/CVPR2021/html/Wang_T2VLAD_Global-Local_Sequence_Alignment_for_Text-Video_Retrieval_CVPR_2021_paper.html" />
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual</title>
		<imprint>
			<date type="published" when="2021-06-19" />
			<biblScope unit="page" from="5079" to="5088" />
		</imprint>
	</monogr>
	<note>Computer Vision Foundation / IEEE</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">E2E-VLP: End-to-End Vision-Language Pre-training Enhanced by Visual Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songfang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenming</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.42</idno>
		<ptr target="https://doi.org/10.18653/v1/2021.acl-long.42" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="503" to="513" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Msr-vtt: A large video description dataset for bridging video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5288" to="5296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Taco: Token-aware cascade contrastive learning for video-text alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Bisk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="11562" to="11572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Tree-Augmented Cross-Modal Encoding for Complex-Query Video Retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401151</idno>
		<ptr target="https://doi.org/10.1145/3397271.3401151" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event</title>
		<editor>Jimmy Huang, Yi Chang, Xueqi Cheng, Jaap Kamps, Vanessa Murdock, Ji-Rong Wen, and Yiqun Liu</editor>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval, SIGIR 2020, Virtual Event<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020-07-25" />
			<biblScope unit="page" from="1339" to="1348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lewei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Runhui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guansong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minzhe</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenguo</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.07783</idno>
		<title level="m">Xin Jiang, and Chunjing Xu. 2021. FILIP: Fine-grained Interactive Language-Image Pre-Training</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Ernie-vil: Knowledge enhanced vision-language representations through scene graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiji</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weichong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Hao Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.16934</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A joint sequence fusion model for video question answering and retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV</title>
		<meeting>the European Conference on Computer Vision (ECCV</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="471" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">End-to-end concept word detection for video captioning, retrieval, and question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngjae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyungjin</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jongwook</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3165" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Cross-modal and hierarchical modeling of video and text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="374" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">RSTNet: Captioning with adaptive attention on visual and non-visual words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunpeng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiayi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongjian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feiyue</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="15465" to="15474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaoyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiyi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunhang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjia</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingbao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liujuan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoshuai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongrong</forename><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.16265</idno>
		<title level="m">SeqTR: A Simple yet Universal Network for Visual Grounding</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Actbert: Learning global-local video-text representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="8746" to="8755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Retrieval performance comparison on MSVD. Text-to-Video Video-to-Text Model R@1? R@5? MnR? R@1? R@5? MnR? CLIP4Clip-MeanP</title>
		<meeting><address><addrLine>ViT-</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Retrieval performance comparison on LSMDC. Text-to-Video Video-to-Text Model R@1? R@5? MnR? R@1? R@5? MnR? CLIP4Clip-MeanP</title>
		<meeting><address><addrLine>ViT-</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">11</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Retrieval performance comparison on DiDeMo. Text-to-Video Video-to-Text Model R@1? R@5? MnR? R@1? R@5? MnR? CLIP4Clip-MeanP</title>
		<meeting><address><addrLine>ViT-</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
			</analytic>
	<monogr>
		<title level="m">Retrieval performance comparison on ActivityNet. Text-to-Video Video-to-Text Model R@1? R@5? MnR? R@1? R@5? MnR? CLIP4Clip-MeanP</title>
		<meeting><address><addrLine>ViT-</addrLine></address></meeting>
		<imprint>
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Transf means using a 3-layer Transformer to model multi-grained features. Text-to-Video Retrieval Video-to-Text Retrieval Model R@1? R@5? MnR? R@1? R@5? MnR?</title>
	</analytic>
	<monogr>
		<title level="j">Table</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
		</imprint>
	</monogr>
	<note>Retrieval performance comparison between Transformer modeling and the AOSM module</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
