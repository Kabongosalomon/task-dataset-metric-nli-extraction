<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CORDEIRO ET AL.: PROPMIX: HARD SAMPLE FILTERING AND PROPORTIONAL MIXUP 1 PropMix: Hard Sample Filtering and Proportional MixUp for Learning with Noisy Labels</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filipe</forename><forename type="middle">R</forename><surname>Cordeiro</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Universidade Federal Rural de Pernambuco Recife</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
							<email>vasileios.belagiannis@uni-ulm.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Universit?t Ulm Ulm</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
							<email>ian.reid@adelaide.edu.au</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Adelaide Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
							<email>gustavo.carneiro@adelaide.edu.au</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Adelaide Adelaide</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CORDEIRO ET AL.: PROPMIX: HARD SAMPLE FILTERING AND PROPORTIONAL MIXUP 1 PropMix: Hard Sample Filtering and Proportional MixUp for Learning with Noisy Labels</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T18:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The most competitive noisy label learning methods rely on an unsupervised classification of clean and noisy samples, where samples classified as noisy are re-labelled and "MixMatched" with the clean samples. These methods have two issues in large noise rate problems: 1) the noisy set is more likely to contain hard samples that are incorrectly re-labelled, and 2) the number of samples produced by MixMatch tends to be reduced because it is constrained by the small clean set size. In this paper, we introduce the learning algorithm PropMix to handle the issues above. PropMix filters out hard noisy samples, with the goal of increasing the likelihood of correctly re-labelling the easy noisy samples. Also, PropMix places clean and re-labelled easy noisy samples in a training set that is augmented with MixUp, removing the clean set size constraint and including a large proportion of correctly re-labelled easy noisy samples. We also include self-supervised pre-training to improve robustness to high noisy label scenarios. Our experiments show that PropMix has state-of-the-art (SOTA) results on CIFAR-10/-100 (with symmetric, asymmetric and semantic label noise), Red Mini-ImageNet (from the Controlled Noisy Web Labels), Clothing1M and WebVision. In severe label noise benchmarks, our results are substantially better than other methods. The code is available at https://github.com/filipe-research/PropMix.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural network models reached promising results in several computer vision applications recently <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b41">39]</ref>. Nevertheless, the superior performance depends on the availability of well-curated large-scale training sets with clean annotations <ref type="bibr" target="#b33">[32]</ref>. The labeling process can produce noisy labels due to human failure, low-quality data, and challenging labelling tasks <ref type="bibr" target="#b11">[11]</ref>. The problem is that noisy labels in the training set harms the learning process by reducing model generalization <ref type="bibr" target="#b59">[57]</ref>. Developing methods that are robust to label noise is important to deal with real-world applications, where noisy annotations is often part of the training set.</p><p>A common approach to address this challenge is based on semi-supervised learning (SSL) methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b44">42,</ref><ref type="bibr" target="#b45">43]</ref>, based on a 2-stage process formed by an unsupervised learning method to classify training samples as clean or noisy, followed by SSL that "Mix-Matches" <ref type="bibr" target="#b3">[4]</ref> the labelled set formed by the samples classified as clean, and the re-labelled samples from the noisy set. There are two issues with such approach. First, samples classified as noisy can be easy or hard to be re-labelled, where the hard samples are unlikely to be correctly re-labelled, which can bias the training process. Second, MixMatch <ref type="bibr" target="#b3">[4]</ref> relies on a one-to-one sampling between the clean and noisy sets, where the total number of samples is constrained by the clean set size. However, the clean set tends to be smaller and the noisy set tends to have more incorrectly re-labelled samples for larger noise rates-such fact impairs the robustness of SSL methods for large noise rate problems. We hypothesise that by filtering out hard noisy samples, we can reduce the risk of over-fitting those samples. Furthermore, by re-labelling the easy noisy samples and mixing them with the clean set with MixUp <ref type="bibr" target="#b60">[58]</ref> for training a classifier, we not only remove the constraint on the clean set size, but also use a noisy set with better chances to have correctly re-labelled samples, allowing the method to be more robust to large noise rate problems.</p><p>In this paper, we propose a new learning algorithm called PropMix that addresses the two points above. PropMix filters out hard noisy samples via a two-stage process, where the first stage classifies samples as clean or noisy using the loss values, and the second stage eliminates hard noisy samples using their classification confidence. Then, by re-labelling the easy noisy samples with the model output, adding these samples to the training set, and running a regular classification training with MixUp <ref type="bibr" target="#b60">[58]</ref>, we show that PropMix is robust to a wide range of noise rates. To improve the feature representation and model confidence in high noise scenarios, we also add a self-supervised pre-training stage <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b20">19]</ref>. Empirical results on CIFAR-10/-100 <ref type="bibr" target="#b26">[25]</ref> under symmetric, asymmetric and semantic noise, show that PropMix outperforms previous approaches. For high-noise rate problems in CIFAR-10/-100 <ref type="bibr" target="#b31">[30]</ref> and Red Mini-ImageNet from the Controlled Noisy Web Labels <ref type="bibr" target="#b55">[53]</ref>, PropMix presents the best results in the field by a substantial margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Prior Work</head><p>Recently proposed noisy label learning methods rely on the following strategies: robust loss functions <ref type="bibr" target="#b37">[36,</ref><ref type="bibr" target="#b51">49,</ref><ref type="bibr" target="#b53">51]</ref>, sample selection <ref type="bibr" target="#b16">[16]</ref>, label cleansing <ref type="bibr" target="#b22">[21,</ref><ref type="bibr" target="#b58">56]</ref>, sample weighting <ref type="bibr" target="#b48">[46]</ref>, meta-learning <ref type="bibr" target="#b15">[15]</ref>, ensemble learning <ref type="bibr" target="#b40">[38]</ref> and semi-supervised learning (SSL) <ref type="bibr" target="#b3">[4]</ref>. The most successful approaches use SSL, combined with other methods <ref type="bibr" target="#b23">[22,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b34">33]</ref>.</p><p>SSL methods <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b44">42]</ref> first classify samples as clean or noisy, where the noisy samples are re-labelled by the model, and these clean and noisy sets are combined with MixMatch <ref type="bibr" target="#b3">[4]</ref>. As mentioned before, SSL methods have two issues: 1) the training set size for the MixMatch stage is limited by the clean set size that reduces with increasing label noise, and 2) the noisy sample re-labelling accuracy also reduces with increasing label noise. Although the first point is not addressed by SSL methods, the second point is mitigated by replacing the cross entropy (CE) loss by the Mean Absolute Error (MAE) loss to fit the noisy samples. Although MAE has been shown to be robust to label noise <ref type="bibr" target="#b13">[13]</ref>, it tends to underfit the training set <ref type="bibr" target="#b37">[36]</ref>.</p><p>An alternative way to reduce the risk of overfitting incorrectly re-labelled noisy samples is by rejecting them altogether <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b47">45]</ref>, but such rejection ignores the information present in noisy samples, which can decrease the effectiveness of the approach. Moreover, in high noise rate scenarios, the filtered clean set can be too small to train the model. The noisy set can be used after re-labelling the noisy samples <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b44">42,</ref><ref type="bibr" target="#b52">50]</ref>, which works well for low noise rate problems, but as the noise rate increases, this approach is not effective given that the incorrectly re-labelled noisy samples tend to bias the training. The use of clean validation sets in a meta-learning approach <ref type="bibr" target="#b1">[2]</ref> can reduce this issue, but the existence of a clean validation set may be infeasible in real world applications. SELFIE <ref type="bibr" target="#b50">[48]</ref> proposes label correction to a subset of samples that present consistent prediction, while discarding samples that are less consistent. The main issue with SELFIE is that the classification of clean samples is based on a loss threshold that might include some relabeling in the predicted clean set, producing false positives. Our method proposes a hybrid approach. We claim that hard noisy samples are unlikely to have their label corrected, mainly in a high noise scenario. On the other hand, we can find easy noisy samples that are likely to be correctly relabelled and used in a supervised training. The main difference of existing filtering methods and our approach is that we filter out hard noisy samples, while keeping easy noisy samples to be relabelled and included in the training process. We show in the experiments that easy noisy samples can be relabelled correctly with high accuracy, whereas hard noisy samples are unlikely to be correctly relabelled and therefore should be removed from training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>Consider the training set be denoted by</p><formula xml:id="formula_0">D = {(x i , y i )} |D| i=1</formula><p>, with x i ? S ? R H?W ?3 being the i th RGB image of size H ?W , and y i ? {0, 1} |Y| denoting a one-hot vector representing the given label, with Y ? {1, ..., |Y|} denoting the set of labels, and ? c?Y y i (c) = 1. The hidden true label? i can differ from the given label y i that is a result of noise process, represented by y i ? p(y|x i , Y,? i ), with p(y( j)|x i , Y,? i (c)) = ? jc (x i ), where the j, c ? Y are the classes, ? jc (x i ) ? [0, 1] the probability of flipping the class c to j, and ? j?Y ? jc (x i ) = 1. There are three common types of noise in the literature: symmetric <ref type="bibr" target="#b25">[24]</ref>, asymmetric <ref type="bibr" target="#b46">[44]</ref>, and semantic <ref type="bibr" target="#b28">[27]</ref>. The symmetric noise is a noisy type where the hidden label are flipped to a random class with a fixed probability ?, where the true label is included into the label flipping options, which means that ? jc (x i ) = ? |Y|?1 , ? j ? Y, such that j = c, and ? cc (x i ) = 1??. The asymmetric noise has its labels flipped between similar-looking object categories <ref type="bibr" target="#b46">[44]</ref>, where ? jc (x i ) depends only on the classes j, c ? Y, but not on x i . Finally, the semantic noise <ref type="bibr" target="#b28">[27]</ref> is the noisy type where the label flipping depends both on the classes j, c ? Y and image x i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">PropMix</head><p>The proposed PropMix algorithm ( <ref type="figure">Fig. 1</ref>) starts with a self-supervised pre-training <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b20">19]</ref>. Then, we perform a supervised training, with a new filtering step to identify clean samples, easy noisy samples, and hard noisy samples, which are removed from training. The easy noisy samples are re-labelled and proportionally combined with clean samples using MixUp <ref type="bibr" target="#b60">[58]</ref> for supervised training.</p><p>The self-supervised pre-training estimates ? of the feature representation f ? : S ? Z ? R d , by minimizing the contrastive loss <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">19]</ref>:  <ref type="figure">Figure 1</ref>: Our proposed PropMix has a self-supervised pre-training stage <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b20">19]</ref>, followed by a supervised training stage, where we first warm-up the classifier with a classification loss, using the pre-trained weights. Then, using the classification loss, we train a GMM to separate the samples into clean and noisy. Next, using classification confidence for the noisy set, we train a second GMM to separate the easy and hard noisy samples. The clean and easy noisy samples are proportionally combined in the MixUp for training.</p><formula xml:id="formula_1">i, j = ?log exp(sim(z i , z j )/?) ? 2N k=1 1 [k =i] exp(sim(z i , z k )/?) (1) CNN FC Warmup GMM</formula><p>with N denoting mini-batch size, z = f ? (x) represents the features extracted from input x, with z i and z j being the feature vectors from two views of the same image (these views are obtained via different data augmentations of the same image), 1 [k =i] ? {0, 1} being an indicator function, ? denoting the temperature parameter, and sim(.) representing the cosine similarity. Following <ref type="bibr" target="#b12">[12]</ref>, we then learn a clustering classifier p ? (.| f ? (x)). More precisely, we form an initial set of K nearest neighbours (KNN) in Z for each training sample, producing the set N</p><formula xml:id="formula_2">x i = {x j } K j=1 (for x j ? D) for each sample x i ? D . We train p ? (.| f ? (x)) with [12]: CLU = N + ? e e ,<label>(2)</label></formula><formula xml:id="formula_3">with N = ? 1 |D| ? |D| i=1 ? x j ?N x i q(z ji ) log p ? (.| f ? (x i )) p ? (.| f ? (x j ))</formula><p>, q(z ji ) = 1 if x i and x j have the same classification result (i.e., arg max c?Y p ? (c| f ? (x i )) = arg max c?Y p ? (c| f ? (x j ))),</p><formula xml:id="formula_4">and e = 1 |D| ? c?Y ? x?D p ? (c| f ? (x)) log p ? (c| f ? (x)</formula><p>) that maximises the entropy of the average classification and is weighted by ? e . The weights from the feature map f ? (x) are also updated in the clustering process using backpropagation.</p><p>After the pre-training, we warm-up the classifier by training it for a few epochs on the (noisy) training data set with the cross-entropy (CE) loss. The clean and noisy sets, X , U ? D, are formed with <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">23,</ref><ref type="bibr" target="#b28">27,</ref><ref type="bibr" target="#b31">30]</ref>:</p><formula xml:id="formula_5">X = {(x i , y i , w i ) : w i = p (clean| i , ?) ? ?} , U = {(x i , y i , w i ) : w i = p (clean| i , ?) &lt; ?} ,<label>(3)</label></formula><p>with ? denoting a classification threshold, i = ?y i log p ? (.| f ? (x i )), and p (clean| i , ?) being a function that estimates the probability that (x i , y i ) is a clean label sample. The function p (clean| i , ?) in Eq.3 is a bi-modal Gaussian mixture model (GMM) <ref type="bibr" target="#b31">[30]</ref>, where ? denotes the GMM parameters and the larger mean component is the noisy component whereas the smaller mean component is the clean component. Next, we obtain the sets of easy and hard noisy samples U E , U H ? U, as follows:</p><formula xml:id="formula_6">U E = (x i , y i ) : p hard|p ? (c * i | f ? (x) i ), ? &lt; ? , U H = (x i , y i ) : p hard|p ? (c * i | f ? (x i )), ? ? ? ,<label>(4)</label></formula><formula xml:id="formula_7">with c * i = arg max c i ?Y p ? (c i | f ? (x i )), p hard|p ? (c * i | f ? (x i ))</formula><p>, ? being a function that estimates the probability that (x i , y i ) is a hard noisy label sample, and ? denoting the hard noisy sample threshold. The function p hard|p ? (c * i | f ? (x i )), ? in Eq. 4 is a GMM, where ? denotes the GMM parameters and the smaller mean component is the hard noise component whereas the larger mean component is the easy noise component. We assume hard noisy samples have wrong label and wrong prediction, so we remove them from training.</p><p>Next, we perform M data augmentations based on geometrical and visual transformations to increase the number of samples in X and U E , generating the augmented sets X =</p><formula xml:id="formula_8">{(x i ,? i )} |X | i=1 and U E = {(? i ,q i )} |U E | i=1</formula><p>, withx i and? i being the augmented samples from X and U E , respectively, and? i andq i the label estimations defined as follows:</p><formula xml:id="formula_9">y i = TempShrp(w i y i + (1 ? w i )p b ; T ),q b = TempShrp(q b ; T )<label>(5)</label></formula><p>with</p><formula xml:id="formula_10">p b = 1 M M ? m=1 p ? (.| f ? (x i,m )), q b = 1 M M ? m=1 p ? (.| f ? (? i ))</formula><p>, and TempShrp(.) being a temperature sharpening <ref type="bibr" target="#b31">[30]</ref>. The linear combination between the clean and easy noisy samples rely on MixUp <ref type="bibr" target="#b60">[58]</ref> data augmentation, which is different from the SOTA SSL approaches <ref type="bibr" target="#b31">[30]</ref> that use Mix-Match <ref type="bibr" target="#b3">[4]</ref> which has a training set size restricted by the clean set size. In particular, MixMatchbased methods select B U = |X | samples from the noisy set and B X = |X | samples from the clean set, so the MixUp proportion of samples from the noisy set is ? =</p><formula xml:id="formula_11">B U B X +B U = 0.5,</formula><p>and the size of the MixMatch training set is |X |. In our approach, we re-label the easy noisy samples in U E and place them together with the clean samples in X for the MixUp data augmentation with D = X ? U E . This operation enables us to obtain the proportionally mixed setD = MixU p(D , shuffled(D )) where the proportion of samples from the noisy set, denoted by ? = |U E | |X |+|U E | , will be larger than 0.5 for high noise rate problems. To optimise the classification term we rely on the regularised CE loss <ref type="bibr" target="#b31">[30]</ref>:</p><formula xml:id="formula_12">L = CE + ? r r ,<label>(6)</label></formula><formula xml:id="formula_13">with CE = ? 1 |D| ? (x,?)?D? log p ? (.| f ? (x)) and r = KL ? |Y| 1 |D| ? (x,y)?D p ? (.| f ? (x))</formula><p>, where ? r weights the regularisation loss, and ? |Y| denotes a vector of |Y| dimensions with values equal to 1/|Y|. Different from SOTA semi-supervised methods <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b43">41]</ref>, we do not have an additional loss for the noisy set because we assume that most of the samples in the noisy set have correct predictions (this is empirically shown in Sec. 4.3). A positive outcome from not using such additional loss for the noisy set is that we no longer need to manually set hyper-parameter values that depend on the noise rate of the problem, as is done by DivideMix <ref type="bibr" target="#b31">[30]</ref>. The pseudo-code for the training of PropMix is shown in Algorithm 1 in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Data sets</head><p>We conduct our experiments on the data sets CIFAR-10, CIFAR-100 <ref type="bibr" target="#b26">[25]</ref>, Controlled Noisy Web Labels (CNWL) <ref type="bibr" target="#b24">[23]</ref>, Clothing1M <ref type="bibr" target="#b54">[52]</ref> and WebVision <ref type="bibr" target="#b32">[31]</ref>. CIFAR-10 and CIFAR-100 have 50k training and 10k testing images of size 32 ? 32 pixels, where CIFAR-10 has 10 classes and CIFAR-100 has 100 classes and all training and testing sets have equal number of images per classes. As CIFAR-10 and CIFAR-100 data sets originally do not contain label noise, we follow the literature <ref type="bibr" target="#b31">[30]</ref> and add the following synthetic noise types (see Sec. 3.1): symmetric (with noise rate ? ? {0.2, 0.5, 0.8, 0.9}, as defined in Sec. 3.1), asymmetric (using the mapping in <ref type="bibr" target="#b31">[30,</ref><ref type="bibr" target="#b46">44]</ref>, with ? jc = 0.4), and semantic <ref type="bibr" target="#b28">[27]</ref> (with noisy labels based on a trained VGG <ref type="bibr" target="#b49">[47]</ref>, DenseNet (DN), and ResNet (RN)).</p><p>The CNWL dataset <ref type="bibr" target="#b24">[23]</ref> is a benchmark to study real-world web label noise in a controlled setting. Both images and labels are crawled from the web and the noisy labels are determined by matching images. The controlled setting provide different magnitudes of label corruption in real applications, varying from 0% to 80%. We study the red Mini-ImageNet that consists of 50k training images and 5k test images, with 100 classes. The original 84?84-pixel images are resized to 32?32 pixels. The noise rates are 20%, 60% and 80%, as used in <ref type="bibr" target="#b55">[53]</ref>.</p><p>Clothing1M consists of 1 million training images acquired from online shopping websites and it has 14 classes. As the images from the data set vary in size, we resized the images to 256 ? 256 for training, as used in <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b31">30]</ref>. The data set provide additional clean sets for training, validation, and testing of 50k, 14k and 10k images, respectively. For our experiments we do not use any of the clean training or validation sets, but we use the clean test set for evaluation.</p><p>WebVision contains 2.4 million images collected from the internet, with the same 1000 classes from ILSVRC12 <ref type="bibr" target="#b8">[9]</ref> and images resized to 256 ? 256 pixels. It provides a clean test set of 50k images, with 50 images per class. We compare our model using the first 50 classes of the Google image subset, as used in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation</head><p>For CIFAR-10 and CIFAR-100 we used a 18-layer PreaAct-ResNet-18 (PRN18) <ref type="bibr" target="#b18">[18]</ref> as our backbone model <ref type="bibr" target="#b31">[30]</ref>. The models are trained with stochastic gradient descent (SGD) with momentum of 0.9, weight decay of 0.0005 and batch size of 64. For the self-supervised pre-training learning task, we adopt SimCLR <ref type="bibr" target="#b5">[6]</ref> with a batch size of 1024, SGD optimiser with a learning rate of 0.4, decay rate of 0.1, momentum of 0.9 and weight decay of 0.0001, and run it for 800 epochs. This pre-trained model produces feature representations of 128 dimensions. Using these representations we mine K = 20 nearest neighbours (as in <ref type="bibr" target="#b12">[12]</ref>) for each sample to form the sets {N x i } |D| i=1 , defined in Sec. 3.2. In the supervised training stage, the model is trained with SGD with momentum of 0.9, weight decay of 0.0005 and batch size of 64. The learning rate is 0.02 which is reduced to 0.002 in the middle of the training. The WarmUp and total number of epochs is defined according to each data set, as defined in <ref type="bibr" target="#b31">[30]</ref>. For CIFAR-10 and CIFAR-100, PRN18 is trained with a WarmUp stage of 30 epochs for CIFAR-10 and 10 epochs for CIFAR-100, and 300 epochs of final training. In our training, we also use a co-teaching approach, as in <ref type="bibr" target="#b16">[16,</ref><ref type="bibr" target="#b31">30,</ref><ref type="bibr" target="#b34">33]</ref>. We estimate noise rate with |U |/|D|, defined in Eq. 3 -if this ratio is larger than 50%, we use strong data augmentation with cutout <ref type="bibr" target="#b7">[8]</ref>, otherwise, we use standard augmentation (i.e., crop and flip).</p><p>For the semantic noise from <ref type="bibr" target="#b28">[27]</ref>, we use DenseNet-100 <ref type="bibr" target="#b21">[20]</ref> as backbone, following <ref type="bibr" target="#b28">[27]</ref>. The pre-training stage uses the same parameters as in CIFAR, except that we use a batch size of 512. In the supervised stage, we use the same protocol as <ref type="bibr" target="#b28">[27]</ref>, which uses SGD with momentum 0.9, weight decay 10-4 and learning rate of 0.1 that is divided by 10 after epochs 40 and 80 for CIFAR-10 (which runs for 120 epochs in total), and after epochs 80, 120 and 160 for CIFAR-100 (runs for 200 epochs in total). The WarmUp epochs is the same as CIFAR-10/CIFAR-100.</p><p>For red Mini-Imagenet we use PRN18 as backbone, following <ref type="bibr" target="#b55">[53]</ref>. For the self-supervised pre-training task, we adopt SimCLR <ref type="bibr" target="#b5">[6]</ref> with batch size 128. All other parameters for the self-supervised pre-training are the same as described for CIFAR. For the supervised stage, we adopt the implementation of <ref type="bibr" target="#b55">[53]</ref>, where we train for 300 epochs, relying on SGD with learning rate of 0.02 (decreased by a factor of ten at epochs 200 and 250), momentum of 0.9 and weight decay of 5e-4. For Clothing1M, we use ResNet-50 as backbone, following <ref type="bibr" target="#b31">[30]</ref>. In this protocol, a ResNet-50 with ImageNet <ref type="bibr" target="#b8">[9]</ref> pre-trained weights is used and we decided to not use the selfsupervised stage for this experiment because the model is already pre-trained. The ResNet-50 is trained for 80 epochs, including a WarmUp stage of 1 epoch, with a batch size of 32, SGD with a learning rate of 0.002 (divided by 10 at epoch 40), momentum of 0.9 and weight decay of 0.0001.</p><p>For Webvision, we use InceptionResNet-V2 as backbone, following <ref type="bibr" target="#b31">[30]</ref>. For selfsupervised pre-training, we adopt MoCo-v2 (4-GPU training) <ref type="bibr" target="#b6">[7]</ref>, trained with 100 epochs, with a batch size of 128, SGD with a learning rate of 0.015 (divided by 10 at epoch 50), momentum of 0.9 and weight decay of 0.0001, and run it for 100 epochs with a WarmUp stage of 1 epoch. The feature representations learned from this process have 128 dimensions. All the other parameters were the same as described above for CIFAR. <ref type="figure">Fig. 2</ref> shows the PropMix hard noisy sample filtering process for CIFAR-100 with 80% symmetric noise, at epoch 150 (i.e., half of the training epochs). The left histogram shows the clean and noisy classification from Eq. 3, while the right one shows the easy and hard noisy sample classification from Eq. (4). As can be seen, the use of confidence in Eq. (4), instead of the loss from Eq. 3, is an effective way to remove noisy samples with wrong prediction. We also evaluated the quality of the filtering stage using PropMix for CIFAR-100 under different symmetric noise rates. <ref type="figure">Fig. 3(a) and (b)</ref> show the precision and recall of the hard noisy sample filtering as a function of training epochs. These graphs show that the larger the noise, the higher the precision. For example, for 90% noise, the hard noise classification reaches 80% at the end of training. <ref type="figure">Fig. 3(c)</ref> shows the size of the noisy sample set during training. <ref type="figure">Fig. 3(d)</ref> compares the easy noise set re-labelling accuracy by PropMix with a baseline that does not filter out hard noisy samples. We can see that PropMix is substantially more accurate, which enables a more successful training with the easy noisy samples. We also evaluated the impact of the parameters ? and ? in the training. <ref type="figure">Fig. 4</ref> shows the PropMix accuracy for CIFAR-10 (a-b) and CIFAR-100 (c-d) under different symmetric noise rates, varying the parameters ?, which is related to the clean/noisy filtering from Eq. 3, and ? , which related to hard noise filtering, from Eq. 4. <ref type="figure">Fig. 4(a,c)</ref> vary ? while fixing ? = 0.5, and <ref type="figure">Fig. 4(b,d)</ref> vary ? while fixing ? = 0.5. We can see that ? does not have a strong impact on the accuracy, which motivated us to use ? = 0.5 independently of the noise rate and data set. The parameter ? is shown to decrease accuracy at higher values. In PropMix we use ? = 0.5 as in other works <ref type="bibr" target="#b31">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">PropMix Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Comparison with State-of-the-Art</head><p>For CIFAR-10 and CIFAR-100, we evaluate our model using symmetric label noise ranging from 20% to 90%, and 40% asymmetric noise. We report both the best test accuracy across all epochs and the averaged test accuracy over the last 10 epochs of training, similar to <ref type="bibr" target="#b31">[30]</ref>. Tab. 1 shows that for CIFAR-10 and CIFAR-100 data sets, our method obtains the best results for almost all evaluated noisy rates. Tab. 2 shows the results for semantic noise <ref type="bibr" target="#b28">[27]</ref>, which is harder and more realistic than the synthetic noise, where PropMix shows significantly more accurate results than any of the methods in <ref type="bibr" target="#b28">[27]</ref>. In Tab. 4, we show that PropMix improves the SOTA by a large margin on the Red Mini-ImageNet semantic noise experiment from <ref type="bibr" target="#b24">[23]</ref>.</p><p>We also evaluate PropMix on the noisy large-scale datasets WebVision <ref type="bibr" target="#b32">[31]</ref> and Cloth-ing1M <ref type="bibr" target="#b54">[52]</ref>. Tab. 5 shows competitive results compared to SOTA. As Clothing1M experimental protocol uses ImageNet pre-trained weights, the training could not benefit from Prop-   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>We show the ablation study of PropMix with CIFAR-10 and CIFAR-100 under symmetric and asymmetric noises at several rates. In Tab. 6, we show Self-superv. pre-train, which has the result from self-supervised training (with SCAN <ref type="bibr" target="#b12">[12]</ref>), where the result is the same across different noise rates because it never uses the noisy labels for training. SSL (DivideMix) <ref type="bibr" target="#b31">[30]</ref>) is current SOTA in noisy label learning, but it has worse accuracy in high noise rate problems (&gt; 80% noise) than SCAN.  <ref type="table">Table 4</ref>: Test accuracy (%) for Red Mini-ImageNet <ref type="bibr" target="#b24">[23]</ref>. Results from baseline methods are as presented in <ref type="bibr" target="#b55">[53]</ref>. Top methods (?1%) are in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Test Accuracy</head><p>Cross-Entropy <ref type="bibr" target="#b31">[30]</ref> 69.21 M-correction <ref type="bibr" target="#b2">[3]</ref> 71.00 PENCIL <ref type="bibr" target="#b56">[54]</ref> 73.49 DeepSelf <ref type="bibr" target="#b17">[17]</ref> 74.45 CleanNet <ref type="bibr" target="#b29">[28]</ref> 74.69 DivideMix <ref type="bibr" target="#b31">[30]</ref> 74.76 PropMix (ours) 74.30  <ref type="table">Table 6</ref>: In this ablation study we show the test accuracy (%) on CIFAR-10 and CIFAR-100 under symmetric and asymmetric noise problems. First, we show the results of selfsupervised pre-training with SCAN <ref type="bibr" target="#b12">[12]</ref>. Then we show the current SOTA SSL learning for noisy label DivideMix <ref type="bibr" target="#b31">[30]</ref>. Next, we show the results of DivideMix pre-trained with SCAN. Then, we remove the MSE loss to train DivideMix re-labelled noisy samples, and add the hard noisy filtering stage (but keeping the MSE loss) to the pre-trained DivideMix. The last row shows our PropMix . The top results within 1% are highlighted.  <ref type="bibr" target="#b36">[35]</ref> 62.68 84.00 MentorNet <ref type="bibr" target="#b23">[22]</ref> 63.00 81.40 Co-teaching <ref type="bibr" target="#b16">[16]</ref> 63.58 85.20 Iterative-CV <ref type="bibr" target="#b4">[5]</ref> 65.24 85.34 MentorMix <ref type="bibr" target="#b24">[23]</ref> 76.00 90.20 DivideMix <ref type="bibr" target="#b31">[30]</ref> 77.32 91.64 ELR+ <ref type="bibr" target="#b34">[33]</ref> 77.78 91.68 PropMix (Ours) 78.84 90.56 <ref type="table">Table 3</ref>: Test accuracy (%) for WebVision <ref type="bibr" target="#b32">[31]</ref> by methods trained with 100 epochs. Baselines come from <ref type="bibr" target="#b31">[30]</ref>. Top methods within 1% in bold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We presented PropMix, a noisy label training algorithm to filter out hard noisy samples, remove samples with incorrect label and incorrect prediction from noisy set with the goal to keep the easy noisy samples that have a higher chance to be correctly relabelled. PropMix reduces noisy-dependent parameters, while promoting the use of the entire filtered noisy set in a fully supervised training, with proportional MixUp data augmentation on the clean set. Our results on CIFAR-10/100, Red Mini-ImageNet, and WebVision outperform the SOTA methods, also demonstrating robustness to over-fitting in several noise rates with substantial improvement in high noise problems.</p><p>IR and GC gratefully acknowledge the support of the Australian Research Council through the Centre of Excellence for Robotic Vision CE140100016 and Future Fellowship (to GC) FT190100525. GC acknowledges the support by the Alexander von Humboldt-Stiftung for the renewed research stay sponsorship.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :Figure 3 :Figure 4 :</head><label>234</label><figDesc>PropMix classification of easy and hard noisy samples. From the (normalized) loss of all samples, PropMix uses GMM to split clean and noisy samples, and then, using the largest classification confidence, it uses another GMM to split the hard and easy noisy samples. This example was run on CIFAR-100, 80% noise rate at epoch 150. Evaluation of hard noisy sample filtering, for CIFAR-100, with 50% to 90% symmetric noise. Graphs in (a,b) show the precision and recall of the classification of hard noisy samples (i.e., noisy samples that are incorrectly labelled by the model), Graph (c) shows the estimated noise set size, and (d) shows the classification accuracy of the re-labelled easy noisy samples, compared to the baseline that does not filter out hard noisy samples. Evaluation of the parameters ? (clean sample threshold) and ? (hard noisy sample threshold), for CIFAR-10 and CIFAR-100, with 20% to 90% symmetric noise. Graphs in (a,b) show the best accuracy when training using PropMix with CIFAR-10, and graphs (c,d) show results for CIFAR-100, at different values of ? and ? . Graph (a,c) vary the parameter ?, while fixes ? at 0.5. Graph (b,d) vary the parameter ? while fixes ? at 0.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Self-supervised Pretrain CNN Loss Clean Samples GMM Hard Noisy Samples Easy Noisy Samples</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Supervised Training</cell><cell></cell></row><row><cell></cell><cell></cell><cell>shared weights</cell><cell></cell><cell>Filtering</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Noisy</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Loss</cell><cell>Samples</cell></row><row><cell>CNN</cell><cell>Contrastive Loss</cell><cell>CNN</cell><cell></cell></row><row><cell>Noisy</cell><cell></cell><cell>MixUp</cell><cell>Label</cell></row><row><cell>data set</cell><cell></cell><cell>(proportional)</cell><cell>estimation</cell><cell>Discard</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Test accuracy (%) for all competing methods on CIFAR-10 and CIFAR-100 under symmetric and asymmetric noises. Results from related approaches are as presented in<ref type="bibr" target="#b31">[30]</ref>. Top methods within 1% are in bold.</figDesc><table><row><cell>Data set</cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell>CIFAR-100</cell><cell></cell></row><row><cell>Method/ noise ratio</cell><cell cols="6">DenseNet (32%) ResNet (38%) VGG (34%) DenseNet (34%) ResNet (37%) VGG (37%)</cell></row><row><cell>D2L + RoG [26]</cell><cell>68.57</cell><cell>60.25</cell><cell>59.94</cell><cell>31.67</cell><cell>39.92</cell><cell>45.42</cell></row><row><cell>CE + RoG [26]</cell><cell>68.33</cell><cell>64.15</cell><cell>70.04</cell><cell>61.14</cell><cell>53.09</cell><cell>53.64</cell></row><row><cell>Bootstrap + RoG [26]</cell><cell>68.38</cell><cell>64.03</cell><cell>70.11</cell><cell>54.71</cell><cell>53.30</cell><cell>53.76</cell></row><row><cell>Forward + RoG [26]</cell><cell>68.20</cell><cell>64.24</cell><cell>70.09</cell><cell>53.91</cell><cell>53.36</cell><cell>53.63</cell></row><row><cell>Backward + RoG [26]</cell><cell>68.66</cell><cell>63.45</cell><cell>70.18</cell><cell>54.01</cell><cell>53.03</cell><cell>53.50</cell></row><row><cell>PropMix (Ours)</cell><cell>84.25</cell><cell>82.51</cell><cell>85.74</cell><cell>60.98</cell><cell>58.44</cell><cell>60.01</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Test accuracy (%) for the semantic noise benchmark<ref type="bibr" target="#b27">[26]</ref>, where baseline results are from<ref type="bibr" target="#b27">[26]</ref>. The results with (*) were produced by locally running the published code provided by the authors. Top methods (?1%) are in bold.Mix strategy. Tab. 3 shows the Top-1/-5 test accuracy using the WebVision and ILSVRC12 test sets. Results show that PropMix is slightly better than the SOTA for WebVision test set. This suggests that our approach is also effective in large-scale, low noise rate problems.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc><ref type="bibr" target="#b55">[53]</ref> 47.36 42.70 37.30 29.76 MixUp [58] 49.10 46.40 40.58 33.58 DivideMix [30] 50.96 46.72 43.14 34.50 MentorMix [23] 51.02 47.14 43.80 33.46 FaMUS [53] 51.42 48.06 45.10 35.50 PropMix (Ours) 61.24 56.22 52.84 43.42</figDesc><table><row><cell>Method/ noise ratio 20%</cell><cell>40%</cell><cell>60%</cell><cell>80%</cell></row><row><cell>Cross-entropy</cell><cell></cell><cell></cell><cell></cell></row></table><note>Self-superv. pre-train+SSL (DivideMix) pre-trains DivideMix with SCAN to improve accuracy in high-noise rate problems, without affecting low-noise rate results. Just removing the MSE loss in Self-superv. pre-train+SSL (DivideMix) w/o MSE does not help much because the noisy samples from DivideMix is not accurately re- labelled. Adding our hard noisy sample filtering in Self-superv. pre-train+SSL (DivideMix) + filtering with MSE does not improve accuracy because DivideMix still uses MSE loss to train the easy noisy samples, which is unnecessary given that these samples are accurately</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Test accuracy (%) for Cloth-ing1M<ref type="bibr" target="#b54">[52]</ref> by methods trained with 80 epochs. Baselines come from<ref type="bibr" target="#b31">[30]</ref>. Top methods within 1% are in bold.re-labelled and can be trained with the CE loss. PropMix can achieve better results for high noise rates, using a simpler loss function (uses only CE loss) with less hyper-parameters, and removing hard noisy samples.</figDesc><table><row><cell>dataset</cell><cell></cell><cell></cell><cell>CIFAR-10</cell><cell></cell><cell></cell><cell></cell><cell cols="2">CIFAR-100</cell><cell></cell></row><row><cell>Noise type</cell><cell></cell><cell cols="2">sym.</cell><cell></cell><cell>asym.</cell><cell></cell><cell cols="2">sym.</cell><cell></cell></row><row><cell>Method/ noise ratio</cell><cell cols="4">20% 50% 80% 90%</cell><cell>40%</cell><cell cols="4">20% 50% 80% 90%</cell></row><row><cell>Self-superv. pre-train [12]</cell><cell>81.6</cell><cell>81.6</cell><cell>81.6</cell><cell>81.6</cell><cell>81.6</cell><cell>44.0</cell><cell>44.0</cell><cell>44.0</cell><cell>44.0</cell></row><row><cell>SSL (DivideMix) [30]</cell><cell>96.1</cell><cell>94.6</cell><cell>93.2</cell><cell>76.0</cell><cell>93.4</cell><cell>77.3</cell><cell>74.6</cell><cell>60.2</cell><cell>31.5</cell></row><row><cell>Self-superv. pre-train + SSL (DivideMix)*</cell><cell>96.2</cell><cell>94.8</cell><cell>93.9</cell><cell>92.2</cell><cell>93.3</cell><cell>77.4</cell><cell>74.7</cell><cell>66.7</cell><cell>56.2</cell></row><row><cell>Self-superv. pre-train + SSL (DivideMix)* w/o MSE</cell><cell>96.2</cell><cell>95.2</cell><cell>91.4</cell><cell>85.1</cell><cell>93.3</cell><cell>77.2</cell><cell>73.9</cell><cell>62.6</cell><cell>52.1</cell></row><row><cell>Self-superv. pre-train + SSL (DivideMix)* + filtering with MSE</cell><cell>96.3</cell><cell>95.3</cell><cell>93.8</cell><cell>91.6</cell><cell>93.0</cell><cell>77.3</cell><cell>75.3</cell><cell>66.7</cell><cell>55.7</cell></row><row><cell>PropMix (Ours)</cell><cell>96.4</cell><cell>95.8</cell><cell>93.9</cell><cell>93.5</cell><cell>94.9</cell><cell>77.4</cell><cell>74.6</cell><cell>67.3</cell><cell>58.6</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">? 2021. The copyright of this document resides with its authors. It may be distributed unchanged freely in print or electronic forms.arXiv:2110.11809v1 [cs.CV] 22 Oct 2021 2 CORDEIRO ET AL.: PROPMIX: HARD SAMPLE FILTERING AND PROPORTIONAL MIXUP</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">CORDEIRO ET AL.: PROPMIX: HARD SAMPLE FILTERING AND PROPORTIONAL MIXUP</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1: PropMix (PM)</head><p>Input: D, number of epochs E, clean sample threshold ?, hard sample threshold ? // Self-supervised pre-training</p><p>// Estimate sets of clean and noisy samples</p><p>, ?) // Estimate sets of hard noisy and easy noisy samples</p><p>Update ? (k1), ? (k2) with L from (5) 21 end</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diagnostic accuracy of deep learning in medical imaging: a systematic review and meta-analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ravi</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Viknesh</forename><surname>Sounderajah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Karthikesalingam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hutan</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ara</forename><surname>Ashrafian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Darzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NPJ digital medicine</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Meta soft label generation for noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?rkem</forename><surname>Algan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilkay</forename><surname>Ulusoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 25th International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021" />
			<biblScope unit="page" from="7142" to="7148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Unsupervised label noise modeling and loss correction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="312" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avital</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.02249</idno>
		<title level="m">MixMatch: A Holistic Approach to Semi-Supervised Learning. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2019-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Understanding and utilizing deep neural networks trained with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benben</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangyong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.05040</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709</idno>
		<title level="m">A Simple Framework for Contrastive Learning of Visual Representations. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.04297</idno>
		<title level="m">Improved Baselines with Momentum Contrastive Learning. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2020-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Autoaugment: Learning augmentation strategies from data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barret</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dandelion</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Mane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="113" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deliang Fan, and Boqing Gong. A semi-supervised twostage approach to learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yifan</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liqiang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1215" to="1224" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>12cordeiro</surname></persName>
		</author>
		<title level="m">PROPMIX: HARD SAMPLE FILTERING AND PROPORTIONAL MIXUP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Classification in the presence of label noise: a survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beno?t</forename><surname>Fr?nay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michel</forename><surname>Verleysen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="845" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Scan: Learning to classify images without labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Wouter Van Gansbeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stamatios</forename><surname>Vandenhende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Georgoulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Proesmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Robust loss functions under label noise for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aritra</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Himanshu</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Sastry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A survey of deep learning techniques for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sorin</forename><surname>Grigorescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Trasnea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiberiu</forename><surname>Cocias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gigel</forename><surname>Macesanu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Field Robotics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="362" to="386" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Pumpout: A meta approach for robustly training deep neural networks with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Co-teaching: Robust training of deep neural networks with extremely noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="8527" to="8537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep self-learning from noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangfan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5138" to="5147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Springer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.05722</idno>
		<title level="m">Momentum Contrast for Unsupervised Visual Representation Learning. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2019-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Forrest</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Moskewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Densenet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.1869</idno>
		<title level="m">Implementing efficient convnet descriptor pyramids</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Photometric transformer networks and label adjustment for breast density prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lee</forename><surname>Jaehwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoo</forename><surname>Donggeun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename><surname>Hyo-Eun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2304" to="2313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Beyond synthetic noise: Deep learning on controlled noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Di</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mason</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilong</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Nlnl: Negative learning for noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youngdong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juseung</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="101" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust inference via generative classifiers for handling noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukmin</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3763" to="3772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Robust inference via generative classifiers for handling noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sukmin</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kibok</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.11300</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cleannet: Transfer learning for scalable image classifier training with label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuang-Huei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5447" to="5456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning to learn from noisy labeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkang</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qi</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohan</forename><forename type="middle">S</forename><surname>Kankanhalli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5051" to="5059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junnan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07394</idno>
		<title level="m">DivideMix: Learning with Noisy Labels as Semi-supervised Learning. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2020-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Webvision database: Visual learning and understanding from web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Limin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eirikur</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02862</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Jeroen Awm Van Der Laak, Bram Van Ginneken, and Clara I S?nchez. A survey on deep learning in medical image analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geert</forename><surname>Litjens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thijs</forename><surname>Kooi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Babak</forename><forename type="middle">Ehteshami</forename><surname>Bejnordi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arnaud Arindra Adiyoso</forename><surname>Setio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesco</forename><surname>Ciompi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohsen</forename><surname>Ghafoorian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical image analysis</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="60" to="88" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Early-learning regularization prevents memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Niles-Weed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narges</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Fernandez-Granda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00151</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Early-learning regularization prevents memorization of noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Niles-Weed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Narges</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Fernandez-Granda</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.00151</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Dimensionality-driven learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">E</forename><surname>Houle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shutao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sudanthi</forename><surname>Wijewickrema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="3355" to="3364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Normalized loss functions for deep learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanxun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarah</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6543" to="6553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Decoupling&quot; when to update&quot; from&quot; how to update</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>Malach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Shalev-Shwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="960" to="970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>14cordeiro</surname></persName>
		</author>
		<title level="m">PROPMIX: HARD SAMPLE FILTERING AND PROPORTIONAL MIXUP</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Rboost: Label noise-robust boosting algorithm based on a nonconvex loss function and the numerically stable base learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiguang</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ge</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoguo</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2216" to="2228" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Image segmentation using deep learning: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shervin</forename><surname>Minaee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><forename type="middle">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fatih</forename><surname>Porikli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Antonio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nasser</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demetri</forename><surname>Kehtarnavaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<idno type="arXiv">arXiv:1910.01842</idno>
		<title level="m">Self: Learning to filter noisy labels with self-ensembling</title>
		<editor>Duc Tam Nguyen, Chaithanya Kumar Mummadi, Thi Phuong Nhung Ngo, Thi Hoai Phuong Nguyen, Laura Beggel, and Thomas Brox</editor>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kento</forename><surname>Nishi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Rich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>H?llerer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02130</idno>
		<title level="m">Augmentation Strategies for Learning with Noisy Labels. arXiv e-prints, art</title>
		<imprint>
			<date type="published" when="2021-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.08741</idno>
		<title level="m">Towards robust learning with different label noise distributions</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Multi-objective interpolation training for robustness to label noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Ortego</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Arazo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E O&amp;apos;</forename><surname>Noel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcguinness</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.04462</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Making deep neural networks robust to label noise: A loss correction approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgio</forename><surname>Patrini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><forename type="middle">Krishna</forename><surname>Menon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lizhen</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1944" to="1952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Identifying mislabeled data using the area under the margin ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><forename type="middle">R</forename><surname>Elenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.10528</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning to reweight examples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengye</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyuan</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4334" to="4343" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for largescale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Selfie: Refurbishing unclean samples for robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hwanjun</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minseok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jae-Gil</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5907" to="5915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Imae for noise-robust learning: Mean absolute error does not treat examples equally and gradient magnitude&apos;s variance matters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil M</forename><surname>Robertson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.12141</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinshao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elyor</forename><surname>Kodirov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil M</forename><surname>Robertson</surname></persName>
		</author>
		<title level="m">Proselflc: Progressive self label correction for training robust deep neural networks. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Symmetric cross entropy for robust learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="322" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Faster meta update strategy for noise-robust deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Youjiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linchao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Probabilistic end-to-end noise correction for learning with noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7017" to="7025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">How does disagreement help generalization against label corruption?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingrui</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangchao</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.04215</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Iterative cross learning on noisy labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodi</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung-Shuo</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Mcmains</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="757" to="765" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.03530</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">40] are formed by an ensemble of two classifiers, where the classifier structure is the same, but their parameters are denoted by ? (1), ? (2) ? ?. The training for ? (1) influences ? (2) and vice-versa, where this can be achieved by cotraining [30, 33] or student-teacher [40] approaches. Our training relies on co-training. The estimation of class prediction for label estimation and evaluation are given by the average outputs of the models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>16cordeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PROPMIX: HARD SAMPLE FILTERING AND PROPORTIONAL MIXUP A PropMix Algorithm SOTA noise-robust classifiers</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
	<note>The pseudo-code for the training of PropMix is shown in Algorithm 1</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
