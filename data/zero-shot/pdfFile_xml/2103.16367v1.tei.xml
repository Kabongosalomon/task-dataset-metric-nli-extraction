<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Complementary Relation Contrastive Distillation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinguo</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Tang</surname></persName>
							<email>tangshixiang@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="institution">The University of Sydney</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="laboratory">Sensetime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename><surname>Shijie</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Shenzhen Institutes of Advanced Technology</orgName>
								<address>
									<region>CAS</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yakun</forename><surname>Liu</surname></persName>
							<email>liuyakun1@sensetime.com</email>
							<affiliation key="aff2">
								<orgName type="laboratory">Sensetime Group Limited</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingzhe</forename><surname>Rong</surname></persName>
							<email>mzrong@mail.xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aijun</forename><surname>Yang</surname></persName>
							<email>yangaijun@mail.xjtu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Complementary Relation Contrastive Distillation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T13:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Knowledge distillation aims to transfer representation ability from a teacher model to a student model. Previous approaches focus on either individual representation distillation or inter-sample similarity preservation. While we argue that the inter-sample relation conveys abundant information and needs to be distilled in a more effective way. In this paper, we propose a novel knowledge distillation method, namely Complementary Relation Contrastive Distillation (CRCD), to transfer the structural knowledge from the teacher to the student. Specifically, we estimate the mutual relation in an anchor-based way and distill the anchorstudent relation under the supervision of its corresponding anchor-teacher relation. To make it more robust, mutual relations are modeled by two complementary elements: the feature and its gradient. Furthermore, the low bound of mutual information between the anchor-teacher relation distribution and the anchor-student relation distribution is maximized via relation contrastive loss, which can distill both the sample representation and the inter-sample relations. Experiments on different benchmarks demonstrate the effectiveness of our proposed CRCD.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Knowledge distillation aims to transfer the knowledge from one deep learning model (the teacher) to another (the student), such as distilling a large network into a smaller one <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b11">12]</ref> or ensembling a collection of models into a single model <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b44">45]</ref>. It has a wide range of applications in the industry especially when a neural network needs to be efficiently deployed on devices with limited computational resources <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b37">38]</ref>. Although great progress has been achieved in the knowledge distillation ? This work was done when Jinguo Zhu was an intern at SenseTime. ? Corresponding authors. regime, there is still no consensus on what kind of knowledge really needs to be preserved in the distillation <ref type="bibr" target="#b13">[14]</ref>.</p><p>As one of the most effective distillation methods, CRD <ref type="bibr" target="#b40">[41]</ref> holds the view that the representational knowledge is structured. So It tries to capture the correlations and higherorder output dependencies for each sample, which is different from the original KD objective introduced in <ref type="bibr" target="#b18">[19]</ref> that treats all dimensions as independent information. CRD leverages the family of contrastive objectives <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b3">4]</ref> to maximize a lower-bound of the mutual information between the teacher and student representations. It essentially performs knowledge distillation based on the individual samples, enforcing the representation consistency between the teacher model and the student model.</p><p>However, neither CRD nor other sample-based distillation methods can effectively preserve inter-sample relations, which are more valuable than the sample representations themselves in many practical tasks, e.g., retrieval and classification. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, when using sample contrastive distillation methods, e.g., CRD, the optimized forces from other neighbors just push the student representation of sample A away when contrasted negatively, which may not be optimal and can break the latent structural geometry of neighboring samples. Some recent works have shown that transferring the mutual similarity instead of actual representation is beneficial to student representation learning <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b32">33]</ref>. These methods directly estimate the relations in teacher space by computing the intersample similarities, then mimic these similarities in the student space via L 2 loss or KL divergence, ignoring the highorder dependency within the representation in both relation estimation and knowledge distillation.</p><p>To robustly distill the structural knowledge of the teacher space, we define a new cross-space relation between two samples and supervise this new relation by its corresponding relation in the teacher representation space. More specifically, given the teacher and student representation of one sample, we select a neighboring sample's representation from the teacher representation space as an anchor. The anchor-student relation is encouraged to be consistent with the anchor-teacher relation. Our method brings at least three merits for distillation. (1) It simultaneously optimizes the representation and relation. When the anchor-student relation is pushed to be consistent with the anchor-teacher relation, the student representation is actually optimized along the optimal direction of representation learning. <ref type="bibr" target="#b1">(2)</ref> The anchor-student relation is more effective for distillation compared with the student-student relation (where two representations are both from the student space) in the conventional KD family <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>. The student-student relation is unstable because the two representations in the student space are not well optimized and they will drift significantly during distillation, while the anchor representation within the anchor-student relation is fixed, which can effectively optimize the representation in the student space. (3) As the anchor can be randomly selected from the neighborhood of the considered sample, the student representation of one sample is supervised by multiple relations from different anchors, which guarantees the robustness of the distillation.</p><p>The representation relation is modeled by two complementary elements: the feature and its gradient. The fea-ture relation reflects the structural information in representational space, and the gradient relation is computed by the feature gradients after backward propagation. As gradients measure the fastest rate and direction for loss minimization, gradient relation can explore the structural information of optimization kinetics in representational space <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39]</ref>. During the distillation, we maximize the mutual information between the anchor-teacher relation and the anchorstudent relation for both two elements. The maximization problem can further surrogate to maximize the lower bound of mutual information which has been well solved by contrastive learning <ref type="bibr" target="#b45">[46]</ref>. Our method is therefore denoted by Complementary Relation Contrastive Distillation(CRCD).</p><p>In summary, the main contributions of CRCD are threefold. First, we define a new anchor-based cross space relation and adopt it to effectively and robustly distill both sample representations and inter-sample relations. Second, the new relation is modeled by two complementary elements, i.e., the feature and its gradients, which capture the structure information of the feature and the optimization kinetics, respectively. Last, we maximize the low bound of mutual information between the anchor-teacher relation and the anchor-student relation and derive an efficient solution in the form of contrastive learning. Extensive experiments empirically validate the effectiveness of CRCD and further improve the current state-of-the-art in various benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Knowledge Distillation. There has been a rising interest in distilling knowledge from one model to another, in which the core issue is that what is the knowledge learned by a teacher and how to best distill the knowledge into a student. In <ref type="bibr" target="#b18">[19]</ref>, the soft probability distribution is transferred by using a higher temperature value. Compared to the onehot label, soft targets can contain much more valuable information that defines a rich similarity structure over the data. Furthermore, not only the soft labels but also the hints from intermediate layers are used to train student networks in <ref type="bibr" target="#b34">[35]</ref>. Moreover, the attention map <ref type="bibr" target="#b50">[51]</ref> and the flow of solution procedure (FSP) <ref type="bibr" target="#b49">[50]</ref> are used to transfer knowledge between networks. These works focus on distilling the knowledge modeled by learned presentations of samples themselves, however, ignore the mutual relations between samples, which contain rich structural information learned by the teacher.</p><p>There are a few recent works analyzing and exploiting the mutual relation between data samples <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25]</ref>. In particular, similarity-preserving knowledge <ref type="bibr" target="#b42">[43]</ref> proposes to transfer the knowledge presented as similar activation between input pairs. In <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b31">[32]</ref>, the sample relations are modeled explicitly to transfer knowledge. However, these methods all use low-dimensional relation methods, such as cosine similarity <ref type="bibr" target="#b42">[43]</ref> or gaus- sian RBF <ref type="bibr" target="#b33">[34]</ref> between features, to model the mutual relation, which may be suboptimal for modeling complex intersample interdependencies. Instead, in our paper, we design sub-networks to learn the high-dimensional across-space relations which can capture the complex mutual dependencies of deep representations from any two feature spaces. Contrastive Learning. Contrastive Learning serves as the core idea of several recent works on self-supervised representation learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b12">13]</ref>. Contrastive losses such as NCE <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b19">20]</ref> measure the similarities of data samples in a deep representation space, which learn representation by contrasting positive and negative representation pairs. For knowledge distillation, CRD <ref type="bibr" target="#b40">[41]</ref> is the first study that combines contrastive learning with knowledge distillation, which aims to maximize mutual information <ref type="bibr" target="#b2">[3]</ref> between the teacher and student representations. Besides, SSKD <ref type="bibr" target="#b46">[47]</ref> proposes to use contrastive tasks as selfsupervised pretext tasks, which can facilitate the extraction of richer knowledge from the teacher to the student. From the usage of the contrastive loss, our method is more similar to CRD, but our objective is the mutual relations of deep representations, instead of the representations themselves. <ref type="figure" target="#fig_0">Fig. 1</ref> presents the overall flowchart of our proposed CRCD. Given a teacher network ? T and a student network ? S , we denote the representation of an input x produced by the two networks as ? T (x) and ? S (x), respectively. Let x i and x j be two training samples randomly chosen from the sample set X. We denote the relation in the teacher space as r T i,j , where r T i,j is a vector computed by a sub-network M T that takes ? T (x i ) and ? T (x j ) as inputs. We further define a new relation r T,S i,j computed by another sub-network M T,S . It is noteworthy that the inputs ? T (x i ) and ? S (x j ) for M T,S are from different spaces. Regarding ? T (x i ) as an anchor representation, the cross-space anchor-student relation r T,S i,j is expected to be consistent with the teacher-space anchor-teacher relation r T i,j , which not only preserves the relation between x i and x j , but also drives the ? S (x j ) to be consistent with ? T (x j ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>In the following sub-sections, we first demonstrate how to use contrastive learning to perform the relation distillation, then two complementary elements are introduced to model the representation relations, and the implementation details and some discussions will be presented at last. The complete mathematical derivation refers to the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Relation Contrastive Distillation</head><p>Assume that we are given a set of training examples with empirical data distribution p(X), the sampling procedure for the conditional marginal distributions p(R T |X), p(R T,S |X) are modeled as</p><formula xml:id="formula_0">xi, xj ? p(X), r T i,j = M T (? T (xi), ? T (xj)), xm, xn ? p(X), r T,S m,n = M T,S (? T (xm), ? S (xn))<label>(1)</label></formula><p>respectively. While the sampling procedure of the conditional joint distribution p(R T , R T,S |X) is modeled as:</p><formula xml:id="formula_1">xi, xj ? p(X), r T i,j = M T (? T (xi), ? T (xj)), r T,S i,j = M T,S (? T (xi), ? S (xj)).</formula><p>(2)</p><p>For ease of notation, we utilize p(R T ), p(R T,S ) and p(R T , R T,S ) to briefly represent p(R T |X), p(R T,S |X) and p(R T , R T,S |X). Intuitively, we aim to maximize the mutual information (MI) of the two relation distributions from R T and R T,S , which is</p><formula xml:id="formula_2">I(R T , R T,S ) = E p(R T ,R T ,S ) log p(R T , R T,S ) p(R T )p(R T,S ) .<label>(3)</label></formula><p>MI Lower Bound. To derive a solvable loss function, we define a distribution q with latent variable C which indicates whether the relation tuple (r T , r T,S ) is drawn from the joint distribution or the product of marginal distributions:</p><formula xml:id="formula_3">q(R T , R T,S |C = 1) = p(R T , R T,S ) q(R T , R T,S |C = 0) = p(R T )p(R T,S ).<label>(4)</label></formula><p>More specifically, C = 1 means r T and r T,S are computed based on the same input pair as in Eq. 2, and C = 0 means r T and r T,S are independently selected as in Eq. 1. In our data, we provide 1 relevant relation pair (C = 1) with N irrelevant relation pair (C = 0). Then the prior q(C = 1) = 1/(N + 1) and q(C = 0) = N/(N + 1). Combing the priors with the Bayes' rule, the posterior for C = 1 is given by:</p><formula xml:id="formula_4">q(C = 1|R T , R T,S ) = p(R T , R T,S ) p(R T , R T,S ) + N p(R T )p(R T,S )</formula><p>. <ref type="formula">(5)</ref> By connection to the mutual information, the posterior</p><formula xml:id="formula_5">log q(C = 1|R T , R T,S ) ? ? log(N ) + log p(R T ,R T ,S ) p(R T )p(R T ,S )</formula><p>. Taking the expectation on both sides w.r.t. p(R T , R T,S ), which is also equivalent to q(R T , R T,S |C = 1), we have:</p><formula xml:id="formula_6">I(R T , R T,S ) ? log(N )+ E q(R T ,R T ,S |C=1) log q(C = 1|R T , R T,S ) (6) where log(N )+E q(R T ,R T ,S |C=1) log q(C = 1|R T , R T,S ) is a lower bound of the mutual information. Distribution Approximation.</formula><p>As there is no knowledge about the true distribution of q(C = 1|R T , R T,S ), we approximate the distribution by fitting a parameterized model h:</p><formula xml:id="formula_7">{R T , R T,S } ? [0, 1] with the samples from q(C = 1|R T , R T,S ).</formula><p>The log-likelihood of the sampled data under this model is defined as:</p><formula xml:id="formula_8">I(h) = E q(R T ,R T ,S |C=1) [log h(R T , R T,S )] + N E q(R T ,R T ,S |C=0) [log(1 ? h(R T , R T,S ))].<label>(7)</label></formula><p>To achieve a good approximation to q(C = 1|R T , R T,S ), we need to maximize the log likelihood. Consider the bound in Eq. 6 and the fact that</p><formula xml:id="formula_9">N E q(R T ,R T ,S |C=0) [log(1 ? h(R T , R T,S ))] is non-positive, we have I(R T , R T,S ) ? log N +E q(R T ,R T ,S |C=1) [log h(R T , R T,S )] + N E q(R T ,R T ,S |C=0) [log(1 ? h(R T , R T,S ))] ? log N + I(h),<label>(8)</label></formula><p>where log N + I(h) is the lower bound of the mutual information with the parameterized model h. The maximization of the log-likelihood is also to maximize the lower bound. Relation Contrastive Loss. In our method, the inputs for the function h are teacher-space relation r T and cross-space relations r T,S , which are the results of the teacher ? T , the student ? S , and the two sub-networks M T , M T,S . Except the teacher ? T , the other three networks ? S , M T and M T,S also need to be optimized during the distillation. We aim to maximize the mutual information, which is equivalent to minimizing the relation contrastive loss L RC :</p><formula xml:id="formula_10">LRC (h, ? S , M T , M T,S ) = ? q(C=1) log h(r T , r T,S ) ? N q(C=0) log[1?h(r T , r T,S )]<label>(9)</label></formula><p>where {(r T , r T,S )|C = 1} act as positive pairs while {(r T , r T,S )|C = 0} act as negative pairs. Due to Eq. 8, the contrastive loss can fit the distribution q(C|R T , R T,S ) to increase the lower-bound of mutual information of R T and R T,S , by which not only the parameterized model h, but also the other three networks ? S , M T and M T,S can be jointly optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Complementary Relation</head><p>Modeling relation between sample representations is the prerequisite for distilling the structural information. We therefore propose two learnable sub-networks M T,S and M T to estimate the relation.</p><p>The sub-network M T,S is to compute the anchor-student relation with representation ? T (x i ) and ? S (x j ):</p><formula xml:id="formula_11">r T,S i,j = M T,S (? T (xi), ? S (xj)) = W A (?(W A i ? T (xi) ? W A j ? S (xj))),<label>(10)</label></formula><p>where W A i and W A j are linear transformations that can solve the dimension mismatch problem. ? is ReLU function and W A is used for transformation. The anchor-student relation is supervised by the fixed anchor-teacher relation r T (x i , x j ), computed by another sub-network M T :</p><formula xml:id="formula_12">r T i,j = M T (? T (xi), ? T (xj)) = W B (?(W B i ? T (xi) ? W B j ? T (xj))).<label>(11)</label></formula><p>It is noteworthy that the relations r T,S and r T are not scalar values but high-dimensional vectors. We claim that the high-dimensional relation can more accurately capture the structural information of deep representations than lowdimensional relation e.g., cosine similarity, which will be validated in section 4.2. Furthermore, the small learnable networks also increases relation flexibility. The relations are modeled by two complementary elements: feature f and its gradient g. Specifically, the representation ?(x) in Eq. 10 and Eq. 11 can be either the feature of the teacher/student model or its gradient. Feature Element. The feature element is the 2 normalized output of teacher/student's backbone. With the feature element f , the representations ? T (x) and ? S (x) reflect the direct activation relative to the input x:</p><formula xml:id="formula_13">? T (x) = f T (x); ? S (x) = f S (x)<label>(12)</label></formula><p>Gradient Element. The gradient element is the gradient with respect to the feature. It reflects the optimization kinetics in the feature space, encoding important structural information. Given an input sample x into a teacher/student network ?, the gradient of task loss L cls relative to the feature f is computed as:</p><formula xml:id="formula_14">g(x ) = ? ?f L cls (?, x).<label>(13)</label></formula><p>With gradient elements, the representation ? T (x) and ? S (x) can reflect the optimization kinetics:</p><formula xml:id="formula_15">? T (x) = g T (x); ? S (x) = g S (x)<label>(14)</label></formula><p>Element Combination. Complementary relation is modeled to leverage feature and gradient elements simultaneously. Specifically, after the one-sided relations: feature relation r f and gradient relation r g , are computed with feature and gradient elements respectively, their corresponding relation contrastive losses can also be calculated by Eq. 9. By optimizing these two losses simultaneously, these two elements can both be utilized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Implementation</head><p>Critic Function. We specify the parameterized critic function h in Eq. 7 to distinguish whether the relation pair (r T , r T,S ) is sampled from the joint distribution p(R T , R T,S ) or the product of marginal distribution p(R T )p(R T,S ). The formulation is similar to NCE <ref type="bibr" target="#b45">[46]</ref>:</p><formula xml:id="formula_16">h(r T , r T,S ) = e h 1 (r T )h 2 (r T ,S )/? e 1/?<label>(15)</label></formula><p>where ? is a temperature hyperparameter, and h 1 and h 2 first perform the linear transformation on relations, then normalize the transformed relations with 2 norm. Sampling Policy. We adopt the following sampling policy: in each forward-propagation, the anchor relation r T ij and positive relation r T,S ij are calculated using representations from any two samples x i and x j in the current minibatch, while the negative relations r T,S ik are calculated using the anchor representation from x i and the representations (indexed with k) sampled from the buffer where features and gradients are stored. Considering a B-size min-batch, we construct the anchor/positive relation for each sample pair thus the number of these two relations can be B 2 . For each anchor relation, we sample N feature/gradient from the buffer to construct N negatives for contrastive learning.</p><p>To make the feature/gradient buffer reflect the current network state better, we propose a queuing sampling method instead of a randomly sampling strategy. The queue records the N sample indices from the immediate preceding mini-batches and is updated after each forward-propagation by replacing the oldest indices with the current mini-batch. According to these recorded indices, the representations of these samples are used to calculated relation contrastive loss, whose effectiveness will be studied in Sec. 4.2. Loss Function. To achieve the superior performance and conduct a fair comparison with other methods, we also incorporate the naive knowledge distillation loss L kd <ref type="bibr" target="#b18">[19]</ref> along with our relation contrastive loss. Given the presoftmax logits z T and z S for teacher and student, the naive KD loss can be expressed as</p><formula xml:id="formula_17">L kd = ? 2 H(?(z T /?), ?(z S /?))<label>(16)</label></formula><p>where ? is the temperature, H refers to the cross-entropy and ? is softmax function. The complete objective is: <ref type="bibr" target="#b16">(17)</ref> where L f RC and L g RC are the relation contrastive loss computed with the feature (f ) and gradient (g), respectively. L cls is the cross entropy loss for classification. We set hyper-parameters to ? = 1 and ? 1 = ? 2 = 0.5 empirically. Discussion. CRD <ref type="bibr" target="#b40">[41]</ref> aims to maximize the mutual information between the representations of the sample themselves from teacher/student models. Meanwhile, the proposed CRCD seeks the consistency between the teacherspace relation and cross-space relation. Indeed, if i = j in Eq. 9, the loss of CRCD essentially optimizes the crossspace relation of one sample, which degrades to the loss of CRD. Moreover, the number of pair-wise relations is at quadratic level relative to the number of samples, which also increases the optimized stability of contrastive loss.</p><formula xml:id="formula_18">L = L cls + ?LKD + ?1L f RC + ?2L g RC</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Experimental Setup</head><p>Datasets. Our experiments are conducted on two widely used classification datasets, i.e., CIFAR100 <ref type="bibr" target="#b23">[24]</ref> and Im-ageNet <ref type="bibr" target="#b9">[10]</ref>. CIFAR100 contains 60000 images for 100 classes, and there are 500 and 100 images per class for  training and testing respectively. ImageNet is a wellknown large-scale image classification benchmark with 1000 classes, consisting of 1281167 images for training and 50000 images for testing. Parameter Setting. For CIFAR, mini-batch size is set to 64 in 1 GPU. SGD optimizer is used with weight decay and momentum of 0.0001 and 0.9 respectively. And the learning rate and schedule strategy follow <ref type="bibr" target="#b40">[41]</ref>, which is included in supplementary materials. For ImageNet, batchsize is set to 256 in 8 GPUs, and the standard training settings for Ima-geNet is adopted. For other competing methods, we use the implementation settings in papers or official shared codes. The relation dimension computed by sub-networks M T,S and M T is set to 256-d since the representation dimension in most of our experimental networks is 256-d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Three teacher-student pairs are selected for ablation study.   The first two are with similar architectures, while the last one is with a very different architecture. These experiments are conducted on CIFAR100, and results are averaged over 3 runs. Effectiveness of relation modeling method. We first demonstrate the effectiveness of anchor-based relation. In contrast to conventional modeling methods, our relation is cross-space and high-dimensional. To verify its superiority, we compare it with four methods using low-dimensional relations: 1) RKD <ref type="bibr" target="#b31">[32]</ref>; 2) CC <ref type="bibr" target="#b33">[34]</ref>; 3) SP <ref type="bibr" target="#b42">[43]</ref>; and 4) PKT <ref type="bibr" target="#b32">[33]</ref>. For a fair comparison, we also use L 2 loss to preserve representation relations and only feature relation is involved. The results are shown in Tab. 1. Over all three teacher-student pairs, our proposed relation boosts the test accuracy by a large margin even with L 2 loss, which means that our relation modelling method is superior. Effectiveness of complementary relation elements. We propose two elements: feature and its gradient, to model representation relation. To verify their complementarity, we test the distilling accuracy of these two elements when used alone and when used simultaneously. As <ref type="figure" target="#fig_2">Fig. 3</ref> shows, their combination can get the best result, which indicates that the feature and the gradient are complementary to each other and can more comprehensively present the representation interdependences. <ref type="table">Table 3</ref>: Contrastive loss functions. To simplify, the anchor relation r T ij , positive relation r T,S ij , and negative relation r T,S ik j =k after critic transformation are denoted as u, v + and v ? respectively. All relations are 2 normalized before inner product. ? is the temperature weight, and m is the margin parameter. Additionally, ? is sigmoid function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Name</head><p>Loss function Effectiveness of critic function h. We propose the critic function h in Eq. 15 to estimate the distribution q(C = 1|R T , R T,S ). To investigate the effectiveness of h 1 and h 2 selection, we conduct three experiments, including specifying the h 1 and h 2 functions with identity mapping, nonlinear projection and linear transformation(default). In particular, h is degraded to cosine similarity estimation when identity mapping is adopted. For nonlinear projection, we use a MLP with one hidden layer h(r) = W (2) ?(W (1) r) where r is input relation and ? is a ReLU nonlinearity. In this study, the output dimension of linear or nonlinear transformation are both 256. <ref type="table" target="#tab_2">Table 2</ref> shows testing results using different transformations. We observe that both the linear and nonlinear projection achieve better results than identity mapping under the same projection dimension, which means that critic function with learnable parameters can better fit the distribution q(C = 1|R T , R T,S ). Effectiveness of relation contrastive loss. We compare our relation contrastive loss L RC with other commonly used contrastive loss, such as triplet loss with margin (L M T ) <ref type="bibr" target="#b35">[36]</ref> and contrastive logistic loss (L CL ) <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b10">11]</ref>. Tab. 3 shows the formulations of four contrastive loss function.</p><formula xml:id="formula_19">LMT [36] max(u v ? ? u v + + m, 0) LCL [26] ? log ?(u v + /? ) ? log (1 ? ?(u v ? /? )) LNCE [31] ?u v + /? + log e u v ? /? LRC ? log e u v + /? e 1/? ? N log (1 ? e u v ? /? e 1/? )</formula><p>To better analyze the loss function, we only use the feature element and gradient is not employed. The hyperparameters in these losses, i.e., temperature ? and margin m, are tuned to achieve the best results. Results reported in Tab. 4 show that, L N CE and L RC can significantly outperform L M T and L CL , because they can benefit from large number of negative samples. While our objective function L RC is better than L N CE in most of teacher-student combinations. We adopt resnet56-resnet20 pair on CIFAR100 for analysis. Number of negative samples. We validate different N : 100, 200, 500, 1000, 2000. As shown in Tab. 4a, increasing the negative number leads to better performance, and the performance is saturated when n &gt; 500. We therefore utilize N = 500 in all other experiments to save computational cost. Compared to CRD <ref type="bibr" target="#b40">[41]</ref>, our CRCD requires fewer negative features to reduce the need of memory. This is because CRCD can utilize few samples to generate a large number of relations, while CRD only depends on the number of samples. Temperature ? . <ref type="figure" target="#fig_3">Fig. 4b</ref> reports the results when ? varies from 0.02 to 0.2. We find that both extremely high or low temperature leads to inferior performance. In general, a temperature between 0.03 to 0.07 works well. We set ? = 0.05 for all other experiments. Sampling policy. To ensure that negative samples are as up-to-date as possible, we store features and gradients in a queue way which will remove the oldest sample when adding the latest sample. We compare the randomly sampling policy and the queuing sampling policy in <ref type="figure" target="#fig_3">Fig. 4</ref>. The queuing sampling policy (denoted as queue) can consistently outperform the naive randomly sampling policy (denoted as random) when varying negative number N and temperature ? . Projection dimension. We investigate the influence of output dimension for critic function h by setting output dimension to 64, 128, and 256 (the input relation dimension is 256-d). As shown in Tab. 2, compared to 128-d or 256-d, transformation with lower dimension (64-d) has some accuracy degradation. We utilize the 128-d linear transformation to make a trade-off between effectiveness and computational cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Hyper-parameter analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-arts</head><p>CIFAR100. We compare our CRCD with other advanced knowledge distillation methods in Tab. 5. Various modern CNN architectures <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b51">52]</ref> are selected as teacher networks or student networks. For a fair comparison, we combine all distillation methods with conventional KD <ref type="bibr" target="#b18">[19]</ref>. From Tab. 5, we can observe that our distillation <ref type="table">Table 5</ref>: The top-1 accuracies (%) of seven different student-teacher pairs on CIFAR100. The accuracies of the teachers' and students' performance when they are trained individually are presented in the second partition after the header. FRCD (or GRCD) is the incomplete version of CRCD which means that only feature relation (or gradient relation) is employed in distillation. The best results are bolded and the best in competing methods are underlined.  <ref type="table">Table 6</ref>: Top-1 and Top-5 error rate (%) on ImageNet validation set. We compare our CRCD with competing methods including AT <ref type="bibr" target="#b50">[51]</ref>, KD <ref type="bibr" target="#b18">[19]</ref>, SP <ref type="bibr" target="#b42">[43]</ref>, CC <ref type="bibr" target="#b33">[34]</ref>, CRD <ref type="bibr" target="#b40">[41]</ref> and SSKD <ref type="bibr" target="#b46">[47]</ref>, and folow the training settings in <ref type="bibr" target="#b40">[41]</ref>. method CRCD can consistently outperform all other distillation methods with a large margin, including the recent state-of-the-arts, CRD and SSKD. Additionally, even only one element (feature or its gradient) is used in the relation distillation, our method can still achieve the competing accuracy when compared to CRD or SSKD. When the feature and its gradient are employed in the representation relation distillation simultaneously, our CRCD can significantly outperform the other methods. In particular, the accuracy gap between CRCD and the other best performing method is 0.9% (averaged over 7 pairs in Tab. 5).</p><p>To evaluate the distillation effectiveness across very different network architectures, we also carry out detailed comparisons in supplementary materials.</p><p>ImageNet. Following <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b46">47]</ref>, we adopt the ResNet34-ResNet18 pair to evaluate the effectiveness of CRCD on ImageNet. As shown in Tab. 6, the Top-1 and Top-5 accuracy between the teacher and student without distillation is 3.56% and 2.43%. Our CRCD reaches the best distillation performance by narrowing the performance gap by 2.21% and 1.87% respectively. Results on ImageNet demonstrates the scalability of our CRCD to large-scale benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we have proposed a novel knowledge distillation method, CRCD, to distill important structural information from a teacher to a student. To better distill the relation knowledge, two sub-networks are used to estimate the cross-space relation and teacher-space relation, respectively. We maximized the mutual information between the two kinds of relations by a newly proposed relation contrastive distillation loss, and utilized two complementary elements, the feature and its gradient, to enhance the representative ability of the relation. With the design of the loss function, the inter-sample relation and representation learning can be optimized simultaneously. Extensive experiments demonstrate the effectiveness of our approach and suggest that the structural information of deep representation can be better exploited during distillation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Sample contrastive distillation vs. Relation preserving distillation. Four neighboring samples and their corresponding features are displayed, and capital letters are used to identify them. While pulling f S A closer to f T A , sample contrastive distillation will simultaneously push f S A away from f T B , f T C and f T D without distinction, whereas relation preserving distillation preserves the feature relations across the feature space, thus f S A can be optimized along the optimal direction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>The Flowchart of CRCD. To distill the structural knowledge from the teacher model ? T to the student model ? S , two complementary elements, the feature f and its gradient g, are utilized to model the representation relations. For each element, auxiliary subnetworks M T and M T,S are used to estimate the anchor-teacher relation R T in the teacher space and anchor-student relation R T,S across space respectively. Meanwhile, the cross-space R T,S is supervised by its corresponding R T . By this way, not only the relation estimation but also the representation learning can be achieved.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Accuracy of different relation elements. The feature relation r f , gradident relation r g , and complementary relation r f &amp;r g are distilled on three teacher-student pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Accuracy of varying negative number N and temperature ? with different sample policies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Several hyper-parameters are worth investigating in our proposed CRCD method. (1) The number of negative samples N ; (2) The temperature used to scale the critic scores in Eq. 15; (3) The sampling policy to construct negative relations; (4) The projection dimension of critic function h.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Testing accuracy (%) on CIFAR100 with different relation modeling methods. L 2 loss and relation contrastive loss L RC are used to distill the feature relation r f .</figDesc><table><row><cell></cell><cell></cell><cell>teacher</cell><cell cols="2">resnet56 resnet110 ResNet50</cell></row><row><cell></cell><cell></cell><cell>student</cell><cell cols="2">resnet20</cell><cell>resnet20</cell><cell>vgg8</cell></row><row><cell></cell><cell></cell><cell>RKD [32]</cell><cell cols="2">70.54</cell><cell>70.98</cell><cell>73.65</cell></row><row><cell></cell><cell></cell><cell>CC [34]</cell><cell cols="2">71.42</cell><cell>70.96</cell><cell>73.76</cell></row><row><cell></cell><cell></cell><cell>SP [43]</cell><cell cols="2">71.59</cell><cell>71.15</cell><cell>73.95</cell></row><row><cell></cell><cell></cell><cell>PKT [33]</cell><cell cols="2">71.68</cell><cell>71.08</cell><cell>74.01</cell></row><row><cell></cell><cell></cell><cell>r f + L2</cell><cell cols="2">71.93</cell><cell>71.54</cell><cell>74.15</cell></row><row><cell></cell><cell></cell><cell>r f + LRC</cell><cell cols="2">72.70</cell><cell>72.02</cell><cell>74.69</cell></row><row><cell>Top-1(%)</cell><cell>72 73 74 75 76</cell><cell>r f r g r f &amp;r g</cell><cell></cell></row><row><cell></cell><cell>71</cell><cell></cell><cell></cell></row><row><cell></cell><cell>70</cell><cell cols="2">resnet56-resnet20</cell><cell>resnet110-resnet20</cell><cell>ResNet50-vgg8</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Teacher-Student pairs</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Testing accuracy (%) on CIFAR100 with different transformations for critic function h. IM : identity mapping; LP : linear projection; N P : nonlinear projection. The transformation dimensions are appended as subscripts.</figDesc><table><row><cell cols="4">teacher resnet56 resnet110 ResNet50</cell></row><row><cell cols="2">student resnet20</cell><cell>resnet20</cell><cell>vgg8</cell></row><row><cell>IM</cell><cell>72.35</cell><cell>71.84</cell><cell>74.25</cell></row><row><cell>N P256</cell><cell>72.52</cell><cell>71.98</cell><cell>74.49</cell></row><row><cell>LP64</cell><cell>72.45</cell><cell>71.92</cell><cell>74.34</cell></row><row><cell>LP128</cell><cell>72.70</cell><cell>72.02</cell><cell>74.69</cell></row><row><cell>LP256</cell><cell>72.65</cell><cell>72.12</cell><cell>74.57</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="5">: Testing accuracy (%) on CIFAR100 with different</cell></row><row><cell cols="3">contrastive loss functions.</cell><cell></cell><cell></cell></row><row><cell>teacher</cell><cell></cell><cell cols="3">resnet56 resnet110 ResNet50</cell></row><row><cell>student</cell><cell></cell><cell>resnet20</cell><cell>resnet20</cell><cell>vgg8</cell></row><row><cell>L2</cell><cell></cell><cell>71.93</cell><cell>71.54</cell><cell>73.89</cell></row><row><cell>LMT</cell><cell>m = 0.4</cell><cell>72.21</cell><cell>71.83</cell><cell>74.25</cell></row><row><cell>LCL</cell><cell>? = 0.05</cell><cell>72.15</cell><cell>71.72</cell><cell>74.07</cell></row><row><cell cols="2">LNCE ? = 0.05</cell><cell>72.53</cell><cell>72.09</cell><cell>74.44</cell></row><row><cell>LRC</cell><cell>? = 0.05</cell><cell>72.70</cell><cell>72.02</cell><cell>74.69</cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variational information distillation for knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungsoo</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shell</forename><forename type="middle">Xu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Damianou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenwen</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9163" to="9171" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adversarial network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Azade</forename><surname>Farshad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Galasso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohamed</forename><forename type="middle">Ishmael</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sai</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.04062</idno>
		<title level="m">Mine: mutual information neural estimation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual features by contrasting cluster assignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathilde</forename><surname>Caron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishan</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Priya</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.09882</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Video person re-identification with competitive snippet-similarity aggregation and co-attentive snippet embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Group consistent similarity learning via deep crf for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Similarity learning with spatial constraints for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Badong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nanning</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A simple framework for contrastive learning of visual representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05709,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Big self-supervised models are strong semi-supervised learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kornblith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10029</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image-image domain adaptation with preserved self-similarity and domain-dissimilarity for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijian</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guoliang</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mutual meanteaching: Pseudo label refinery for unsupervised domain adaptation on person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<biblScope unit="volume">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Self-paced contrastive learning with hybrid memory for domain adaptive object re-id</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Knowledge distillation: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baosheng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">John</forename><surname>Maybank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05525</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Bootstrap your own latent: A new approach to self-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Bastien</forename><surname>Grill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florent</forename><surname>Altch?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corentin</forename><surname>Tallec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Richemond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Buchatskaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardo</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaohan</forename><forename type="middle">Daniel</forename><surname>Avila Pires</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><forename type="middle">Gheshlaghi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Azar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07733</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Momentum contrast for unsupervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoqi</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9729" to="9738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Knowledge transfer via distillation of activation boundaries formed by hidden neurons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Byeongho</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsik</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangdoo</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jin</forename><forename type="middle">Young</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3779" to="3787" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning deep representations by mutual information estimation and maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>R Devon Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karan</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Menglong</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hartwig</forename><surname>Andreetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04861</idno>
		<title level="m">Mobilenets: Efficient convolutional neural networks for mobile vision applications</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Like what you like: Knowledge distill via neuron selectivity transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naiyan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01219</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Paraphrasing complex network: Network compression via factor transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jangho</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seonguk</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nojun</forename><surname>Kwak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2760" to="2769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Memory-based neighbourhood embedding for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suichan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deepreid: Deep filter pairing neural network for person reidentification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iou-Jen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.05878</idno>
		<title level="m">Improve upon your teachers</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Knowledge distillation via instance relationship graph</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yufan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajiong</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunfeng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiming</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunqiang</forename><surname>Duan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7096" to="7104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Mlodozeniec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Gales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.00076</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">Ensemble distribution distillation. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yazhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Relational knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonpyo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongju</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minsu</forename><surname>Cho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3967" to="3976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning deep representations with probabilistic knowledge transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Passalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Tefas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="268" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Correlation congruence for knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaheng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shunfeng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaoning</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="5007" to="5016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><forename type="middle">Ebrahimi</forename><surname>Kahou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Chassang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlo</forename><surname>Gatta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6550</idno>
		<title level="m">Fitnets: Hints for thin deep nets</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Meal: Multi-model ensemble via adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiqiang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhankui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyang</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4886" to="4893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mutual crf-gnn for few shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixiao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Layerwise optimization by gradient decomposition for continual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shixiang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dapeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinguo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shijie</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanli</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05849</idno>
		<title level="m">Contrastive multiview coding</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10699</idno>
		<title level="m">Contrastive representation distillation</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">What makes for good views for contrastive learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonglong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dilip</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10243,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Similarity-preserving knowledge distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederick</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Mori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1365" to="1374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Understanding contrastive representation learning through alignment and uniformity on the hypersphere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tongzhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.10242,2020.3</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Distilled person re-identification: Towards a more scalable system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ancong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Shi</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaowei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Huang</forename><surname>Lai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1187" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning via non-parametric instance discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Knowledge distillation meets self-supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guodong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen Change</forename><surname>Loy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07114</idno>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Training shallow and thin networks for acceleration via knowledge distillation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.00513</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A gift from knowledge distillation: Fast optimization, network minimization and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junho</forename><surname>Yim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donggyu</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jihoon</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junmo</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4133" to="4141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03928</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07146</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Wide residual networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Shufflenet: An extremely efficient convolutional neural network for mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengxiao</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="6848" to="6856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Crowded human detection via an anchor-pair network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinguo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zejian</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanchao</forename><surname>Chi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonggen</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1391" to="1399" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
