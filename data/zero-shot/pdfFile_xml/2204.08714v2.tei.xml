<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NAFSSR: Stereo Image Super-Resolution Using NAFNet</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Chu</surname></persName>
							<email>chuxiaojie@stu.pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangyu</forename><surname>Chen</surname></persName>
							<email>chenliangyu@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqing</forename><surname>Yu</surname></persName>
							<email>yuwenqing@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">NAFSSR: Stereo Image Super-Resolution Using NAFNet</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T20:09+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Stereo image super-resolution aims at enhancing the quality of super-resolution results by utilizing the complementary information provided by binocular systems. To obtain reasonable performance, most methods focus on finely designing modules, loss functions, and etc. to exploit information from another viewpoint. This has the side effect of increasing system complexity, making it difficult for researchers to evaluate new ideas and compare methods. This paper inherits a strong and simple image restoration model, NAFNet, for single-view feature extraction and extends it by adding cross attention modules to fuse features between views to adapt to binocular scenarios. The proposed baseline for stereo image super-resolution is noted as NAFSSR. Furthermore, training/testing strategies are proposed to fully exploit the performance of NAFSSR. Extensive experiments demonstrate the effectiveness of our method. In particular, NAFSSR outperforms the state-ofthe-art methods on the KITTI 2012, KITTI 2015, Middlebury, and Flickr1024 datasets. With NAFSSR, we won 1st place in the NTIRE 2022 Stereo Image Super-resolution Challenge. Codes and models will be released at https: //github.com/megvii-research/NAFNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Stereo image super-resolution (SR), which aims at reconstructing high-resolution (HR) details from a pair of low-resolution (LR) left and right images, has attracted much attention in recent years. To solve this task, both context information within a single view (i.e. intra-view information) and information between left and right image (i.e. cross-view information) are crucial <ref type="bibr" target="#b37">[38]</ref>. On the one hand, recent works in stereo image SR <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b40">41]</ref> mainly focus on the finely designing novel network architectures, losses, and etc. to effectively incorporate additional information from another viewpoint, as the cross-view information provided by binocular systems enhances the image quality.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NAFSSR-B</head><p>(Ours)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NAFSSR-L</head><p>(Ours) <ref type="figure">Figure 1</ref>. Parameters vs. PSNR of models for 4? stereo SR on Flickr1024 <ref type="bibr" target="#b32">[33]</ref> test set. Our NAFSSR families achieve the stateof-the-art performance with up to 79% of parameter reduction.</p><p>But the system complexity is increasing, which may hinder the convenient analysis and comparison of methods. On the other hand, remarkable progress in single image restoration has been witnessed with deep learning techniques, e.g. Transformer-based SwinIR <ref type="bibr" target="#b18">[19]</ref> outperforms state-of-the-art methods on single-image SR. NAFNet <ref type="bibr" target="#b1">[2]</ref> achieves state-of-the-art performance without nonlinear activation functions on denoising and deblurring tasks. However, these single image restorers are suboptimal for stereo image SR as they cannot utilize the cross-view information. Inspired by NAFNet <ref type="bibr" target="#b1">[2]</ref> which achieves competitive performance on single image restoration tasks with low system complexity, we propose a novel baseline for stereo image SR, NAFSSR, by adding simple cross attention modules to NAFNet. It can fully utilize both intra-view information and cross-view information to achieve the competitive performance of stereo super-resolution. Specifically, we stack NAFNet blocks (NAFBlocks for short) and extract intraview features for both views in a weight-sharing manner. It inherits the strong representation (within the viewpoint) of NAFNet. Specifically, to further improve the representation of NAFNet, we propose stereo cross-attention module (SCAM) to attend and fuse the left/right viewpoint features. It first computes bidirectional cross attention from left to right and right to left views, and then fuses the interacted cross-view features with intra-view features. In contrast to the original cross-attention used in a standard Transformer decoder <ref type="bibr" target="#b29">[30]</ref>, which attends to all locations in an image, our stereo cross-attention attends to corresponding features along the horizontal epipolar line, following <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>Although NAFSSR has strong representational power, it may suffer from overfitting due to the lack of data for the stereo SR task. To solve this, we adopt stochastic depth <ref type="bibr" target="#b12">[13]</ref> as regularization and channel shuffle (i.e., shuffle the RGB channels of input images randomly) as data augmentation during the training phase. Besides, we reveal that there is also the train/test inconsistency issue mentioned in TLSC <ref type="bibr" target="#b2">[3]</ref> in the stereo SR task. Thus we adopt TLSC <ref type="bibr" target="#b2">[3]</ref> in the testing phase to alleviate the inconsistency issue. These training/testing strategies, together with NAFSSR, constitute a baseline for the stereo SR task. As shown in <ref type="figure">Figure 1</ref>, our NAFSSR families have better performance and parameters trade-off than existing methods.</p><p>Our contributions can be summarized as follows:</p><p>? We analyze the drawbacks of existing methods and propose NAFSSR, which is simple and easily implemented. It inherits the advantages of NAFNet's simplicity and power, and uses the characteristics of the stereo SR task to improve the representation through a simple stereo cross-attention module. ? Based on NAFSSR, we design its training/testing strategies, thus addressing the obstacles to its competitive performance on the stereo SR task. The strategies together with NAFSSR constitute a strong baseline for this task: the baseline achieves the state-of-the-art performance with fewer parameters ( <ref type="figure">Figure 1</ref>) and faster inference speed ( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Single Image Super-resolution</head><p>Single image restoration tasks, e.g., image superresolution (SR), aim at reconstructing high-quality images by using only intra-view information from low-quality input. Deep learning-based methods have dominated single image super-resolution tasks since the pioneering work of Super-Resolution Convolutional Neural Network (SR-CNN <ref type="bibr" target="#b7">[8]</ref>). More complicated neural network architecture designs have been presented to improve model representation ability by increasing the depth and width of models <ref type="bibr" target="#b14">[15]</ref>, applying residual <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b38">39]</ref> and dense <ref type="bibr" target="#b39">[40]</ref> connections, as well as introducing different attention mechanism (e.g., channel attention <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39]</ref>, channel-spatial at-tention <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>). Specifically, SwinIR <ref type="bibr" target="#b18">[19]</ref> proposes a Swin Transformer-based image restoration method and achieves state-of-the-art performance on single image SR. In this paper, we extend NAFNet <ref type="bibr" target="#b1">[2]</ref>, a simple baseline with competitive performance on single image restoration tasks, to stereo image SR task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Stereo Super-Resolution</head><p>Stereo super-resolution task aims at reconstructing highresolution details of a pair of low-resolution images on the left and right views. StereoSR <ref type="bibr" target="#b13">[14]</ref> learns a mapping between continuous parallax shifts and a high-resolution image by jointly training two cascaded sub-networks for luminance and chrominance, respectively. To handle different stereo images with large disparity variations, PASS-Rnet <ref type="bibr" target="#b31">[32]</ref> introduces a parallax-attention mechanism with a global receptive field along the epipolar line. Ying et al. <ref type="bibr" target="#b37">[38]</ref> propose a stereo attention module (SAM) to extend pre-trained single image SR networks for stereo image SR. StereoIRN <ref type="bibr" target="#b36">[37]</ref> introduces two disparity attention losses and uses a pre-trained disparity flow network to align two views features. Song et al. <ref type="bibr" target="#b28">[29]</ref> propose self and parallax attention mechanism for simultaneously aggregating information from its own image and the counterpart stereo image. To effectively interact cross-view information, iPASSR <ref type="bibr" target="#b33">[34]</ref> propose symmetric bi-directional parallax attention module (biPAM) and an inline occlusion handling scheme to exploit symmetry cues for stereo image SR. CVCnet <ref type="bibr" target="#b40">[41]</ref> integrates cross view spatial features from both global and local perspectives. SSRDE-FNet <ref type="bibr" target="#b3">[4]</ref> simultaneously handles the stereo image SR and disparity estimation in a unified framework and interacts two tasks in a mutually boosted way.</p><p>We also design a simple stereo cross-attention module to extend single image restoration networks for stereo image SR. In contrast to SAM <ref type="bibr" target="#b37">[38]</ref>, which uses single image SR models pretrained on extra datasets and only fine-tunes on stereo datasets with multiple losses, our NAFSSR is trained directly on stereo images from scratch with only L1 loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Training and Testing Strategies</head><p>Regularizations (e.g., weight decay <ref type="bibr" target="#b34">[35]</ref>, dropout and stochastic depth <ref type="bibr" target="#b12">[13]</ref>) are widely used to improve model performance in high-level computer vision tasks <ref type="bibr" target="#b34">[35]</ref>. However, there is still no consensus on whether regularization techniques should be used in image super-resolution (SR) tasks. For example, Lin et al. <ref type="bibr" target="#b20">[21]</ref> discover that underfitting is still the main issue limiting the model capability of RCAN <ref type="bibr" target="#b38">[39]</ref>. On the contrary, Kong et al. <ref type="bibr" target="#b15">[16]</ref> demonstrate that proper use of dropout <ref type="bibr" target="#b9">[10]</ref> benefits SR networks by preventing overfitting to a specific degradation. In this paper, we find that the proposed networks (except the smallest one) are overfitting to the stereo training data, so we use stochastic depth to improve their generality.  <ref type="figure" target="#fig_6">Figure 4</ref>).  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we introduce our method in details. We first describe the architecture of our network in Section 3.1, then discuss the training and testing strategies throughout the paper in Section 3.2 and 3.3, respectively. Two weight-sharing networks (stacked by NAFBlock) extract the intra-view features of the left and the right images separately. And Stereo Cross-Attention Modules (SCAMs) are provided to fuse features extracted from the left and the right images. In detail, NAFSSR can be divided into three parts: intra-view feature extraction, cross-view feature fusion, and reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>Intra-view feature extraction and reconstruction. In the beginning, a 3 ? 3 convolution layer is used to map the input image space to a higher dimensional feature space. Then, N NAFBlocks are used for deep intra-view feature extraction. The details of NAFBlock are described in Section 3.1.2. After feature extraction, a 3?3 convolution layer followed by a pixel shuffle layer <ref type="bibr" target="#b27">[28]</ref> is used to upsample the feature by a scale factor of s. Furthermore, to alleviate the burden of feature learning, we use global residual learning and predict only the residual between the bilinearly upsampled low-resolution image and the ground-truth highresolution image <ref type="bibr" target="#b17">[18]</ref>.</p><p>Cross-view feature fusion. To interact with cross-view information, we insert SCAM after each NAFBlock. It uses stereo features generated by previous NAFBlocks as inputs to perform bidirectional cross-view interactions, and outputs interacted features fused with input intra-view features. The details of SCAM are described in Section 3.1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">NAFBlock</head><p>The NAFBlock is introduced by NAFNet [2], and its details are shown in <ref type="figure" target="#fig_3">Figure 3</ref>. It should be noticed that there are no nonlinear activation functions in it. NAFBlock consists of two parts: (1) Mobile convolution module (MB-Conv) based on point-wise and depth-wise convolution with channel attention (simplified SE <ref type="bibr" target="#b11">[12]</ref>); (2) a feed-forward network (FFN) module that has two fully-connected layers (implemented by point-wise convolution). The LayerNorm (LN <ref type="bibr" target="#b0">[1]</ref>) layer is added before both MBConv and FFN, and the residual connection is employed for both modules. The whole process is formulated as:</p><formula xml:id="formula_0">X = MBConv(LN(X)) + X X = FFN(LN(X)) + X<label>(1)</label></formula><p>The main differences between NAFBlock and original blocks (e.g., MBConv in MobileNetV3 <ref type="bibr" target="#b10">[11]</ref> and FFN in Transformer <ref type="bibr" target="#b29">[30]</ref>) lie in the simple gate mechanism, which makes block nonlinear activation free. Specifically, NAF-Block uses SimpleGate unit to replace nonlinear activation (e.g., ReLU, GELU). Given an input X ? R H?W ?C , Sim-pleGate first split the input into two features X 1 , X 2 ? R H?W ?C/2 along channel dimension. Then, it computes the output with linear gate as:</p><formula xml:id="formula_1">SimpleGate(X) = X 1 ? X 2 ,<label>(2)</label></formula><p>where ? represents element-wise multiplication. The Sim-pleGate unit is added after depth-wise convolution and between two fully-connected layers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Stereo Cross Attention Module</head><p>The details of the proposed Stereo Cross Attention Module (SCAM) are shown in <ref type="figure" target="#fig_6">Figure 4</ref>. It is based on Scaled Dot-Product Attention <ref type="bibr" target="#b29">[30]</ref>, which computes the dot products of the query with all keys and applies a softmax function to obtain the weights on the values:</p><formula xml:id="formula_2">Attention(Q, K, V) = softmax QK T / ? C V (3)</formula><p>where Q ? R H?W ?C is query matrix projected by source intra-view feature (e.g., left-view), and K, V ? R H?W ?C are key, value matrices projected by target intra-view feature (e.g., right-view). Here, H, W, C represent height, width and number of channels of feature map. Since stereo images are highly symmetric under epipolar constraint <ref type="bibr" target="#b33">[34]</ref>, we use the same Q and K to represent each intra-view features, and calculates the correlation of cross-view features on a horizontal line (i.e., along W dimension). In detail, given the input stereo intra-view features X L , X R ? R H?W ?C , we can get layer normalized stereo featuresX L = LN(X L ) andX R = LN(X R ). Then, we calculate bidirectional cross-attention between left-right views by:</p><formula xml:id="formula_3">F R?L = Attention(W L 1XL , W R 1XR , W R 2 X R ), F L?R = Attention(W R 1XR , W L 1XL , W L 2 X L ),<label>(4)</label></formula><p>where W L 1 , W R 1 , W L 2 and W R 2 are projection matrices. Note that we can calculate the left-right attention matrix only once to generate both F R?L and F L?R (as shown in <ref type="figure" target="#fig_6">Figure 4</ref>). Finally, the interacted cross-view information F R?L , F L?R and intra-view information X L , X R are fused by element-wise addition:</p><formula xml:id="formula_4">F L = ? L F R?L + X L , F R = ? R F L?R + X R ,<label>(5)</label></formula><p>where ? L and ? R are trainable channel-wise scale and initialized with zeros for stabilizing training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training Strategies</head><p>Combat overfitting. In stereo image SR tasks, it is common practice to train models with small patches cropped from full-resolution images <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>. These patches are randomly flipped horizontally and vertically for data augmentation. To further utilize the training data, we introduce Channel Shuffle: which randomly shuffles the RGB channels of input images for color augmentation. In addition, we adopt stochastic depth <ref type="bibr" target="#b12">[13]</ref> as regularization.</p><p>Loss. For simplicity, we only use the pixel-wise L1 distance between the super-resolution and ground-truth stereo images:</p><formula xml:id="formula_5">L = I SR L ? I HR L 1 + I SR R ? I HR R 1<label>(6)</label></formula><p>where I SR L and I SR R represent the super-resolution left and right images generated by model respectively, and I HR L and I HR R represent their ground-truth high-resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Train-test Inconsistency</head><p>Chu et al. <ref type="bibr" target="#b2">[3]</ref> discover that the distribution of imagebased features during inference differs from that of patchbased features during training, and show that this traintest inconsistency harms model performance on debluring, denosising, deraining, and dehazing tasks. For stereo image super-resolution task, the regional range of the inputs for training and inference also varies greatly, e.g., the range of region for each patch is only 4.5% of low-resolution images (30 ? 90 vs. 300 ? 200) in Flickr1024 dataset. This prompts us to check the potential train-test inconsistency issue of channel attention used in our network.</p><p>In detail, given input features X, the channel attention (CA) first aggregates global spatial information using global average pooling (pool), and then redistributes the pooled information to input features as follows:</p><formula xml:id="formula_6">CA(X) = X * W pool(X),<label>(7)</label></formula><p>where W represents learnable matrix and * is a channelwise product operation. We apply TLSC <ref type="bibr" target="#b2">[3]</ref> to CA in Equation 7, which converts pool operation from global average pooling to local average pooling during inference, allowing it to extract representations based on local spatial region of features as in training phase. According to <ref type="bibr" target="#b2">[3]</ref>, the local size for pooling is simply set to 1.5? the size of the training patch. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Evaluation Metrics. Peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) were used as quantitative metrics. These metrics are calculated on RGB color space with a pair of stereo images (i.e., (Left + Right) /2).</p><p>Architecture. As shown in <ref type="table" target="#tab_2">Table 1</ref>, we construct 4 different size of NAFSSR networks by adjusting the number of channels and blocks, which are named NAFSSR-T (Tiny), NAFSSR-S (Small), NAFSSR-B (Base) and NAFSSR-L (Large). Besides, we use TLSC <ref type="bibr" target="#b2">[3]</ref> during inference as described in Section 3.3.</p><p>Training. All models are optimized by the AdamW with ? 1 = 0.9 and ? 2 = 0.9 with weight decay 0 by default. The learning rate is set to 3 ? 10 ?3 , and decreased to 1 ? 10 ?7 with cosine annealing strategy <ref type="bibr" target="#b21">[22]</ref>. If not specified, models are trained on 40 ? 100 patches with a batch size of 32 for 1 ? 10 5 iterations. We apply skipinit <ref type="bibr" target="#b6">[7]</ref> in our network, which may facilitate the training process. Data augmentation is implemented as described in Section 3.2. To overcome the overfitting issue, we use stochastic depth <ref type="bibr" target="#b12">[13]</ref> with 0.1, 0.2 and 0.3 probability for NAFSSR-S, NAFSSR-B and NAFSSR-L, respectively. In particular, since our lightweight model NAFSSR-T encounters underfitting rather than overfitting, it uses 4? training iterations without stochastic depth.</p><p>Datasets. We use the training dataset and validation dataset provided by NTIRE Stereo Image Super-Resolution Challenge <ref type="bibr" target="#b30">[31]</ref>. In detail, we use 800 stereo images from the training set of Flickr1024 <ref type="bibr" target="#b32">[33]</ref> dataset as the training data and 112 stereo images in the validation set of the Flickr1024 <ref type="bibr" target="#b32">[33]</ref> dataset as the validation set. The lowresolution images are generated by bicubic downsampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Study</head><p>Stereo Cross-Attention Module. Here, we take NAFSSR-S without Stereo Cross-Attention Module (SCAM) as a naive baseline to investigate the impact of the proposed SCAM on the model performance. In this experiment, we apply different number of SCAM to the naive baseline, ranging from 0 to 32. In detail, we use SCAM after a specific number of NAFBlocks in the middle of the naive baseline. Note that our naive baseline (with 0 SCAM) only uses single-view information. In contrast, our NAFSSR-S (with 32 SCAMs) interacts with cross-view information after every NAFBlocks.</p><p>As demonstrated by the results in <ref type="table">Table 2</ref>, our SCAM offers significant performance improvements compared to the baseline. The more number of SCAMs, the better performance. Compared to the naive baseline that uses only intra-view information, the PSNR on the Flickr1024 dataset can be improved by 0.18 dB with only one SCAM and by 0.29 dB with 32 SCAMs. These results indicate the importance of incorporating both cross-view information (introduced by our SCAM) and intra-view information (extracted by the NAFBlock).</p><p>Data augmentations. We trained our NAFSSR-S using different data augmentations to validate their effectiveness. Since we focus on data augmentation, we do not use Stochastic-Depth in this experiment. As shown in <ref type="table" target="#tab_3">Table 3</ref>, the performance of NAFSSR-S is improved by introducing the data augmentation: random flip horizontally, random flip vertically, and channel shuffle mentioned in Section 3.2.</p><p>When applying each data augmentation individually, the PSNR value of NAFSSR-S is improved by 0.19 dB with channel shuffle augmentation, which is compatible with random horizontal flip (+0.21 dB) and random vertical flip (+0.20 dB). This shows the effectiveness of channel shuffle augmentation. Moreover, channel shuffle is complementary to other augmentations. Using all three data augmentations boosts the PSNR value of NAFSSR-S from 23.43 dB to 23.82 dB, which is 0.09 dB better than random flip only.</p><p>Stochastic-Depth and TLSC. We use NAFSSR-S and NAFSSR-B to investigate the impact of stochastic depth <ref type="bibr" target="#b12">[13]</ref> during training and TLSC <ref type="bibr" target="#b2">[3]</ref> during inference. In <ref type="table">Table 4</ref>, we report results on one in-distribution <ref type="table">Table 4</ref>. Effect of stochastic depth <ref type="bibr" target="#b12">[13]</ref> and TLSC <ref type="bibr" target="#b2">[3]</ref> to PSNR values of different models for 4? SR on different datasets. dataset (i.e., Flickr1024 <ref type="bibr" target="#b31">[32]</ref> validation set) and three outdistribution datasets (i.e., KITTI 2012 <ref type="bibr" target="#b8">[9]</ref>, KITTI 2015 <ref type="bibr" target="#b24">[25]</ref>, Middlebury <ref type="bibr" target="#b26">[27]</ref>). During training, stochastic depth <ref type="bibr" target="#b12">[13]</ref> slightly improves the performance on all datasets (+0.03 dB) for NAFSSR-S, while it improves more for larger model NAFSSR-B on both model performance (+0.11 dB on in-distribution data) and generality (+0.16 dB on out-distribution test data). When training without stochastic depth, NAFSSR-B performs 0.16 dB better than NAFSSR-T on Flickr1024 but only 0.07 dB better on out-distribution data. However, when using stochastic depth, NAFSSR-B outperforms NAFSSR-T on Flickr1024 and out-of-distribution data by 0.25 dB and 0.2 dB, respectively. This shows that large models suffer from overfitting on Flickr1024 training data, while stochastic depth benefits networks and improves generality.</p><p>During inference, TLSC <ref type="bibr" target="#b2">[3]</ref> achieves similar improvements to both NAFSSR-T and NAFSSR-B on all datasets. This indicates that NAFSSR without TLSC provides suboptimal performance at test time due to the train-test inconsistency in stereo image SR tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to state-of-the-arts methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Settings</head><p>Training data. We use training data that are identical to iPASSR <ref type="bibr" target="#b33">[34]</ref> to provide a fair comparison with previous work. In detail, the 800 images from training set of Flickr1024 <ref type="bibr" target="#b32">[33]</ref> and 60 Middlebury <ref type="bibr" target="#b26">[27]</ref> images are used for training. Following <ref type="bibr" target="#b33">[34]</ref>, we perform bicubic downsampling by a factor of 2 on images from the Middlebury dataset to generate high-resolution (HR) ground truth images so that they match the spatial resolution of the Flickr1024 dataset. To produce low-resolution images, we apply bicubic downsampling to HR images on specific scaling factors (i.e., 2? and 4?) and then crop 30 ? 90 patches with a stride of 20 as inputs. Limited by the size of the offline cropped patches, we do not use additional random crop in this section.</p><p>Evaluation details. To evaluate SR results, 20 images from KITTI 2012 <ref type="bibr" target="#b8">[9]</ref> and 20 images from KITTI 2015 <ref type="bibr" target="#b24">[25]</ref>, 5 images from Middlebury <ref type="bibr" target="#b26">[27]</ref>, and 112 images from the test set of Flickr1024 <ref type="bibr" target="#b31">[32]</ref> are utilized for testing. Note that different from Section 4.1, the test images used in this section are from the test set instead of the validation set of Flickr1024 dataset. Following <ref type="bibr" target="#b33">[34]</ref>, we report PSNR/SSIM scores on the left images with their left boundaries (64 pixels) cropped, and average scores on stereo image pairs (i.e., (Left + Right) /2) without any boundary cropping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Results</head><p>We compare our NAFSSR (with 3 different variants) with existing super-resolution (SR) methods, including single image SR methods (i.e., VDSR <ref type="bibr" target="#b14">[15]</ref>, EDSR <ref type="bibr" target="#b19">[20]</ref>, RDN <ref type="bibr" target="#b39">[40]</ref>, and RCAN <ref type="bibr" target="#b38">[39]</ref>) and stereo image SR methods (i.e., Stere-oSR <ref type="bibr" target="#b13">[14]</ref>, PASSRnet <ref type="bibr" target="#b31">[32]</ref>, SRRes+SAM <ref type="bibr" target="#b37">[38]</ref>, IMSSRnet <ref type="bibr" target="#b16">[17]</ref>, iPASSR <ref type="bibr" target="#b33">[34]</ref> and SSRDE-FNet <ref type="bibr" target="#b3">[4]</ref>). This methods are trained on the same training datasets as ours and their PSNR and SSIM scores are reported by <ref type="bibr" target="#b3">[4]</ref>.</p><p>Quantitative Evaluations. The quantitative comparisons with existing SR methods are shown in <ref type="table">Table 5</ref>. Our smallest NAFSSR-T achieves competitive results as previous state-of-the-art (SSRDE-FNet <ref type="bibr" target="#b3">[4]</ref>), and our NAFSSR-S outperforms the state-of-the-art results on all datasets and upsampling factors (?2, ?4). Furthermore, our NAFSSR-B improves state-of-the-art results of all datasets by a significant margin. For example, for 4? stereo SR, our NAFSSR-B surpass previous state-of-the-art model SSRDE-FNet <ref type="bibr" target="#b3">[4]</ref> by 0.38 dB, 0.48 dB, 0.66 dB, 0.48 dB on KITTI 2012 <ref type="bibr" target="#b8">[9]</ref>, KITTI 2015 <ref type="bibr" target="#b24">[25]</ref>, Middlebury <ref type="bibr" target="#b26">[27]</ref> and Flickr1024 <ref type="bibr" target="#b31">[32]</ref>, respectively. This clearly shows the effectiveness of the proposed NAFSSR.</p><p>Parameter Efficiency and Scaling Ability. We also visualize the trade-off results between total numbers of parameters and PSNR on Flickr1024 dataset for 4? stereo SR. As shown in <ref type="figure">Figure 1</ref>, compared with SSRDE-FNet <ref type="bibr" target="#b3">[4]</ref>, our NAFSSR-T achieves state-of-the-art result with 79% parameter reduction. This shows that NAFSSR has high parameter efficiency. Furthermore, by scaling up the model size, our NAFSSR-S clearly surpasses competitive methods with similar total numbers of parameters, and NAFSSR-B and NAFSSR-L further push the state-of-the-art stereo SR performance. This shows the scaling ability of NAFSSR. <ref type="table">Table 5</ref>. Quantitative results achieved by different methods on the KITTI 2012 <ref type="bibr" target="#b8">[9]</ref>, KITTI 2015 <ref type="bibr" target="#b24">[25]</ref>, Middlebury <ref type="bibr" target="#b26">[27]</ref>, and Flickr1024 <ref type="bibr" target="#b31">[32]</ref> datasets. #P represents the number of parameters of the networks. Here, PSNR/SSIM values achieved on both the left images (i.e., Left) and a pair of stereo images (i.e., (Left + Right) /2) are reported. The best results are in bold faces. The results of NAFSSR-L are reported only for reference (gray).  Runtime Efficiency. We also report the runtimes (evaluated with 128 ? 128 input on RTX 2080Ti GPU) to compare the computational complexity between existing best model SSRDE-FNet <ref type="bibr" target="#b3">[4]</ref> and our NAFSSR. As shown in <ref type="table" target="#tab_0">Table 6</ref>, all variants of NAFSSR outperform SSRDE-FNet by a PSNR margin of 0.05 ? 0.48 dB on Flickr1024 <ref type="bibr" target="#b31">[32]</ref> Bicubic RCAN <ref type="bibr" target="#b38">[39]</ref> SRRes+SAM <ref type="bibr" target="#b37">[38]</ref> iPASSR <ref type="bibr" target="#b33">[34]</ref> SSRDE-FNet <ref type="bibr" target="#b3">[4]</ref> NAFSSR-B (ours) Reference Bicubic RCAN <ref type="bibr" target="#b38">[39]</ref> SRRes+SAM <ref type="bibr" target="#b37">[38]</ref> iPASSR <ref type="bibr" target="#b33">[34]</ref> SSRDE-FNet <ref type="bibr" target="#b3">[4]</ref> NAFSSR-B (ours) Reference <ref type="figure">Figure 6</ref>. Visual results (?4) achieved by different methods on the KITTI 2012 <ref type="bibr" target="#b8">[9]</ref> (top) and KITTI 2015 <ref type="bibr" target="#b24">[25]</ref> (bottom) dataset. The images with red and green borders represent the left and right views respectively. img sword2 (Left)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bicubic</head><p>StereoSR <ref type="bibr" target="#b13">[14]</ref> EDSR <ref type="bibr" target="#b19">[20]</ref> RDN <ref type="bibr" target="#b39">[40]</ref> RCAN <ref type="bibr" target="#b38">[39]</ref> SRRes+SAM <ref type="bibr" target="#b37">[38]</ref> iPASSR <ref type="bibr" target="#b33">[34]</ref> SSRDE-FNet <ref type="bibr" target="#b3">[4]</ref> NAFSSR-B (ours) Reference img sword2 (Right)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bicubic</head><p>StereoSR <ref type="bibr" target="#b13">[14]</ref> EDSR <ref type="bibr" target="#b19">[20]</ref> RDN <ref type="bibr" target="#b39">[40]</ref> RCAN <ref type="bibr" target="#b38">[39]</ref> SRRes+SAM <ref type="bibr" target="#b37">[38]</ref> iPASSR <ref type="bibr" target="#b33">[34]</ref> SSRDE-FNet <ref type="bibr" target="#b3">[4]</ref> NAFSSR-B (ours) Reference <ref type="figure">Figure 7</ref>. Visual results (?4) achieved by different methods on the Middlebury <ref type="bibr" target="#b26">[27]</ref> dataset.  <ref type="bibr" target="#b24">[25]</ref> and Middlebury <ref type="bibr" target="#b26">[27]</ref>. These figures show that our NAFSSR-B reconstructs pleasing SR images with rich details and clear edges. In contrast, other compared methods may suffer from unsatisfactory artifacts. This confirms the effectiveness of our NAFSSR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">NTIRE Stereo Image SR Challenge</head><p>We submitted a result obtained by the presented approach to the NTIRE 2022 Stereo Image Super-Resolution Challenge <ref type="bibr" target="#b30">[31]</ref>. In order to maximize the potential performance of our method, we further enlarge the NAFSSR-Base by increasing its depth and width. We adopt stronger stochastic depth <ref type="bibr" target="#b12">[13]</ref> with 0.3 or 0.4 probability to overcome the overfitting issue. During test-time, we adopt both self-ensemble <ref type="bibr" target="#b19">[20]</ref> and model ensemble strategy. Specifically, the data augmentations mentioned in Section 3.2 are used as test-time data augmentations for self-ensemble. Inspired by <ref type="bibr" target="#b35">[36]</ref>, we further ensemble multiple models trained with various hyper-parameters. As a result, our final submission achieves 24.239 dB PSNR on the validation set and won the first place with 23.787 dB PSNR on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper proposes a simple baseline named NAFSSR for stereo image super-resolution (SR). We use a stack of NAFBlock for intra-view feature extraction and combine it with stereo cross attention modules for cross-view feature interaction. Furthermore, we adopt stronger data augmentations for training and solve the train-test inconsistency in stereo image SR tasks by the test-time local converter. We also employ stochastic depth technique to improve the generality of large models. Extensive experiments show that NAFSSR surpasses current models and achieves state-ofthe-art performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*</head><label></label><figDesc>Equal contribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>The overall architecture of NAFSSR. SCAM represents Stereo Cross Attention Module (shown in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>NAFBlock. Simple Gate and Channel Attention Module are shown in Equation 2 and Equation 7, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3. 1 . 1</head><label>11</label><figDesc>Overall Framework An overview of our proposed NAFNet-based [2] Stereo Super-Resolution network (NAFSSR) is illustrated in Figure 2. NAFSSR takes the low-resolution stereo image pair as input and super-resolves both left and right view images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Stereo Cross Attention Module (SCAM). It fuses the features of the left and right views.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Visual results (?4) achieved by different methods on the Flickr1024<ref type="bibr" target="#b31">[32]</ref> dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 6</head><label>6</label><figDesc></figDesc><table /><note>).? Extensive experiments are conducted to demonstrate the effectiveness of our proposed NAFSSR. With the help of NAFSSR, we won 1st place in the NTIRE 2022 Stereo Image Super-resolution Challenge [31].</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Architecture Variants of NAFSSR.</figDesc><table><row><cell cols="2">Models</cell><cell cols="5">#Channels #Blocks #Params</cell></row><row><cell cols="2">NAFSSR-T</cell><cell>C = 48</cell><cell></cell><cell>N = 16</cell><cell>0.46M</cell><cell></cell></row><row><cell cols="2">NAFSSR-S</cell><cell>C = 64</cell><cell></cell><cell>N = 32</cell><cell>1.56M</cell><cell></cell></row><row><cell cols="2">NAFSSR-B</cell><cell>C = 96</cell><cell></cell><cell>N = 64</cell><cell>6.80M</cell><cell></cell></row><row><cell cols="2">NAFSSR-L</cell><cell>C = 128</cell><cell></cell><cell cols="3">N = 128 23.83M</cell></row><row><cell cols="7">Table 2. 4? SR results (PSNR) achieved on the Flickr1024 [33]</cell></row><row><cell cols="6">dataset by NAFSSR-S with different number of SCAMs.</cell><cell></cell></row><row><cell>#SCAM</cell><cell>0</cell><cell>1</cell><cell>4</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell cols="7">PSNR 23.56 23.74 23.76 23.79 23.82 23.85</cell></row><row><cell>?PSNR</cell><cell>-</cell><cell cols="5">+0.18 +0.20 +0.23 +0.26 +0.29</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>4? SR results (PSNR) achieved on Flickr1024 [33] by NAFSSR-S trained with different data augmentations. hflip and vflip represent horizontal flip and vertical flip, respectively. hflip vflip channel shuffle PSNR ?PSNR</figDesc><table><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>23.43</cell><cell>-</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>23.64</cell><cell>+0.21</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>23.63</cell><cell>+0.20</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>23.62</cell><cell>+0.19</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>23.73</cell><cell>+0.30</cell></row><row><cell>?</cell><cell>?</cell><cell>?</cell><cell>23.82</cell><cell>+0.39</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>80M 26.99/0.8121 26.17/0.8020 29.94/0.8561 27.08/0.8181 26.91/0.8245 30.04/0.8568 24.07/0.7551 NAFSSR-L (Ours) ?4 23.83M 27.04/0.8135 26.22/0.8034 30.11/0.8601 27.12/0.8194 26.96/0.8257 30.20/0.8605 24.17/0.7589</figDesc><table><row><cell>Method</cell><cell>Scale</cell><cell>#P</cell><cell></cell><cell>Left</cell><cell></cell><cell></cell><cell cols="2">(Left + Right) /2</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>KITTI 2012</cell><cell>KITTI 2015</cell><cell>Middlebury</cell><cell>KITTI 2012</cell><cell>KITTI 2015</cell><cell>Middlebury</cell><cell>Flickr1024</cell></row><row><cell>VDSR [15]</cell><cell>?2</cell><cell cols="8">0.66M 30.17/0.9062 28.99/0.9038 32.66/0.9101 30.30/0.9089 29.78/0.9150 32.77/0.9102 25.60/0.8534</cell></row><row><cell>EDSR [20]</cell><cell>?2</cell><cell cols="8">38.6M 30.83/0.9199 29.94/0.9231 34.84/0.9489 30.96/0.9228 30.73/0.9335 34.95/0.9492 28.66/0.9087</cell></row><row><cell>RDN [40]</cell><cell>?2</cell><cell cols="8">22.0M 30.81/0.9197 29.91/0.9224 34.85/0.9488 30.94/0.9227 30.70/0.9330 34.94/0.9491 28.64/0.9084</cell></row><row><cell>RCAN [39]</cell><cell>?2</cell><cell cols="8">15.3M 30.88/0.9202 29.97/0.9231 34.80/0.9482 31.02/0.9232 30.77/0.9336 34.90/0.9486 28.63/0.9082</cell></row><row><cell>StereoSR [14]</cell><cell>?2</cell><cell cols="8">1.08M 29.42/0.9040 28.53/0.9038 33.15/0.9343 29.51/0.9073 29.33/0.9168 33.23/0.9348 25.96/0.8599</cell></row><row><cell>PASSRnet [32]</cell><cell>?2</cell><cell cols="8">1.37M 30.68/0.9159 29.81/0.9191 34.13/0.9421 30.81/0.9190 30.60/0.9300 34.23/0.9422 28.38/0.9038</cell></row><row><cell>IMSSRnet [17]</cell><cell>?2</cell><cell>6.84M</cell><cell>30.90/-</cell><cell>29.97/-</cell><cell>34.66/-</cell><cell>30.92/-</cell><cell>30.66/-</cell><cell>34.67/-</cell><cell>-/-</cell></row><row><cell>iPASSR [34]</cell><cell>?2</cell><cell cols="8">1.37M 30.97/0.9210 30.01/0.9234 34.41/0.9454 31.11/0.9240 30.81/0.9340 34.51/0.9454 28.60/0.9097</cell></row><row><cell>SSRDE-FNet [4]</cell><cell>?2</cell><cell cols="8">2.10M 31.08/0.9224 30.10/0.9245 35.02/0.9508 31.23/0.9254 30.90/0.9352 35.09/0.9511 28.85/0.9132</cell></row><row><cell>NAFSSR-T (Ours)</cell><cell>?2</cell><cell cols="8">0.45M 31.12/0.9224 30.19/0.9253 34.93/0.9495 31.26/0.9254 30.99/0.9355 35.01/0.9495 28.94/0.9128</cell></row><row><cell>NAFSSR-S (Ours)</cell><cell>?2</cell><cell cols="8">1.54M 31.23/0.9236 30.28/0.9266 35.23/0.9515 31.38/0.9266 31.08/0.9367 35.30/0.9514 29.19/0.9160</cell></row><row><cell>NAFSSR-B (Ours)</cell><cell>?2</cell><cell cols="8">6.77M 31.40/0.9254 30.42/0.9282 35.62/0.9545 31.55/0.9283 31.22/0.9380 35.68/0.9544 29.54/0.9204</cell></row><row><cell>NAFSSR-L (Ours)</cell><cell>?2</cell><cell cols="8">23.79M 31.45/0.9261 30.46/0.9289 35.83/0.9559 31.60/0.9291 31.25/0.9386 35.88/0.9557 29.68/0.9221</cell></row><row><cell>VDSR [15]</cell><cell>?4</cell><cell cols="8">0.66M 25.54/0.7662 24.68/0.7456 27.60/0.7933 25.60/0.7722 25.32/0.7703 27.69/0.7941 22.46/0.6718</cell></row><row><cell>EDSR [20]</cell><cell>?4</cell><cell cols="8">38.9M 26.26/0.7954 25.38/0.7811 29.15/0.8383 26.35/0.8015 26.04/0.8039 29.23/0.8397 23.46/0.7285</cell></row><row><cell>RDN [40]</cell><cell>?4</cell><cell cols="8">22.0M 26.23/0.7952 25.37/0.7813 29.15/0.8387 26.32/0.8014 26.04/0.8043 29.27/0.8404 23.47/0.7295</cell></row><row><cell>RCAN [39]</cell><cell>?4</cell><cell cols="8">15.4M 26.36/0.7968 25.53/0.7836 29.20/0.8381 26.44/0.8029 26.22/0.8068 29.30/0.8397 23.48/0.7286</cell></row><row><cell>StereoSR [14]</cell><cell>?4</cell><cell cols="8">1.42M 24.49/0.7502 23.67/0.7273 27.70/0.8036 24.53/0.7555 24.21/0.7511 27.64/0.8022 21.70/0.6460</cell></row><row><cell>PASSRnet [32]</cell><cell>?4</cell><cell cols="8">1.42M 26.26/0.7919 25.41/0.7772 28.61/0.8232 26.34/0.7981 26.08/0.8002 28.72/0.8236 23.31/0.7195</cell></row><row><cell>SRRes+SAM [38]</cell><cell>?4</cell><cell cols="8">1.73M 26.35/0.7957 25.55/0.7825 28.76/0.8287 26.44/0.8018 26.22/0.8054 28.83/0.8290 23.27/0.7233</cell></row><row><cell>IMSSRnet [17]</cell><cell>?4</cell><cell>6.89M</cell><cell>26.44/-</cell><cell>25.59/-</cell><cell>29.02/-</cell><cell>26.43/-</cell><cell>26.20/-</cell><cell>29.02/-</cell><cell>-/-</cell></row><row><cell>iPASSR [34]</cell><cell>?4</cell><cell cols="8">1.42M 26.47/0.7993 25.61/0.7850 29.07/0.8363 26.56/0.8053 26.32/0.8084 29.16/0.8367 23.44/0.7287</cell></row><row><cell>SSRDE-FNet [4]</cell><cell>?4</cell><cell cols="8">2.24M 26.61/0.8028 25.74/0.7884 29.29/0.8407 26.70/0.8082 26.43/0.8118 29.38/0.8411 23.59/0.7352</cell></row><row><cell>NAFSSR-T (Ours)</cell><cell>?4</cell><cell cols="8">0.46M 26.69/0.8045 25.90/0.7930 29.22/0.8403 26.79/0.8105 26.62/0.8159 29.32/0.8409 23.69/0.7384</cell></row><row><cell>NAFSSR-S (Ours)</cell><cell>?4</cell><cell cols="8">1.56M 26.84/0.8086 26.03/0.7978 29.62/0.8482 26.93/0.8145 26.76/0.8203 29.72/0.8490 23.88/0.7468</cell></row><row><cell cols="4">NAFSSR-B (Ours) 6.img 0035 (Left) ?4 Bicubic SRRes+SAM [38]</cell><cell>StereoSR [14] iPASSR [34]</cell><cell cols="2">EDSR [20] SSRDE-FNet [4]</cell><cell>RDN [40] NAFSSR-B (ours)</cell><cell cols="2">RCAN [39] Reference</cell></row><row><cell></cell><cell></cell><cell>Bicubic</cell><cell></cell><cell>StereoSR [14]</cell><cell cols="2">EDSR [20]</cell><cell>RDN [40]</cell><cell cols="2">RCAN [39]</cell></row><row><cell>img 0035 (Right)</cell><cell></cell><cell cols="2">SRRes+SAM [38]</cell><cell>iPASSR [34]</cell><cell cols="2">SSRDE-FNet [4]</cell><cell>NAFSSR-B (ours)</cell><cell cols="2">Reference</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>PSNR v.s. runtimes on Flickr1024 dataset for 4? SR. up to 5.11? speedup. This indicates that the NAFSSR architecture is fast and efficient.Visual Comparison. InFigures 5, 6 and 7, we show the visual comparisons for ?4 stereo SR on Flickr1024<ref type="bibr" target="#b31">[32]</ref>, KITTI 2012<ref type="bibr" target="#b8">[9]</ref>, KITTI 2015</figDesc><table><row><cell>Models</cell><cell>PSNR</cell><cell cols="2">Time(ms) Speedup</cell></row><row><cell>SSRDEFNet [4]</cell><cell>23.59</cell><cell>238.5</cell><cell>1.00?</cell></row><row><cell cols="2">NAFSSR-T (Ours) 23.64 (+0.05)</cell><cell>46.7</cell><cell>5.11?</cell></row><row><cell cols="2">NAFSSR-S (Ours) 23.88 (+0.29)</cell><cell>91.8</cell><cell>2.60?</cell></row><row><cell cols="2">NAFSSR-B (Ours) 24.07 (+0.48)</cell><cell>224.9</cell><cell>1.06?</cell></row><row><cell>dataset, with</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">ton. Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Simple baselines for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.04676</idno>
		<imprint>
			<date type="published" when="2022" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Revisiting global statistics aggregation for improving image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojie</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liangyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengpeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04491</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Feedback network for mutually boosted stereo image super-resolution and disparity estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinyan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiaosi</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faming</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guixu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Second-order attention network for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="11065" to="11074" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image super-resolution via residual block attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hua</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Batch normalization biases residual blocks towards the identity function in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soham</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="3354" to="3361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan R</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Searching for mo-bilenetv3</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Sandler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grace</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang-Chieh</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingxing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoming</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1314" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kilian Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Enhancing the spatial resolution of stereo images using a parallax prior</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel S Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seung-Hwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inchang</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><forename type="middle">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1721" to="1730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename><forename type="middle">Kwon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangtao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xina</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinjin</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.12089</idno>
		<title level="m">Reflash dropout in image super-resolution</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep stereoscopic image super-resolution via interaction module</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianjun</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoting</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rakesh</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12288,2022.3</idno>
		<title level="m">Vrt: A video restoration transformer</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Swinir: Image restoration using swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingyun</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guolei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1833" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bee</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanghyun</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungjun</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoung Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zudi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Atmadeep</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel</forename><surname>Salma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deqing</forename><surname>Magid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pfister</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11279</idno>
		<title level="m">Revisiting rcan: Improved training for image super-resolution</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Sgdr: Stochastic gradient descent with warm restarts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Dynamic high-pass filtering and multi-spectral attention for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Salma Abdel Magid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donglai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Won-Dong</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zudi</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanspeter</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4288" to="4297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image superresolution with non-local sparse attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiqun</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuqian</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="3517" to="3526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="3061" to="3070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Single image super-resolution via a holistic attention network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weilei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenqi</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangde</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lianping</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuzhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaihao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaochun</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haifeng</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="191" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">High-resolution stereo datasets with subpixel-accurate ground truth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiko</forename><surname>Hirschm?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">York</forename><surname>Kitajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Krathwohl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nera</forename><surname>Ne?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Porter</forename><surname>Westling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German conference on pattern recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Stereoscopic image super-resolution with stereo consistent feature</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonil</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungil</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Somi</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Second Innovative Applications of Artificial Intelligence Conference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="12031" to="12038" />
		</imprint>
	</monogr>
	<note>The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Ntire 2022 challenge on stereo image super-resolution: Methods and results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuhang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPRW, 2022</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning parallax attention for stereo image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengfa</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zaiping</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="12250" to="12259" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Flickr1024: A large-scale dataset for stereo image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="0" to="0" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Symmetric parallax attention for stereo image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jungang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="766" to="775" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Resnet strikes back: An improved training procedure in timm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Wightman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herv?</forename><surname>J?gou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.00476</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Wortsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Ilharco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rebecca</forename><surname>Samir Yitzhak Gadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Roelofs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ari</forename><forename type="middle">S</forename><surname>Gontijo-Lopes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongseok</forename><surname>Morcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Carmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kornblith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.05482</idno>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Disparity-aware domain adaptation in stereo image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bahetiyaer</forename><surname>Bare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weimin</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="13179" to="13187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A stereo attention module for stereo image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyi</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Longguang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weidong</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulan</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="496" to="500" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image super-resolution using very deep residual channel attention networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kunpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European conference on computer vision (ECCV)</title>
		<meeting>the European conference on computer vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="286" to="301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Residual dense network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yapeng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bineng</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2472" to="2481" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cross view capture for stereo image superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyuan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kehua</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
