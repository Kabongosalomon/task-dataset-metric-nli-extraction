<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simple Baselines for Image Restoration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Xiaojie</roleName><forename type="first">Liangyu</forename><surname>Chen</surname></persName>
							<email>chenliangyu@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>CN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chu</forename><forename type="middle">?</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>CN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
							<email>zhangxiangyu@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>CN</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
							<email>sunjian@megvii.com</email>
							<affiliation key="aff0">
								<orgName type="institution">MEGVII Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<region>CN</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Simple Baselines for Image Restoration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T10:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image Restoration</term>
					<term>Image Denoise</term>
					<term>Image Deblur</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although there have been significant advances in the field of image restoration recently, the system complexity of the state-of-the-art (SOTA) methods is increasing as well, which may hinder the convenient analysis and comparison of methods. In this paper, we propose a simple baseline that exceeds the SOTA methods and is computationally efficient. To further simplify the baseline, we reveal that the nonlinear activation functions, e.g. Sigmoid, ReLU, GELU, Softmax, etc. are not necessary: they could be replaced by multiplication or removed. Thus, we derive a Nonlinear Activation Free Network, namely NAFNet, from the baseline. SOTA results are achieved on various challenging benchmarks, e.g. 33.69 dB PSNR on GoPro (for image deblurring), exceeding the previous SOTA 0.38 dB with only 8.4% of its computational costs; 40.30 dB PSNR on SIDD (for image denoising), exceeding the previous SOTA 0.28 dB with less than half of its computational costs. The code and the pre-trained models are released at github.com/megvii-research/NAFNet.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the development of deep learning, the performance of image restoration methods improve significantly. Deep learning based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25]</ref> have achieved tremendous success. E.g. <ref type="bibr" target="#b38">[39]</ref> and <ref type="bibr" target="#b7">[8]</ref> achieve 40.02/33.31 dB of PSNR on SIDD <ref type="bibr" target="#b0">[1]</ref>/GoPro <ref type="bibr" target="#b25">[26]</ref> for image denoising/deblurring respectively.</p><p>Despite their good performance, these methods suffer from high system complexity. For a clear discussion, we decompose the system complexity into two parts: inter-block complexity and intra-block complexity. First, the inter-block complexity, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref> introduce connections between varioussized feature maps; <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37]</ref> are multi-stage networks and the latter stage refine the results of the previous stage. Second, the intra-block complexity, i.e. the various design choices inside the block. E.g. Multi-Dconv Head Transposed Attention Module and Gated Dconv Feed-Forward Network in <ref type="bibr" target="#b38">[39]</ref> (as we shown in <ref type="figure" target="#fig_5">Figure 3a</ref>), Swin Transformer Block in <ref type="bibr" target="#b21">[22]</ref>, HINBlock in <ref type="bibr" target="#b4">[5]</ref>, and etc. It is not practical to evaluate the design choices one by one.</p><p>Based on the above facts, a natural question arises: Is it possible that a network with low inter-block and low intra-block complexity can achieve SOTA performance? To accomplish the first condition (low inter-block complexity), this paper adopts the single-stage UNet as architecture (following some SOTA methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b35">36]</ref>) and focuses on the second condition. To this end, we start with a plain block with the most common components, i.e. convolution, ReLU, and shortcut <ref type="bibr" target="#b13">[14]</ref>. From the plain block, we add/replace components of SOTA methods and verify how much performance gain do these components bring. By extensive ablation studies, we propose a simple baseline, as shown in <ref type="figure" target="#fig_5">Figure 3c</ref>, that exceeds the SOTA methods and is computationally efficient. It has the potential to inspire new ideas and make their verification easier. The baseline, which contains GELU <ref type="bibr" target="#b14">[15]</ref> and Channel Attention Module <ref type="bibr" target="#b15">[16]</ref> (CA), can be further simplified: we reveal that the GELU in the baseline can be regarded as a special case of the Gated Linear Unit <ref type="bibr" target="#b9">[10]</ref> (GLU), and from this we empirically demonstrate that it can be replaced by a simple gate, i.e. element-wise product of feature maps. In addition, we reveal the similarity of the CA to GLU in form, and the nonlinear activation functions in CA could be removed either. In conclusion, the simple baseline could be further simplified to a nonlinear activation free network, noted as NAFNet. We mainly conduct experiments on SIDD <ref type="bibr" target="#b0">[1]</ref> for image denoising, and GoPro <ref type="bibr" target="#b25">[26]</ref> for image deblurring, following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b36">37]</ref>. The main results are shown in <ref type="figure" target="#fig_0">Figure 1</ref>, our proposed baseline and NAFNet achieves SOTA results while being computationally efficient: 33.40/33.69 dB on GoPro, exceed previous SOTA <ref type="bibr" target="#b7">[8]</ref> 0.09/0.38 dB, respectively, with 8.4% of its computational cost; 40.30 dB on SIDD, exceed <ref type="bibr" target="#b38">[39]</ref> 0.28 dB with less than half of its computational costs. Extensive quantity and quality experiments are conducted to illustrate the effectiveness of our proposed baselines. The contributions of this paper are summarized as follows:</p><p>1. By decomposing the SOTA methods and extracting their essential components, we form a baseline (in <ref type="figure" target="#fig_5">Figure 3c</ref>) with lower system complexity, which can exceed the previous SOTA methods and has a lower computational cost, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. It may facilitate the researchers to inspire new ideas and evaluate them conveniently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">By revealing the connections between GELU, Channel Attention to Gated</head><p>Linear Unit, we further simplify the baseline by removing or replacing the nonlinear activation functions (e.g. Sigmoid, ReLU, and GELU), and propose a nonlinear activation free network, namely NAFNet. It can match or surpass the baseline although being simplified. To the best of our knowledge, it is the first work demonstrates that the nonlinear activation functions may not be necessary for SOTA computer vision methods. This work may have the potential to expand the design space of SOTA computer vision methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image Restoration</head><p>Image restoration tasks aim to restore a degraded image (e.g. noisy, blur) to a clean one. Recently, deep learning based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25]</ref> achieve SOTA results on these tasks, and most of the methods could be viewed as variants of a classical solution, UNet <ref type="bibr" target="#b28">[29]</ref>. It stacks blocks to a U-shaped architecture with skip-connection. The variants bring performance gain, as well as the system complexity, and we broadly categorized the complexity as inter-block complexity and intra-block complexity.</p><p>Inter-block Complexity <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b4">5]</ref> are multi-stage networks, i.e. the latter stage refine the results of the previous stage, and each stage is a U-shaped architecture. This design is based on the assumption that breaking down the difficult image restoration task into several subtasks contributes to performance. Differently, <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref> adopt the single-stage design and achieve competitive results, but they introduce complicated connections between various sized feature maps. Some methods adopt the above strategies both, e.g. <ref type="bibr" target="#b31">[32]</ref>. Other SOTA methods, e.g. <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b35">36]</ref> maintain the simple structure of single-stage UNet, yet they introduce intra-block complexity, which we will discuss next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-block Complexity</head><p>There are numerous different intra-block design schemes, we pick a few examples here. <ref type="bibr" target="#b38">[39]</ref> reduces the memory and time complexity of self-attention <ref type="bibr" target="#b33">[34]</ref> by channelwise attention map rather than spatialwise. Besides, gated linear units <ref type="bibr" target="#b9">[10]</ref> and depthwise convolution are adopted in the feed-forward network. <ref type="bibr" target="#b35">[36]</ref> introduces window-based multi-head self-attention, which is similar to <ref type="bibr" target="#b21">[22]</ref>. In addition, it introduces locally-enhanced feed-forward network in its block, which adds depthwise convolution to feed-forward network to enhance the local information capture ability. Differently, we reveal that increasing system complexity is not the only way to improve performance: SOTA performance could be achieved by a simple baseline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Gated Linear Units</head><p>Gated Linear Units <ref type="bibr" target="#b9">[10]</ref> (GLU) can be interpreted by the element-wise production of two linear transformation layers, one of which is activated with the non- The multi-stage architecture <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b36">37]</ref> stacks UNet architecture serially. (b) The multi-scale fusion architecture <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b6">7]</ref> fusions the features in different scales. (c)UNet architecture, which is adopted by some SOTA methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b35">36]</ref>. We use it as our architecture. Some details have been deliberately omitted for simplicity, e.g. downsample/upsample layers, feature fusion modules, input/output shortcut, and etc.</p><p>linearity. GLU or its variants has verified their effectiveness in NLP <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9]</ref>, and there is a prosperous trend of them in computer vision <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b19">20]</ref>. In this paper, we reveal the non-trivial improvement brought by GLU. Different from <ref type="bibr" target="#b29">[30]</ref>, we remove the nonlinear activation function in GLU without performance degradation. Furthermore, based on the fact that the nonlinear activation free GLU contains nonlinearity itself (as the product of two linear transformations raises nonlinearity), our baseline could be simplified by replacing the nonlinear activation functions with the multiplication of two feature maps. To the best of our knowledge, it is the first computer vision model achieves SOTA performance without nonlinear activation functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Build A Simple Baseline</head><p>In this section, we build a simple baseline for image restoration tasks from scratch. To keep the structure simple, our principle is not to add entities if they are not necessary. The necessity is verified by empirical evaluation of restoration tasks. We mainly conduct experiments with the model size around 16 GMACs following HINet Simple <ref type="bibr" target="#b4">[5]</ref>, and the MACs are estimated by an input with the spatial size of 256 ? 256. The results of models with different capacities are in the experimental section. We mainly validate the results (PSNR) on two popular datasets for denoising (i.e. SIDD <ref type="bibr" target="#b0">[1]</ref>) and deblurring (i.e. GoPro <ref type="bibr" target="#b25">[26]</ref> dataset), based on the fact that those tasks are fundamental in low-level vision. The design choices are discussed in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>To reduce the inter-block complexity, we adopt the classic single-stage U-shaped architecture with skip-connections, as shown in <ref type="figure" target="#fig_1">Figure 2c</ref>, following <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b35">36]</ref>. We believe the architecture will not be a barrier to performance. The experimental results confirmed our conjecture, in <ref type="table" target="#tab_8">Table 6</ref>, 7 and Figure 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A Plain Block</head><p>Neural Networks are stacked by blocks. We have determined how to stack blocks in the above (i.e. stacked in a UNet architecture), but how to design the internal structure of the block is still a problem. We start from a plain block with the most common components, i.e. convolution, ReLU, and shortcut <ref type="bibr" target="#b13">[14]</ref>, and the arrangement of these components follows <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b21">22]</ref>, as shown in <ref type="figure" target="#fig_5">Figure 3b</ref>. We will note it as PlainNet for simplicity. Using a convolution network instead of a transformer is based on the following considerations. First, although transformers show good performance in computer vision, some works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref> claim that they may not be necessary for achieving SOTA results. Second, depthwise convolution is simpler than the self-attention <ref type="bibr" target="#b33">[34]</ref> mechanism. Third, this paper is not intended to discuss the advantages and disadvantages of transformers and convolutional neural networks, but just to provide a simple baseline. The discussion of the attention mechanism is proposed in the subsequent subsection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Normalization</head><p>Normalization is widely adopted in high-level computer vision tasks, and there is also a popular trend in low-level vision. Although <ref type="bibr" target="#b25">[26]</ref> abandoned Batch Normalization <ref type="bibr" target="#b17">[18]</ref> as the small batch size may bring the unstable statistics <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b4">[5]</ref> re-introduce the Instance Normalization <ref type="bibr" target="#b32">[33]</ref> and avoids the small batch size issue. However, <ref type="bibr" target="#b4">[5]</ref> shows that adding instance normalization does not always bring performance gains and requires manual tuning. Differently, under the prosperity of transformers, Layer Normalization <ref type="bibr" target="#b2">[3]</ref> is used by more and more methods, including SOTA methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b21">22]</ref>. Based on these facts we conjecture Layer Normalization may be crucial to SOTA restorers, thus we add Layer Normalization to the plain block described above. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Activation</head><p>The activation function in the plain block, Rectified Linear Unit <ref type="bibr" target="#b27">[28]</ref> (ReLU), is extensively used in computer vision. However, there is a tendency to replace ReLU with GELU <ref type="bibr" target="#b14">[15]</ref> in SOTA methods <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12]</ref>. This replacement is implemented in our model either. The performance stays comparable on SIDD (from 39.73 dB to 39.71 dB) which is consistent with the conclusion of <ref type="bibr" target="#b22">[23]</ref>, yet it brings 0.21 dB performance gain (31.90 dB to 32.11 dB) on GoPro. In short, we replace ReLU with GELU in the plain block, because it keeps the performance of image denoising while bringing non-trivial gain on image deblurring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Attention</head><p>Considering the recent popularity of the transformer in computer vision, its attention mechanism is an unavoidable topic in the design of the internal structure of the block. There are many variants of attention mechanisms, and we discuss only a few of them here. The vanilla self-attention mechanism <ref type="bibr" target="#b33">[34]</ref>, which is adopted by <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b3">4]</ref>, generate the target feature by the linear combination of all features which are weighted by the similarity between them. Therefore, each feature contains global information, while it suffers from the quadratic computational complexity with the size of the feature map. Some image restoration tasks process data at high resolution which makes the vanilla self-attention not practical. Alternatively, <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b35">36]</ref> apply self-attention only in a fix-sized local window to alleviate the issue of increased computation. While it lacks global information. We do not take the window-based attention, as the local information could be well captured by the depthwise convolution <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref> in the plain block. Differently, <ref type="bibr" target="#b38">[39]</ref> modifies the spatial-wise attention to channel-wise, avoids the computation issue while maintaining global information in each feature. It could be seen as a special variant of channel attention <ref type="bibr" target="#b15">[16]</ref>. Inspired by <ref type="bibr" target="#b38">[39]</ref>, we realize the vanilla channel attention meets the requirements: computational efficiency and brings global information to the feature map. In addition, the effectiveness of channel attention has been verified in the image restoration task <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b7">8]</ref>, thus we add the channel attention to the plain block. It brings 0.14 dB on SIDD <ref type="bibr" target="#b0">[1]</ref> (39.71 dB to 39.85 dB), 0.24 dB on GoPro <ref type="bibr" target="#b25">[26]</ref> dataset (32.11 dB to 32.35 dB).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Summary</head><p>So far, we build a simple baseline from scratch, as we shown in <ref type="table" target="#tab_2">Table 1</ref>. The architecture and the block are shown in <ref type="figure" target="#fig_1">Figure 2c</ref> and <ref type="figure" target="#fig_5">Figure 3c</ref>, respectively. Each component in the baseline is trivial, e.g. Layer Normalization, Convolution, GELU, and Channel Attention. But the combination of these trivial components leads to a strong baseline: it can surpass the previous SOTA results on SIDD and GoPro dataset with only a fraction of computation costs, as we shown in <ref type="figure" target="#fig_0">Figure 1</ref> and <ref type="table" target="#tab_8">Table 6</ref>,7. We believe the simple baseline could facilitate the researchers to evaluate their ideas.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Nonlinear Activation Free Network</head><p>The baseline described above is simple and competitive, but is it possible to further improve performance while ensuring simplicity? Can it be simpler without performance loss? We try to answer these questions by looking for commonalities from some SOTA methods <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b16">17]</ref>. We find that in these methods, Gated Linear Units <ref type="bibr" target="#b9">[10]</ref>(GLU) are adopted. It implies that GLU might be promising. We will discuss it next. Gated Linear Units The gated linear units could be formulated as:</p><formula xml:id="formula_0">Gate(X, f, g, ?) = f (X) ? ?(g(X)),<label>(1)</label></formula><p>where X represents the feature map, f and g are linear transformers, ? is a non-linear activation function, e.g. Sigmoid, and ? indicates element-wise multiplication. As discussed above, adding GLU to our baseline may improve the performance yet the intra-block complexity is increasing as well. This is not what we expected. To address this, we revisit the activation function in the baseline, i.e. GELU <ref type="bibr" target="#b14">[15]</ref>:</p><formula xml:id="formula_1">GELU (x) = x?(x),<label>(2)</label></formula><p>where ? indicates the cumulative distribution function of the standard normal distribution. And based on <ref type="bibr" target="#b14">[15]</ref>, GELU could be approximated and implemented by:</p><formula xml:id="formula_2">0.5x(1 + tanh[ 2/?(x + 0.044715x 3 )]).<label>(3)</label></formula><p>From Eqn. 1 and Eqn. 2, it can be noticed that GELU is a special case of GLU, i.e. f , g are identity functions and take ? as ?. Through the similarity, we conjecture from another perspective that GLU may be regarded as a generalization of activation functions, and it might be able to replace the nonlinear activation functions. Further, we note that the GLU itself contains nonlinearity and does not depend on ?: even if the ? is removed, Gate(X) = f (X) ? g(X) contains nonlinearity. Based on these, we propose a simple GLU variant: directly divide the feature map into two parts in the channel dimension and multiply them, as we shown in <ref type="figure" target="#fig_2">Figure 4c</ref>, noted as SimpleGate. Compared to the complicated implementation of GELU in Eqn.3, our SimpleGate could be implemented by an element-wise multiplication, that's all:</p><formula xml:id="formula_3">SimpleGate(X, Y) = X ? Y,<label>(4)</label></formula><p>where X and Y are feature maps of the same size. By replacing GELU in the baseline to the proposed SimpleGate, the performance of image denoising (on SIDD <ref type="bibr" target="#b0">[1]</ref>) and image deblurring (on GoPro <ref type="bibr" target="#b25">[26]</ref> dataset) boost 0.08 dB (39.85 dB to 39.93 dB) and 0.41 dB (32.35 dB to 32.76 dB) respectively. The results demonstrate that GELU could be replaced by our proposed SimpleGate. At this point, only a few types of nonlinear activations left in the network: Sigmoid and ReLU in the channel attention module <ref type="bibr" target="#b15">[16]</ref>, and we will discuss the simplifications of it next.</p><p>Simplified Channel Attention In Section 3, we adopt the channel attention <ref type="bibr" target="#b15">[16]</ref> into our block as it captures the global information and it is computationally efficient. It is illustrated in <ref type="figure" target="#fig_2">Figure 4a</ref>: it squeezes the spatial information into channels first and then a multilayer perceptual applies to it to calculate the channel attention, which will be used to weight the feature map. It could be represented as:</p><formula xml:id="formula_4">CA(X) = X * ?(W 2 max(0, W 1 pool(X))),<label>(5)</label></formula><p>where X represents the feature map, pool indicates the global average pooling operation which aggregates the spatial information into channels. ? is a nonlinear activation function, Sigmoid, W 1 , W 2 are fully-connected layers and ReLU is adopted between two fully-connected layers. Last, * is a channelwise product operation. If we regard the channel-attention calculation as a function, noted as ? with input X , Eqn. 5 could be re-writed as:</p><formula xml:id="formula_5">CA(X) = X * ? (X).<label>(6)</label></formula><p>It can be noticed that Eqn. 6 is very similar to Eqn. 1. This inspires us to consider channel attention as a special case of GLU, which can be simplified like GLU in the previous subsection. By retaining the two most important roles of channel attention, that is, aggregating global information and channel information interaction, we propose the Simplified Channel Attention:</p><formula xml:id="formula_6">SCA(X) = X * W pool(X).<label>(7)</label></formula><p>The notations follows Eqn. 5. Apparently, Simplified Channel Attention (Eqn. 7) is simpler than the original one (Eqn. 5), as shown in <ref type="figure" target="#fig_2">Figure 4a</ref> and <ref type="figure" target="#fig_2">Figure 4b</ref>.</p><p>Although it is simpler, there is no loss of performance: +0.03 dB (39.93 dB to 39.96 dB) on SIDD and +0.09 dB (32.76 dB to 32.85 dB) on GoPro.</p><p>Summary Starting from the baseline proposed in Section 3, we further simplify it by replacing the GELU with SimpleGate and Channel Attention to Simplified Channel Attention, without loss of performance. We emphasize that after the simplification, there are no nonlinear activation functions (e.g. ReLU, GELU, Sigmoid, etc.) in the network. So we call this baseline Nonlinear Activation Free Network, namely NAFNet. It can match or surpass the baseline although without nonlinear activation functions, as we shown in <ref type="figure" target="#fig_0">Figure 1</ref> and <ref type="table" target="#tab_8">Table 6</ref>,7.</p><p>We can now answer the questions in the begining of this section by yes, because of the simplicity and effectiveness of NAFNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we analyze the effect of the design choices of NAFNet described in previous sections in detail.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Ablations</head><p>The ablation studys are conducted on image denoising (SIDD <ref type="bibr" target="#b0">[1]</ref>) and deblurring (GoPro <ref type="bibr" target="#b25">[26]</ref>) tasks. We follow experiments setting of <ref type="bibr" target="#b4">[5]</ref> if not specified, e.g. <ref type="bibr" target="#b15">16</ref> GMACs of computational budget, gradient clip, and PSNR loss. We train models with Adam <ref type="bibr" target="#b18">[19]</ref> optimizer (? 1 = 0.9, ? 2 = 0.9, weight decay 0) for total 200K iterations with the initial learning rate 1e ?3 gradually reduced to 1e ?6 with the cosine annealing schedule <ref type="bibr" target="#b23">[24]</ref>. The training patch size is 256 ? 256 and batch size is 32. Training by patches and testing by the full image raises performance degradation <ref type="bibr" target="#b7">[8]</ref>, we solve it by adopting TLC <ref type="bibr" target="#b7">[8]</ref> following MPRNet-local <ref type="bibr" target="#b7">[8]</ref>. The effectiveness of TLC on GoPro 1 is shown in Tab 4. We mainly compare TLC with "test by patches" strategy, which is adopted by <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b24">[25]</ref>, and etc. It brings performance gains and avoids the artifacts brought by patches. Moreover, we apply skip-init <ref type="bibr" target="#b10">[11]</ref> to stabilize training following <ref type="bibr" target="#b22">[23]</ref>. The default width and number of blocks are 32 and 36, respectively. We adjust the width to keep the computational budget hold if the number of blocks changed. We report Peak Signal to Noise Ratio (PSNR) and Structural SIMilarity (SSIM) in our experiments. The speed/memory/computational complexity evaluation is conducted with an input size of 256 ? 256, on an NVIDIA 2080Ti GPU.</p><p>From PlainNet to the simple baseline: PlainNet is defined in Section 3, and its block is illustrated in <ref type="figure" target="#fig_5">Figure 3b</ref>. We find that the training of PlainNet is unstable under the default settings. As an alternative, we reduce the learning rate (lr) by a factor of 10 to make the model trainable. This issue is solved by introducing Layer Normalization (LN): the learning rate can be increased from 1e ?4 to 1e ?3 with a more stable training process. In PSNR, LN brings 0.46 dB and 3.39 dB on SIDD and GoPro respectively. Besides, GELU and Channel Attention (CA) also demonstrated their effectiveness in <ref type="table" target="#tab_2">Table 1</ref>.</p><p>From the simple baseline to NAFNet: As described in Section 3, NAFNet can be obtained by simplifying the baseline. In Tab 2, we show that there is no performance penalty for this simplification. Instead, the PSNR boosts 0.11 dB and 0.50 dB in SIDD and GoPro respectively. The computational complexity  is consistent for a fair comparison, and details in the supplementary material. The speedup of modifications compared to Baseline is provided. In addition, no significant extra memory consumption compares to Baseline in inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of blocks:</head><p>We verify the effect of the number of blocks on NAFNet in <ref type="table" target="#tab_4">Table 3</ref>. We mainly consider the latency at spatial size 720 ? 1280, as this is the size of the entire GoPro image. In the process of increasing the number of blocks to 36, the performance of the model has been greatly improved, and the latency has not increased significantly (+14.5% compares to 9 blocks). When the number of blocks further increases to 72, the performance improvement of the model is not obvious, but the latency increases significantly (+30.0% compares to 36 blocks). Because 36 blocks can achieve a better performance/latency balance, we use it as the default option.</p><p>Variants of ? in SimpleGate: Vanilla gated linear unit (GLU) contains a nonlinear activation function ? as formulated in Eqn. 1. Our proposed SimpleGate, as shown in Eqn. 4 and <ref type="figure" target="#fig_2">Figure 4c</ref> removes it. In other words, ? in SimpleGate is set as an identity function. We variants the ? from the identity function to different nonlinear activation functions in <ref type="table" target="#tab_6">Table 5</ref> to judge the importance of nonlinearity in ?. PSNR on SIDD is basically unaffected (fluctuates from 39.96   , which indicates that in NAFNet, the ? in SimpleGate may not be necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Applications</head><p>We apply NAFNet to various image restoration tasks, follow the training settings of ablation study if not specified, except that it is enlarged by increasing the width from 32 to 64. Besides, batch size and total training iterations are 64 and 400K respectively, following <ref type="bibr" target="#b4">[5]</ref>. Random crop augmentation is applied. We report the mean of three experimental results. The baseline is enlarged to achieve better results, details in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RGB Image Denoising</head><p>We compare the RGB Image Denoising results with other SOTA methods on SIDD, show in <ref type="table" target="#tab_8">Table 6</ref>. Baseline and its simplified version NAFNet, exceed the previous best result Restormer 0.28 dB with only a fraction of its computational cost, as shown in <ref type="figure" target="#fig_0">Figure 1</ref>. The qualitative results are shown in <ref type="figure" target="#fig_3">Figure 5</ref>. Our proposed baselines can restore more fine details compared to other methods. Moreover, we achieve SOTA result (40.15 dB) on the online benchmark, exceed previous top-ranked methods 0.23 dB. Image Deblurring We compare the deblurring results of SOTA methods on GoPro <ref type="bibr" target="#b25">[26]</ref> dataset, flip and rotate augmentations are adopted. As we shown in <ref type="table" target="#tab_9">Table 7</ref> and <ref type="figure" target="#fig_0">Figure 1</ref>, our baseline and NAFNet surpass the previous best method MPRNet-local <ref type="bibr" target="#b7">[8]</ref> 0.09 dB and 0.38 dB in PSNR, respectively, with only 8.4% of its computational costs. The visualization results are shown in <ref type="figure">Figure  6</ref>, our baselines can restore sharper results compares to other methods.  Raw Image Denoising We apply NAFNet to a raw image denoising task. The training and testing settings follow PMRID <ref type="bibr" target="#b34">[35]</ref>, and we noted the testing set as 4Scenes (as the dataset contains 39 raw images of 4 different scenes in various light conditions) for simplicity. In addition, we make fair comparison by changing the width and number of blocks of NAFNet from 32 to 16, 36 to 7, respectively, so that the computational cost is less than PMRID. The results shown in <ref type="table" target="#tab_10">Table 8</ref> and <ref type="figure" target="#fig_4">Figure 7</ref> demonstrate NAFNet can surpass PMRID quantitatively and qualitatively. In addition, this experiment indicates our NAFNet can be scaled flexibly (from 1.1 GMACs to 65 GMACs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image Deblurring with JPEG artifacts</head><p>We conduct experiments on REDS <ref type="bibr" target="#b26">[27]</ref> dataset, the training setting follows <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32]</ref>, and we evaluate the result on 300 images from the validation set (noted as REDS-val-300) following <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">32]</ref>. As shown in <ref type="table" target="#tab_11">Table 9</ref>, our method outperforms other competing methods, including the    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>By decomposing the SOTA methods, we extract the essential components and adopt them on a naive PlainNet. The obtained baseline reaches SOTA performance on image denoising and image deblurring tasks. By analyzing the baseline, we reveal that it can be further simplified: The nonlinear activation functions in it can be completely replaced or removed. From this, we propose a nonlinear activation free network, NAFNet. Although simplified, its performance is equal to or better than baseline. Our proposed baselines may facilitate the researchers to evaluate their ideas. In addition, this work has the potential to influence future computer vision model design, as we demonstrate that nonlinear activation functions are not necessary to achieve SOTA performance.  <ref type="figure" target="#fig_0">Fig. 1</ref>: Additional qualitatively comparison of raw image denoising results with PMRID <ref type="bibr" target="#b34">[35]</ref>. Zoom in to see details</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Channel Attention and Simplified Channel Attention</head><p>For a feature map with width of c, the channel attention module shrinks it by a factor of r and then project it back into c (by fully-connect layer). The computational cost could be approximated by c ? c/r + c/r ? c. As to the simplified channel attention module, its computational cost is c ? c. For a fair comparison, we choose r = 2 so that their computational costs are consistent in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Feature Fusion</head><p>There are skip connections from the encoder block to the decoder block, and there are several ways to fuse the features of encoder/decoder. In <ref type="bibr" target="#b4">[5]</ref>, the encoder features are transformed by a convolution and then concatenate with the decoder features. In <ref type="bibr" target="#b38">[39]</ref>, features are concatenated first and then transformed by a convolution. Differently, we simply element-wise add the encoder and decoder features as the feature fusion approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Downsample/Upsample Layer</head><p>For the downsample layer, we use the convolution with a kernel size of 2 and a stride of 2. This design choice is inspired by <ref type="bibr" target="#b1">[2]</ref>. For the upsample layer, we double the channel width by a pointwise convolution first, and then follows a pixel shuffle module <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B More Visualization Results</head><p>We provide additional visualization results of raw image denoising, image deblurring, RGB image denoising tasks, as we shown in <ref type="figure" target="#fig_0">Figure 1, 2</ref>  <ref type="bibr" target="#b4">[5]</ref> MPRNet-local <ref type="bibr" target="#b7">[8]</ref> 26.96 dB 25.67 dB Restormer <ref type="bibr" target="#b38">[39]</ref> MPRNet <ref type="bibr" target="#b36">[37]</ref> 28.11 dB 28.71 dB Baseline(ours) NAFNet(ours) <ref type="figure" target="#fig_1">Fig. 2</ref>: Additional qualitative comparison of image deblurring methods baselines can restore more fine details compare to other methods. It is recommended to zoom in to compare the details in the red box.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>?Fig. 1 :</head><label>1</label><figDesc>Equally contribution. arXiv:2204.04676v4 [cs.CV] 1 Aug 2022 PSNR vs. computational cost on Image Deblurring (left) and Image Denoising (right) tasks</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Multi-Scale Fusion Architecture (c) UNet Architecture(ours) Comparison of architectures of image restoration models. Dashes to distinguish features of different sizes. (a)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Illustration of (a) Channel Attention<ref type="bibr" target="#b15">[16]</ref> (CA), (b) Simplified Channel Attention (SCA), and (c) Simple Gate (SG). ?/ * : element-wise/channel-wise multiplication</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Qualitative comparison of image denoising methods on SIDD<ref type="bibr" target="#b0">[1]</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 :</head><label>7</label><figDesc>Qualitatively compare the noise reduction effects of PMRID<ref type="bibr" target="#b34">[35]</ref> and our porposed NAFNet. Zoom in to see details</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3 :</head><label>3</label><figDesc>Additional qualitative comparison of image denoising methods</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>This change can make training smooth, even with a 10? increase in learning rate. The larger learning rate brings significant performance gain: +0.44 dB (39.29 dB to 39.73 dB) on SIDD[1], +3.39 dB (28.51 dB to 31.90 dB) on GoPro<ref type="bibr" target="#b25">[26]</ref> dataset. To sum up, we add Layer Normalization to the plain block as it can stabilize the training process. Intra-block structure comparison. ?:matrix multiplication, ?/?:elementwise multiplication/addition. dconv: Depthwise convolution. Nonlinear activation functions are represented by yellow boxes. (a) Restormer's block<ref type="bibr" target="#b38">[39]</ref>, some details are omitted for simplicity, e.g. reshaping the feature maps. (b) PlainNet's block, which contains the most common components. (c) Our proposed baseline.</figDesc><table><row><cell>(a)Restormer's</cell><cell>(b)PlainNet's</cell><cell>(c)Baseline's</cell><cell>(d)NAFNet's</cell></row><row><cell>Block</cell><cell>Block</cell><cell>Block(ours)</cell><cell>Block(ours)</cell></row><row><cell>Fig. 3:</cell><cell></cell><cell></cell><cell></cell></row></table><note>Compares to (b), Channel Attention (CA) and LayerNorm are adopted. Be- sides, ReLU is replaced by GELU. (d) Our proposed Nonlinear Activation Free Network's block. It replaces CA/GELU with Simplified Channel Attention(SCA) and SimpleGate respectively. The details of these components are shown in Fig 4</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Next, we apply our proposed NAFNet to various image restoration applications, including RGB image denoising, image deblurring, raw image denoising, and image deblurring with JPEG artifacts.</figDesc><table><row><cell>PSNR</cell><cell>19.01 dB</cell><cell>35.21 dB</cell><cell>35.01 dB</cell><cell>34.96 dB</cell><cell>35.97 dB</cell><cell>35.77 dB</cell></row><row><cell>Reference</cell><cell>Noisy</cell><cell>HINet[5]</cell><cell>Restormer[39]</cell><cell>MPRNet[37]</cell><cell cols="2">Baseline(ours) NAFNet(ours)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Build a simple baseline from PlainNet. The effectiveness of Layer Normalization (LN), GELU, and Channel Attention (CA) have been verified. * indicates that the training is unstable due to the large learning rate (lr)</figDesc><table><row><cell>lr</cell><cell>LN</cell><cell>ReLU?GELU</cell><cell>CA</cell><cell cols="2">SIDD PSNR SSIM PSNR SSIM GoPro</cell></row><row><cell>PlainNet 1e ?4</cell><cell></cell><cell></cell><cell></cell><cell cols="2">39.29 0.956 28.51 0.907</cell></row><row><cell>PlainNet  *  1e ?3</cell><cell></cell><cell></cell><cell></cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>1e ?3</cell><cell>?</cell><cell></cell><cell></cell><cell cols="2">39.73 0.959 31.90 0.952</cell></row><row><cell>1e ?3</cell><cell>?</cell><cell>?</cell><cell></cell><cell cols="2">39.71 0.958 32.11 0.954</cell></row><row><cell>Baseline 1e ?3</cell><cell>?</cell><cell>?</cell><cell>?</cell><cell cols="2">39.85 0.959 32.35 0.956</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>NAFNet is derived from the simplification of baseline, i.e. replacing GELU to SimpleGate (SG), and replacing Channel Attention (CA) to Simplified Channel Attention (SCA).</figDesc><table><row><cell></cell><cell cols="2">GELU?SG CA?SCA</cell><cell>SIDD PSNR SSIM PSNR SSIM GoPro</cell><cell>speedup</cell></row><row><cell>Baseline</cell><cell></cell><cell></cell><cell cols="2">39.85 0.959 32.35 0.956 1.00?</cell></row><row><cell></cell><cell>?</cell><cell></cell><cell cols="2">39.93 0.960 32.76 0.960 0.98?</cell></row><row><cell></cell><cell></cell><cell>?</cell><cell cols="2">39.95 0.960 32.54 0.958 1.11?</cell></row><row><cell>NAFNet</cell><cell>?</cell><cell>?</cell><cell cols="2">39.96 0.960 32.85 0.960 1.09?</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The effect of the number of blocks. The width is adjusted to keep the computational budget hold. Latency-256 and Latency-720 is based on the input size 256 ? 256 and 720 ? 1280 respectively, in milliseconds</figDesc><table><row><cell></cell><cell># of blocks</cell><cell>SIDD PSNR SSIM PSNR SSIM GoPro</cell><cell cols="2">Latency-256 Latency-720</cell></row><row><cell></cell><cell>9</cell><cell>39.78 0.959 31.79 0.951</cell><cell>11.8</cell><cell>154.7</cell></row><row><cell>NAFNet</cell><cell>18 36</cell><cell>39.90 0.960 32.64 0.951 39.96 0.960 32.85 0.959</cell><cell>19.9 39.1</cell><cell>151.7 177.1</cell></row><row><cell></cell><cell>72</cell><cell>39.95 0.960 32.88 0.961</cell><cell>73.8</cell><cell>230.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell cols="3">: Effectiveness of TLC[8]</cell></row><row><cell cols="2">on GoPro[26]</cell><cell></cell></row><row><cell></cell><cell cols="2">patches? TLC? PSNR SSIM</cell></row><row><cell></cell><cell></cell><cell>33.08 0.963</cell></row><row><cell>NAFNet</cell><cell>?</cell><cell>33.65 0.966</cell></row><row><cell></cell><cell>?</cell><cell>33.69 0.967</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="2">: Variants of ? in SimpleGate(X, Y) =</cell></row><row><cell>X ? ?(Y)</cell><cell></cell></row><row><cell>?</cell><cell>SIDD PSNR SSIM PSNR SSIM GoPro</cell></row><row><cell cols="2">Identity(ours) 39.96 0.960 32.85 0.960</cell></row><row><cell>ReLU</cell><cell>39.98 0.960 32.59 0.958</cell></row><row><cell>GELU</cell><cell>39.97 0.960 32.72 0.959</cell></row><row><cell>Sigmoid</cell><cell>39.99 0.960 32.50 0.958</cell></row><row><cell>SiLU</cell><cell>39.96 0.960 32.74 0.960</cell></row><row><cell cols="2">dB to 39.99 dB), while PSNR on GoPro drops significantly (-0.11 dB to -0.35</cell></row><row><cell>dB)</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Image Denoising Results on SIDD<ref type="bibr" target="#b0">[1]</ref> </figDesc><table><row><cell>Method</cell><cell cols="8">MPRNet MIRNet NBNet UFormer MAXIM HINet Restormer Baseline NAFNet [37] [40] [6] [36] [32] [5] [39] ours ours</cell></row><row><cell>PSNR</cell><cell>39.71</cell><cell>39.72</cell><cell>39.75</cell><cell>39.89</cell><cell>39.96 39.99</cell><cell>40.02</cell><cell>40.30</cell><cell>40.30</cell></row><row><cell>SSIM</cell><cell>0.958</cell><cell>0.959</cell><cell>0.959</cell><cell>0.960</cell><cell>0.960 0.958</cell><cell>0.960</cell><cell>0.962</cell><cell>0.962</cell></row><row><cell>MACs(G)</cell><cell>588</cell><cell>786</cell><cell>88.8</cell><cell>89.5</cell><cell>169.5 170.7</cell><cell>140</cell><cell>84</cell><cell>65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Image Deblurring Results on GoPro<ref type="bibr" target="#b25">[26]</ref> </figDesc><table><row><cell>Method</cell><cell cols="8">MIMO-UNet HINet MAXIM Restormer UFormer DeepRFT MPRNet Baseline NAFNet [7] [5] [32] [39] [36] [25] -local[8] ours ours</cell></row><row><cell>PSNR</cell><cell>32.68</cell><cell>32.71 32.86</cell><cell>32.92</cell><cell>32.97</cell><cell>33.23</cell><cell>33.31</cell><cell>33.40</cell><cell>33.69</cell></row><row><cell>SSIM</cell><cell>0.959</cell><cell>0.959 0.961</cell><cell>0.961</cell><cell>0.967</cell><cell>0.963</cell><cell>0.964</cell><cell>0.965</cell><cell>0.967</cell></row><row><cell>MACs(G)</cell><cell>1235</cell><cell>170.7 169.5</cell><cell>140</cell><cell>89.5</cell><cell>187</cell><cell>778.2</cell><cell>84</cell><cell>65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Raw image denoising results on 4Scenes<ref type="bibr" target="#b34">[35]</ref> </figDesc><table><row><cell>Method</cell><cell cols="2">PSNR SSIM MACs(G)</cell></row><row><cell>PMRID[35]</cell><cell>39.76 0.975</cell><cell>1.2</cell></row><row><cell cols="2">NAFNet(ours) 40.05 0.977</cell><cell>1.1</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc></figDesc><table><row><cell cols="3">Image deblurring results on</cell></row><row><cell cols="2">REDS-val-300[27]</cell><cell></cell></row><row><cell>Method</cell><cell cols="2">PSNR SSIM MACs(G)</cell></row><row><cell>MPRNet[37]</cell><cell>28.79 0.811</cell><cell>776.7</cell></row><row><cell>HINet[5]</cell><cell>28.83 0.862</cell><cell>170.7</cell></row><row><cell>MAXIM[32]</cell><cell>28.93 0.865</cell><cell>169.5</cell></row><row><cell cols="2">NAFNet(ours) 29.09 0.867</cell><cell>65</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>, and 3. Our</figDesc><table><row><cell>PSNR</cell><cell>19.45 dB</cell></row><row><cell>Reference</cell><cell>Blurry</cell></row><row><cell>25.81 dB</cell><cell>27.03 dB</cell></row><row><cell>HINet</cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">SIDD test on 256 ? 256 patches avoid the inconsistent issue.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Other Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Inverted Bottleneck</head><p>Following <ref type="bibr" target="#b22">[23]</ref> we adopt inverted bottleneck design in the baseline and NAFNet. We first discuss the setting of the ablation studies. In the baseline, the channel width within the first skip connection is always consistent with the input, its computational cost could be approximated by:</p><p>where H, W represent the spatial size of the feature map, c indicates the input dimension, and k is the kernel size of the depthwise convolution (3 in our experiments). In practice,</p><p>The hidden dimension within the second skip connection is twice the input dimension, its computational cost is:</p><p>notations following Eqn. <ref type="bibr" target="#b0">(1)</ref>. As a result, the overall computational cost of one baseline block ? 6 ? H ? W ? c ? c.</p><p>As for NAFNet's block, the SimpleGate module shrinks the channel width by half. We double the hidden dimension in the first skip connection, and its computational cost could be approximated by:</p><p>notations following Eqn. <ref type="bibr" target="#b0">(1)</ref>. And the hidden dimension in the second skip connection follows baseline. Its computational cost is:</p><p>As a result, the overall computational cost of one NAFNet's block ? 6 ? H ? W ? c ? c, which is consistent with the baseline's block. The advantage of this is that the baseline and NAFNet can share hyperparameters, such as the number of blocks, learning rate, etc. As for the applications, the hidden dimension of the baseline's first skip connection is expanded to achieve better results. In addition, it should be noted that the above discussion omits the computation of some modules, e.g. layer normalization, GELU, channel attention, and etc., as their computational cost is negligible compared to convolution.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A high-quality denoising dataset for smartphone cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Abdelhamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alsallakh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kokhlikyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Miglani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Reblitz-Richardson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02178</idno>
		<title level="m">Mind the pad-cnns can develop blind spots</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Pre-trained image processing transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="12299" to="12310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Hinet: Half instance normalization network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="182" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nbnet: Noise basis learning for image denoising with subspace projection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4896" to="4906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Rethinking coarse-to-fine approach in single image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Ko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4641" to="4650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2112.04491</idno>
		<title level="m">Improving image restoration by revisiting global information aggregation</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1901.02860</idno>
		<title level="m">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="933" to="941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization biases residual blocks towards the identity function in deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.11929</idno>
		<title level="m">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Demystifying local vision transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04263</idno>
	</analytic>
	<monogr>
		<title level="m">Sparse connectivity, weight sharing, and dynamic weight</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hendrycks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gimpel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.08415</idno>
		<title level="m">Gaussian error linear units (gelus)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.10447</idno>
		<title level="m">Transformer quality in linear time</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.12288</idno>
		<title level="m">Vrt: A video restoration transformer</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Swinir: Image restoration using swin transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="1833" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="10012" to="10022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.03545</idno>
		<title level="m">A convnet for the 2020s</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.03983</idno>
		<title level="m">Sgdr: Stochastic gradient descent with warm restarts</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.11745</idno>
		<title level="m">Deep residual fourier transformation for single image deblurring</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hyun Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3883" to="3891" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ntire 2021 challenge on image deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="149" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Icml</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.05202</idno>
		<title level="m">Glu variants improve transformer</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Husz?r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Talebi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.02973</idno>
		<title level="m">Maxim: Multi-axis mlp for image processing</title>
		<imprint>
			<date type="published" when="2022" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Practical deep raw image denoising on mobile devices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Uformer: A general u-shaped transformer for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.03106</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Waqas Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shahbaz Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<title level="m">Multi-stage progressive image restoration. arXiv e-prints pp</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">2102</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.06838</idno>
		<title level="m">Towards stabilizing batch statistics in backward propagation of batch normalization</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Restormer: Efficient transformer for high-resolution image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.09881</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning enriched features for real image restoration and enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hayat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="492" to="511" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
