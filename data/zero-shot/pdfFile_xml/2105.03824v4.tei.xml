<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FNet: Mixing Tokens with Fourier Transforms</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Lee-Thorp</surname></persName>
							<email>jamesleethorp@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
							<email>jainslie@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Eckstein</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Onta??n</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">FNet: Mixing Tokens with Fourier Transforms</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:49+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that "mix" input tokens. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the "efficient Transformers" on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts. 1 . 2020. Masked language modeling for proteins via linearly scalable long-context transformers. arXiv preprint arXiv:2006.03555.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Transformer architecture <ref type="bibr" target="#b32">(Vaswani et al., 2017)</ref> has achieved rapid and widespread dominance in NLP. At its heart is a attention mechanism -an inductive bias that connects each token in the input through a relevance weighted basis of every other token. Many papers have prodded and probed the Transformer, and in particular the attention sublayers, in an effort to better understand the architecture; see, for example, <ref type="bibr" target="#b29">Tenney et al. (2019)</ref>; <ref type="bibr" target="#b33">Vig and Belinkov (2019)</ref>; <ref type="bibr">Clark et al. (2019)</ref>; <ref type="bibr" target="#b34">Voita et al. (2019)</ref>. Although potentially limited in their effectiveness <ref type="bibr">(Hewitt and Liang, 2019)</ref>, these probes generally back the intuition that, by allowing higher order units to form out of compositions of the input, Transformer models can flexibly capture diverse syntactic and semantic relationships.</p><p>In this work, we investigate whether simpler token mixing mechanisms can wholly replace the relatively complex self-attention layers in Transformer encoder architectures. We first replace the attention sublayer with two parameterized matrix multiplications -one mixing the sequence dimension and one mixing the hidden dimension. Seeing promising results in this simple linear mixing scheme, we further investigate the efficacy of faster, structured linear transformations. Surprisingly, we find that the Fourier Transform, despite having no parameters at all, achieves nearly the same performance as dense linear mixing and scales very efficiently to long inputs, especially on GPUs (owing to the O(N log N ) Fast Fourier Transform (FFT) algorithm). We call the resulting model FNet.</p><p>While Fourier Transforms have previously been used to approximate or speed up computations in Convolutional Neural Networks <ref type="bibr">(El-Bakry and Zhao, 2004;</ref><ref type="bibr" target="#b6">Mathieu et al., 2014;</ref><ref type="bibr">Highlander and Rodriguez, 2015;</ref><ref type="bibr" target="#b13">Pratt et al., 2017;</ref><ref type="bibr">Chitsaz et al., 2020;</ref><ref type="bibr">Goldberg et al., 2020)</ref>, <ref type="bibr">Recurrent Neural Networks (Koplon and Sontag, 1997;</ref><ref type="bibr" target="#b41">Zhang and Chan, 2000;</ref><ref type="bibr" target="#b40">Zhang et al., 2018)</ref>, Transformers <ref type="bibr">(Choromanski et al., 2020;</ref><ref type="bibr" target="#b23">Tamkin et al., 2020)</ref>, and MLP layers more generally <ref type="bibr">(Cheng et al., 2015;</ref><ref type="bibr" target="#b9">Moczulski et al., 2016;</ref><ref type="bibr" target="#b22">Sindhwani et al., 2015)</ref>, we believe our work is the first to wholly replace particular neural network sublayers with a Fourier Transform. This approach of viewing the Fourier Transform as a first class mixing mechanism is reminiscent of the MLP-Mixer <ref type="bibr" target="#b30">(Tolstikhin et al., 2021)</ref> for vision, which replaces attention with MLPs; although in contrast to MLP-Mixer, FNet has no learnable parameters that mix along the spatial dimension.</p><p>Given the favorable asymptotic complexity of the FFT, our work also connects with the literature on "long sequence" or "efficient" Transformers, which aim to make the attention mechanism scale better via sparsity patterns <ref type="bibr">(Child et al., 2019;</ref><ref type="bibr" target="#b14">Qiu et al., 2020;</ref><ref type="bibr" target="#b11">Parmar et al., 2018;</ref><ref type="bibr" target="#b3">Beltagy et al., 2020;</ref><ref type="bibr" target="#b39">Zaheer et al., 2020;</ref><ref type="bibr">Tay et al., 2020b,a;</ref><ref type="bibr">Kitaev et al., 2020;</ref><ref type="bibr" target="#b35">Vyas et al., 2020;</ref> or via linearization of the attention matrix <ref type="bibr">(Katharopoulos et al., 2020;</ref><ref type="bibr">Choromanski et al., 2021;</ref><ref type="bibr" target="#b12">Peng et al., 2021)</ref>. As we will show in our experiments, while some of those works achieve O(N ) scaling of attention, this complexity often hides large constants, which make them less scalable in practice than FNet.</p><p>The contributions of our paper are:</p><p>? We show that simple linear transformations, including even (parameter-free) Fourier Transforms, along with standard MLPs in feedforward layers, are competent at modeling diverse relationships in text. That such a simple linear transformation works at all is surprising, and suggests that, for at least some NLP problems, attention may not be the principal component driving the performance of Transformers.</p><p>? We introduce a new model, FNet, that uses the Fourier Transform as a mixing mechanism. FNet offers an excellent compromise between speed, memory footprint, and accuracy, achieving 92% and 97%, respectively, of the accuracy of BERT-Base and BERT-Large (Devlin et al., 2019) on the GLUE benchmark , while training 80% faster on GPUs and 70% faster on TPUs.</p><p>? We find that FNet hybrid models containing only two self-attention sublayers achieve 97 ? 99% of their BERT counterparts' accuracy on GLUE, while still running 40 ? 70% faster. This indicates that, while attention can improve accuracy, it may not be necessary to use in every layer.</p><p>? We demonstrate FNet scales very well to long inputs and offers a better compromise between speed and accuracy than the efficient Transformers evaluated on the Long-Range Arena (LRA) benchmark <ref type="bibr" target="#b26">(Tay et al., 2021a)</ref>. Specifically, FNet achieves accuracy comparable to the most accurate efficient Transformer architectures but is significantly faster at both training and inference than all of the evaluated Transformer architectures across all sequence lengths on GPUs. On TPUs, FNet is faster for relatively shorter sequence lengths; for longer sequences, the only efficient Transformers that are faster than FNet on TPUs are less accurate on the LRA benchmark. Based on this, we argue that rather than seeking more efficient approximations of the attention, there may be more value in seeking out completely new mixing mechanisms.</p><p>2 Related work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Fourier Transforms in neural networks</head><p>Fourier analysis features heavily in studies of the universal approximation properties of neural networks; see, for example, <ref type="bibr">(Cybenko, 1989;</ref><ref type="bibr" target="#b2">Barron, 1993)</ref>. In terms of practical applications, discrete Fourier Transforms (DFT), and in particular the Fast Fourier Transform (FFT), have been used to tackle signal processing problems such as fitting neural networks to FFTs of electrocardiogram signals <ref type="bibr" target="#b7">(Minami et al., 1999;</ref><ref type="bibr">Gothwal et al., 2011;</ref><ref type="bibr" target="#b8">Mironovova and B?la, 2015)</ref> and vibration signals <ref type="bibr" target="#b42">(Zhang et al., 2013)</ref>, or to evolve solutions of Partial Differential Equations <ref type="bibr" target="#b10">(Li et al., 2021)</ref>. Because ordinary multiplication in the frequency domain corresponds to a convolution in the time domain, FFTs have been deployed in Convolutional Neural Networks to speed up computations, in Recurrent Neural Networks to speed up training and reduce exploding and vanishing gradients, and generally to approximate dense, linear layers to reduce computational complexity; see references cited in Section 1. DFTs have also been used indirectly in several Transformer works. The Performer <ref type="bibr">(Choromanski et al., 2020)</ref> linearizes the Transformer selfattention mechanism by leveraging random Fourier features to approximate a Gaussian representation of the softmax kernel. In our work, rather than approximating attention, we replace attention with the Fourier Transform, which acts as an alternate hidden representation mixing mechanism. <ref type="bibr" target="#b23">Tamkin et al. (2020)</ref> use spectral filters to generate hierarchical features, showing that the filtered embeddings perform well in different tasks (word-level, sentence-level or document-level), depending on which frequency scales are filtered. In contrast to FNet, they separate Fourier frequencies, rather than using the transform to combine features. Finally, through personal communication, we were alerted to concurrent, unpublished work <ref type="bibr" target="#b1">(Backurs et al., 2021)</ref> that describes an FFT based neural model that is very similar to FNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Modeling semantic relations via attention</head><p>Attention models have achieved state of the art results across virtually all NLP tasks and even some image tasks <ref type="bibr">(Dosovitskiy et al., 2021)</ref>. This success is generally attributed to the flexibility and capacity of attention. Although some works <ref type="bibr" target="#b17">(Ramsauer et al., 2021)</ref> have endeavoured to gain a deeper understanding of attention, the pervasive intuition is that the success of attention models derives from the token-dependent attention patterns in different layers; see, for example, <ref type="bibr" target="#b29">(Tenney et al., 2019)</ref>. However, it is natural to ask: Do we really need the flexibility, and associated cost, of attention? <ref type="bibr" target="#b24">Tay et al. (2020a)</ref> empirically investigated the importance of the dot product operation in the attention mechanism in their Synthesizer model (related to our "Linear" baseline below). They find that learnt token-dependent attention weights are highly expressive, but not necessarily crucial for realizing accurate NLP models. <ref type="bibr" target="#b38">You et al. (2020)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Efficient and long sequence models</head><p>The standard attention mechanism <ref type="bibr" target="#b32">(Vaswani et al., 2017</ref>) has a quadratic time and memory bottleneck with respect to sequence length. This limits its applicability in tasks involving long range dependencies. Most efforts to improve attention efficiency are based on sparsifying the attention matrix. <ref type="bibr" target="#b27">Tay et al. (2020c)</ref> survey many of the recent efficient attention works; see also citations in Section 1. Several "efficient Transformers" achieve O(N ? N ) or even O(N ) theoretical complexity. However, the constants hidden by this notation can be large. For example, in models such as Longformer <ref type="bibr" target="#b3">(Beltagy et al., 2020)</ref>, ETC , and Big-Bird <ref type="bibr" target="#b39">(Zaheer et al., 2020)</ref>, attention is O(N ) as a function of the input length, but quadratic in the number of "global tokens"; the latter must be sufficiently large to ensure good performance.</p><p>The Long-Range Arena benchmark <ref type="bibr" target="#b26">(Tay et al., 2021a)</ref> attempts to compare many of the efficient Transformers in a series of tasks requiring long range dependencies, finding that the Performer <ref type="bibr">(Choromanski et al., 2021</ref><ref type="bibr">), Linear Transformer (Katharopoulos et al., 2020</ref>, Linformer , and Image Transformer (Local Attention) <ref type="bibr" target="#b11">(Parmar et al., 2018)</ref> were the fastest on TPUs and had the lowest peak memory usages per device. 2 Instead, in this paper we completely replace self-attention with a different mixing, namely the Fourier Transform, which offers: (1) performance, (2) reduced model size (no learnable parameters), and (3) simplicity.</p><p>Finally, we note that, in an effort to investigate different token mixing mechanisms, we compare a vanilla BERT model (Devlin et al., 2019) with a vanilla FNet, ignoring more recent Transformer optimizations, which we consider orthogonal to this work; see, for example, <ref type="bibr" target="#b10">(Narang et al., 2021;</ref><ref type="bibr">Kim and Hassan, 2020;</ref><ref type="bibr" target="#b21">Shleifer and Rush, 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Discrete Fourier Transform</head><p>The Fourier Transform decomposes a function into its constituent frequencies. Given a sequence {x n } with n ? [0, N ? 1], the discrete Fourier Transform (DFT) is defined by the formula:</p><formula xml:id="formula_0">X k = N ?1 n=0 x n e ? 2?i N nk , 0 ? k ? N ? 1. (1)</formula><p>For each k, the DFT generates a new representation X k as a sum of all of the original input tokens x n , with so-called "twiddle factors". There are two primary approaches to computing the DFT: the Fast Fourier Transform (FFT) and matrix multiplication. The standard FFT algorithm is the Cooley-Tukey algorithm <ref type="bibr">(Cooley and Tukey, 1965;</ref><ref type="bibr">Frigo and Johnson, 2005)</ref>, which recursively re-expresses the DFT of a sequence of length N = N 1 N 2 in terms of N 1 smaller DFTs of sizes N 2 to reduce the computation time to O(N log N ). An alternative approach is to simply apply the DFT matrix to the input sequence. The DFT matrix, W , is a Vandermonde matrix for the roots of unity up to a normalization factor:</p><formula xml:id="formula_1">W nk = e ? 2?i N nk / ? N ,<label>(2)</label></formula><p>where n, k = 0, . . . , N ? 1. This matrix multiplication is an O(N 2 ) operation, which has higher asymptotic complexity than the FFT, but turns out to be faster for relatively shorter sequences on TPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">FNet architecture</head><p>FNet is an attention-free Transformer architecture, wherein each layer consists of a Fourier mixing sublayer followed by a feed-forward sublayer. The architecture is shown in <ref type="figure" target="#fig_1">Figure 1</ref>. Essentially, we replace the self-attention sublayer of each Transformer encoder layer with a Fourier sublayer, which applies a 2D DFT to its (sequence length, hidden dimension) embedding input -one 1D DFT along the sequence dimension, F seq , and one 1D DFT along the hidden dimension, F h : 3</p><formula xml:id="formula_2">y = F seq (F h (x)) .<label>(3)</label></formula><p>As indicated by Equation <ref type="formula" target="#formula_2">(3)</ref>, we only keep the real part of the result; hence, we do not need to modify the (nonlinear) feed-forward sublayers or output layers to handle complex numbers. We found that FNet obtained the best results when the real part of the total transformation was only extracted at the end of the Fourier sublayer; that is, after applying both F seq and F h . We also experimented with the Hadamard, Hartley and Discrete Cosine Transforms. Of these three, the Hartley Transform was the strongest alternative, obtaining comparable accuracy to Equation <ref type="formula" target="#formula_2">(3)</ref>; see Appendix A.3 for details. The simplest interpretation for the Fourier Transform is as a particularly effective mechanism for mixing tokens, which provides the feed-forward sublayers sufficient access to all tokens. Because of the duality of the Fourier Transform, we can also view each alternating encoder block as applying alternating Fourier and inverse Fourier Transforms, transforming the input back and forth between the "time" and frequency domain. Because multiplying by the feed-forward sublayer coefficients in the frequency domain is equivalent to convolving (with a related set of coefficients) in the time domain, FNet can be thought of as alternating between multiplications and convolutions. <ref type="bibr">4</ref> We use the same embedding layers as in Devlin et al. <ref type="formula" target="#formula_1">(2019)</ref>; namely, we combine the word embeddings, absolute position embeddings of the tokens and type embeddings of the sentences. Because of the positional information encoded by the Fourier Transform in Equation (1) (see n, k indices), FNet performs just as well without position embeddings. Nevertheless, we include the position embeddings to allow for a cleaner comparison with BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Implementation</head><p>Empirically, we found that on GPUs: the FFT is faster than matrix multiplications for all sequence lengths we consider (512 ? 8192 tokens), whereas on TPUs: for relatively shorter sequences (? 4096 tokens), it is faster to cache the DFT matrix and then compute the DFT through matrix multiplications than using the FFT; for longer sequences, the FFT is faster. As a result, our GPU FNet implementation always uses the FFT, while our TPU implementation computes the 2D DFT using matrix multiplications for sequences up to lengths of 4096 and the FFT for longer lengths. Presumably the GPU vs TPU difference is primarily a result of two factors: (1) TPUs are even more highly optimized for matrix multiplications than GPUs, and (2) GPUs offer a more efficient FFT implementa- </p><formula xml:id="formula_3">d h + 4nd 2 h 112M 339M Linear n 2 d h + nd 2 h 94M 269M FNet (mat) n 2 d h + nd 2 h 83M 238M FNet (FFT) nd h log(n)+ 83M 238M nd h log(d h ) Random n 2 d h + nd 2 h 83M 238M FF-only 0 83M 238M</formula><p>tion than TPUs. We suspect that FNet will only become more performant on TPUs as the TPU implementation of the FFT improves. Our model uses JAX and, in particular, the Flax framework 5 . Core model code is given in Appendix A.7 and the full source core is available online. 6 4 Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Transfer learning</head><p>We compare FNet and Transformer architectures in a common transfer learning setting. For a fuller picture, we compare multiple models (see <ref type="table" target="#tab_0">Table 1</ref> for parameter counts in "Base" configuration):</p><p>? BERT-Base: a Transformer encoder model.</p><p>? FNet encoder: we replace every self-attention sublayer with a Fourier sublayer.</p><p>? Linear encoder: we replace each self-attention sublayer with a two learnable, dense, linear sublayers, one applied to the hidden dimension and one to the sequence dimension.</p><p>? Random encoder: we replace each selfattention sublayer with a two constant random matrices, one applied to the hidden dimension and one applied to the sequence dimension.</p><p>? Feed Forward-only (FF-only) encoder: we completely remove the self-attention sublayer; so that this model has no token mixing.</p><p>Despite its simplicity, the Linear baseline turns out to be surprisingly accurate and fast. Our Linear model is similar to the MLP-Mixer (Tolstikhin et al., 2021) (for vision) and also the Random Synthesizer <ref type="bibr" target="#b24">(Tay et al., 2020a)</ref>, but simplifies the latter model further by removing the multiple heads and softmax projections, resulting in just two matrix multiplications in the mixing sublayer.</p><p>It is reasonable to expect that the Linear encoder, which uses densely parameterized mixing layers, will learn more flexibly than FNet, which uses parameter-free mixing layers. As we will show, although the Linear-Base model outperforms FNet-Base slightly on GLUE (0.3 points), it has several efficiency drawbacks relative to FNet: it has a much larger memory footprint (see <ref type="table" target="#tab_3">Table 4b</ref>), it is slower to train on regular 512 sequence lengths (see <ref type="table" target="#tab_2">Table  3</ref>), and scales significantly worse on long sequence lengths (see <ref type="table" target="#tab_3">Tables 4b-4c</ref>). <ref type="bibr">7</ref> We also found that Linear-Large was more difficult to train due to gradient blow up (see "Large" scores in <ref type="table" target="#tab_1">Table 2</ref>).</p><p>We adopt the same fixed "Base" and "Large" model and training configurations as for the original <ref type="bibr">BERT (Devlin et al., 2019)</ref>, except that we pretrain on the much larger C4 dataset <ref type="bibr" target="#b15">(Raffel et al., 2020)</ref> and use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) (see Appendix A.1 for full pre-training details). For fine-tuning on the GLUE benchmark , we found that different BERT runs with the same base learning rate could yield slightly different results. Consequently, for the Base (Large) models, we performed 3 (6) trials, respectively, for each base learning rate and reported the best result across all experiments. This reflects our observation that BERT-Large was less stable than BERT-Base, as noted in <ref type="bibr">Devlin et al. (2019)</ref>.</p><p>We report the results for the best base learning rate (no early stopping) on the GLUE Validation split in <ref type="table" target="#tab_1">Table 2</ref>. 8 For Base models, results mirror the pre-training metrics (see Appendix A.1): BERT performs best. FNet and the Linear model both underperform BERT by 7.5 ? 8%. Referring to <ref type="table" target="#tab_2">Table 3</ref>, we see that although less accurate, FNet trains significantly faster than BERT -80% faster on GPUs and 70% faster on TPUs -and performs  63% of BERT's FLOPS. Measured in isolation, the Fourier sublayers perform forward and backward passes an order of magnitude faster than the self-attention sublayers (see Appendix A.4), but FNet's overall training speed is impeded by the feed-forward sublayers that all models share.</p><p>Returning to <ref type="table" target="#tab_1">Table 2</ref>: the FF-only model severely underperforms all other models: as expected, token mixing is critical to the expressivity of the model. For example, 50% accuracy scores on the binary classification tasks (QNLI, SST-2, RTE), indicate that the model fails to learn the tasks. The weak accuracy of the Random model suggests that not just any mixing will do; rather, a structured mixing is required. We also include metrics from a hybrid FNet attention model. In the hybrid model, we replace the final two Fourier sublayers of FNet with self-attention sublayers -other configurations are possible, but we generally found that replacing the final layers worked best; see Appendix A.5. With the addition of just two self-attention sublayers, the hybrid FNet models achieve 97% and 99% of their respective BERT counterpart's accuracies with only limited speed degradations (see <ref type="table" target="#tab_2">Table 3</ref>).</p><p>Interestingly, the gap between BERT and FNet shrinks to just 3% for Large models; this is likely due to FNet-Large being more stable during training than BERT-Large. <ref type="bibr">9</ref> The Linear-Large model severely underperforms its Base counterpart on GLUE benchmark due to training instabilities. We generally found that the Linear model and BERT were less stable than the models with no param- eters in their mixing sublayers, namely the FNet, Random and FF-only models.</p><p>The speed vs MLM accuracy curve for GPU (8 V100 chips) pre-training is shown in <ref type="figure" target="#fig_2">Figure 2</ref> (see Appendix A.2 for TPU results). Both TPU and GPU models are trained for 1 million steps as in <ref type="bibr">Devlin et al. (2019)</ref>. Motivated by the models considered in <ref type="bibr" target="#b31">Turc et al. (2019)</ref>, we evaluated several model sizes; see <ref type="table">Table 6</ref> in Appendix A.1. We found that the smaller model architectures benefited from larger learning rates, so we select the best result using 10 ?3 and 10 ?4 for all models. <ref type="bibr">10</ref> The GPU <ref type="figure" target="#fig_2">(Figure 2</ref>), and TPU <ref type="figure" target="#fig_3">(Figure 3</ref> in Appendix A.2) results display the same trends. For larger, slower models, BERT and FNet-Hybrid define the Pareto speed-accuracy efficiency frontier. For smaller, faster models, FNet and the Linear model define the efficiency frontier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Long-Range Arena (LRA) benchmark</head><p>Of the efficient Transformers evaluated on LRA benchmark by <ref type="bibr" target="#b26">Tay et al. (2021a)</ref>, their results suggest that (1) the vanilla Transformer is (by a small margin) the second most accurate model, and (2) the Performer (Choromanski et al., 2021) is the fastest model. We benchmark FNet's accuracy against both of these models using Tay et al. <ref type="bibr">10</ref> We have opted to compare FNet with Transformer models as the latter are the most commonly used models in NLP transfer learning settings. It would also be interesting to compare FNet with convolutional-based models, although, to our knowledge, such models have only recently found limited success in pre-training NLP setups <ref type="bibr">(Tay et al., 2021b)</ref>; and even there, the authors did not consider the small model regime.</p><p>(2021a)'s codebase and running on the same hardware (4 ? 4 TPU v3 chips); the results are shown in <ref type="table" target="#tab_3">Table 4a</ref>. <ref type="bibr">11</ref> To ensure a fair comparison, we also report the results of our own experiments for the vanilla Transformer (see Appendix A.6 for details). <ref type="table" target="#tab_3">Table 4a</ref> suggests that, in aggregate, the (vanilla) Transformer and FNet obtain comparable results. Given that the Transformer is the second most accurate model evaluated by <ref type="bibr" target="#b26">Tay et al. (2021a)</ref> and that the relative differences in the average accuracy scores within <ref type="table" target="#tab_3">Table 4a</ref> are small, our results suggest that FNet is competitive with the most accurate of the efficient Transformers on LRA.</p><p>Turning to efficiency, in <ref type="table" target="#tab_3">Table 4b</ref>, we provide training speed and memory usage statistics from our experiments on GPUs (8 V100 chips); see Appendix A.2 for results on TPUs. We perform a sweep over sequence lengths {512, 1024, 2048, 4096, 8192}. On GPUs, FNet is much faster than all other models across all sequence lengths, due to the highly efficient FFT implementation on GPUs. <ref type="table" target="#tab_3">Table 4b</ref> also indicates that FNet has a lighter memory footprint (this holds for both GPUs and TPUs; see extended results in Appendix A.2). This is partly because FNet has no learnable parameters in its mixing sublayer, but also due to the FFT's efficiency, especially at longer sequence lengths. Lastly, <ref type="table" target="#tab_3">Table 4c</ref> shows that training speed gains generally carry over to inference gains (see Appendix A.2 for detailed TPU results). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this work, we studied simplified token mixing modules for Transformer-like encoder architectures, making several contributions. First, we showed that simple, linear mixing transformations, along with the nonlinearities in feed-forward layers, can competently model diverse semantic relationships in text. Second, we introduced FNet, a Transformer-like model wherein the self-attention sublayer is replaced by an unparameterized Fourier Transform. FNets achieve 92 and 97% of their respective BERT-Base and BERT-Large counterparts' accuracy on the GLUE benchmark, but train 70 ? 80% faster on GPUs/TPUs. Third, because of its favorable scaling properties, FNet is very competitive with the "efficient Transformers" evaluated on the Long-Range Arena benchmark, matching the accuracy of the most accurate models while being much faster and lighter on memory.</p><p>Our work highlights the potential of linear units as a drop-in replacement for the attention mechanism in text classification tasks. We found the Fourier Transform to be a particularly efficient and effective mixing mechanism, due to the speed of the FFT. However, we only performed a cursory survey of other linear transformations (see also Appendix A.3), and additional fast alternatives are worth exploring.</p><p>Given the speed and accuracy advantages of smaller FNet models relative to Transformers, we suspect that FNet will be effective as a lightweight, distilled student model deployed in resourceconstrained settings such as production services or on edge devices. The need for such lightweight serving models is only forecast to grow given the interest in giant models <ref type="bibr" target="#b15">(Raffel et al., 2020;</ref><ref type="bibr">Brown et al., 2020;</ref><ref type="bibr">Lepikhin et al., 2021)</ref>. A natural avenue to explore in this regard is knowledge distillation of small FNet models from larger Transformer teacher models, following, for example, <ref type="bibr" target="#b19">Sanh et al. (2019)</ref>; Jiao et al. (2020); <ref type="bibr" target="#b31">Turc et al. (2019)</ref>.</p><p>Another aspect of interest and worthy of further study is hybrid FNet-attention models. We found that adding only a few self-attention sublayers to FNet offers a simple way to trade speed for accuracy. Specifically, replacing the final two Fourier sublayers with self-attention provided 97 ? 99% of BERT's accuracy with limited speed penalties.</p><p>Throughout this work we have restricted our focus to encoders. FNet decoders can be designed by "causally" masking the Vandermonde matrix, but a lower level implementation is required to introduce causal masking to FFTs. How to adapt Fourier mixing for encoder-decoder cross-attention is an open question as evidence suggests that crossattention may be crucial to performance <ref type="bibr" target="#b38">(You et al., 2020)</ref>. We have focused on tasks which do not require generation so we leave FNet decoders and encoder-decoder setups to future work; although we do remark that the FNet encoder could be used as a drop in replacement in a Transformer as other works have successfully demonstrated; see, for example, <ref type="bibr" target="#b39">(Zaheer et al., 2020;</ref><ref type="bibr">Guo et al., 2021)</ref>.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Pre-training details</head><p>We adopt the same fixed "Base" and "Large" model and learning configurations as for the original <ref type="bibr">BERT (Devlin et al., 2019)</ref>. We train on the much larger C4 dataset <ref type="bibr" target="#b15">(Raffel et al., 2020)</ref> and use a 32000 SentencePiece vocabulary model <ref type="bibr">(Kudo and Richardson, 2018)</ref> trained on a 100 million sentence subset of C4. Our TPU experiments use a batch size of 256 as in <ref type="bibr">Devlin et al. (2019)</ref> and are each run on 4 ? 4 TPU v3 chips. Our GPU experiments use a smaller batch size of 64 and are run on 8 V100 chips. Because the training configuration is lifted from <ref type="bibr">Devlin et al. (2019)</ref>, it may be slightly biased towards the BERT attention model. <ref type="table" target="#tab_5">Table 5</ref> summarizes the pre-training metrics for the different models; the pre-training speeds are shown in <ref type="table" target="#tab_2">Table 3</ref> in the main text. Although they have weaker accuracy metrics, the Linear model and FNet train nearly 80% faster than BERT on GPUs, and 70% faster on TPUs (see <ref type="table" target="#tab_2">Table 3</ref>). We also find that the three models with no learnable parameters in their mixing layer, namely FNet, the Random model and the FF-only model, are the most stable during training.</p><p>BERT's higher accuracy on the MLM pretraining task is not simply a result of having more parameters than the other models. Indeed, <ref type="table" target="#tab_5">Table 5</ref> shows that BERT-Base is actually more accurate than FNet-Large, which contains more than twice as many parameters. BERT is presumably more expressive because the mixing (attention) weights are both task specific and token dependent, determined <ref type="table">Table 6</ref>: Pre-training model sizes (ignoring output projection layers). As in <ref type="bibr" target="#b31">Turc et al. (2019)</ref>, for all models, we fix the feed-forward size to 4d h and the number of self-attention heads to d h /64. Smaller architectures have a similar number of parameters across all models because the majority of parameters are in the embedding layers. Each FNet-Hybrid ("FNet-H") model contains 2 self-attention sublayers. We exclude FNet-Hybrid models with only 2 total layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dimensions</head><p>Parameters <ref type="formula">(</ref>  <ref type="bibr" target="#b24">Tay et al. (2020a)</ref>. FNet's mixing weights, on the other hand, are neither task specific nor token dependent. Finally, <ref type="table">Table 6</ref> shows the model sizes that were used to construct <ref type="figure" target="#fig_2">Figure 2</ref> (main text) and <ref type="figure" target="#fig_3">Figure 3</ref> (Appendix A.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 TPU results</head><p>In this section, we report FNet efficiency results for TPUs; the main text focuses on GPUs. <ref type="figure" target="#fig_3">Figure  3</ref> shows the speed vs MLM pre-training accuracy curve when training on TPU (4 ? 4 v3 chips). As on GPUs, FNet and the Linear model define the Pareto efficiency frontier for smaller, faster models, while BERT defines the frontier for larger, slower models. <ref type="table" target="#tab_7">Table 7</ref> shows Long Range Arena Text classification efficiency results on TPUs (4 ? 4 v3 chips). The Linear model and FNet train faster than all the efficient Transformers for sequence lengths ? 2048 and 512, respectively. For longer sequences, FNet is slower than the Performer and, based on results in <ref type="bibr" target="#b26">Tay et al. (2021a)</ref>, likely also slower than the other efficient Transformers that linearize attention, namely Local Attention <ref type="bibr" target="#b11">(Parmar et al., 2018)</ref>, Linformer  and Linear Transformer <ref type="bibr">(Katharopoulos et al., 2020)</ref>. However, it is worth noting that <ref type="table" target="#tab_3">Table 4a</ref> suggests that FNet is more accurate than all of the aforementioned models. Moreover, we expect that the GPU speed gains will   mimics convolutions. However, these setups degraded accuracy and lead to a more unstable model during training. Adding extra feed-forward sublayers to this layering, or swapping out the feedforward sublayers for simpler dense sublayers, did not help either.</p><p>A.4 Mixing layer speeds <ref type="table" target="#tab_8">Table 8</ref> summarizes the inference and training speeds for the different mixing layers. For each of the Base and Large configurations, we have removed all other sublayers and transformations and then calculated the speed per batch of input examples. The FNet training speeds are particularly fast because no parameters are updated. The Linear model has faster inference than FNet on TPUs because it is performing real matrix multiplications, whereas FNet performs complex matrix multiplications; see Equation <ref type="formula" target="#formula_1">(2)</ref>. Although the Fourier mixing sublayer itself performs forward and backward passes significantly faster than the self-attention sublayer, FNet is overall 70-80% faster than BERT because the overall training and inference speeds are bottle-necked by the feed-forward sublayers that all models share. <ref type="table" target="#tab_9">Table 9</ref> shows the effects of varying the number of attention sublayers and the attention layout in the FNet-Hybrid model. For the "BOTTOM" layout, all attention sublayers are placed in the first few encoder layers, where they replace the Fourier mixing sublayers. For the "TOP" layout, attention sublayers are placed in the final encoder layers; for the "MIDDLE" layout they are placed in the middle layers; and for the "MIXED" layout, they are distributed through the model.  <ref type="table" target="#tab_9">Table 9</ref>, we can make two observations: (1) more attention improves accuracy at the cost of speed, and ultimately with diminishing returns;</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 FNet-Hybrid ablations</head><p>(2) placing attention layers at the top of the model gives the best accuracy results. Given our focus on speed, we chose to focus FNet-Hybrid experiments in the main text of the paper on the 2 attention layer, "TOP" configuration variant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 A note on Long-Range Arena hyperparameter settings</head><p>Concerning the Long-Range Arena setup, several hyperparameters are not described in <ref type="bibr" target="#b26">Tay et al. (2021a)</ref> and there a few mismatches between the configurations described in the paper and the code repository. Where possible, we prioritize configurations described in the paper with only two exceptions. Firstly, for the CIFAR10 (Image) task, we perform a sweep of the number of layers in the range <ref type="bibr">[1,</ref><ref type="bibr">2,</ref><ref type="bibr">3,</ref><ref type="bibr">4]</ref>. We found that 1 layer worked best for all models; <ref type="bibr" target="#b26">Tay et al. (2021a)</ref> suggest 3 layers yielded the best results. Secondly, for the Pathfinder task, we found that a base learning rate of 0.001 (as given in the code repository) yielded better results for all models than the 0.01 value indicated in <ref type="bibr" target="#b26">Tay et al. (2021a)</ref>. We also perform a very small sweep over the embedding dimension and batch size, which are not listed in <ref type="bibr" target="#b26">Tay et al. (2021a)</ref>. We also remark that the accuracy comparisons between our runs and those from <ref type="bibr" target="#b26">Tay et al. (2021a)</ref> should be performed with the caveat that we found that results for certain tasks -Text and Retrieval in particular -can vary quite a bit between runs, especially for the Transformer; we report the best results.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>replace attention weights in the Transformer encoder and decoder with unparameterized Gaussian distributions, showing minimal performance degradation provided they retain learnable cross-attention weights. Similarly, Raganato et al. (2020) find little to no accuracy degradation when replacing all but one of the attention heads of each attention layer in the encoder with fixed, non-learnable positional patterns. Finally, Tolstikhin et al. (2021) present MLP-Mixer, where attention is replaced by MLPs, with limited performance degradation in image classification tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>FNet architecture with N encoder blocks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Speed-accuracy trade-offs for GPU pre-training. The dashed line shows the Pareto efficiency frontier, indicating the best trade-offs. For smaller models (faster training speeds; left-hand side of figure), the FNet (yellow squares) and Linear (red triangles) models define the frontier, while for larger models (slower training speeds; righthand side of figure), BERT (blue circles) and FNet-Hybrid (green stars) define the frontier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Speed-accuracy trade-offs for TPU pre-training. The dashed line shows the Pareto efficiency frontier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Number of mixing layer operations (forward pass) and learnable parameters, excluding any task specific output projection layers. n is the sequence length and d h is the model hidden dimension. The mixing layer operations are given on a per layer basis.</figDesc><table><row><cell></cell><cell cols="2">Mixing layer ops Model params</cell></row><row><cell>Model</cell><cell>(per layer)</cell><cell>Base Large</cell></row><row><cell>BERT</cell><cell>2n 2</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>GLUE Validation results on TPUs, after finetuning on respective tasks. We report the mean of accuracy and F1 scores for QQP and MRPC, Spearman correlations for STS-B and accuracy scores for all other tasks. The MNLI metrics are reported by the match/mismatch splits. Average scores exclude any failure cases. After controlling for batch size and training steps, the GPU metrics (not shown) are similar.</figDesc><table><row><cell>Model</cell><cell cols="9">MNLI QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg.</cell></row><row><cell>BERT-Base</cell><cell>84/81</cell><cell>87</cell><cell>91</cell><cell>93</cell><cell>73</cell><cell>89</cell><cell>83</cell><cell>69</cell><cell>83.3</cell></row><row><cell>Linear-Base</cell><cell>74/75</cell><cell>84</cell><cell>80</cell><cell>94</cell><cell>67</cell><cell>67</cell><cell>83</cell><cell>69</cell><cell>77.0</cell></row><row><cell>FNet-Base</cell><cell>72/73</cell><cell>83</cell><cell>80</cell><cell>95</cell><cell>69</cell><cell>79</cell><cell>76</cell><cell>63</cell><cell>76.7</cell></row><row><cell>Random-Base</cell><cell>51/50</cell><cell>70</cell><cell>61</cell><cell>76</cell><cell>67</cell><cell>4</cell><cell>73</cell><cell>57</cell><cell>56.6</cell></row><row><cell>FF-only-Base</cell><cell>34/35</cell><cell>31</cell><cell>52</cell><cell>48</cell><cell>67</cell><cell>FAIL</cell><cell>73</cell><cell>54</cell><cell>49.3</cell></row><row><cell>FNet-Hybrid-Base</cell><cell>78/79</cell><cell>85</cell><cell>88</cell><cell>94</cell><cell>76</cell><cell>86</cell><cell>79</cell><cell>60</cell><cell>80.6</cell></row><row><cell>BERT-Large</cell><cell>88/88</cell><cell>88</cell><cell>92</cell><cell>95</cell><cell>71</cell><cell>88</cell><cell>86</cell><cell>66</cell><cell>84.7</cell></row><row><cell>Linear-Large</cell><cell>35/36</cell><cell>84</cell><cell>80</cell><cell>79</cell><cell>67</cell><cell>24</cell><cell>73</cell><cell>60</cell><cell>59.8</cell></row><row><cell>FNet-Large</cell><cell>78/76</cell><cell>85</cell><cell>85</cell><cell>94</cell><cell>78</cell><cell>84</cell><cell>88</cell><cell>69</cell><cell>81.9</cell></row><row><cell cols="2">FNet-Hybrid-Large 79/80</cell><cell>87</cell><cell>89</cell><cell>92</cell><cell>81</cell><cell>88</cell><cell>86</cell><cell>70</cell><cell>83.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table><row><cell></cell><cell cols="2">Pre-training</cell><cell cols="2">Inference</cell><cell>GFLOPS</cell></row><row><cell>Model</cell><cell>GPU</cell><cell>TPU</cell><cell>GPU</cell><cell>TPU</cell><cell>/example</cell></row><row><cell>BERT-Base</cell><cell>305</cell><cell>213</cell><cell>82</cell><cell>32</cell><cell>98</cell></row><row><cell>Linear-Base</cell><cell cols="3">199 (1.5x) 149 (1.4x) 52 (1.6x)</cell><cell>20 (1.6x)</cell><cell>71 (73%)</cell></row><row><cell>FNet-Base</cell><cell cols="3">169 (1.8x) 128 (1.7x) 46 (1.8x)</cell><cell>23 (1.4x)</cell><cell>62 (63%)</cell></row><row><cell>Random-Base</cell><cell cols="3">182 (1.7x) 130 (1.6x) 52 (1.6x)</cell><cell>22 (1.4x)</cell><cell>71 (73%)</cell></row><row><cell>FF-only-Base</cell><cell cols="3">162 (1.9x) 118 (1.8x) 43 (1.9x)</cell><cell>16 (2.0x)</cell><cell>59 (60%)</cell></row><row><cell cols="4">FNet-Hybrid-Base 198 (1.5x) 149 (1.4x) 51 (1.6x)</cell><cell>24 (1.3x)</cell><cell>68 (69%)</cell></row><row><cell>BERT-Large</cell><cell>OOM</cell><cell>503</cell><cell>263</cell><cell>111</cell><cell>337</cell></row><row><cell>Linear-Large</cell><cell>592</cell><cell cols="4">397 (1.3x) 170 (1.5x) 108 (1.0x) 247 (73%)</cell></row><row><cell>FNet-Large</cell><cell>511</cell><cell cols="4">275 (1.8x) 149 (1.8x) 82 (1.4x) 217 (64%)</cell></row><row><cell>FNet-Hybrid-Large</cell><cell>541</cell><cell cols="4">294 (1.7x) 157 (1.7x) 84 (1.3x) 227 (67%)</cell></row></table><note>Pre-training and inference speeds in milliseconds per batch of 64 examples on GPU (8 V100 chips) and 256 examples on TPU (4 ? 4 v3 chips), alongside GFLOPS for a forward pass of a single example. Speed-up multipliers relative to BERT are given in parentheses.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Accuracy, inference speed and memory usage results on the Long-Range Arena (LRA) benchmark. GPU training for sequence lengths up to 8192. Only the fastest efficient Transformer, namely Performer, from Tay et al. (2021a) is shown. Left: training speeds (in steps per second; larger is better), with speed-up multipliers relative to the Transformer given in parentheses. Right: peak memory usage (in GB; smaller is better).</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="6">ListOps Text Retrieval Image Pathfinder Path-X Avg.</cell></row><row><cell cols="2">Transformer (ours)</cell><cell>36.06</cell><cell>61.54</cell><cell>59.67</cell><cell>41.51</cell><cell cols="2">80.38</cell><cell>OOM 55.83</cell></row><row><cell cols="2">Linear (ours)</cell><cell>33.75</cell><cell>53.35</cell><cell>58.95</cell><cell>41.04</cell><cell cols="2">83.69</cell><cell>FAIL 54.16</cell></row><row><cell>FNet (ours)</cell><cell></cell><cell>35.33</cell><cell>65.11</cell><cell>59.61</cell><cell>38.67</cell><cell cols="2">77.80</cell><cell>FAIL 55.30</cell></row><row><cell cols="2">Transformer (*)</cell><cell>36.37</cell><cell>64.27</cell><cell>57.46</cell><cell>42.44</cell><cell cols="2">71.40</cell><cell>FAIL 54.39</cell></row><row><cell cols="2">Local Attention (*)</cell><cell>15.82</cell><cell>52.98</cell><cell>53.39</cell><cell>41.46</cell><cell cols="2">66.63</cell><cell>FAIL 46.06</cell></row><row><cell cols="2">Sparse Trans. (*)</cell><cell>17.07</cell><cell>63.58</cell><cell>59.59</cell><cell>44.24</cell><cell cols="2">71.71</cell><cell>FAIL 51.24</cell></row><row><cell cols="2">Longformer (*)</cell><cell>35.63</cell><cell>62.85</cell><cell>56.89</cell><cell>42.22</cell><cell cols="2">69.71</cell><cell>FAIL 53.46</cell></row><row><cell cols="2">Linformer (*)</cell><cell>35.70</cell><cell>53.94</cell><cell>52.27</cell><cell>38.56</cell><cell cols="2">76.34</cell><cell>FAIL 51.36</cell></row><row><cell cols="2">Reformer (*)</cell><cell>37.27</cell><cell>56.10</cell><cell>53.40</cell><cell>38.07</cell><cell cols="2">68.50</cell><cell>FAIL 50.67</cell></row><row><cell cols="2">Sinkhorn Trans. (*)</cell><cell>33.67</cell><cell>61.20</cell><cell>53.83</cell><cell>41.23</cell><cell cols="2">67.45</cell><cell>FAIL 51.39</cell></row><row><cell cols="2">Synthesizer (*)</cell><cell>36.99</cell><cell>61.68</cell><cell>64.67</cell><cell>41.61</cell><cell cols="2">69.45</cell><cell>FAIL 52.88</cell></row><row><cell>BigBird (*)</cell><cell></cell><cell>36.05</cell><cell>64.02</cell><cell>59.29</cell><cell>40.83</cell><cell cols="2">74.87</cell><cell>FAIL 55.01</cell></row><row><cell cols="2">Linear Trans. (*)</cell><cell>16.13</cell><cell>65.90</cell><cell>53.09</cell><cell>42.34</cell><cell cols="2">75.30</cell><cell>FAIL 50.55</cell></row><row><cell cols="2">Performer (*)</cell><cell>18.01</cell><cell>65.40</cell><cell>53.82</cell><cell>42.77</cell><cell cols="2">77.05</cell><cell>FAIL 51.41</cell></row><row><cell></cell><cell></cell><cell cols="3">Training Speed (steps/s)</cell><cell></cell><cell cols="2">Peak Memory Usage (GB)</cell></row><row><cell>Seq. length</cell><cell>512</cell><cell>1024</cell><cell>2048</cell><cell cols="4">4096 8192 512 1024 2048 4096 8192</cell></row><row><cell>Transformer</cell><cell>21</cell><cell>10</cell><cell>4</cell><cell cols="4">OOM OOM 1.6 4.0 12.2 OOM OOM</cell></row><row><cell>Linear</cell><cell cols="3">34 (1.6x) 19 (1.8x) 9 (2.0x)</cell><cell>4</cell><cell cols="3">OOM 0.9 1.6 2.8</cell><cell>6.9 OOM</cell></row><row><cell cols="4">FNet (FFT) 43 (2.0x) 24 (2.3x) 14 (3.2x)</cell><cell>7</cell><cell>4</cell><cell cols="2">0.8 1.3 2.2</cell><cell>3.9</cell><cell>7.4</cell></row><row><cell>Performer</cell><cell cols="3">28 (1.3x) 15 (1.5x) 9 (1.9x)</cell><cell>4</cell><cell>2</cell><cell cols="2">1.1 1.9 3.1</cell><cell>5.5</cell><cell>10.4</cell></row><row><cell cols="2">(b) Seq. length</cell><cell>512</cell><cell>1024</cell><cell cols="2">2048</cell><cell>4096</cell><cell>8192 16384</cell></row><row><cell cols="2">Transformer</cell><cell>12</cell><cell>28</cell><cell>76</cell><cell></cell><cell>244</cell><cell>OOM OOM</cell></row><row><cell cols="2">Linear</cell><cell cols="5">9 (1.4x) 14 (2.0x) 30 (2.6x) 72 (3.4x)</cell><cell>208</cell><cell>OOM</cell></row><row><cell cols="2">FNet (FFT)</cell><cell cols="5">8 (1.5x) 12 (2.3x) 23 (3.4x) 43 (5.7x)</cell><cell>83</cell><cell>164</cell></row><row><cell cols="2">Performer</cell><cell cols="5">11 (1.2x) 17 (1.6x) 32 (2.4x) 60 (4.0x)</cell><cell>116</cell><cell>238</cell></row></table><note>(a) Accuracy results obtained on TPUs as in Tay et al. (2021a). Asterisked results quoted from Tay et al. (2021a). Average does not include the Path-X task, which all models fail (Transformer due to memory limits; others perform no better than chance).(c) GPU inference speeds on the LRA Text classification task (in milliseconds per batch; smaller is better). Only the fastest efficient Transformer, Performer, from Tay et al. (2021a) is shown. Speed up relative to the Transformer is given in parentheses.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. 2020. Data movement is all you need: A case study of transformer networks. arXiv preprint arXiv:2007.00072.</figDesc><table><row><cell></cell><cell>Andrei Ivanov, Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,</cell></row><row><cell></cell><cell>Xiao Chen, Linlin Li, Fang Wang, and Qun Liu.</cell></row><row><cell></cell><cell>2020. TinyBERT: Distilling BERT for natural lan-</cell></row><row><cell></cell><cell>guage understanding. In Findings of the Association</cell></row><row><cell></cell><cell>for Computational Linguistics: EMNLP 2020, pages</cell></row><row><cell></cell><cell>4163-4174. Association for Computational Linguis-</cell></row><row><cell></cell><cell>tics.</cell></row><row><cell></cell><cell>Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-</cell></row><row><cell></cell><cell>pas, and Fran?ois Fleuret. 2020. Transformers are</cell></row><row><cell></cell><cell>rnns: Fast autoregressive transformers with linear</cell></row><row><cell></cell><cell>attention. In International Conference on Machine</cell></row><row><cell></cell><cell>Learning, pages 5156-5165. PMLR.</cell></row><row><cell></cell><cell>Young Jin Kim and Hany Hassan. 2020. FastFormers:</cell></row><row><cell></cell><cell>Highly efficient transformer models for natural lan-</cell></row><row><cell></cell><cell>guage understanding. In Proceedings of SustaiNLP:</cell></row><row><cell></cell><cell>Workshop on Simple and Efficient Natural Language</cell></row><row><cell></cell><cell>Processing, pages 149-158. Association for Compu-</cell></row><row><cell></cell><cell>tational Linguistics.</cell></row><row><cell></cell><cell>Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.</cell></row><row><cell></cell><cell>2020. Reformer: The efficient transformer. In 8th</cell></row><row><cell></cell><cell>International Conference on Learning Representa-</cell></row><row><cell></cell><cell>tions, ICLR 2020, Addis Ababa, Ethiopia, April 26-</cell></row><row><cell></cell><cell>30, 2020.</cell></row><row><cell></cell><cell>Ren?e Koplon and Eduardo D Sontag. 1997. Using</cell></row><row><cell></cell><cell>fourier-neural recurrent networks to fit sequential in-</cell></row><row><cell></cell><cell>put/output data. Neurocomputing, 15(3-4):225-248.</cell></row><row><cell></cell><cell>Taku Kudo and John Richardson. 2018. SentencePiece:</cell></row><row><cell></cell><cell>A simple and language independent subword tok-</cell></row><row><cell></cell><cell>enizer and detokenizer for neural text processing. In</cell></row><row><cell></cell><cell>Proceedings of the 2018 Conference on Empirical</cell></row><row><cell></cell><cell>Methods in Natural Language Processing: System</cell></row><row><cell></cell><cell>Demonstrations, pages 66-71, Brussels, Belgium.</cell></row><row><cell></cell><cell>Association for Computational Linguistics.</cell></row><row><cell></cell><cell>Henry O. Kunz. 1979. On the equivalence between</cell></row><row><cell></cell><cell>one-dimensional discrete walsh-hadamard and mul-</cell></row><row><cell></cell><cell>tidimensional discrete fourier transforms. IEEE</cell></row><row><cell></cell><cell>Computer Architecture Letters, 28(03):267-268.</cell></row><row><cell>John Hewitt and Percy Liang. 2019. Designing and</cell><cell>Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,</cell></row><row><cell>interpreting probes with control tasks. In Proceed-</cell><cell>Dehao Chen, Orhan Firat, Yanping Huang, Maxim</cell></row><row><cell>ings of the 2019 Conference on Empirical Methods</cell><cell>Krikun, Noam Shazeer, and Zhifeng Chen. 2021.</cell></row><row><cell>in Natural Language Processing and the 9th Inter-</cell><cell>Gshard: Scaling giant models with conditional com-</cell></row><row><cell>national Joint Conference on Natural Language Pro-</cell><cell>putation and automatic sharding. In 9th Inter-</cell></row><row><cell>cessing (EMNLP-IJCNLP), pages 2733-2743, Hong</cell><cell>national Conference on Learning Representations,</cell></row><row><cell>Kong, China. Association for Computational Lin-</cell><cell>ICLR 2021, Virtual Event, Austria, May 3-7, 2021.</cell></row><row><cell>guistics.</cell><cell></cell></row><row><cell></cell><cell>Zongyi Li, Nikola Borislavov Kovachki, Kamyar Az-</cell></row><row><cell>Tyler Highlander and Andres Rodriguez. 2015. Very</cell><cell>izzadenesheli, Burigede Liu, Kaushik Bhattacharya,</cell></row><row><cell>efficient training of convolutional neural networks</cell><cell>Andrew M. Stuart, and Anima Anandkumar. 2021.</cell></row><row><cell>using fast fourier transform and overlap-and-add. In</cell><cell>Fourier neural operator for parametric partial differ-</cell></row><row><cell>Proceedings of the British Machine Vision Confer-</cell><cell>ential equations. In 9th International Conference on</cell></row><row><cell>ence 2015, BMVC 2015, Swansea, UK, September</cell><cell>Learning Representations, ICLR 2021, Virtual Event,</cell></row><row><cell>7-10, 2015, pages 160.1-160.9. BMVA Press.</cell><cell>Austria, May 3-7, 2021.</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Loss and accuracy pre-training metrics on TPUs. The GPU metrics are very similar. "B" denotes Base, "L" is Large and "H" is Hybrid.</figDesc><table><row><cell></cell><cell>Loss</cell><cell>Accuracy</cell></row><row><cell>Model</cell><cell cols="2">Total MLM NSP MLM NSP</cell></row><row><cell>BERT-B</cell><cell cols="2">1.76 1.48 0.28 0.68 0.86</cell></row><row><cell>Linear-B</cell><cell cols="2">2.12 1.78 0.35 0.62 0.83</cell></row><row><cell>FNet-B</cell><cell cols="2">2.45 2.06 0.40 0.58 0.80</cell></row><row><cell cols="3">Random-B 5.02 4.48 0.55 0.26 0.70</cell></row><row><cell cols="3">FF-only-B 7.54 6.85 0.69 0.13 0.50</cell></row><row><cell cols="3">FNet-H-B 2.13 1.79 0.34 0.63 0.84</cell></row><row><cell>BERT-L</cell><cell cols="2">1.49 1.23 0.25 0.72 0.88</cell></row><row><cell>Linear-L</cell><cell cols="2">1.91 1.60 0.31 0.65 0.85</cell></row><row><cell>FNet-L</cell><cell cols="2">2.11 1.75 0.36 0.63 0.82</cell></row><row><cell cols="3">FNet-H-L 1.89 1.58 0.31 0.67 0.85</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>TPU training speeds (in steps per second; larger is better), inference speeds (in milliseconds per batch; smaller is better) and peak memory usage during training (in GB; smaller is better) on the Long-Range Arena Text classification task. Speed up multipliers relative to the Transformer are given in parentheses.</figDesc><table><row><cell>Seq. length</cell><cell>512</cell><cell>1024</cell><cell>2048</cell><cell>4096</cell><cell>8192</cell><cell>16386</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Training Speed (steps/s)</cell><cell></cell><cell></cell></row><row><cell>Transformer</cell><cell>8.0</cell><cell>5.6</cell><cell>1.7</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell></row><row><cell>Linear</cell><cell>9.4 (1.2x)</cell><cell>9.1 (1.6x)</cell><cell>7.6 (4.5x)</cell><cell>3.9</cell><cell>1.4</cell><cell>OOM</cell></row><row><cell>FNet (mat)</cell><cell>9.5 (1.2x)</cell><cell>9.1 (1.6x)</cell><cell>6.1 (3.6x)</cell><cell>3.0</cell><cell>0.8</cell><cell>0.2</cell></row><row><cell>FNet (FFT)</cell><cell>8.6 (1.1x)</cell><cell>6.0 (1.1x)</cell><cell>3.2 (1.9x)</cell><cell>1.6</cell><cell>0.8</cell><cell>0.3</cell></row><row><cell>Performer</cell><cell>9.2 (1.2x)</cell><cell>8.4 (1.5x)</cell><cell>6.9 (4.1x)</cell><cell>4.2</cell><cell>2.2</cell><cell>1.1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Inference Speed (ms/batch)</cell><cell></cell><cell></cell></row><row><cell>Transformer</cell><cell>7.0</cell><cell>13.2</cell><cell>39.4</cell><cell>129.9</cell><cell>490.2</cell><cell>OOM</cell></row><row><cell>Linear</cell><cell>5.6 (1.2x)</cell><cell>6.5 (2.0x)</cell><cell cols="4">9.6 (4.1x) 20.4 (6.4x) 54.6 (9.0x) OOM</cell></row><row><cell>FNet (mat)</cell><cell>6.0 (1.2x)</cell><cell cols="5">7.7 (1.7x) 15.4 (2.6x) 40.7 (3.2x) 137.0 (3.6x) 454.5</cell></row><row><cell cols="7">FNet (FFT) 10.8 (0.7x) 16.8 (0.8x) 29.9 (1.3x) 58.8 (2.2x) 113.6 (4.3x) 263.2</cell></row><row><cell>Performer</cell><cell>6.1 (1.2x)</cell><cell cols="5">7.2 (1.8x) 10.1 (3.9x) 17.5 (7.4x) 31.8 (15.4x) 61.0</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Peak Memory Usage (GB)</cell><cell></cell><cell></cell></row><row><cell>Transformer</cell><cell>1.1</cell><cell>2.1</cell><cell>5.8</cell><cell>9.1</cell><cell>OOM</cell><cell>OOM</cell></row><row><cell>Linear</cell><cell>0.9</cell><cell>1.1</cell><cell>1.9</cell><cell>4.9</cell><cell>14.8</cell><cell>OOM</cell></row><row><cell>FNet (mat)</cell><cell>0.8</cell><cell>0.9</cell><cell>1.3</cell><cell>2.2</cell><cell>4.8</cell><cell>11.9</cell></row><row><cell>FNet (FFT)</cell><cell>0.8</cell><cell>0.9</cell><cell>1.3</cell><cell>2.0</cell><cell>3.5</cell><cell>6.3</cell></row><row><cell>Performer</cell><cell>1.0</cell><cell>1.3</cell><cell>1.8</cell><cell>3.0</cell><cell>5.1</cell><cell>9.6</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>Training (forward and backward passes; left) and inference (forward pass; left) speeds for only the mixing sublayers -all other model sublayers are removed. Both speeds are measured in milliseconds per batch (smaller is better), with batch sizes of 64 (GPU) and 256 (TPU). All batch examples have the sequence length fixed at 512. FNet uses the FFT for GPUs and matrix multiplications for TPUs. Speed up multipliers relative to self-attention are given in parentheses.</figDesc><table><row><cell></cell><cell cols="4">Training speed (ms/batch) Inference speed (ms/batch)</cell></row><row><cell></cell><cell>GPU</cell><cell>TPU</cell><cell>GPU</cell><cell>TPU</cell></row><row><cell>Self-attention (Base)</cell><cell>136</cell><cell>76</cell><cell>43</cell><cell>16</cell></row><row><cell>Linear (Base)</cell><cell>36 (3.7x)</cell><cell>12 (6.1x)</cell><cell>15 (2.8x)</cell><cell>4 (3.9x)</cell></row><row><cell>FNet (Base)</cell><cell>11 (12.2x)</cell><cell>8 (9.9x)</cell><cell>11 (4.0x)</cell><cell>8 (2.1x)</cell></row><row><cell>Self-attention (Large)</cell><cell>404</cell><cell>212</cell><cell>128</cell><cell>43</cell></row><row><cell>Linear (Large)</cell><cell>103 (3.9x)</cell><cell>35 (6.1x)</cell><cell>36 (3.6x)</cell><cell>10 (4.5x)</cell></row><row><cell>FNet (Large)</cell><cell>18 (22.2x)</cell><cell>22 (9.7x)</cell><cell>18 (7.3x)</cell><cell>22 (2.0x)</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>GPU pre-training accuracy and speed ablations for FNet-Hybrid models in the Base configuration. Batch size is 64. Metrics are recorded after 100k steps, which we have generally found to be a good indicator of final relative performance. See text for a description of the layouts.</figDesc><table><row><cell></cell><cell>Attention</cell><cell>Accuracy</cell><cell>Speed</cell></row><row><cell cols="4">Layers Layout MLM NSP (ms/batch)</cell></row><row><cell>2</cell><cell cols="2">BOTTOM 0.497 0.733</cell><cell>193</cell></row><row><cell>2</cell><cell cols="2">MIDDLE 0.499 0.686</cell><cell>196</cell></row><row><cell>2</cell><cell cols="2">MIXED 0.509 0.727</cell><cell>194</cell></row><row><cell>2</cell><cell>TOP</cell><cell>0.526 0.738</cell><cell>193</cell></row><row><cell>0</cell><cell>TOP</cell><cell>0.486 0.679</cell><cell>173</cell></row><row><cell>2</cell><cell>TOP</cell><cell>0.526 0.738</cell><cell>193</cell></row><row><cell>4</cell><cell>TOP</cell><cell>0.539 0.740</cell><cell>214</cell></row><row><cell>6</cell><cell>TOP</cell><cell>0.546 0.746</cell><cell>235</cell></row><row><cell cols="2">From the</cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code is available at https://github.com/ google-research/google-research/tree/ master/f_net.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Memory usage is often overlooked, but empirical studies have shown that Transformer architectures are often memorybound(Ivanov et al., 2020;<ref type="bibr" target="#b20">Shazeer, 2019)</ref>.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The relative ordering of Fseq and Fh in Equation(3)is immaterial because the two 1D DFTs commute.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This is merely an intuition; the reality is more complicated due to the presence of residual connections and since the transformation in Equation(3)is no longer invertible if we only use the real component.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">https://github.com/google/flax 6 https://github.com/google-research/ google-research/tree/master/f_net</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7">On the other hand, the smaller sized Linear models do generally perform well on 512 sequence lengths; seeFigure 2. 8 WNLI is excluded in Devlin et al.(2019). BERT's accuracy on WNLI is below baseline, unless a special training recipe is used. See also (12) in https:// gluebenchmark.com/faq.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9"> Devlin et al. (2019)  obtain a roughly 2.5 average point boost on the Test split going from BERT-Base to BERT-Large. We only see a roughly 1.5 boost on the Validation split, which may be due to reduced headroom.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11">The "Linear" model inTable 4is the baseline model introduced in Section 4.1.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12">Whereas the DFT matrix in Equation (2) contains N roots of unity, the Hadamard Transform simply contains two roots of unity: {?1}; see alsoKunz (1979).</note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Additional configurations that we experimented with</head><p>We experimented with a number of additional ideas to improve FNet. Fourier Transform algorithm. On GPUs, the FFT was the fastest algorithm for computing the DFT across all sequence lengths that we experimented with (512 ? 8192). On TPUs, it is faster to compute the DFT directly using matrix multiplications for relatively shorter sequence lengths (up to lengths of 4096; see <ref type="table">Table 7</ref>). This efficiency boundary between matrix multiplication and FFT on TPUs will change depending on the XLA precision for the matrix multiplications. We found that, although (slower) HIGHEST XLA precision was required to very accurately reproduce FFT in computing the DFT, (faster) DEFAULT XLA precision was sufficient to facilitate accurate model convergence.</p><p>Modifying the Fourier Transform computation. To keep the entire FNet architecture simple, the Fourier sublayer accepts real input and returns real output. The standard Fourier sublayer in FNet simply extracts the real part after computing the 2D DFT. We found that FNet was less accurate and less stable during training if only the real part of the DFT was used throughout the computation. Simply extracting the absolute value (instead of the real part) also led to a significantly less accurate model. Because the feed-forward sublayer mixes the hidden dimension, we experimented with applying a 1D DFT along the token dimension only in the Fourier sublayer (i.e. no hidden dimension mixing in the Fourier sublayer). This yielded some training speed gains but hurt accuracy. The 1D (token mixing only) DFT model still significantly outperformed the (no token mixing) FF-only model, indicating that token mixing is most important mechanism in the Fourier sublayer.</p><p>Other transforms. We experimented with three natural alternatives to the Fourier Transform:</p><p>? Discrete Cosine Transform (DCT). The DCT is closely related to the DFT but transforms real input to real output. However, we found that the DCT model underperformed FNet (? 4% accuracy degradation).</p><p>? Hadamard Transform 12 . Although the Hadamard Transform was slightly faster than the DFT, it yielded less accurate results (? 2% accuracy degradation).</p><p>? Hartley Transform. The Hartley Transform, which transforms real input to real output, can be described in terms of the Fourier Transform: H = {F} ? {F }. We found that the Hartley Transform matched the Fourier Transform on GLUE (76.7 vs. 76.7).</p><p>Introducing learnable parameters to the Fourier sublayer. Our attempts to introduce learnable parameters into the Fourier sublayer were either detrimental or inconsequential, and generally slightly slowed the model. For the (sequence length, hidden dimension) input in each Fourier sublayer, we tried two approaches to introduce learnable parameters: (1) element wise multiplication with a (sequence length, hidden dimension) matrix, and (2) regular matrix multiplication with (sequence length, sequence length) and (hidden dimension, hidden dimension) matrices. We experimented with these approaches in various configurations: preceding and/or following the DFT, and also in combination with inverse DFT (e.g. transform to frequency domain, apply element wise multiplication, transform back to time domain), but most setups degraded accuracy and reduced training stability, while a few did not change accuracy but lead to small speed decreases. In a slightly different set of experiments and in an effort to provide more flexibility to the model, we added (complex) learnable weights to the 2D DFT matrix. This model was stable but did not yield any accuracy gains, suggesting that the DFT is locally optimal in some sense.</p><p>FNet block modifications. The standard FNet encoder block structure follows that of the Transformer: a Fourier sublayer followed by a feedforward sublayer, with residual connections and layer norms after each sublayer; see <ref type="figure">Figure 1</ref>. We tried several modifications to this structure, based on the intuition of moving in and out of the frequency domain between multiplications. For example, the sandwiching of Fourier, feed-forward, Fourier (or inverse Fourier) sublayers and only applying the residual connections and layer norms to the final result, yields a structure that more closely k e r n e l _ i n i t =nn . i n i t i a l i z e r s . normal ( 2 e ?2) , 20 b i a s _ i n i t =nn . i n i t i a l i z e r s . normal ( 2 e ?2) , 21</p><p>name= " i n t e r m e d i a t e " ) ( x ) 22</p><p>x = nn . g e l u ( x ) 23</p><p>x = nn . Dense ( x . shape [ ? 1 ] , 24 k e r n e l _ i n i t =nn . i n i t i a l i z e r s . normal ( 2 e ?2) , </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Etc: Encoding long and structured inputs in transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaclav</forename><surname>Cvicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Sanghai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="268" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A note on more efficient architectures for nlp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arturs</forename><surname>Backurs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<ptr target="http://www.mit.edu/~backurs/NLP.pdf" />
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Universal approximation bounds for superpositions of a sigmoidal function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew R Barron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information theory</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="930" to="945" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arman</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fft-based deep learning deployment in embedded systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Nazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongjia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caiwen</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massoud</forename><surname>Pedram</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 Design, Automation &amp; Test in Europe Conference &amp; Exhibition (DATE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1045" to="1050" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generating wikipedia by summarizing long sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Pot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Goodrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Sepassi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th International Conference on Learning Representations, ICLR 2018, Vancouver</title>
		<meeting><address><addrLine>BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-04-30" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikael</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Fast training of convolutional networks through ffts: International conference on learning representations (iclr2014), cbls</title>
		<imprint>
			<date type="published" when="2014-04" />
		</imprint>
	</monogr>
	<note>2nd International Conference on Learning Representations. ICLR</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Real-time discrimination of ventricular tachyarrhythmia with fourier-transform neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Kei-Ichiro Minami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeshi</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toyoshima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="185" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast fourier transform for feature extraction and neural network for classification of electrocardiogram signals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martina</forename><surname>Mironovova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jir?</forename><surname>B?la</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 Fourth International Conference on Future Generation Communication Technology (FGCT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">ACDC: A structured efficient linear layer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcin</forename><surname>Moczulski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeremy</forename><surname>Appleyard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>De Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International Conference on Learning Representations</title>
		<meeting><address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-05-02" />
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Do transformer modifications transfer across implementations and applications?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hyung Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liam</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thibault</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fevry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karishma</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Malkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ding</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.465</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Adam Roberts, and Colin Raffel</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="5758" to="5773" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4055" to="4064" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Random feature attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fcnn: Fourier convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harry</forename><surname>Pratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frans</forename><surname>Coenen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yalin</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="786" to="798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Blockwise selfattention for long document understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wen-Tau Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.232</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2555" to="2565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="67" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fixed encoder self-attention patterns in transformer-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Raganato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yves</forename><surname>Scherrer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J?rg</forename><surname>Tiedemann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.49</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="556" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hopfield networks is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Sch?fl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Lehner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Seidl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Widrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Gruber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Holzleitner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Adler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">P</forename><surname>Kreil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">K</forename><surname>Kopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G?nter</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Brandstetter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<meeting><address><addrLine>Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-05-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53" to="68" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Distilbert, a distilled version of bert: smaller, faster, cheaper and lighter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01108</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Fast transformer decoding: One write-head is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.02150</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.13002</idno>
		<title level="m">Pretrained summarization distillation</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Structured transforms for small-footprint deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="3088" to="3096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Language through a prism: A spectral approach for multiscale language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Tamkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5492" to="5504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Synthesizer: Rethinking self-attention in transformer models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00743</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Sparse sinkhorn attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9438" to="9447" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Long range arena : A benchmark for efficient transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">9th International Conference on Learning Representations, ICLR 2021, Virtual Event</title>
		<imprint>
			<date type="published" when="2021-05-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06732</idno>
		<title level="m">Efficient transformers: A survey</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Zhen Qin, and Donald Metzler. 2021b. Are pretrained convolutions better than pretrained transformers?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jai</forename><surname>Prakash Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vamsi</forename><surname>Aribandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.335</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<biblScope unit="page" from="4349" to="4359" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">BERT rediscovers the classical NLP pipeline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Tenney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dipanjan</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ellie</forename><surname>Pavlick</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1452</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4593" to="4601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Ilya O Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uszkoreit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Well-read students learn better: On the importance of pre-training compact models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iulia</forename><surname>Turc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.08962</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Attention is all you need. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Analyzing the structure of attention in a transformer language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Vig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonatan</forename><surname>Belinkov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4808</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="63" to="76" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elena</forename><surname>Voita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Talbot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fedor</forename><surname>Moiseev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Titov</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1580</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5797" to="5808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast transformers with clustered attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21665" to="21674" />
		</imprint>
	</monogr>
	<note>Angelos Katharopoulos, and Fran?ois Fleuret</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">GLUE: A multi-task benchmark and analysis platform for natural language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanpreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Felix</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Bowman</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-5446</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP</title>
		<meeting>the 2018 EMNLP Workshop Black-boxNLP: Analyzing and Interpreting Neural Networks for NLP<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="353" to="355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Linformer: Selfattention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Hard-coded Gaussian attention for neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiqiu</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohit</forename><surname>Iyyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.687</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="7689" to="7700" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kumar Avinava Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17283" to="17297" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning long term dependencies via fourier recurrent units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibo</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5815" to="5823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Forenet: fourier recurrent networks for time series prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lai-Wan</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Neural Information Processing (ICONIP 2000)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="576" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Fault diagnosis and prognosis using wavelet packet decomposition, fourier transform and artificial neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kesheng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Intelligent Manufacturing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1213" to="1227" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
