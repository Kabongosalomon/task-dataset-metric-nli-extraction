<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh-Quan</forename><surname>Cao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Inria</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gilles</forename><surname>Puy</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandre</forename><surname>Boulch</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renaud</forename><surname>Marlet</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Ecole des Ponts</orgName>
								<orgName type="laboratory">LIGM</orgName>
								<orgName type="institution" key="instit1">Univ Gustave Eiffel</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<addrLine>Marne-la-Vall?e</addrLine>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<address>
									<addrLine>1 Valeo.ai</addrLine>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PCAM: Product of Cross-Attention Matrices for Rigid Registration of Point Clouds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T16:56+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Rigid registration of point clouds with partial overlaps is a longstanding problem usually solved in two steps: (a) finding correspondences between the point clouds; (b) filtering these correspondences to keep only the most reliable ones to estimate the transformation. Recently, several deep nets have been proposed to solve these steps jointly. We built upon these works and propose PCAM: a neural network whose key element is a pointwise product of crossattention matrices that permits to mix both low-level geometric and high-level contextual information to find point correspondences. These cross-attention matrices also permits the exchange of context information between the point clouds, at each layer, allowing the network construct better matching features within the overlapping regions. The experiments show that PCAM achieves state-of-the-art results among methods which, like us, solve steps (a) and (b) jointly via deepnets. Our code and trained models are available at https://github.com/valeoai/PCAM.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Point cloud registration is the problem of estimating the rigid transformation that aligns two point clouds. It has many applications in various domains such as autonomous driving, motion and pose estimation, 3D reconstruction, simultaneous localisation and mapping (SLAM), and augmented reality. The most famous method to solve this task is ICP <ref type="bibr" target="#b2">[3]</ref>, for which several improvements have been proposed, modifying the original optimisation process <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b46">47]</ref> or using geometric feature descriptors <ref type="bibr" target="#b32">[33]</ref> to match points.</p><p>Recently, end-to-end learning-based methods combining point feature extraction, point matching, and point-pairs filtering, have been developed to solve this task. Deep Closest Point (DCP) <ref type="bibr" target="#b39">[40]</ref>, improved by PRNet <ref type="bibr" target="#b38">[39]</ref>, finds point cor-* Most of the work was done during an internship at valeo.ai in 2020. ? Inria, Mines ParisTech, PSL Research University. respondences via an attention matrix and estimate the transformation by solving a least-squares problem. Deep Global Registration (DGR) <ref type="bibr" target="#b6">[7]</ref> tackles partial-to-partial point cloud registration by finding corresponding points via deep features, computing confidence scores for these pairs, and solving a weighted least-squares problem to estimate the transformation. IDAM <ref type="bibr" target="#b22">[23]</ref> considers all possible pairs of points in a similarity matrix computed using deep or hand-crafted features, and proposes a learned two-step filter to select only relevant pairs for transformation estimation. Finally, <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b42">43]</ref> use deep networks and the Sinkhorn algorithm to find and filter corresponding points.</p><p>Our observation is that, when matching points between point clouds, one would like to find correspondences using both local fine geometric information, to precisely select the best corresponding point, and high-level contextual information, to differentiate between points with similar local geometry but from different parts of the scene. The fine geometric information is naturally extracted at the first layers of a deep convnet, while the context information is found in the deepest layers. The existing deep registration methods estimate the correspondences between point clouds using features extracted at the deepest layers. Therefore, there is no explicit control on the amount of fine geometric and high-level context information encoded in these features.</p><p>Instead, we propose to compute point correspondences at every layer of our deep network via cross-attention matrices, and to combine these matrices via a point-wise multiplication. This simple yet very effective solution naturally ensures that both low-level geometric and high-level context information are exploited when matching points. It also permits to remove spurious matches found only at one scale. Furthermore, we also exploit these cross-attention matrices to exchange information between the point clouds at each layer, allowing the network to exploit context information from both point clouds to find the best matching point within the overlapping regions.</p><p>The design of our method is inspired by DGR <ref type="bibr" target="#b6">[7]</ref>, DCP <ref type="bibr" target="#b39">[40]</ref> and PRNet <ref type="bibr" target="#b38">[39]</ref>. We exploit cross-attention matrices to exchange context information between point clouds, like in <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b11">12]</ref>, to find the best matches within overlapping region. Our main contribution is to propose to compute such cross-attention matrices at every network layer and to combine them to exploit both fine and high-level information when matching points. Our second contribution is to show that our method achieves state-of-the-art results on two real datasets (indoor, outdoor) and on a synthetic one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Optimisation-based methods. Iterative Closest Point (ICP) <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b45">46]</ref> is the most known algorithm for point cloud registration. It takes in two point clouds and alternate between point matching via nearest-neighbour search and transformation estimation by solving a least-squares problem. Several improvements of ICP's steps have been introduced to improve speed, solve the discretisation problem, become robust to outliers, or incorporate confidence scores on the correspondences <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b33">34]</ref>. ICP often converges to a local minimum due to the non-convexity of the objective function. Therefore, some works propose solutions to widen the basin of attraction of the global optimum <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b36">37]</ref>, use genetic algorithms <ref type="bibr" target="#b34">[35]</ref>, or find a good crude alignment to initialise ICP <ref type="bibr" target="#b25">[26]</ref>. Another line of works concerns algorithms with theoretical convergence guarantees to a global optimum thanks to, e.g., the combination of ICP and a branch-and-bound technique <ref type="bibr" target="#b41">[42]</ref>, via convex relaxations <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b29">30]</ref>, or thanks to mixed-integer programming <ref type="bibr" target="#b18">[19]</ref>. While theoretically appealing, these approaches usually suffers from a high computational complexity.</p><p>Feature-based point matching. Instead of relying solely on point coordinates to find correspondences between points, several approaches have proposed to extract point feature descriptors by analysing the local, and possibly global, 3D geometry. Classical methods use handcrafted features such as <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b35">36]</ref> and, more recently, features extracted thanks to deep networks that take as inputs either hand-crafted local features <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b44">45]</ref>, or directly point cloud coordinates <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>. These methods use these point features to establish correspondences and rely on RANSAC to estimate the transformation, possibly using a pre-filtering of the points unlikely to provide good correspondences <ref type="bibr" target="#b17">[18]</ref>.</p><p>End-to-end learning. Another type of approaches consists in training a network that will establish correspondences between the points of both point clouds, filter these correspondences to keep only the reliable pairs, and estimate the transformation based on the best pairs of points. Several methods fall in this category, such as <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b42">43]</ref> which we described in the introduction. In addition, let us also mention PointNetLK <ref type="bibr" target="#b1">[2]</ref> that extracts a global feature vector per point cloud and unroll the Lucas-Kanade algorithm <ref type="bibr" target="#b24">[25]</ref> to find the best transforma-tion aligning the point clouds in feature space, DeepGMR <ref type="bibr" target="#b43">[44]</ref> that maps each point cloud to a parametric probability distribution via a neural network and estimates the transformation by minimising the KL-divergence between distributions. <ref type="bibr" target="#b21">[22]</ref> proposes a probabilistic registration method using deep features and learned attention weights. Finally, <ref type="bibr" target="#b14">[15]</ref> proposes a method for multiview registration by jointly training a pairwise registration module and a global refinement module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Network Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Statement</head><p>We consider the problem of partial-to-partial rigid registration between two point clouds P = {p 1 , . . . , p N } and Q = {q 1 , . . . , q M }, with p i , q j ? R 3 . The point clouds P and Q represent two views of the same scene with partial overlap. Hence, only a subset of the points in P can have matching points in Q, and vice versa from Q to P. Let P v (resp. Q v ) be the subset of points in P (resp. Q) visible in Q (resp. P). Our goal is to estimate the rotation matrix R gt ? SO(3) and translation vector t gt ? R 3 that aligns P v on Q v . This transformation can be estimated by solving</p><formula xml:id="formula_0">min R,t p?Pv R p + t ? m Q (p) 2 2 ,<label>(1)</label></formula><p>where m Q : P v ? Q v maps any p ? P v to its best matching point q ? Q v . Note that we can also estimate the inverse transformation from Q to P with the map m P : Q v ? P v . Problem (1) can be solved via an SVD decomposition <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b16">17]</ref>. The challenging tasks are the estimations of the subset of points P v and Q v , and of the mappings m Q (?) and m P (?), given only P and Q as inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Method Overview</head><p>Our method is composed of two classical modules: point matching and point-pair filtering (see <ref type="figure">Figure 1</ref>). The pointmatching module, denoted by g(?, ?), permits us to estimate two mapsm Q : P ? Q andm P : Q ? P that provide pairs of corresponding points between P and Q (even in nonoverlapping regions). The point-pair filtering module, denoted by h(?, ?), provides a confidence score to all pairs (p i ,m Q (p i )) 1 i N , (q j ,m P (q j )) 1 j M , hence allowing detection of valid pairs in overlapping regions.</p><p>Our main contribution is in g, where we propose the construction of cross-attention matrices at each layer of g and their combination to obtain point correspondences. These successive cross-attention matrices are obtained using point features with an increasing field of view, or scale, thus establishing point correspondences with increasing scene context. We obtainm Q andm P by combining all these attention matrices via pointwise matrix multiplications. Within overlapping regions, it permits us to improve the quality of <ref type="figure">Figure 1</ref>. Overview of our architecture. P and Q enter in the point matching module g(?) that permits the extraction of attention matrices used to compute one matching point for each input point. Each pair of matching points is then given a confidence score via h(?), which permits the estimation of the rigid transformation by solving a weighted least-squares problem via SVD.</p><p>the mapsm Q andm P by filtering out the matches that are inconsistent across scales. We also use these cross-attention matrices to exchange information between P and Q: each point in P receives, in addition to its own feature, the feature of the best corresponding point in Q (and vice-versa) before computing the point feature at the next layer. This process favours the discovery or rejection of correspondences at the next layer depending on the level of similarity between corresponding points at the current layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">First Module: Point Matching</head><p>Our point matching module, denoted by g : </p><formula xml:id="formula_1">(R N ?3 , R M ?3 ) ? ? (R N ?M , R N ?M ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Layer-wise Cross-Attention Matrices</head><formula xml:id="formula_2">A ( ) PQ , A ( ) QP Let F P ? R N ?c ( ?1) and F Q ? R M ?c ( ?1)</formula><p>be the features on P and Q, respectively, at the input of layer , where c ( ?1) is the size of the feature vectors at the output of layer ? 1. Each point is thus described by a c ( ?1) -dimensional feature vector. These features enter the th point convnet e : R ??c ( ?1) ? ? R ??c ( ) /2 to extract new features of size c ( ) /2 for each input point, to be doubled after concatenation of the best corresponding point feature in the other point cloud (see below). This encoder extracts local geometry information around each point in the point cloud thanks to three residual blocks, each containing two FKAConv <ref type="bibr" target="#b4">[5]</ref> sub-layers. The supplementary material details e(?).</p><p>The new feature vectors e(F P ) and e(F Q ) are used to compute two attention matrices A ( )</p><formula xml:id="formula_3">PQ , A ( ) QP ? R N ?M with (A ( ) PQ ) ij = e aij /s M k=1 e a ik /s , (A ( ) QP ) ij = e aij /s N k=1 e a kj /s ,<label>(2)</label></formula><p>where s &gt; 0 is the softmax temperature and</p><formula xml:id="formula_4">a ij = e(F P ) i e(F Q ) j e(F P ) i 2 e(F Q ) j 2 .<label>(3)</label></formula><p>A PQ , A QP differ in the normalisation dimension: A PQ transfers information from Q to P, A QP from P to Q. Finally, we transfer information between point clouds by </p><formula xml:id="formula_5">computing E P = [e(F P ), A ( ) PQ e(F Q )] ? R N ?c ( ) , E Q = [e(F Q ), A ( ) QP e(F P )] ? R M ?c ( ) . E P , E Q</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Global Attention Matrices</head><formula xml:id="formula_6">A ( * ) PQ , A ( * ) QP</formula><p>We combine the attention matrices of each layer via a simple pointwise multiplication, denoted by , to obtain A ( * )</p><formula xml:id="formula_7">PQ = A (1) PQ . . . A (L) PQ . (4) A ( * )</formula><p>QP is defined similarly. The motivation for this strategy is that the successive attention matrices are constructed using features with an increasing field of view or scale, and we want to match points only if their features are similar at all scales, hence the entry-wise multiplication of the attention coefficients. Another motivation is to permit to backpropagate gradients directly from the loss to each layer .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3">Soft and Sparse Maps</head><p>There exists two standards ways to match points once the global attention matrices are available: soft and sparse mappings. We explore the performance of both approaches in this work. The soft mapm Q (?) is defined as</p><formula xml:id="formula_8">m Q (p i ) = M j=1 (A ( * ) PQ ) i,j q j M k=1 (A ( * ) PQ ) i,k<label>(5)</label></formula><p>for points (p i ) 1 i N . It maps P to R 3 rather than Q. The sparse mapm Q (?) is defined as</p><formula xml:id="formula_9">m Q (p i ) = q j * , where j * = argmax j (A ( * ) PQ ) ij . (6)</formula><p>It maps P to Q, but it is not differentiable and it prevents backpropagation from the confidence network to the pointmatching network. Yet, the point-matching network will still be trained using a cross-entropy loss on the attention matrices (Sec. 3.6). The mappingm P from Q to P or R 3 is constructed similarly using A ( * ) QP .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Second Module: Confidence Estimation</head><p>The module in Sec. 3.3 produces pairs of matching points (p i ,m Q (p i )) and (q j ,m P (q j )) for all points in P and Q. However, as we are tackling partial-to-partial registration, only subsets of P and Q match to one another. Hence, there are many incorrect matches which need to be filtered out. We detect these incorrect matches by concatenating each pair in a vector [p i ,m Q (p i ) ] ? R 6 , and then pass these N pairs into the confidence estimation module h : R ??6 ? ? (0, 1) ??1 which outputs a confidence score w pi for each input pair. The same module h is used for pairs (q j ,m P (q j )) and yields M corresponding scores w qj . The confidence estimator is a point convnet with 9 residual blocks, each containing 2 FKAConv sub-layers. The confidence score are thus estimated using context information provided by the point convolution on P (or Q). The detailed network architecture is in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Transformation Estimation</head><p>We estimate the rotation and translation from P to Q by solving a weighted least-squares problem, using the weights w p i given by the confidence estimator:</p><formula xml:id="formula_10">(R est , t est ) = argmin R,t N i=1 ?(w p i ) Rp i + t ?m Q (p i ) 2 2 .<label>(7)</label></formula><p>The function ? : R ? ? R applies hard-thresholding on the confidence scores by setting to zero the less confident ones. The above problem can be solved with an SVD <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b39">40]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Losses</head><p>The networks g(?, ?) and h(?, ?) and trained under supervision using a first loss, denoted by L ca that applies on the attention matrices A ( * )</p><formula xml:id="formula_11">PQ , A ( * )</formula><p>QP and the sum of other losses, L cc and L gc , that apply on the confidence scores w pi , w qj . The complete training loss satisfies L ca + L cc + L gc .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.1">Loss on the Attention Matrices</head><p>The role of the attention matrices A ( * )</p><formula xml:id="formula_12">PQ , A ( * )</formula><p>QP is to identify mappings between P and Q for the final registration task. The pairs of points identified at this stage should include, as a subset, the ideal pairs encoded by m P and m Q . Identifying this subset is the role of the confidence estimator h(?, ?).</p><p>The ideal map m P and m Q are estimated as follows. For each point p i , we search the closest point q j * (i) to R gt p i + t gt in Q and consider the pair</p><formula xml:id="formula_13">(p i , q j * (i) ) as valid if R gt p i + t gt is also the closest point to q j * (i) in {R gt p u + t gt } u . More precisely, let us de- fine j * (u) = argmin j R gt p u + t gt ? q j 2 and i * (v) = argmin v R gt p i + t gt ? q v 2 , The ideal maps satisfy m Q (p u ) = q v , m P (q v ) = p u , ?(u, v) ? C, where C = {(u, v) | j * (u) = v, u = i * (v)}.</formula><p>Only a subset of the points are in C, even in overlapping regions. For convenience, we also define the set of points in P for which a corresponding point exists in Q by</p><formula xml:id="formula_14">C P = {u | ? v ? M s.t. (u, v) ? C}.</formula><p>The set C Q is defined similarly for the inverse mapping.</p><p>We consider a contrastive classification loss, such as used in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b8">9]</ref>, denoted by L ca = L ca PQ + L ca QP . L ca PQ enforces a good mapping from P to Q and satisfies</p><formula xml:id="formula_15">L ca PQ = ? 1 N (u,v)?C log (A ( * ) PQ ) uv = ? 1 N L =1 (u,v)?C log ? ? e (a ( ) uv /s) M j=1 e (a ( ) uj /s) ? ? .<label>(8)</label></formula><p>L ca QP is defined likewise. All points are constrained in (8) thanks to the softmax involved in the attention matrices. In cases where C contains only few points, rather than augmenting C with pairs of worse quality, we leave C untouched to force the point matching network to identify the most reliable pairs (along with bad ones) and let the second module learn how to filter the bad pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.2">Losses on the Confidence Scores</head><p>To train the confidence estimator h(?, ?), we consider classification and geometric losses.</p><p>The classification losses are built by measuring the quality of the mapsm Q ,m P at each input point. If a mapped pointm Q (p i ) is close to the ideal target point R gt p i + t gt , then the pair (p i ,m Q (p i )) is considered accurate and w pi should be close to 1, and 0 otherwise. The classification loss thus satisfies L cc = L cc</p><formula xml:id="formula_16">PQ + L cc QP , where L cc PQ = ? 1 N i yp i log(wp i ) + (1 ? yp i ) log(1 ? wp i ) , y pi = 1, if R gt p i + t gt ?m Q (p i ) 2 ?, 0, otherwise,<label>(9)</label></formula><p>and ? &gt; 0 is a threshold to decide if a pair of points is accurate or not. The second loss L cc QP is defined similarly by considering the inverse mapping.</p><p>The threshold ? &gt; 0 to decide pair accuracy is somewhat arbitrary. To mitigate the effect of this choice, we also consider the geometric loss L gc = L gc</p><formula xml:id="formula_17">PQ + L gc QP where L gc PQ = i w pi N R gt p i + t gt ?m Q (p i ) 2 .<label>(10)</label></formula><p>L gc QP is defined using w qj and the inverse transformation. Whenever the distance between a mapped point and its ideal location is large, then w pi should be small. On the contrary, when this distance is small, then w pi can be large. Note that this geometric losses can be used only in combination with the classification losses L cc as otherwise w pi , w qj = 0 is a trivial useless solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Training Parameters</head><p>We evaluate our method on real (indoor, outdoor) and synthetic datasets. The indoor dataset is 3DMatch <ref type="bibr" target="#b44">[45]</ref>. We use the standard train/test splits and the procedure of <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b7">8]</ref> to generate pairs of scans with at least 30% of overlap for training and testing. During training, as in <ref type="bibr" target="#b6">[7]</ref>, we apply data augmentation using random rotations in [0 ? , 360 ? ) around a random axis, and random scalings in [0.8, 1.2]. For the experiments on outdoor data, we use the KITTI odometry dataset <ref type="bibr" target="#b13">[14]</ref> and follow the same protocol as <ref type="bibr" target="#b6">[7]</ref>: GPS-IMU is used to create pairs of scans that are at least 10m apart; the ground-truth transformation is computed using GPS followed by ICP. Unlike in <ref type="bibr" target="#b6">[7]</ref>, we do not use data augmentation during training on KITTI. For synthetic data, we use ModelNet40 <ref type="bibr" target="#b40">[41]</ref> and follow the setup of <ref type="bibr" target="#b38">[39]</ref> to simulate partial registration problems.</p><p>All models are trained using AdamW <ref type="bibr" target="#b23">[24]</ref>, with a weight decay of 0.001, a batch size of 1, and a learning rate of 0.001. On 3Dmatch and KITTI, the models are trained for 100 epochs with a learning rate divided by 10 after 60 and 80 epochs. On ModelNet40, it is sufficient to train the models for 10 epochs (with a learning rate divided by 10 after 6 and 8 epochs) to observe convergence. All results are reported using the models obtained at the last epoch. The temperature s in (2) is set to s = 0.03.  <ref type="bibr" target="#b6">[7]</ref>, and those of the other concurrent methods are reported from <ref type="bibr" target="#b6">[7]</ref>. The columns '?', 'Opt.', and 'Saf.' indicate respectively whether hard-thresholding on the confidence scores, the pose refinement of <ref type="bibr" target="#b6">[7]</ref>, and the safeguard registration of <ref type="bibr" target="#b6">[7]</ref>, are used or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Metrics</head><p>The performance on 3DMatch and KITTI is measured using the metrics of <ref type="bibr" target="#b6">[7]</ref>: the translation error (TE) and rotation error (RE) are defined as TE(t est ) = ||t est ? t gt || 2 and RE(R est ) = acos (Tr(R gt R est ) ? 1)/2 , respectively. We also computed the metric coined 'recall' in <ref type="bibr" target="#b6">[7]</ref>, which is the percentage of registrations whose rotation and translation errors are both smaller than predefined thresholds, i.e., the proportion of successful registrations. We report the average rotation error RE all and translation error TE all on all pairs of scans, as well as average rotation error RE and translation error TE on the subset of successful registrations. Performance on ModelNet40 is computed using the metrics of <ref type="bibr" target="#b38">[39]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with Existing Methods</head><p>We report in this section the performance of PCAM on 3DMatch and KITTI. The results obtained on ModelNet40 are available in the supplementary material and show that PCAM outperforms the concurrent methods on this dataset. In what follows, PCAM-Soft and PCAM-Sparse refer to our method with soft (5) and sparse maps (6), respectively.  <ref type="table">Table 2</ref>. Results on KITTI test split. The recall is computed using 0.6 m and 5 ? as thresholds. The scores of all variants of DGR are obtained using the official implementation of DGR <ref type="bibr" target="#b6">[7]</ref>, and those of the other concurrent methods are reported from <ref type="bibr" target="#b6">[7]</ref>. The columns '?', 'Opt.', and 'ICP' indicate respectively whether hardthresholding on the confidence scores, the pose refinement of <ref type="bibr" target="#b6">[7]</ref>, and post-processing by ICP, are used or not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Indoor Dataset: 3DMatch</head><p>We train PCAM on 3DMatch using L = 6 layers, with point clouds obtained by sampling N = M = 4096 points at random from the voxelised point clouds (voxel size of 5 cm). The parameter ? in <ref type="formula" target="#formula_16">(9)</ref> is set at 12 cm. For a thorough comparison with DGR <ref type="bibr" target="#b6">[7]</ref>, we study the performance of PCAM after applying each DGR's postprocessing steps: filtering the confidence weights with ?, refining the pose estimation using the optimisation proposed in <ref type="bibr" target="#b6">[7]</ref>, and using the same safeguard as in <ref type="bibr" target="#b6">[7]</ref>. The threshold applied in ? is tuned on 3DMatch's validation set. The safeguard is activated when the 1 -norm of the confidence weights is below a threshold, in which case the transformation is estimated using RANSAC (see details in <ref type="bibr" target="#b6">[7]</ref>). For a fair comparison with DGR, we compute this second threshold such that DGR and PCAM use the safeguard on the same proportion of scans.</p><p>We report the performance of PCAM and concurrent methods in <ref type="table" target="#tab_0">Table 1</ref>. PCAM achieves the best results and this is confirmed in the curves of <ref type="figure" target="#fig_3">Fig. 2</ref>. We provide illustrations of the quality of matched points and registrations in the supplementary material. We did not compare PCAM with RLL, which is not performing as well as DGR on 3DMatch <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Outdoor Dataset: KITTI</head><p>We train PCAM on 3DMatch using L = 6 layers with point clouds of (at most) N = M = 2048 points drawn at random from the voxelised point clouds (voxel size of 30 cm). The parameter ? in (9) is set at 60 cm.</p><p>The performance of our method is compared to others in <ref type="table">Table 2</ref>. As scores after refinement of DGR registration by ICP are reported on this dataset in <ref type="bibr" target="#b6">[7]</ref>, we also include the scores of PCAM after the same refinement in this table. PCAM outperforms DGR on all metrics with both sparse <ref type="bibr" target="#b5">(6)</ref> and soft (5) maps, except on the recall when using sparse maps, where it is just 0.1 point behind DGR. When combined with ICP, PCAM also achieves better results than DGR except on the recall but where it is just 0.2 point behind with soft maps and 0.8 with sparse maps. Finally, we notice in <ref type="figure" target="#fig_3">Fig. 2</ref> a clear advantage of PCAM over DGR for the estimation of the translation. Note that PCAM achieves better performance than DGR without the need to use ?, nor used the pose refinement of <ref type="bibr" target="#b6">[7]</ref>; we did not notice any significant improvement when using them on KITTI. We compare PCAM to RLL on the version of KITTI used in <ref type="bibr" target="#b21">[22]</ref>: PCAM outperforms RLL with a recall of 84.7% for sparse maps and 86.5% for soft maps, vs 76.9% for RLL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Study</head><p>In this section, we conduct two ablation studies: the first justifies our choice of the training loss; the second demonstrates the benefit of the product of attention matrices and of the exchange of contextual information between point clouds. The performance of the trained models are evaluated on the validation set of the considered datasets. On 3DMatch and KITTI, the input point clouds are obtained by drawing at random (at most) N = M = 2048 points from the voxelised point clouds. Because of these random draws, different evaluations lead to different scores. To take into account these variations, we report the average scores and the standard deviations obtained using three evaluations on the validation set for each trained model. On ModelNet40, we do not subsample the point clouds. The rotation R est and translation t est are computed using the method described in Sec. 3.5 without using any hard-thresholding on the confidence scores. We conduct experiments with L = 2 or L = 6 layers. For the experiments at L = 2, two models are trained from different initialisation and the reported average scores and the standard deviations are computed using both models. One model is trained for the experiments at L = 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Training Loss</head><p>The experiments in this section are conducted with L = 2 layers and using the product of attention matrices <ref type="bibr" target="#b3">(4)</ref>.</p><p>We recall that the loss L cc cannot be removed as there exists a trivial solution where all the confidence scores are equal to zero in absence of this loss. Hence, the only loss that can be removed is L gc . The results in <ref type="table">Table 3</ref> show that removing L gc yields worse performance both with soft and sparse mappings. We explain this result by the fact that R est and t est are obtained using a slightly modified version of L gc at test time (see Sec. 3.5), hence the presence of L gc at 3DMatch KITTI ModelNet40   training time probably improves the quality of confidence scores for the estimation of the transformation.</p><formula xml:id="formula_18">L ca L ga L gc A ( ) A (L) A L</formula><p>With soft maps (5), we can use another loss to train g in replacement or in complement to L ca . The motivation for this loss is that L ca penalises all points that are not mapped correctly to the ideal target point in the same way, whether the mapped point is close to the ideal location or far away. Instead, we can consider the possibility to use a geometric loss, L ga = L ga PQ + L ga QP , that takes into account the distance between the estimated and ideal corresponding points:</p><formula xml:id="formula_19">L ga PQ = 1 |C P | u?C P m Q (p u ) ? m Q (p u ) 2 .<label>(11)</label></formula><p>L ga QP is defined likewise using C Q . Note that it can be used to train g only in the case of soft maps (5) asm Q is not differentiable when using sparse maps <ref type="bibr" target="#b5">(6)</ref>. The results in <ref type="table">Table 3</ref> show that the performance drops significantly when using L ga alone. Note that a similar observation was made in <ref type="bibr" target="#b47">[48]</ref>. Finally, using L ga + L ca yields mildly better performance only on 3DMatch compared to using L ca alone.</p><p>In view of these results, we chose to train PCAM using L ca + L cc + L gc , both for soft and sparse maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Role of the Attention Matrices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Product of attention matrices:</head><p>A ( ) vs A (L) . To show the benefit of mixing low-level geometric information and high-level context information via the proposed product of attention matrices (4), we compare the performance of PCAM when replacing (4) by A ( * ) = A (L) , i.e., using only the cross-attention matrix at the deepest layer. Note that the matrices A (1) , . . . , A (L?1) are still present in g.</p><p>We expect the interest of the mix of information (4) to appear at large L (so that there is enough information to mix) and on challenging datasets where small overlaps and large transformations yield many ambiguities to be resolved for accurate point matching. In that respect, the most challenging dataset used in this work is 3DMatch. One can indeed verify in <ref type="table">Table 3</ref> that all models yields quite similar and very good results on KITTI and ModelNet40, leaving little room to clearly observe the impact of different network architectures. On ModelNet40, we highlight that all scores are close to zero (the scores are multiplied by 10 3 in <ref type="table">Table 3</ref>) which makes it difficult to rank the different strategies as the difference between them could be due to training noise. In contrast, the impact of different choices of architecture is much more visible on 3DMatch.</p><p>We clearly observe the benefit of increasing L from 2 to 6, and of mixing low-level geometric information and high-level context via the product of cross-attention matrices at L = 6 on 3DMatch. At L = 2, we observe a benefit of the product of attention matrices when using soft maps on 3DMatch, while this advantage is not visible when using sparse maps. We explain this fact because the product can make the soft maps more tightly concentrated around the found corresponding points, yielding better quality maps. This effect is invisible with sparse maps at L = 2 because, by construction, these maps are tightly concentrated around the corresponding points that are found. Sharing contextual information: A (L) vs A. To show the benefit of exchanging information between point clouds via the intermediate cross-attention matrices A (1) , . . . , A (L?1) , we conduct experiments where we remove all intermediate matrices but the last one, which we denote by A. As before, we expect the benefit of this strategy to be more visible at large L and on challenging datasets such as 3DMatch. The results in <ref type="table">Table 3</ref> confirm this result where the benefit of exchanging contextual information is particularly noticeable at L = 6 on 3DMatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this section we differentiate our method from the most closely related works on three main aspects: exchange of contextual information between point clouds, computation of the pairs of corresponding points, and point-pair filtering.</p><p>Exchange of contextual information. Multiple crossattention layers in PCAM allows the network to progressively exchange information between point clouds at every layer to find the best matches in overlapping regions. DCP <ref type="bibr" target="#b39">[40]</ref>, PRNet <ref type="bibr" target="#b38">[39]</ref>, and OPRNet <ref type="bibr" target="#b8">[9]</ref> use transformer layers that also exchange information between point clouds, but only in the deepest layers of their convnet. PREDATOR <ref type="bibr" target="#b17">[18]</ref> uses one cross-attention layer in the middle of their U-Net, hence only exchanging high-level contextual information. The newly published method <ref type="bibr" target="#b11">[12]</ref> uses several crossattention layers but, unlike us, does not merge them for point matching. There is no exchange of information between point clouds in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b42">43]</ref>.</p><p>Point matching. PCAM uses multiple cross-attention matrices that allows the network to exploit both fine-grained geometric information and high-level context to find pairs of corresponding points. In contrast, DCP <ref type="bibr" target="#b39">[40]</ref> and PR-Net <ref type="bibr" target="#b38">[39]</ref> use only one attention layer at the deepest layer of their network, which might limit their performance due to the lack of this fine-grained geometric information. RPM-Net <ref type="bibr" target="#b42">[43]</ref>, OPRNet <ref type="bibr" target="#b8">[9]</ref> use a single Sinkhorn layer to establish the correspondences. IDAM <ref type="bibr" target="#b22">[23]</ref> replaces the dot product operator by a learned convolution module that computes the similarity score between point features. The pointmatching module in <ref type="bibr" target="#b14">[15]</ref> is equivalent to ours in the nonoptimal setup of one attention A.</p><p>Point filtering. PCAM first constructs pairs of corresponding points and assign to each of them a confidence score using a point convolution network. A similar strategy is used in <ref type="bibr" target="#b14">[15]</ref>. DCP <ref type="bibr" target="#b39">[40]</ref> does not have any filtering step, hence cannot handle partial-to-partial registration. Before starting to match points, PRNet <ref type="bibr" target="#b38">[39]</ref> first selects a subset of points to be matched in each point cloud using the 2 -norm of point features. Similarly to PRNet <ref type="bibr" target="#b38">[39]</ref>, PREDATOR <ref type="bibr" target="#b17">[18]</ref> also start with a selection of points to be matched in each point cloud, using the learned overlap probability and matchability scores. DGR <ref type="bibr" target="#b6">[7]</ref> uses a similar process to PCAM but uses 6D convolutions to predict the confidence score, whereas we use 3D convolutions with 6D point-features (the concatenation of the coordinates of the point itself and its match point). IDAM <ref type="bibr" target="#b22">[23]</ref> proposes a two-stage point elimination technique where points are individually filtered first; pairs of points are then discarded. Finally, <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b42">43]</ref> use their Sinkhorn layer to filter outliers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this work, we have proposed a novel network architecture where we transfer information between point clouds to find the best match within the overlapping regions, and mix low-level geometric information and high-level contextual knowledge to find correspondences between point clouds. Our experiments show that this architecture achieves stateof-the-art results on several datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Network Architecture</head><p>A.1. Multiscale Attention Module g(?) <ref type="figure" target="#fig_2">Fig. 3</ref> shows the detailed architecture of the multiscale attention module and of its L different encoders e ( ) :</p><formula xml:id="formula_20">R ??c ( ?1) ? ? R ??c ( ) /2 .</formula><p>We recall that c ( ) denotes the number of channels at the output of the th multiscale attention module. The encoder e ( ) is made of three residual blocks, each yielding features with c ( ) /2 channels. The architecture of the residual block is also presented in <ref type="figure" target="#fig_2">Fig. 3</ref> where the first 1D convolution is used only when the number of input and output channels differs. Each residual block consists of two FKAConv layers <ref type="bibr" target="#b4">[5]</ref> with c ( ) /2 channels at input and output, a neighborhood of 32 nearest neighbours, a stride of size 1 (no downsampling of the point clouds), and a kernel of size 16. All convolutional layers are followed by instance normalization (IN) <ref type="bibr" target="#b37">[38]</ref> with learned affine correction and a ReLU activation <ref type="bibr" target="#b27">[28]</ref>. The number of channels c ( ) used in our experiments in given in <ref type="table" target="#tab_4">Table 4</ref>.   <ref type="figure" target="#fig_2">Fig. 3</ref> shows the detailed architecture of the confidence estimator h : R ??6 ? ? (0, 1) ??1 . It consists of nine residual blocks, a FKAConv layer that reduces the number of channels to 1 and a final sigmoid activation. The FKAConv layer has a neighborhood of 32 nearest neighbours, a stride of size 1, and a kernel of size 16. The residual blocks have the same structure as the one used to construct the encoders in the multiscale attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Indoor dataset: 3DMatch</head><p>A detailed analysis of the performance of our method for each of the 8 scenes in the test set of 3DMatch <ref type="bibr" target="#b44">[45]</ref> is presented in <ref type="figure">Fig. 4</ref>. The results confirm the global scores presented in the main part of the paper. Each of the postprocessing steps permits to improve the recall on all the scenes. DGR and PCAM-Sparse achieve nearly similar recall on all scenes with a very noticeable difference on Hotel3 where PCAM-Sparse with hard-thresholding on the weights outperforms DGR with all post-processing steps used.</p><p>We present in <ref type="figure">Fig. 5</ref> and <ref type="figure">Fig. 6</ref> examples of success and failure, respectively, of PCAM-Sparse with filtering with ? <ref type="figure" target="#fig_2">Figure 3</ref>. Detailed network architecture of the multiscale attention module g(?) and the confidence estimator module h(?, ?). Each module is constructed using the same type residual block, in which all internal layers have the same number of channels C indicated in the notation "Residual block, C". Note that the 1D convolution in the residual block is used only when the number of input and output channels differ. on 3DMatch <ref type="bibr" target="#b44">[45]</ref>. We notice that the misregistrations are due to wrong mappings between points on planar surfaces in the examples of row 2, 3 in <ref type="figure">Fig. 6</ref>. We also remark in the example at row 5 that our method found matching points between points of similar but different object. Finally, row 1 of <ref type="figure">Fig. 6</ref> shows a case where the estimated correspondences are correct but the confidence estimator considers them as unreliable which leads to a wrong estimation of the transformation (unless the safeguard is activated). This shows that the performance of the confidence estimator can still be improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Outdoor dataset: KITTI</head><p>We present in <ref type="figure">Fig. 7</ref> and <ref type="figure">Fig. 8</ref> examples of successful and failed registration results, respectively, with our method on the KITTI odometry dataset <ref type="bibr" target="#b13">[14]</ref>. We observe in <ref type="figure">Fig. 7</ref> that the static objects are well aligned after registrations. Concerning the failed registration results, it seems that this is due to mapping between similar but different structure in the scene, at least for the scans with very large registration errors such as on row 1 or 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Synthetic dataset: ModelNet40</head><p>We provide in <ref type="table">Table 5</ref> the results obtained for Model-Net40 on the split corresponding to unseen objects, unseen categories, and unseen objects with noise <ref type="bibr" target="#b38">[39]</ref>. Our method achieves better results than the recent methods that provided scores on these versions of the dataset. <ref type="bibr" target="#b42">[43]</ref> is also evaluated on ModelNet40, but using a slightly different setting than the one used by the methods in <ref type="table">Table 5</ref>. We also test PCAM on this variant of Model-Net40. RPM-Net uses several passes/iterations to align two point clouds while PCAM, which is not trained on small displacements for refinement, uses only one. For fairness, we evaluate both methods after the first main pass. PCAM outperforms RPM-Net on the 'clean' version of ModelNet40 (Chamfer error of 1.8 ? 10 ?5 for RPM-Net, 3.4 ? 10 ?9 for PCAM) and on its 'noisy' version (7.9 ? 10 ?4 for RPM-Net, 6.9 ? 10 ?4 for PCAM).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RPM-Net</head><p>We present in <ref type="figure">Fig. 9</ref> examples of registration results with our method for pairs of scans in the unseen objects with Gaussian noise split. The results presented in these figures illustrate the accurate registrations as well as the good quality of the matched pairs of points.  <ref type="table">Table 5</ref>. Results on ModelNet40 for unseen objects, unseen categories and unseen objects with Gaussian noise. The scores are reported from <ref type="bibr" target="#b38">[39]</ref> for the first 3 methods, and from the associated papers for the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4. Sparse vs FKAConv convolutions</head><p>DGR uses sparse convolutions, whereas PCAM uses FKAConv point convolutions. To test if DGR suffers from a large disadvantage due to these sparse convolutions, we experiment replacing our point matching network by DGR's pre-trained point matching network. We then retrain our confidence estimator on KITTI using DGR's matched points. The performance of this new system reaches a recall of 94.6%, an RE all of 2.9 and TE all of 0.46 on KITTI validation set. This is on par with the results obtained with our point matching network, showing using sparse convolutions in the point matching network do not perform much worse than FKAConv. <ref type="figure">Figure 4</ref>. Analysis of 3DMatch registration results per scene. Row 1-2: Average TE and RE measured on all pairs (lower is better). Row 3: recall rate (higher is better). Row 4-5: TE and RE measured on successfully registered pairs (lower is better). Note that the recall of two methods have to be almost identical for their errors RE (resp. TE) to be comparable. <ref type="figure">Figure 5</ref>. Examples of successful registrations on 3DMatch with PCAM-Sparse + ?. From left to right: overlaid, non-registered input scans (blue and yellow colors); ground-truth registration; scans registered with our method; and top 256 pairs of matched points with highest confidence. <ref type="figure">Figure 6</ref>. Examples of failed registrations on 3DMatch with PCAM-Sparse + ?. From left to right: overlaid, non-registered input scans (blue and yellow colors); ground-truth registration; scans registered with our method; and top 256 pairs of matched points with highest confidence. <ref type="figure">Figure 7</ref>. Examples in bird-eye view of successful registrations on the KITTI odometry dataset <ref type="bibr" target="#b13">[14]</ref>. From left to right: overlaid, nonregistered input scans (blue and red colors); ground-truth registration; scans registered with our method; and top 256 pairs of matched points with highest confidence. Note that we have used the full non-voxelized Lidar scans for a better visualisation. However, all registrations are done using 2048 points drawn at random after voxelization of the full point cloud. <ref type="figure">Figure 8</ref>. Examples in bird-eye view of failed registrations on the KITTI odometry dataset <ref type="bibr" target="#b13">[14]</ref>. From left to right: overlaid, nonregistered input scans (blue and red colors); ground-truth registration; scans registered with our method; and top 256 pairs of matched points with highest confidence. Note that we have used the full non-voxelized Lidar scans for a better visualisation. However, all registrations are done using 2048 points drawn at random after voxelization of the full point cloud. <ref type="figure">Figure 9</ref>. Examples of registration results on ModelNet40 on the split with unseen objects, with Gaussian noise <ref type="bibr" target="#b38">[39]</ref>. From left to right: overlaid, non-registered input scans (blue and red colors); ground-truth registration; scans registered with our method; and top 128 pairs of matched points with highest confidence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>takes two unaligned point clouds P and Q as input, and produces two global attention matrices A ( * ) PQ and A ( * ) QP which inform us on the correspondences between point clouds. These global attention matrices are constructed (see Sec. 3.3.2) from L attention matrices, A ( ) PQ and A ( ) QP , obtained at layers = 1, . . . , L of our deep network (see Sec. 3.3.1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>are the output features of layer and input features of layer ( + 1). At the end of the process, we have extracted two sets of L attention matrices: (A</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>3 .</head><label>3</label><figDesc>Ablation study on the validation set of 3DMatch, KITTI, and ModelNet40. The losses L ca , L gc L ga are defined in the main paper. The global attention matrix A ( * ) is either equal to the pointwise multiplication of all the attention matrices ( A ( ) ), or to the last attention matrix (A (L) ), or to the attention matrix (A) computed at the last layer of our network in a version where we have removed all the intermediate attention matrices = 1, . . . , L (see main paper for details).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Registration recall vs rotation and translation error thresholds for KITTI (left) and 3DMatch (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>?Table 1</head><label>1</label><figDesc>Opt. Saf. TE all RE all Recall TE RE</figDesc><table><row><cell>FGR [47]</cell><cell></cell><cell>42.7 0.11 4.08</cell></row><row><cell>RANSAC [32]</cell><cell></cell><cell>74.9 0.09 2.92</cell></row><row><cell>DCP [40]</cell><cell></cell><cell>3.2 0.21 8.42</cell></row><row><cell>PointNetLK [2]</cell><cell></cell><cell>1.6 0.21 8.04</cell></row><row><cell>DGR [7]</cell><cell cols="2">0.47 17.4 73.9 0.09 2.97</cell></row><row><cell>DGR [7]</cell><cell cols="2">0.38 13.4 81.5 0.08 2.56</cell></row><row><cell>DGR [7]</cell><cell cols="2">0.33 11.8 86.6 0.07 2.34</cell></row><row><cell>DGR [7]</cell><cell>0.25</cell><cell>9.5 91.2 0.07 2.42</cell></row><row><cell>PCAM-Sparse</cell><cell cols="2">0.44 16.3 74.9 0.08 2.98</cell></row><row><cell>PCAM-Sparse</cell><cell cols="2">0.35 13.0 83.8 0.07 2.43</cell></row><row><cell>PCAM-Sparse</cell><cell cols="2">0.32 11.8 87.0 0.07 2.11</cell></row><row><cell>PCAM-Sparse</cell><cell>0.23</cell><cell>8.9 92.4 0.07 2.16</cell></row><row><cell>PCAM-Soft</cell><cell cols="2">0.46 17.5 74.7 0.09 3.01</cell></row><row><cell>PCAM-Soft</cell><cell cols="2">0.37 13.1 81.4 0.08 2.50</cell></row><row><cell>PCAM-Soft</cell><cell cols="2">0.33 11.9 85.6 0.07 2.12</cell></row><row><cell>PCAM-Soft</cell><cell>0.24</cell><cell>9.8 91.3 0.07 2.25</cell></row></table><note>. Results on 3DMatch test split. The recall is computed using 0.3 m and 15 ? as thresholds. The scores of all variants of DGR are obtained using the official implementation of DGR</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>?</head><label></label><figDesc>Opt. ICP TE all RE all Recall TE RE</figDesc><table><row><cell>FGR [47]</cell><cell>0.2 0.41 1.02</cell></row><row><cell>RANSAC [32]</cell><cell>34.2 0.26 1.39</cell></row><row><cell>FCGF [8]</cell><cell>98.2 0.1 0.33</cell></row><row><cell>DGR [7]</cell><cell>0.77 2.32 63.1 0.28 0.56</cell></row><row><cell>DGR [7]</cell><cell>0.76 2.32 63.4 0.28 0.56</cell></row><row><cell>DGR [7]</cell><cell>0.34 1.62 96.6 0.21 0.33</cell></row><row><cell>DGR [7]</cell><cell>0.16 1.43 98.2 0.03 0.14</cell></row><row><cell>PCAM -Soft</cell><cell>0.18 1.00 97.2 0.08 0.33</cell></row><row><cell>PCAM -Sparse</cell><cell>0.22 1.17 96.5 0.08 0.31</cell></row><row><cell>PCAM -Soft</cell><cell>0.12 0.79 98.0 0.03 0.14</cell></row><row><cell>PCAM -Sparse</cell><cell>0.17 1.04 97.4 0.03 0.14</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Rec. (%) RE all TE all Rec. (%) RE all</figDesc><table><row><cell></cell><cell></cell><cell>TE all</cell><cell>RMSE (R) (?10 ?3 )</cell><cell>RMSE (t) (?10 ?3 )</cell></row><row><cell></cell><cell></cell><cell>2 53.7 (1.8) 36.4 (1.9) 0.49 (0.03) 93.4 (0.6) 3.0 (0.2) 0.45 (0.02)</cell><cell>18 (1.2)</cell><cell>0.13 (0.002)</cell></row><row><cell></cell><cell></cell><cell>2 52.3 (0.6) 36.3 (1.0) 0.49 (0.01) 94.3 (0.5) 2.5 (0.1) 0.42 (0.02)</cell><cell>15 (2.7)</cell><cell>0.09 (0.014)</cell></row><row><cell>Soft map</cell><cell></cell><cell cols="2">2 45.9 (1.5) 41.9 (2.5) 0.55 (0.03) 93.7 (0.3) 3.0 (0.1) 0.44 (0.01) 2 19.4 (2.7) 67.4 (1.5) 0.85 (0.02) 90.9 (1.1) 3.1 (0.1) 0.52 (0.04) 494 (218) 18 (1.1) 2 37.1 (2.9) 47.9 (2.2) 0.65 (0.02) 90.4 (0.9) 3.5 (0.1) 0.61 (0.08) 38 (3.9)</cell><cell>0.12 (0.015) 1.78 (0.879) 0.28 (0.007)</cell></row><row><cell></cell><cell></cell><cell>2 37.3 (1.4) 47.2 (0.6) 0.65 (0.01) 92.7 (0.4) 2.8 (0.2) 0.53 (0.01)</cell><cell>66 (10.5)</cell><cell>0.39 (0.010)</cell></row><row><cell></cell><cell></cell><cell>6 82.4 (0.3) 13.5 (0.3) 0.21 (0.01) 95.4 (0.3) 2.2 (0.1) 0.33 (0.01)</cell><cell>16</cell><cell>0.11</cell></row><row><cell></cell><cell></cell><cell>6 76.6 (1.9) 18.2 (0.8) 0.27 (0.01) 93.5 (0.8) 2.9 (0.03) 0.46 (0.03)</cell><cell>22</cell><cell>0.16</cell></row><row><cell></cell><cell></cell><cell>6 64.4 (1.1) 27.8 (0.6) 0.38 (0.01) 93.9 (0.0) 2.0 (0.1) 0.33 (0.01)</cell><cell>28</cell><cell>0.16</cell></row><row><cell></cell><cell>N/A</cell><cell>2 52.4 (2.1) 38.6 (2.0) 0.51 (0.02) 94.6 (0.5) 2.8 (0.5) 0.39 (0.02)</cell><cell>19 (1.9)</cell><cell>0.10 (0.016)</cell></row><row><cell>Sparse map</cell><cell>N/A N/A N/A N/A</cell><cell>2 48.5 (1.0) 39.4 (1.5) 0.53 (0.02) 94.4 (0.4) 2.6 (0.4) 0.36 (0.03) 2 54.1 (3.8) 36.0 (2.7) 0.49 (0.03) 94.2 (0.5) 2.6 (0.2) 0.43 (0.05) 2 50.9 (1.3) 38.9 (1.5) 0.52 (0.02) 94.6 (0.8) 2.7 (0.3) 0.43 (0.03) 6 81.6 (1.2) 16.3 (1.2) 0.24 (0.01) 95.9 (0.3) 2.3 (0.05) 0.33 (0.01)</cell><cell>24 (6.6) 15 (0.4) 17 (1.3) 36</cell><cell>0.14 (0.031) 0.09 (0.002) 0.09 (0.004) 0.20</cell></row><row><cell></cell><cell>N/A</cell><cell>6 78.4 (1.1) 17.5 (1.3) 0.24 (0.01) 96.5 (0.3) 2.4 (0.1) 0.46 (0.003)</cell><cell>27</cell><cell>0.15</cell></row><row><cell></cell><cell>N/A</cell><cell>6 68.0 (1.1) 25.9 (0.5) 0.34 (0.01) 95.7 (0.3) 1.9 (0.1) 0.33 (0.01)</cell><cell>18</cell><cell>0.10</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table</head><label></label><figDesc></figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Number of channels c ( ) at the ouput of the th block in the multiscale attention module. c (0) = 3 corresponds to the three coordinates x, y, z of the input point cloud.</figDesc><table /><note>A.2. Confidence Estimator h(?, ?)</note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work was partly performed using HPC resources from GENCI-IDRIS (Grant 2020-AD011012040). We thank Raoul de Charette for his constructive feedbacks on an earlier version of this paper.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sorrenti. Point Clouds Registration with Probabilistic Data Association</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Agamennoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fontana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Siegwart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4092" to="4098" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Point-NetLK: Robust &amp; Efficient Point Cloud Registration Using PointNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aoki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Goforth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Srivatsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="7156" to="7165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A method for registration of 3-D shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Besl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Mckay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="239" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sparse Iterative Closest Point</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bouaziz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tagliasacchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pauly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="113" to="123" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">FKAConv: Feature-Kernel Alignment for Point Cloud Convolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Boulch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Puy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Marlet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Object modeling by registration of multiple range images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2724" to="2729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep Global Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="2511" to="2520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fully convolutional geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="8957" to="8965" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04523</idno>
		<title level="m">Learning 3D-3D Correspondences for One-Shot Partial-to-partial Registration</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">PPFNet: Global Context Aware Local Features for Robust 3D Point Matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ilic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="195" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">StickyPillars: Robust and Efficient Feature Matching on Point Clouds Using Graph Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Olsner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Milz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mader</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="313" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust Registration of 2D and 3D Point Sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1145" to="1153" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Are we ready for Autonomous Driving? The KITTI Vision Benchmark Suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Multiview 3D Point Cloud Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Birdal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The Perfect Match: 3D Point Cloud Matching With Smoothed Densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5540" to="5549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Generalized procrustes analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gower</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="33" to="51" />
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">PREDATOR: Registration of 3D Point Clouds with Low Overlap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Gojcic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Usvyatsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2021" />
			<biblScope unit="page" from="4267" to="4276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Globally Optimal Object Pose Estimation in Point Clouds with Mixed-Integer Programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Izatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tedrake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Robotics Research</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using spin images for efficient object recognition in cluttered 3d scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">E</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="433" to="449" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning Compact Geometric Features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Khoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="153" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Registration loss learning for deep probabilistic point set registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Lawin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-E</forename><surname>Forss?n</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Iterative Distance-Aware Similarity Matrix Convolution with Mutual-Supervised Point Elimination for Efficient Point Cloud Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An Iterative Image Registration Technique with an Application to Stereo Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International joint conference on Artificial intelligence</title>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="674" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully automatic registration of 3d point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makadia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1297" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Point Registration via Efficient Convex Relaxation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dym</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kezurer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kovalsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">A Review of Point Cloud Registration Algorithms for Mobile Robotics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pomerleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Colas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Siegwart</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Certifiably Correct Algorithm for Synchronization over the Special Euclidean Group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Carlone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Bandeira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Leonard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on the Algorithmic Foundations of Robotics (WAFR)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="volume">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient variants of the ICP algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rusinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Levoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3-D Digital Imaging and Modeling</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fast Point Feature Histograms (FPFH) for 3D registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="3212" to="3217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">SHOT: Unique Signatures of Histograms for Surface and Texture Description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">125</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generalized-ICP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>H?hnel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Precision range image registration using a robust surface interpenetration measure and enhanced genetic algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">R P</forename><surname>Bellon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Boyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Unique shape context for 3d data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM workshop on 3D object retrieval</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A correlation-based approach to robust point set registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="558" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PRNet: Self-Supervised Learning for Partial-to-Partial Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8814" to="8826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Deep Closest Point: Learning Representations for Point Cloud Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3522" to="3531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">3D ShapeNets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Go-ICP: A Globally Optimal Solution to 3D ICP Point-Set Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2241" to="2254" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Rpm-net: Robust point matching using learned features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">J</forename><surname>Yew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deepgmr: Learning latent gaussian mixture models for registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Eckart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="733" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nie?ner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<title level="m">3DMatch: Learning Local Geometric Descriptors from RGB-D Reconstructions. In Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Iterative point matching for registration of freeform curves and surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal on Computer Vision</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="152" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fast Global Registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q.-Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="766" to="782" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Correspondence matrices are underrated</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zodage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chakwate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sarode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Srivatsan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choset</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on 3D Vision (3DV)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
