<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Multipart Learning for Action Recognition in Depth Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Student Member, IEEE</roleName><forename type="first">Shahroudy</forename><surname>Amir</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Gang</forename><surname>Wang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Tian-Tsong</forename><surname>Ng</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>Member, IEEE</roleName><forename type="first">Qingxiong</forename><surname>Yang</surname></persName>
						</author>
						<title level="a" type="main">Multimodal Multipart Learning for Action Recognition in Depth Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T05:33+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-Action recognition</term>
					<term>Kinect</term>
					<term>Joint sparse regression</term>
					<term>Mixed norms</term>
					<term>Structured sparsity</term>
					<term>Group feature selection</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The articulated and complex nature of human actions makes the task of action recognition difficult. One approach to handle this complexity is dividing it to the kinetics of body parts and analyzing the actions based on these partial descriptors. We propose a joint sparse regression based learning method which utilizes the structured sparsity to model each action as a combination of multimodal features from a sparse set of body parts. To represent dynamics and appearance of parts, we employ a heterogeneous set of depth and skeleton based features. The proper structure of multimodal multipart features are formulated into the learning framework via the proposed hierarchical mixed norm, to regularize the structured features of each part and to apply sparsity between them, in favor of a group feature selection. Our experimental results expose the effectiveness of the proposed learning method in which it outperforms other methods in all three tested datasets while saturating one of them by achieving perfect accuracy.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Human actions consist of simultaneous flow of different body parts. Based on this complex articulated essence of human movements, the analysis of these signals could be highly complicated. To ease the task of classification, actions could be broken down into their components. This is done by a body part detection on depth sequences of human body movements <ref type="bibr" target="#b0">[1]</ref>. Having the 3D locations of body joints in the scene, we can separate the complicated motion of body into a concurrent set of behaviors on major skeleton joints; therefore human action sequences could be considered as multipart signals. Throughout this paper, we use the term "part" to denote each body joint as defined in <ref type="bibr" target="#b0">[1]</ref>.</p><p>Limiting the learning into skeleton based features cannot deliver high levels of performance in action recognition, because: <ref type="bibr" target="#b0">(1)</ref> most of the usual human actions are defined based on the interaction of body with other objects, and (2) depth based skeleton data is not always accurate due to the noise and occlusion of body parts. To alleviate these issues, different depth based appearance features can be leveraged. The work in <ref type="bibr" target="#b1">[2]</ref> proposed LOP (local occupancy patterns) around each of the body joints in order to represent 3D appearance of the interacting objects. Another solution is HON4D (histogram of oriented 4D normals) <ref type="bibr" target="#b2">[3]</ref>, which gives more descriptive and robust models of the local depth based appearance and motion, around the joints. Based on the complementary properties of mentioned features, it is beneficial to utilize all of them as different descriptors for each joint. Combining heterogeneous features of each part of the skeleton, leads into a multimodalmultipart combination, which demands sophisticated fusion algorithms.</p><p>An interesting approach to handle the articulation of actions was recently proposed by <ref type="bibr" target="#b1">[2]</ref>. As the key intuition, they have shown each individual action class can be represented by the behavior and appearance of few informative joints in the body. They utilized a data mining technique to find these discriminative sets of joints for each class of the available actions and tied up the features of those parts as "actionlets". They employed a multi-kernel learning method to build up ensembles of actionlets as kernels for action classification. This method is highly robust against the noise in depth maps, and the results show its strength to characterize the human body motion and also human-object interactions. However the downside of this approach is the inconsistency of their heuristic selection process (mining actionlets) with the following learning step. Moreover, it simply concatenates different types of features for multimodal fusion, which is another drawback of this work. In this fashion, achieving the optimal combination of features regarding the classification task cannot be guaranteed.</p><p>To overcome the limitations mentioned above, we propose a joint structured sparsity regression based learning method which integrates part selection into the learning process considering the heterogeneity of features for each joint. We associate all the features for each part as a bundle and apply a group sparsity regularization to select a small number of active parts for each action class. To model the precise hierarchy of the multimodal-multipart features in an integrated learning and selection framework, we propose a hierarchical mixed norm which includes three levels of regularization over learning weights. To apply the modality based coupling over heterogeneous features of each part, it applies a mixed norm with two degrees of "diversity" induction <ref type="bibr" target="#b3">[4]</ref>, followed by a group sparsity among the feature groups of different parts to apply part selection.</p><p>The main contributions of this paper are two-fold: First, we integrated the part selection process into our learning in order to select discriminative body parts for different action classes latently, and utilize them to learn classifiers. Second, a hierarchical mixed norm is proposed to apply the desired simultaneous sparsity and regularization over different levels of learning weights corresponding to our special multimodal-multipart features in a joint group sparsity regression framework.</p><p>We evaluate our method on three challenging depth based action recognition datasets: MSR-DailyActivity dataset <ref type="bibr" target="#b1">[2]</ref>, MSR-Action3D dataset <ref type="bibr" target="#b4">[5]</ref>, and 3D-ActionPairs dataset <ref type="bibr" target="#b2">[3]</ref>. Our experimental results show that the proposed method is superior to other available methods for action recognition on depth sequences. The rest of this paper is organized as follows: Section 2 reviews the related works on depth based action recognition, joint sparse regression, mixed norms, and multitask learning. Section 3 presents the proposed integrated feature selection and learning scheme. It also introduces the new multimodal-multi part mixed norm which applies regularization and group sparsity into the proposed learning model. Experimental results on three above-mentioned benchmarks are covered in section 4 and we conclude the paper in section 5. arXiv:1507.08761v1 [cs.CV] 31 Jul 2015 2 RELATED WORK Visual features extracted from depth signals can be classified into two major classes. The first are skeleton based features, which extract information from the provided 3D locations of body joints on each frame of the sequence. Essentially, skeletons have a very succinct and highly discriminative representation of the actions. <ref type="bibr" target="#b5">[6]</ref> utilized them to extract "eigenjoints" for action classification using a na?ve-bayes-nearest-neighbor classifier. In <ref type="bibr" target="#b6">[7]</ref> spherical histograms of 3D locations of the joints went through HMM to model the temporal changes and final action classification. Presence of noise in depth maps and occlusion of body parts bounds the reliability of this type of features. Another major deficiency of skeleton data is their incapacity to represent the interactions of the body with other objects which is crucial for activity interpretation.</p><p>The other group, consists of features which are extracted directly from depth maps. Most of the features in this class consider depth maps as spatio-temporal signals and tried to extract local or holistic descriptions from input sequences. <ref type="bibr" target="#b4">[5]</ref> proposed a depth based action graph model in which each node indicates a salient posture and actions were represented as paths through graph nodes. To deal with occlusion and noise issues in depth maps, <ref type="bibr" target="#b7">[8]</ref> proposed "random occupancy pattern" features and applied an elastic-net regularization <ref type="bibr" target="#b8">[9]</ref> to find the most discriminative subset of features for action recognition. STIP (space-time interest point) detection described by HOG (histogram of oriented gradients) <ref type="bibr" target="#b9">[10]</ref> and HOF (histogram of optical flow) was originally proposed for recognition purposes on RGB videos <ref type="bibr" target="#b10">[11]</ref>, but <ref type="bibr" target="#b11">[12]</ref> showed this could be easily generalized into RGB+D signals. To improve the discrimination of descriptors, they generalized the idea of "motion history images" [13] over depth maps. Noise-suppression could also boost up the performance of STIP detection on depth sequences <ref type="bibr" target="#b13">[14]</ref>. Four dimensional surface normals were shown to be very powerful representations of body movements over depth signals <ref type="bibr" target="#b2">[3]</ref>. This idea was a generalization of HOG3D <ref type="bibr" target="#b14">[15]</ref> into four dimensional depth videos. They quantized the 4D normal vectors of depth surfaces by taking their histograms over the vertices of a 4D regular polychoron, which were shown to be highly informative for action classification.</p><p>Regarding the strengths and weaknesses of aforementioned classes of features, we infer they are complementary to each other and to achieve higher levels of performance, we have to combine them. <ref type="bibr" target="#b1">[2]</ref> used histograms of 3D point clouds around the joints (LOP) to be added into skeleton based features for action classification using an "actionlet ensemble" framework. <ref type="bibr" target="#b15">[16]</ref> added local HON4D <ref type="bibr" target="#b2">[3]</ref> into joint features to learn a maxmargin temporal warping based action classifier. We utilize skeletons, LOP and HON4D as state-of-the-art depth based features to build up our multimodal input for the task of action recognition.</p><p>The main intuition behind the work of <ref type="bibr" target="#b1">[2]</ref> was the fact that features of few informative joints are good enough for recognizing each class of the actions. They defined "actionlet" as the combination of features of a limited numbers of joints and based on the discriminative power of each joint and each actionlet, they performed a data mining procedure to find the best actionlets for each class of the actions. They used mined actionlets as kernels in a multi-kernel multiclass SVM. We further extend this idea by applying group sparsity in a joint feature selection framework. To do so, we group the features of each part (joint) and applied L 1 norm between these groups to achieve a sparse set of active parts to represent each action class.</p><p>Mixed norms are powerful tools to inject simultaneous sparsity and coupling effects between the learning coefficients. They have been studied in a variety of fields. In statistical domain, <ref type="bibr" target="#b16">[17]</ref> proposed the "group Lasso", as an extension over "Lasso" <ref type="bibr" target="#b17">[18]</ref> for a grouped variable selection in regression. <ref type="bibr" target="#b18">[19]</ref> introduced "composite absolute penalty" for hierarchical variable selection. "Hierarchical penalization" is also proposed to utilize prior structure of the variables for a better fitting model <ref type="bibr" target="#b19">[20]</ref>. In sparse regression, mixed norms have been used as regularization terms to link sparsity and persistence of variables <ref type="bibr" target="#b20">[21]</ref>. A generalized shrinkage scheme was proposed by <ref type="bibr" target="#b21">[22]</ref> for structured sparse regression. <ref type="bibr" target="#b22">[23]</ref> used mixed norms as structured sparsity regularizers for heterogeneous feature fusion, and <ref type="bibr" target="#b23">[24]</ref> extended this idea for a multi-view clustering. <ref type="bibr" target="#b24">[25]</ref> proposed a robust self-taught learning using mixed norms and <ref type="bibr" target="#b25">[26]</ref> utilized a fractional mixed norm for robust adaptive dictionary learning. In this paper, to regularize the multimodal features of each part, we apply a mixed L 2 /L 4 norm. To achieve the sparsity between parts, we generalize this into an</p><formula xml:id="formula_0">L 1 /L 2 /L 4 hierarchical norm.</formula><p>If multiple learning tasks at hand share some inherent constituents or structures, "Multitask Learning" <ref type="bibr" target="#b26">[27]</ref> techniques could be globally beneficial. In joint sparse regression, multitask learning is formulated by a mixed norm. <ref type="bibr" target="#b27">[28]</ref> proposed an L 1 /L ? norm to add this into Lasso for variable selection. In joint feature selection, L 1 /L 2 norm can provide multitask learning by applying selection between the L 2 regularized parameters of each feature <ref type="bibr" target="#b28">[29]</ref>. Same is used in <ref type="bibr" target="#b29">[30]</ref> as a generalization of L 1 norm in a multitask joint sparsity representation model to fuse complementary visual features across recognition tasks. <ref type="bibr" target="#b30">[31]</ref> studied different mixed norms when they applied multitask sparse learning in visual tracking and based on their experimental results, they showed L 1 /L 2 is superior among them. In this work, we use a similar norm to utilize the shared latent factors between different binary action classifiers. We apply L 2 regularization over the weights corresponding to each feature across all the tasks, followed by an L 1 between all the features at hand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MULTIMODAL MULTIPART LEARNING Notations</head><p>Throughout this paper, we use bold uppercase letters to represent matrices and bold lowercase letters to indicate vectors. For a matrix X, we denote its j-th row as x j and its i-th column as xi.</p><p>Assume the partition ? is defined over a vector z to divide its elements into |?| disjoint sets. We use ?i to represent the indices of i-th set in ?, and its corresponding elements in z are referred to as z ? i , also z ? i ,k represents the k-th element of z ? i . The L p /L q norm of z regarding ? is represented by z q,p|? and is defined as the L q norms of the elements inside each set of ? followed by an L p norm of the L q values across the sets; mathematically:</p><formula xml:id="formula_1">z q,p|? = ? ? |?| i=1 z ? i p q ? ? 1/p = ? ? ? |?| i=1 ? ? |? i | k=1 |z ? i ,k | q ? ? p/q ? ? ? 1/p (1) in which |?i| indicates the cardinality of set ?i.</formula><p>Now consider the elements of each set ?i are further partitioned by operator ? into |?| disjoint subsets. Similarly, we indicate j-th ?-subset of i-th ?-set of z as z ? i ,? j and z ? i ,? j ,k represents its k-th element. The L p /L q /L r norm of z regarding ? and ? is also represented by z r,q,p|?,? and is defined as the L q /L r norms (regarding ?) of all |?| sets followed by an L p norm of the L q /L r values across the sets of ?; mathematically:</p><formula xml:id="formula_2">z r,q,p|?,? = ? ? |?| i=1 z ? i p r,q|? ? ? 1/p = ? ? ? |?| i=1 ? ? ? |?| j=1 ? ? |? j | k=1 |z ? i ,? j ,k | r ? ? q/r ? ? ? p/q ? ? ? 1/p (2)</formula><p>This representation can be easily extended into higher orders of structural mixed norms by further partitioning the subsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multipart Learning by Structured Sparsity</head><p>Our purpose of learning is to recognize the actions in depth videos, based on depth based and skeleton based features extracted. The set of input features we use to describe each action sample is a combination of multimodal multipart features. The entire body is separated into a number of parts (as illustrated in <ref type="figure" target="#fig_0">Fig.1</ref>) and for each part we have different types of features to represent the movement and local depth appearance. Therefore, our input feature set for each input sample, can be represented by a vector: z ? R d , which consists of feature groups of different parts and modalities. Assume operator ? is partitioning z into P parts, and ? is defined over sets of ? to further partition them based on M number of features modalities. So, the hierarchy of features inside this vector is indicated by:</p><formula xml:id="formula_3">z = [z ? 1 T , ..., z ? P T ] T , in which each z ? i = [z ? 1 ,? i T , ..., z ? M ,? i T ] T .</formula><p>Now the problem of multiclass action recognition can be considered as multiple binary regression based classification problems in a one versus all manner. Given n training samples X = [x1, ..., xn] in which xi ? R d and their corresponding labels for C distinct classes: Y = [y1, ..., yC ] with yc ? {0, 1} n and ?i : C c=1 y i c = 1; we are looking for a projection matrix W * ? R d?C which minimizes a set of loss functions J c ( xi, w * c , y i c ) for all classes c ? {1, ..., C} and samples i ? {1, ..., n}. Our choice for the total loss function, without loss of generality, is sum of squared errors (?c :</p><formula xml:id="formula_4">J c (a, b) = (a ? b) 2 ).</formula><p>The most common shrinkage methods to regularize the learning weights against overfitting are to penalize L p norms of the learning weights for each class:</p><formula xml:id="formula_5">w * c = argmin wc n i=1 J c ( xi, wc , y i c ) + ? wc p<label>(3)</label></formula><p>in which ? is the regularization factor. Employing L 2 norm (p = 2) leads into a general weight decay and minimization of the magnitude of W, and applying L 1 norm (p = 1) yields simultaneous shrinkage and sparsity among the individual features. Such methods simply ignore the structural information between the features, which can be useful for classification; therefore, it is beneficial to embed these feature relations into our learning scheme via structured sparsity inducing mixed norms.</p><p>In the context of depth based action recognition, features are naturally partitioned into parts. "Actionlet ensemble" method <ref type="bibr" target="#b1">[2]</ref> tried to discover discriminative joint groups using a data We combine two levels of regularization inside modality groups and between them for each part, followed by a sparsity inducing norm between the parts to apply part selection.</p><p>mining process, which led into an interesting improvement on the performance; however, their heuristic selection process is discrete and separated from the following learning step.</p><p>To address these issues, we propose to apply group sparsity to perform part selection and classification in a regression based framework, in contrast to the mining based joint group discovery of <ref type="bibr" target="#b1">[2]</ref>. We know that the discriminative strength of features in each part are highly correlated regarding all the classes at hand. So we expect the corresponding learning parameters (elements of each wc) to be triggered or halted concurrently within each set of ? partitioning (for each action class). To apply a grouping effect on these features, we consider each set in ? as a unit and measure its strength with an L 2 norm of the included learning weights. On the other hand, we seek a sparse set of parts to be activated for each class at hand, so we apply an L 1 norm between the L 2 values of the groups. Such an intuition can be formulated by an L 1 /L 2 mixed norm based on ? for each class:</p><formula xml:id="formula_6">w * c = argmin wc n i=1 J c ( xi, wc , y i c ) + ? wc 2,1|?<label>(4)</label></formula><p>Adding this up for all the action classes with the same regularization factor, we have:</p><formula xml:id="formula_7">W * = argmin W C c=1 n i=1 J c ( xi, wc , y i c ) + ? C c=1 wc 2,1|? = argmin W J(X T W, Y) + ? vec(W) 2,1,1|?,?<label>(5)</label></formula><p>in which vec(.) is the vectorization operator and ? is the partitioning operator of vec(W) elements based on their corresponding tasks (or columns here): ?(k, c) : ? (w k c ) = c. We will refer to this multipart learning method as "MP".</p><p>Minimization of (5) applies the desired grouping effect into the features of each part and guarantees the sparsity on the number of active parts for each class in a smooth and simpler way, compared to the actionlet method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multimodal Multipart Learning via Hierarchical Mixed Norm</head><p>In the above formulation, we apply an L 2 regularization norm over heterogeneous features of all the modalities for each part, and ignore the modality structures between them. In other words, applying a general L 2 norm may cause the suppression of the information at some dimensions. These issues are more severe when training samples are limited (which is the case for action recognition in depth), in which it might lead to weak generalization of the learning.</p><p>To overcome these limitations, we utilize L ? to regularize the coefficients inside each modality, so that "diversity" <ref type="bibr" target="#b20">[21]</ref> can be encouraged. It is already known that the behavior of L p norm for p &gt; 2 rapidly moves towards L ? <ref type="bibr" target="#b31">[32]</ref>; since L ? is not easy to optimize directly, we picked L 4 as the most efficient approximation of it. Higher order norms like L 6 apply the same effect but with a slightly more expensive processing cost.</p><p>By applying the L 4 norm to regularize the weights in each modality group of each part, now we have a three-level L 1 /L 2 /L 4 mixed norm. Inner L 4 gives more "diversity" to regularize the features inside each partiality-modality subset. L 2 norm employs a magnitude based regularization over the L 4 values to link different modalities of each part, and the outer L 1 applies the soft part selection between the L 2 /L 4 values of each action class <ref type="figure" target="#fig_0">(Fig.1)</ref>.</p><p>Replacing the previous structured norm by the proposed hierarchical mixed norm in (5), we have: here, ? indicates the partitioning of features based on their source body part, and ? represents further partitioning of each part's set regarding the modalities of the features. In the rest of this paper, we use the abbreviation "MMMP" to refer to this method. It is worthwhile to note, changing the inner norm to L 2 will reduce the hierarchical norm into a two level mixed norm, i.e. vec(W) 2,2,1,1|?,?,? = vec(W) 2,1,1|?,? derived directly from the definition of hierarchical norm <ref type="bibr" target="#b1">(2)</ref>.</p><formula xml:id="formula_8">W * = argmin W J(X T W, Y) + ?</formula><p>When different learning tasks have similar latent features, "Multitask Learning" <ref type="bibr" target="#b26">[27]</ref> techniques can improve the performance of the entire system by applying information sharing between the tasks. Here we are learning classifiers for C different classes which essentially have lots of latent components in common, so pushing them to share some features is beneficial for the classification task. This can be done by applying an L 2 grouping on all the weights corresponding to each individual feature. Each of these L 2 values represents the magnitude of strength for its corresponding feature among all the tasks. Then applying an L 1 over the magnitudes can apply a shared variable selection considering all the tasks. Adding the new multitask term into (6), we have:</p><formula xml:id="formula_9">W * = argmin W J(X T W, Y) + ?1 d k=1 w k 2 +?2 vec(W) 4,2,1,1|?,?,?<label>(7)</label></formula><p>= argmin</p><formula xml:id="formula_10">W J(X T W, Y) + ?1 vec(W) 2,1|? +?2 vec(W) 4,2,1,1|?,?,?<label>(8)</label></formula><p>here, d is the number of rows in W which is equal to the size of the entire feature vector, and ? defines the partitioning of vec(W) elements based on their corresponding individual features: ?(k, c) : ?(w k c ) = k. Combining these two regularization terms can be considered as a trade off between sparsity and persistence of features <ref type="bibr" target="#b32">[33]</ref> based on their relations across the parts, modalities, and between the action classes.</p><p>In our experiments, we use P = 20 body joints as partitioning operator ?. Since each column of W has the same hierarchical partitioning as input features: W = [w j c ], in which c counts the number of classes and j counts the feature groups for P joints. The features for each joint come from M = 3 different modalities: skeletons, LOP, and HON4D; this defines the ? operator. Therefore, each is the corresponding weight elements to class c, joint j and modality m. This way (8) will be expanded to:</p><formula xml:id="formula_11">W * = argmin W X T W ? Y 2 F + ?1 d k=1 w k 2 + ?2 C c=1 P j=1 ( M m=1 w j,m c 2 4 ) 1/2 (9)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Two Step Learning Approach</head><p>The downside of current formulation is the large number of weights to be learned simultaneously, compared to the size of training samples which are highly limited in current depth based action recognition benchmarks. To resolve this, we first learn the partially optimum weights for multipart features of each modality separately and then fine-tune them by the proposed multimodal multipart learning.</p><p>To learn the partially optimum weights for each modality m, we optimize:</p><formula xml:id="formula_12">Wm = argmin Wm J(X T m Wm, Y) +?1 vec(Wm) 2,1|?</formula><p>+?2 vec(Wm) 2,1,1|?,?</p><p>After achieving the partially optimum point for each modality, we merge the Wm values for all M modalities:</p><formula xml:id="formula_14">W = [ W T 1 , ..., W T M ] T<label>(11)</label></formula><p>Next is to fine-tune the weights in the multimodal-multipart learning fashion, on a neighborhood of W values. To do so, we expect the global optimum weight not to diverge too much from their partially optimal points:</p><formula xml:id="formula_15">W * = argmin W J(X T W, Y) + ?1 vec(W) 2,1|? +?2 vec(W) 4,2,1,1|?,?,? + ?3 W ? W 2 F (12)</formula><p>The last term in <ref type="bibr" target="#b11">(12)</ref> will limit the deviation of learning weights from their partially optimal point, as we expect them to be just fine-tuned in this step.</p><p>Upon optimization over training data, the detection of the learned classifier for each testing sample xi can be obtained by:</p><formula xml:id="formula_16">f (xi) = argmax c xi, w * c<label>(13)</label></formula><p>The optimization steps are all done by "L-BFGS" algorithm using off-the-shelf "minFinc" tool <ref type="bibr" target="#b33">[34]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>This section describes our experimental setup details and then provides the results of the proposed method on three depth based action recognition benchmarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>All the provided experiments are done on Kinect based datasets. Kinect captures RGB frames, depth map signals and 3D locations of major joints. To have a fair comparison with other depth based methods, we ignore the RGB signals. Skeleton extraction is done automatically by Kinect's SDK based on the part-based human pose recognition system of <ref type="bibr" target="#b0">[1]</ref>. On each frame, we have an estimation of 3D positions of 20 joints in the body. All of our features are defined based on these joints as the multipart partitioning operator (?); therefore, each feature necessarily belongs to one of these parts. To represent skeleton based features, first we normalize the 3D locations of joints against size, position and direction of the body in the scene. This normalization step eases the task of comparison between body poses. On the other hand, the extracted body locations and directions could also be highly discriminative for some action classes like "walking" or "lying down"; therefore we add them into the features under a new auxiliary part. To encode the dynamics of skeleton based features, we apply "Fourier temporal pyramid" as suggested by <ref type="bibr" target="#b1">[2]</ref> and keep first four frequency coefficients of each short time Fourier transformation. This leads into a feature vector of size 1,876 for each action sample.</p><p>In addition to skeleton based features, other modalities we use are local HON4D <ref type="bibr" target="#b2">[3]</ref> and LOP <ref type="bibr" target="#b1">[2]</ref> to represent depth based local dynamics and appearance around each joint. On each frame, LOPs are extracted on a (96,96,320)-sized depth neighborhood of each joint, which is divided into 3?3?4 number of (32,32,80)-sized bins. To represent LOP based kinetics, we use a similar Fourier temporal pyramid transformation. HON4D features are also extracted locally over the location of joints on each frame. We encode HON4D features using LLC (localityconstrained linear coding) <ref type="bibr" target="#b34">[35]</ref> to reduce their dimensionality while preserving the locality of 4D surface normals. Dictionary size of 100 is picked for the clustering step. LLC codes go through a max pooling over a 3 level temporal pyramid. Dimension of the features for LOP and HON4D are 5,040 and 14,000 respectively. The overall dimensionality of input features for each sample is 20,916.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">MSR-DailyActivity3D Dataset</head><p>According to its intra-class variations and choices of action classes, MSR-DailyActivity dataset <ref type="bibr" target="#b1">[2]</ref>, is one of the most challenging benchmarks for action recognition in depth sequences. It contains RGB, depth, and skeleton information of 320 action  samples, from 16 classes of daily activities in a living room. Each activity is done by 10 distinct subjects in two different ways and evaluations are applied over a fixed cross-subject setting; first five subjects are taken for training and others for testing. Unlike other datasets, MSR-DailyActivity has a more realistic variation within each class. Subjects used both hands randomly to do the activities, and samples of each class are captured in different poses. First, to verify the strengths of our proposed hierarchical mixed norm, we evaluate the performance of the classification in a subject-wise cross-validation scenario. We evaluate the performance of the plain L 2 norm, the multipart structured norm (MP), and the proposed hierarchical mixed norm (MMMP), in all 252 possible train/test splits of 5 out of 10 subjects. To have a proper comparison between these norms, we have not applied the multitask term. The results of this experiment are shown in <ref type="table" target="#tab_0">Table 1</ref>. Adding part based grouping, when it ignores the modality associations between the features, can slightly improve the performance from 80.61% into 81.55%. By adding multimodality grouping and applying the proposed hierarchical mixed norm, improvement is more significant and reaches 84.03%.</p><p>Next, we verify the results of our method by applying mentioned norms on the standard train/test split of the subjects. As provided in <ref type="table" target="#tab_1">Table 2</ref>, applying simple feature selection using a plain L 1 norm leads into 86.88% of accuracy. By applying a plain L 2 norm on all the features we get 87.50%. Multipart learning regardless of heterogeneity of the modalities leads Dataset on Three Action Subsets of <ref type="bibr" target="#b4">[5]</ref> Method (protocol of <ref type="bibr" target="#b4">[5]</ref>) Accuracy</p><p>Action Graph on Bag of 3D Points <ref type="bibr" target="#b4">[5]</ref> 74.7%</p><p>Histogram of 3D Joints <ref type="bibr" target="#b6">[7]</ref> 79.0%</p><p>EigenJoints <ref type="bibr" target="#b5">[6]</ref> 83.3%</p><p>Random Occupancy Patterns <ref type="bibr" target="#b7">[8]</ref> 86.5%</p><p>Depth HOG <ref type="bibr" target="#b37">[38]</ref> 91.6%</p><p>Lie Group <ref type="bibr" target="#b38">[39]</ref> 92.5% JAS+HOG 2 <ref type="bibr" target="#b39">[40]</ref> 94.8% DL-GSGC+TPM <ref type="bibr" target="#b40">[41]</ref> 96.7%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed MMMP 98.2%</head><p>into 88.13%. Finally by adding the multipart learning via the proposed hierarchical mixed norm we reach the interesting accuracy of 91.25% on this dataset. Applying higher orders for the inner-most norm (like L 1 /L 2 /L 6 ) achieved the same level of accuracy at a slightly higher processing time.</p><p>To assess the strength of the proposed multipart learning, we evaluate our method on single modality setting using <ref type="bibr" target="#b9">(10)</ref>. As shown in <ref type="table" target="#tab_2">Table 3</ref>, on skeleton based features, we got 79.38% compared to 74% of the baseline actionlet method. Using LOPs, our method achieved 79.38% which is more than 18% higher than the actionlet's performance. For local HON4D features, we achieved 81.88% compared to 80.00% of the baseline local HON4D method. Now we use the partially learned weights of single modality multipart learning and employ them for the optimization of (12) to learn globally optimum projections. First we try the combination of skeleton based features with LOP. Using proposed learning, we get 88.13% of accuracy which outperforms the baseline's best result of 85.75%. <ref type="bibr" target="#b15">[16]</ref> used skeleton and HON4D features in a temporal warping framework and got 88.75%. Our method outperforms it using the same set of features by achieving 89.38% of accuracy. And finally using all three modalities, our method achieves the performance level of 91.25%. <ref type="table" target="#tab_2">Table 3</ref> shows the complete set of results for this experiment.</p><p>Our implementation is done in MATLAB, and not fully optimized for time efficiency. The average training and testing time of MMMP on a 3.2 GHz Core-i5 machine are 170 and 2 ? 10 ?4 seconds respectively, with no parallel processing.</p><p>It is worth pointing out some of the published works on this dataset applied other train/test splits, e.g. <ref type="bibr" target="#b36">[37]</ref> reported 93.1% of accuracy on a leave-one-subject-out cross validation. On this setup, proposed MMMP method achieves 97.5%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">MSR-Action3D Dataset</head><p>MSR-Action3D <ref type="bibr" target="#b4">[5]</ref> is another depth based action dataset which provided depth sequences and skeleton information of 567 samples for 20 action classes. Actions are done by 10 different subjects, two or three times each. Evaluations are applied over another fixed cross-subject setting; Odd numbered subjects are taken for training and evens for testing. On one hand, depth sequences in this dataset have clean background which eases the recognition, and on the other hand, number of classes are higher than other datasets which could be a challenge for classification.</p><p>The reported results on this dataset are divided in two different scenarios. First is the average cross subject performance </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method (protocol of [2]) Accuracy</head><p>Depth HOG <ref type="bibr" target="#b37">[38]</ref> (as reported in <ref type="bibr" target="#b15">[16]</ref>) 85.5%</p><p>Actionlet Ensemble <ref type="bibr" target="#b1">[2]</ref> 88.2%</p><p>HON4D <ref type="bibr" target="#b2">[3]</ref> 88.9%</p><p>DSTIP <ref type="bibr" target="#b13">[14]</ref> 89.3%</p><p>Lie Group <ref type="bibr" target="#b38">[39]</ref> 89.5%</p><p>HOPC <ref type="bibr" target="#b41">[42]</ref> 91.6%</p><p>Max Margin Time Warping <ref type="bibr" target="#b15">[16]</ref> 92.7%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed MMMP 93.1%</head><p>on three action subsets defined in <ref type="bibr" target="#b4">[5]</ref>, and second is the overall cross subject accuracy regardless of subsets, as done in <ref type="bibr" target="#b1">[2]</ref>. Following <ref type="bibr" target="#b38">[39]</ref>, we call them as protocols of <ref type="bibr" target="#b4">[5]</ref> and <ref type="bibr" target="#b1">[2]</ref>. <ref type="table" target="#tab_3">Tables  4 and 5</ref> show the results. Although we still have the highest accuracy among the reported results, the achieved margin is not as large as other datasets. This is because of the simplicity of actions in this dataset. Since there is not any interaction with other objects, most of the classes are highly distinguishable using skeleton only features; therefore our multimodality could not boost up the results that much, but the multipart learning still shows its advantage over other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">3D Action Pairs Dataset</head><p>To emphasize the importance of the temporal order of body poses on the meaning of the actions, <ref type="bibr" target="#b2">[3]</ref> proposed 3D Action Pairs dataset. It covers 6 pairs of similar actions. The only difference between each pair is their temporal order so they have similar skeleton, poses, and object shapes. Each action is performed by 10 subjects, 3 times. First five subjects are taken for testing and others for training. Based on the fewer number of the action classes and absence of intra-class variations, this is the easiest benchmark among depth based action recognition datasets and other methods already achieved very high accuracies on it.</p><p>Here we apply our full multimodal multipart learning method using all three available modalities of features. As shown in <ref type="table" target="#tab_5">Table 6</ref>, the proposed method, outperforms all others and saturates the benchmark by achieving the perfect performance level on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>This paper presents a new multimodal multipart learning approach for action classification in depth sequences. We show that a sparse combination of multimodal part-based features can effectively and discriminatively represent all the available action classes at hand. Based on the nature of the problem, we utilize a heterogeneous set of features from skeleton based 3D joint trajectories, depth occupancy patterns and histograms of depth surface normals and show the proper way of using them as multimodal features set for each part.</p><p>The proposed method does the group feature selection, weight regularization, and classifier learning in a consistent optimization step. It applies the proposed hierarchical mixed norm to model the proper structure of multimodal multipart input features by applying a diversity norm over the coefficients of each part-modality group, linking different modalities </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Accuracy</head><p>Depth HOG <ref type="bibr" target="#b37">[38]</ref> (as reported in <ref type="bibr" target="#b15">[16]</ref>) 66.11%</p><p>Actionlet Ensemble <ref type="bibr" target="#b1">[2]</ref> (as reported in <ref type="bibr" target="#b15">[16]</ref>) 82.22%</p><p>HON4D <ref type="bibr" target="#b2">[3]</ref> 96.67%</p><p>Max Margin Time Warping <ref type="bibr" target="#b15">[16]</ref> 97.22%</p><p>HOPC <ref type="bibr" target="#b41">[42]</ref> 98.33%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposed MMMP 100.0%</head><p>of each part by a magnitude based norm, and utilizing a soft part selection by a sparsity inducing norm. The provided experimental evaluations on three challenging depth based action recognition datasets show the proposed method can successfully apply the structure of the input features into a concurrent group feature selection and learning and confirm the strengths of the suggested framework compared to other methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Three Levels of the Proposed Hierarchical Mixed Norm for Multimodal Multipart Learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>T W, Y) + ? vec(W) 4,2,1,1|?,?,? (6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>w j c = [w j, 1 cT</head><label>1</label><figDesc>, ..., w j,M c T ] T , in which each w j,m c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1</head><label>1</label><figDesc>Subject-wise Cross-Validation Performance Comparison of the Proposed Hierarchical Mixed Norm with Plain and MultipartGroup Sparsity Norm on the MSR-DailyActivity Dataset</figDesc><table><row><cell>Method</cell><cell>Structure/Hierarchical Norm Used</cell><cell>Accuracy</cell></row><row><cell>L 2</cell><cell>vec(W) 2 2</cell><cell>80.61?2.49%</cell></row><row><cell>MP</cell><cell>vec(W) 2,1,1|?,?</cell><cell>81.55?2.43%</cell></row><row><cell>MMMP</cell><cell>vec(W) 4,2,1,1|?,?,?</cell><cell>84.03?2.16%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 Performance</head><label>2</label><figDesc>Comparison of the Proposed Method Using Plain/Structured/Hierarchical Norms on the Standard Evaluation Split of the MSR-DailyActivity Dataset</figDesc><table><row><cell>Method</cell><cell>Structure/Hierarchical Norm Used</cell><cell>Accuracy</cell></row><row><cell>L 1</cell><cell>vec(W) 1</cell><cell>86.88%</cell></row><row><cell>L 2</cell><cell>vec(W) 2 2</cell><cell>87.50%</cell></row><row><cell>MP</cell><cell>vec(W) 2,1,1|?,?</cell><cell>88.13%</cell></row><row><cell>MMMP</cell><cell>vec(W) 4,2,1,1|?,?,?</cell><cell>91.25%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="3">Performance Comparison on the Standard Evaluation Split of</cell></row><row><cell cols="3">the MSR-DailyActivity Dataset using Single Modality and</cell></row><row><cell cols="2">Multimodal Features.</cell><cell></cell></row><row><cell>Method</cell><cell>Modalities</cell><cell>Accuracy</cell></row><row><cell>Actionlet Ensemble [2]</cell><cell>LOP</cell><cell>61%</cell></row><row><cell>Proposed MP</cell><cell>LOP</cell><cell>79.38%</cell></row><row><cell>Orderlet Mining [36]</cell><cell>Skeleton</cell><cell>73.8%</cell></row><row><cell>Actionlet Ensemble [2]</cell><cell>Skeleton</cell><cell>74%</cell></row><row><cell>Proposed MP</cell><cell>Skeleton</cell><cell>79.38%</cell></row><row><cell>Local HON4D [3]</cell><cell>HON4D</cell><cell>80.00%</cell></row><row><cell>Proposed MP</cell><cell>HON4D</cell><cell>81.88%</cell></row><row><cell>Actionlet Ensemble [2]</cell><cell>Skeleton+LOP</cell><cell>85.75%</cell></row><row><cell>Proposed MMMP</cell><cell>Skeleton+LOP</cell><cell>88.13%</cell></row><row><cell>MMTW [16]</cell><cell>Skeleton+HON4D</cell><cell>88.75%</cell></row><row><cell>Proposed MMMP</cell><cell>Skeleton+HON4D</cell><cell>89.38%</cell></row><row><cell>DSTIP [14]</cell><cell>DCSF+LOP</cell><cell>88.20%</cell></row><row><cell>Proposed MMMP</cell><cell>Skeleton+LOP+HON4D</cell><cell>91.25%</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 4</head><label>4</label><figDesc>Average Cross Subject Performance for MSR-Action3D</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5 Performance</head><label>5</label><figDesc>Comparison for MSR-Action3D Dataset Over All Action Classes</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 6</head><label>6</label><figDesc>Performance Comparison for 3D Action Pairs Dataset</figDesc><table /><note></note></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning actionlet ensemble for 3d human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hon4d: Histogram of oriented 4d normals for activity recognition from depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An affine scaling methodology for best basis selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kreutz-Delgado</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Action recognition based on a bag of 3d points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Effective 3d action recognition using eigenjoints</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">View invariant human action recognition using histograms of 3d joints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Robust 3d action recognition with random occupancy patterns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Regularization and variable selection via the elastic net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Rgbd-hudaact: A color-depth video database for human daily activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The recognition of human movement using temporal templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bobick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatio-temporal depth cuboid similarity feature for activity recognition using depth camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aggarwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaeser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszalek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning maximum margin temporal warping for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Model selection and estimation in regression with grouped variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Regression shrinkage and selection via the lasso</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The composite absolute penalties family for grouped and hierarchical variable selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rocha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical penalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Szafranski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Morizet-Mahoudeaux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sparse regression using mixed norms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kowalski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Structured Sparsity: from Mixed Norms to Structured Shrinkage</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Torr?sani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal Processing with Adaptive Sparse Structured Representations (SPARS)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Heterogeneous visual features fusion via sparse multimodal machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multi-view clustering and feature learning via structured sparsity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Robust and discriminative selftaught learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised robust dictionary learning via efficient l 2,0 + -norms minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Blockwise coordinate descent procedures for the multi-task lasso, with applications to neural semantic basis discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palatucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint covariate selection and joint subspace selection for multiple classification problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Visual classification with multitask joint sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-T</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Robust visual tracking via multi-task sparse learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">lp ? lq penalty for sparse linear and sparse multiple kernel multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gasso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Canu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Sparsity and persistence: mixed norms provide simple signal models with dependent coefficients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kowalski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Torrsani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Signal, Image and Video Processing</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Minfunc</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<ptr target="http://www.di.ens.fr/?mschmidt/Software/minFunc.html" />
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Localityconstrained linear coding for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Discriminative orderlet mining for real-time recognition of human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Human activity recognition using multi-features and multiple kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Althloothi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Mahoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Voyles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recognizing actions using depth motion maps-based histograms of oriented gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia (MM)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint angles similarities and hog 2 for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ohn-Bar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPR Workshops)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Group sparsity and geometry constrained dictionary learning for action recognition from depth maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Hopc: Histogram of oriented principal components of 3d pointclouds for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
