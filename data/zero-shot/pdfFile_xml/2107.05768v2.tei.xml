<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Combiner: Full Attention Transformer with Sparse Computation Cost</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
							<email>hyren@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
							<email>hadai@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
							<email>zihangd@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengjiao</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dale</forename><surname>Schuurmans</surname></persName>
							<email>schuurmans@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
							<email>bodai@google.com</email>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Google Research, Brain Team</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Alberta</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Combiner: Full Attention Transformer with Sparse Computation Cost</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T17:14+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Transformers provide a class of expressive architectures that are extremely effective for sequence modeling. However, the key limitation of transformers is their quadratic memory and time complexity O(L 2 ) with respect to the sequence length in attention layers, which restricts application in extremely long sequences. Most existing approaches leverage sparsity or low-rank assumptions in the attention matrix to reduce cost, but sacrifice expressiveness. Instead, we propose Combiner, which provides full attention capability in each attention head while maintaining low computation and memory complexity. The key idea is to treat the self-attention mechanism as a conditional expectation over embeddings at each location, and approximate the conditional distribution with a structured factorization. Each location can attend to all other locations, either via direct attention, or through indirect attention to abstractions, which are again conditional expectations of embeddings from corresponding local regions. We show that most sparse attention patterns used in existing sparse transformers are able to inspire the design of such factorization for full attention, resulting in the same sub-quadratic cost (O(L log(L)) or O(L ? L)). Combiner is a drop-in replacement for attention layers in existing transformers and can be easily implemented in common frameworks. An experimental evaluation on both autoregressive and bidirectional sequence tasks demonstrates the effectiveness of this approach, yielding state-of-the-art results on several image and text modeling tasks.</p><p>Recently, there have been several attempts to scale up attention to long sequences. A popular class of methods sparsifies the attention matrix with different sparsity patterns, including local * indicates equal contribution. The work was completed during HR's internship at Google Brain.</p><p>35th Conference on Neural Information Processing Systems (NeurIPS 2021).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The Transformer <ref type="bibr" target="#b0">[1]</ref> is a powerful neural network architecture that has demonstrated state-of-the-art performance in machine translation <ref type="bibr" target="#b1">[2]</ref> and many other natural language processing (NLP) tasks via pretraining, using either unidirectional language modeling <ref type="bibr" target="#b2">[3]</ref> or bidirectional language modeling <ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref>. It has also achieved excellent results in other domains like image recognition <ref type="bibr" target="#b8">[9]</ref>, code understanding <ref type="bibr" target="#b9">[10]</ref>, speech recognition <ref type="bibr" target="#b10">[11]</ref>, protein <ref type="bibr" target="#b11">[12]</ref>, music <ref type="bibr" target="#b12">[13]</ref> and image <ref type="bibr" target="#b13">[14]</ref> generative modeling. The core component of Transformer is the attention mechanism, which computes dependencies between all pairs of positions in a sequence. However, for a sequence of length L, the expressiveness of pairwise attention comes at a quadratic cost O(L 2 ) in both time and memory consumption. This makes the vanilla Transformer <ref type="bibr" target="#b0">[1]</ref> prohibitive for applications that involve long sequences, including high-resolution images, protein sequences, or raw speech signals <ref type="bibr" target="#b14">[15]</ref>, where the sequence length L is often larger than 10, 000 <ref type="bibr" target="#b13">[14]</ref>. window <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, local+stride <ref type="bibr" target="#b13">[14]</ref>, log-sparse <ref type="bibr" target="#b17">[18]</ref>, axial <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref>, or learnable patterns through hashing <ref type="bibr" target="#b20">[21]</ref> or clustering <ref type="bibr" target="#b21">[22]</ref>. Sparse attention enjoys sub-quadratic cost, but is lossy in capturing all-pair relationships. Generally, sparse attention requires more layers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23]</ref> to achieve full autoregressive or bidirectional dependencies (or receptive fields <ref type="bibr" target="#b19">[20]</ref>) for each location in a long sequence.</p><p>Alternatively, another line of research has tried to achieve scalability with an explicit low-rank assumption <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> on the attention matrix or by using explicit feature maps of some kernels <ref type="bibr" target="#b25">[26]</ref>. However these explicit low dimensional approximations might be too restricted for the potentially full rank attention matrix, which uses exponential kernels that are effectively infinite dimensional <ref type="bibr" target="#b26">[27]</ref>. The Performer <ref type="bibr" target="#b27">[28]</ref> is among the first works that attempts to approximate regular full-rank attention with the random feature trick <ref type="bibr" target="#b28">[29]</ref>. However such random-feature based approaches <ref type="bibr" target="#b29">[30]</ref> require many more bases to better approximate the exponential kernel <ref type="bibr" target="#b26">[27]</ref>, and empirically we found it produces inferior results in some sequence modeling tasks, such as density estimation.</p><p>In this paper we propose Combiner, a drop-in replacement for the vanilla quadratic attention mechanism with sub-quadratic computation and memory cost. Combiner still achieves full attention capability within each head of Multi-Head Attention, unlike approaches that adopt sparse or low-rank approximations. As we will discuss, the standard attention computed at each location can be seen as the conditional expectation of the value embeddings at all feasible locations given the current location. Based on such an understanding, Combiner explicitly approximates the conditional distribution in through a structured factorization of the probability space. Specifically, given a location x, the probability of attending to location y can be either directly calculated via the query vector of x and key vector of y, or indirectly through a local abstraction where x first attends to the key vector that represents a group of locations containing y, and multiplying the probability of choosing y within that group. We refer to this model as Combiner since the conditional distributions in attention become a combination between several local attentions and direct attentions. This structured decomposition enables Combiner to take existing sparse attention patterns and convert them into corresponding design choices for probability factorizations that achieve full attention. As shown in <ref type="figure">Figure 1</ref>, Combiner achieves full attention with the same asymptotic complexity as sparse variants. Combiner can be easily implemented in most existing deep learning frameworks without the need for specialized hardware implementation, and is GPU/TPU friendly. In fact, both the fixed and learnable sparse attention patterns from many existing Transformer variants <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22]</ref> can be enhanced with such structured factorizations, with the same order of time or memory cost.</p><p>We validate Combiner on both autoregressive and bidirectional sequence modeling tasks over a variety of domains including text and images. We show that Combiner can achieve better perplexity and accuracy when using the same transformer architectures while being much faster in terms of runtime, and achieves state of the art performance on density estimation on standard datasets CIFAR-10 (2.77 bits/dim) and ImageNet-64 (3.42 bits/dim), as well as the Long-Range Arena <ref type="bibr" target="#b30">[31]</ref>. The implementation of Combiner can be found at https://github.com/google-research/googleresearch/tree/master/combiner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Attention as Conditional Expectation</head><p>In this section, we revisit the formulation of the standard Transformer <ref type="bibr" target="#b0">[1]</ref> from the perspective of conditional expectation, which inspires the derivation of Combiner.</p><p>Without loss of generality, we use a single sequence in the self-attention scenario. Given a sequence of L embeddings X = [x 1 , x 2 , . . . , x L ], where X ? R L?d and each embedding x i ? R d is a d-dimensional vector, the core component of Transformer is the multi-head attention, where each head h is a scaled dot-product attention:</p><formula xml:id="formula_0">A h (X) = softmax Q h ? d K h V h , Q h = XW Q h , K h = XW K h , V h = XW V h ? R L?d ,<label>(1)</label></formula><p>and the attention vector from each head A h (X) is concatenated and projected:</p><formula xml:id="formula_1">MultiHeadAttn(X) = [A 1 (X), A 2 (X), . . . , A H (X)] W o , W o ? R Hd?d .<label>(2)</label></formula><p>Here H is the total number of heads per Transformer layer. In this paper, we focus on how to approximate full attention within each head of multi-head attention. For ease of notation, we drop the head index h whenever possible, and use lower-case letters</p><formula xml:id="formula_2">x i , q i , k i , v i ? R d to denote rows in (D) Combiner-Fixed (A) Fixed (B) Logsparse (E) Combiner-Logsparse Direct Expectation</formula><p>Local Expectation (F) Combiner-Axial (C) Axial <ref type="figure">Figure 1</ref>: Attention matrices of several instantiations of Combiner in the autoregressive setting. We transform several sparse attention patterns: Fixed (A) <ref type="bibr" target="#b13">[14]</ref>, Logsparse (B) <ref type="bibr" target="#b17">[18]</ref> and Axial (C) <ref type="bibr" target="#b19">[20]</ref> to Combiner-Fixed (D), Combiner-Logsparse (E) and Combiner-Axial (F). Combiner approximates the conditional expectation (3) with a combination of direct expectation (blue) and local expectation (yellow). Our instantiations (D)(E)(F) achieves full attention with the same sub-quadratic complexity.</p><p>X, Q, K, V respectively, which corresponds to a location i in the original sequence of length L. We use [n] to denote the set of positive integers {1, 2, . . . , n}.</p><p>For a position i ? [L], the attention formulation (1) can be viewed as conditional expectation of rows in V . Specifically, since softmax outputs a probability distribution, we can rewrite (1) as</p><formula xml:id="formula_3">A(x i ) = E p(j|i) [v j ] , p(j|i) = 1 Z (x i ) exp q i ? d k j ,<label>(3)</label></formula><p>where p(j|i) denotes the conditional probability at position j given the token at position i and the partition function Z (x i ) = j??i exp qi ? d k j over support ? i . The support ? i of p (j|i) defines the set of valid locations that the i-th token can attend to. For instance, the support set in autoregressive language modeling (LM) consists of all previous tokens, i.e., ? LM </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Combiner: Full Attention via Structured Conditional Expectation</head><p>The complexity of p (j|i) is the bottleneck of the computation for A (x i ). Generally, in existing sparse transformers, the support of p (j|i) is sparsified to reduce the computation and memory complexity, e.g., ? Sparse for MLM, while still maintaining sub-quadratic computation and memory cost. Below we denote ? i as the support for full attention if there is no ambiguity or need to distinguish between LM or MLM. We introduce the main design framework in Section 3.1 and possible parameterizations in Section 3.2. Then in Section 3.3 we analyze the trade-off of Combiner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Local Factorization for Conditional Expectation</head><p>The main idea of Combiner is to exploit a hierarchical structure for conditional probability modeling in <ref type="bibr" target="#b2">(3)</ref>, which provides the opportunity for reducing computation complexity while maintaining the same support. Specifically, we introduce support variables ? r i , for r = 0, . . . , n i and i ? [L]. The support variables are disjoint, i.e., ? r i ? ? s i = ?, ?r = s, and ? ni r=0 ? r i = ? i . Then we can factorize p(j|i) as</p><formula xml:id="formula_4">p(j|i) = ni r=0 p(j, ? r i |i) = ni r=0 p(j|? r i , i)p(? r i |i) = p(j|? rj i , i)p(? rj i |i),<label>(4)</label></formula><p>where r j denotes the index of the support to which j belongs. The last equation arises from the fact that the ? r i are disjoint from each other (? r i ? ? s i = ?, ?r = s). Therefore, there is only one support, ? rj i , containing j. The remaining terms, where j ? ? r i for r = r j , are all zero since p (j|? r i , i) = 0. Furthermore, assume ? rj i is a sufficient statistic, i.e., j and i are independent given ? rj i , we obtain p(j|i) = p(j|? </p><p>Given the partition {? r i } ni r=0 , the attention form in (3) can be rewritten as</p><formula xml:id="formula_6">A (x i ) = E p(j|i) [v j ] = ni r=0 j?? r i p (j, ? r i |i) v j (6) = j?? 0 ip (j|i)v j direct expectation + ni r=1 p(? r i |i) j?? r i p(j|? r i )v j local expectation ,<label>(7)</label></formula><p>where we consider direct attention in partition ? 0 i and apply the local factorization (5) to the partition r = 1, . . . , n i . Herep(j|i) ? p(j|i) but with different normalization constants, which will be explained below. We refer to this model as Combiner since the structured attention <ref type="bibr" target="#b6">(7)</ref> combines the direct expectation of ? 0 i and multiple local expectations via p(j|? r i ) and p(? r i |i) to form the final conditional expectation.</p><p>Equivalently, we can also rewrite the structured attention <ref type="formula" target="#formula_6">(7)</ref> as</p><formula xml:id="formula_7">A(x i ) = j??i I(j ? ? 0 i )p(j|i) + ni r=1 I(j ? ? r i )p(j|? r i )p(? r i |i) the new effective conditional probability q(j|i) v j ,<label>(8)</label></formula><p>where I(?) is a binary indicator function. After reordering, one can see from <ref type="formula" target="#formula_7">(8)</ref> that we obtain the effective conditional probability q(j|i) that tries to approximate the original p(j|i). Each probability term depends on both current location i and other location j, and the expectation is still obtained with respect to a valid conditional probability (non-negative and sums up to 1 over ? i ).</p><p>Requirement for Sub-quadratic Cost. We can immediately see the benefit of this formulation from the fact that the local expectation in <ref type="formula" target="#formula_6">(7)</ref> is independent of the position i. The full dependence is achieved via the multiplier p(? r i |i) where j ? ? r i . If we can design the local factorization such that: 1. the order of number of terms in <ref type="bibr" target="#b6">(7)</ref> for p(?|i), ?i ? [L]: <ref type="bibr">[1,ni]</ref> be the unique set of partitions used for local expectation calculation, then the order of |U| (i.e., the number of unique partitions in U) is sub-quadratic; 3. the order of total number of unique calculations of local expectation across all locations in <ref type="formula" target="#formula_6">(7)</ref>, ??U |?| is sub-quadratic; then one can see that the overall computation and memory cost will be sub-quadratic with full attention support ? Combiner</p><formula xml:id="formula_8">L i=1 (n i + |? 0 i |) is sub-quadratic; and 2. let U = {? r i } i?[L],r?</formula><formula xml:id="formula_9">i = ? i , ?i ? [L]</formula><p>. We will discuss in detail in Section 4 how to instantiate such a principle by drawing inspiration from existing sparse transformers, and how to convert them into a full attention model almost for free with identical asymptotic complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark (Further Hierarchical Decomposition):</head><p>We introduce the local decomposition with a one layer partition of support of p(?|i) for simplicity. In fact, such local decompositions can be stacked further, which introduces a partition tree. Specifically, we can further partition ? r i with disjoint subsets ? rk i nr k=1 , and consider local decomposition p(j, ? r i |i) = p(j|?</p><formula xml:id="formula_10">rkj i , i)p(? rkj i |? r i , i)p(? r i |i),</formula><p>where k j is the index of sub-region which j belongs to. Thus, we obtain a hierarchical decomposition of p(j|i), which can also be plugged to <ref type="bibr" target="#b5">(6)</ref> and yield a new full attention formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parameterizing Conditional Probabilities</head><p>While we obtained a possible way to speed up the standard Transformer via a combination of direct expectation and local expectations, it is also important to have an efficient design choice for the probability terms in <ref type="bibr" target="#b6">(7)</ref>, namelyp(j|i) from direct expectation, p(j|? r i ) from local expectation and p(? r i |i) for r ? [1, n i ]. For simplicity we use the scaled dot-product, which means that we will associate positions i, j and variable sets ? r i with the corresponding embedding representation, and thus the probability is proportional to the exponential of the embedding inner products. Specifically:</p><p>?p(j|i): As this term is for the direct expectation, we can letp(j|i) ? exp( qi ? d k j ), which is the same as vanilla attention (3) but with different normalizations, which will be explained in Equation 9. ? p(? r i |i): This term aims to capture the joint event probability, i.e., p(</p><formula xml:id="formula_11">? r i |i) ? exp qi ? d k ? r i .</formula><p>Thus the design choice of k ? r i should make an abstraction of the corresponding support ? r i . We find k ? r i = max pooling j?? r i k j already provides good empirical results without introducing additional parameters; we can also use DeepSets <ref type="bibr" target="#b31">[32]</ref> to obtain such abstraction. ? p(j|? r i ): This term is the probability of getting j within this local span ? r i . We make p(j|? r i ) ? exp</p><formula xml:id="formula_12">q ? r i ? d k j ,</formula><p>where we use max pooling or DeepSets over {q j } j?? r i to obtain q ? r i similarly. Normalizing Probability Terms. The terms in each local expectation p(j|? r i ), ?j ? ? r i can be normalized within the local span; the direct expectationp(j|i) and the terms in p(? r i |i) should be normalized together,</p><formula xml:id="formula_13">Z(x i ) = j?? (0) i exp q i ? d k j + ni r=1 exp q i ? d k ? r i ,<label>(9)</label></formula><p>and Z(x i ) is the normalizing constant when calculatingp(j|i) and p(? r i |i).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Trade-offs in Combiner</head><p>Combiner achieves full attention with reduced cost without making explicit sparsity or low-rank assumptions over the attention matrix. However this efficiency gain is not free. In this section we discuss the limitations of the simplification made by Combiner, and provide a simple workaround.</p><p>Structured Attention Approximation. We obtain the local decomposition (5) under the conditional independence assumption. Therefore, the local expectation in <ref type="formula" target="#formula_6">(7)</ref> is independent of the position i, this suggests that any two locations i 1 and i 2 with ? r i1 = ? r i2 = ? would have linearly dependent attention scores over the region ?. Formally, the probabilities formed by the effective conditional distribution a(?) i1 = q(j 1 |i 1 ), q(j 2 |i 1 ), . . . , q(j |? r i 1</p><formula xml:id="formula_14">| |i 1 ) = p(? r i 1 |i1) p(? r i 2 |i2) a(?) i2 .</formula><p>In other words, the rank of the sub-matrix over the same partition in the resulting attention matrix is 1, therefore, the attention matrix is locally low-rank based on the partition. On the other hand, the direct expectation fully attends to each position in sub-support ? 0 , which ensures the full-rank block. These two attention schemes make the attention matrix of Combiner structured. Compared with the low-rank approximation for attention <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30]</ref>, which is inspired from random features <ref type="bibr" target="#b28">[29]</ref> in the kernel community, a structured approximation that exploits both the locally low-rank and full-rank blocks has been proved more powerful theoretically and empirically in large-scale kernel machines <ref type="bibr" target="#b26">[27]</ref>.</p><p>Improving Expressiveness Using a Mixture Model. One way to further improve the expressiveness of the local factorization is to use a mixture model. This idea is adapted from the mixture of softmaxs <ref type="bibr" target="#b32">[33]</ref> to obtain high-rank softmax layer in language modeling. Let ? be a certain partition of the support (i.e., collection of ? r i ) of ? i , then one can easily use A(</p><formula xml:id="formula_15">x i ) = 1 M M m=1 A(x i ; ? m ) to compute the attention, where each component of the mixture A(x i ; ? m )</formula><p>is the term (7) using a specific factorization plan ? m . Empirically we find two components are already sufficient to improve performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Combiner Instantiations</head><p>In this section we show several local factorization schemes satisfying the requirements in Section 3.1. As we will see, Combiner is able to convert several sparse transformers <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref> into full attention, with the same order of computation and memory consumption. One can also design other factorization patterns, which can be easily instantiated in Combiner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Combiner-Fixed</head><p>The Sparse Transformer <ref type="bibr" target="#b13">[14]</ref> is one of the most representative variants that can achieve O(L ? L) computation and memory cost with sparse attention. Here we show how to convert this fixed pattern proposed in <ref type="bibr" target="#b13">[14]</ref>  <ref type="figure">(Figure 1(A)</ref>) into a factorization plan, and instantiate a full attention variant named the Combiner-Fixed <ref type="figure">(Figure 1(D)</ref>).</p><p>In the fixed-sparse attention, the support is</p><formula xml:id="formula_16">? sparse MLM i = {j : j mod s = 0} ? {j : j ? i (div s)}</formula><p>where s is a hyper-parameter, div is integer division, and j ? i (div s) denotes that the quotients of i and j w.r.t. s are the same. In the autoregressive case,</p><formula xml:id="formula_17">? sparse LM i = ? sparse MLM i ? [i]</formula><p>. Please refer to <ref type="figure">Figure 1(A)</ref> for an illustration of the LM version.</p><p>Our design of ? MLM fixed has the following form:  <ref type="formula" target="#formula_6">(7)</ref> ; the local expectation has (L div s) terms . The overall complexity is O(L ? (s + 2(L div s))). The optimal s is O( ? L), and we can achieve O(L ? L) computation and memory complexity, which is the same as <ref type="bibr" target="#b13">[14]</ref> but here we gain full attention capability in each attention head. For the LM case, we can simply have ? LM fixed :</p><formula xml:id="formula_18">? 0 i = {j : j ? i (div s)} , ? r i = j : j div s = r, j / ? ? 0 i , ?r ? [L div s], ?i ? [L]<label>(10)</label></formula><formula xml:id="formula_19">{? r i ? [i] | ? r i ? ? MLM fixed }, which has the same O(L ? L) optimal complexity.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Combiner-Logsparse</head><p>The Logsparse Transformer is proposed in <ref type="bibr" target="#b17">[18]</ref> and can theoretically achieve O(L log L) cost. The general idea is to make the size of support ? sparse i no larger than log 2 i . For the ease of notation, we first define bits(n) = [b 1 , b 2 , . . . , b log 2 n ] to be the binary representation of integer n, with b t ? {0, 1} the coefficient of basis 2 t . Thus we have n = log 2 n t=1 b t * 2 t . One of the possible design choices to make Logsparse in the LM case is ? sparse LM i = suff t :=</p><formula xml:id="formula_20">log 2 i?1 ? =t b ? * 2 ? log 2 i?1 t=1</formula><p>? {i}, i.e., attend to the location indices that equal to the suffix sum of the weighted bits(i ? 1), as well as location i itself. This serves as our base sparse version as shown in <ref type="figure">Figure 1(B)</ref>.</p><p>To exploit this scheme in the Combiner framework, we can define log 2 n non-overlapping supports, where ? r i = [suff r ] \ [suff r+1 ] with the boundary case [suff log 2 i?1 +1 ] = ?. Note that for the ease of notation, some of the ? r i are empty which will be ignored. In this case, the direct attention set ? 0 i includes {i}, as well as {i ? 1} when i is an even number. Such a factorization leads to Combiner-Logsparse, as shown in <ref type="figure">Figure 1</ref>(E). From the <ref type="figure">Figure,</ref> we observe that in total we will have span summaries for every 2, 4, 8, . . . , 2 log 2 L locations, resulting in total log 2 L t=1 L 2 t or O(L) summaries. Each location i will select at most O(log(i)) non-overlapping spans to cover the full support ? i , and thus, the total cost will be O (L log L). We leave the design of MLM case to Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Combiner-Axial</head><p>The Axial Transformer <ref type="bibr" target="#b19">[20]</ref> builds the attention along each axis of the input data. Without loss of generality, we focus on 2D case where the input sequence is reshaped into a matrix of size n ? m = L. Specifically, the location i in original sequence will be in row i = (i ? 1) div m + 1 and col i = (i ? 1) mod m + 1. We show how to simply enable full attention with factorization on 2D matrix, hence Combiner-Axial.</p><p>The sparse axial has ? sparse MLM</p><formula xml:id="formula_21">i = {j : j ? 1 ? i ? 1(mod m)} ? {j : j ? 1 ? i ? 1(div m)}, and ? sparse LM i = ? sparse MLM i ? [i]</formula><p>, which all have at most O(m + n) entries for each i, as illustrated in <ref type="figure">Figure 1(C)</ref>. We propose several factorization schemes to make it an attention with full support. obtain the abstraction. To obtain such abstraction for all the locations, we can leverage the cummax operator for each column to efficiently obtain the prefix-max.</p><p>? ? LM axial-horizontal : similar as ? axial-vertical except that each ? r i summarizes the row r before row i and excludes col i <ref type="figure" target="#fig_4">(Figure 2(B)</ref>).</p><p>? ? LM axial-rowmajor :</p><formula xml:id="formula_22">? 0 i = {j : j ? 1 ? i ? 1(div m)} ? [i],</formula><p>i.e., elements in the same row are directly attended, while ? r i = {j : j ? r(div m)} ? [i ? col i ] captures the rows before row i . This structure is similar to Combiner-Fixed, except for the way that the abstraction (and thus the local expectation) is computed. Combiner-Fixed computes the abstraction only based on r of partition ? r i , where ? axial-rowmajor depends on both r and the column col i <ref type="figure">(Figure 1(F)</ref>).</p><p>In all cases above, the cost is similar to the Axial Transformer <ref type="bibr" target="#b19">[20]</ref>, which is O(L ? L) if we reshape the sequence to a 2D matrix with n, m = O( ? L). We defer the MLM case to Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Combiner-Learnable</head><p>Inspired by the Reformer <ref type="bibr" target="#b20">[21]</ref> and Routing Transformer <ref type="bibr" target="#b21">[22]</ref>, we can also learn the factorization plan ? from the data. We illustrate this with Routing Transformer and provide a way to enable full attention in Routing Transformer following the Combiner principle.</p><p>For a specific layer, suppose we have a learned disjoint region (or cluster in Routing Transformer)</p><formula xml:id="formula_23">{? r } n r=1 where ? r ? r = [L].</formula><p>In Routing Transformer, we simply have ? sparse MLM i = ? ri where ? ri denotes the region where position i belongs to. To define the Combiner factorization, we let ? routing MLM : ? 0 i = ? ri , ? r i = ? r \ ? 0 i , ?r ? [n i ].</p><p>(11) Note that n i = n (i.e., number of learned clusters) for all locations. The above factorization can only work for MLM. LM requires the following definition:</p><formula xml:id="formula_24">? routing LM : ? 0 i = ? ri ? [i], ? r i = ? r \ ? 0 i ? [i], ?r ? [n i ].<label>(12)</label></formula><p>In general, both LM and MLM can have sub-quadratic cost when n = O( ? L). However, routing variants (including the Routing Transformer) require a gather operation, which can be slow on TPUs (see illustration in Appendix D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation</head><p>We evaluate Combiner with different full attention patterns on both autoregressive and bidirectional sequence modeling tasks, covering a wide range of input data from images to texts. All tasks considered involve long sequences for up to 12,000 in length, some of which prevent the applicability of the vanilla transformer. We compare Combiner with state-of-the-art Transformers. We also perform a series of ablation studies where all of the models being compared use the exact same architecture that only differ in the attention module, avoiding individual tricks employed in the original works (e.g., using both learnable and fixed patterns in Routing Transformer <ref type="bibr" target="#b21">[22]</ref>). Details to reproducing all experimental results can be found in Appendix E. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Autoregressive Sequence Modeling</head><p>In this subsection, we first perform density estimation on text and image using Combiner.  For language modeling, we focus on the Wiki-40B-En dataset <ref type="bibr" target="#b33">[34]</ref>, which consists of clean Wikipedia pages in English. We use a sentence piece model with vocabulary size 32K to tokenize the text and measure the perplexity at the sentence piece level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Language Modeling</head><p>To ensure fair comparison, all models being compared again have the same number of layers and hidden sizes, are are implemented under the same code base. <ref type="table" target="#tab_1">Table 2</ref> shows the results of the comparison. As we can see, under 2k sequence length, Combiner variants are consistently better than their corresponding baselines, and are very close to the standard Transformer. When sequence length goes to 8k, the standard Transformer runs out of memory, whereas Combiner continues to achieve improved perplexity, surpassing the result of Transformer-2k. If we further use DeepSets to calculate the summarization terms q ? r i and k ? r i , we may further achieve lower perplexity as shown in <ref type="table" target="#tab_2">Table 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Image Generative Models</head><p>CIFAR-10. We first perform a sanity check where we compare sparse attention baselines against Combiner with full attention under the same architecture on the CIFAR-10 dataset. The sequence length is 3072. For all the methods, we use a same 6-layer transformer with 8 attention heads and 512 embedding dimensions. We train all models for 500k iterations using batch size 32 on TPU v2. As shown in <ref type="table" target="#tab_0">Table 1</ref>, given the same model architecture, Combiner-X performs significantly better than the base model X under the bits per dimension (BPD) metric on the 10,000 test images. In particular, Combiner significantly decreases BPD by 0.887, 0.087, and 0.626 compared to the base models Logsparse, Fixed and Axial, respectively. Note that all of the Combiner variants achieve better performance than the best of the base models. This demonstrates the advantage of Combiner over the baselines given the same 6-layer architecture. We observe a similar trend under a 12-layer architecture. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CIFAR-10</head><p>Bits/Dim PixelCNN <ref type="bibr" target="#b14">[15]</ref> 3.03 PixelCNN++ <ref type="bibr" target="#b35">[36]</ref> 2.92 Image Transformer <ref type="bibr" target="#b15">[16]</ref> 2.90 PixelSNAIL <ref type="bibr" target="#b36">[37]</ref> 2.85 Sparse Transformer <ref type="bibr" target="#b13">[14]</ref> 2.80 Combiner-Axial (ours) 2.77</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ImageNet 64x64</head><p>Bits/Dim PixelCNN <ref type="bibr" target="#b14">[15]</ref> 3.57 Parallel Multiscale <ref type="bibr" target="#b37">[38]</ref> 3.70 Glow <ref type="bibr" target="#b38">[39]</ref> 3.81 SPN <ref type="bibr" target="#b39">[40]</ref> 3.52 Sparse Transformer <ref type="bibr" target="#b13">[14]</ref> 3.44 Axial Transformer <ref type="bibr" target="#b19">[20]</ref> 3.44 Routing Transformer <ref type="bibr" target="#b21">[22]</ref> 3.43 Combiner-Axial (ours) 3.42</p><p>Following the 128-layer architecture in Child et al. <ref type="bibr" target="#b13">[14]</ref>, we apply Combiner-Axial and achieve state-of-the-art performance, 2.77 BPD on CIFAR-10, as listed in <ref type="table" target="#tab_3">Table 4</ref>. We run all of the models in <ref type="table" target="#tab_3">Table 4</ref> without data augmentation <ref type="bibr" target="#b34">[35]</ref>.</p><p>ImageNet-64. We also evaluate performance under the autoregressive setting on ImageNet-64, where sequence length is 12,288. We first perform the same analysis as CIFAR-10 and compare Combiner-X with the baselines using the same model architecture. As shown in <ref type="table" target="#tab_0">Table 1</ref>, Combiner consistently outperforms the baselines with the same attention pattern. We further apply Combiner-Axial to a 30-layer Transformer, which achieves state-of-the-art performance on density estimation on ImageNet-64, demonstrating the effectiveness of full attention achieved by Combiner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Bidirectional Sequence Modeling</head><p>Besides autoregressive tasks, we also evaluate Combiner on a set of standard bidirectional tasks to show the general applicability of the method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Long-Range Arena</head><p>Long-Range Arena (LRA) is a unified benchmark <ref type="bibr" target="#b30">[31]</ref> for probing the capability of efficient transformers on handling long sequences. We evaluate our models on five tasks from LRA: ListOps, Text Classification, Retrieval, Image Classification and Pathfinder. All of the tasks are sequence-level multi-class classification. Please refer to the original LRA paper for more details. As shown in <ref type="table" target="#tab_4">Table 5</ref>, Combiner is able to match the performance of vanilla Transformer and achieves even better performance in some tasks. Following the protocol of LRA, all methods use the same architecture and hyperparameters for a controllable comparison. We use the numbers from Tay et al. <ref type="bibr" target="#b30">[31]</ref> for all tasks except for Pathfinder. Since we were unable to reproduce the original Pathfinder results using the default setup in LRA Github repository, we rerun all the baselines using Pathfinderinter configuration to conduct fair comparison. However, as the benchmark is still of small-scale and the LRA official website discourages hyperparameter tuning, <ref type="table" target="#tab_4">Table 5</ref> should be treated as results for the test bench of expressiveness compared to vanilla Transformer.  <ref type="figure">Figure 3</ref>: We measure the inference runtime and memory usage for eight models. Overall Combiner has similar speed with Performer and its sparse counterpart but Vanilla Transformer quickly goes OOM when sequence length grows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Masked Language Modeling</head><p>As the core element of BERT langauge pretraining <ref type="bibr" target="#b4">[5]</ref>, masked language modeling (MLM) refers to the task of reconstructing tokens that are randomly masked out in the input sequence. As with the LM task, we use perplexity as the main metric, which correlates relatively well with down-stream task performance. Specifically, we use the large scale C4 dataset <ref type="bibr" target="#b7">[8]</ref> for training and evaluation, and consider different sequence lengths. Following the original BERT setup, we mask out 15% of the tokens in each input sequence. The comparison is summarized in <ref type="table" target="#tab_5">Table 6</ref>. Similar to the LM result, different Combiner variants consistently outperform their corresponding baselines under 2k sequence length. However, apart from the standard Transformer, Combiner-2k also falls behind BigBird-2k. We conjecture that this is related to the special design in BigBird such as all tokens can always attend to the &lt;cls&gt; token directly, which is only applicable in non-causal problems. That said, when we further increase sequence length to 8k, the standard Transformer runs into OOM issue, whereas Combiner not only outperforms BigBird but also substantially surpasses Transformer-2k. This suggests that Combiner can truly benefit from scaling learning to longer sequence lengths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Runtime and Memory Usage of Combiner</head><p>Here we evaluate the inference runtime and memory usage of five baselines -Transformer, Performer, BigBird, Sparse-Fixed and Sparse-Axial, as well as three variants of Combiner-Combiner-Fixed, Combiner-Axial and Combiner-Mixture. We run inference of all the models on a TPU v3-16 (16 cores x 16GB) with batch size 16, and we test sequences of length from 2 10 to 2 14 . As shown in <ref type="figure">Figure 3</ref>, Combiner instantiations achieve comparable runtime and memory usage with their sparse counterpart and Performer. Note Combiner achieves much better empirical performance than the sparse models and Performer. Combiner-Mixture has the same asymptotic complexity with Combiner-Fixed and Combiner-Axial, however, since it requires running two partition plans, it is slower than Combiner-Fixed and Combiner-Axial. Due to the gather operation required by the random attention which is not very TPU/GPU friendly, BigBird is very computationally expensive. And the Transformer model quickly runs out of memory when sequence length increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>Inspired by the conditional expectation view of attention mechanism, we propose Combiner, a drop-in replacement of the attention module. By introducing structured decomposition to the conditional probability, Combiner achieves full attention capability while maintaining sub-quadratic computational and memory cost. We instantiate several Combiner variants converting existing sparse transformers to full attention. Combiner achieves state-of-the-art performance on both autoregressive and bidirectional tasks for image and text modeling, showing benefits in both modeling effectiveness and runtime efficiency. Future work includes additional factorization pattern designs, as well as applications of Combiner in domains like bioinformatics and speech.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Combiner-Axial in MLM Case</head><p>Besides the ? LM axial-vertical , ? LM axial-horizontal and ? LM axial-rowmajor introduced in section 4.3, here we introduce how we extend these three models to the MLM case.</p><p>? ? MLM axial-vertical :</p><formula xml:id="formula_25">? 0 i = ? sparse MLM i = {j : j ? 1 ? i ? 1(mod m)} ? {j : j ? 1 ? i ? 1(div m)}, and ? r i = {j : j ? r(mod m)}, for r ? [m] \ {col i }.</formula><p>As depicted in <ref type="figure" target="#fig_4">Figure 2(A)</ref>, ? r i corresponds to the column r above row i , where we use max pooling to obtain the abstraction. To obtain such abstraction for all the locations, we can leverage the cummax operator for each column to efficiently obtain the prefix-max.</p><p>? ? MLM axial-horizontal : similar as ? MLM axial-vertical except that each ? r i summarizes all rows r and excludes col i . ? ? MLM axial-rowmajor : </p><formula xml:id="formula_26">? 0 i = {j : j ? 1 ? i ? 1(div m)}, i.e.,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Combiner-Learnable</head><p>As discussed in section 4.4. we design Combiner-learnable as an extension to the routing transformer <ref type="bibr" target="#b21">[22]</ref>, which learns to cluster the tokens. Each token in the routing transformer only attends to the tokens in the same cluster. As shown in <ref type="figure" target="#fig_5">figure 4</ref>, our Combiner-learnable combines direct expectation with local expectation (yellow tokens), each of which summarizes one cluster (red, blue or green). Following the routing transformer <ref type="bibr" target="#b21">[22]</ref>, we apply the combiner principle, so that we can achieve full attention in each head with identical complexity with the routing transformer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 CIFAR-10</head><p>Here we list the hyperparameters we used on the CIFAR-10 dataset. Our experiments include (1) an ablation study, where all the models share the exact same architecture; and (2) the main result, where our Combiner achieves the state-of-the-art result under the setting that no data augmentation is allowed.</p><p>For the ablation study, the embedding and hidden size is 512. We use 8 attention heads in each layer with in total 6 transformer layers. We train all the models for 400,000 steps with learning rate 1e-3 and batch size 32. For the main result, we use the same architecture as introduced in Child et al. <ref type="bibr" target="#b13">[14]</ref>, and we train our Combiner-Axial for 1,200,000 steps with cosine learning rate scheduling. We rerun the main result for 3 times and the standard deviation is 0.003.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 ImageNet-64</head><p>Regarding the details of the ImageNet-64, we use the same setup with CIFAR-10, which consists of an ablation study and the main result. The architecture used in the ablation study is identical with the one we used in CIFAR-10. For the main result of Combiner-Axial, we used a 30-layer architecture with 768 hidden size and embedding dimension. We train this architecture for 1,200,000 steps with cosine learning rate scheduling. We also rerun the main result for 3 times and the standard deviation is 0.005.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Wiki-40B Language Modeling</head><p>The main purpose of this experiment is not to chase the state-of-the-art performance, as generally speaking, the more parameters/data, the better the perplexity would be for language modeling. So instead, we let all the methods have the same neural network backbone, while only varying the attention implementations to compare their effectiveness. This is similar in spirit to the ablation study in CIFAR-10 and ImageNet-64.</p><p>Specifically, we use the word embedding size and hidden size of 768 for all the layers. We use 12 attention heads in each layer, with in total 12 transformer layers. We use the Pre-Norm architecture, and the MLP layers have hidden size equals to 4 ? 768. The maximum sequence length can vary in {2048, 8192}, depends on the memory limit of each methods. All the methods are trained for 125,000 stochastic gradient updates, with batch size equals to 128. We also enable the cosine learning rate scheduling, with 10,000 warm-up steps. The optimizer is Adam with gradient clipping.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 LRA Benchmark</head><p>We mainly follow the guideline of LRA, where all the models should use roughly the same number of parameters and same hyperparameters like batchsize, number of iterations, etc.. We tried our best to reproduce the experimental results using the code in https://github.com/google-research/longrange-arena, and we found that we cannot reproduce the pathfinder-32 results. We have communicated with the authors but didn't get the issue resolved. So instead, we rerun all the baselines using the same network configurations, on the pathfinder-32-inter setup. We found some of the methods favor the 'MEAN' pooling to get the sequence representation, while others favor the 'CLS' pooling. So we try both of them for each of the method, and report the best result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.5 C4 Masked Language Modeling</head><p>Similar to the purpose of section E.3, we perform masked language modeling task on C4 dataset, which is typically used for BERT pretraining. As the perplexity metric correlates with the downstream task performance well, we thus perform the controlled experiments with all the methods using the same network architecture.</p><p>The architecture used and the hyperparameters are almost the same as in section E.3, except that we have maximum number of segments equal 2.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>i=</head><label></label><figDesc>[i] 2 ; in masked language modeling (MLM) the support consists of all tokens in the sequence, i.e., ? MLM i = [L]. That is, ? LM i and ? MLM i represent the full attention capability respectively in the LM and MLM setting.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>but this can lead to either reduced capacity or limited applicability. We defer detailed discussion of the full capacity of the model to Appendix A. In this section we introduce the Combiner, which achieves ? Combiner i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where each local expectation is performed in each span of size s, and there are totally L div s spans across all locations. For each position i ? [L], there are (s + (L div s)) terms in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>? ? LM axial-vertical :? 0 i = ? sparse LM i , and ? r i = {j : j ? r(mod m)} ? [i ? col i ], for r ? [m] \ {col i }.As depicted inFigure 2(A), ? r i corresponds to the column r above row i , where we use max pooling to Attention matrices and sequence being attended (e.g., a 3x4 image) of vertical and horizontal variants of Combiner-Axial. Blue and yellow correspond to direct and local attention respectively for location i (purple). Locations connected by arrows correspond to the same support ? r .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Left: Combiner-logsparse in the MLM case. Right: Combiner-Learnable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ablation results in Bits per Dimension (Bits/Dim) on CIFAR-10 and ImageNet-64.</figDesc><table><row><cell>Model</cell><cell cols="3">Layers CIFAR-10 ImageNet-64</cell></row><row><cell>Reformer [21]</cell><cell>6</cell><cell>-</cell><cell>3.740</cell></row><row><cell>Performer [28]</cell><cell>6</cell><cell>3.335</cell><cell>3.719</cell></row><row><cell>Logsparse [18]</cell><cell>6</cell><cell>4.253</cell><cell>4.351</cell></row><row><cell>Combiner-Logsparse (Ours)</cell><cell>6</cell><cell>3.366</cell><cell>3.795</cell></row><row><cell>Fixed [14]</cell><cell>6</cell><cell>3.408</cell><cell>3.696</cell></row><row><cell>Combiner-Fixed (Ours)</cell><cell>6</cell><cell>3.321</cell><cell>3.654</cell></row><row><cell>Axial [20]</cell><cell>6</cell><cell>3.666</cell><cell>4.032</cell></row><row><cell>Combiner-Axial (Ours)</cell><cell>6</cell><cell>3.050</cell><cell>3.585</cell></row><row><cell>Combiner-Mixture (Ours)</cell><cell>6</cell><cell>3.040</cell><cell>3.585</cell></row><row><cell>Reformer [21]</cell><cell>12</cell><cell>-</cell><cell>3.710</cell></row><row><cell>Performer [28]</cell><cell>12</cell><cell>3.310</cell><cell>3.636</cell></row><row><cell>Routing Transformer [22]</cell><cell>12</cell><cell>2.950</cell><cell>-</cell></row><row><cell>Combiner-Mixture (Ours)</cell><cell>12</cell><cell>2.885</cell><cell>3.504</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>LM Perplexity on Wiki-40B (Main).</figDesc><table><row><cell>Model</cell><cell>Perplexity</cell></row><row><cell>Transformer-2k [1]</cell><cell>17.26</cell></row><row><cell>Performer-2k [28]</cell><cell>19.66</cell></row><row><cell>Routing-2k [22]</cell><cell>20.85</cell></row><row><cell>Fixed-2k [14]</cell><cell>18.04</cell></row><row><cell>Combiner-Fixed-2k (Ours)</cell><cell>17.70</cell></row><row><cell>Axial-2k [20]</cell><cell>20.82</cell></row><row><cell>Combiner-Axial-2k (Ours)</cell><cell>17.56</cell></row><row><cell>Combiner-Fixed-8k (Ours)</cell><cell>16.60</cell></row><row><cell>Combiner-Axial-8k (Ours)</cell><cell>16.49</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>LM Perplexity on Wiki-40B (Ablation).</figDesc><table><row><cell>Model</cell><cell>Perplexity</cell></row><row><cell>Transformer-2k [1]</cell><cell>17.26</cell></row><row><cell>Combiner-DeepSets-Max-8k (Ours)</cell><cell>16.29</cell></row><row><cell>Combiner-DeepSets-Mean-8k (Ours)</cell><cell>16.48</cell></row><row><cell>Combiner-Max-8k (Ours)</cell><cell>16.60</cell></row><row><cell>Combiner-Mean-8k (Ours)</cell><cell>16.54</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Bits per Dimension (Bits/Dim) on CIFAR-10 and ImageNet-64.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Experimental results on Long-Range Arena benchmark.</figDesc><table><row><cell>Model</cell><cell cols="5">ListOps Text Retrieval Image Pathfinder</cell><cell>Avg</cell></row><row><cell>Chance</cell><cell>10.00</cell><cell>50.00</cell><cell>50.00</cell><cell>10.00</cell><cell>50.00</cell><cell>34.00</cell></row><row><cell>Transformer</cell><cell>36.38</cell><cell>64.27</cell><cell>57.46</cell><cell>42.44</cell><cell>88.81</cell><cell>57.87</cell></row><row><cell>Local Attention</cell><cell>15.95</cell><cell>52.98</cell><cell>53.39</cell><cell>41.46</cell><cell>84.64</cell><cell>49.68</cell></row><row><cell>Sparse TRans.</cell><cell>35.78</cell><cell>63.58</cell><cell>59.59</cell><cell>44.24</cell><cell>83.90</cell><cell>57.42</cell></row><row><cell>Longformer</cell><cell>36.03</cell><cell>62.85</cell><cell>56.89</cell><cell>42.22</cell><cell>86.68</cell><cell>56.93</cell></row><row><cell>Linformer</cell><cell>35.49</cell><cell>53.94</cell><cell>52.27</cell><cell>38.56</cell><cell>86.17</cell><cell>53.28</cell></row><row><cell>Reformer</cell><cell>36.30</cell><cell>56.10</cell><cell>53.40</cell><cell>38.07</cell><cell>79.18</cell><cell>52.61</cell></row><row><cell>Sinkhorn Trans.</cell><cell>34.20</cell><cell>61.20</cell><cell>53.83</cell><cell>41.23</cell><cell>73.36</cell><cell>52.76</cell></row><row><cell>Synthesizer</cell><cell>36.50</cell><cell>61.68</cell><cell>54.67</cell><cell>41.61</cell><cell>81.61</cell><cell>55.21</cell></row><row><cell>BigBird</cell><cell>37.08</cell><cell>64.02</cell><cell>59.29</cell><cell>40.83</cell><cell>86.75</cell><cell>57.59</cell></row><row><cell>Linear Trans.</cell><cell>17.15</cell><cell>65.90</cell><cell>53.09</cell><cell>42.34</cell><cell>88.13</cell><cell>53.32</cell></row><row><cell>Performer</cell><cell>36.00</cell><cell>65.40</cell><cell>53.82</cell><cell>42.77</cell><cell>88.76</cell><cell>57.35</cell></row><row><cell>Combiner-Fixed</cell><cell>36.65</cell><cell>64.99</cell><cell>59.81</cell><cell>41.67</cell><cell>88.59</cell><cell>58.34</cell></row><row><cell>Combiner-Axial</cell><cell>36.15</cell><cell>64.36</cell><cell>56.10</cell><cell>41.33</cell><cell>88.43</cell><cell>57.27</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>MLM perplexity on C4 dataset.</figDesc><table><row><cell>Model</cell><cell>Perplexity</cell><cell cols="4">2 13 Vanilla Transformer Performer</cell><cell cols="3">BigBird Combiner-Axial</cell><cell>2 3</cell><cell cols="3">Combiner-Fixed Sparse-Axial</cell><cell>Sparse-Fixed Combiner-Mixture</cell></row><row><cell>Transformer-2k [1] BigBird-2k [41] Performer-2k [28] Fixed-2k [14] Combiner-Fixed-2k (Ours)</cell><cell>4.552 4.696 10.940 5.279 5.170</cell><cell>Milliseconds / Iteration</cell><cell>2 7 2 8 2 9 2 10 2 11 2 12</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Memory (GB)</cell><cell>2 0 2 1 2 2</cell><cell></cell><cell></cell></row><row><cell>Axial-2k [20] Combiner-Axial-2k (Ours)</cell><cell>5.370 4.809</cell><cell></cell><cell>2 10</cell><cell>2 11</cell><cell cols="2">2 12 Sequence Length</cell><cell>2 13</cell><cell>2 14</cell><cell></cell><cell>2 10</cell><cell>2 11</cell><cell>2 12 Sequence Length</cell><cell>2 13</cell><cell>2 14</cell></row><row><cell>Routing-2k [22]</cell><cell>6.703</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Combiner-Routing-2k (Ours)</cell><cell>6.539</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>BigBird-8k [41]</cell><cell>4.542</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Combiner-Axial-8k (Ours)</cell><cell>4.190</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Combiner-Fixed-8k (Ours)</cell><cell>4.139</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>elements in the same row are directly attended, while ? r i = {j : j ? r(div m)} for r ? [n] \ {row i } captures all the rows except row i . It is trivial to see that the complexity remains O(L</figDesc><table><row><cell>?</cell><cell>L) if n, m = O(</cell><cell>?</cell><cell>L).</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Following the conventional implementation, the input sequence will be "right-shifted" so that the position i can attent to itself in LM setting.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments and Disclosure of Funding</head><p>We would like to thank Richard Song and David Dohan for the help on introducing Performer codebase and experiment configurations, Yi Tay and Mostafa Dehghani for clarifications on the LRA benchmark, James Lee-Thorp, Joshua Ainslie, and Ilya Eckstein for clarification on their LRA experiment results, Adams Yu for performing internal paper review and helpful suggestions. We also gratefully acknowledge the support of DARPA under Nos. HR00112190039 (TAMI), N660011924033 </p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Universal Approximation</head><p>Here we show in Proposition 1 that our Combiner-X achieves universal approximation property <ref type="bibr" target="#b41">[42]</ref> if the sparse transformer X achieves universal approximation property. For approaches like BigBird <ref type="bibr" target="#b40">[41]</ref>, they maintain the universal approximation property using the global tokens (CLS). However, the global attention makes it hard to be applied to the unidirectional autoregressive modeling (LM). Besides, the random attention requires the gather operation, making it very slow on dense hardware like TPUs <ref type="figure">(Figure 3</ref>). Proposition 1. The proposed Combiner will not break the universal approximation property of the original sparse transformers.</p><p>Specifically, we consider the function class constructed by stacking the attention block with a two-layer fully connected network. Formally, following the notations in <ref type="bibr" target="#b41">[42]</ref> we have the block as </p><p>Yun et al. <ref type="bibr" target="#b41">[42]</ref> shows that the function class (15) is still universal approximation w.r.t. the norm</p><p>with softmax in (1) and several requirements on the sparsity patterns in attention scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Combiner-Logsparse in MLM Case</head><p>Here we extend the Combiner-logsparse introduced in section 4.2 to the MLM case.</p><p>Besides the log 2 i non-overlapping supports in the LM case, we can define addtional log 2 i non-overlapping supports to attend to the tokens after the current token in the sequence. We illustrate this design choice in figure 4.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The best of both worlds: Combining recent advances in neural machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mia</forename><surname>Xu Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Orhan</forename><surname>Firat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankur</forename><surname>Bapna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melvin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Tom B Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nick</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Melanie</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Girish</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amanda</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Askell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Xlnet: Generalized autoregressive pretraining for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Albert: A lite bert for self-supervised learning of language representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenzhong</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingda</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Gimpel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piyush</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Radu</forename><surname>Soricut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to-text transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter J</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An image is worth 16x16 words: Transformers for image recognition at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning and evaluating contextual embedding of source code</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petros</forename><surname>Maniatis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gogul</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kensen</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Speech-transformer: a no-recurrence sequence-tosequence model for speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linhao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Progen: Language modeling for protein generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Madani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Mccann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Namrata</forename><surname>Anand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Po-Ssu</forename><surname>Raphael R Eguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.03497</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Music transformer: Generating music with long-term structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng-Zhi Anna</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Curtis</forename><surname>Hawthorne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.10509</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Ku</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Compressive transformers for long-range sequence modelling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potapenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Siddhant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Jayakumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lillicrap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Enhancing the locality and breaking the memory bottleneck of transformer on time series forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoyong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Xuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xifeng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Ccnet: Criss-cross attention for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinggang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lichao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunchao</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenyu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Axial attention in multidimensional transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1912.12180</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Reformer: The efficient transformer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikita</forename><surname>Kitaev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anselm</forename><surname>Levskaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Efficient content-based sparse attention with routing transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurko</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Saffar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Grangier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="53" to="68" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Transformer-xl: Attentive language models beyond a fixed-length context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaime</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Factorized attention: Self-attention with linear complexities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuoran</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiyu</forename><surname>Zhao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Linformer: Self-attention with linear complexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Belinda</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Madian</forename><surname>Khabsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Ma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04768</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Transformers are rnns: Fast autoregressive transformers with linear attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelos</forename><surname>Katharopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Apoorv</forename><surname>Vyas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
	<note>Nikolaos Pappas, and Fran?ois Fleuret</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Memory efficient kernel approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Si</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Inderjit S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rethinking attention with performers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krzysztof</forename><surname>Choromanski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Valerii</forename><surname>Likhosherstov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingyou</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreea</forename><surname>Gane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tamas</forename><surname>Sarlos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Hawkins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jared</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Afroz</forename><surname>Mohiuddin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Random feature attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Pappas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dani</forename><surname>Yogatama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roy</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Noah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lingpeng</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Long range arena: A benchmark for efficient transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samira</forename><surname>Abnar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yikang</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<biblScope unit="page">2021</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Breaking the softmax bottleneck: A high-rank rnn language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Wiki-40b: Multilingual language model dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mandy</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Vrande?i?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th Language Resources and Evaluation Conference</title>
		<meeting>The 12th Language Resources and Evaluation Conference</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Distribution augmentation for generative modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heewoo</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pixelsnail: An improved autoregressive generative model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikhil</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mostafa</forename><surname>Rohaninejad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Parallel multiscale autoregressive density estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A?ron</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><forename type="middle">G?mez</forename><surname>Colmenarejo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Belov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nando</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Glow: Generative flow with invertible 1x1 convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prafulla</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhariwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Generating high fidelity images with subscale pixel networks and multidimensional upscaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Menick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Big bird: Transformers for longer sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guru</forename><surname>Guruganesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avinava</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Ainslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Alberti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Ravula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">2020</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">o(n) connections are expressive enough: Universal approximability of sparse transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulhee</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin-Wen</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Srinadh</forename><surname>Bhojanapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><surname>Singh Rawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjiv</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.04862</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
