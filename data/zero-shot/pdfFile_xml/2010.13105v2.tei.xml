<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TWO-STAGE TEXTUAL KNOWLEDGE DISTILLATION FOR END-TO-END SPOKEN LANGUAGE UNDERSTANDING</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seongbin</forename><surname>Kim</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Inha University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gyuwan</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seongjin</forename><surname>Shin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Inha University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sangmin</forename><surname>Lee</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Dept. of Electrical and Computer Engineering</orgName>
								<orgName type="institution">Inha University</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Clova AI</orgName>
								<orgName type="institution" key="instit2">NAVER Corp</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TWO-STAGE TEXTUAL KNOWLEDGE DISTILLATION FOR END-TO-END SPOKEN LANGUAGE UNDERSTANDING</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T06:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-spoken language understanding</term>
					<term>pre-training</term>
					<term>knowledge distillation</term>
					<term>data augmentation</term>
					<term>vq-wav2vec</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>End-to-end approaches open a new way for more accurate and efficient spoken language understanding (SLU) systems by alleviating the drawbacks of traditional pipeline systems. Previous works exploit textual information for an SLU model via pre-training with automatic speech recognition or finetuning with knowledge distillation. To utilize textual information more effectively, this work proposes a two-stage textual knowledge distillation method that matches utterancelevel representations and predicted logits of two modalities during pre-training and fine-tuning, sequentially. We use vq-wav2vec BERT as a speech encoder because it captures general and rich features. Furthermore, we improve the performance, especially in a low-resource scenario, with data augmentation methods by randomly masking spans of discrete audio tokens and contextualized hidden representations. Consequently, we push the state-of-the-art on the Fluent Speech Commands, achieving 99.7% test accuracy in the full dataset setting and 99.5% in the 10% subset setting. Throughout the ablation studies, we empirically verify that all used methods are crucial to the final performance, providing the best practice for spoken language understanding. Code is available at https://github.com/clovaai/textual-kd-slu.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Conventional spoken language understanding (SLU) systems consist of the pipeline of automatic speech recognition (ASR) and natural language understanding (NLU) modules. However, these systems are prone to error propagation because it is difficult for an NLU model to yield correct intent from erroneous ASR outputs. Moreover, rich information from speech signals such as prosody that might help final prediction is ignored when text inputs are given to NLU after ASR. Therefore, end-to-end (E2E) SLU recently receives attention as a promising research direction for better SLU <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Despite an excellent representation capability of neural networks, a simple E2E SLU approach <ref type="bibr" target="#b2">[3]</ref> does not work well * Equal contribution.</p><p>? SK was an intern at Clova AI while doing this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>vq-wav2vec</head><p>DeepSpeech2</p><p>Raw waveform x s VQ BERT Overview of the proposed method. Our E2E SLU model is a combination of vq-wav2vec BERT and Deep-Speech2 acoustic model. We perform knowledge distillation from the text BERT model to the speech encoder during (1) additional pre-training (PT-KD) and <ref type="bibr" target="#b1">(2)</ref> fine-tuning (FT-KD). We use (3) data augmentation methods (DA) in fine-tuning. because it is difficult to extract useful features to identify intents of given utterances effectively. This problem becomes severe when training data is scarce and noisy. We propose a novel method based on textual knowledge distillation, pretraining, and data augmentation to address this issue.</p><p>Text is a more compressed form than audio signals to represent the same meaning. Therefore, a text model can guide training a speech model that requires extracting semantic representations from complex audio signals. In this respect, we utilize knowledge distillation (KD) <ref type="bibr" target="#b3">[4]</ref> to inject textual knowledge to speech encoder in both the pre-training and the finetuning stage. During pre-training, we match a hidden representation of an audio sequence from a speech encoder to a hidden representation of its transcription from a text encoder. Then, during fine-tuning, we match predicted logits for intent classification from classifiers in the two modalities.</p><p>As shown in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7]</ref>, pre-training speech encoder from ASR is helpful in that it learns features that are also useful for arXiv:2010.13105v2 [cs.CL] 10 Jun 2021 SLU to improve performance and generalize to unseen speech or text. Besides, we utilize vq-wav2vec BERT model <ref type="bibr" target="#b7">[8]</ref> as a pre-trained speech encoder. Pre-training with self-supervised context prediction instead of only specialized for ASR can capture more general and richer representations. For example, there might be features that can be used as a cue for SLU but not ASR.</p><p>On top of the vq-wav2vec BERT, we use DeepSpeech2 <ref type="bibr" target="#b8">[9]</ref> acoustic model to aggregate feature vectors over multiple segments into a single vector for intent classification. We pretrain this part with the ASR objective for better initialization following previous works mentioned earlier.</p><p>Motivated by SpecAugment <ref type="bibr" target="#b9">[10]</ref>, which is a simple and successful data augmentation technique for ASR, we apply several data augmentation methods at different positions in our model architecture and investigate their effect on SLU. Data augmentation is also essential to achieve good performance in SLU because data augmentation has a regularization effect and is very helpful in a low-resource scenario.</p><p>In sum, we present an effective method for E2E SLU by training with knowledge distillation from the text BERT to the vq-wav2vec BERT speech encoder (see <ref type="figure">Figure 1</ref>) and data augmentation techniques. We empirically achieve remarkable accuracy superior to previous state-of-the-art results on the Fluent Speech Command dataset. Moreover, we conduct an extensive ablation study to measure the impact of each component. Experimental results confirm that all three components are crucial to obtain the best performance: (1) textual knowledge distillation, (2) pre-training, and (3) data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Pre-training for SLU</head><p>Most of the recent E2E SLU approaches consist of a twostage procedure, pre-training a speech encoder with ASR followed by fine-tuning it for the final SLU task <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. Acoustic features learned from ASR are transferred to SLU.</p><p>Representations from self-supervised learning on a huge amount of unlabeled audio data <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref> have shown to be effective in low-resource scenario <ref type="bibr" target="#b7">[8]</ref>. We believe that they are also adequate to SLU, which usually has a few labeled data. To the best of our knowledge, this is the first work that takes advantage of a general and powerful pre-trained speech encoder, in our case vq-wav2vec BERT, for SLU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Knowledge Distillation for SLU</head><p>Several works use cross-modal distillation approach on SLU <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> to exploit textual knowledge. Cho et al. <ref type="bibr" target="#b12">[13]</ref> use knowledge distillation from a fine-tuned text BERT to an SLU model by making predicted logits for intent classification close to each other in fine-tuning. Denisov and Vu <ref type="bibr" target="#b13">[14]</ref> match an utterance embedding and a sentence embeddings of ASR pairs using knowledge distillation as a pre-training. Compared to them, we perform knowledge distillation in both pre-training and fine-tuning, meaning that we match sequence-level hidden representations and predicted logits of two modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Data Augmentation for SLU</head><p>Due to the lack of high-quality SLU data, a data augmentation approach is quite essential <ref type="bibr" target="#b14">[15]</ref>. SpecAugment <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref> is a simple yet effective data augmentation method for E2E ASR, resulting in consistent accuracy gain. For E2E SLU, Price <ref type="bibr" target="#b16">[17]</ref> uses SpecAugment except time warping on log-spectrum input features for data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">PROPOSED METHOD</head><p>In this section, we describe the architecture of our E2E SLU model ( ? 3.1) and its training pipeline ( ? 3.2). <ref type="figure">Figure 1</ref> illustrates an overview of our proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Model Architecture</head><p>Our SLU model is a stack of vq-wav2vec BERT <ref type="bibr" target="#b7">[8]</ref> based speech encoder and DeepSpeech2 <ref type="bibr" target="#b8">[9]</ref> acoustic model (AM). We use DeepSpeech2 to extract acoustic features but do not explicitly perform ASR. Text is not given in the test time. We use a text BERT model <ref type="bibr" target="#b17">[18]</ref> as a text encoder for textual knowledge distillation to assist the training of the SLU model. vq-wav2vec BERT vq-wav2vec maps a speech data x s , which is given as a raw waveform, into a sequence of discrete codes c = (c 1 , ? ? ? , c T ). This discrete sequence after appending a special [CLS] token is represented as a sequence of contextualized hidden vectors h s = (h s</p><p>[CLS] , h s 1 , ? ? ? , h s T ) via a BERT model.</p><p>DeepSpeech2 AM DeepSpeech2 AM consists of 2D-CNN layers and bidirectional LSTM layers. The output of AM passes a max-pooling layer followed by a projection layer to calculate predicted logits o s for the intent classification.</p><formula xml:id="formula_0">Text BERT A sentence x t = (x t 1 , ? ? ? , x t N )</formula><p>is encoded using a text BERT as a sequence of contextualized hidden vectors h t = (h t [CLS] , h t 1 , ? ? ? , h t N ) after appending a special [CLS] token. h t</p><p>[CLS] is regarded as a sentence representation. This sentence representation passes a projection layer to calculate predicted logits o t for the intent classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training</head><p>We borrow a pre-trained vq-wav2vec BERT from <ref type="bibr" target="#b7">[8]</ref>. Our baseline SLU model is a combination of this vq-wav2vec BERT and DeepSpeech2 AM. During the fine-tuning, vq-wav2vec BERT, which is regarded as a feature extractor, remains frozen. To inject textual knowledge into the model, we perform additional training process: (1) pre-training KD, (2) fine-tuning KD, and (3) AM pre-training.</p><p>Pre-training KD (PT-KD) vq-wav2vec BERT was pretrained by the masked language modeling (MLM) task. Using speech-text pairs from the ASR dataset, we further pre-train it by minimizing an L1 loss, h s</p><p>[CLS] ? h t [CLS] 1 , to make a sequence-level contextualized representation of speech data close to that of text data in addition to the original MLM loss of <ref type="bibr" target="#b7">[8]</ref>. We train MLM in conjunction with the hidden representation matching to keep learned useful acoustic features. vq-wav2vec is not updated in additional pre-training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Fine-tuning KD (FT-KD)</head><p>Compared to the SLU model given speech utterance as input, a text BERT model fine-tuned with pairs of a ground text and an intent label surprisingly performs well. Using SLU data, we fine-tune the SLU model by minimizing an L1 loss between predicted logits of two modalities, o s ? o t 1 , in addition to the original supervised loss.</p><p>AM Pre-training (AM-PT) Before fine-tuning the SLU model, we pre-train DeepSpeech2 AM with speech-to-text pairs for the better initialization of model weights used for SLU. It resembles other SLU approaches that rely on ASR pre-training. During the AM pre-training, pre-trained vq-wav2vec BERT is not updated to prevent a catastrophic forgetting of distilled weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Data Augmentation (DA)</head><p>During the fine-tuning, we apply two data augmentation methods. First, we mask spans of consecutive discretized speech tokens similar to the pre-training of vq-wav2vec BERT <ref type="bibr" target="#b7">[8]</ref> though it is for the regularization effect rather than the masked prediction task. Following SpanBERT <ref type="bibr" target="#b18">[19]</ref>, we randomly choose p d of all tokens as a starting index and mask M d consecutive tokens from every selected index with allowing overlap. Second, we apply time masking and channel masking with the same mechanism as token masking with parameters of (p t , M t ) and (p c , M c ). It is almost the same as SpecAugment <ref type="bibr" target="#b9">[10]</ref> except time-warping but differs in that masking is applied on contextualized features rather than input features. In our preliminary experiments, we find that applying two DA methods are complementary to each other. Therefore, we use both of them together in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>We conduct experiments on three SLU datasets: Fluent Speech Commands (FSC) <ref type="bibr" target="#b5">[6]</ref>, SNIPS <ref type="bibr" target="#b19">[20]</ref>, and Smartlights <ref type="bibr" target="#b20">[21]</ref>. We use FSC for comparison with other works because it is one of the most widely used datasets to evaluate SLU systems. The class of FSC dataset is a triple of (action, object, location), so the number of total classes is 336 (= <ref type="figure">6 ? 14 ? 4)</ref>. Test accuracy on FSC almost reaches 100%, implying that there is little room to improve and evaluate the newly proposed method's effectiveness. Following <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13]</ref>, we simulate a data shortage scenario using only 10% of the speech-text pairs in training. We randomly divide FSC dataset into ten parts and report the average accuracy on them.</p><p>In addition to FSC, we experiment on SNIPS and Smartlights to prove that our approach is generally applicable to other settings. SNIPS is an NLU benchmark, so they only provide text utterances and their corresponding labels. We generate speech data by Google's commercial speech synthesis toolkit 1 similar to <ref type="bibr" target="#b21">[22]</ref> to use SNIPS for SLU evaluation. We use a single speaker option by setting as a basic voice type named en-US-Standard-B. Because other works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> use their own speech synthesis methods and does not mention exact details to reproduce, a fair comparison between them and ours is impossible. Smartlights 2 <ref type="bibr" target="#b20">[21]</ref> has two subsets, Close field and Far field. The Close set is gathered by recording utterances using crowd-sourcing. The Far set is collected by playing these recorded utterances with a neutral speaker and recording them at a distance of 2 meters. Those two sets include the same set of utterances, but the Far set is much noisy. We split each set into a training, validation, and test set as the ratio of the number of utterances to be 70:10:20 following <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model Configurations</head><p>We borrow a pretrained vq-wav2vec BERT supported in the fairseq [24] library 3 . Its vector quantization is based on the k-means clustering, and its BERT is a base size of 12-layers trained similar to <ref type="bibr" target="#b24">[25]</ref>. Each token after the vector quantization represents 10ms of audio data.</p><p>DeepSpeech2 AM is a stack of two 2D-CNN layers and five bidirectional LSTM layers. The two CNN layers have a channel size of 32, kernel sizes of 41?11 and 21?11, respectively, and strides of 2 ? 2 and 2 ? 1, respectively. The hidden size of all LSTM layers is 768.  <ref type="table">Table 2</ref>. Results on FSC dataset. Because test accuracy is almost close to 100%, we also report validation accuracy to compare between methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>We use pretrained RoBERTa-base <ref type="bibr" target="#b24">[25]</ref> from the fairseq for the text BERT and a softmax layer for intent classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training Details</head><p>We further pre-train vq-wav2vec BERT from the pre-trained weights on 960h of Librispeech <ref type="bibr" target="#b25">[26]</ref>. The learning rate is linearly decayed from 1 ? 10 ?6 to 0 over 250k steps. A slightly small learning rate is used for PT-KD because we initialize it from the pre-trained weights. We use the batch size of 256 with 8 V100 GPUs and gradient accumulation. To not forget general speech representations of vq-wav2vec BERT, we early stop when the MLM loss starts increasing, so training ends after the update of 10k steps.</p><p>We fine-tune for 200 epochs with the learning rate annealed from 1 ? 10 ?4 by the factor of ? at each epoch. We use ? ? {1.01, 1.05, 1.1} depending on the training set size. For each dataset, we choose the best checkpoint in terms of validation accuracy. We tune data augmentation hyperparameter values depending on the dataset in following ranges: masking span length M d , M c , M t ? {5, 10} and maximum masking ratios</p><formula xml:id="formula_1">p d M d ? {0.1, 0.2}, p t M t , p c M c ? {0.1, 0.2, 0.3, 0.4, 0.5}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results and Discussion</head><p>We do an ablation study to evaluate the effectiveness of components in our proposed method. Methods in the order of PT-KD, FT-KD, AM-PT, and DA starting from the baseline using our model architecture without any additional training techniques are incrementally added one-by-one. Overall the gap between the full dataset setting and 10% subset setting is reduced. It indicates that our method is especially effective in a low-resource setting. All of the training methods are beneficial to improve accuracy. <ref type="table">Table 3</ref> shows the results on the SNIPS and Smartlights dataset. We achieve fine accuracy on these datasets, although comparison with other works is difficult. PT-KD and FT-KD boost accuracy significantly. Data augmentation makes SLU model robust to a noisy environment and generalizes to unseen speakers. However, data augmentation is not helpful in the SNIPS dataset. We presume that this is because its utterances in the training set and the test set are clean and synthesized with a single speaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">CONCLUSION</head><p>This work proposes a novel method, which leverages textual information using knowledge distillation in both pre-training and fine-tuning stages for end-to-end spoken language understanding. To do so, we utilize vq-wav2vec BERT as a speech encoder, which has learned general and rich speech representations and thus allows matching with text representations from the text BERT natural. Moreover, we use data augmentation methods by randomly masking spans of representations at different positions. With a thorough ablation study, we prove that two-stage knowledge distillation, AM pre-training, and data augmentation are crucial to learning a robust SLU model. As a result, we achieve a state-of-the-art result on FSC and good accuracy on other datasets, SNIPS and Smartlights. We leave the extension of our method to other downstream tasks such as speech emotion recognition <ref type="bibr" target="#b26">[27]</ref> and spoken question answering <ref type="bibr" target="#b27">[28]</ref> as future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 1. Overview of the proposed method. Our E2E SLU model is a combination of vq-wav2vec BERT and Deep-Speech2 acoustic model. We perform knowledge distillation from the text BERT model to the speech encoder during (1) additional pre-training (PT-KD) and (2) fine-tuning (FT-KD). We use (3) data augmentation methods (DA) in fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 Table 1 .</head><label>11</label><figDesc>summarizes their statistics in terms of the number of speakers and utterances. Dataset Statistics of FSC, SNIPS, and Smartlights.They have 336, 7, and 6 intent classes, respectively.</figDesc><table><row><cell></cell><cell></cell><cell>FSC (336)</cell><cell></cell><cell></cell><cell>SNIPS (7)</cell><cell></cell><cell cols="3">Smartlights (6)</cell></row><row><cell></cell><cell cols="9">Train Valid Test Train Valid Test Train Valid Test</cell></row><row><cell># Speakers</cell><cell>77</cell><cell>10</cell><cell>10</cell><cell>1</cell><cell>1</cell><cell>1</cell><cell>48</cell><cell>2</cell><cell>2</cell></row><row><cell cols="10"># Utterances 23,132 3,118 3,793 13,084 700 700 1,162 166 332</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 displayTable 3 .</head><label>23</label><figDesc>Results on SNIPS and Smartlights dataset. We report test accuracy. curacy and comparable result with only 10% of training pairs.</figDesc><table><row><cell>7% test ac-</cell></row></table><note>the results on FSC dataset. Previous state-of- the-art [17] achieves almost perfect accuracy of 99.5%. Our best model surpasses that result by achieving 99.</note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://cloud.google.com/text-to-speech 2 https://github.com/sonos/spoken-language-understanding-researchdatasets 3 https://github.com/pytorch/fairseq</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">ACKNOWLEDGEMENTS</head><p>This research was supported by NAVER Corp. The authors greatly appreciate Donghyun Kwak, Gichang Lee, Sang-woo Lee, Minjeong Kim, Jingu Kang, and Woomyoung Park at Naver Clova AI for constructive feedback. We use Naver Smart Machine Learning <ref type="bibr" target="#b28">[29]</ref> platform for the experiments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Exploring asr-free end-to-end modeling to improve spoken language understanding in a cloud-based dialog system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ubale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanaryanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lange</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Suendermann-Oeft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Evanini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tsuprun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="569" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Spoken language understanding without speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bangalore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="6189" to="6193" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fuegen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">in ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5754" to="5758" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">End-to-end spoken language understanding: Bootstrapping in low resource scenarios.,&quot; in Interspeech</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhosale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Dumpala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Kopparapu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="1188" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Speech model pre-training for end-to-end spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lugosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ignoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Tomar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03670</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Understanding semantics from speech through pretraining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Nie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10924</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">vq-wav2vec: Self-supervised learning of discrete speech representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.05453</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-to-end speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Specaugment: A simple data augmentation method for automatic speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Cubuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.08779</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Representation learning with contrastive predictive coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.03748</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An unsupervised autoregressive model for speech representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-A</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-N</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.03240</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Speech to text adaptation: Towards an efficient cross-modal distillation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">I</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08213</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Pretrained semantic speech embeddings for end-to-end spoken language understanding via cross-modal teacher-student learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Denisov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.01836</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data augmentation for spoken language understanding via joint variational generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-G</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7402" to="7409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="6879" to="6883" />
		</imprint>
	</monogr>
	<note>Specaugment on large scale datasets,&quot; in ICASSP, 2020</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">End-to-end spoken language understanding without matched language speech model pretraining data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Price</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="7979" to="7983" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spanbert: Improving pre-training by representing and predicting spans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="64" to="77" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Snips voice platform: an embedded spoken language understanding system for private-bydesign voice interfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doumouro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caltagirone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lavril</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10190</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caulier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ball</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Bluche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Leroy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doumouro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gisselbrecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Caltagirone</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.12735</idno>
		<imprint>
			<biblScope unit="page">208</biblScope>
		</imprint>
	</monogr>
	<note>Spoken language understanding on the edge</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning asr-robust contextualized embeddings for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-N</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="8009" to="8013" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Style attuned pre-training and parameter efficient fine-tuning for spoken language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hamza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vanee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04355</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">fairseq: A fast, extensible toolkit for sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Edunov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Grangier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Auli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01038</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Librispeech: an asr corpus based on public domain audio books</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panayotov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Povey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khudanpur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICASSP</title>
		<imprint>
			<biblScope unit="page" from="5206" to="5210" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Jointly fine-tuning&quot; bert-like&quot; self supervised models to improve multimodal speech emotion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Siriwardhana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Reis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weerasekera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nanayakkara</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2008.06682</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Spoken squad: A study of mitigating the impact of speech recognition errors on listening comprehension</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.00320</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.09957</idno>
		<title level="m">Nsml: Meet the mlaas platform with a real-world case study</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
