<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">3D AffordanceNet: A Benchmark for Visual Object Affordance Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengheng</forename><surname>Deng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<addrLine>2 I2R</addrLine>
									<region>A-STAR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xun</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chaozheng</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<addrLine>2 I2R</addrLine>
									<region>A-STAR</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<addrLine>2 I2R</addrLine>
									<region>A-STAR</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kui</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">South China University of Technology</orgName>
								<address>
									<addrLine>2 I2R</addrLine>
									<region>A-STAR</region>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Pazhou Laboratory</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">3D AffordanceNet: A Benchmark for Visual Object Affordance Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T14:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ability to understand the ways to interact with objects from visual cues, a.k.a. visual affordance, is essential to vision-guided robotic research. This involves categorizing, segmenting and reasoning of visual affordance. Relevant studies in 2D and 2.5D image domains have been made previously, however, a truly functional understanding of object affordance requires learning and prediction in the 3D physical domain, which is still absent in the community. In this work, we present a 3D AffordanceNet dataset, a benchmark of 23k shapes from 23 semantic object categories, annotated with 18 visual affordance categories. Based on this dataset, we provide three benchmarking tasks for evaluating visual affordance understanding, including full-shape, partial-view and rotation-invariant affordance estimations. Three state-of-the-art point cloud deep learning networks are evaluated on all tasks. In addition we also investigate a semi-supervised learning setup to explore the possibility to benefit from unlabeled data. Comprehensive results on our contributed dataset show the promise of visual affordance understanding as a valuable yet challenging benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The concept of affordance was first defined as what the environment offers the animal, introduced by <ref type="bibr" target="#b5">[6]</ref>. Affordance understanding is concerned with the interactions between human and environment. For instance, human can sit on the chair, grasp a cup or lift a bag. Being able to understand the affordance of objects is crucial for robots to operate in dynamic and complex environments <ref type="bibr" target="#b7">[8]</ref>. Many applications are supported by affordance understanding including, anticipating and predicting future actions <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13]</ref>, recognizing agent's activities <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b26">26]</ref>, providing valid functionality of the objects <ref type="bibr" target="#b6">[7]</ref>, understanding social scene situations <ref type="bibr" target="#b1">[2]</ref> and understanding the hidden values of the <ref type="figure">Figure 1</ref>. The 3D AffordanceNet dataset. The mesh was first annotated with affordance keypoints. Then we densely sample points and obtain the ground truth data via label propagation. objects <ref type="bibr" target="#b33">[33]</ref>. Tasks including affordance categorization, reasoning, semantic labeling, activity recognition, etc. are defined as specific instantiations of affordance understanding <ref type="bibr" target="#b7">[8]</ref>. Among all these we find semantic labeling <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b33">33]</ref> is of the most importance because the ability to localize the position of possible affordance is highly desired by robotic research. We refer semantic labeling as affordance estimation throughout this paper.</p><p>The most important and proper modality for affordance understanding is through visual sensors <ref type="bibr" target="#b7">[8]</ref>. Visual affordance understanding has been extensively studied recently with computer vision techniques. Many algorithms are built upon deep neural networks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b22">23]</ref> thus require large labeled affordance dataset for benchmarking. Relevant datasets are developed for these purposes with data collected from 2D (RGB) sensors <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b21">22]</ref> or 2.5D (RGBD) sensors <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref>. Nevertheless, we believe that the affordance understanding requires learning in the 3D domain which conveys the geometric properties. For example, the affordance of grasp is highly correlated with vertical structure with small perimeter and sittable is correlated with flat surface. Unfortunately, such detailed geometry is not captured by the existing 2D datasets while the 2.5 ones <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b22">23]</ref> are often captured with small depth variation and do not carry enough geometric information.</p><p>To encourage research into visual affordance understanding in more realistic scenarios, a benchmark on real 3D dataset is highly desired. Therefore, we are inspired by PartNet <ref type="bibr" target="#b16">[17]</ref>, a recently proposed dataset containing the fine-grained part hierarchy information of 3D shapes based on the large-scale 3D CAD model dataset ShapeNet <ref type="bibr" target="#b0">[1]</ref> and 3D Warehouse. Although PartNet mentioned affordance as potential application, there is still no benchmark purposely established for affordance yet. More importantly, we discover, via user annotations, that the human perceived affordance often do not fully overlap with the individual parts specified in PartNet dataset. For example, In the first row of <ref type="figure">Fig. 1</ref>, the Pour, Wrap-Grasp and Contain affordance from Mug do not perfectly match any part indicated by the colored image on the 1st column. Therefore, we believe it is necessary to provide a new set of affordance labels on the PartNet dataset.</p><p>Creating 3D visual affordance benchmark is challenging due to the subjective definition. We take into account the affordance definitions from existing research on visual affordance learning in 2D and 2.5D domains <ref type="bibr" target="#b7">[8]</ref> and select possible interactions that one can take with 3D shapes from PartNet. Finally, 18 types of affordance were formally defined over 23 semantic objects. Additional challenge associated with annotation on 3D model is the scalability issue. In order to provide highly quality annotation on such a large scale, we use label propagation method to propagate affordance sparsely labeled on individual points. Eventually, we obtain point-wise probabilistic score of affordance for each individual shape in PartNet. We name the new benchmark 3D AffordanceNet to reflect the focus on visual affordance on 3D point cloud data.</p><p>3D AffordanceNet enables benchmarking a diverse set of tasks, in particular, we put forward full-shape, partial-view and rotation-invariant affordance estimations. Three stateof-the-art point cloud deep learning networks are evaluated on all tasks. We also propose a semi-supervised affordance estimation method to take the advantage of large amount of unlabeled data for affordance estimation.</p><p>In summary, we make the following contributions:</p><p>? We introduce 3D AffordanceNet, consisting of 56307 well-defined affordance information annotations for 22949 shapes covering 18 affordance classes and 23 semantic object categories. To the best of our knowledge, this is the first large-scale dataset with welldefined probabilistic affordance score annotations;</p><p>? We propose three affordance learning tasks which are supported by 3D AfffodanceNet to demonstrate the value of annotated data: full-shape affordance estimation, partial-view affordance estimation and rotationinvariant affordance estimation.</p><p>https://3dwarehouse.sketchup.com ? We benchmark three baseline methods for proposed affordance learning tasks and further propose a semisupervised affordance estimation method to take advantage of unlabeled data for affordance estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Affordance refers to the possible action an agent could make to interact with the environment <ref type="bibr" target="#b5">[6]</ref>. Examples include a cup can afford 'pouring', a bed is 'sittable' and 'layable', etc. Affordance understanding is the core function in developing autonomous systems. In particular, the visual affordance understanding is the most promising way due to the rich information carried by visual sensors. We mainly review the recent development in visual affordance dataset and approaches, a detailed review can be found in <ref type="bibr" target="#b7">[8]</ref>. Recent advances in visual affordance are mostly demonstrated on affordance recognition <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b2">3]</ref>, detection <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref> and segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b17">18]</ref>. Beyond the low-level visual tasks, there is substantial attention on-the-rise paid to affordance reasoning, affordance-based activity recognition and social affordances. In this work we are interested in providing a benchmark for affordance segmentation, a.k.a. prediction, due to the clear definition and high demand in robotic applications.</p><p>To benchmark visual affordance segmentation, UMD <ref type="bibr" target="#b17">[18]</ref>, CAD120 <ref type="bibr" target="#b22">[23]</ref> and IIT-AFF <ref type="bibr" target="#b18">[19]</ref> are respectively developed recently. All datasets feature affordance segmentation on RGBD images covering from 10-20 objects, 6-9 affordance types and 3k-10k labelled images. In particular, IAF-IIT and CAD120 capture complex scenes while UMD mainly focuses on well-controlled scenes. Nevertheless, none of these datasets carry the rich geometric properties of objects that robotic application would expect and only a single view-point is present. As a result, these datasets no longer pose challenges to modern computer vision techniques.</p><p>With the easy access to 3D point cloud data, e.g. collected from LiDAR and SFM, and potential application in robotics, atunomous driving, etc., there is a recent surge in research towards 3D point cloud. ShapeNet <ref type="bibr" target="#b0">[1]</ref> collected 3D CAD models from open-sourced 3D repositories, with more than 3,000,000 models and 3,135 object categories. It was further developed by <ref type="bibr" target="#b29">[29]</ref> for shape part segmentation. Partially motivated by affordance understanding, the subsequent PartNet dataset <ref type="bibr" target="#b16">[17]</ref> was proposed with 26k objects, featuring fine-grained semantic segmentation task. Hierarchical segmentation was also addressed by a recursive part decomposition <ref type="bibr" target="#b31">[31]</ref>. Though affordance is briefly mentioned as the motivation for creating above 3D shape datasets, to the best of our knowledge, there is no existing dataset which explicitly addresses the task of visual affordance prediction. The only known attempt on 3D shape functionality understanding <ref type="bibr" target="#b8">[9]</ref> is still limited to a few types</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Start</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observe shape information</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>What affordances does shape support?</head><p>Which points on the shape support the affordances?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Select a functional point</head><p>Does the adjacent parts support the same affordances?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Select several supported parts</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Yes</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>No</head><p>Repeat several times Annotated point <ref type="figure">Figure 2</ref>. The annotation workflow. The blue arrows indicate the annotation procedures, the green arrows refer to the corresponding 3D GUI actions. The annotators are first asked to determine the supported affordance classes and then select the functional points. The annotators need to confirm whether the adjacent parts support the same affordances.</p><p>of objects and did not make connection to the well-studied visual affordance understanding in 2D image domains. In contrast, we created a new benchmark for visual affordance estimation on 3D point cloud. The affordance types are selectively inherited from a summary of existing works and annotations are made on 3D point cloud data directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset Construction</head><p>We present 3D AffordanceNet as a dataset for affordance estimation 3D point cloud. To construct this dataset, we first define a set of affordance types by referring to the existing visual affordance works <ref type="bibr" target="#b7">[8]</ref>. Raw 3D shape data are collected from the shapes in PartNet <ref type="bibr" target="#b16">[17]</ref> which covers common object types in typical indoor scenes. A questionanswering 3D GUI is developed to collect raw point-wise annotation on mesh shapes. In total, we hired 42 professional annotators for annotations, the average annotation time per shape is 2 minutes and each shape is annotated by 3 annotators.Finally, label propagation is employed to obtain probabilistic ground-truth for the shape point cloud.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Affordance Type</head><p>We refer to <ref type="bibr" target="#b7">[8]</ref> for a full review of affordance types adopted in visual affordance research. From the full list of possible affordances, we select those suitable for 3D objects present in PartNet <ref type="bibr" target="#b16">[17]</ref> and remove the irrelevant ones, e.g. 'reachable' and 'carryable'. Overall, we filter out 18 categories of affordances, namely 'grasp', 'lift', 'contain', 'open', 'lay', 'sit', 'support', 'wrap-grasp', 'pour', 'display', 'push', 'pull', 'listen', 'wear', 'press', 'cut', 'stab', and 'move'. Then, we associate the affordance types to each category of object in PartNet according to its attributes and functionality that it can afford to interact with human or robot. For example, a chair is 'sittable' but not 'layable', a table can afford 'support' but not 'contain', etc. The affordances of each category are shown in Tab. 2. The annotators are allowed to freely determine where the affordance locates on the object, e.g. 'grasp' of bag can be annotated at its handle, webbing or straps. Notice that we allow the annotators to select the supported affordance for each shape, thus some shapes may not have all affordances defined for its own shape category.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Annotation Interface</head><p>We created a web-based 3D GUI to collect raw annotations. The process of annotation is designed to be a question-answering workflow as illustrated in <ref type="figure">Fig. 2</ref>. A user is given one shape at a time visualized in 3D. Each individual parts are colored according to the pre-defined colormap in PartNet dataset <ref type="bibr" target="#b16">[17]</ref>. Annotators are allowed to freely rotate, translate and change the scale of the shape using mouse, which allow the annotators to observe the shape from more angles. After observation, annotators are first asked to determine the supported affordances by choosing from a list ('What affordances does this shape support?'). Considering that some annotators may not understand the affordances, we provide the explanation of each affordance on the interface. Annotators are then asked to select keypoints that support the specified affordance ('What points on the shape support the affordance?'). At least 3 keypoints will be labeled by one annotator for each affordance. Annotators will also decide whether the selected affordance will propagate beyond the part which the labeled keypoint sits on. If yes, annotators are asked to select eligible parts that the affordance can propagate to, otherwise, more annotations will be made on the same part until enough keypoints are collected.</p><p>The questions that the annotation interface proposes for each affordance directly determine how the annotators perceive the affordances. Therefore, we define questions carefully tailored for each affordance. Some questions are shown in Tab. 1. A complete list of affordance question is given in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Ground-Truth Construction</head><p>After obtaining affordance keypoints, we propagate labels to all points on the shape to create ground-truth for downstream learning tasks. We first record the coordinates of the selected keypoints. We then propagate the labels to N points densely sampled on the shape mesh surface, note  <ref type="figure">Figure 3</ref>. The example of annotated data. Different affordances are shown in different colors, points annotated with multiple affordances are colored by the affordance that has the highest scores. The brighter the color, the higher the score.</p><p>that we only propagate on the parts that support the specific affordance that are recorded during user annotation. Formally, we construct a kNN graph on sampled points where the adjacency matrix A writes as,</p><formula xml:id="formula_0">a ij = v i ? v j 2 , v j ? N N k (v i ) 0, otherwise<label>(1)</label></formula><p>where v is the xyz spatial coordinate of point and N N k denotes the set of k nearest neighbors. The adjacency matrix is symmetrized by W = 1/2(A + A ). Then we normalize the adjacency matrix by W = D ?0.5 WD ?0.5 . where D is the degree matrix. Finally the scores S for all points is propagated by the closed-form solution S = (</p><formula xml:id="formula_1">I?? W ?1 )Y.</formula><p>where Y ? {0, 1} N ?18 is the one-hot label vector and 1 indicates positive label. ? is a hyper-parameter controlling the decreasing speed of S, we empirically set ? to 0.998 throughout the experiments. Finally we linearly normalize S to the range between 0 and 1 so that it is a probabilistic score. Example shapes with propagated affordance groundtruth are shown in <ref type="figure">Fig. 3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Statistics</head><p>The final dataset provides well-defined visual affordance score map annotations for 22949 shapes from 23 shape categories with at most 5 affordance types defined for each category. From the perspective of affordance categories, 3D AfffodanceNet contains 56307 affordance annotations from 18 affordance classes. It is worth noting that due to the multi-label nature, each point could be labeled with multiple affordances. More details of the dataset are presented in Tab. <ref type="bibr" target="#b1">2</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Tasks and Benchmarks</head><p>In this section, we benchmark three tasks to demonstrate the 3D AfffodanceNet dataset, namely, full-shape, partialview and rotation-invariant affordance estimation. The 3D AffordanceNet dataset is split into train, validation and test Support Move</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sit</head><p>Contain <ref type="table" target="#tab_0">Open Grasp Pour Display Wrap-Grasp  #Annot  14848  14540 6516  5155  4506  2253  2086  1914  1889  Press  Cut  Stab  Wear  Listen  Pull  Push  Lay  Lift  #Annot  588  393  393  231  228  225  225  194  123   Table 3</ref>. The number of shapes that are positive for each category of affordance.</p><p>sets with a ratio of 70%, 10% and 20%, respectively according to the shape semantic category. The first experiment estimates point-wise affordance given full 3D point cloud as input. The second experiment estimates the affordance of partially visible objects observed from different viewpoints. The last experiment estimates the affordance of rotated 3D objects under two different rotation settings. We also create a semi-supervised affordance estimation benchmark to explore the opportunity of exploiting unlabeled data for affordance estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Full-Shape Affordance Estimation</head><p>Given an object as 3D point cloud without knowing the affordances supported by the object, the full-shape affordance estimation task aims to estimate the supported affordance type and predict the point-wise probabilistic score of affordance. We show that state-of-the-art 3D point cloud segmentation networks predict reasonable results on 3D Af-fordanceNet. Network and Training. We evaluate three network architecture, namely PointNet++ <ref type="bibr" target="#b19">[20]</ref>, DGCNN <ref type="bibr" target="#b27">[27]</ref> and U-Net <ref type="bibr" target="#b28">[28]</ref> for this task. To obtain the point-wise score, we utilized the segmentation branch of PointNet++ and DGCNN as shared backbones to extract features for each point, then for each affordance type, we pass the features through multiple classification heads and used a sigmoid function to obtain the posterior scores. The classification heads were set up for each affordance category individually while the backbone networks were shared. We use cross-entropy loss L CE for training the network as below,</p><formula xml:id="formula_2">l CE = 1 N M i N j ?(1 ? t ij )log(1 ? p ij ) ? t ij log(p ij ) (2)</formula><p>where M is the total number of affordance types, N is the number of points within each shape, s ij is the ground truth score of jth point of ith affordance category and p ij is the predicted score.</p><p>Since the points with zero score account for a relatively large proportion of the total dataset, we further propose to use dice loss <ref type="bibr" target="#b14">[15]</ref> to mitigate the imbalance issue. The dice loss l DICE is defined as:</p><formula xml:id="formula_3">l DICE = M i 1 ? N j s i,j p i,j + N j s i,j + p i,j + ? N j (1 ? s i,j )(1 ? p i,j ) + N j 2 ? s i,j ? p i,j +<label>(3)</label></formula><p>Finally the loss function is defined as l = l CE + l DICE . We train PointNet++ and DGCNN on the 3D AffordanceNet us-ing the default training strategies and hyper-parameters described in respective papers <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b27">27]</ref>. For Unet, we fine-tune the network initialized by the pre-trained weight provided by PointContrast <ref type="bibr" target="#b28">[28]</ref>. Evaluation and Results. We evaluate four metrics for affordance estimation, including mean Average Precision (mAP) scores, mean squared error (MSE), Area Under ROC Curve (AUC) and average Intersection Over Union (aIoU). For AP, we calculate the Precision-Recall Curve and AP is calculated for each affordance. For AUC, we report the area under ROC Curve. For MSE, we calculate mean squared error of each affordance category and sum up the results from all affordance categories. For aIoU, we gradually tune up the threshold from 0 to 0.99 with 0.01 step to binarize the prediction, and the aIoU is the arithmetic average of all IoUs at each threshold. Except for the MSE, all the other metrics for each category are averaged over all shapes, a.k.a. macro-average. For each affordance category, the groundtruth map is binarized with 0.5 threshold before evaluation. The results are reported in Tab. 4 under the Full-Shape section and some qualitative examples from PointNet++ are selected and visualized in <ref type="figure" target="#fig_0">Fig. 4</ref>.</p><p>As shown in Tab. 4, the performances of three networks are close and all achieve a relatively low aIOU score, which indicates that affordance estimation is still a challenging task. Comparing the second row of <ref type="figure" target="#fig_0">Fig. 4</ref> to corresponding ground truth, we found that PointNet++ produces some reasonable results. For example, the estimations of grasp on a bag are successfully localized on both the handles and the webbing. However, the results of pour on a bottle fail since the network predicts the scores mainly on the lid of the bottle rather than the body edge of the bottle where is the place that the water flow out. More qualitative examples are given in the supplementary. Room for Performance Improvement The performances of PointNet++ and DGCNN on the tasks mentioned above are relatively weak. Hence, we evaluate the trained network on full-shape affordance estimation task over the training, validation and testing sets to investigate the room for performance improvement with results reported in Tab. 5. From the results we observe that both two networks still underfit, meaning that the proposed affordance estimation task is very challenging for existing point cloud analysis networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Partial-View Affordance Estimation</head><p>Although complete point clouds or meshes can provide detailed geometric information for affordance estimation, in real-world application scenarios, we can only expect partial view of 3D shapes, represented as partial point cloud. Therefore, another important task we are concerned with is to estimate the affordance from partial point cloud. Network and Training. To obtain partial point clouds, we follow <ref type="bibr" target="#b10">[11]</ref> to synthesize point cloud observed from certain  camera viewpoints. Only points directly facing the camera will be preserved as visible points and each point is assigned a radius to create occlusion effect. In specific, because all shapes are well aligned within the (-1,-1,-1) to (1,1,1) cube, we set up 4 affine cameras located at (1,1,1), (-1,-1,1), (1,-1,-1), (-1,1,-1) in Cartesian coordinate system, facing towards the origin. After obtaining the partial point clouds, we sample 2048 points from each viewpoint via furthest point sampling, if the number of points of the point cloud is fewer than 2048, we utilize the point cloud up-sampling method proposed in <ref type="bibr" target="#b30">[30]</ref> to up-sample the data. We use ex-actly the same backbone networks and training strategies described in previous sections.</p><p>Evaluation and Results. During testing stage, we estimate the affordance on the visible partial point cloud only. The evaluation protocol follows the one described in Section 4.1. All evaluation metrics are reported in Tab. 4 with qualitative results from PointNet++ shown in <ref type="figure" target="#fig_0">Fig. 4</ref>.</p><p>Unsurprisingly, the quantitative performances of three networks have decreased due to the loss of geometric information of partial point cloud relative to complete point cloud. Nevertheless, we still observe reasonable qualitative results even though only a partial view is observed. For instance, the network produces high prediction for move on the upside of the legs of a table despite the unseen parts of the legs. The grasp for bag, hear for earphone, sit for chair, etc., are all more-or-less correctly predicted. In contrast, the estimation for contain on storage furniture are par-  tially missing since it predicts the scores on the top of the furniture which is not fully observed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bag Earphone</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Rotation-Invariant Affordance Estimation</head><p>The shapes in 3D AffordanceNet are all aligned in canonical poses, however, the data observed by sensors in real world are not always in canonical poses. The difference in rotation between real data and training data will lead to a performance drop in real-world usage which inspired research into rotation equivariant network <ref type="bibr" target="#b4">[5]</ref>. Hence, it is critical to train the algorithms to estimate affordance on rotated objects. In this section, we provide a benchmark for affordance estimation subject to two types of rotations. Network and Training. We used the same backbone networks, training strategies and hyper-parameters described in the Sect. 4.1. We propose two different rotation settings for experiment: z/z and SO(3)/SO(3) where z/z means rotation is applied along z axis only for both training and inference stages while SO(3)/SO(3) refers to SO(3) rotation, i.e. freely rotation along x, y and z axes. During training session, we randomly sample rotation poses between [0,2?] for each shape in the training mini-batch on-the-fly. We train proposed methods on complete point cloud. For testing phase, we randomly sample 5 rotation poses for each shape for both rotation settings and fix the sampled rotations for testing data. Evaluation and Results. We calculate mAP, AUC, aIOU on the proposed methods. Quantitative results are presented in Tab. 4 and qualitative results are shown in the fourth and fifth rows of <ref type="figure" target="#fig_0">Fig. 4</ref> for z/z and SO(3)/SO(3) settings with PointNet++ as backbone. We observe that the performances on both z/z and SO(3)/SO(3) settings dropped compared to canonical view experiments. In particular, for z/z setting, the performance dropped around 1% in all metrics for all networks as backbone. While more significant loss of performance is observed for SO(3)/SO(3) setting with 5 ? 10% drop in all metrics. This is aligned with our expectation that SO(3)/SO(3) is a much more challenging task. We further make observations from the qualitative results in <ref type="figure" target="#fig_0">Fig. 4</ref>. First, under z/z rotation scheme, despite the consistent performance drop, affordance estimations are largely correct across most categories. Obvious mistakes are made in bottle and microwave where the former missed the tip which supports pour while the latter mistakenly predict the while door of microwave for open. Under the more challenging SO(3)/SO(3) scheme, there is still a visually satisfying results for most shapes. The most prominent error is made on storage furniture where contain is totally missed, probably because the complex geometric structure (many concave shapes) renders contain a hard affordance to learn under arbitrary rotation. In general, we believe affordance estimation under SO(3)/SO(3) a very challenging task and it deserves further investigation. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Semi-Supervised Affordance Estimation</head><p>Although the label propagation procedure allows the annotators to only annotate a few keypoints on the object surface, affordance annotation still remains as an expensive and labor intensive procedure. Inspired by the recent success in semi-supervised learning (SSL) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b15">16]</ref> we establish a benchmark for semi-supervised affordance estimation. We synthesize a semi-supervised setting by randomly sampling 1% training data, assumed to be labeled, and the rest are assumed to be unlabeled data. The validation and testing sets are kept the same with standard benchmarks. Network and Training. We utilize DGCNN <ref type="bibr" target="#b27">[27]</ref> as our backbone. During every mini-batch in the training stage, we randomly sample a equal number of labeled data X l and unlabeled data X ul . To fully exploit the unlabeled data, we employ a state-of-the-art semi-supervised learning framework, namely Virtual Adversarial Training (VAT) <ref type="bibr" target="#b15">[16]</ref>. It encourages the consistency between the posterior of unlabeled sample and its augmentation, measured by mean square error,</p><formula xml:id="formula_4">l mse = 1 N M i N j ||p i,j ?p i,j || 2 2<label>(4)</label></formula><p>wherep i,j is the posterior prediction for augmented sample.</p><p>To best exploit the consistency power, the augmentation is obtained by first applying a one step adversarial attack, the corresponding adversarial perturbation is then added to the original point cloud to produce the augmentation. Finally, the total loss for semi-supervised affordance estimation combines both losses defined for labeled data and unlabeled data.</p><formula xml:id="formula_5">l = l CE + l DICE + l l mse + l u mse<label>(5)</label></formula><p>where l l mse and l u mse is the mean square error (MSE) calculated between labeled and unlabeled data, respectively. We compare the semi-supervised approach against a fully supervised baseline which is trained on the 1% labeled data alone with cross-entropy and dice loss. We use a mini-batch of 16, 8 for labeled data and 8 for unlabeled data, and follow the same training strategies and hyper-parameters described in <ref type="bibr" target="#b15">[16]</ref>. We train a full-shape affordance estimation method based on DGCNN only on the labelled data following the description in Sect. 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation and Results</head><p>We evaluated the methods following the metrics described in Sect. 4.1 with results reported in Tab. 6. Comparing the performance of semisupervised affordance estimation to the full-shape one, we found that semi-supervised affordance estimation outperforms the fully supervised baseline on all three metrics. Specifically, the gains for some affordance categories (e.g. open) that have low metrics on full-shape affordance estimation are high, which indicates that unlabeled data can provide useful information for affordance learning. In conclusion, we believe that exploiting unlabeled data to improve the performance has practical value and should receive more attention in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we proposed 3D AffordanceNet, a 3D point cloud benchmark consisting of 22949 shapes from 23 semantic object categories, annotated with 56307 affordance annotations and covering 18 visual affordance categories. Based on this dataset, we define three individual affordance estimation tasks and benchmarked three stateof-the-art point cloud deep learning networks. The results suggested future research is required to achieve better performance on difficult affordance categories and under SO(3) rotation. Furthermore, we proposed a semisupervised affordance estimation method to take advantage of large amount of unlabeled data. The proposed dataset encourage the community to focus on affordance estimation research.</p><p>This supplementary material provides additional dataset visualization, qualitative results, technical details, full question list and the interface of annotation tool.</p><p>In Sec. A we present all questions visible to annotators during data annotation procedure. Data annotation agreement is discuss in Sec.B. We further provide more groundtruth visualizations for each shape category in 3D Affor-danceNet dataset in Sec. C. The Sec. D presents more qualitative examples of full-shape, partial-view and rotationinvariant affordance estimation results with PointNet++ and DGCNN as backbones. In Sec. E we describe more details about neural network architecture and training parameters. We visualize the annotation interface and the main components of our web-based annotation tool in Sec. F. Then we explore the performance gain benefit from fine-tuning in Sec. G. At last we demonstrate why a truly functional understanding of object affordance requires learning and prediction in the 3D physical domain in Sec. H.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Complete List of Questions</head><p>We list all questions that the annotation interface displays to annotators. The complete question list is shown in Tab. 7. The questions that the annotation interface proposes directly determine how the annotators understand the affordance, thus we carefully define the questions for each affordance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Annotation</head><p>We demonstrate that the human perceived affordance often do not fully overlap with the individual part specified in PartNet dataset. To investigate the relation between parts and affordances, we report the maximal IoU between each affordance and a best combination of parts (most finegrained level) in Tab.8. The low IoU indicates that it is impossible to combine several parts to derive affordance, suggesting the necessity to label affordance from scratch.</p><p>We have 3 annotators to label each object, which gives certain diversities and partially addresses the issue of ambiguities. We report the inconsistency as average variance across 3 annotators in Tab.8. The relatively smaller variance of each affordance indicates that there exists patterns in the annotations, and models can therefore learn affordance from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. More Ground-Truth Visualizations</head><p>We present more ground-truth visualization in <ref type="figure" target="#fig_5">Fig. 10</ref> and <ref type="figure">Fig. 11</ref>. From the visualization of ground-truth, we can observe that the human perceived affordances often do not fully overlap with the individual parts specified in PartNet dataset, therefore it justifies the need to annotate affordance separately from existing part annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. More Qualitative Examples</head><p>We present more qualitative examples for full-shape, partial-view and rotation-invariant affordance estimation experiments with both PointNet++ and DGCNN as backbone in <ref type="figure" target="#fig_1">Fig. 5, 6, 7 and 8</ref>.</p><p>The estimation results from PointNet++ and DGCNN are quite interesting. The predicted affordance locations from the two networks are close while the confidences of the points belonging to specific affordances have different tendencies. In many cases, e.g. grasp for bag and press for laptop, PointNet++ tends to predict scores with low confidence which will cause more false-negative predictions while DGCNN predicts scores more aggressively, leading to more false-positive examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Training Details</head><p>In this section we describe more details of training procedure. We conduct all experiments using the segmentation branch of PointNet++ and DGCNN as shared backbones.</p><p>In specific, the dimension of point-wise features by PointNet++ and DGCNN are 128 and 256, respectively. We formulate affordance estimation as a binary classification problem, therefore, we set up classification heads for each affordance category, in total there are 18 classification heads which have the same architecture with different parameters. Specifically, the classification head for each affordance category consists of F C(m, 128), F C(128, 1), where the function F C(x, y) denotes a fully connected layer that takes x dimension vectors as inputs and outputs y dimension vectors, the number m denotes the dimension of point-wise features by the shared backbones (in our case, it will be 128 for PointNet++ and 256 for DGCNN). In practice, the first F C is followed by a Batch Normalization layer.</p><p>For PointNet++, we set the training learning rate 0.001 and optimize the parameters with Adam optimizer, the learning rate is reduced by half every 20 epochs, we train the network for 200 epochs, the batch size is 16. The weight decay for Adam optimizer is set to 1e-8. For DGCNN, we set the learning rate to 0.1 and optimize the parameters with SGD optimizer, the momentum and weight decay for SGD are set as 0.9 and 1e-4, respectively. We use a cosine annealing algorithm to adjust the learning rate where the algorithm can be described as followed:</p><formula xml:id="formula_6">? t = ? min + 1 2 (? max ? ? min )(1 + cos( T cur T max ?)) (6)</formula><p>where ? t is the adjusted learning rate, ? min is the minimum learning rate, ? max is set to the initial learning rate, T cur is If you want to pull the door, which points on the door will you pull with your finger?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Earphone Grasp</head><p>If you want to grab this earphone, where will your palm position be? Listen</p><p>If you want to listen to music with headphones, which points on the headphones will point to your ears?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Faucet Grasp</head><p>If you want to grab this faucet, which points on the faucet will your palm touch? Open</p><p>If you want to boil water, at which points on the tap would you open the water valve?</p><p>Hat Grasp If you want to grab this hat, which points on the hat will your palm touch? Wear</p><p>If you want to wear this hat, which points on the hat will make contact with your head? Keyboard Press If you want to press keys on the keyboard, which points on the keyboard would you press? Knife Grasp If you want to grab this knife, at which points on the handle will your palm touch? Cut</p><p>If you want to cut something with this knife, which points on the blade will come into contact with the object? Stab</p><p>If you use this knife to poke an object, which points on the blade will come into contact with the object?</p><p>Laptop Display If you look on the computer screen, which points on the screen will you look at? Press</p><p>If you want to press keys on a computer keyboard, which points on the keyboard would you press? Microwave If you put something in the microwave, at which points in the microwave would you put the object?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mug</head><p>Pour Suppose there is water in the mug, and you want to pour the water out of the mug. From which points on the mug will the water flow? Contain</p><p>If you pour water into the mug, which points will the water first touch when it falls into the mug? Wrap-Grasp If you wrap-grasp this mug with your hand, which points on the mug will your palm touch? Grasp</p><p>If you grab this mug, which points on the mug handle will your palm touch? If you pour water into the vase, which points will the water first touch when it falls into the vase? Pour</p><p>Suppose there is water in the vase, and you want to pour the water out of the vase. From which points on the vase will the water flow out? Wrap-Grasp If you wrap-grasp this vase with your hands, which points on the vase will your palm touch? the number of current epochs. We set the batch size to 16 and train the network for 200 epochs. Particularly, for semi-supervised affordance estimation, we use DGCNN as shared backbone and follow the training strategies described above. We set the batch size to <ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b7">8</ref> for labeled data and 8 for unlabeled data. We set the ? and of Virtual Adversarial Training to 1e-6 and 2.0, which are the default hyper-parameters described in its paper. We calculate the virtual adversarial direction in 1 iteration which is recommended by the original paper. We implement all experiments with PyTorch.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. GUI Interface for Annotation</head><p>We show the GUI interface of web-based annotation tool in <ref type="figure">Fig. 9</ref>. We manually modify the annotation system released by PartNet fit our requirements. We color the parts of shapes according to the pre-defined colormap in PartNet dataset.</p><p>The annotators can observe the geometric information of shapes and are able to freely translate, rotate and change the scale of shapes in 3D GUI. In Question Workflow, the annotation interface asks the annotators some questions to guide them to select keypoints on the surface of shapes. From Supported Affordance, the annotators can check the supported affordances that are determined by selecting the corresponding affordances in Affordance List.  <ref type="table">Table 9</ref>. The full-shape affordance estimation performance comparison between the U-Net fine-tuned on our dataset and the U-Net trained from scratch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. PointContrast Fine-Tune</head><p>In 2D vision, in order to boost the performance, it is popular to fine-tune a network on the smaller target set where the network was pre-trained on a rich source set. Recently, PointContrast shows that by pre-training the network on the ScanNet dataset using contrastive learning, the pre-trained network can achieve the state-of-the art performances via fine-tuning on several downstream tasks. To explore the opportunity of boosting performances of affordance estimation by fine-tuning the pre-trained network on our 3D AffordanceNet dataset, we utilize the U-Net architecture and the pre-trained weight provided by PointContrast. We then fine-tune the network using SGD optimizer with learning rate=0.1, weight decay=1e-4 and momentum=0.9 for 60 epochs. The loss function that we use to fine-tune the network is the same as the one proposed in the Section 4.1. We also train the network straightly from scratch with the same network architecture for the comparison.</p><p>Tab. 9 reports the performances of both fine-tuned and trained-from-scratch U-Net on full-shape affordance estimation task. The results show that the performances of the fine-tuned one surpass the one that is training from scratch, meaning that the network can benefit from the pre-training on a rich source dataset during the fine-tune process on the   Previous works on affordance understanding focus on learning affordances in 2D or 2.5D domain, however, many types of affordance are related to functional attributes of objects, and the relevant actions can only be accomplished given 3D affordance reasoning on the object surface. For example, a successful grasp of mug replies on inference of surface grasp points (i.e., prediction of the grasp affordance) that may be self-occluded in a single-view observation (i.e., 2.5D). Annotated 3D affordance data facilitate reasoning on the complete object surface.</p><p>To quantify the benefit, we conduct the following experiments based on our dataset. We randomly sample one single view (2.5D) from each object for training, namely single-view partial (SGV), and randomly sample 4 views from each object, namely multi-view partial (MTV), then we test the SGV/MTV models on full-shape data. We compare SGV and MTV with training on full 3D data (3DV). Results in Tab. 10 verify our analysis. It is worth noting that the ground-truth of single view (2.5D) also relies on 3D annotation.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supported Affordance</head><p>Affordance List Question Workflow 3D GUI <ref type="figure">Figure 9</ref>. The annotation interface of our web-based annotation tool. We show the GUI and main component of the annotation interface.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grasp Contain Lift</head><p>Open Lay Sit Support Wrap. Pour   <ref type="table">Table   Trash</ref> Can Vase <ref type="figure">Figure 11</ref>. Ground Truth data visualization(2/2).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 4 .</head><label>4</label><figDesc>Qualitative results for affordance estimation. The top row shows the ground truth. The second row shows the full-shape estimated results, the third row shows the partial-view estimated result, the fourth and the bottom row show the z/z and SO(3)/SO(3) rotationinvariant estimated results, respectively. All results come from PointNet++. The top words indicate the semantic category of each column and the bottom words indicate the affordance category. The greener the color of the points, the higher the confidence about specific affordance types. Wrap. is the abbreviation of Wrap-Grasp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 .</head><label>5</label><figDesc>Qualitative results for affordance estimation from PointNet++(1/2). The top row shows the ground-truth. The second row shows the full-shape estimated results, the third row shows the partial-view estimated result, the fourth and the bottom row show the z/z and SO(3)/SO(3) rotation-invariant estimated results, respectively. All results come from PointNet++. The top words indicate the semantic category of each column and the bottom words indicate the affordance category. The greener the color of the points, the higher the confidence about specific affordance types. Wrap. is the abbreviation of Wrap-Grasp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Qualitative results for affordance estimation from PointNet++(2/2). The top row shows the ground truth. The second row shows the full-shape estimated results, the third row shows the partial-view estimated result, the fourth and the bottom row show the z/z and SO(3)/SO(3) rotation-invariant estimated results, respectively. All results come from PointNet++. The top words indicate the semantic category of each column and the bottom words indicate the affordance category. The greener the color of the points, the higher the confidence about specific affordance types. Wrap. is the abbreviation of Wrap-Grasp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 .</head><label>7</label><figDesc>Qualitative results for affordance estimation from DGCNN(1/2). The top row shows the ground truth. The second row shows the full-shape estimated results, the third row shows the partial-view estimated result, the fourth and the bottom row show the z/z and SO(3)/SO(3) rotation-invariant estimated results, respectively. All results come from DGCNN. The top words indicate the semantic category of each column and the bottom words indicate the affordance category. The greener the color of the points, the higher the confidence about specific affordance types. Wrap. is the abbreviation of Wrap-Grasp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Qualitative results for affordance estimation from DGCNN(2/2). The top row shows the ground truth. The second row shows the full-shape estimated results, the third row shows the partial-view estimated result, the fourth and the bottom row show the z/z and SO(3)/SO(3) rotation-invariant estimated results, respectively. All results come from DGCNN. The top words indicate the semantic category of each column and the bottom words indicate the affordance category. The greener the color of the points, the higher the confidence about specific affordance types. Wrap. is the abbreviation of Wrap-Grasp.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 10 .</head><label>10</label><figDesc>Ground Truth data visualization(1/2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>to lift this bag, at which points are your finger most likely to carry the bag? SitChair If you were sitting on this chair, on which points would you sit? MoveTable If you want to move this table, at which points on this table will you exert your strength? Open Trash Can If you want to open the lid of this trash can, from which points on the trash can you open it? PourBottle Suppose there is water in the bottle, and you want to pour the water out of the bottle. From which points on the bottle will the water flow out? Some examples of the proposed questions for affordance annotation.</figDesc><table><row><cell>Affordance</cell><cell>Object</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>and Tab. 3.</figDesc><table><row><cell>Object</cell><cell>Affordance</cell><cell>Num</cell></row><row><cell>Bag</cell><cell>grasp, lift, contain, open</cell><cell>125</cell></row><row><cell>Bed</cell><cell>lay, sit, support</cell><cell>181</cell></row><row><cell>Bowl</cell><cell>contain, wrap-grasp, pour</cell><cell>187</cell></row><row><cell>Clock</cell><cell>display</cell><cell>524</cell></row><row><cell>Dishwasher</cell><cell>open, contain</cell><cell>166</cell></row><row><cell>Display</cell><cell>display</cell><cell>887</cell></row><row><cell>Door</cell><cell>open, push, pull</cell><cell>220</cell></row><row><cell>Earphone</cell><cell>grasp, listen</cell><cell>223</cell></row><row><cell>Faucet</cell><cell>grasp, open</cell><cell>628</cell></row><row><cell>Hat</cell><cell>grasp, wear</cell><cell>222</cell></row><row><cell>Storage Furniture</cell><cell>contain, open</cell><cell>2186</cell></row><row><cell>Keyboard</cell><cell>press</cell><cell>156</cell></row><row><cell>Knife</cell><cell>grasp, cut, stab</cell><cell>314</cell></row><row><cell>Laptop</cell><cell>display, press</cell><cell>421</cell></row><row><cell>Microwave</cell><cell>open, contain, support</cell><cell>184</cell></row><row><cell>Mug</cell><cell>contain, pour, wrap-grasp, grasp</cell><cell>190</cell></row><row><cell>Refrigerator</cell><cell>contain, open</cell><cell>185</cell></row><row><cell>Chair</cell><cell>sit, support, move</cell><cell>6113</cell></row><row><cell>Scissors</cell><cell>grasp, cut, stab</cell><cell>68</cell></row><row><cell>Table</cell><cell>support, move</cell><cell>7990</cell></row><row><cell>Trash Can</cell><cell>contain, pour, open</cell><cell>315</cell></row><row><cell>Vase</cell><cell>contain, pour, wrap-grasp</cell><cell>1048</cell></row><row><cell>Bottle</cell><cell>contain, open, wrap-grasp, grasp, pour</cell><cell>411</cell></row></table><note>. 3D AffordanceNet statistics. The first column shows the object category. The second column shows the defined affordance classes for each category.The third column shows the amount of each shape semantic category.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table /><note>The performances of two different networks on full-shape affordance estimation task over train, validate and test sets. P rep- resents PointNet++ and D refers to DGCNN.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table Chair</head><label>Chair</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Bottle</cell><cell>Storage Furniture</cell><cell>Microwave</cell></row><row><cell>Ground Truth</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Full-Shape</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Partial</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rotate z/z</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Rotate SO(3)/SO(3)</cell><cell>Grasp</cell><cell>Listen</cell><cell>Move</cell><cell>Sit</cell><cell>Pour</cell><cell>Contain</cell><cell>Open</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 .</head><label>6</label><figDesc>The Results of Semi-Supervised Affordance Estimation. All numbers are in % except for MSE. We only implement semisupervised affordance estimation on DGCNN. The words Full-Shape and VAT represent full-shape estimation and semi-supervised affordance estimation with virtual adversarial training. Wrap. is the abbreviation of Wrap-Grasp.</figDesc><table><row><cell>VAT</cell><cell>Avg</cell><cell>Grasp</cell><cell>Lift</cell><cell cols="2">Contain Open</cell><cell>Lay</cell><cell>Sit</cell><cell cols="4">Support Wrap. Pour Display</cell><cell>Push</cell><cell>Pull</cell><cell cols="4">Listen Wear Press Move</cell><cell>Cut</cell><cell>Stab</cell></row><row><cell>mAP</cell><cell>36.6</cell><cell>34.8</cell><cell>78.8</cell><cell>44.3</cell><cell>22.6</cell><cell>34.9</cell><cell>64.8</cell><cell>41.4</cell><cell>13.3</cell><cell>41.5</cell><cell>58.9</cell><cell>13.5</cell><cell>8.3</cell><cell>20.5</cell><cell>8.5</cell><cell>29.5</cell><cell>19.4</cell><cell>33.1</cell><cell>90.2</cell></row><row><cell>AUC</cell><cell>78.8</cell><cell>75.8</cell><cell>95.3</cell><cell>78.4</cell><cell>79.2</cell><cell>71.2</cell><cell>92.4</cell><cell>86.7</cell><cell>56.5</cell><cell>83.7</cell><cell>89.9</cell><cell>69.0</cell><cell>74.8</cell><cell>74.1</cell><cell>51.4</cell><cell>88.8</cell><cell>62.7</cell><cell>89.3</cell><cell>99.1</cell></row><row><cell>aIOU</cell><cell>11.2</cell><cell>12.7</cell><cell>20.3</cell><cell>14.2</cell><cell>4.8</cell><cell>6.4</cell><cell>16.9</cell><cell>7.7</cell><cell>5.1</cell><cell>17.6</cell><cell>30.2</cell><cell>2.5</cell><cell>1.1</cell><cell>8.4</cell><cell>3.6</cell><cell>12.8</cell><cell>2.9</cell><cell>9.5</cell><cell>24.3</cell></row><row><cell>MSE</cell><cell cols="3">0.155 0.007 0.0001</cell><cell>0.02</cell><cell cols="3">0.006 0.0007 0.012</cell><cell>0.023</cell><cell cols="2">0.023 0.007</cell><cell>0.008</cell><cell cols="5">0.0003 0.0001 0.002 0.002 0.005</cell><cell>0.034</cell><cell cols="2">0.004 0.0003</cell></row><row><cell>Full-Shape</cell><cell>Avg</cell><cell>Grasp</cell><cell>Lift</cell><cell cols="2">Contain Open</cell><cell>Lay</cell><cell>Sit</cell><cell cols="4">Support Wrap. Pour Display</cell><cell>Push</cell><cell>Pull</cell><cell cols="4">Listen Wear Press Move</cell><cell>Cut</cell><cell>Stab</cell></row><row><cell>mAP</cell><cell>34.3</cell><cell>34.5</cell><cell>77.9</cell><cell>42.3</cell><cell>18.2</cell><cell>36.5</cell><cell>62.9</cell><cell>38.8</cell><cell>11.2</cell><cell>38.9</cell><cell>53.3</cell><cell>13.4</cell><cell>7.3</cell><cell>9</cell><cell>6.7</cell><cell>24.5</cell><cell>17.4</cell><cell>35.8</cell><cell>89.5</cell></row><row><cell>AUC</cell><cell>77.5</cell><cell>75.4</cell><cell>94.8</cell><cell>78.1</cell><cell>75.7</cell><cell>73.3</cell><cell>92.1</cell><cell>85.8</cell><cell>54.4</cell><cell>82.1</cell><cell>87.5</cell><cell>74.5</cell><cell>77.4</cell><cell>61.6</cell><cell>46.7</cell><cell>83.4</cell><cell>64.1</cell><cell>89.8</cell><cell>98.9</cell></row><row><cell>aIOU</cell><cell>9.8</cell><cell>11.2</cell><cell>28.1</cell><cell>14.3</cell><cell>2.5</cell><cell>10.0</cell><cell>23.4</cell><cell>9.8</cell><cell>2.2</cell><cell>7.5</cell><cell>19.9</cell><cell>1.9</cell><cell>1.0</cell><cell>1.6</cell><cell>1.6</cell><cell>6.8</cell><cell>2.3</cell><cell>5.6</cell><cell>26.5</cell></row><row><cell>MSE</cell><cell cols="3">0.105 0.009 0.0002</cell><cell>0.013</cell><cell cols="3">0.003 0.001 0.013</cell><cell>0.021</cell><cell cols="2">0.004 0.003</cell><cell>0.003</cell><cell cols="8">0.0002 0.0001 0.0004 0.001 0.0008 0.031 0.0007 0.0001</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>to lie on this bed, which points would you lie on the bed? Sit If you were to sit on this bed, at which points on the bed would you sit? Support If you put something on this bed, at which points on the bed would you put it? Bag Grasp If you want to grab the bag, at which points will your palm position be? Lift If you want to lift the bag, at which point is your finger most likely to carry the bag? Contain If you package things into the bag, at which points in this bag would you put? Open If you want to open the bag, from which points of the package would you open it? water into the bottle, which points will the water first touch when it falls into the bottle? Open If you want to open this bottle, from which points on the cap would you open it? PourSuppose there is water in the bottle, and you want to pour the water out of the bottle. From which points on the bottle will the water flow out?</figDesc><table><row><cell>Object</cell><cell>Affordance</cell><cell>Question</cell></row><row><cell cols="3">Bed If you were Bottle Lay Grasp If you grab the bottle, at which points on the bottle handle will your palm touch? Wrap-Grasp If you wrap-grasp the bottle, at which points on the bottle wall will your palm touch? Contain Wrap-Grasp If you wrap-grasp the bowl, at which points on the bowl wall will your palm touch? If you pour Bowl Contain If you want to put something in the bowl, at which points in the bowl would you put it?</cell></row><row><cell></cell><cell>Pour</cell><cell>Suppose there is water in the bowl, and you want to pour the water out of the bowl. From which points on the bowl will the water flow out?</cell></row><row><cell></cell><cell>Sit</cell><cell>If you were sitting on this chair, on which points would you sit?</cell></row><row><cell>Chair</cell><cell>Support</cell><cell>If you want to put something on the chair, at which points on the chair would you put it?</cell></row><row><cell></cell><cell>Move</cell><cell>If you want to move this chair, at which points on the chair will you exert force?</cell></row><row><cell>Clock</cell><cell>Display</cell><cell>If you want to look at the time, which points on this clock would you look at?</cell></row><row><cell>Dishwasher</cell><cell>Contain Open</cell><cell>If you want to load things in the dishwasher, at which points in the dishwasher would you put the things? If you want to open this dishwasher, from which points on the dishwasher door would you open it?</cell></row><row><cell>Display</cell><cell>Display</cell><cell>If you look on the screen, which points on the screen will you look at?</cell></row><row><cell></cell><cell>Push</cell><cell>If you want to push the door, at which points on the door will your palm touch?</cell></row><row><cell>Door</cell><cell>Open</cell><cell>If you were to open the door, from which points on the door would you open it?</cell></row><row><cell></cell><cell>Pull</cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>to grab this scissors, which points on the handle of the scissors will your palm touch? Cut If you want to use scissors to cut something, which points on the scissors blade will contact the object? StabIf you poke an object with this pair of scissors, which points on the blade will come into contact with the object?StorageFurniture Contain If you want to put something in the cabinet, at which points in the cabinet would you put it? Open If you want to open this cabinet, from which points on the cabinet door would you open it? Table Support If you want to put something on the table, at which points on the table would you put the object? Move If you want to move this table, at which points on this table will you exert your strength TrashCan Contain If you put trash in the trash can, which points will the trash drop first touch? Pour If you want to dump out the trash in the trash can, at which points on the trash can will the trash slip out? Open If you want to open the lid of this trash can, from which points on the trash can you open it?</figDesc><table><row><cell>Refrigerator</cell><cell>Contain Open</cell><cell>If you put things in the refrigerator, at which points in the refrigerator would you put? If you want to open the refrigerator, from which points on the refrigerator door would you open it?</cell></row><row><cell cols="3">Scissors If you want Vase Grasp Contain</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 .</head><label>7</label><figDesc>The complete list of questions that the annotation interface proposes to annotators</figDesc><table><row><cell cols="2">Affordance Grasp</cell><cell>Lift</cell><cell cols="2">Contain Open</cell><cell>Lay</cell><cell>Sit</cell><cell cols="3">Support Wrap. Pour</cell></row><row><cell>IOU</cell><cell>22.7</cell><cell>35.9</cell><cell>13.8</cell><cell>28.3</cell><cell>26.4</cell><cell>18.8</cell><cell>14.4</cell><cell>6.8</cell><cell>14.2</cell></row><row><cell>Variance</cell><cell>0.007</cell><cell>0.004</cell><cell>0.002</cell><cell cols="3">0.003 0.005 0.004</cell><cell>0.005</cell><cell cols="2">0.005 0.004</cell></row><row><cell cols="3">Affordance Display Push</cell><cell>Pull</cell><cell cols="3">Listen Wear Press</cell><cell>Move</cell><cell>Cut</cell><cell>Stab</cell></row><row><cell>IOU</cell><cell>39.8</cell><cell>13.3</cell><cell>34.1</cell><cell>17.4</cell><cell>7.3</cell><cell>29.7</cell><cell>12.3</cell><cell>16</cell><cell>3.5</cell></row><row><cell>Variance</cell><cell>0.005</cell><cell>0.005</cell><cell>0.002</cell><cell cols="3">0.002 0.006 0.18</cell><cell>0.009</cell><cell cols="2">0.004 0.001</cell></row><row><cell cols="10">Table 8. IOU between each affordance and a best combination of</cell></row><row><cell cols="10">parts. Variance of each affordance category. Numbers of IOU are</cell></row><row><cell>in %.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 10 .</head><label>10</label><figDesc>affordance estimation task, which may also works for the other networks such as PointNet++ and DGCNN. The comparisons between 3D and 2.5D. P and D refer to PointNet++ and DGCNN respectively.</figDesc><table><row><cell cols="6">H. Affordance Understanding in 3D</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">mAP AUC aIOU</cell><cell></cell><cell cols="3">mAP AUC aIOU</cell></row><row><cell>P 3DV</cell><cell>48.0</cell><cell>87.4</cell><cell>19.3</cell><cell>D 3DV</cell><cell>46.4</cell><cell>85.5</cell><cell>17.8</cell></row><row><cell cols="2">P MTV 45.1</cell><cell>84.4</cell><cell>16.6</cell><cell cols="2">D MTV 41.6</cell><cell>82.3</cell><cell>13.4</cell></row><row><cell>P SGV</cell><cell>35.0</cell><cell>77.8</cell><cell>12.9</cell><cell>D SGV</cell><cell>35.0</cell><cell>78.8</cell><cell>11.5</cell></row></table><note></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pat</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Hanrahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zimo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03012</idno>
		<title level="m">An information-rich 3d model repository</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to act properly: Predicting and explaining affordances from images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Yao</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaman</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Affordancenet: An end-to-end deep learning approach for object affordance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thanh-Toan</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An efficient context-free parsing algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jay</forename><surname>Earley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communications of the ACM</title>
		<imprint>
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning so (3) equivariant representations with spherical cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Esteves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christine</forename><surname>Allen-Blanchette</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note>Ameesh Makadia, and Kostas Daniilidis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The ecological approach to visual perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>J Gibson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What makes a chair a chair?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helmut</forename><surname>Grabner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Visual affordance and function understanding: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammed</forename><surname>Hassanin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salman</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Murat</forename><surname>Tahtali</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.06775</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning how objects function via co-analysis of interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruizhen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bojian</forename><surname>Oliver Van Kaick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ariel</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structural-rnn: Deep learning on spatio-temporal graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashesh</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Direct visibility of point sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagi</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ayellet</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronen</forename><surname>Basri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Special Interest Group on GRAPHics and Interactive Techniques</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning human activities and object affordances from rgb-d videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rudhir</forename><surname>Hema Swetha Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Anticipating human activities using object affordances for reactive robotic response</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hema</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashutosh</forename><surname>Koppula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Saxena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Temporal ensembling for semisupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">V-net: Fully convolutional neural networks for volumetric medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fausto</forename><surname>Milletari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nassir</forename><surname>Navab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seyed-Ahmad</forename><surname>Ahmadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 fourth international conference on 3D vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Partnet: A largescale benchmark for fine-grained and hierarchical part-level 3d object understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Angel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subarna</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Affordance detection of tool parts from geometric features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Austin</forename><surname>Myers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ching</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cornelia</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiannis</forename><surname>Ferm?ller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aloimonos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Object-based affordances detection with convolutional neural networks and dense conditional random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitrios</forename><surname>Kanoulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darwin</forename><forename type="middle">G</forename><surname>Caldwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><forename type="middle">G</forename><surname>Tsagarakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles Ruizhongtai Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Predicting human activities using stochastic grammar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyuan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song-Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A multi-scale cnn for affordance segmentation in rgb images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirban</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sinisa</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weakly supervised affordance detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johann</forename><surname>Sawatzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhilash</forename><surname>Srikantha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Learning to detect visual grasp affordance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyun Oh</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mario</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Goehring</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automation Science and Engineering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Tarvainen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Predicting actions from static scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuan-Hung</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Catherine</forename><surname>Olsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivan</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dynamic graph cnn for learning on point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sanjay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><forename type="middle">M</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Solomon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pointcontrast: Unsupervised pretraining for 3d point cloud understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiatao</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Demi</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Or</forename><surname>Guibas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Litany</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision</title>
		<meeting>the European Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Alla Sheffer, and Leonidas Guibas. A scalable active framework for region annotation in 3d shape collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Vladimir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Chao</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Daniel Cohen-Or, and Olga Sorkine-Hornung. Patch-based progressive 3d point set upsampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename><surname>Yifan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Partnet: A recursive part decomposition network for fine-grained and hierarchical shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fenggen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantic understanding of scenes through the ade20k dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tete</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanja</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adela</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Understanding tools: Task-oriented object modeling, learning and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yibiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Song</forename><forename type="middle">Chun</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
