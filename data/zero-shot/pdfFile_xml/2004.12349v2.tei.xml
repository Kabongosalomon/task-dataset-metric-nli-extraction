<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">When CNNs Meet Random RNNs: Towards Multi-Level Analysis for RGB-D Object and Scene Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Caglayan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Advanced Industrial Science and Technologhy (AIST)</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevrez</forename><surname>Imamoglu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Advanced Industrial Science and Technologhy (AIST)</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ahmet</forename><forename type="middle">Burak</forename><surname>Can</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Engineering</orgName>
								<orgName type="institution">Hacettepe University</orgName>
								<address>
									<settlement>Ankara</settlement>
									<country key="TR">Turkey</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryosuke</forename><surname>Nakamura</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Institute of Advanced Industrial Science and Technologhy (AIST)</orgName>
								<address>
									<settlement>Tokyo</settlement>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">When CNNs Meet Random RNNs: Towards Multi-Level Analysis for RGB-D Object and Scene Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T14:08+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recognizing objects and scenes are two challenging but essential tasks in image understanding. In particular, the use of RGB-D sensors in handling these tasks has emerged as an important area of focus for better visual understanding. Meanwhile, deep neural networks, specifically convolutional neural networks (CNNs), have become widespread and have been applied to many visual tasks by replacing hand-crafted features with effective deep features. However, it is an open problem how to exploit deep features from a multi-layer CNN model effectively. In this paper, we propose a novel two-stage framework that extracts discriminative feature representations from multi-modal RGB-D images for object and scene recognition tasks. In the first stage, a pretrained CNN model has been employed as a backbone to extract visual features at multiple levels. The second stage maps these features into high level representations with a fully randomized structure of recursive neural networks (RNNs) efficiently. To cope with the high dimensionality of CNN activations, a random weighted pooling scheme has been proposed by extending the idea of randomness in RNNs. Multi-modal fusion has been performed through a soft voting approach by computing weights based on individual recognition confidences (i.e. SVM scores) of RGB and depth streams separately. This produces consistent class label estimation in final RGB-D classification performance. Extensive experiments verify that fully randomized structure in RNN stage encodes CNN activations to discriminative solid features successfully. Comparative experimental results on the popular Washington RGB-D Object and SUN RGB-D Scene datasets show that the proposed approach achieves superior or on-par performance compared to state-of-the-art methods both in object and scene recognition tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks (CNNs) have attracted researchers to handle many visual recognition tasks since their breakthrough emergence. However, building an effective model can be quite challenging due to the lack of labeled training data, limited time and computational resources, and the need for well defined hyperparameter settings for a good generalization capability. Especially in many real-world tasks, it is not preferable to train a model from scratch. Luckily, CNNs offer highly efficient solutions with their transferable off-the-shelf features. Consequently, many approaches take advantage of these features to propose new solutions for object recognition (e.g. Sharif <ref type="bibr" target="#b60">Razavian et al. (2014)</ref>; <ref type="bibr" target="#b58">Schwarz et al. (2015)</ref>), scene recognition (e.g. <ref type="bibr" target="#b39">Liao et al. (2016)</ref>; <ref type="bibr" target="#b68">Song et al. (2019)</ref>), object detection (e.g. <ref type="bibr" target="#b24">Girshick et al. (2014)</ref>; <ref type="bibr" target="#b59">Sermanet et al. (2014)</ref>), and semantic segmentation (e.g. <ref type="bibr" target="#b24">Girshick et al. (2014)</ref>; <ref type="bibr" target="#b22">Farabet et al. (2013)</ref>) due to their high representation ability and capability of generalization among different tasks when trained with large scale datasets. The most common and straightforward strategy among these methods is to utilize the features obtained from final layers which provide semantically rich information with smaller dimensions comparing to the earlier layers <ref type="bibr" target="#b60">(Sharif Razavian et al., 2014;</ref><ref type="bibr" target="#b58">Schwarz et al., 2015;</ref><ref type="bibr" target="#b24">Girshick et al., 2014;</ref><ref type="bibr" target="#b59">Sermanet et al., 2014;</ref><ref type="bibr" target="#b22">Farabet et al., 2013)</ref>. How- <ref type="figure">Fig. 1</ref>: General overview of the proposed framework. The framework accepts RGB and depth images as inputs. In the CNN-Stage, activations at different levels of a pretrained model are extracted. In the RNN-Stage, first, CNN activations are adjusted through preprocessing operations so that they can be utilized by RNNs to output same number of feature vectors at each level. Then, multiple random RNNs are applied to map these inputs into high level representations. Finally, multiple level fusion and classification steps are deployed for recognition tasks. ever, one of the concerns about this semantics is the fact that as features evolve towards the final layers, they are increasingly dependent on the chosen dataset and task <ref type="bibr" target="#b80">(Yosinski et al., 2014)</ref>, which might diminish the generalization capabilities of these features when transferred. Moreover, this strategy ignores the locally activated distinctive information of the earlier layers which is less sensitive to semantics <ref type="bibr" target="#b27">(Hariharan et al., 2015;</ref><ref type="bibr" target="#b82">Zaki et al., 2016)</ref>. One of the main challenges in earlier layers of deep CNNs is the high dimensionality of extracted features. In addition, when these features are used as is, it makes the feature space untraceable. Eventually, while features are transformed from low-level general to high-level specific representations throughout the network, the relational information is distributed across the network at different levels <ref type="bibr" target="#b80">(Yosinski et al., 2014;</ref><ref type="bibr" target="#b27">Hariharan et al., 2015)</ref>. However, it remains unclear how to exploit the information effectively.</p><p>Motivation and Proposed Work: In this paper, we aim to present an effective deep feature extraction framework to derive powerful image representations through transfer learning. The proposed pipeline relies on two key insights. The first one is to employ a pretrained CNN as the backbone model and exploit activations at different layers of the network to cover the predominant information of the underlying localities. The second one is to implement multiple random recursive neural networks (RNNs) on top of CNNs to encode the CNN activations into a robust representation with reduced dimensionality and suf-ficient descriptiveness. Our motivation is twofold. We want our framework to generate highly discriminative deep feature representations without the need for training during the feature extraction stage, yet provide a training capability for extra performance boost. We question whether a fully random neural network stage lacks representation power.</p><p>In developing our framework, we particularly deal with the RGB-D object and scene recognition problems, which are challenging yet crucial tasks especially with the today's wider application of robotics technologies. Moreover, the multi-modality of the RGB-D sensors arises additional difficulties in representation of input data such as handling different modalities and devising solutions that captures complementary information from both RGB and depth data effectively. Besides these challenges, alleviating limitations on time and memory consumption is another challenge to deal with. To address these challenges, we propose a novel framework that gathers feature representations at different levels in a compact and representative feature vector for both of RGB and depth data. After obtaining CNN activations, we first apply a preprocessing operation to the activation maps of each level through reshaping or randomized pooling. This not only provides a generic structure for each level by fixing an RNN tree but also it allows us to improve recognition accuracy through multi-level fusion. We then give the outputs of these operations to multiple random RNNs <ref type="bibr" target="#b63">(Socher et al., 2012)</ref> to acquire higher level compact feature representations. Incor-porating multiple fixed RNNs together with the pretrained CNN models allows feature transition at different levels to preserve both semantic and spatial structure of objects. In order to transfer learning from a pretrained CNN model for depth modality, we embed depth data into the RGB domain with a highly efficient depth colorization technique based on surface normals. As for the multi-modal fusion of RGB and depth modalities, we explore different fusion techniques. Moreover, we present an approach that provides a decisive fusion of RGB and depth modalities based on the modality importance through a weighting scheme (see <ref type="bibr">Sec. 3.3)</ref>.</p><p>The proposed framework is evaluated with exhaustive experiments on two popular public datasets (i) Washington RGB-D Object dataset  for RGB-D object recognition task and (ii) Sun RGB-D Scene dataset <ref type="bibr" target="#b65">(Song et al., 2015)</ref> for RGB-D scene recognition task. The experimental results demonstrate the effectiveness of our approach in terms of accuracy by achieving superior performance over the current stateof-the-art methods. A preliminary version of this work appeared in <ref type="bibr" target="#b11">(Caglayan and Can, 2018)</ref> for RGB-D object recognition. In our preliminary research, we have already explored various properties of RNNs, such as non-linearity functionality, comparative accuracy performance and feature size over the use of CNN-only features. In this work, we present an extended and enhanced version of our work in <ref type="bibr" target="#b11">(Caglayan and Can, 2018)</ref> with a novel framework and make the following improvements. First, we improve the idea by designing and implementing the pipeline from scratch. Second, we have made the proposed work applicable to a variety of backbone models from shallow to deep. To this end, we introduce a random pooling strategy as a preprocessing step to deal with the high dimensionality of the activation maps of deep models such as ResNet and DenseNet, so that early layers of these models could be utilized in our random recursive neural networks. Third, we introduce a soft voting approach for multi-modal RGB-D fusion based on individual classification confidences of each modality. This provides better accuracy performance in recognition tasks. Fourth, although prior research has presented randomness in neural networks with various approaches such as feature extraction systems in <ref type="bibr" target="#b33">(Jarrett et al., 2009;</ref><ref type="bibr" target="#b63">Socher et al., 2012;</ref><ref type="bibr" target="#b16">Cheng et al., 2015b;</ref><ref type="bibr" target="#b10">Bui et al., 2016)</ref> and stochastic pooling <ref type="bibr" target="#b85">(Zeiler and Fergus, 2013)</ref>, in this work, we improve on these and elaborate randomness both in technical perspective and empirical investigation. Finally, we extend the proposed approach to RGB-D scene recognition task and achieve the state-of-the-art results in challenging benchmarks.</p><p>Contributions: To sum up, the main contributions of this paper can be listed as follows:</p><p>? We present a novel framework (see <ref type="figure">Fig. 1</ref>) for deep features with two-stage organization where information at different levels is encoded by incorporation of multiple random RNNs with a pretrained CNN model for RGB-D object and scene recognition (see Sec. 3). The framework is applicable to a variety of pretrained CNN models including AlexNet <ref type="bibr" target="#b35">(Krizhevsky et al., 2012)</ref>, VGGNet (Simonyan and Zisserman, 2015), ResNet <ref type="bibr" target="#b28">(He et al., 2016)</ref>, and DenseNet . The overall struc-ture has been designed in a modular and extendable way through a unified CNN and RNN process. Thus, it offers easy and flexible use. These also can easily be extended with new capabilities and combined with different setups and other models for implementing new ideas. In fact, our preliminary approach has been already successfully applied to another challenging robotics task in a SLAM system <ref type="bibr" target="#b25">(Guclu et al., 2019)</ref>.</p><p>? We extend the idea of randomness in RNNs as a novel pooling strategy to cope with the high dimensionality of CNN activations from different levels (see Sec. 3.2.1). This strategy has been applied as a preprocessing stage before RNNs and it allows us to evaluate and utilize multiple level information in deep models such as ResNet <ref type="bibr" target="#b28">(He et al., 2016)</ref> and DenseNet  models. In addition, we give the experimental results of different pooling strategies in terms of accuracy and show the effectiveness of our pooling strategy over other pooling methods (see <ref type="bibr">Sec. 4.4.3)</ref>.</p><p>? We study several aspects of transfer learning through an empirical investigation including level-wise analysis of different baselines and the effects of finetuning over fixed pretrained CNN models (see Sec. 4.4 and the supplementary material for further experimental analysis). In regard to multi-model fusion, unlike our previous work using concatenation of features, we propose a soft voting approach based on individual SVM confidences of RGB and depth streams (see Sec. 3.3) and show the strength of our approach experimentally (see <ref type="bibr">Sec. 4.4.5)</ref>. We also provide experimental results demonstrating that our approach improves the state-of-the-art results on two challenging realworld public datasets: Washington RGB-D Object dataset for RGB-D object recognition (see Sec. 4.2) and SUN RGB-D scene dataset for RGB-D scene recognition (see Sec. 4.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The proposed work can be related with different areas, such as multi-modal CNN based approaches, transfer learning based approaches, and random recursive neural networks. In this section, we narrow our focus to RGB-D based recognition and give a brief review of the relevant approaches with stating the current work in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multi-Modal CNN based Approaches</head><p>Following their success in computer vision, CNN-based solutions have replaced conventional methods such as the works in <ref type="bibr" target="#b8">Bo et al. (2011b)</ref>, <ref type="bibr" target="#b7">Bo et al. (2011a)</ref>, and <ref type="bibr" target="#b71">Tang et al. (2012)</ref> in the field of RGB-D object recognition, as in many other areas. For instance, the authors of  present CNN-based multi-modal learning systems motivated by the intuition of common patterns shared between RGB and depth modalities. They enforce their systems to correlate features of the two modalities in a multi-modal fusion layer with a pretrained model  and their custom network  respectively.  extend the idea of considering multi-modal intrinsic relationship with intra-class and inter-class similarities for indoor scene classification by providing a two-stage training approach. In <ref type="bibr" target="#b53">Rahman et al. (2017)</ref>, a three-streams multi-modal CNN architecture has been proposed in which depth images are represented with two different encoding methods in two-streams and the remaining stream is used for RGB images. Despite the extra burden, this naturally has increased the depth accuracy in particular. Similar multi-representational approach has been proposed by <ref type="bibr" target="#b89">Zia et al. (2017)</ref> where a hybrid 2D/3D CNN model initialized with pretrained 2D CNNs is employed together with 3D CNNs for depth images. <ref type="bibr" target="#b15">Cheng et al. (2015a)</ref> propose convolutional fisher kernel (CFK) method which integrates a single CNN layer with fisher kernel encoding and utilizes Gaussian mixture models for feature distribution. The drawback of their approach is the very high dimensional of the feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Transfer Learning based Approaches</head><p>Deep learning algorithms require a significant amount of annotated training data and obtaining such data can be difficult and expensive. Therefore, it is important to leverage transfer learning for enhancing high-performance learner on a target domain and the task at hand. Especially, applying a trained deep network and then fine-tuning the parameters can speed up the learning process or improve the classification performance <ref type="bibr" target="#b76">(Wang et al., 2017)</ref>. Furthermore, many works show that a pretrained CNN on a large-scale dataset can generate good generic representations that can effectively be used for other visual recognition tasks as well <ref type="bibr" target="#b60">(Sharif Razavian et al., 2014;</ref><ref type="bibr" target="#b80">Yosinski et al., 2014;</ref><ref type="bibr" target="#b45">Oquab et al., 2014;</ref><ref type="bibr">Azizpour et al., 2015b,a)</ref>. This is particularly important in vision tasks on RGB-D datasets, which is hard to collect with labeled data and generally amount of data is much less than that of the labeled images in RGB datasets.</p><p>There are many successful approaches that use transfer learning in the field of RGB-D object recognition. <ref type="bibr" target="#b58">Schwarz et al. (2015)</ref> use the activations of two fully connected layers, i.e. fc7 and fc8, extracted from the pretrained AlexNet <ref type="bibr" target="#b35">(Krizhevsky et al., 2012)</ref> for RGB-D object recognition and pose estimation. <ref type="bibr" target="#b26">Gupta et al. (2014)</ref> study the problem of object detection and segmentation on RGB-D data and present a depth encoding approach referred as HHA to utilize a pretrained CNN model on RGB datasets. Asif et al. introduce a cascaded architecture of random forests together with the use of the fc7 features of the pretrained models of <ref type="bibr" target="#b14">(Chatfield et al., 2014)</ref> and <ref type="bibr" target="#b62">(Simonyan and Zisserman, 2015)</ref> to encode the appearance and structural information of objects in their works of <ref type="bibr" target="#b0">Asif et al. (2015)</ref> and <ref type="bibr" target="#b1">Asif et al. (2017)</ref>, respectively. <ref type="bibr" target="#b13">Carlucci et al. (2018)</ref> propose a colorization network architecture and use a pretrained model as feature extractor after fine-tuning it. They also make use of the final fully-connected layer in their approach. So, these above-mentioned studies mainly focus on the outputs of the fully-connected layers.</p><p>On the other hand, many studies <ref type="bibr" target="#b41">(Liu et al., 2015;</ref><ref type="bibr" target="#b82">Zaki et al., 2016</ref><ref type="bibr" target="#b83">Zaki et al., , 2017</ref><ref type="bibr" target="#b67">Song et al., 2017b;</ref><ref type="bibr" target="#b11">Caglayan and Can, 2018)</ref> have concluded that using fully connected layers from pretrained or finetuned networks might not be the optimum approach to capture discriminating properties in visual recognition tasks. Moreover, combining the activations obtained in different levels of the same modal enhances recognition performance further, especially for multi-modal representations, where earlier layers capture modality-specific patterns <ref type="bibr" target="#b79">(Yang and Ramanan, 2015;</ref><ref type="bibr" target="#b67">Song et al., 2017b;</ref><ref type="bibr" target="#b11">Caglayan and Can, 2018)</ref>. Hence, utilizing information at different levels in the works of <ref type="bibr" target="#b79">(Yang and Ramanan, 2015;</ref><ref type="bibr" target="#b82">Zaki et al., 2016</ref><ref type="bibr" target="#b83">Zaki et al., , 2017</ref><ref type="bibr" target="#b67">Song et al., 2017b;</ref><ref type="bibr" target="#b11">Caglayan and Can, 2018;</ref><ref type="bibr" target="#b84">Zaki et al., 2019)</ref> yields better performances. More recent approach of <ref type="bibr" target="#b42">Loghmani et al. (2019)</ref> utilizes the pretrained model of residual networks <ref type="bibr" target="#b28">(He et al., 2016)</ref> to extract features from multiple layers and combines them through a recurrent neural network. Their experimental results also verify that multi-level feature fusion provides better performance than single-level features. While their approach is based on a gated recurrent unit (GRU) <ref type="bibr" target="#b17">(Cho et al., 2014)</ref> with a number of memory neurons, our approach employs multiple random neural networks with no necessarily need for training. A different related approach is proposed by <ref type="bibr" target="#b2">Asif et al. (2018)</ref>. They handle the classification task by dividing it into imagelevel and pixel-level branches and fusing through a Fisher encoding branch. <ref type="bibr" target="#b21">Eitel et al. (2015)</ref> and <ref type="bibr" target="#b70">Tang et al. (2019)</ref> employ two-stream CNNs, one for each modality of RGB and depth channels and each stream uses the pretrained model of <ref type="bibr" target="#b35">(Krizhevsky et al., 2012)</ref> on the ImageNet. In both works <ref type="bibr" target="#b21">(Eitel et al., 2015;</ref><ref type="bibr" target="#b70">Tang et al., 2019)</ref>, the two-streams are finally connected by a fully-connected fusion layer and a canonical correlation analysis (CCA) module, respectively. While feature fusion approaches (e.g. concatenation) may provide good accuracy for the visual recognition task, feature fusion may not be the only solution for multi-level decision process since increased feature space may not be good for recognition with small number of data. We experiment and show that voting on the SVM confidence scores for selected levels can also provide reliable and improved performance. Moreover, this also enables us to use confidence score based importance to RGB and depth domains in multi-modal fusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Random Recursive Neural Networks</head><p>Randomization in neural networks has been researched for a long time in various studies <ref type="bibr" target="#b57">(Schmidt et al., 1992;</ref><ref type="bibr" target="#b47">Pao and Takefuji, 1992;</ref><ref type="bibr" target="#b46">Pao et al., 1994;</ref><ref type="bibr" target="#b32">Igelnik and Pao, 1995;</ref><ref type="bibr" target="#b31">Huang et al., 2006;</ref><ref type="bibr" target="#b51">Rahimi and Recht, 2008;</ref><ref type="bibr" target="#b63">Socher et al., 2012)</ref> due to its benefits, such as simplicity and computationally cheapness over optimization <ref type="bibr" target="#b52">(Rahimi and Recht, 2009</ref>). Since a complete overview of these variations is beyond the scope of this paper, we give an overview specifically with the focus of random recursive neural networks <ref type="bibr" target="#b63">(Socher et al., 2012)</ref>. Recursive neural networks (RNNs) <ref type="bibr" target="#b49">(Pollack, 1990;</ref><ref type="bibr" target="#b29">Hinton, 1990;</ref><ref type="bibr" target="#b64">Socher et al., 2011)</ref> are graphs that process a given input into recursive tree structures to make a high-level reasoning possible in a part-whole hierarchy by repeating the same process over the trees. RNNs have been employed for various research purposes in computer vision including image super-resolution <ref type="bibr" target="#b34">(Kim et al., 2016)</ref>, semantic segmentation <ref type="bibr" target="#b64">(Socher et al., 2011;</ref><ref type="bibr" target="#b61">Sharma et al., 2014)</ref>, and RGB-D object recognition <ref type="bibr" target="#b63">(Socher et al., 2012;</ref><ref type="bibr" target="#b6">Bai et al., 2015;</ref><ref type="bibr" target="#b16">Cheng et al., 2015b)</ref>. <ref type="bibr" target="#b63">Socher et al. (2012)</ref> have introduced a two-stage RGB-D object recognition architecture where the first stage is a single CNN layer using a set of k-means centroids as the convolution filters and the second stage is multiple random recursive neural networks to process outputs of the first stage. <ref type="bibr" target="#b6">Bai et al. (2015)</ref> propose a subset based approach of the pioneer work in <ref type="bibr" target="#b63">(Socher et al., 2012)</ref> where they use a sparse auto-encoder instead of the kmeans clustering for convolution filters. <ref type="bibr" target="#b16">Cheng et al. (2015b)</ref> employ the same architecture of <ref type="bibr" target="#b63">Socher et al. (2012)</ref> for a semisupervised learning system with a modification by adding a spatial pyramid pooling to prevent a potential performance degradation during resizing input images. <ref type="bibr" target="#b10">Bui et al. (2016)</ref> have replaced the single CNN layer in <ref type="bibr" target="#b63">(Socher et al., 2012)</ref> with a pretrained CNN model for RGB object recognition and achieved impressive results. Following their success, in our preliminary work <ref type="bibr" target="#b11">(Caglayan and Can, 2018)</ref>, we propose an approach that aims to improve on this idea by gathering feature representations at different levels in a compact and representative feature vector for both of RGB and depth data. To this end, we reshape CNN activations in each layer that provides a generic structure for each layer by fixing the tree structure without hurting performance and it allows us to improve recognition accuracy by combining feature vectors at different levels. In this work, we propose a pooling strategy to handle large dimensional CNN activations by extending the idea of randomness in RNNs. This can be related with the stochastic pooling in <ref type="bibr" target="#b85">Zeiler and Fergus (2013)</ref>, which picks the normalized activations of a region according to a multinomial distribution by computing the probabilities within the region. Instead of using probabilities, our pooling approach here is a form of averaging based on uniform distributed random weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Approach</head><p>The proposed pipeline has two main stages. In the first stage, a pretrained CNN model has been employed as the underlying feature extractor. In this work, we have examined several models in this stage. The second stage transforms convolutional features through a randomized recursive neural network based structure that aims to acquire more compact representations. To cope with the high dimensionality of CNN activations, a pooling strategy based on random weights has been proposed. The final representative outcomes have been passed through a linear SVM classifier for categorization of objects and scenes. The overall pipeline can be related as a deeper analogy to <ref type="bibr" target="#b33">Jarrett et al. (2009)</ref> where a proper architecture with random weights for object recognition task has been explored.</p><p>In order to use pretrained CNN models, it is important to process input images appropriately. To this end, we perform a set of data preparation processes on both RGB and depth data and represent depth data into an effective 3-channel structure similar to RGB data using surface normal estimation (see the supplementary material for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">CNN-Stage</head><p>The backbone of our approach is a pretrained CNN model. Since size of available RGB-D datasets are much smaller than that of RGB's, it is important to make use of an efficient knowledge transfer from pretrained models on large RGB datasets. In addition, it saves time by eliminating the need for training from scratch. In the previous work <ref type="bibr" target="#b11">(Caglayan and Can, 2018)</ref>, the available pretrained CNN model of <ref type="bibr" target="#b14">(Chatfield et al., 2014)</ref>, named VGG f, in MatConvNet toolbox <ref type="bibr" target="#b72">(Vedaldi and Lenc, 2015)</ref> has been used. In this work, we employ several available pretrained models of the ImageNet including AlexNet <ref type="bibr" target="#b35">(Krizhevsky et al., 2012)</ref>, VGGNet <ref type="bibr" target="#b62">(Simonyan and Zisserman, 2015)</ref> (specifically VGGNet-16 model with batch normalization), ResNet <ref type="bibr" target="#b28">(He et al., 2016)</ref> (specifically ResNet-50 and ResNet-101 models), and DenseNet . We extract features from seven different levels of CNN models. For AlexNet, outputs of the five successive convolutional layers and the following two fully-connected (FC) layers have been considered, while for VGGNet, the first two FC layers are taken into account together with the outputs of each convolution block that includes several convolutions and a final max pooling operations. Unlike AlexNet and VGGNet, ResNet and DenseNet models consist of blocks such as residual, dense or transition blocks where there are multiple layers. While ResNet extends the sequential behaviour of AlexNet and VGGNet with the introduction of the skip-connections, DenseNet takes one step further by concatenating the incoming activations rather than summing up them. The ResNet models consist of five stages and a following average pooling and an FC layer. Therefore, each output of the five successive stages and the output of the final average pool have been considered for the six of the seven extraction points. As for the remaining extraction level for these models (ResNet-50 and ResNet-101), the middle point of the third block (which is the largest block) has been taken. Similarly, for DenseNet model, the output of all the four dense blocks (for the last dense block, the output of normalization that follows the dense block has been taken) and the transition blocks between them have been considered as the extraction points. Since common and straightforward model of AlexNet has a minimum depth with a seven layer stack-ups, the above-mentioned CNN extraction points for each model are selected to evaluate and compare level-wise model performances. In addition, these levels are also related to the CNN model in the previous work <ref type="bibr" target="#b11">(Caglayan and Can, 2018</ref>) that we improve on by considering their intrinsic reasoning behind the use of blocks and the approximate distance differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">RNN-Stage</head><p>Random recursive neural networks offer a feasible solution by randomly fixing the network connections and eliminate the need for selection in the parameter space. Motivated by this, we employ multiple random RNNs, whose inputs are the activation maps of a pretrained CNN model. RNNs map a given 3D matrix input into a vector of higher level representations of it by applying the same operations recursively in a tree structure. In each layer, adjacent blocks are merged into a parent vector with tied weights where the objective is to map inputs C ? R K?s?s into a lower dimensional space p ? R K through multiple levels in the end. Then, the output of a parent vector is passed through a nonlinear function. A typical choice for this purpose is the tanh function. In our previous work <ref type="bibr" target="#b11">(Caglayan and Can, 2018)</ref>, we give the comparative results of different activation functions in terms of accuracy success and show hyperbolic functions work well. Therefore, in this work, we employ tanh activation function as in <ref type="bibr" target="#b63">(Socher et al., 2012;</ref><ref type="bibr" target="#b11">Caglayan and Can, 2018</ref>). <ref type="figure" target="#fig_0">Fig. 2</ref> shows a graphical representation of a pooled CNN output with the size K ?8?8 and an RNN structure with 3 levels and blocks of 2 ? 2 = 4 child nodes (Note that this figure is inspired by the RNN graphical representation of <ref type="bibr" target="#b63">Socher et al. (2012)</ref>). In our case, inputs of RNNs are activation maps obtained from different levels of the underlying CNN model. Let x be an input image that pass through f (x) l a given CNN model, where l = 1, .., 7 are the extraction levels and f (x) l = C l , where the output convolution maps are either a 3D matrix C l ? R K?s?s for l convolutional layers or a 1D vector of C l ? R M for l FC layers/global average pooling. Since RNN requires a 3D input of C ? R K?s?s , we first process the convolution maps at each level to ensure the required form. Moreover, by applying this step, we ensure that RNNs are able to handle inputs fast and effectively by reducing the receptive field area and/or the number of activation maps of high-dimensional feature levels (e.g. the outputs of early levels for models such as VGGNet, ResNet, DenseNet, etc.). In addition, we apply preprocessing to obtain similar output structures with the previous work <ref type="bibr" target="#b11">Caglayan and Can (2018)</ref>. However, it was enough to apply only reshaping in the previous work due to less dimensional size of layers in VGG f model. In this work, we introduce random weighted pooling that copes with high dimensionality of layers in the underlying deeper models such as ResNet <ref type="bibr" target="#b28">(He et al., 2016)</ref> and DenseNet . Our pooling mechanism can downsample CNN activations in both number and spatial di-mension of maps. After applying the preprocessing step to obtain suitable forms for RNNs, we compute parent vector as</p><formula xml:id="formula_0">? ? ?8?8 (1) ? ? ?4?4 (2) ? ? ?2?2 ? ? ? ? ? 2 ? (1) = g( ) (2) = g( (1) ) = g(<label>(2)</label></formula><formula xml:id="formula_1">p = g (WC l ) (1) where C l = ? ? ? ? ? ? ? ? ? ? ? ? c 1 . . . c s 2 ? ? ? ? ? ? ? ? ? ? ? ?</formula><p>for each CNN extraction level l = 1, ..., 7, g is a nonlinearity function which is tanh in this study, s is block size of an RNN. Instead of a multi-level structured RNN, an RNN in this study is of one-level with a single parent vector. In fact, our experiments have shown that the single-level structure provides better or comparable results over the multi-level structure in terms of accuracy (see the supplementary material). Moreover, the single-level is more efficient with less computational burden. Thus, s block size is actually the receptive field size in an RNN. In Eq. 1, the parameter weight matrix is W ? R K?s 2 K and it is randomly generated from a predefined distribution that satisfies the following probability density function</p><formula xml:id="formula_2">W ? h ? b a h(w)dw = P(a ? W ? b)<label>(2)</label></formula><p>where h is a predefined distribution and a and b are boundaries of the distribution. In our case, the weights are set to be uniform random values in [?0.1, +0.1], which have been assigned by following our previous work <ref type="bibr" target="#b11">Caglayan and Can (2018)</ref> and specifically with the assumption of preventing possible explosion of tensor values due to our aggregating pooling strategy.</p><p>On the other hand, <ref type="bibr" target="#b56">Saxe et al. (2011)</ref> find that the distribution of random weights such as uniform, Laplacian, or Gaussian does not affect classification performances as long as the distribution is 0-centered. We refer readers to <ref type="bibr" target="#b51">Rahimi and Recht (2008)</ref> and <ref type="bibr" target="#b55">Rudi and Rosasco (2017)</ref> for more insights and further details on the properties of random features. Keeping in mind that in order to obtain sufficient descriptive power from the randomness, we need to generate enough samples from the range. In <ref type="bibr" target="#b63">Socher et al. (2012)</ref>, it has been demonstrated experimentally that increasing the number of random RNNs up to 64 improves performance and gives the best result with 128 RNNs. In <ref type="bibr" target="#b11">Caglayan and Can (2018)</ref>, it has also been verified that K = 128 number of RNN weights can be generated for feature encoding with high performance in classification on both of RGB and depth data. Therefore, as a standard usage in this work, we do feature encoding on CNN features using 128 random RNNs with 64 channel representations, leading us to 8192 dimensional feature vector at each level in a model. The reason why random weights work well for object recognition tasks seems to lie in the fact that particular convolutional pooling architectures can naturally produce frequency selective and translational invariant features <ref type="bibr" target="#b56">(Saxe et al., 2011)</ref>. As stated before, in analogy to the convolutional-pooling architecture in <ref type="bibr" target="#b33">Jarrett et al. (2009)</ref>, our approach intuitively incorporates both selectivity due to the CNN stage and translational invariance due to the RNN stage. Moreover, we have to point out that there is biological plausibility lies in the use of randomness as well. <ref type="bibr" target="#b54">Rigotti et al. (2010)</ref> have shown that random connections between inter-layer neurons are needed to implement mixed selectivity for optimal performance during complex cognitive tasks. Before concluding this section, we give details of our random pooling approach, where we extend the idea of random RNN as a downsampling mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Random Weighted Pooling</head><p>In our previous work <ref type="bibr" target="#b11">(Caglayan and Can, 2018)</ref>, we give CNN outputs to RNNs after a reshaping process. However, due to the high dimensional output size of the models used in this study, it is necessary to process CNN activations further. In this work, we propose a random pooling strategy to reduce the dimension in either size of the activation maps (s block size or receptive field area of an RNN) or number of maps (K) at CNN levels where reshaping is insufficient. In our random weighted pooling approach, we aggregate the CNN activation maps by sampling from a uniform distribution as in Eq. 2 from each pooling area. More precisely, for l extraction level, the pooling reduces C l activations by mapping into A l area as P : C l ? A l where C l ? R K?s?s and A l ? R K ?s ?s in Eq. 3.</p><formula xml:id="formula_3">A l = i?A l W (i) l C (i) l<label>(3)</label></formula><p>where A l is pooling area, C l convolutional activations, i is the index of each element within the pooling, and W l is random weights. K &lt; K and s = s when pooling is over number of maps whereas K = K and s &lt; s when pooling is over size of maps. <ref type="figure" target="#fig_1">Fig. 3</ref> illustrates proposed random weighted pooling for both of downsampling in number of maps and size of maps. In this work, by extending the randomness in RNNs along the pipeline with the proposed pooling strategy, we aim to show that randomness can actually work quite effectively. In fact, as we can see in the comparative results (see <ref type="bibr">Sec. 4.4</ref>.3), this randomness in our approach works generally better comparing to the other common pooling methods such as max pooling and average pooling, especially at the semantic levels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Fusion and Classification</head><p>After obtaining encoded features from the RNN-Stage, we investigate multi-level fusions to capture more distinctive information at different levels for further recognition performance. To minimize the cross entropy error between output predictions and the target values, we could give multi-level outputs to fully connected layers and back-propagate through them. However, following the success in our previous study <ref type="bibr" target="#b11">(Caglayan and Can, 2018)</ref>, we perform classification by employing linear SVM with the scikit-learn 1 <ref type="bibr" target="#b48">(Pedregosa et al., 2011)</ref> implementation. To this end, in our previous work <ref type="bibr" target="#b11">(Caglayan and Can, 2018)</ref>, we have performed the straightforward feature concatenation on various combinations of the best mid-level representations. In this work, in addition to the feature concatenation, we also apply soft voting by averaging SVM confidence scores on these best trio of levels. Finally, RGB and depth features are fused to evaluate combined RGB-D accuracy performance.</p><p>The motivation behind the need for a complementary multimodal fusion is twofold. The fact that shiny, transparent, or thin surfaces may cause corruption in depth information since depth sensors do not properly handle reflections from such surfaces, resulting better performance in favor of RGB in such cases. On the other hand, depth sensors work well in a certain range and are insensitive to changes in lighting conditions. Therefore, to take full advantage of both modalities in a complementary way, a compact multi-modal combination based on the success of input type is important in devising the best performing fusion. To this end, we present a decision mechanism using weighted soft voting based on the confidence scores obtained from RGB and depth streams. Modality weighting in this way is used to compensate imbalance and complement decision in different data modalities. Once the modality-specific branches proceed, we combine the predictions through the weighted SVM as follows. Let S i represents SVM confidence scores of each category class n = 0...N?1, where N is number of classes, and i ? {rgb, depth} indicates RGB and depth modalities. Then, weights w i are computed as in Eq. 4.</p><formula xml:id="formula_4">w i = e m i i e m i<label>(4)</label></formula><p>where m i is normalized squared magnitudes for each modality and defined as:</p><formula xml:id="formula_5">m i = S i 2 max( S rgb 2 , S depth 2 )<label>(5)</label></formula><p>Finally, multi-modal RGB-D predictions are estimated as follows, in Eq. 6:?</p><formula xml:id="formula_6">RGBD = arg max n i w i S i<label>(6)</label></formula><p>where n is a category class. Concretely, if RGB and depth results are balanced in confidence scores, then the final soft voting decision is based on equal contribution from each stream similar to averaging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>The proposed framework has been evaluated on two challenging benchmarks for two tasks: (i) RGB-D object recognition (Sec. 4.2) using Washington RGB-D object dataset  and (ii) RGB-D scene recognition (Sec. 4.3) using SUN RGB-D scene dataset <ref type="bibr" target="#b65">(Song et al., 2015)</ref>. After introducing the datasets and setups, we first compare our results with state-of-the-art results for both benchmarks. Results of other methods are taken from the original papers. Then, we carry out extensive experiments (Sec. 4.4) on the challenging Washington RGB-D object dataset, which is a larger-scale RGB-D dataset comparing to other RGB-D benchmarks, to evaluate effects of various model parameters and setup properties in our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset and Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Washington RGB-D Object Dataset</head><p>Washington RGB-D object dataset includes a total of 41, 877 images for each modality under 51 object categories and 300 category instances. Categories are commonly used household objects such as cups, camera, keyboards, vegetables, fruits, etc. Each instance of a category has images taken from 30 ? , 45 ? and 60 ? elevation angles. The dataset provides 10 train/test splits where in each split, one instance for each category is used for testing and the remaining instances are for training. Thus, for a single split run, a total of 51 category instances (roughly 7, 000 images) are used at testing and the remaining 249 instances (roughly 35, 000 images) are used at training phase. We evaluate the proposed work on the provided cropped images with the same setup in <ref type="bibr" target="#b36">Lai et al. (2011)</ref> for the 10 splits and average accuracy results are reported for the comparison to the related works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">SUN RGB-D Scene Dataset</head><p>SUN RGB-D scene dataset is the largest real-world RGB-D scene understanding benchmark to the date and contains RGB-D images of indoor scenes. Following the publicly available configuration of the dataset, we choose 19 scene categories with a total of 4, 845 images for training and 4, 659 images for testing. We use the same train/test split of <ref type="bibr" target="#b65">Song et al. (2015)</ref> to evaluate the proposed work for scene recognition. <ref type="table" target="#tab_0">Table 1</ref> shows average accuracy performance of our approach along with the state-of-the-art methods for object recognition on Washington RGB-D object benchmark. Our approach greatly improves the previous state-of-the-art results for both of RGB and depth modalities with a margin of 2.4% and 1.3%, respectively. As for the combined RGB-D results, our approach surpasses all the other methods except that of <ref type="bibr" target="#b42">Loghmani et al. (2019)</ref>, which is slightly better than ours (0.3%). As stated before (see Sec. 2), their approach is based on a gated recurrent unit with a set of memory neurons and is powered by a multimodal fusion learning schema. On the other hand, in this paper, we focus on a simple yet effective multi-modal feature extraction framework with a soft voting SVM classification. These results emphasize the importance of deep features in a unified framework based on the incorporation of CNNs and random RNNs. What is interesting here is that even a simple model like AlexNet can yield quite successful results. Concretely, our previous work <ref type="bibr" target="#b11">(Caglayan and Can, 2018)</ref> with AlexNet architecture, called VGG f in MatConvNet toolbox, gives impressive results as the models used in this work.   74.5 ? 3.1 64.7 ? 2.2 83.9 ? 3.5 KDES <ref type="bibr" target="#b9">(Bo et al., 2011c)</ref> 77.7 ? 1.9 78.8 ? 2.7 86.2 ? 2.1 CNN-RNN <ref type="bibr" target="#b63">(Socher et al., 2012)</ref> 80.8 ? 4.2 78.9 ? 3.8 86.8 ? 3.3 CaRFs <ref type="bibr" target="#b0">(Asif et al., 2015)</ref> --88.1 ? 2.4 MMDL  74.6 ? 2.9 75.5 ? 2.7 86.9 ? 2.6 Subset-RNN <ref type="bibr" target="#b6">(Bai et al., 2015)</ref> 82.8 ? 3.4 81.8 ? 2.6 88.5 ? 3.1 CNN Features <ref type="bibr" target="#b58">(Schwarz et al., 2015)</ref> 83.1 ? 2.0 -89.4 ? 1.3 CNN-SPM-RNN <ref type="bibr" target="#b16">(Cheng et al., 2015b)</ref> 85.2 ? 1.2 83.6 ? 2.3 90.7 ? 1.1 CFK <ref type="bibr" target="#b15">(Cheng et al., 2015a)</ref> 86.8 ? 2.7 85.8 ? 2.3 91.2 ? 1.4 Fus-CNN <ref type="bibr" target="#b21">(Eitel et al., 2015)</ref> 84.1 ? 2.7 83.8 ? 2.7 91.3 ? 1.4 AlexNet-RNN <ref type="bibr" target="#b10">(Bui et al., 2016)</ref> 89.7 ? 1.7 --Fusion 2D/3D CNNs <ref type="bibr" target="#b89">(Zia et al., 2017)</ref> 89.0 ? 2.1 78.4 ? 2.4 91.8 ? 0.9 STEM-CaRFs <ref type="bibr" target="#b1">(Asif et al., 2017)</ref> 88.8 ? 2.0 80.8 ? 2.1 92.2 ? 1.3 MM-LRF-ELM <ref type="bibr" target="#b40">(Liu et al., 2018)</ref> 84.3 ? 3.2 82.9 ? 2.5 89.6 ? 2.5 VGG f-RNN <ref type="bibr" target="#b11">(Caglayan and Can, 2018)</ref> 89.9 ? 1.6 84.0 ? 1.8 92.5 ? 1.2 DECO <ref type="bibr" target="#b13">(Carlucci et al., 2018)</ref> 89.5 ? 1.6 84.0 ? 2.3 93.6 ? 0.9 MDSI-CNN <ref type="bibr" target="#b2">(Asif et al., 2018)</ref> 89.9 ? 1.8 84.9 ? 1.7 92.8 ? 1.2 HP-CNN <ref type="bibr" target="#b84">(Zaki et al., 2019)</ref> 87.6 ? 2.2 85.0 ? 2.1 91.1 ? 1.4 RCFusion <ref type="bibr" target="#b42">(Loghmani et al., 2019)</ref> 89.6 ? 2.2 85.9 ? 2.7 94.4 ? 1.4 MMFLAN <ref type="bibr" target="#b50">(Qiao et al., 2021)</ref> 83.9 ? 2.2 84.0 ? 2.6 93. We also present average accuracy performance of individual object categories on the 10 evaluation splits of Washinton RGB-D Object dataset using the best-performing structure, ResNet101-RNN. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, our approach is highly accurate in recognition of the most of the object categories. Categories with lower accuray results are mushroom, peach, and pitcher. The common reason that leads to the lower performance in these categories seems to be due to their less number of instances. In particular, these categories have only 3 instances, which is the minimum number for any category in the dataset. Considering the other categories with up to 14 instances, this imbalance of the data may have biased the learning to favor of categories with more examples. Moreover, the accuracy of our combined RGB and depth based on weighted confidences of modalities reflects that the fusion of RGB and depth data in this way can provide strong discrimination capability for object categories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object Recognition Performance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Scene Recognition Performance</head><p>To test the generalization ability of our approach, we also carry out comparative analysis of our best-performing model, namely ResNet101-RNN, on SUN RGB-D Scene <ref type="bibr" target="#b65">(Song et al., 2015)</ref> dataset for scene recognition as a more challenging task of scene understanding. To this end, we first apply ResNet101 pretrained model without finetuning, namely Fix ResNet101-RNN, for both of RGB and depth modalities. Then, we finetune the pretrained CNN model on SUN RGB-D Scene dataset using the same hyper-parameters of object recognition task (see <ref type="bibr">Sec. 4.4.4)</ref>. The results of these experiments together with the-state-of-the-art results on this dataset are reported in <ref type="table" target="#tab_2">Table  2</ref>. Our best system greatly improves the-state-of-the-art methods not only for RGB-D final result but also for individual data modalities. It is worth mentioning that we use the pretrained CNN model on object-centric dataset of ImageNet <ref type="bibr" target="#b18">(Deng et al., 2009)</ref>, which is less commonly used for scene recognition task than the pretrained models on scene-centric datasets such as Places <ref type="bibr" target="#b87">(Zhou et al., 2014)</ref>. Nevertheless, our approach produces superior results compared to existing state-of-the-art methods for RGB-D scene recognition task. Moreoever, it is interesting that our system even with fixed pretrained CNN model is already discriminative enough and achieves impressive accuracy performances. Contrary to our findings on Washington RGB-D Object dataset, finetuning provides much better results not only for depth domain but also for the RGB domain as well. This is what we expect as scene recognition is a cross-domain task for our approach that has the pretrained CNN model of the object-centric ImageNet as the backbone. Specifically, finetuning on depth data boosts the accuracy greatly by providing both domain and modality adaptation. <ref type="figure" target="#fig_3">Fig. 5</ref> shows the confusion matrix of our approach with finetuning over the 19 categories of SUN RGB-D Scene dataset for RGB-D. The matrix demonstrates the degree of confusion between pairs of scene categories and implies the similarity between scenes on this dataset. The largest misclassification errors happen to be between extremely similar scene categories such as computer roomoffice, conference room-classroom, discussion area-rest space, lecture theatre-classroom, study space-classroom, lab-office, etc. In addition to the inter-class similarity, other reasons for poor performance might be intraclass variations of the scenes and lack of getting enough rep-  <ref type="bibr" target="#b87">(Zhou et al., 2014)</ref> 38.1 27.7 39.0 SS-CNN-R6 <ref type="bibr" target="#b39">(Liao et al., 2016)</ref> 36.1 -41.3 DMFF <ref type="bibr" target="#b88">(Zhu et al., 2016)</ref> 37.0 -41.5 Places CNN-RCNN  40.4 36.3 48.1 MSMM <ref type="bibr" target="#b67">(Song et al., 2017b)</ref> 41.5 40.1 52.3 RGB-D-CNN <ref type="bibr" target="#b66">(Song et al., 2017a)</ref> 42.7 42.4 52.4 MDSI-CNN <ref type="bibr" target="#b2">(Asif et al., 2018)</ref> 39.6 35.2 45.2 DF 2 Net  --54.6 HP-CNN-T <ref type="bibr" target="#b84">(Zaki et al., 2019)</ref> 38.8 28.5 42.2 LM-CNN <ref type="bibr" target="#b12">(Cai and Shao, 2019)</ref> 44.3 34.6 48.7 RGB-D-OB <ref type="bibr" target="#b68">(Song et al., 2019)</ref> -42.4 53.8 Cross-Modal Graph <ref type="bibr" target="#b81">(Yuan et al., 2019)</ref> 45.7 -55.1 RAGC (Mosella-Montoro and Ruiz-Hidalgo, 2019) --42.1 MAPNet <ref type="bibr" target="#b38">(Li et al., 2019)</ref> --56.2 TRecgNet Aug <ref type="bibr" target="#b20">(Du et al., 2019)</ref> 50.6 47.9 56.7 G-L-SOOR <ref type="bibr" target="#b69">(Song et al., 2020)</ref> 50.5 44.1 55.5 MSN <ref type="bibr" target="#b77">(Xiong et al., 2020)</ref> --56.2 CBCL <ref type="bibr" target="#b3">(Ayub and Wagner, 2020)</ref> 48.8 37.3 59.5 ASK <ref type="figure" target="#fig_0">(Xiong et al., 2021</ref> resentative knowledge transfer from the ImageNet models. To further analyse the performance of our system, we give top-3 and top-5 classification accuracy together with top-1 results as in <ref type="table" target="#tab_4">Table 3</ref>. While the top-1 accuracy shows the percentage of test images that exactly matches with the predicted classes, the top-3 and top-5 indicates the percentage of test images that are among the top ranked 3 and 5 predictions, respectively. The top-3 and top-5 results demonstrate the effectiveness of our system more closely by overcoming ambiguity among scene cat- egories greatly. <ref type="figure">Fig. 6</ref> depicts some test examples of scene categories confused with each other frequently on SUN RGB-D Scene dataset. As shown in the figure, these scene categories have similar appearances that make them hard to distinguish even for a human expert without sufficient context knowledge in the evaluation. Nevertheless, our approach is able to identify scene category labels among the top-3 and top-5 predictions with high accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Model Ablation</head><p>We have analyzed and validated the proposed framework with extensive experiments using a variety of architectural configurations on the popular benchmark of Washington RGB-D dataset, which is more than 4 times larger than the SUN RGB-D scene dataset. In this section, the analysis and evaluations of the model ablative investigations are presented. Further experiments and analysis are given in the supplementary material.</p><p>The developmental experiments are carried out on two splits of Washington RGB-D Object dataset for both modalities in order to evaluate on more stable results. The average results are analyzed. However, in some experiments, more runs have been carried out, which are clearly stated in the related sections. In Sec. 4.2 and Sec. 4.3, the best performing models are compared with the state-of-the-art methods with the exact provided evaluation setups. We assess the proposed framework on a PC with AMD Ryzen 9 3900X 12-Core Processor, 3.8 GHz Base, 128 GB DDR4 RAM 2666 MHz, and NVIDIA GeForce GTX 1080 Ti graphics card with 11 GB memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1.">Empirical Evaluation of the Effect of Randomness</head><p>The use of random weights both in pooling and RNN structures leads to the question of how stable are the results. Thus, we experimentally investigate to see whether there is a decisive difference between different runs that generate and use new random weights. We run the pipeline with different random weights on two splits, 5 times for each. <ref type="figure" target="#fig_5">Fig. 7</ref> reports average results with their standard deviations for each level. The figure clearly shows that randomness does not cause any instability in the model and produces similar results with very small deviations. <ref type="figure">Fig. 8</ref> shows level-wise average accuracy performances of all the baseline models for both of RGB and depth modalities on all the 10 evaluation splits. The graphs show a similar performance trend line with an upward at the beginning and a downward at the end. Although the levels at which optimum performance is obtained vary according to the model, what is common to all models in general is that instead of final level representations, intermediate level representations present the optimal results. These experiments also verify that while deep models transform attributes from general to specific through the network eventually <ref type="bibr" target="#b60">(Sharif Razavian et al., 2014;</ref><ref type="bibr" target="#b86">Zeiler and Fergus, 2014)</ref>, intermediate layers present the optimal representations. This makes sense because while early layers response to low-level raw features such as corners and edges, late layers extract more object-specific features of the trained datasets. This is more clear on the depth plot in <ref type="figure">Fig. 8</ref>, where the dataset difference is obvious due to the domain difference. We should state that RNN encoding on features extracted from FC layers with less than 8192 dimension might not be efficient since they are already compact enough. Therefore, encoding outputs of these layers to a larger feature space through RNNs might lead to redundancy in representations. This might be another reason why there is a drop in accuracy of these layers (e.g. see L7 in <ref type="figure">Fig. 8</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2.">Level-wise Performance of Different Models</head><p>In addition, depth plot contains more fluctuations and irregularities comparing to the RGB plot, since the pretrained models of the RGB ImageNet are used as fixed extractors without finetuning. As for the different baseline model comparison, ResNet-101 and DenseNet-121 models perform similarly in terms of accuracy and are better than others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3.">Comparative Results of Random Weighted Pooling</head><p>In our approach, we extend the idea of randomness into a pooling strategy to cope with the high dimensionality of CNN activations, which could not be only applied to map/window size but also can be used to reduce the number of maps. We particularly employ random pooling to confirm that randomness works greatly in overall RNN-Stage even in such a pooling strategy together with random RNNs. To this end, we investigate the comparative accuracy performances of random pooling together with average pooling and max pooling. We use the DenseNet-121 model, where pooling is used extensively on each level (except in level 4), and we conduct experiments using the same RNN weights for fair comparison. <ref type="figure">Fig. 10</ref> shows average accuracy results of two splits for each pooling on both RGB and depth data. As seen from the figure, random weighted pooling generally performs similar to average pooling, while its performance in average is better than max pooling. Moreover, it is seen that random pooling acquires better results especially in middle/late levels(L4-L7), which presents more stable and semantically meaningful representations comparing to the early levels. The results also show that the proposed random pooling and average pooling can be used interchangeably as their performances are similar.</p><p>We further investigate the comparative accuracy performance of the proposed random pooling in our final ResNet-101 based pipeline. As it can be seen in <ref type="table" target="#tab_5">Table 4</ref>, when the proposed pipeline is set to use random weighted pooling, it produces better or similar accuracy than max pooling and average poolingbased pipelines. This validates the power of randomness in a pooling strategy and the use of random pooling as an alternative way for down-sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4.">Contribution of Finetuning</head><p>We have not used any training or finetuning in our approach to feature extraction in the ablative experiments so far (except <ref type="table" target="#tab_5">Table 4</ref>, where depth modality-based ResNet101 is finetuned). Although impressive results are obtained on RGB data, the same success is not achieved on depth data.  The reason for this difference is that the baseline CNN models are pretrained models on RGB dataset of the ImageNet. Therefore, as the next step, we analyze the changes in accuracy performance of RGB and depth data modalities by finetuning the baseline CNN models in our approach. To this end, we first carry out a systematic inquiry to find optimal finetuning hyper-parameters on a predefined set of values using only one split of Washington RGB-D dataset as a validation set for AlexNet and DenseNet-121 models. Then, finetuning of the models are performed by stochastic gradient descent (SGD) with momentum. The hyper-parameters of momentum, learning rate, batch size, learning rate decay factor and decay step size, and number of epochs, respectively are used as following; (0.9, 0.001, 32, 0.01, 10, 40) and (0.9, 0.0001, 8, 0.1, 10, 40) are used for AlexNet on RGB and depth data, respectively, whereas (0.95, 0.0001, 16, 0.1, 10, 40) and (0.95, 0.001, 8, 0.1, 10, 40) are used for DenseNet-121. Apart from these two models, we also perform finetuning on the ResNet-101 model. We use the same finetuning hyperparameters of DenseNet-121 for ResNet-101, since they are in a similar architectural structure. <ref type="figure">Fig. 9</ref> shows average accuracy performance of finetuned CNN models together with fixed models on all the 10 evaluation splits of Washington RGB-D object dataset. The plot shows a clear upward in performance on depth data as expected. However, there is a loss of accuracy in general, when finetuning is performed on RGB data. Washington RGB-D object dataset contains a subset of the categories in ImageNet. Accordingly, pretrained models of Im-ageNet already satisfy highly correlated distribution on RGB data. Therefore, there is no need for finetuning on RGB data. In contrast, in order to ensure coherence and relevance, finetuning is required for depth data due to domain difference of the inputs with the pretrained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.5.">Weighted Voting based RGB-D Fusion Performance</head><p>Finally, we provide RGB-D combined results for AlexNet, DenseNet-121, and ResNet-101 models as shown in <ref type="table" target="#tab_6">Table 5</ref> based on the SVM confidences. The table reports average results for fusion of the best levels of RGB and depth, and the best trio levels (see the supplementary material). We evaluate two types of soft voting, our proposed weighted vote and average vote. The proposed weighted vote increases accuracy comparing to the average vote for all the models both on the multi-modal fusion of the best single and best trio levels of RGB and depth streams. The results also confirm the strength of our multi-modal voting approach that combines RGB and depth modalities effectively. On the other hand, the reason why RGB-D fusion improves the individual RGB and depth results lies in the fact that these different data modalities support each other towards a more accurate representation by capturing different aspects of the data with a strong complementary approach. RGB data are rich in terms of texture and color information. Depth data have additional geometric information to depict object shapes. Moreover, depth sensors are more insensitive to changes in lighting conditions. Therefore, multi-modal f <ref type="figure">Fig. 9</ref>: Level-wise average accuracy performance of finetuned CNN models together with fixed models on all the 10-splits of Washington RGB-D dataset. data combination is useful not only for its integrative characteristic, but also for its complementarity when one modality data is lacking such as RGB data in dark environment or depth data on shiny surfaces. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Discussion</head><p>Our framework presents an effective solution for deep feature extraction in an efficient way by integrating a pretrained CNN model with random weights based RNNs. Randomization throughout our RNN-Stage raises the question of whether the results are stable enough. The carefully implemented experiments in Sec. 4.4.1 are an empirical justification for the stability of random weights. On the other hand, our multi-level analysis shows that the optimum performance gain from a single level always comes from an intermediate level for all the models with/without finetuning for both of RGB and depth modalities. The only exception is in the use of finetuned DenseNet-121 model on depth data. This is an interesting finding, because one expects better representation capabilities of final layers, especially in the use of finetuned models. Yet, as expected, performance generally increases from the first level to the last level throughout the networks when the underlying CNN models are finetuned. Since Washington RGB-D Object  dataset includes a subset of object categories in the ImageNet <ref type="bibr" target="#b18">(Deng et al., 2009</ref>), finetuning does not improve accuracy success on RGB data. In contrast, accuracy gain is significant due to the need for domain adaptation in depth data. This also shows that using an appropriate technique to handle depth data as in our approach (see the supplementary material), leads impressive performance improvement by knowledge transfer between modalities.</p><p>In this study, although we have explored different techniques to fuse representations of multiple levels to further increase the classification success, a single optimum level may actually be sufficient enough for many tasks. In this way, especially for tasks where computational time is more critical, results can be obtained much faster without sacrificing accuracy success. Another point of interest is that the data imbalance in Washington RGB-D Object dataset results in poor performance for the individual categories with less instances and consequently leads to a drop in the overall success of the system. Hence, this imbalance might be overcome by applying data augmentation on the categories with less instances.</p><p>The success of our approach for RGB-D scene recognition confirms the generalization ability of the proposed framework. Unlike object recognition, when the underlying CNN models are finetuned, success in both RGB and depth modalities increases significantly in scene recognition task. This is due to the need for cross-domain task adaptation of object-centric based pretrained models. Therefore, similar findings in object recognition could be observed if scene-centric based pretrained models are employed for scene recognition (e. g. Places <ref type="bibr" target="#b87">(Zhou et al., 2014)</ref>). Moreover, such pretrained models could improve the results further within our framework. Another potential that could improve the success for scene recognition is embedding contextual knowledge by jointly employing attention mechanism such as <ref type="bibr" target="#b23">Fukui et al. (2019)</ref> in our structure.</p><p>This work has been implemented as the extension of our previous work <ref type="bibr" target="#b11">(Caglayan and Can, 2018)</ref>. Therefore, we have not explored further multimodal architectures. However, instead of SVM, combining level-wise outputs through a multilayer perceptron (MLP) might be more convenient for RGB-D multimodal design. In particular, it would be interesting to use the soft voting approach proposed in this study with MLP. In the future, we plan to investigate such an approach for a better RGB-D multimodal tasks, such that success is focused on the ultimate RGB-D fusion rather than the individual accuracy success of the RGB and depth modalities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we have presented a framework that incorporates pretrained CNN models together with multiple random recursive neural networks. The proposed approach greatly improves RGB-D object and scene recognition performances over the-state-of-the-art results in the literature on the widely used Washington RGB-D Object and SUN RGB-D Scene datasets. The proposed randomized pooling schema allows us to deal with high-dimensional activations of CNN models effectively. The extensive experimental analysis of various parameters and setup properties show that the incorporation of multiple random RNNs with a pretrained CNN model provides a robust and effective general solution for both of RGB-D object and scene recognition tasks. Utilizing depth data by mapping it into RGB-like image domain allows knowledge transfer from RGB pretrained CNN models effectively. The generic design and the generalization capability of the proposed framework allow to utilize it for other visual recognition tasks. Thus, we have opened our code along with models to the community in order to help future studies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>)Fig. 2 :</head><label>2</label><figDesc>Graphical representation of a single recursive neural network (RNN). The same random weights have been applied to compute each node and level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 :</head><label>3</label><figDesc>, ? ? ? ? ? pooling result where ? ? ?3 ? ? tensor product where , ? ? ? ? ? pooling result where ? ? ? ? 2 ? 2 Illustration of random weighted pooling over number of maps (top) and window size of maps (below).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 :</head><label>4</label><figDesc>Per-category average accuracy performances of ResNet101-RNN on Washington RGB-D Object dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>RGB-D confusion matrix of ResNet101-RNN on SUN RGB-D Scene dataset (best viewed with magnification).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Top-5 RGB-D predictions of our system using sample test images of frequently confused scene categories on SUN RGB-D Scene dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 :</head><label>7</label><figDesc>Effect of randomness on the accuracy results for each level (L1 to L7). Values indicate standard deviations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 :Fig. 10 :</head><label>810</label><figDesc>Level-wise average accuracy performance of different baseline models on all the 10-splits of Washington RGB-D dataset. p Average accuracy performance of different pooling methods on RGB and depth data for the baseline model of DenseNet-121 on two splits of Washington RGB-D dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Average accuracy comparison of our approach with the related methods on Washington RGB-D Object dataset (%). Red: Best result, Blue: Second best result, Green: Third best result.</figDesc><table><row><cell>Method</cell><cell>RGB</cell><cell>Depth</cell><cell>RGB-D</cell></row><row><cell>Kernel SVM</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Accuracy comparison of our approach with the related methods on</figDesc><table><row><cell cols="4">SUN RGB-D Scene dataset (%). Red: Best result, Blue: Second best result,</cell></row><row><cell>Green: Third best result.</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Method</cell><cell>RGB</cell><cell>Depth</cell><cell>RGB-D</cell></row><row><cell>Places CNN-Lin SVM (Zhou et al., 2014)</cell><cell>35.6</cell><cell>25.5</cell><cell>37.2</cell></row><row><cell>Places CNN-RBF SVM</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3</head><label>3</label><figDesc></figDesc><table><row><cell cols="4">: Scene recognition accuracy of top-1, top-3, and top-5 on SUN RGB-D</cell></row><row><cell>Scene dataset (%).</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Accuracy</cell><cell>RGB</cell><cell>Depth</cell><cell>RGB-D</cell></row><row><cell>top-1</cell><cell>58.5</cell><cell>50.1</cell><cell>60.7</cell></row><row><cell>top-3</cell><cell>81.0</cell><cell>71.5</cell><cell>83.5</cell></row><row><cell>top-5</cell><cell>88.5</cell><cell>80.9</cell><cell>89.9</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Average accuracy performance of different pooling methods in the best performing ResNet101-RNN pipeline on Washington RGB-D dataset (%).</figDesc><table><row><cell>Accuracy</cell><cell>RGB</cell><cell>Depth</cell><cell>RGB-D</cell></row><row><cell>Max</cell><cell>91.1 ? 1.4</cell><cell>87.1 ? 2.5</cell><cell>93.8 ? 0.9</cell></row><row><cell>Average</cell><cell>91.6 ? 1.6</cell><cell>87.2 ? 2.5</cell><cell>94.0 ? 1.0</cell></row><row><cell>Random</cell><cell>92.3 ? 1.0</cell><cell>87.2 ? 2.5</cell><cell>94.1 ? 1.0</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Average accuracy performance of RGB-D (RGB + Depth) with different fusion combinations on Washington RGB-D dataset (%).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>AlexNet</cell><cell>DenseNet-121</cell><cell>ResNet-101</cell></row><row><cell>Avg Vote</cell><cell>RGBLB1</cell><cell>+ Depth LB1</cell><cell>90.2 ? 1.3</cell><cell>92.9 ? 1.4</cell><cell>92.7 ? 1.6</cell></row><row><cell>Weighted Vote</cell><cell>RGBLB1</cell><cell>+ Depth LB1</cell><cell>90.2 ? 1.2</cell><cell>93.5 ? 1.0</cell><cell>93.8 ? 1.1</cell></row><row><cell>Avg Vote</cell><cell cols="2">RGBLB1+LB2+LB3 + Depth LB1+LB2+LB3</cell><cell>90.6 ? 1.6</cell><cell>92.6 ? 1.4</cell><cell>93.0 ? 1.3</cell></row><row><cell>Weighted Vote</cell><cell cols="2">RGBLB1+LB2+LB3 + Depth LB1+LB2+LB3</cell><cell>90.9 ? 1.3</cell><cell>93.5 ? 1.0</cell><cell>94.1 ? 1.0</cell></row><row><cell cols="6">LB1: Best performing level LB2: Second best performing level LB3: Third best performing level</cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/scikit-learn/scikit-learn</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This paper is based on the results obtained from a project commissioned by the New Energy and Industrial Technology Development Organization (NEDO).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient rgb-d object categorization using cascaded ensembles of randomized decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1295" to="1302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rgb-d object recognition and grasp detection using hierarchical cascaded forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Sohel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="547" to="564" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A multi-modal, discriminative and spatially invariant cnn for rgb-d object labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Asif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Sohel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2051" to="2065" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Centroid based concept learning for rgb-d indoor scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ayub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Factors of transferability for a generic convnet representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="1790" to="1802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From generic to specific deep representations for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="36" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Subset based deep learning for rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="280" to="292" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Object recognition with hierarchical kernel descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1729" to="1736" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Depth kernel descriptors for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="821" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Depth kernel descriptors for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="821" to="826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Object recognition using deep convolutional features transformed by a recursive network structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lech</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Neville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Burnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="10059" to="10066" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Exploiting multi-layer features using a cnnrnn approach for rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Can</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The European Conference on Computer Vision (ECCV) Workshops</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Rgb-d scene classification via multi-modal feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="825" to="840" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">(de) 2 co: Deep depth colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2386" to="2393" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Convolutional fisher kernels for rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV), 2015 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="135" to="143" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Semi-supervised learning and feature evaluation for rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="149" to="160" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Van Merri?nboer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1724" to="1734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database, in: Computer Vision and Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Cross-modal pyramid translation for rgb-d scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Translate-to-recognize networks for rgb-d scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multimodal deep learning for robust rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Eitel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Spinello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2015 IEEE/RSJ International Conference on, IEEE</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="681" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Attention branch network: Learning of attention mechanism for visual explanation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirakawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fujiyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Rgb-d indoor mapping using deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Guclu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Caglayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Can</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Learning rich features from rgb-d images for object detection and segmentation, in: European Conference on Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="345" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel?ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="447" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mapping part-whole hierarchies into connectionist networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="47" to="75" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Universal approximation using incremental constructive feedforward networks with random hidden nodes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Siew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="879" to="892" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stochastic choice of basis functions in adaptive function approximation and the functional-link net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Igelnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Pao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1320" to="1329" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">What is the best multi-stage architecture for object recognition?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jarrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 12th international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2146" to="2153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A large-scale hierarchical multi-view rgb-d object dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1817" to="1824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Df2net: Discriminative feature learning and fusion network for rgb-d indoor scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Mapnet: Multi-modal attentive pooling network for rgb-d indoor scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="436" to="449" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Understand scene categories by objects: A semantic regularized scene classifier using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kodagoda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE international conference on robotics and automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2318" to="2325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Multi-modal local receptive field extreme learning machine for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">277</biblScope>
			<biblScope unit="page" from="4" to="11" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The treasure beneath convolutional layers: Cross-convolutional-layer pooling for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4749" to="4757" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Recurrent convolutional fusion for rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Loghmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Planamente</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Vincze</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="2878" to="2885" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Residual attention graph convolutional network for geometric 3d scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mosella-Montoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruiz-Hidalgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">2021. 2d-3d geometric fusion network using multi-neighbourhood graph convolution for rgb-d indoor scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mosella-Montoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ruiz-Hidalgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="46" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1717" to="1724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning and generalization characteristics of the random vector functional-link net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Sobajic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="163" to="180" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Functional-link net computing: theory, system architecture, and functionalities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Pao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Takefuji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="76" to="79" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Scikitlearn: Machine learning in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Recursive distributed representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Pollack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="77" to="105" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Private and common feature learning with adversarial network for rgbd object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">423</biblScope>
			<biblScope unit="page" from="190" to="199" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Weighted sums of random kitchen sinks: Replacing minimization with randomization in learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1313" to="1320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rgb-d object recognition with multimodal deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Rahman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="991" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Internal representation of task rules by recurrent dynamics: the importance of the diversity of neural responses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rigotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Ben Dayan Rubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fusi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in computational neuroscience</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Generalization properties of learning with random features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rudi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3215" to="3225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">On random weights and unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Saxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bhand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1089" to="1096" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Feed forward neural networks with random weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">F</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Kraaijveld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Duin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th IAPR International Conference on Pattern Recognition</title>
		<meeting>the 11th IAPR International Conference on Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1992" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Rgb-d object recognition and pose estimation based on pre-trained convolutional neural network features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schwarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Behnke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1329" to="1335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ICLR</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition workshops</title>
		<meeting>the IEEE conference on computer vision and pattern recognition workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Recursive context propagation network for semantic scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2447" to="2455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Convolutionalrecursive deep learning for 3d object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="656" to="664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Parsing natural scenes and natural language with recursive neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Sun rgb-d: A rgb-d scene understanding benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Lichtenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Depth cnns for rgb-d scene recognition: Learning from scratch better than transferring from rgb-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-First AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Combining models from multiple sources for rgb-d scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Herranz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4523" to="4529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Learning effective rgb-d representations for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Herranz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="980" to="993" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Image representations with spatial object-to-object relations for rgb-d scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="525" to="537" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Canonical correlation analysis regularization: An effective deep multiview learning baseline for rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="107" to="118" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Histogram of oriented normal vectors for object recognition with a depth sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Skubic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="525" to="538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Matconvnet: Convolutional neural networks for matlab</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia, ACM</title>
		<meeting>the 23rd ACM international conference on Multimedia, ACM</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="689" to="692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Mmss: Multi-modal sharable and specific feature learning for rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Modality and component aware feature fusion for rgb-d scene classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Cham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Large-margin multimodal deep learning for rgb-d object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1887" to="1898" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Knowledge guided disambiguation for large-scale scene classification with multi-resolution cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="2055" to="2068" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Msn: Modality separation networks for rgb-d scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">373</biblScope>
			<biblScope unit="page" from="81" to="89" />
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Ask: Adaptively selecting key local features for rgb-d scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="2722" to="2733" />
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Multi-scale recognition with dag-cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1215" to="1223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Acm: Adaptive cross-modal graph convolutional neural networks for rgb-d scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="9176" to="9184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Convolutional hypercube pyramid for accurate rgb-d object category and instance recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Learning a deeply supervised multimodal rgb-d embedding for semantic scene and object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="page" from="41" to="52" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Viewpoint invariant semantic object and scene categorization with rgb-d sensors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Zaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Robots</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="1005" to="1022" />
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Stochastic pooling for regularization of deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Discriminative multi-modal feature fusion for rgbd indoor scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Weibel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Rgb-d object recognition using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yuksel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yuret</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yemez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision Workshop</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="887" to="894" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
