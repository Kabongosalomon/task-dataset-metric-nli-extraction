<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Labeling Trick: A Theory of Using Graph Neural Networks for Multi-Node Representation Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute for Artificial Intelligence</orgName>
								<orgName type="institution">Peking University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Beijing Institute for General Artificial Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Purdue University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jin</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Facebook AI</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Labeling Trick: A Theory of Using Graph Neural Networks for Multi-Node Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T15:47+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we provide a theory of using graph neural networks (GNNs) for multinode representation learning (where we are interested in learning a representation for a set of more than one node, such as link). We know that GNN is designed to learn single-node representations. When we want to learn a node set representation involving multiple nodes, a common practice in previous works is to directly aggregate the single-node representations obtained by a GNN into a joint node set representation. In this paper, we show a fundamental constraint of such an approach, namely the inability to capture the dependence between nodes in the node set, and argue that directly aggregating individual node representations does not lead to an effective joint representation for multiple nodes. Then, we notice that a few previous successful works for multi-node representation learning, including SEAL, Distance Encoding, and ID-GNN, all used node labeling. These methods first label nodes in the graph according to their relationships with the target node set before applying a GNN. Then, the node representations obtained in the labeled graph are aggregated into a node set representation. By investigating their inner mechanisms, we unify these node labeling techniques into a single and most general form-labeling trick. We prove that with labeling trick a sufficiently expressive GNN learns the most expressive node set representations, thus in principle solves any joint learning tasks over node sets. Experiments on one important two-node representation learning task, link prediction, verified our theory. Our work explains the superior performance of previous node-labeling-based methods, and establishes a theoretical foundation of using GNNs for multi-node representation learning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph neural networks (GNNs) <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref> have achieved great successes in recent years. While GNNs have been well studied for single-node tasks (such as node classification) and whole-graph tasks (such as graph classification), using GNNs to predict a set of multiple nodes is less studied and less understood. Among such multi-node representation learning problems, link prediction (predicting the link existence/class/value between a set of two nodes) is perhaps the most important one due to its wide applications in practice, including friend recommendation in social networks <ref type="bibr" target="#b10">[11]</ref>, movie recommendation in Netflix <ref type="bibr" target="#b11">[12]</ref>, protein interaction prediction <ref type="bibr" target="#b12">[13]</ref>, drug response prediction <ref type="bibr" target="#b13">[14]</ref>, knowledge graph completion <ref type="bibr" target="#b14">[15]</ref>, etc. In this paper, we use link prediction as a medium to study GNN's multi-node representation learning ability. Note that although our examples and experiments are all around link prediction, our theory applies generally to all multi-node representation learning problems such as triplet <ref type="bibr" target="#b15">[16]</ref>, motif <ref type="bibr" target="#b16">[17]</ref> and subgraph <ref type="bibr" target="#b17">[18]</ref> prediction tasks.</p><p>There are two main classes of GNN-based link prediction methods: Graph AutoEncoder (GAE) <ref type="bibr" target="#b18">[19]</ref> and SEAL <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. GAE (and its variational version VGAE <ref type="bibr" target="#b18">[19]</ref>) first applies a GNN to the entire network to compute a representation for each node. The representations of the two end nodes of the link are then aggregated to predict the target link. GAE represents a common practice of using GNNs to learn multi-node representations. That is, first obtaining individual node representations through a GNN as usual, and then aggregating the representations of those nodes of interest as the multi-node representation. On the contrary, SEAL applies a GNN to an enclosing subgraph around each link, where nodes in the subgraph are labeled differently according to their distances to the two end nodes before applying the GNN. Despite both using GNNs for link prediction, SEAL often shows much better practical performance than GAE. As we will see, the key lies in SEAL's node labeling step. isomorphic; links (v1, v2) and (v4, v3) are isomorphic; link (v1, v2) and link (v1, v3) are not isomorphic. However, if we aggregate two node representations learned by a GNN as the link representation, we will give (v1, v2) and (v1, v3) the same prediction.</p><p>We first give a simple example to show when GAE fails. In <ref type="figure" target="#fig_0">Figure 1</ref>, v 2 and v 3 have symmetric positions in the graph-from their respective views, they have exactly the same h-hop neighborhood for any h. Thus, without node features, GAE will learn the same representation for v 2 and v 3 . However, when we want to predict which one of v 2 and v 3 is more likely to form a link with v 1 , GAE will aggregate the representations of v 1 and v 2 as the link representation of (v 1 , v 2 ), and aggregate the representations of v 1 and v 3 to represent (v 1 , v 3 ), thus giving (v 1 , v 2 ) and (v 1 , v 3 ) the same representation and prediction. The failure to distinguish links (v 1 , v 2 ) and (v 1 , v 3 ) that have apparently different structural roles in the graph reflects one key limitation of GAE-type methods: by computing v 1 and v 2 's representations independently of each other, GAE cannot capture the dependence between two end nodes of a link. For example, (v 1 , v 2 ) has a much shorter path between them than that of (v 1 , v 3 ); and (v 1 , v 2 ) has both nodes in the same hexagon, while (v 1 , v 3 ) does not.</p><p>Take common neighbor (CN) <ref type="bibr" target="#b21">[22]</ref>, one elementary heuristic feature for link prediction, as another example. CN counts the number of common neighbors between two nodes to measure their likelihood of forming a link, which is widely used in social network friend recommendation. CN is the foundation of many other successful heuristics such as Adamic-Adar <ref type="bibr" target="#b10">[11]</ref> and Resource Allocation <ref type="bibr" target="#b22">[23]</ref>, which are also based on neighborhood overlap. However, GAE cannot capture such neighborhood-overlapbased features. This can be seen from <ref type="figure" target="#fig_0">Figure 1</ref> too. There is 1 common neighbor between (v 1 , v 2 ) and 0 between (v 1 , v 3 ), but GAE always gives (v 1 , v 2 ) and (v 1 , v 3 ) the same representation. The failure to learn common neighbor demonstrates GAE's severe limitation for link prediction. The root cause still lies in that GAE computes node representations independently of each other-when computing the representation of one end node, it is not aware of the other end node.</p><p>One way to alleviate the above failure is to use one-hot encoding of node indices or random features as input node features <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. With such node-discriminating features, v 2 and v 3 will have different node representations, thus (v 1 , v 2 ) and (v 1 , v 3 ) may also have different link representations after aggregation, enabling GAE to discriminate (v 1 , v 2 ) and (v 1 , v 3 ). However, using node-discriminating features loses GNN's inductive learning ability to map nodes and links with identical neighborhoods (such as nodes v 2 and v 3 , and links (v 1 , v 2 ) and (v 4 , v 3 )) to the same representation, which results in a great loss of generalization ability. The resulting model is no longer permutation invariant/equivariant, violating the fundamental design principle of GNNs. Is there a way to improve GNNs' link discriminating power (so that links like (v 1 , v 2 ) and (v 1 , v 3 ) can be distinguished), while maintaining their inductive learning ability (so that links (v 1 , v 2 ) and (v 4 , v 3 ) have the same representation)?</p><p>In this paper, we analyze the above problem from a structural representation learning point of view. Srinivasan and Ribeiro <ref type="bibr" target="#b25">[26]</ref> prove that the multi-node prediction problem on graphs ultimately only requires finding a most expressive structural representation of node sets, which gives two node sets the same representation if and only if they are isomorphic (a.k.a. symmetric, on the same orbit) in the graph. For example, link (v 1 , v 2 ) and link (v 4 , v 3 ) in <ref type="figure" target="#fig_0">Figure 1</ref> are isomorphic. A most expressive structural representation for links should give any two isomorphic links the same representation while discriminating all non-isomorphic links (such as (v 1 , v 2 ) and (v 1 , v 3 )). According to our discussion above, GAE-type methods that directly aggregate node representations cannot learn a most expressive structural representation. Then, how to learn a most expressive structural representation of node sets?</p><p>To answer this question, we revisit the other GNN-based link prediction framework, SEAL, and analyze how node labeling helps a GNN learn better node set representations. We find out that two properties of a node labeling are crucial for its effectiveness: 1) target-nodes-distinguishing and 2) permutation equivariance. With these two properties, we define labeling trick (Section 4.1), which unifies previous node labeling methods into a single and most general form. Theoretically, we prove that with labeling trick a sufficiently expressive GNN can learn most expressive structural representations of node sets (Theorem 1), which reassures GNN's node set prediction ability. It also closes the gap between GNN's node representation learning nature and node set tasks' multi-node representation learning requirement. We further extend our theory to local isomorphism (Section 5). And finally, experiments on four OGB link existence prediction datasets <ref type="bibr" target="#b26">[27]</ref> verified our theory.</p><p>Note that the labeling trick theory allows the presence of node/edge features/types, thus is not restricted to non-attributed and homogeneous graphs. Previous works on heterogeneous graphs, such as knowledge graphs <ref type="bibr" target="#b27">[28]</ref> and recommender systems <ref type="bibr" target="#b28">[29]</ref> have already seen successful applications of labeling trick. Labeling trick is also not restricted to two-node link representation learning tasks, but generally applies to any multi-node representation learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>In this section, we introduce some important concepts that will be used in the analysis of the paper, including permutation, set isomorphism and most expressive structural representation.</p><p>We consider a graph G = (V, E, A), where V = {1, 2, . . . , n} is the set of n vertices, E ? V ? V is the set of edges, and A ? R n?n?k is a 3-dimensional tensor containing node and edge features. The diagonal components A i,i,: denote features of node i, and the off-diagonal components A i,j,: denote features of edge (i, j). For heterogeneous graphs, the node/edge types can also be expressed in A using integers or one-hot encoding vectors. We further use A ? {0, 1} n?n to denote the adjacency matrix of G with A i,j = 1 iff (i, j) ? E. We let A be the first slice of A, i.e., A = A :,:,1 . Since A contains the complete information of a graph, we sometimes directly use A to denote the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1.</head><p>A permutation ? is a bijective mapping from {1, 2, . . . , n} to {1, 2, . . . , n}. Depending on the context, ?(i) can mean assigning a new index to node i ? V , or mapping node i to node ?(i) of another graph. All n! possible ?'s constitute the permutation group ? n . For joint prediction tasks over a set of nodes, we use S to denote the target node set. For example, S = {i, j} if we want to predict the link between i, j. We define ?(S) = {?(i)|i ? S}. We further define the permutation of A as ?(A), where ?(A) ?(i),?(j),: = A i,j,: .</p><p>Next, we define set isomorphism, which generalizes graph isomorphism to arbitrary node sets. Definition 2. (Set isomorphism) Given two n-node graphs G = (V, E, A), G = (V , E , A ), and two node sets S ? V , S ? V , we say (S, A) and (S , A ) are isomorphic (denoted by (S, A) (S , A )) if ?? ? ? n such that S = ?(S ) and A = ?(A ).</p><p>When (V, A) (V , A ), we say two graphs G and G are isomorphic (abbreviated as A A because V = ?(V ) for any ?). Note that set isomorphism is more strict than graph isomorphism, because it not only requires graph isomorphism, but also requires the permutation maps a specific node set S to another node set S . In practice, when S = V , we are often more concerned with the case of A = A , where isomorphic node sets are defined in the same graph (automorphism). For example, when S = {i}, S = {j} and (i, A) (j, A), we say nodes i and j are isomorphic in graph A (or they have symmetric positions/same structural role in graph A). An example is v 2 and v 3 in <ref type="figure" target="#fig_0">Figure 1</ref>.</p><p>We say a function f defined over the space of (S, A) is permutation invariant (or invariant for abbreviation) if ?? ? ? n , f (S, A) = f (?(S), ?(A)). Similarly, f is permutation equivariant if ?? ? ? n , ?(f (S, A)) = f (?(S), ?(A)). Permutation invariance/equivariance ensures representations learned by a GNN is invariant to node indexing, which is a fundamental design principle of GNNs. Now we define most expressive structural representation of a node set, following <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b20">21]</ref>. Basically, it assigns a unique representation to each equivalence class of isomorphic node sets. For simplicity, we will briefly use structural representation to denote most expressive structural representation in the rest of the paper. We will omit A if it is clear from context. We call ?(i, A) a structural node representation for i, and call ?({i, j}, A) a structural link representation for (i, j).</p><p>Definition 3 requires that the structural representations of two node sets are the same if and only if the two node sets are isomorphic. That is, isomorphic node sets always have the same structural representation, while non-isomorphic node sets always have different structural representations. This is in contrast to positional node embeddings such as DeepWalk <ref type="bibr" target="#b29">[30]</ref> and matrix factorization <ref type="bibr" target="#b30">[31]</ref>, where two isomorphic nodes can have different node embeddings <ref type="bibr" target="#b31">[32]</ref>. GAE using node-discriminating features also learns positional node embeddings.</p><p>Why do we study structural representations? Formally speaking, Srinivasan and Ribeiro <ref type="bibr" target="#b25">[26]</ref> prove that any joint prediction task over node sets only requires a structural representation of node sets. They show that positional node embeddings carry no more information beyond that of structural representations. Intuitively speaking, it is because two isomorphic nodes in a network are perfectly symmetric and interchangeable with each other, and should be indistinguishable from any perspective. Learning a structural node representation guarantees that isomorphic nodes are always classified into the same class. Similarly, learning a structural link representation guarantees isomorphic links, such as (v 1 , v 2 ) and (v 4 , v 3 ) in <ref type="figure" target="#fig_0">Figure 1</ref>, are always predicted the same, while non-isomorphic links, such as (v 1 , v 2 ) and (v 1 , v 3 ), are always distinguishable, which is not guaranteed by positional node embeddings. Structural representation characterizes the maximum representation power a model can reach on graphs. We use it to study GNNs' multi-node representation learning ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The limitation of directly aggregating node representations</head><p>In this section, using GAE for link prediction as an example, we show the key limitation of directly aggregating node representations as a node set representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">GAE for link prediction</head><p>Given a graph A, GAE methods <ref type="bibr" target="#b18">[19]</ref> first use a GNN to compute a node representation z i for each node i, and then use an aggregation function f ({z i , z j }) to predict link (i, j):</p><formula xml:id="formula_0">A i,j = f ({z i , z j }), where z i = GNN(i, A), z j = GNN(j, A).</formula><p>Here? i,j is the predicted score for link (i, j). The model is trained to maximize the likelihood of reconstructing the true adjacency matrix. The original GAE uses a two-layer GCN <ref type="bibr" target="#b4">[5]</ref> as the GNN, and let f ({z i , z j }) := ?(z i z j ). In principle, we can replace GCN with any GNN, and replace ?(z i z j ) with an MLP over any aggregation function over {z i , z j }. Besides inner product, other aggregation choices include mean, sum, bilinear product, concatenation, and Hadamard product. In the following, we will use GAE to denote a general class of GNN-based link prediction methods. GAE uses a GNN to learn node representations and then aggregates pairwise node representations as link representations. Two natural questions to ask are: 1) Is the node representation learned by the GNN a structural node representation? 2) Is the link representation aggregated from two node representations a structural link representation? We answer them respectively in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GNN and structural node representation</head><p>Practical GNNs <ref type="bibr" target="#b32">[33]</ref> usually simulate the 1-dimensional Weisfeiler-Lehman (1-WL) test <ref type="bibr" target="#b33">[34]</ref> to iteratively update each node's representation by aggregating its neighbors' representations. We use 1-WL-GNN to denote a GNN with 1-WL discriminating power, such as GIN <ref type="bibr" target="#b34">[35]</ref>.</p><p>A 1-WL-GNN ensures that isomorphic nodes always have the same representation. But the opposite direction is not guaranteed. For example, a 1-WL-GNN gives the same representation to all nodes in an r-regular graph. Despite this, 1-WL is known to discriminate almost all non-isomorphic nodes <ref type="bibr" target="#b35">[36]</ref>. This indicates that a 1-WL-GNN can always give the same representation to isomorphic nodes, and can give different representations to almost all non-isomorphic nodes.</p><p>To study GNN's maximum expressive power for multi-node representation learning, we also define a node-most-expressive GNN, which gives different representations to all non-isomorphic nodes.</p><formula xml:id="formula_1">Definition 4. A GNN is node-most-expressive if ?i, A,j, A , GNN(i, A) = GNN(j, A ) ? (i, A) (j, A ).</formula><p>That is, node-most-expressive GNN learns structural node representations <ref type="bibr" target="#b2">3</ref> . We define such a GNN because we want to answer: whether GAE, even equipped with a node-most-expressive GNN (so that GNN's node representation power is not a bottleneck), can learn structural link representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GAE cannot learn structural link representations</head><p>Suppose GAE is equipped with a node-most-expressive GNN which outputs structural node representations. Then the question becomes: does the aggregation of structural node representations of i and j result in a structural link representation of (i, j)? The answer is no, as shown in previous works <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref>. We have also illustrated it in the introduction: In <ref type="figure" target="#fig_0">Figure 1</ref>, we have two isomorphic nodes v 2 and v 3 , thus v 2 and v 3 will have the same structural node representation. By aggregating structural node representations, GAE will give (v 1 , v 2 ) and (v 1 , v 3 ) the same link representation. However, (v 1 , v 2 ) and (v 1 , v 3 ) are not isomorphic in the graph. This indicates: Proposition 1. GAE cannot learn structural link representations no matter how expressive node representations a GNN can learn. <ref type="figure" target="#fig_0">Figure 1</ref> for multi-node representation learning problems involving more than two nodes to show that directly aggregating node representations from a GNN does not lead to a structural representation for node sets. The root cause of this problem is that GNN computes node representations independently, without being aware of the other nodes in the target node set S. Thus, even GNN learns the most expressive single-node representations, there is never a guarantee that their aggregation is a structural representation of a node set. In other words, the multi-node representation learning problem is not breakable into multiple independent single-node representation learning problems. If we have to break it, the multiple single-node representation learning problems should be dependent on each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Similarly, we can give examples like</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Labeling trick for multi-node representation learning</head><p>In this section, we first define the general form of labeling trick, and use a specific implementation, zero-one labeling trick, to intuitively explain why labeling trick helps GNNs learn better link representations. Next, we present our main theorem showing that labeling trick enables a node-mostexpressive GNN to learn structural representations of node sets, which formally characterizes GNN's maximum multi-node representation learning ability. Then, we review SEAL and show it exactly uses one labeling trick. Finally, we discuss other labeling trick implementations in previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Labeling trick</head><p>Definition 5. (Labeling trick) Given (S, A), we stack a labeling tensor</p><formula xml:id="formula_2">L (S) ? R n?n?d in the third dimension of A to get a new A (S) ? R n?n?(k+d) , where L satisfies: ?S, A, S , A , ? ? ? n , 1. (target-nodes-distinguishing) L (S) = ?(L (S ) ) ? S = ?(S ), and 2. (permutation equivariance) S = ?(S ), A = ?(A ) ? L (S) = ?(L (S ) ).</formula><p>To explain a bit, labeling trick assigns a label vector to each node/edge in graph A, which constitutes the labeling tensor L (S) . By concatenating A and L (S) , we get the new labeled graph A (S) . By definition we can assign labels to both nodes and edges. However, in this paper, we only consider node labels for simplicity, i.e., we let the off-diagonal components L (S) i,j,: be all zero.</p><p>The labeling tensor L (S) should satisfy two properties in Definition 5. Property 1 requires that if a permutation ? preserving node labels (i.e., L (S) = ?(L (S ) )) exists between nodes of A and A , then the nodes in S must be mapped to nodes in S by ? (i.e., S = ?(S )). A sufficient condition for property 1 is to make the target nodes S have distinct labels from those of the rest nodes, so that S is  <ref type="figure">Figure 2</ref>: When we predict (v1, v2), we will label these two nodes differently from the rest, so that a GNN is aware of the target link when learning v1 and v2's representations. Similarly, when predicting (v1, v3), nodes v1 and v3 will be labeled differently. This way, the representation of v2 in the left graph will be different from that of v3 in the right graph, enabling GNNs to distinguish the non-isomorphic links (v1, v2) and (v1, v3).</p><p>distinguishable from others. Property 2 requires that when (S, A) and (S , A ) are isomorphic under ? (i.e., S = ?(S ), A = ?(A )), the corresponding nodes i ? S, j ? S , i = ?(j) must always have the same label (i.e., L (S) = ?(L (S ) )). A sufficient condition for property 2 is to make the labeling function permutation equivariant, i.e., when the target (S, A) changes to (?(S), ?(A)), the labeling tensor L (S) should equivariantly change to ?(L (S) ). Now we introduce a simplest labeling trick satisfying the two properties in Definition 5, and use it to illustrate how labeling trick helps GNNs learn better node set representations. Definition 6. (Zero-one labeling trick) Given a graph A and a set of nodes S to predict, we give it a diagonal labeling matrix</p><formula xml:id="formula_3">L (S) ? R n?n?1 such that L (S) i,i,1 = 1 if i ? S and L (S) i,i,1 = 0 otherwise.</formula><p>In other words, the zero-one labeling trick assigns label 1 to nodes in S, and label 0 to all nodes not in S. It is a valid labeling trick because firstly, nodes in S get distinct labels, and secondly, the labeling function is permutation equivariant by always giving nodes in the target node set a label 1. These node labels serve as additional node features fed to a GNN together with the original node features.</p><p>Let's return to the example in <ref type="figure" target="#fig_0">Figure 1</ref> to see how the zero-one labeling trick helps GNNs learn better link representations. This time, when we want to predict link (v 1 , v 2 ), we will label v 1 , v 2 differently from the rest nodes, as shown by the different color in <ref type="figure">Figure 2</ref> left. With nodes v 1 and v 2 labeled, when the GNN is computing v 2 's representation, it is also "aware" of the source node v 1 , instead of the previous agnostic way that treats v 1 the same as other nodes. Similarly, when we want to predict link (v 1 , v 3 ), we will again label v 1 , v 3 differently from other nodes as shown in <ref type="figure">Figure 2</ref> right. This way, v 2 and v 3 's node representations are no longer the same in the two differently labeled graphs (due to the presence of the labeled v 1 ), and we are able to predict</p><formula xml:id="formula_4">(v 1 , v 2 ) and (v 1 , v 3 ) differently.</formula><p>The key difference from GAE is that the node representations are no longer computed independently, but are conditioned on each other in order to capture the dependence between nodes.</p><p>At the same time, isomorphic links, such as (v 1 , v 2 ) and (v 4 , v 3 ), will still have the same representation, since the zero-one labeled graph for (v 1 , v 2 ) is still symmetric to the zero-one labeled graph for (v 4 , v 3 ). This brings an exclusive advantage over GAE using node-discriminating features.</p><p>With v 1 and v 2 labeled, a GNN can also learn their common neighbor easily: in the first iteration, only (v 1 , v 2 )'s common neighbors will receive the distinct message from both v 1 and v 2 ; then in the next iteration, all common neighbors will pass their distinct messages back to both v 1 and v 2 , which effectively encode the number of common neighbors into v 1 and v 2 's updated representations. Now we introduce our main theorem showing that with a valid labeling trick, a node-most-expressive GNN can learn structural representations of node sets. Theorem 1. Given a node-most-expressive GNN and an injective set aggregation function AGG, for any S, A, S , A , GNN(S,</p><formula xml:id="formula_5">A (S) ) = GNN(S , A (S ) ) ? (S, A) (S , A ), where GNN(S, A (S) ) := AGG({GNN(i, A (S) )|i ? S}).</formula><p>We include all proofs in the appendix. Theorem 1 implies that AGG({GNN(i, A (S) )|i ? S}) is a structural representation for (S, A). Remember that directly aggregating the structural node representations learned from the original graph A does not lead to structural representations of node sets (Section 3.3). Theorem 1 shows that aggregating the structural node representations learned from the labeled graph A (S) , somewhat surprisingly, results in a structural representation for (S, A).</p><p>The significance of Theorem 1 is that it closes the gap between GNN's single-node representation nature and node set prediction problems' multi-node representation requirement. It demonstrates that GNNs are able to learn most expressive structural representations of node sets, thus are suitable for joint prediction tasks over node sets too. This answers the open question raised in <ref type="bibr" target="#b25">[26]</ref> questioning GNNs' link prediction ability: are structural node representations in general-and GNNs in particularfundamentally incapable of performing link (dyadic) and multi-ary (polyadic) prediction tasks? With Theorem 1, we argue the answer is no. Although GNNs alone have severe limitations for learning joint representations of multiple nodes, GNNs + labeling trick can learn structural representations of node sets too by aggregating structural node representations obtained in the labeled graph.</p><p>Theorem 1 assumes a node-most-expressive GNN. To augment Theorem 1, we give the following theorem, which demonstrates labeling trick's power for 1-WL-GNNs. Theorem 2. In any non-attributed graph with n nodes, if the degree of each node in the graph is between 1 and O(log 1? 2h n) for any constant &gt; 0, then there exists ?(n 2 ) many pairs of nonisomorphic links (u, w), (v, w) such that an h-layer 1-WL-GNN gives u, v the same representation, while with labeling trick the 1-WL-GNN gives u, v different representations.</p><p>Theorem 2 shows that in any non-attributed graph there exists a large number (?(n 2 )) of link pairs <ref type="figure" target="#fig_0">Figure 1</ref>) which are not distinguishable by 1-WL-GNNs alone but distinguishable by 1-WL-GNNs + labeling trick.</p><formula xml:id="formula_6">(like the examples (v 1 , v 2 ) and (v 1 , v 3 ) in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">SEAL uses a labeling trick</head><p>SEAL <ref type="bibr" target="#b19">[20]</ref> is a state-of-the-art link prediction method based on GNNs. It first extracts an enclosing subgraph (h-hop subgraph) around each target link to predict.</p><formula xml:id="formula_7">Definition 7. (Enclosing subgraph) Given (S, A), the h-hop enclosing subgraph A (S,h) of S is the subgraph induced from A by ? j?S {i | d(i, j) ? h}, where d(i, j)</formula><p>is the shortest path distance between nodes i and j.</p><p>Then, SEAL applies Double Radius Node Labeling (DRNL) to give an integer label to each node in the enclosing subgraph. DRNL assigns different labels to nodes with different distances to the two end nodes of the link. It works as follows: The two end nodes are always labeled 1. Nodes farther away from the two end nodes get larger labels (starting from 2). For example, nodes with distances {1, 1} to the two end nodes will get label 2, and nodes with distances {1, 2} to the two end nodes will get label 3. So on and so forth. Finally the labeled enclosing subgraph is fed to a GNN to learn the link representation and output the probability of link existence. Theorem 3. DRNL is a labeling trick.</p><p>Theorem 3 is easily proved by noticing: across different subgraphs, 1) nodes with label 1 are always those in the target node set S, and 2) nodes with the same distances to S always have the same label, while distances are permutation equivariant. Thus, SEAL exactly uses a specific labeling trick to enhance its power, which explains its often superior performance than GAE <ref type="bibr" target="#b19">[20]</ref>. SEAL only uses a subgraph A (S,h) within h hops from the target link instead of using the whole graph. This is not a constraint but rather a practical consideration (just like GAE typically uses less than 3 message passing layers in practice), and its benefits will be discussed in detail in Section 5. When h ? ?, the subgraph becomes the entire graph, and SEAL is able to learn structural link representations from the labeled (entire) graph. Proposition 2. When h ? ?, SEAL can learn structural link representations with a node-mostexpressive GNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Discussion</head><p>DE and DRNL In <ref type="bibr" target="#b20">[21]</ref>, SEAL's distance-based node labeling scheme is generalized to Distance Encoding (DE) that can be applied to |S| &gt; 2 problems. Basically, DRNL is equivalent to DE-2 using shortest path distance. Instead of encoding two distances into one integer label, DE injectively aggregates the embeddings of two distances into a label vector. DE is also a valid labeling trick, as it can also distinguish S and is permutation equivariant. However, there are some subtle differences between DE and DRNL's implementations, which are discussed in Appendix D.</p><p>ID-GNN You et al. <ref type="bibr" target="#b37">[38]</ref> propose Identity-aware GNN (ID-GNN), which assigns a unique color to the "identity" nodes and performs message passing for them with a different set of parameters.</p><p>ID-GNN's coloring scheme is similar to the zero-one labeling trick that distinguishes nodes in the target set with 0/1 labels. However, when used for link prediction, ID-GNN only colors the source node, while the zero-one labeling trick labels both the source and destination nodes. Thus, ID-GNN can be seen as using a partial labeling trick. The idea of conditioning on only the source node is also used in NBFNet <ref type="bibr" target="#b38">[39]</ref>. We leave the exploration of partial labeling trick's power for future work.</p><p>Labeling trick for heterogeneous graphs Since our graph definition A allows the presence of node/edge types, our theory applies to heterogeneous graphs, too. In fact, previous works have already successfully used labeling trick for heterogeneous graphs. IGMC <ref type="bibr" target="#b28">[29]</ref> uses labeling trick to predict ratings between users and items (recommender systems), where a user node k-hop away from the target link receives a label 2k, and an item node k-hop away from the target link receives a label 2k + 1. It is a valid labeling trick since the target user and item always receive distinct labels 0 and 1. On the other hand, GRAIL <ref type="bibr" target="#b27">[28]</ref> applies the DRNL labeling trick to knowledge graph completion.</p><p>Directed case. Despite that we do not restrict our graphs to be undirected, our node set definition (Definition 2) does not consider the order of nodes in the set (i.e., direction of link when |S| = 2). The ordered case assumes S = (1, 2, 3) is different from S = (3, 2, 1). One way to solve this is to define labeling trick respecting the order of S. In fact, if we define ?(S) = ?(S[i]) | i = 1, 2, . . . , |S| (where S[i] denotes the i th element in the ordered set S) in Definition 1, and modify our definition of labeling trick using this new definition of permutation, then Theorem 1 still holds.</p><p>Complexity. Despite the power, labeling trick may introduce extra computational complexity. The reason is that for every node set S to predict, we need to relabel the graph A according to S and compute a new set of node representations within the labeled graph. In contrast, GAE-type methods compute node representations only in the original graph. For small graphs, GAE-type methods can compute all node representations first and then predict multiple node sets at the same time, which saves a significant amount of time. However, for large graphs that cannot fit into the GPU memory, mini-batch training (which extracts a neighborhood subgraph for every node set to predict) has to be used for both GAE-type methods and labeling trick, resulting in similar computation cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Local isomorphism: a more practical view of isomorphism</head><p>The concept of most expressive structural representation is based on assigning node sets the same representation if and only if they are isomorphic to each other in the graph. However, exact isomorphism is not very common. For example, Babai and Kucera <ref type="bibr" target="#b35">[36]</ref> prove that at least (n ? log n) nodes in almost all n-node graphs are non-isomorphic to each other. In practice, 1-WL-GNN also takes up to O(n) message passing layers to reach its maximum power for discriminating non-isomorphic nodes, making it very hard to really target on finding exactly isomorphic nodes/links. Lemma 1. Given a graph with n nodes, a 1-WL-GNN takes up to O(n) message passing layers to discriminate all the nodes that 1-WL can discriminate.</p><p>In this regard, we propose a more practical concept, called local isomorphism. Local h-isomorphism only requires the h-hop enclosing subgraphs around S and S are isomorphic, instead of the entire graphs. We argue that this is a more useful definition than isomorphism, because: 1) Exact isomorphism is rare in real-world graphs. 2) Algorithms targeting on exact isomorphism are more likely to overfit. Only assigning the same representations to exactly isomorphic nodes/links may fail to identify a large amount of nodes/links that are not isomorphic but have very similar neighborhoods. Instead, nodes/links locally isomorphic to each other may better indicate that they should have the same representation. With local h-isomorphism, all our previous conclusions based on standard isomorphism still apply. For example, GAE (without node-discriminating features) still cannot discriminate locally h-non-isomorphic links. And a node-most-expressive GNN with labeling trick can learn the most expressive structural representations of node sets w.r.t. local h-isomorphism, i.e., learn the same representation for two node sets if and only if they are locally h-isomorphic: Corollary 1 demonstrates labeling trick's power in the context of local isomorphism. To switch to local h-isomorphism, all we need to do is to extract the h-hop enclosing subgraph around a node set, and apply labeling trick and GNN only to the extracted subgraph. This is exactly what SEAL does.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>There is emerging interest in studying the representation power of graph neural networks recently. Xu et al. <ref type="bibr" target="#b34">[35]</ref> and Morris et al. <ref type="bibr" target="#b39">[40]</ref> first show that the discriminating power of GNNs performing neighbor aggregation is bounded by the 1-WL test. Many works have since been proposed to increase the power of GNNs by simulating higher-order WL tests <ref type="bibr" target="#b39">[40]</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref>. However, most previous works focus on improving GNN's whole-graph representation power. Little work has been done to analyze GNN's node/link representation power. Srinivasan and Ribeiro <ref type="bibr" target="#b25">[26]</ref> first formally studied the difference between structural representations of nodes and links. Although showing that structural node representations of GNNs cannot perform link prediction, their way to learn structural link representations is to give up GNNs and instead use Monte Carlo samples of node embeddings learned by network embedding methods. In this paper, we show that GNNs combined with labeling trick can as well learn structural link representations, which reassures using GNNs for link prediction.</p><p>Many works have implicitly assumed that if a model can learn node representations well, then combining the pairwise node representations can also lead to good link representations <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b43">44]</ref>. However, we argue in this paper that simply aggregating node representations fails to discriminate a large number of non-isomorphic links, and with labeling trick the aggregation of structural node representations leads to structural link representations. Li et al. <ref type="bibr" target="#b20">[21]</ref> proposed distance encoding (DE), whose implementations based on S-discriminating distances can be shown to be specific labeling tricks. They proved that DE can improve 1-WL-GNNs' discriminating power, enabling them to differentiate almost all (S, A) tuples sampled from r-regular graphs. Our paper contributes to an important aspect that Li et al. <ref type="bibr" target="#b20">[21]</ref> overlooked: 1) Our theory focuses on the gap between a GNN's single-node and multi-node representation power. We show even a GNN has maximum node representation power, it still fails to learn structural representations of node sets unless combined with a labeling trick. However, the theory of DE cannot explain this. 2) Our theory is not restricted to r-regular graphs, but applies to any graphs. 3) Our theory points out that a valid labeling trick is not necessarily distance based-it need only be permutation equivariant and S-discriminating. More discussion on the difference between DE's theory and the theory in this paper is given in Appendix E.</p><p>You et al. <ref type="bibr" target="#b44">[45]</ref> also noticed that structural node representations of GNNs cannot capture the dependence (in particular distance) between nodes. To learn position-aware node embeddings, they propose P-GNN, which randomly chooses some anchor nodes and aggregates messages only from the anchor nodes. In P-GNN, nodes with similar distances to the anchor nodes, instead of nodes with similar neighborhoods, have similar embeddings. Thus, P-GNN cannot learn structural node/link representations. P-GNN also cannot scale to large datasets. You et al. <ref type="bibr" target="#b37">[38]</ref> later proposed ID-GNN. As discussed in Section 4.3, ID-GNN's node coloring scheme can be seen as a partial labeling trick.</p><p>Finally, although labeling trick is formally defined in this paper, various forms of specific labeling tricks have already been used in previous works. To our best knowledge, SEAL <ref type="bibr" target="#b19">[20]</ref> proposes the first labeling trick, which is designed to improve GNN's link prediction power. It is later adopted in inductive knowledge graph completion <ref type="bibr" target="#b27">[28]</ref> and matrix completion <ref type="bibr" target="#b28">[29]</ref>, and is generalized into DE <ref type="bibr" target="#b20">[21]</ref> which works for |S| &gt; 2 cases. Wan et al. <ref type="bibr" target="#b45">[46]</ref> use labeling trick for hyperedge prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head><p>In this section, we use a two-node task, link prediction, to empirically validate the effectiveness of labeling trick for multi-node representation learning. We use four link existence prediction datasets in Open Graph Benchmark (OGB) <ref type="bibr" target="#b26">[27]</ref>: ogbl-ppa, ogbl-collab, ogbl-ddi, and ogbl-citation2.</p><p>These datasets are open-sourced, large-scale (up to 2.9M nodes and 30.6M edges), adopt realistic train/validation/test splits, and have standard evaluation procedures, thus providing an ideal place to benchmark an algorithm's realistic link prediction power. The evaluation metrics include Hits@K and MRR. Hits@K counts the ratio of positive edges ranked at the K-th place or above against all the negative edges. MRR (Mean Reciprocal Rank) computes the reciprocal rank of the true target node against 1,000 negative candidates, averaged over all the true source nodes. Both metrics are higher the better. We include more details and statistics of these datasets in Appendix F. Our code is available at https://github.com/facebookresearch/SEAL_OGB.</p><p>Baselines. We use the following baselines for comparison. We use 5 non-GNN methods: CN (common neighbor), AA (Adamic-Adar), MLP, MF (matrix factorization) and Node2vec. Among them, CN and AA are two simple link prediction heuristics based on counting common neighbors, which are used for sanity checking. We use 3 plain GAE baselines: GraphSAGE <ref type="bibr" target="#b43">[44]</ref>, GCN <ref type="bibr" target="#b18">[19]</ref>, and GCN+LRGA <ref type="bibr" target="#b46">[47]</ref>. These methods use the Hadamard product of pairwise node representations output by a GNN as link representations, without using a labeling trick. Finally, we compare 3 GNN methods using labeling tricks: GCN+DE <ref type="bibr" target="#b20">[21]</ref>, GCN+DRNL, and SEAL <ref type="bibr" target="#b19">[20]</ref>. GCN+DE/GCN+DRNL enhance GCN with the DE/DRNL labeling trick. SEAL uses a GCN and the DRNL labeling trick, with an additional subgraph-level readout SortPooling <ref type="bibr" target="#b8">[9]</ref>. More details are in Appendix G. Moreover, we test the zero-one labeling trick in our ablation experiments. Results can be found in Appendix H.</p><p>Results and discussion. We present the main results in <ref type="table" target="#tab_0">Table 1</ref>. Firstly, we can see that GAE methods without labeling trick do not always outperform non-GNN methods. For example, on ogbl-ppa and ogbl-collab, simple heuristics CN and AA outperform plain GAE methods by large margins. This suggests that GAE methods cannot even learn simple neighborhood-overlapbased heuristics, verifying our argument in Introduction. In contrast, when GNNs are enhanced by labeling trick, they are able to beat heuristics. With labeling trick, GNN methods achieve new state-of-the-art performance on 3 out of 4 datasets. In particular, we observe that SEAL outperforms GAE and positional embedding methods, sometimes by surprisingly large margins. For example, in the challenging ogbl-ppa graph, SEAL achieves an Hits@100 of 48.80, which is 87%-195% higher than GAE methods without using labeling trick. On ogbl-ppa, ogbl-collab and ogbl-citation2, labeling trick methods also achieve state-of-the-art results.</p><p>Despite obtaining the best results on three datasets, we observe that labeling trick methods do not perform well on ogbl-ddi. ogbl-ddi is considerably denser than the other graphs. It has 4,267 nodes and 1,334,889 edges, resulting in an average node degree of 500.5. In ogbl-ddi, labeling trick methods fall behind GAE methods using trainable node embeddings. One possible explanation is that ogbl-ddi is so dense that a practical GNN with limited expressive power is hard to inductively learn any meaningful structural patterns. In comparison, the transductive way of learning free-parameter node embeddings makes GAEs no longer focus on learning inductive structural patterns, but focus on learning node embeddings. The added parameters also greatly increase GAEs' model capacity.</p><p>An interesting future topic is to study how to improve labeling tricks' performance on dense graphs. Appendix H presents more ablation experiments to study the power of different labeling tricks, the effect of subgraph pooling, and the number of hops/layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusions</head><p>In this paper, we proposed a theory of using GNNs for multi-node representation learning. We first pointed out the key limitation of a common practice in previous works that directly aggregates node representations as a node set representation. To address the problem, we proposed labeling trick which gives target nodes distinct labels in a permutation equivariant way. We proved that labeling trick enables GNNs to learn most expressive structural representations of node sets, which formally characterizes GNNs' maximum multi-node representation learning ability. Experiments on four OGB datasets verified labeling trick's effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proof of Theorem 1</head><p>We restate Theorem 1: Given a node-most-expressive GNN and an injective set aggregation function AGG, for any S, A, S , A , GNN(S, A (S) ) = GNN(S , A (S ) ) ? (S, A) (S , A ), where GNN(S, A (S) ) := AGG({GNN(i, A (S) )|i ? S}).</p><p>Proof. We need to show AGG <ref type="figure">({GNN(i, A (S)</ref> </p><formula xml:id="formula_8">)|i ? S}) = AGG({GNN(i, A (S ) |i ? S }) iff (S, A) (S , A ).</formula><p>To prove the first direction, we notice that with an injective AGG,</p><formula xml:id="formula_9">AGG({GNN(i, A (S) ))|i ? S}) = AGG({GNN(i, A (S ) ))|i ? S }) =? ? v 1 ? S, v 2 ? S , such that GNN(v 1 , A (S) ) = GNN(v 2 , A (S ) ) (1) =? (v 1 , A (S) ) (v 2 , A (S ) ) (because GNN is node-most-expressive) (2) =? ? ? ? ? n , such that v 1 = ?(v 2 ), A (S) = ?(A (S ) ).<label>(3)</label></formula><p>Remember A (S) is constructed by stacking A and L (S) in the third dimension, where L (S) is a tensor satisfying: ?? ? ? n , (1) L (S) = ?(L (S ) ) ? S = ?(S ), and (2) S = ?(S ), A = ?(A ) ? L (S) = ?(L (S ) ). With A (S) = ?(A (S ) ), we have both A = ?(A ), L (S) = ?(L (S ) ).</p><p>Because L (S) = ?(L (S ) ) ? S = ?(S ), continuing from Equation <ref type="formula" target="#formula_9">(3)</ref>, we have</p><formula xml:id="formula_10">AGG({GNN(i, A (S) )|i ? S}) = AGG({GNN(i, A (S ) )|i ? S }) =? ? ? ? ? n , such that A = ?(A ), L (S) = ?(L (S ) )<label>(4)</label></formula><p>=? ? ? ? ? n , such that A = ?(A ), S = ?(S ) (5)</p><formula xml:id="formula_11">=? (S, A) (S , A ).<label>(6)</label></formula><p>Now we prove the second direction. Because S = ?(S ), A = ?(A ) ? L (S) = ?(L (S ) ), we have:</p><formula xml:id="formula_12">(S, A) (S , A ) =? ? ? ? ? n , such that S = ?(S ), A = ?(A )<label>(7)</label></formula><p>=? ? ? ? ? n , such that S = ?(S ), A = ?(A ), L (S) = ?(L (S ) ) (8)</p><p>=? ? ? ? ? n , such that S = ?(S ), A (S) = ?(A (S ) ) (9)</p><formula xml:id="formula_13">=? ? ? ? ? n , such that ?v 1 ? S, v 2 ? S , v 1 = ?(v 2 ), we have GNN(v 1 , A (S) ) = GNN(v 2 , A (S ) ) (10) =? AGG({GNN(v 1 , A (S) )|v 1 ? S}) = AGG({GNN(v 2 , A (S ) )|v 2 ? S }),<label>(11)</label></formula><p>which concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Theorem 2</head><p>We restate Theorem 2: In any non-attributed graph with n nodes, if the degree of each node in the graph is between 1 and O(log Proof. Our proof has two steps. First, we would like to show that there are ?(n ) nodes that are locally h-isomorphic (see <ref type="bibr">Definition 8)</ref> to each other. Then, we prove that among these nodes, there are at least ?(n 2 ) pairs of nodes such that there exists another node constructing locally h non-isomorphic links with either of the two nodes in each node pair.</p><p>Step 1. Consider an arbitrary node v and denote the subgraph induced by the nodes that are at most h-hop away from v as G </p><formula xml:id="formula_14">(h) v , denoted by |V (G (h) v )|, satisfies |V (G (h) v )| ? h i=0 d i = O(d h ) = O(log 1? 2 n).</formula><p>We set the max K = max v?V |V (G </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Now we expand subgraphs G</head><formula xml:id="formula_15">(h) v to? (h) v by adding K ? |V (G (h) v )| independent nodes for each node v ? V . Then, all? (h)</formula><p>v have the same number of nodes, which is K, though they may not be connected graphs.</p><p>Next, we consider the number of non-isomorphic graphs over K nodes. Actually, the number of non-isomorphic graph structures over K nodes is bounded by 2 ( K 2 ) = exp(O(log 1? n)) = o(n 1? ).</p><p>Therefore, due to the pigeonhole principle, there exist n/o(n 1? ) = ?(n ) many nodes v whos? G (h)</p><p>v are isomorphic to each other. Denote the set of these nodes as V iso , which consist of nodes that are all locally h-isomorphic to each other. Next, we focus on looking for other nodes to form locally h-non-isomorphic links with nodes V iso .</p><p>Step 2. Let us partition V iso = ? q i=1 V i so that for all nodes in V i , they share the same first-hop neighbor sets. Then, consider any pair of nodes u, v such that u, v are from different V i 's. Since u, v share identical h-hop neighborhood structures, an h-layer 1-WL-GNN will give them the same representation. Then, we may pick one u's first-hop neighbor w that is not v's first-hop neighbor. We know such w exists because of the definition of V i . As w is u's first-hop neighbor and is not v's first-hop neighbor, (u, w) and (v, w) are not isomorphic. With labeling trick, the h-layer 1-WL-GNN will give u, v different representations immediately after the first message passing round due to w's distinct label. Therefore, we know such a (u, w), (v, w) pair is exactly what we want.</p><p>Based on the partition V iso , we know the number of such non-isomorphic link pairs (u, w) and (v, w) is at least:</p><formula xml:id="formula_16">Y ? q i,j=1,i =j |V i ||V j | = 1 2 ( q i=1 |V i |) 2 ? q i=1 |V i | 2 .<label>(12)</label></formula><p>Because of the definitions of the partition, q i=1 |V i | = |V iso | = ?(n ) and the size of each V i satisfies</p><formula xml:id="formula_17">1 ? |V i | ? d w = O(log 1? 2h n),</formula><p>where w is one of the common first-hop neighbors shared by all nodes in V i and d w is its degree.</p><p>By plugging in the range of |V i |, Eq.12 leads to</p><formula xml:id="formula_18">Y ? 1 2 (?(n 2 ) ? ?(n )O(log 1? 2h n)) = ?(n 2 ),</formula><p>which concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proof of Lemma 1</head><p>We restate Lemma 1: Given a graph with n nodes, a 1-WL-GNN takes up to O(n) message passing layers to discriminate all the nodes that 1-WL can discriminate.</p><p>Proof. We first note that after one message passing layer, 1-WL-GNN gives different embeddings to any two nodes that 1-WL gives different colors to after one iteration. So we only need to show how many iterations 1-WL takes to converge in any graph.</p><p>Note that if two nodes are given different colors by 1-WL at some iteration (they are discriminated by 1-WL), their colors are always different in any future iteration. And if at some iteration, all nodes' colors are the same as their colors in the last iteration, then 1-WL will stop (1-WL fails to discriminate any more nodes and has converged). Therefore, before termination, 1-WL will increase its total number of colors by at least 1 after every iteration. Because there are at most n different final colors given an n-node graph, 1-WL takes at most n ? 1 = O(n) iterations before assigning all nodes different colors.</p><p>Now it suffices to show that there exists an n-node graph that 1-WL takes O(n) iterations to converge. Suppose there is a path of n nodes. Then by simple calculation, it takes n/2 iterations for 1-WL to converge, which concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Comparisons between DRNL and DE</head><p>In this section, we discuss the relationships and differences between DRNL <ref type="bibr" target="#b19">[20]</ref> and DE <ref type="bibr" target="#b20">[21]</ref> (using shortest path distance). Although they are theoretically equivalent in the context of link prediction, there are some subtle differences that might result in significant performance differences.</p><p>Suppose x and y are the two end nodes of the link. DRNL (Double Radius Node Labeling) always assigns label 1 to x and y. Then, for any node i with (d(i, x), d(i, y)) = (1, 1), it assigns a label 2.</p><p>Nodes with radius (1, 2) or (2, 1) get label 3. Nodes with radius (1, 3) or (3, 1) get 4. Nodes with (2, 2) get 5. Nodes with <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b3">4)</ref> or (4, 1) get 6. Nodes with (2, 3) or (3, 2) get 7. So on and so forth. In other words, DRNL iteratively assigns larger labels to nodes with a larger radius w.r.t. both the two end nodes. The DRNL label f l (i) of a node i can be calculated by the following hashing function:</p><formula xml:id="formula_19">f l (i) = 1 + min(d x , d y ) + (d/2)[(d/2) + (d%2) ? 1],<label>(13)</label></formula><p>where d x := d(i, x), d y := d(i, y), d := d x + d y , (d/2) and (d%2) are the integer quotient and remainder of d divided by 2, respectively. This hashing function allows fast closed-form computations of DRNL labels. For nodes with d(i, x) = ? or d(i, y) = ?, DRNL assigns them a null label 0. Later, the one-hot encoding of these labels are fed to a GNN as the initial node features, or equivalently, we can feed the raw integer labels to an embedding layer first.</p><p>Instead of encoding (d(i, x), d(i, y)) into a single integer label, DE (distance encoding) directly uses the vector [d(i, x), d(i, y)] as a size-2 label for node i. Then, these size-2 labels will be transformed to two-hot encoding vectors to be used as the input node features to GNN. Equivalently, we can also input the size-2 labels to an embedding layer and use the sum-pooled embedding as the initial node features.</p><p>These two ways of encoding (d(i, x), d(i, y)) theoretically have the same expressive power. However, DRNL and DE have some subtle differences in their implementations. The first difference is that DE sets a maximum distance d max (a small integer such as 3) for each d(i, x) or d(i, y), i.e., if d(i, x) ? d max , DE will let d(i, x) = d max . This potentially can avoid some overfitting by reducing the number of possible DE labels as claimed in the original paper <ref type="bibr" target="#b20">[21]</ref>.</p><p>The second difference is that when computing the distance d(i, x), DRNL will temporarily mask node y and all its edges, and when computing the distance d(i, y), DRNL will temporarily mask node x and all its edges. The reason for this "masking trick" is because DRNL aims to use the pure distance between i and x without the influence of y. If we do not mask y, d(i, x) will be upper bounded by d(i, y) + d(x, y), which obscures the "true distance" between i and x and might hurt the node labels' ability to discriminate structurally-different nodes. As we will show in Appendix H, this masking trick has a great influence on the performance, which explains DE's inferior performance than DRNL in our experiments.</p><p>As we will show in <ref type="table" target="#tab_0">Table 1</ref>, DRNL has significantly better performance than DE on some datasets. To study what is the root cause for these in-principle equivalent methods's different practical performance, we propose DE + , which adopts DRNL's masking trick in DE. We also try to not set a maximum distance in DE + . This way, there are no more differences in terms of the expressive power between DE + and DRNL. And we indeed observed that DE + is able to catch up with DRNL in those datasets where DE does not perform well, as we will show in Appendix H.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More discussion on the differences between DE's theory and ours</head><p>Inspired by the empirical success of SEAL <ref type="bibr" target="#b19">[20]</ref>, Li et al. <ref type="bibr" target="#b20">[21]</ref> proposed distance encoding (DE). It generalizes SEAL's distance-based node labeling (DRNL) for link prediction to arbitrary node set prediction, and theoretically studies how the distance information improves 1-WL-GNN's discriminating power. The main theorem in <ref type="bibr" target="#b20">[21]</ref> (Theorem 3.3) proves that under mild conditions, a 1-WL-GNN combined with DE can discriminate any (S, A), (S , A ) pair sampled uniformly from all r-regular graphs, with high probability. This is a significant result, as 1-WL-GNN's discriminating power is bounded by 1-WL, which fails to discriminate any nodes or node sets from r-regular graphs. DE's theory shows that with DE we can break the limit of 1-WL and 1-WL-GNN on this major class of graphs where without DE they always fail.</p><p>Despite the success, DE's theory also has several limitations. Firstly, its analysis focuses on the space of random graphs (in particular regular graphs that 1-WL-GNNs fail to represent well). Secondly, DE's theory does not answer whether a GNN combined with DE can learn structural representations, which are the core for joint node set prediction tasks such as link prediction according to <ref type="bibr" target="#b25">[26]</ref>. Thirdly, although DE's definition (Definition 3.1 of <ref type="bibr" target="#b20">[21]</ref>) only requires permutation invariance, its theory and practical implementations require distance-based node labeling. It is unknown whether other node labeling tricks (including those do not rely on distance) are also useful.</p><p>Our theory partly addresses these limitations and is orthogonal to DE's theory, as: 1) We define labeling trick, which is not necessarily distance-based. We show a valid labeling trick need only be permutation equivariant and target-node-set-discriminating. is that the answer is yes for |S| = 1 and |S| = 2. This is because, with an injective message passing layer, we can propagate the unique labels of S to other nodes, thus "recovering" the distance information through iterative message passing. We leave a rigorous proof or disproof to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F More details about the datasets</head><p>We compare the link prediction performance of different baselines on ogbl-ppa, ogbl-collab, ogbl-ddi, and ogbl-citation2. Among them, ogbl-ppa is a protein-protein association graph where the task is to predict biologically meaningful associations between proteins. ogbl-collab is an author collaboration graph, where the task is to predict future collaborations. ogbl-ddi is a drugdrug interaction network, where each edge represents an interaction between drugs which indicates the joint effect of taking the two drugs together is considerably different from their independent effects. ogbl-citation2 is a paper citation network, where the task is to predict missing citations. We present the statistics of these OGB datasets in <ref type="table" target="#tab_2">Table 2</ref>. More information about these datasets can be found in <ref type="bibr" target="#b26">[27]</ref>. OGB has an official leaderboard 4 , too, providing a place to fairly compare different methods' link prediction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G More details about the baselines</head><p>We include baselines achieving top places on the OGB leaderboard. All the baselines have their open-sourced code and paper available from the leaderboard. We adopt the numbers published on the leaderboard if available, otherwise we run the method ourselves using the open-sourced code. Note that there are other potentially strong baselines that we have to omit here, because they cannot easily scale to OGB datasets. For example, we have contacted the authors of P-GNN <ref type="bibr" target="#b44">[45]</ref>, and confirmed that P-GNN is not likely to scale to OGB datasets due to the computation of all-pairs shortest path distances.</p><p>All the compared methods are in the following. We briefly describe how each method obtains its final node representations.  <ref type="bibr" target="#b19">[20]</ref>: The same as GCN+DRNL with an additional subgraph-level readout. Note that we reimplemented SEAL in this paper with a greatly improved efficiency and flexibility than the original implementation <ref type="bibr" target="#b4">5</ref> . The code will be released in the future.</p><p>Except SEAL, all models use the Hadamard product between pairwise node representations as the link representations. The link representations are fed to an MLP for final prediction. All the GAE methods' GNNs have 3 message passing layers with 256 hidden dimensions, with a tuned dropout ratio in {0, 0.5}. All the labeling trick methods (GCN+DE, GCN+DRNL and SEAL) extract 1-hop enclosing subgraphs. The GCNs in GCN+DRNL and GCN+DE also use 3 message passing layers with 256 hidden dimensions for consistency. The GNN in SEAL follows the DGCNN in the original paper, which has 3 GCN layers with 32 hidden dimensions each, plus a SortPooling layer <ref type="bibr" target="#b8">[9]</ref> and several 1D convolution layers after the GCN layers to readout the subgraph. The use of a subgraph-level readout instead of only reading out two nodes is not an issue for SEAL, because 1) the two center nodes' information is still included in the output of the subgraph-level readout, and 2) the inclusion of additional neighborhood node representations may help learn better neighborhood features than only reading out two center nodes. As we will show in Appendix H.3, a subgraph-level readout sometimes improves the performance.</p><p>The ogbl-ddi graph contains no node features, so MLP is omitted, and the GAE methods here use free-parameter node embeddings as the GNN input node features and train them together with the GNN parameters. For labeling trick methods, the node labels are input to an embedding layer and then concatenated with the node features (if any) as the GNN input. Note that the original SEAL can also include pretrained node embeddings as additional features. But according to <ref type="bibr" target="#b25">[26]</ref>, node embeddings bring no additional value given structural representations. This is also consistent with our observation and the experimental results of <ref type="bibr" target="#b19">[20]</ref>, where including node embeddings gives no better results. Thus, we give up node embeddings in SEAL.</p><p>For the baseline GCN+LRGA, its default hyperparameters result in out of GPU memory on ogbl-citation2, even we use an NVIDIA V100 GPU with 32GB memory. Thus, we have to reduce its hidden dimension to 16 and matrix rank to 10. It is possible that it can achieve better performance with a larger hidden dimension and larger matrix rank using a GPU with a larger memory.</p><p>We implemented the labeling trick methods (GCN+DE, GCN+DRNL and SEAL) using the PyTorch Geometric <ref type="bibr" target="#b47">[48]</ref> package. For all datasets, labeling trick methods only used a fixed 1% to 10% of all the available training edges as the positive training links, and sampled an equal number of negative training links randomly. Labeling trick methods showed excellent performance even without using the full training data, which indicates its strong inductive learning ability. Due to using different labeled subgraphs for different links, labeling trick methods generally take longer running time than GAE methods. On the largest ogbl-citation2 graph, SEAL takes about 7 hours to finishing its training of 10 epochs, and takes another 28 hours to evaluate the validation and test MRR each. For ogbl-ppa, SEAL takes about 20 hours to train for 20 epochs and takes about 4 hours for evaluation. The other two datasets are finished within hours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H Ablation study</head><p>In this section, we conduct several ablation experiments to more thoroughly study the effect of different components around labeling trick on the final link prediction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.1 How powerful is the zero-one labeling trick?</head><p>Firstly, we aim to understand how powerful the proposed zero-one labeling (Definition 6) is. Although zero-one labeling is a also valid labeling trick that theoretically enables a node-most-expressive GNN to learn structural representations, in practice our GNNs may not be expressive enough. Then how will the zero-one labeling trick perform compared to those more sophisticated ones such as DE and DRNL? We conduct experiments on ogbl-collab and ogbl-citation2 to answer this question.</p><p>In <ref type="table" target="#tab_4">Table 3</ref>, we compare GCN (1-hop) + all-zero labeling (not a valid labeling trick), GCN (1-hop) + zero-one labeling trick, and GCN (1-hop) + DRNL. All methods use the same 3 GCN layers with 256 hidden dimensions, 1-hop enclosing subgraphs, and Hadamard product of the two end node representations as the link representations. All the remaining settings follow those of GCN+DRNL. From <ref type="table" target="#tab_4">Table 3</ref>, we can see that GCN+zero-one labeling trick indeed has better performance than GCN without labeling trick, which aligns with our theoretical results that even a simple zero-one labeling is also a valid labeling trick that enables learning structural representations. Nevertheless, the zero-one labeling trick is indeed less powerful than DRNL, as shown by the performance gaps especially on the ogbl-citation2 dataset. We are then interested in figuring out what could cause such large performance differences between two (both valid) labeling tricks, because as Theorem 1 shows, any valid labeling trick can enable a node-most-expressive GNN to learn structural link representations.</p><p>We suspect that the insufficient expressive power of GCN is the cause. Therefore, we change GCN to Graph Isomorphism Network (GIN) <ref type="bibr" target="#b34">[35]</ref>. By replacing the linear feature transformations in GCN with MLPs, GIN is one of the most expressive GNNs based on message passing. The results are shown in the last column of  <ref type="table" target="#tab_0">Table 1</ref>). All of them use GCN as the GNN with the same hyperparameters. The results are shown in <ref type="table" target="#tab_6">Table 4</ref>. We can observe that DE + outperforms DE by large margins. This indicates that the masking trick used in DRNL is very important. Intuitively, temporarily masking the target node y when computing distances to the source node x can give more diverse node labels. Without the masking, d(i, x) will be upper bounded by d(i, y) + d(x, y). Because the distance between x and y can be small in positive links, without the masking d(i, x) will be restricted to small numbers, which hurts their ability to detect subtle differences between nodes' relative positions within the subgraph. Nevertheless, the benefit of the masking trick is not observed in smaller datasets such as ogbl-collab <ref type="table" target="#tab_0">(Table 1)</ref>.</p><p>We can also find that DE + without setting a maximum distance has very close performance to DRNL, which aligns with our discussion in Appendix D. By removing the maximum distance restriction, DE + essentially becomes DRNL. However, there are still small performance differences, possibly because DRNL has a larger embedding table than DE + (DRNL's maximum label is larger) which results in a slightly larger model capacity. Nevertheless, this can be alleviated by doubling the embedding dimension of DE + . In summary, we can conclude that the masking trick used in DRNL is crucial to the performance on some datasets. Compared to DE, DE + and DRNL show better practical performance. Studying more powerful labeling tricks is also an important future direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3 Is a subgraph-level readout useful?</head><p>In <ref type="table" target="#tab_0">Table 1</ref>, we observe that SEAL is generally better than GCN+DRNL. SEAL also uses GCN and the DRNL labeling trick, so the main difference is the subgraph-level readout in SEAL. That is, instead of only reading out the two center nodes' representations as the link representation, SEAL performs a readout over all the nodes in the enclosing subgraph. Here we study this effect further by testing whether a subgraph-level sum-pooling readout is also useful. We replace the Hadamard product of two center node representations in GCN+DRNL with the sum over all node representations within the enclosing subgraph. The results are shown in <ref type="table" target="#tab_7">Table 5</ref>. As we can see, using sum-pooling has a similar effect to the SortPooling in SEAL, i.e., it greatly improves the performance on ogbl-citation2, while slightly reduces the performance on ogbl-collab. This means, using a subgraph-level readout can sometimes be very helpful. Although according to Theorem 1 we only need to aggregate the representations of the two center nodes (two end nodes of the link) as the link representation, in practice, because our GNNs only have limited expressive power, reading out all nodes within the enclosing subgraph could help GNNs learn better subgraph-level features thus better detecting the target link's local h-isomorphism class. Such subgraph representations can be more expressive than only the two center nodes' representations, especially when the number of message passing layers is small so that the center nodes have not gained enough information from the whole subgraph.</p><p>H.4 Is it helpful to make number of layers larger than number of hops?</p><p>In all labeling trick methods, we have used a fixed enclosing subgraph hop number h = 1, and a fixed number of message passing layers l = 3. Using a number of message passing layers larger than the number of hops is different from the practice of previous work. For example, in GAE, we always select h = l hops of neighbors if we decide to use l message passing layers. So is it really helpful to use l &gt; h? Intuitively, using l &gt; h layers can make GNNs more sufficiently absorb the entire enclosing subgraph information and learn better link representations. Theoretically, as we have shown in Lemma 1, to reach the maximum representation power of 1-WL-GNN, we need to use O(n) number of message passing layers, where n is the number of nodes in the enclosing subgraph. Thus, using l &gt; h can enhance GNN's representation power and learn more expressive link representations. To validate the above , we conduct experiments on GCN+DRNL by using l = 1 message passing layers (and still h = 1). The results are shown in <ref type="table" target="#tab_8">Table 6</ref>. As we can observe, using l = 1 results in lower performance than using l = 3 in all three datasets. On ogbl-collab, this effect is very small. However, on ogbl-ppa and ogbl-citation2, the performance gaps are significant. These results demonstrate the usefulness of using more message passing layers than hops.</p><p>Nevertheless, we are unsure whether it is still helpful to make l &gt; h when we use a large h, such as h = 2 or h = 3. We cannot generally verify this because increasing h will exponentially increase our subgraph sizes. And considering the huge computation cost on two relatively large datasets ogbl-ppa and ogbl-citation2, using h = 1 is currently the maximum h we can afford. We thus only conduct experiments using different h's on the smallest ogbl-collab dataset. We have tried different combinations of (l, h) from (1, 1) all the way up to (4, 3), and the testing scores are consistently around 63 to 64. This seems to indicate increasing h or l is not helpful in this dataset. Nevertheless, ogbl-collab may not be representative enough to derive a general conclusion. For example, in the original SEAL paper <ref type="bibr" target="#b19">[20]</ref>, the authors found using h = 2 is helpful for many datasets. Thus, fully answering this question might need further investigations. But when h = 1, we can conclude that using l &gt; h is better.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>In this graph, nodes v2 and v3 are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 3 .</head><label>3</label><figDesc>Given an invariant function ?(?), ?(S, A) is a most expressive structural representation for (S, A) if ?S, A, S , A , ?(S, A) = ?(S , A ) ? (S, A) (S , A ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Definition 8 .</head><label>8</label><figDesc>(Local h-isomorphism) ?S, A, S , A , we say (S, A) and (S , A ) are locally hisomorphic to each other if (S, A (S,h) ) (S , A (S ,h) ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Corollary 1 .</head><label>1</label><figDesc>Given a node-most-expressive GNN and an injective set aggregation function AGG, then for any S, A, S , A , h, GNN(S, A (S) (S,h) ) = GNN(S , A (S ) (S ,h) ) ? (S, A (S,h) ) (S , A (S ,h) ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>for any constant &gt; 0, then there exists ?(n 2 ) many pairs of non-isomorphic links (u, w), (v, w) such that an h-layer 1-WL-GNN gives u, v the same representation, while with labeling trick the 1-WL-GNN gives u, v different representations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>v</head><label></label><figDesc>(the h-hop enclosing subgraph of v). As each node is with degree d = O(log 1? 2h n), then the number of nodes in G</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Results for ogbl-ppa, ogbl-collab, ogbl-ddi and ogbl-citation2.CN   28.23?0.00<ref type="bibr" target="#b26">27</ref>.6?0.00 60.36?0.00 61.37?0.00 9.47?0.00 17.73?0.00 51.19?0.00 51.47?0.00 AA 32.68?0.00 32.45?0.00 63.49?0.00 64.17?0.00 9.66?0.00 18.61?0.00 51.67?0.00 51.89?0.00 MLP 0.46?0.00 0.46?0.00 24.02?1.45 19.27?1.29 --29.03?0.17 29.06?0.16 Node2vec 22.53?0.88 22.26?0.88 57.03?0.52 48.88?0.54 32.92?1.21 23.26?2.09 61.24?0.11 61.41?0.11 MF 32.28?4.28 32.29?0.94 48.96?0.29 38.86?0.29 33.70?2.64 13.68?4.75 51.81?4.36 51.86?4.43 Plain GAE GraphSAGE 17.24?2.64 16.55?2.40 56.88?0.77 54.63?1.12 62.62?0.37 53.90?4.74 82.63?0.23 82.60?0.36 GCN 18.45?1.40 18.67?1.32 52.63?1.15 47.14?1.45 55.50?2.08 37.07?5.07 84.79?0.23 84.74?0.21 GCN+LRGA 25.75?2.82 26.12?2.35 60.88?0.59 52.21?0.72 66.75?0.58 62.30?9.12 66.48?1.61 66.49?1.59 Labeling Trick GCN+DE 36.31?3.59 36.48?3.78 64.13?0.16 64.44?0.29 29.85?2.25 26.63?6.82 60.17?0.63 60.30?0.61 GCN+DRNL 46.43?3.03 45.24?3.95 64.51?0.42 64.40?0.45 29.47?1.54 22.81?4.93 81.07?0.30 81.27?0.31 SEAL 51.25?2.52 48.80?3.16 64.95?0.43 64.74?0.43 28.49?2.69 30.56?3.86 87.57?0.31 87.67?0.32</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ogbl-ppa</cell><cell cols="2">ogbl-collab</cell><cell cols="2">ogbl-ddi</cell><cell cols="2">ogbl-citation2</cell></row><row><cell></cell><cell></cell><cell cols="2">Hits@100 (%)</cell><cell cols="2">Hits@50 (%)</cell><cell cols="2">Hits@20 (%)</cell><cell>MRR (%)</cell><cell></cell></row><row><cell>Category</cell><cell>Method</cell><cell>Validation</cell><cell>Test</cell><cell>Validation</cell><cell>Test</cell><cell>Validation</cell><cell>Test</cell><cell>Validation</cell><cell>Test</cell></row><row><cell>Non-GNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics and evaluation metrics of OGB link prediction datasets.</figDesc><table><row><cell>Dataset</cell><cell>#Nodes</cell><cell>#Edges</cell><cell>Avg. node deg.</cell><cell>Density</cell><cell>Split ratio</cell><cell>Metric</cell></row><row><cell>ogbl-ppa</cell><cell>576,289</cell><cell>30,326,273</cell><cell>73.7</cell><cell>0.018%</cell><cell>70/20/10</cell><cell>Hits@100</cell></row><row><cell>ogbl-collab</cell><cell>235,868</cell><cell>1,285,465</cell><cell>8.2</cell><cell>0.0046%</cell><cell>92/4/4</cell><cell>Hits@50</cell></row><row><cell>ogbl-ddi</cell><cell>4,267</cell><cell>1,334,889</cell><cell>500.5</cell><cell>14.67%</cell><cell>80/10/10</cell><cell>Hits@20</cell></row><row><cell cols="3">ogbl-citation2 2,927,963 30,561,187</cell><cell>20.7</cell><cell>0.00036%</cell><cell>98/1/1</cell><cell>MRR</cell></row></table><note>We choose OGB datasets for benchmarking our methods because these datasets adopt realistic train/validation/test splitting methods, such as by resource cost in laboratory (ogbl-ppa), by time (ogbl-collab and ogbl-citation2), and by drug target in the body (ogbl-ddi). They are also large-scale (up to 2.9M nodes and 30.6M edges), open-sourced, and have standard evaluation metrics.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>GAE method with GraphSAGE as the GNN. ? GCN [19]: A GAE method with GCN as the GNN. ? LRGA [47]: A GAE method with LRGA-module-enhanced GCN. ? GCN+DE: Apply GCN to the DE [21] labeled graphs. ? GCN+DRNL: Apply GCN to the DRNL [20] labeled graphs.</figDesc><table><row><cell>? MLP: Node features are directly used as the node representations without considering graph</cell></row><row><cell>structure.</cell></row><row><cell>? Node2vec [30, 43]: The node representations are the concatenation of node features and Node2vec</cell></row><row><cell>embeddings.</cell></row><row><cell>? MF (Matrix Factorization): Use free-parameter node embeddings trained end-to-end as the node</cell></row><row><cell>representations.</cell></row><row><cell>? GraphSAGE [44]: A</cell></row></table><note>? SEAL</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Ablation study on the power of the zero-one labeling trick. 35?1.28 25.92?1.47 36.97?0.56 36.98?0.57 GCN (1-hop) + zero-one labeling trick 44.45?1.39 44.79?1.26 38.73?0.86 38.78?0.88 GCN (1-hop) + DRNL 64.51?0.42 64.40?0.45 81.07?0.30 81.27?0.31 GIN (1-hop) + zero-one labeling trick 60.31?0.81 59.48?1.17 78.32?1.07 78.50?1.08</figDesc><table><row><cell></cell><cell cols="2">ogbl-collab</cell><cell cols="2">ogbl-citation2</cell></row><row><cell></cell><cell cols="2">Hits@50 (%)</cell><cell>MRR (%)</cell><cell></cell></row><row><cell>Method</cell><cell>Validation</cell><cell>Test</cell><cell>Validation</cell><cell>Test</cell></row><row><cell>GCN (1-hop) + all-zero labeling</cell><cell>24.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>As we can see, GIN (1-hop) + zero-one labeling trick has much better performance than GCN (1-hop) + zero-one labeling trick, and is almost catching up with GCN (1-hop) + DRNL. The results very well align with our theory-as long as we have a sufficiently expressive GNN, even a simple zero-one labeling trick can be very powerful in terms of enabling learning structural representations. Nevertheless, in practice when we only have less powerful GNNs, we should better choose those more sophisticated labeling tricks such as DE and DRNL for better link prediction performance.H.2 DE vs. DE + vs. DRNLIn Appendix D, we have discussed the differences of the implementations of DE and DRNL. That is, although DE and DRNL are equivalent in theory, there are two differences in their implementations: 1) DE sets a maximum distance d max (by default 3) while DRNL does not, and 2) DRNL masks the other end node when computing the distances to one end node and vice versa, while DE does not. To study whether it is these implementation differences between DE and DRNL that result in the large performance differences inTable 1, we propose DE + which no longer sets a maximum distance in DE and additionally does the masking trick like DRNL. We compare DE, DE + , and DRNL on ogbl-ppa and ogbl-citation2 (where DE shows significantly lower performance than DRNL in</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Comparison of DE, DE + and DRNL. 31?3.59 36.48?3.78 60.17?0.63 60.30?0.61 CCN+DE + (d max = 3) 47.17?1.84 45.70?3.46 74.75?1.18 75.00?1.20 CCN+DE + (d max = ?) 45.81?3.53 43.88?5.18 79.37?4.50 78.85?0.17 GCN+DRNL 46.43?3.03 45.24?3.95 81.07?0.30 81.27?0.31</figDesc><table><row><cell></cell><cell cols="2">ogbl-ppa</cell><cell cols="2">ogbl-citation2</cell></row><row><cell></cell><cell cols="2">Hits@100 (%)</cell><cell>MRR (%)</cell><cell></cell></row><row><cell>Method</cell><cell>Validation</cell><cell>Test</cell><cell>Validation</cell><cell>Test</cell></row><row><cell>GCN+DE (d max = 3)</cell><cell>36.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on subgraph-level readout. 51?0.42 64.40?0.45 81.07?0.30 81.27?0.31 GCN+DRNL (sum-pooling) 64.64?0.24 63.26?0.35 84.98?0.23 85.20?0.26 SEAL 64.95?0.43 64.74?0.43 87.57?0.31 87.67?0.32</figDesc><table><row><cell></cell><cell cols="2">ogbl-collab</cell><cell cols="2">ogbl-citation2</cell></row><row><cell></cell><cell cols="2">Hits@50 (%)</cell><cell>MRR (%)</cell><cell></cell></row><row><cell>Method</cell><cell>Validation</cell><cell>Test</cell><cell>Validation</cell><cell>Test</cell></row><row><cell>GCN+DRNL</cell><cell>64.</cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Ablation study on subgraph-level readout. ) 46.43?3.03 45.24?3.95 64.51?0.42 64.40?0.45 81.07?0.30 81.27?0.31 GCN+DRNL (l = 1) 31.59?2.79 33.57?3.06 64.38?0.13 63.95?0.42 77.77?0.42 78.02?0.44</figDesc><table><row><cell></cell><cell cols="2">ogbl-ppa</cell><cell cols="2">ogbl-collab</cell><cell cols="2">ogbl-citation2</cell></row><row><cell></cell><cell cols="2">Hits@100 (%)</cell><cell cols="2">Hits@50 (%)</cell><cell>MRR (%)</cell><cell></cell></row><row><cell>Method</cell><cell>Validation</cell><cell>Test</cell><cell>Validation</cell><cell>Test</cell><cell>Validation</cell><cell>Test</cell></row><row><cell>GCN+DRNL (l = 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Although a polynomial-time implementation is not known for node-most-expressive GNNs, many practical softwares can discriminate all non-isomorphic nodes quite efficiently<ref type="bibr" target="#b36">[37]</ref>, which provides a promising direction.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://ogb.stanford.edu/docs/leader_linkprop/ 5 https://github.com/muhanzhang/SEAL</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors greatly thank the actionable suggestions from the reviewers. Li is partly supported by the 2021 JP Morgan Faculty Award and the National Science Foundation (NSF) award HDR-2117997.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6203</idno>
		<title level="m">Spectral networks and locally connected networks on graphs</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dougal</forename><surname>David K Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafael</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Al?n</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan P</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2224" to="2232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Micha?l</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3837" to="3845" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Discriminative embeddings of latent variable models for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2702" to="2711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Petar</forename><surname>Veli?kovi?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<title level="m">Graph attention networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An end-to-end deep learning architecture for graph classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marion</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4438" to="4445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4800" to="4810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Friends and neighbors on the web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Adamic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social networks</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="230" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The netflix prize</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Lanning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of KDD cup and workshop</title>
		<meeting>KDD cup and workshop<address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page">35</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Evaluation of different biological data and computational classification methods for use in protein interaction prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanjun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziv</forename><surname>Bar-Joseph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><surname>Klein-Seetharaman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proteins: Structure, Function, and Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="490" to="500" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Drug response prediction as a link prediction problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Stanfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Co?kun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehmet</forename><surname>Koyut?rk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="13" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">A review of relational machine learning for knowledge graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Nickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volker</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.00759</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Neural higher-order pattern (motif) prediction in temporal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunyu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianzhu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06039</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Besta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raphael</forename><surname>Grob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cesare</forename><surname>Miglioli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicola</forename><surname>Bernold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Kwasniewski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Gjini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raghavendra</forename><surname>Kanakagiri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saleh</forename><surname>Ashkboos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukas</forename><surname>Gianinazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikoli</forename><surname>Dryden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.00761</idno>
		<title level="m">Motif prediction with graph neural networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emily</forename><surname>Alsentzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><forename type="middle">M</forename><surname>Finlayson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zitnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10538</idno>
		<title level="m">Subgraph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07308</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note type="report_type">Variational graph auto-encoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Link prediction based on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5165" to="5175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Distance encoding-design provably more powerful gnns for structural representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanbang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.00142</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The link-prediction problem for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Liben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Nowell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American society for information science and technology</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1019" to="1031" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Predicting missing links via local information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linyuan</forename><surname>L?</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Cheng</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The European Physical Journal B</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="623" to="630" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Loukas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03199</idno>
		<title level="m">What graph neural networks cannot learn: depth vs width</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryoma</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Makoto</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.03155</idno>
		<title level="m">Random features strengthen graph neural networks</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the equivalence between positional node embeddings and structural graph representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Balasubramaniam</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJxzFySKwH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.00687</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Inductive relation prediction by subgraph reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Komal</forename><surname>Teru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Etienne</forename><surname>Denis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="9448" to="9457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Inductive matrix completion based on graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yixin</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ByxxgCEYDS" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rami</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Probabilistic matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1257" to="1264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning node representations from structural identity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leonardo Fr Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Pedro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">R</forename><surname>Saverese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Figueiredo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A reduction of a graph to a canonical form and an algebra arising during this reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boris</forename><surname>Weisfeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lehman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="12" to="16" />
		</imprint>
	</monogr>
	<note type="report_type">Nauchno-Technicheskaya Informatsia</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Canonical labelling of graphs in linear average time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L?szl?</forename><surname>Babai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludik</forename><surname>Kucera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th Annual Symposium on Foundations of Computer Science (sfcs 1979)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1979" />
			<biblScope unit="page" from="39" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Practical graph isomorphism, ii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Brendan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adolfo</forename><surname>Mckay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Piperno</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Symbolic Computation</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="page" from="94" to="112" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Gomes-Selman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.10320</idno>
		<title level="m">Identity-aware graph neural networks</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Neural bellman-ford networks: A general graph neural network framework for link prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuobai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Louis-Pascal</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.06935</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Provably powerful graph networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hadar</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Serviansky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="2156" to="2167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">On the equivalence between graph isomorphism testing and function approximation with gnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengdao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soledad</forename><surname>Villar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="15894" to="15902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Position-aware graph neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.04817</idno>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Principled hyperedge prediction with structural spectral features and neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changlin</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Muhan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sha</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.04292</idno>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">From graph low-rank global attention to 2-fwl approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omri</forename><surname>Puny</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heli</forename><surname>Ben-Hamu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Lipman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.07846</idno>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><forename type="middle">Eric</forename><surname>Lenssen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.02428</idno>
		<title level="m">Fast graph representation learning with pytorch geometric</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
