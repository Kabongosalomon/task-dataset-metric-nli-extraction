<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lbl2Vec: An Embedding-Based Approach for Unsupervised Document Retrieval on Predefined Topics</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Schopf</surname></persName>
							<email>tim.schopf@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<addrLine>Boltzmannstrasse 3</addrLine>
									<settlement>Garching</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Braun</surname></persName>
							<email>daniel.braun@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<addrLine>Boltzmannstrasse 3</addrLine>
									<settlement>Garching</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Matthes</surname></persName>
							<email>matthes@tum.de</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Informatics</orgName>
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<addrLine>Boltzmannstrasse 3</addrLine>
									<settlement>Garching</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Lbl2Vec: An Embedding-Based Approach for Unsupervised Document Retrieval on Predefined Topics</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-11T10:44+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Natural Language Processing</term>
					<term>Document Retrieval</term>
					<term>Unsupervised Document Classification</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In this paper, we consider the task of retrieving documents with predefined topics from an unlabeled document dataset using an unsupervised approach. The proposed unsupervised approach requires only a small number of keywords describing the respective topics and no labeled document. Existing approaches either heavily relied on a large amount of additionally encoded world knowledge or on term-document frequencies. Contrariwise, we introduce a method that learns jointly embedded document and word vectors solely from the unlabeled document dataset in order to find documents that are semantically similar to the topics described by the keywords. The proposed method requires almost no text preprocessing but is simultaneously effective at retrieving relevant documents with high probability. When successively retrieving documents on different predefined topics from publicly available and commonly used datasets, we achieved an average area under the receiver operating characteristic curve value of 0.95 on one dataset and 0.92 on another. Further, our method can be used for multiclass document classification, without the need to assign labels to the dataset in advance. Compared with an unsupervised classification baseline, we increased F1 scores from 76.6 to 82.7 and from 61.0 to 75.1 on the respective datasets. For easy replication of our approach, we make the developed Lbl2Vec code publicly available as a ready-to-use tool under the 3-Clause BSD license. a a https://github.com/sebischair/Lbl2Vec</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In this paper, we combine the advantage of an unsupervised approach with the possibility to predefine topics. Precisely, given a large number of unlabeled documents, we would like to retrieve documents related to certain topics that we already know are present in the corpus. This is becoming a common task, considering not only the simplicity of retrieving documents by, e.g., scraping web pages, mails or other sources, but also the labeling cost. For illustration purposes, we imagine the following scenario: we possess a large number of news articles extracted from sports sections of different newspapers and would like to retrieve articles that are related to certain sports, such as hockey, soccer or basketball. Unfortunately, we can only rely on the article texts for this task, as the metadata of the articles contain no information about their content. Initially, this appears like a common text classification task. However, there arise two issues that make the use of conventional classification methods unsuitable. First, we would have to annotate our articles at a high cost, as conventional supervised text classification methods need a large amount of labeled training data <ref type="bibr" target="#b26">(Zhang et al., 2020)</ref>. Second, we might not be interested in any sports apart from the previously specified ones. However, our dataset of sports articles most likely also includes articles on other sports, such as swimming or running. If we want to apply a supervised classification method, we would either have to annotate even those articles that are of no interest to us or think about suitable previous cleaning steps, to remove unwanted articles from our dataset. Both options would require significant additional expense. In this paper, we present the Lbl2Vec approach, which provides the retrieval of documents on predefined topics from a large corpus based on unsupervised learning. This enables us to retrieve the wanted sports articles related to hockey, soccer and basketball only, without having to annotate any data. The proposed Lbl2Vec approach solely relies on semantic similarities between documents and keywords describing a certain topic. Using semantic meanings intuitively matches the approach of a human being and has previously been proven to be capable of categorizing unlabeled texts <ref type="bibr" target="#b6">(Chang et al., 2008)</ref>. With this approach, we significantly decrease the cost of annotating data, as we only need a small number of keywords instead of a large number of labeled documents. Lbl2Vec works by creating jointly embedded word, document, and label vectors. The label vectors are deducted from predefined keywords of each topic. Since label and document vectors are embedded in the same feature space, we can subsequently measure their semantic relationship by calculating their cosine similarity. Based on this semantic similarity, we can decide whether to assign a document to a certain topic or not. We show that our approach produces reliable results while saving annotation costs and requires almost no text preprocessing steps. To this end, we apply our approach to two publicly available and commonly used document classification datasets. Moreover, we make our Lbl2Vec code publicly available as a ready-to-use tool.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Most related research can be summarized under the notion of dataless classification, introduced by <ref type="bibr" target="#b6">Chang et al. (2008)</ref>. Broadly, this includes any approach that aims to classify unlabeled texts based on label descriptions only. Our approach differs slightly from these, as we primarily attempt to retrieve documents on predefined topics from an unlabeled document dataset without the need to consider documents belonging to different topics of no interest. Nevertheless, some similarities, such as the ability of multiclass document classification emerge, allowing a rough comparison of our approach with those from the dataless classification, which can further be divided along two dimensions: 1) semi-supervised vs. unsupervised approaches and 2) approaches that use a large amount of additional world knowledge vs. ones that mainly rely on the plain document corpus.</p><p>Semi-supervised approaches seek to annotate a small subset of the document corpus unsupervised and subsequently leverage the labeled subset to train a supervised classifier for the rest of the corpus. In one of the earliest approaches that fit into this category, <ref type="bibr" target="#b16">Ko and Seo (2000)</ref> derive training sentences from manually defined category keywords unsupervised. Then, they used the derived sentences to train a supervised Na?ve Bayes classifier with minor modifications. Similarly, <ref type="bibr" target="#b19">Liu et al. (2004)</ref> extracted a subset of documents with keywords and then applied a supervised Na?ve Bayes-based expectation-maximization algorithm <ref type="bibr" target="#b10">(Dempster et al., 1977)</ref> for classification.</p><p>Unsupervised approaches, by contrast, use similarity scores between documents and target categories to classify the entire unlabeled dataset. <ref type="bibr" target="#b15">Haj-Yahia et al. (2019)</ref> proposed keyword enrichment (KE) and subsequent unsupervised classification based on latent semantic analysis (LSA) <ref type="bibr" target="#b9">(Deerwester et al., 1990)</ref> vector cosine similarities. Another approach worth mentioning in this context is the pure dataless hierarchical classification used by <ref type="bibr" target="#b23">Song and Roth (2014)</ref> to evaluate different semantic representations. Our approach also fits into this unsupervised dimension, as we do not employ document labels and retrieve documents from the entire corpus based on cosine similarities only.</p><p>A large amount of additional world knowledge from different data sources has been widely exploited in many previous approaches to incorporate more context into the semantic relationship between documents and target categories. <ref type="bibr" target="#b6">Chang et al. (2008)</ref> used Wikipedia as source of world knowledge to compute explicit semantic analysis embeddings <ref type="bibr" target="#b13">(Gabrilovich and Markovitch, 2007)</ref> of labels and documents. Afterward, they applied the nearest neighbor classification to assign the most likely label to each document. In this regard, their early work had a major impact on further research, which subsequently heavily focused on adding a lot of world knowledge for dataless classification. <ref type="bibr" target="#b24">Yin et al. (2019)</ref> used various public entailment datasets to train a bidirectional encoder representations from transformers (BERT) model <ref type="bibr" target="#b11">(Devlin et al., 2019)</ref> and used the pretrained BERT entailment model to directly classify texts from different datasets.</p><p>Using mainly the plain document corpus for this task, however, has been rather less researched so far. In one of the earlier approaches, <ref type="bibr" target="#b21">Rao et al. (2006)</ref> derived and assigned document labels based on a k-means word clustering. Besides, <ref type="bibr" target="#b7">Chen et al. (2015)</ref> introduce descriptive latent Dirichlet allocation, which could perform classification with only category description words and unlabeled documents, thereby eradicating the need for a large amount of world knowledge from external sources. Since our approach only needs some predefined topic keywords besides the unlabeled document corpus, it also belongs to this category. However, unlike previous approaches that mainly used the plain document corpus, we do not rely on term-document frequency scores but learn new semantic embeddings from scratch, which was inspired by the topic modeling approach of <ref type="bibr" target="#b2">Angelov (2020)</ref>.</p><p>A different related research area addresses adhoc document retrieval. Approaches related to this area attempt to rank documents based on a relevance score to a specific user query (Baeza-Yates and <ref type="bibr" target="#b3">Ribeiro-Neto, 1999)</ref>. For instance, <ref type="bibr" target="#b14">Gysel et al. (2018)</ref> proposed a neural vector space model that learns document representations unsupervised, and <ref type="bibr" target="#b1">Ai et al. (2016)</ref> introduce a modified paragraph vector model for ad hoc document retrieval. However, our approach differs from these, as we do not want to receive documents based on user queries but topics. Further, we are not particularly interested in ranking within the retrieved documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LBL2VEC METHOD</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">General Approach</head><p>In the first step, our Lbl2Vec model learns jointly embedded word vectors W and document vectors D from an unlabeled document corpus. Afterward, we use the embeddings K ? W of manually defined keywords that describe topics T to learn label embeddings L within the same feature space. Since all learned embeddings (W, D, L) share the same feature space, their distance can be considered their semantic similarity. To learn a label embedding l i , we find document embeddings d i 1 , ..., d i m that are close to the descriptive keyword embeddings k i 1 , ..., k i n of topic t i . Afterward, we compute the centroid of the outlier cleaned document embeddings as the label embedding l i of topic t i . We compute document rather than keyword centroids since our experiments showed that it is more difficult to retrieve documents based on similarities to keywords only, even if they share the same feature space. Moreover, we clean outliers to remove documents that may be related to some of the descriptive keywords but do not properly match the intended topic. As a result, our experiments showed a more accurate label embedding and slightly improved document retrieval performance. <ref type="figure" target="#fig_0">Figure 1</ref> provides an exemplary illustration of the different learned embeddings. After learning, we can consider the distance of label embedding l i to an arbitrary document embedding d as their semantic similarity. Since we argue that the learned label embeddings are mappings of topics in the semantic feature space, this also represents the semantic similarity between t i and d. Hence, we use these semantic similarities to finally retrieve those documents related to our predefined topics. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning Jointly Embedded Semantic Representations</head><p>To train our jointly embedded word and document vectors, we use the paragraph vector framework introduced by <ref type="bibr" target="#b18">Le and Mikolov (2014)</ref>. Since the distributed bag of words version of paragraph vector (PV-DBOW) is proven to perform better than its alternative <ref type="bibr" target="#b17">(Lau and Baldwin, 2016)</ref>, we consequently use this architecture. However, PV-DBOW only trains document embeddings but not word embeddings in its original version. Therefore, we employ a slightly modified implementation that concurrently learns word embeddings and is first mentioned by <ref type="bibr" target="#b8">Dai et al. (2015)</ref>. In this modified version, we interleave the PV-DBOW training with Skip-gram <ref type="bibr" target="#b20">(Mikolov et al., 2013)</ref> word embedding training on the same corpus.</p><p>As the Skip-gram architecture is very similar to the PV-DBOW architecture, we simply need to exchange the predicting paragraph vector with a predicting word vector for this purpose. Then, iterative training on the interleaved PV-DBOW and Skip-gram architectures enable us to simultaneously learn word and document embedding that share the same feature space.</p><p>After learning all document and word embeddings, we use the topic keywords for label embedding training. For each topic of interest, we need to manually define at least one keyword that can describe the topic properly. Once all keywords are defined, we perform the following procedure for each topic of interest. By applying</p><formula xml:id="formula_0">e = 1 n n ? x=1 e x<label>(1)</label></formula><p>to calculate a centroid e of embeddings e 1 , ..., e n , we obtain the centroid k i of keyword embeddings for a topic t i . Afterward, we calculate the cosine similarity of k i to each d ? D and sort the document embeddings in descending order. Beginning at the document embedding with the highest cosine similarity, we now successively add each document embedding to a set of candidate document embeddings D c i ? D that has a high semantic similarity to the descriptive keywords of topic t i . To include only document embeddings with high cosine similarities in D c i , we additionally need to set values for the three following parameters.</p><formula xml:id="formula_1">? s : {s ? R| ? 1 ? s ? 1} as similarity threshold. Add only document embeddings to D c i succes- sively while cos (k i , d) &gt; s is true. ? d min : {d min ? N|1 ? d min ? d max ? |D|}</formula><p>as the minimum number of document embeddings that have to be added to D c i successively. This parameter prevents the selection of an insufficient number of documents in case we set s too restrictive.</p><formula xml:id="formula_2">? d max : {d max ? N|1 ? d min ? d max ? |D|}</formula><p>as the maximum number of document embeddings that may be added to D c i successively.</p><p>To ensure a more accurate label embedding later, we now clean outliers from the resulting set of candidate document embeddings D c i . Therefore, we apply local outlier factor (LOF) <ref type="bibr" target="#b5">(Breunig et al., 2000)</ref> cleaning. If the LOF algorithm identifies document embeddings d i outlier with significantly lower local density than that of their neighbors, we remove these document embeddings from D c i . Hence, we receive the set of relevant document embeddings D r i ? D c i for topic t i . Finally, we compute the centroid of all document embeddings in D r i and define this as our label embedding l i of topic t i . Consequently, we obtain jointly embedded semantic representations of words, documents, and topic labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Receiving Documents on Predefined Topics</head><p>To decide whether the content of document d is semantically similar to a single topic t i , we need to calculate the cosine similarity between document embedding d and label embedding l i . Subsequently, the affiliation of d to t i is indicated if cos ( l i , d) exceeds a previously manually defined threshold value ? t i : {? t i ? R| ? 1 ? ? t i ? 1}. Moreover, we can use the cosine similarities for classifying d between multiple different predefined topics t 1 , ...,t n .</p><p>To achieve this, we assign the label of topic t i to d if cos ( l i , d) = max({cos ( l x , d) : x = 1, ..., n}). Finally, we can also decide whether a document d does not fit into one of our predefined topics. Therefore, we define threshold values ? t 1 , ..., ? t n . In case that d is classified as most similar to topic t i , we discard the label assignment if cos ( l i , d) ? ? t i . As a result, d remains unlabeled, and we assume that the content of this document is unrelated to any of our predefined topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>We use the two publicly available classification datasets, 20Newsgroups 1 and AG's Corpus 2 , described in class as an independent topic and use the provided class labels solely for evaluation. The 20Newsgroups dataset consists of almost 20,000 documents heterogeneously split across 20 different newsgroup classes. The original AG's Corpus is a collection of over 1 million news articles. We use the version of <ref type="bibr" target="#b25">Zhang et al. (2015)</ref> that construct four evenly distributed classes from the original dataset, resulting in more than 120,000 labeled documents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Keywords Definition</head><p>To determine suitable keywords for each topic represented by a class, we adopt the expert knowledge approach of Haj-Yahia et al. <ref type="bibr">(2019)</ref>. Hence, we emulate human experts ourselves, that define some initial keywords based on the class descriptions only. Then, we randomly select some documents from each class to further derive some salient keywords. In the case of a strict unsupervised setting with completely unlabeled datasets, human experts might describe a topic with keywords based on their specific domain knowledge alone and without necessarily being familiar with the document contents.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Training</head><p>For model training, we need to convert all document words and topic keywords to lowercase. To finish our short preprocessing, we only have to tokenize the documents and assign IDs to them. For each dataset, we train an individual model. Accordingly, we pass the corresponding preprocessed documents and defined keywords to its own model. For our models to learn suitable embeddings, we need to set the hyperparameter values prior to training. Therefore, we conduct a short manual hyperparameter optimization by training Lbl2Vec models on the respective training datasets and evaluating the performance on the test datasets, which allows us to learn more precise embeddings while simultaneously avoiding overfitting. In the case of completely unlabeled datasets, the given standard hyperparameters can be used. The only significant hyperparameter setting difference between the two models, resulting from our hyperparameter optimization, is that we set a similarity threshold of s = 0.30 and s = 0.43 for the AG's Corpus and 20Newsgroups models, respectively. For both models, we choose d min = 100, d max = |D|, and 10 as the number of epochs for PV-DBOW training. As we use an unsupervised approach, we train our final models, similar to Haj-Yahia et al. <ref type="formula" target="#formula_0">(2019)</ref>, on the entire corpora of the respective aggregated training and test datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Topic Representation Analysis</head><p>We want to evaluate whether our Lbl2Vec approach is capable of adequately modeling predefined topics and thereby can return documents related to them. For that, we classify all documents in the AG's Corpus using our pretrained Lbl2Vec model. Afterward, we define the documents assigned to the same class by our model as one topic and analyze these topics using LDAvis <ref type="bibr" target="#b22">(Sievert and Shirley, 2014)</ref>. In addition, we compare the modeling capabilities on predefined topics of our Lbl2Vec approach to a common topic modeling approach. To this end, we apply latent Dirichlet allocation (LDA) <ref type="bibr" target="#b4">(Blei et al., 2003)</ref> with K = 4 number of topics to the same dataset and visualize the modeled topics. <ref type="figure">Figure 2</ref> shows that the LDA model finds two similar and two dissimilar topics. However, the topic sizes are distributed very heterogeneously, which contrasts with the uniform distribution of documents across all classes in the AG's Corpus. As opposed to this, our Lbl2Vec model finds topics that are equally sized, which is aligned with the underlying AG's Corpus. Further, the topics Science/Technology and Business are similar, whereas Sports and World are highly dissimilar to all other topics.   our Lbl2Vec model can capture the semantic meaning of each predefined topic very well. In addition, the occurrence of technology companies such as Microsoft and Apple in the Science/Technology topic explains the similarity to the Business topic, as such companies are also highly relevant in a business context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Multiclass Document Classification Results</head><p>When using our trained models to classify the entire document corpus of each dataset, we achieve the re-sults stated in <ref type="table">Table 4</ref>. We compared our models with a recent fully unsupervised text classification approach and a supervised baseline classifier. First, we observed  <ref type="table">Table 4</ref>: Performance of our Lbl2Vec models when classifying all documents in the respective corpus. KE + LSA refers to the best possible fully unsupervised classification results of <ref type="bibr" target="#b15">Haj-Yahia et al. (2019)</ref> on the datasets. The last row states their baseline classification results of a supervised multinomial Na?ve Bayes approach. As we used micro-averaging to calculate our classification metrics, we realized equal F1, Precision, and Recall scores within each model. that our Lbl2Vec models significantly outperformed the recent KE + LSA approach for each metric. This success indicated that using our jointly created embeddings for unsupervised classification is more suitable than using term-document frequencies on which LSA is heavily reliant. Moreover, the results showed that our Lbl2Vec approach allowed for unsupervised classification in case the labeling effort was estimated to be more expensive than the benefit of a more accurate classification. However, comparing our approach to the supervised baseline results, we observed that providing labels for each document is paramount if highly accurate classification results are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Document Retrieval Evaluation</head><p>One of the main features of our Lbl2Vec approach is retrieving related documents on a single or multiple predefined topics without actually having to consider any further topics contained in the dataset that may not be of interest. For both datasets, we see each class as an independent topic. Therefore, we can use our trained Lbl2Vec models to retrieve topic-related documents for each class independently. When adjusting the topic similarity thresholds ? t 1 , ..., ? t m for each topic t 1 , ...,t m in the respective datasets, we can observe the receiver operating characteristic (ROC) curves in <ref type="figure">Figures 3 and 4</ref>. By adjusting the topic similarity parameter ? to be closer to 1, we can reduce the false positive rate and retrieve proportionally more documents that are truly related to a topic. <ref type="figure">Figure 3</ref> shows that the topics, Business and Science/Technology, have the lowest area under the ROC curve (AUC) values of all topics within the AG's Corpus. Further, we know from <ref type="figure">Figure 2</ref> that these topics are similar. Hence, we infer that it is hard for our Lbl2Vec approach to distinguish between related topics. However, the better AUC values for the Sports and World topics in <ref type="figure">Figure  3</ref> and their distance to other topics in <ref type="figure">Figure 2</ref>  that our Lbl2Vec approach can create suitable topic representations given the absence of other similar topics in the dataset. The micro-average ROC curves of <ref type="figure">Figures 3 and 4</ref> indicate that, if we want to achieve a false positive rate of less than 1% on average, we retrieve ? 20% of documents that are truly relevant for a topic. Therefore, we argue that our Lbl2Vec approach can sample a small dataset with high precision from a large corpus of documents. This smaller dataset can then be used, for example, as a starting point for a subsequent semi-supervised classification approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Keywords Analysis</head><p>We are additionally interested in how the choice of keywords affects our Lbl2Vec results. Since the keywords also directly affect the predefined topics, this simultaneously involves the analysis of topic distributions. We conduct some hypothesis tests to address the question of what characterizes good keywords and topics. For all our tests, we use the defined keywords of each topic from the concatenation of the two datasets to compute correlation coefficients and determine a significance level of 0.05. We choose Kendall's ? as our correlation coefficient to measure monotonic relationships. It is robust against outliers and small datasets.</p><p>First, we test whether the trained Lbl2Vec model is subsequently better able to distinguish topic-related documents from unrelated ones the more topic-related keywords are used to describe a topic. This test assumes that more accurate descriptions of topics also require more topic-related keywords. Accordingly, we define our null hypothesis H  <ref type="table">Table   Correlation</ref> </p><formula xml:id="formula_3">coefficient p-value Kendall's ? = 0.19</formula><p>0.20 <ref type="table">Table 5</ref>: Correlation values that measure the relationship between X 1 = number of defined topic keywords and Y = AUC value of a topic. X 1 min = 10 and X 1 max = 44. 5 suggested a tendency toward a slightly positive correlation. However, the p-value exceeded our defined significance level. Therefore, our test results were statistically insignificant, hence we cannot reject H</p><p>(1) 0 . Consequently, we found no support for the assumption that Lbl2Vec can yield better topic models if we use more topic-related keywords, as there is insufficient evidence to infer a relationship between X 1 and Y .</p><p>Second, we asses whether using many similar keywords to describe a topic provides a better distinction from other topics than using many dissimilar keywords. As a result, we anticipate Lbl2Vec topic models are better at distinguishing topic-related documents from unrelated ones if we define mostly similar keywords for a single topic. To test this, we initially define the average intratopic similarity of keyword embeddings K i of a topic t i as follows:</p><formula xml:id="formula_4">?(i) = ? k ix , k iy ?K i k ix = k iy cos ( k i x , k i y ) |K i | ? (|K i | ? 1)<label>(2)</label></formula><p>Subsequently, we determine our null hypothesis H  <ref type="table">Table 6</ref>, we</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correlation coefficient p-value</head><p>Kendall's ? = 0.33 0.02 <ref type="table">Table 6</ref>: Correlation values that measure the relationship between X 2 = average intratopic similarity of topic keywords and Y = AUC value of a topic. X 2 min = 0.15 and X 2 max = 0.37.</p><p>rejected H</p><p>(2) 0 and from the correlation coefficient, we concluded a statistically significant medium positive correlation between X 2 and Y . From this evidence, we found support for our original assumption that using similar keywords to describe a topic yields better Lbl2Vec models.</p><p>The third test is based on our observation from Subsection 4.6, that Lbl2Vec models more accurate representations of topics dissimilar to all other topics within a dataset. We further investigate this aspect, by examining whether topic keywords highly dissimilar to all other topic keywords allow Lbl2Vec to model more precise topic representations. For this test, we define the average intertopic similarity of keyword embeddings K i of a topic t i as</p><formula xml:id="formula_5">?(i) = 1 (|T | ? 1) (|T |?1) ? n =i ? k ix ?K i k ny ?K n cos ( k i x , k n y ) |K i | ? |K n | .<label>(3)</label></formula><p>Afterward, we define our null hypothesis H  <ref type="table">Table 7</ref>, we concluded a moderate negative monotonic relationship between X 3 and Y . Moreover, from the p-value, we infer that</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Correlation coefficient p-value</head><p>Kendall's ? = -0.35 0.02 <ref type="table">Table 7</ref>: Correlation values that measure the relationship between X 3 = average intertopic similarity of topic keywords and Y = AUC value of a topic. X 3 min = 0.07 and X 3 max = 0.11. our third hypothesis test is statistically significant and we can reject H</p><p>(3) 0 . The defined topic keywords provide the foundation for the subsequent Lbl2Vec feature space embedding of a topic. The feature space location, in turn, determines the similarity of topics to each other. Accordingly, the dissimilarity of topic keywords transfers to the resulting Lbl2Vec topic representations and vice versa. Hence, in this statistically significant inter-topic keywords similarity test, we found further support for our earlier observation that topics dissimilar to all other topics may be modeled more precisely by Lbl2Vec. Consequently, to obtain a more precise topic representation by Lbl2Vec, we need to define topic keywords making them as dissimilar as possible to the keywords of other topics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>In this work, we introduced Lbl2Vec, an approach to retrieve documents from predefined topics unsupervised. It is based on jointly embedded word, document, and label vectors learned solely from an unlabeled document corpus. We showed that Lbl2Vec yields better fitting models of predefined topics than conventional topic modeling approaches, such as LDA. Further, we demonstrated that Lbl2Vec allowed for unsupervised document classification and could retrieve documents on predefined topics with high precision by adjusting the topic similarity parameter ?. Finally, we analyzed how to define keywords that yield good Lbl2Vec models and concluded that we need to aim for high intratopic similarities and high intertopic dissimilarities of keywords. Lbl2Vec facilitates the retrieval of documents on predefined topics from an unlabeled document corpus, avoiding costly labeling work. We made our Lbl2Vec code as well as the data publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ETHICAL CONSIDERATIONS</head><p>We provide our work in good faith and in accordance with the ACL Code of Ethics 3 . However, our approach depends heavily on the underlying data. Therefore, users should preprocess the targeted datasets according to the ethics' guidelines to prevent discrimination in the modeled topics. Further, our approach is heavily prone to bias introduced by the human expert defining the keywords and unprotected against intentional misuse, allowing malicious users to abuse the retrieved topics. Another concern, as with many models, is the environmental and financial costs incurred in the training process. Although such costs are naturally involved in our case, they are quite low compared with current state-of-the-art language models. Thus, our approach is comparably environmentally friendly and enables financially disadvantaged users to conduct further research.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Example illustration of a semantic feature space related to Basketball. Blue: Descriptive keyword embeddings. Black: Document embeddings that are semantically similar to the keywords and each other. Red: Outlier document embeddings. Green: Label embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Multi Topic ROC Curve ROC curve of "Sports" topic (area = 0.98) ROC curve of "World" topic (area = 0.94) ROC curve of "Business" topic (area = 0.87) ROC curve of "Science/Technology" topic (area = 0.88) micro-average ROC curve (area = 0.92) ROC curves of the Lbl2Vec model trained on the AG's Corpus. ROC curves of the Lbl2Vec model trained on the 20Newsgroups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>AUC values of topics modeled by Lbl2Vec are unrelated to the number of topic-related predefined keywords and our alternative hypothesis H (1) a as the AUC values of topics modeled by Lbl2Vec are positively related to the number of topic-related predefined keywords. At first glance, the correlation coefficient in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>values of topics modeled by Lbl2Vec are unrelated to the average intratopic similarity of topic keywords and our alternative hypothesis H (2) a as the AUC values of topics modeled by Lbl2Vec are positively related to the average intratopic similarity of topic keywords. Based on the p-value in</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>of topics modeled by Lbl2Vec are unrelated to the average intertopic similarity of topic keywords and our alternative hypothesis H (3) a as the AUC values of topics modeled by Lbl2Vec are negatively related to the average intertopic similarity of topic keywords. From</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .Table 1</head><label>11</label><figDesc>In the following, we consider each</figDesc><table><row><cell>Datasets</cell><cell>#Training documents</cell><cell>#Test documents</cell><cell>#Classes</cell></row><row><cell>20Newsgroups</cell><cell>11314</cell><cell>7532</cell><cell>20</cell></row><row><cell>AG's Corpus</cell><cell>120000</cell><cell>7600</cell><cell>4</cell></row></table><note>: Summary of the used classification datasets.</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc></figDesc><table><row><cell>in-</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Top 10 most relevant terms for each topic of the LDA model; we use the LDAvis relevance with ? = 0.1. low us to relate the modeled topics to the AG's Corpus classes. However, fromTable 3we can conclude that</figDesc><table><row><cell></cell><cell>Lbl2Vec</cell></row><row><cell>World</cell><cell>iraq; killed; minister; prime; military; palestinian; minister; israeli; troops; darfur;</cell></row><row><cell>Sports</cell><cell>cup; coach; sox; league; championship; yankees; champions; win; season; scored;</cell></row><row><cell>Business</cell><cell>stocks; fullquote; profit; prices; aspx; quickinfo; shares; earnings; investor; oil;</cell></row><row><cell>Science/</cell><cell>microsoft; windows; users; desktop; music;</cell></row><row><cell>Technology</cell><cell>linux; version; apple; search; browser;</cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Top 10 most relevant terms for each topic of the Lbl2Vec model; we use the LDAvis relevance with ? = 0.1.</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">qwone.com/ jason/20Newsgroups 2 groups.di.unipi.it/?gulli/AG corpus of news articles</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">https://www.aclweb.org/portal/content/acl-code-ethics</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGEMENTS</head><p>The authors would like to thank Thomas Kinkeldei of ROKIN for his contributions to this paper. This work has been supported by funds from the Bavarian Ministry of Economic Affairs, Regional Development and Energy as part of the program "Bayerischen Verbundf?rderprogramms (BayVFP) -F?rderlinie Digitalisierung -F?rderbereich Informations-und Kommunikationstechnik".</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">ROC curve of &quot;comp.sys.ibm.pc.hardware&quot; class (area = 0.94) ROC curve of &quot;comp.sys.mac.hardware&quot; class (area = 0.93) ROC curve of &quot;comp.windows.x&quot; class (area = 0.97) ROC curve of &quot;misc.forsale&quot; class (area = 0.95) ROC curve of &quot;rec.autos&quot; class (area = 0.95) ROC curve of &quot;rec.motorcycles&quot; class (area = 0.99) ROC curve of &quot;rec.sport.baseball&quot; class (area = 0.99) ROC curve of &quot;rec.sport.hockey&quot; class (area = 0.99) ROC curve of &quot;sci.crypt&quot; class (area = 0.99) ROC curve of &quot;sci.electronics&quot; class (area = 0.90) ROC curve of &quot;sci.med&quot; class (area = 0.95) ROC curve of &quot;sci.space&quot; class (area = 0.98) ROC curve of &quot;soc.religion.christian&quot; class (area = 0.98) ROC curve of &quot;talk.politics.guns&quot; class (area = 0.98) ROC curve of &quot;talk.politics.mideast&quot; class (area = 0.97) ROC curve of</title>
		<idno>alt.atheism&quot; class (area = 0.97) ROC curve of &quot;comp.graphics&quot; class (area = 0.91) ROC curve of &quot;comp.os.ms-windows.misc&quot; class (area = 0.90</idno>
		<imprint/>
	</monogr>
	<note>talk.politics.misc&quot; class (area = 0.93) ROC curve of &quot;talk.religion.misc&quot; class (area = 0.91</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Improving language estimation with the paragraph vector model for ad-hoc retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Croft</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;16</title>
		<meeting>the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;16<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="869" to="872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Angelov</surname></persName>
		</author>
		<title level="m">Top2vec: Distributed representations of topics</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Modern information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Baeza-Yates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ribeiro-Neto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999" />
			<publisher>ACM press New York</publisher>
			<biblScope unit="volume">463</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Lof: Identifying density-based local outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Breunig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2000 ACM SIGMOD international conference on Management of data</title>
		<meeting>the 2000 ACM SIGMOD international conference on Management of data<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2000" />
			<biblScope unit="page" from="93" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Importance of semantic representation: Dataless classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-A</forename><surname>Ratinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Srikumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Third AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="830" to="835" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dataless text classification with descriptive lda</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carroll</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<title level="m">Document embedding with paragraph vectors</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Dumais</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Furnas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Landauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Harshman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society for Information Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019</title>
		<meeting>the 2019</meeting>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting><address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computing semantic relatedness using wikipedia-based explicit semantic analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Joint Conference on Artifical Intelligence, IJCAI&apos;07</title>
		<meeting>the 20th International Joint Conference on Artifical Intelligence, IJCAI&apos;07<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1606" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural vector spaces for unsupervised information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Gysel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kanoulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Towards unsupervised text classification leveraging experts and word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Haj-Yahia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sieg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Deleris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="371" to="379" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic text categorization by unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference on Computational Linguistics</title>
		<meeting>the 18th Conference on Computational Linguistics<address><addrLine>USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="453" to="459" />
		</imprint>
	</monogr>
	<note>COLING &apos;00</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">An empirical evaluation of doc2vec with practical insights into document embedding generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Baldwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 1st Workshop on Representation Learning for NLP</title>
		<meeting>the 1st Workshop on Representation Learning for NLP<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="78" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributed representations of sentences and documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<editor>Xing, E. P. and Jebara, T.</editor>
		<meeting>the 31st International Conference on Machine Learning<address><addrLine>Bejing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="1188" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Text classification by labeling words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth National Conference on Artificial Intelligence, Sixteenth Conference on Innovative Applications of Artificial Intelligence</title>
		<editor>McGuinness, D. L. and Ferguson, G.</editor>
		<meeting>the Nineteenth National Conference on Artificial Intelligence, Sixteenth Conference on Innovative Applications of Artificial Intelligence<address><addrLine>San Jose, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press / The MIT Press</publisher>
			<date type="published" when="2004-07-25" />
			<biblScope unit="page" from="425" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Corpus based unsupervised labeling of documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Khemani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth International Florida Artificial Intelligence Research Society Conference</title>
		<editor>Sutcliffe, G. and Goebel, R.</editor>
		<meeting>the Nineteenth International Florida Artificial Intelligence Research Society Conference<address><addrLine>Melbourne Beach, Florida, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2006-05-11" />
			<biblScope unit="page" from="321" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">LDAvis: A method for visualizing and interpreting topics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sievert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Shirley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces</title>
		<meeting>the Workshop on Interactive Language Learning, Visualization, and Interfaces<address><addrLine>Baltimore, Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="63" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">On dataless hierarchical text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Eighth AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1579" to="1585" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Benchmarking zeroshot text classification: Datasets, evaluation and entailment approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roth</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="3914" to="3923" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Neural Information Processing Systems</title>
		<meeting>the 28th International Conference on Neural Information Processing Systems<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="649" to="657" />
		</imprint>
	</monogr>
	<note>NIPS&apos;15</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Minimally supervised categorization of text with metadata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1231" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
