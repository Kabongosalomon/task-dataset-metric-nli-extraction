<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 /nfs/home/kabenamualus/Research/task-dataset-metric-extraction/../grobid-0.6.0/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FEW-NERD: A Few-shot Named Entity Recognition Dataset</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yulin</forename><surname>Chen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Alibaba Group</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
							<email>zheng.haitao@sz.tsinghua.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Shenzhen International Graduate School</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">?</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FEW-NERD: A Few-shot Named Entity Recognition Dataset</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>

		<encodingDesc>
			<appInfo>
				<application version="0.6.0" ident="GROBID-SDO" when="2022-11-12T17:46+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid-sdo"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, considerable literature has grown up around the theme of few-shot named entity recognition (NER), but little published benchmark data specifically focused on the practical and challenging task. Current approaches collect existing supervised NER datasets and reorganize them into the few-shot setting for empirical study. These strategies conventionally aim to recognize coarse-grained entity types with few examples, while in practice, most unseen entity types are fine-grained. In this paper, we present FEW-NERD, a large-scale human-annotated few-shot NER dataset with a hierarchy of 8 coarse-grained and 66 finegrained entity types. FEW-NERD consists of 188,238 sentences from Wikipedia, 4,601,160 words are included and each is annotated as context or a part of a two-level entity type. To the best of our knowledge, this is the first few-shot NER dataset and the largest humancrafted NER dataset. We construct benchmark tasks with different emphases to comprehensively assess the generalization capability of models. Extensive empirical results and analysis show that FEW-NERD is challenging and the problem requires further research. We make FEW-NERD public at https:// ningding97.github.io/fewnerd/. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Named entity recognition (NER), as a fundamental task in information extraction, aims to locate and classify named entities from unstructured natural language. A considerable number of approaches equipped with deep neural networks have shown promising performance (Chiu and Nichols, 2016) on fully supervised NER. Notably, pre-trained language models (e.g., BERT <ref type="bibr" target="#b6">(Devlin et al., 2019a)</ref>   with an additional classifier achieve significant success on this task and gradually become the base paradigm. Such studies demonstrate that deep models could yield remarkable results accompanied by a large amount of annotated corpora.</p><p>With the emerging of knowledge from various domains, named entities, especially ones that need professional knowledge to understand, are difficult to be manually annotated on a large scale. Under this circumstance, studying NER systems that could learn unseen entity types with few examples, i.e., few-shot NER, plays a critical role in this area. There is a growing body of literature that recognizes the importance of few-shot NER and contributes to the task <ref type="bibr" target="#b14">(Hofer et al., 2018;</ref><ref type="bibr" target="#b11">Fritzler et al., 2019;</ref><ref type="bibr" target="#b35">Yang and Katiyar, 2020;</ref><ref type="bibr" target="#b19">Li et al., 2020a;</ref><ref type="bibr" target="#b16">Huang et al., 2020)</ref>. Unfortunately, there is still no dataset specifically designed for few-shot NER. Hence, these methods collect previously proposed supervised NER datasets and reorganize them into a few-shot setting. Common options of datasets include OntoNotes <ref type="bibr" target="#b33">(Weischedel et al., 2013)</ref>, <ref type="bibr">CoNLL'03 (Tjong Kim Sang, 2002)</ref>, WNUT'17 <ref type="bibr" target="#b5">(Derczynski et al., 2017)</ref>, etc. These research efforts of few-shot learning for named entities mainly face two challenges: First, most datasets used for few-shot learning have only 4-18 coarse-grained entity types, making it hard to construct an adequate variety of "N-way" metatasks and learn correlation features. And in reality, we observe that most unseen entities are finegrained. Second, because of the lack of benchmark datasets, the settings of different works are inconsistent <ref type="bibr" target="#b16">(Huang et al., 2020;</ref><ref type="bibr" target="#b35">Yang and Katiyar, 2020)</ref>, leading to unclear comparisons. To sum up, these methods make promising contributions to few-shot NER, nevertheless, a specific dataset is urgently needed to provide a unified benchmark dataset for rigorous comparisons.</p><p>To alleviate the above challenges, we present a large-scale human-annotated few-shot NER dataset, FEW-NERD, which consists of 188.2k sentences extracted from the Wikipedia articles and 491.7k entities are manually annotated by well-trained annotators (Section 4.3). To the best of our knowledge, FEW-NERD is the first dataset specially constructed for few-shot NER and also one of the largest human-annotated NER dataset (statistics in Section 5.1). We carefully design an annotation schema of 8 coarse-grained entity types and 66 fine-grained entity types by conducting several pre-annotation rounds. (Section 4.1). In contrast, as the most widely-used NER datasets, CoNLL has 4 entity types, WNUT'17 has 6 entity types and OntoNotes has 18 entity types (7 of them are value types). The variety of entity types makes FEW-NERD contain rich contextual features with a finer granularity for better evaluation of fewshot NER. The distribution of the entity types in FEW-NERD is shown in <ref type="figure" target="#fig_1">Figure 1</ref>, more details are reported in Section 5.1. We conduct an analysis of the mutual similarities among all the entity types of FEW-NERD to study knowledge transfer (Section 5.2). The results show that our dataset can provide sufficient correlation information between different entity types for few-shot learning.</p><p>For benchmark settings, we design three tasks on the basis of FEW-NERD, including a standard supervised task (FEW-NERD (SUP)) and two few-shot tasks (FEW-NERD-INTRA) and FEW-NRTD (INTER)), for more details see Section 6. FEW-NERD (SUP), FEW-NERD (INTRA), and FEW-NERD (INTER) assess instance-level generalization, type-level generalization and knowledge transfer of NER methods, respectively. We implement models based on the recent state-of-theart approaches and evaluate them on FEW-NERD (Section 7). And empirical results show that FEW-NERD is challenging on all these three settings. We also conduct sets of subsidiary experiments to analyze promising directions of few-shot NER. Hopefully, the research of few-shot NER could be further facilitated by FEW-NERD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>As a pivotal task of information extraction, NER is essential for a wide range of technologies <ref type="bibr" target="#b4">(Cui et al., 2017;</ref><ref type="bibr" target="#b21">Li et al., 2019b;</ref><ref type="bibr" target="#b8">Ding et al., 2019;</ref><ref type="bibr" target="#b28">Shen et al., 2020)</ref>. And a considerable number of NER datasets have been proposed over the years. For example, CoNLL'03 (Tjong Kim <ref type="bibr" target="#b31">Sang, 2002</ref>) is regarded as one of the most popular datasets, which is curated from Reuters News and includes 4 coarsegrained entity types. Subsequently, a series of NER datasets from various domains are proposed <ref type="bibr" target="#b0">(Balasuriya et al., 2009;</ref><ref type="bibr" target="#b27">Ritter et al., 2011;</ref><ref type="bibr" target="#b33">Weischedel et al., 2013;</ref><ref type="bibr" target="#b30">Stubbs and Uzuner, 2015;</ref><ref type="bibr" target="#b5">Derczynski et al., 2017)</ref>. These datasets formulate a sequence labeling task and most of them contain 4-18 entity types. Among them, due to the high quality and size, OntoNotes 5.0 <ref type="bibr" target="#b33">(Weischedel et al., 2013)</ref> is considered as one of the most widely used NER datasets recently.</p><p>As approaches equipped with deep neural networks have shown satisfactory performance on NER with sufficient supervision <ref type="bibr" target="#b17">(Lample et al., 2016;</ref><ref type="bibr" target="#b24">Ma and Hovy, 2016)</ref>, few-shot NER has received increasing attention <ref type="bibr" target="#b14">(Hofer et al., 2018;</ref><ref type="bibr" target="#b11">Fritzler et al., 2019;</ref><ref type="bibr" target="#b35">Yang and Katiyar, 2020;</ref><ref type="bibr" target="#b19">Li et al., 2020a)</ref>. Few-shot NER is a considerably challenging and practical problem that could facilitate the understanding of textual knowledge for neural model <ref type="bibr" target="#b16">(Huang et al., 2020)</ref>. Due to the lack of specific benchmarks of few-shot NER, current methods collect existing NER datasets and use different few-shot settings. To provide a benchmark that could comprehensively assess the generalization of models under few examples, we annotate FEW-NERD. To make the dataset practical and close to reality, we adopt a fine-grained schema of entity annotation, which is inspired and modified from previous fine-grained entity recognition studies <ref type="bibr" target="#b22">(Ling and Weld, 2012;</ref><ref type="bibr" target="#b12">Gillick et al., 2014;</ref><ref type="bibr" target="#b2">Choi et al., 2018;</ref><ref type="bibr" target="#b26">Ringland et al., 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Formulation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Named Entity Recognition</head><p>NER is normally formulated as a sequence labeling problem. Specifically, for an input sequence of tokens x = {x 1 , x 2 , ..., x t }, NER aims to assign each token x i a label y i ? Y to indicate either the token is a part of a named entity (such as Person, Organization, Location) or not belong to any entities (denoted as O class), Y being a set of pre-defined entity-types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Few-shot Named Entity Recognition</head><p>N -way K-shot learning is conducted by iteratively constructing episodes. For each episode in training, N classes (N -way) and K examples (K-shot) for each class are sampled to build a support set</p><formula xml:id="formula_0">S train = {x (i) , y (i) } N * K i=1 ,</formula><p>and K examples for each of N classes are sampled to construct a query set Q train = {x (j) , y (j) } N * K j=1 , and S Q = ?. Few-shot learning systems are trained by predicting labels of query set Q train with the information of support set S train . The supervision of S train and Q train are available in training. In the testing procedure, all the classes are unseen in the training phase, and by using few labeled examples of support set S test , few-shot learning systems need to make predictions of the unlabeled query set Q test (S Q = ?). However, in the sequence labeling problem like NER, a sentence may contain multiple entities from different classes. And it is imperative to sample examples in sentence-level since contextual information is crucial for sequence labeling problems, especially for NER. Thus the sampling is more difficult than conventional classification tasks like relation extraction <ref type="bibr" target="#b13">(Han et al., 2018)</ref>.</p><p>Some previous works <ref type="bibr" target="#b35">(Yang and Katiyar, 2020;</ref><ref type="bibr" target="#b19">Li et al., 2020a)</ref> use greedy-based sampling strategies to iteratively judge if a sentence could be added into the support set, but the limitation becomes gradually strict during the sampling. For example, when it comes to a 5-way 5-shot setting, if the support set already had 4 classes with 5 examples and 1 class with 4 examples, the next sampled sentence must only contain the specific one entity to strictly meet the requirement of 5 way 5 shot. It is not suitable for FEW-NERD since it is annotated with dense entities. Thus, as shown in Algorithm 1 we adopt a N -way K?2K-shot setting in our paper, the primary principle of which is to ensure that each class in S contain K?2K examples, effectively alleviating the limitations of sampling. 4 Collection of FEW-NERD</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Schema of Entity Types</head><p>The primary goal of FEW-NERD is to construct a fine-grained dataset that could specifically be used in the few-shot NER scenario. Hence, schemas of traditional NER datasets such as CoNLL'03, OntoNotes that only contain 4-18 coarse-grained types could not meet the requirements. The schema of FEW-NERD is inspired by FIGER <ref type="bibr" target="#b22">(Ling and Weld, 2012)</ref>, which contains 112 entity tags with good coverage. On this basis, we make some modifications according to the practical situation. It is worth noting that FEW-NERD focuses on named entities, omitting value/numerical/time/date entity types <ref type="bibr" target="#b33">(Weischedel et al., 2013;</ref><ref type="bibr" target="#b26">Ringland et al., 2019)</ref> like Cardinal, Day, Percent, etc. First, we modify the FIGER schema into a two-level hierarchy to incorporate simple domain information <ref type="bibr" target="#b12">(Gillick et al., 2014)</ref>. The coarse-grained types are {Person, Location, Organization, Art, Building, Product, Event, Miscellaneous }. Then we statistically count the frequency of entity types in the automatically annotated FIGER. By removing entity types with low frequency, there are 80 finegrained types remaining. Finally, to ensure the practicality of the annotation process, we conduct rounds of pre-annotation and make further modifications to the schema. For example, we combine the types of Country, Province/State, City, Restrict into a class GPE, since it is difficult to distinguish these types only based on context (especially GPEs at different times). For another example, we create a Person-Scholar type, because in the pre-annotation step, we found that there are numerous person entities that express the semantics of research, such as mathematician, physicist, chemist, biologist, paleontologist, but the Figer schema does not define this kind of entity type. We also conduct rounds of manual denoising to select types with truly high frequency.</p><p>Consequently, the finalized schema of FEW-NERD includes 8 coarse-grained types and 66 fine-grained types, which is detailedly shown accompanied by selected examples in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Paragraph Selection</head><p>The raw corpus we use is the entire Wikipedia dump in English, which has been widely used in constructions of NLP datasets <ref type="bibr" target="#b13">(Han et al., 2018;</ref><ref type="bibr" target="#b36">Yang et al., 2018;</ref><ref type="bibr" target="#b32">Wang et al., 2020)</ref>. Wikipedia contains a large variety of entities and rich contextual information for each entity.</p><p>FEW-NERD is annotated in paragraph-level, and it is crucial to effectively select paragraphs with sufficient entity information. Moreover, the category distribution of the data is expected to be balanced since the data is applied in a fewshot scenario. It is also a key difference between FEW-NERD and previous NER datasets, whose entity distributions are usually considerably uneven. In order to do so, we construct a dictionary for each fine-grained type by automatically collecting entity mentions annotated in FIGER, then the dictionaries are manually denoised. We develop a search engine to retrieve paragraphs including entity mentions of the distant dictionary. For each entity, we choose 10 paragraphs and construct a candidate set. Then, for each fine-grained class, we randomly select 1000 paragraphs for manual annotation. Eventually, 66,000 paragraphs are selected, consisting of 66 fine-grained entity types, and each paragraph contains an average of 61.3 tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Paragraph</head><p>London <ref type="bibr">[Art-Music]</ref> is the fifth album by the British <ref type="bibr">[Loc-GPE]</ref> rock band Jesus Jones <ref type="bibr">[Org-ShowOrg]</ref> in 2001 through Koch Records <ref type="bibr">[Org-Company]</ref> . Following the commercial failure of 1997's "Already <ref type="bibr">[Art-Music]</ref> " which led to the band and EMI <ref type="bibr">[Org-Company]</ref> parting ways, the band took a hiatus before regathering for the recording of "London [Art-Music] " for Koch/Mi5 Recordings, with a more alternative rock approach as opposed to the techno sounds on their previous albums. The album had low-key promotion, initially only being released in the United States <ref type="bibr">[Loc-GPE]</ref> . Two EP's were released from the album, "Nowhere Slow <ref type="bibr">[Art-Music]</ref> " and "In the Face Of All This [Art-Music] ". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Human Annotation</head><p>As named entities are expected to be contextdependent, annotation of named entities is complicated, especially with such a large number of entity types. For example, shown in <ref type="table" target="#tab_2">Table 1</ref>, "London is the fifth album by the British rock band Jesus Jones..", where London should be annotated as an entity of Art-Music rather than Location-GPE. Such a situation requires that the annotator has basic linguistic training and can make reasonable judgments based on the context.</p><p>Annotators of FEW-NERD include 70 annotators and 10 experienced experts. All the annotators have linguistic knowledge and are instructed with detailed and formal annotation principles. Each paragraph is independently annotated by two welltrained annotators. Then, an experienced expert goes over the paragraph for possible wrong or omissive annotations, and make the final decision. With 70 annotators participated, each annotator spends an average of 32 hours during the annotation process. We ensure that all the annotators are fairly compensated by market price according to their workload (the number of examples per hour). The data is annotated and submitted in batches, and each batch contains 1000?3000 sentences. To ensure the quality of FEW-NERD, for each batch of data, we randomly select 10% sentences and conduct double-checking. If the accuracy of the annotation is lower than 95 % (measured in sentencelevel), the batch will be re-annotated. Furthermore, we calculate the Cohen's Kappa <ref type="bibr" target="#b3">(Cohen, 1960)</ref> to measure the aggreements between two annotators, the result is 76.44%, which indicates a high degree of consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Data Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Size and Distribution of FEW-NERD</head><p>FEW-NERD is not only the first few-shot dataset for NER, but it also is one of the biggest humanannotated NER datasets. We report the the statistics of the number of sentences, tokens, entity types and entities of FEW-NERD and several widely-used NER datasets in <ref type="table" target="#tab_6">Table 2</ref>, including CoNLL'03, WikiGold, OntoNotes 5.0, WNUT'17 and I2B2. We observe that although OntoNotes and I2B2 are considered as large-scale datasets, FEW-NERD is significantly larger than all these datasets. Moreover, FEW-NERD contains more entity types and annotated entities. As introduced in Section 4.2, FEW-NERD is designed for few-shot learning and the distribution could not be severely uneven. Hence, we balance the dataset by selecting paragraphs through a distant dictionary. The data distribution is illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>, where Location (especially GPE) and Person are entity types with the most examples. Although utilizing a distant dictionary to balance the entity types could not produce a fully balanced data distribution, it still ensures that each fine-grained type has a sufficient number of examples for few-shot learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Knowledge Correlations among Types</head><p>Knowledge transfer is crucial for few-shot learning <ref type="bibr" target="#b18">(Li et al., 2019a)</ref>. To explore the knowledge correlations among all the entity types of FEW-NERD, we conduct an empirical study about entity type similarities in this section. We train a BERT-Tagger (details in Section 7.1) of 70% arbitrarily selected data on FEW-NERD and use 10% data to select the model with best performance (it is actually the setting of FEW-NERD (SUP) in Section 6.1). After obtaining a contextualized encoder, we produce entity mention representations of the remaining 20% data of FEW-NERD. Then, for each fine-grained types, we randomly select 100 instances of entity embeddings. We mutually compute the dot product among entity embeddings for each type two by two and average them to obtain the similarities among types, which is illustrated in <ref type="figure" target="#fig_2">Figure 2</ref>. We observe that entity types shared identical coarse-grained types typically have larger similarities, resulting in an easier knowledge transfer. In contrast, although some of the fine-grained types have large similari-  ties, most of them across coarse-grained types share little correlations due to distinct contextual features. This result is consistent with intuition. Moreover, it inspires our benchmark-setting from the perspective of knowledge transfer (see Section 6.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Benchmark Settings</head><p>We collect and manually annotate 188,238 sentences with 66 fine-grained entity types in total, which makes FEW-NERD one of the largest human-annotated NER datasets. To comprehensively exploit such rich information of entities and contexts, as well as evaluate the generalization of models from different perspectives, we construct three tasks based on FEW-NERD (Statistics are reported in <ref type="table" target="#tab_7">Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Standard Supervised NER</head><p>FEW-NERD (SUP) We first adopt a standard supervised setting for NER by randomly splitting 70% data as the training data, 10% as the validation data and 20% as the testing data. In this setting, the training set, dev set, and test set contain the whole 66 entity types. Although the supervised setting is not the ultimate goal of the construction of FEW-NERD, it is still meaningful to assess the instance-level generalization for NER models. As shown in Section 6.2, due to the large number of entity types, FEW-NERD is very challenging even in a standard supervised setting.  <ref type="bibr" target="#b33">(Weischedel et al., 2013)</ref> 103.8k 2067k 161.8k 18 General WNUT'17 <ref type="bibr" target="#b5">(Derczynski et al., 2017)</ref> 4.7k 86.1k 3.1k 6 SocialMedia I2B2 <ref type="bibr" target="#b30">(Stubbs and Uzuner, 2015)</ref> 107   this setting is to explore if the coarse information will affect the prediction of new entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Models</head><p>Recent studies show that pre-trained language models with deep transformers (e.g., BERT <ref type="bibr" target="#b6">(Devlin et al., 2019a)</ref>) have become a strong encoder for NER <ref type="bibr" target="#b20">(Li et al., 2020b)</ref>. We thus follow the empirical settings and use BERT as the backbone encoder in our experiments. We denote the parameters as ? and the encoder as f ? . Given a sequence x = {x 1 , ..., x n }, for each token x i , the encoder produces contextualized representations as:</p><formula xml:id="formula_1">h = [h 1 , ..., h n ] = f ? ([x 1 , ..., x n ]).<label>(1)</label></formula><p>Specifically, we implement four BERT-based models for supervised and few-shot NER, which are BERT-Tagger <ref type="bibr" target="#b7">(Devlin et al., 2019b)</ref>, Proto-BERT <ref type="bibr" target="#b29">(Snell et al., 2017)</ref>, NNShot <ref type="bibr" target="#b35">(Yang and Katiyar, 2020)</ref> and StructShot <ref type="bibr" target="#b35">(Yang and Katiyar, 2020)</ref>.</p><p>BERT-Tagger As stated in Section 6.1, we construct a standard supervised task based on FEW-NERD, thus we implement a simple but strong baseline BERT-Tagger for supervised NER. BERT-Tagger is built by adding a linear classifier on top of BERT and trained with a cross-entropy objective under a full supervision setting.</p><p>ProtoBERT Inspired by achievements of metalearning approaches <ref type="bibr" target="#b10">(Finn et al., 2017;</ref><ref type="bibr" target="#b29">Snell et al., 2017;</ref><ref type="bibr" target="#b9">Ding et al., 2021)</ref> on few-shot learning. The first baseline model we implement is Proto-BERT, which is a method based on prototypical network <ref type="bibr" target="#b29">(Snell et al., 2017)</ref> with a backbone of BERT <ref type="bibr" target="#b6">(Devlin et al., 2019a)</ref> encoder. This approach derives a prototype z for each entity type by computing the average of the embeddings of the tokens that share the same entity type. The computation is conducted in support set S. For the i-th type, the prototype is denoted as z i and the support set is S i ,</p><formula xml:id="formula_2">z i = 1 |S i | x?S i f ? (x).</formula><p>(2)</p><p>While in the query set Q, for each token x ? Q, we firstly compute the distance between x and all the prototypes. We use the l-2 distance as the metric function d(f ? (x), z) = ||f ? (x) ? z|| 2 2 . Then, through the distances between x and all other prototypes, we compute the prediction probability of x over all types. In the training step, parameters are updated in each meta-task. In the testing step, the prediction is the label of the nearest prototype to x. That is, for a support set S Y with types of Y and a query x, the prediction process is given as</p><formula xml:id="formula_3">y * = arg min y?Y d y (x), d y (x) = d(f ? (x), z y ).</formula><p>(3)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NNShot &amp; StructShot</head><p>NNShot and Struct-Shot <ref type="bibr" target="#b35">(Yang and Katiyar, 2020)</ref> are the state-of-theart methods based on token-level nearest neighbor classification. In our experiments, we use BERT as the backbone encoder to produce contextualized representations for fair comparison. Different from the prototype-based method, NNShot determines the tag of one query based on the token-level distance, which is computed as</p><formula xml:id="formula_4">d(f ? (x), f ? (x )) = ||f ? (x) ? f ? (x )|| 2 2 .</formula><p>Hence, for a support set S Y with type of Y and a query x,</p><formula xml:id="formula_5">y * = arg min y?Y d y (x), d y (x) = min x ?Sy d(f ? (x), f ? (x )).<label>(4)</label></formula><p>With the identical basic structure as NNShot, StructShot adopts an additional Viterbi decoder during the inference phase <ref type="bibr" target="#b15">(Hou et al., 2020)</ref> (not in training phase), where we estimate a transition distribution p(y |y) and an emission distribution  p(y|x) and solve the problem:</p><formula xml:id="formula_6">y * = arg max y T t=1 p(y t |x) ? p(y t |y t?1 ). (5)</formula><p>To sum up, BERT-Tagger is a wellacknowledged baseline that could produce pronounced results on supervised NER. Proto-BERT, and NNShot &amp; StructShot respectively use prototype-level and token-level similarity scores to tackle the few-shot NER problem. These baselines are strong and representative models of the NER task. For implementation details, please refer to Appendix.</p><p>We evaluate models by considering query sets Q test of test episodes. We calculate the precision (P), recall (R) and micro F1-score over all test episodes. Instead of the popular BIO schema, we utilize the IO schema in our experiments, using I-type to denote all the tokens of a named entity and O to denote other tokens.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">The Overall Results</head><p>We evaluate all baseline models on the three benchmark settings introduced in Section 6, including FEW-NERD (SUP), FEW-NERD (INTRA) and FEW-NERD (INTER). Supervised NER As mentioned in Section 6.1, we first split the FEW-NERD as a standard supervised NER dataset. As shown in <ref type="table" target="#tab_9">Table 4</ref>, BERT-Tagger yields promising results on the two widely used supervised datasets. The F1-score is 91.34%, 89.11%, respectively. However, the model suffers a grave drop in the performance on FEW-NERD (SUP) because the number of types of FEW-NERD (SUP) is larger than others. The results indicate that FEW-NERD is challenging in the supervised setting and worth studying.</p><p>We further analyze the performance of different entity types (see <ref type="figure" target="#fig_3">Figure 3</ref>). We find that the model achieves the best performance on the Person type and yields the worst performance on the Product type. And almost for all the coarse-grained types, the Coarse-Other type has the lowest F1-score.     This is because the semantics of such fine-grained types are relatively sparse and difficult to be recognized. A natural intuition is that the performance of each entity type is related to the portion of the type.</p><p>But surprisingly, we find that they are not linearly correlated. For examples, the model performs very well on the Art type, although this type represents only a small fraction of FEW-NERD. Few-shot NER For the few-shot benchmarks, we adopt 4 sampling settings, which are 5 way 1?2 shot, 5 way 5?10 shot, 10 way 1?2 shot, and 10 way 5?10 shot. Intuitively, 10 way 1?2 shot is the hardest setting because it has the largest number of entity types and the fewest number of examples, and similarly, 5 way 5?10 shot is the easiest setting. All results of FEW-NERD (INTRA) and FEW-NERD (INTER) are reported in <ref type="table" target="#tab_12">Table 5</ref> and <ref type="table" target="#tab_13">Table 6</ref> respectively. Overall, we observe that the previous state-of-the-art methods equipped by BERT encoder could not yield promising results on FEW-NERD. From a perspective of high level, models generally perform better on FEW-NERD (INTER) than FEW-NERD (INTRA), and the latter is regarded as a more difficult task as we analyze in Section 5.2 and Section 6, it splits the data according to the coarse-grained entity types, which means entity types between the training set and test set share less knowledge. In a horizontal comparison, consistent with intuition, almost all the methods produce the worst results on 10 way 1?2 shot and achieve the best performance on 5 way 5?10. In the comparison across models, ProtoBERT generally achieves better performance than NNShot and StructShot, especially in 5?10 shot setting where calculation by prototype may differ more from calculation by entity. StructShot has seen a large improvement in precision in FEW-NERD (INTRA). It shows that Viterbi decoder at the inference stage can help remove false positive predictions when knowledge transfer is hard. It is also observed that NNShot and StructShot may suffer from the instability of the nearest neighbor mechanism in the training phase, and prototypical models are more stable because  <ref type="table">Table 7</ref>: Error analysis of 5 way 5?10 shot on FEW-NERD (INTER), "Within" indicates "within the coarse types" and "Outer" is "outer the coarse types". the calculation of prototypes essentially serves as regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Error Analysis</head><p>We conduct error analysis to explore the challenges of FEW-NERD, the results are reported in <ref type="table">Table 7</ref>. We choose the setting of FEW-NERD (INTER) because the test set contains all the coarse-grained types. We analyze the errors of models from two perspectives. Span Error denotes the misclassifying in token-level classification. If an O token is misclassified as a part of entity, i.e., I-type, it is an FP case, and if a token with the type I-type is misclassified to O, it is FN. Type Error indicates the misclassification of entity types when the spans are correctly classified. A "Within" error represents the entity is misclassified to another type within the same coarse-grained type, while "Outer" denotes the entity is misclassified to another type in a different coarse-grained type. As the statistics of type errors may be impacted by the sampled episodes in testing, we conduct 5 rounds of experiments and report the average results. The results demonstrate that the token-level accuracy is not that low since most O tokens could be detected. But an entity mention is considered to be wrong if one token is wrong, which becomes the main reason for the challenge of FEW-NERD. If an entity span could be accurately detected, the models could yield relatively good performance on entity typing, indicating the effectiveness of metric learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and Future Work</head><p>We propose FEW-NERD, a large-scale few-shot NER dataset with fine-grained entity types. This is the first few-shot NER dataset and also one of the largest human-annotated NER dataset. FEW-NERD provides three unified benchmarks to assess approaches of few-shot NER and could facilitate future research in this area. By implementing state-of-the-art methods, we carry out a series of experiments on FEW-NERD, demonstrating that few-shot NER remains a challenging problem and worth exploring. In the future, we will extend FEW-NERD by adding cross-domain annotations, distant annotations, and finer-grained entity types. FEW-NERD also has the potential to advance the construction of continual knowledge graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Data Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Processing</head><p>We use the dump 2 of English Wikipedia, and extract the raw text by WikiExtractor 3 . NLTK language tool 4 is used for word and sentence tokenization in the preprocessing stage. As stated in Section 4.2, we develope a search engine to index and select paragraphs with key words in distant dictionaries. If the search is performed with linear operations, the calculation process will be extremely slow, instead, we adopt a search engine with Lucene 5 to conduct effective indexing and searching.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 More Details of the Schema</head><p>As stated in Section 4.1, we use FIGER <ref type="bibr" target="#b22">(Ling and Weld, 2012)</ref> as the start point and conduct rounds of make a series of modifications. Despite the modifications mentioned in Section 4.1, we also conduct manual denoising of the automatically annotated data of FIER. For each entity type and the corresponding automatically annotated mentions, we randomly select 500 mentions and compute the accuracy to obtain the real frequency. For example, statistics report that cemetery is a type with high frequency. However, a plenty number of the mentions labeled as cemetery are actually GPE. Similarly, engineer is also affected by noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Interface</head><p>The interface in shown in <ref type="figure" target="#fig_4">Figure 4</ref>, where annotators could expediently select entity spans and annotate the corresponding coarse and fine types. And annotators could check the current annotation information on the interface. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementation Details</head><p>All the four models use BERT base <ref type="bibr" target="#b6">(Devlin et al., 2019a)</ref> and the backbone encoder and initialized with the corresponding pre-trained uncased weights 6 . The hidden size is 768, and the number of layers and heads are 12. Models are implemented by Pytorch framework 7 <ref type="bibr" target="#b25">(Paszke et al., 2019)</ref> and Huggingface transformers 8 <ref type="bibr" target="#b34">(Wolf et al., 2020)</ref>. BERT models are optimized by AdamW 9 <ref type="bibr" target="#b23">(Loshchilov and Hutter, 2019)</ref> with the learning rate of 1e-4. We evaluate our implementations of NNShot and StructShot on the datasets used in the original paper, producing similar results. For supervised NER, the batch size is 8, and we train BERT-Tagger for 70000 steps and evaluate it on the test set. For 5 way 1?2 and 5?10 shot settings, the batch sizes are 16 and 4, and for 10 way 1?2 and 5?10 shot settings, the batch sizes are 8 and 1. We train 12000 episodes and use 500 episodes of the dev set to select the best model, and test it on 5000 episodes of the test set. Most hyper-parameters are from original settings. We manually tune the hyper-parameter ? in Viterbi for StructShot, and the value for 1?2 settings shot is 0.320, for 5?10 shot settings is 0.434. All the experiments are conducted with CUDA on NVIDIA Tesla V100 GPUs. With 2 GPUs used, the average time to train 10000 episodes is 135 minutes. The number of parameters of the models is 120M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Entity Types</head><p>As introduced in Section 4.1 in main text, FEW-NERD is manually annotated with 8 coarsegrained and 66 fine-grained entity types, and we list all the types in <ref type="table">Table 8</ref>. The schema is designed under practical situation, we hope the schema could help to better understand FEW-NERD. Note that ORG is the abbreviation of Organization, and MISC is the abbreviation of Miscellaneous. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPE</head><p>The company moved to a new office in Las Vegas, Nevada.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Body of Water</head><p>The Finke River normally drains into the Simpson Desert to the north west of the Macumba.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Island</head><p>An invading army of Teutonic Knights conquered Gotland in 1398.</p><p>Mountain C.G.E. Mannerheim met Thubten Gyatso in Wutai Shan during the course of his expedition from Turkestan to Peking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Park</head><p>Victoria Park contains examples of work by several architects including Alfred Waterhouse (Xaverian College).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Road/Transit</head><p>The thirty-first race of the 1951 season was held on October 7 at the one-mile dirt Occoneechee Speedway.</p><p>Other Herodotus (7.59) reports that Doriscus was the first place Xerxes the Great stopped to review his troops.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Person Actor</head><p>The first performance of any work of Gustav Holst given in that capital.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Artist/Author</head><p>A film adaption was made by Arne Bornebusch in 1936.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Athlete</head><p>Smith was named co-Player of the Week in the Big Ten on offense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Director</head><p>Margin for Error is a 1943 American drama film directed by Otto Preminger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Politician</head><p>Then-President Gloria Macapagal Arroyo led the inauguration rites of the facility on August 19, 2002.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scholar</head><p>Jeffery Westbrook and Robert Tarjan (1992) developed an efficient data structure for this problem based on disjoint-set data structures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Soldier</head><p>Sadowski was promoted to general, and took command of the freshly created Fortified Area of Silesia.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other</head><p>In Albany, Doane planned a cathedral like those in England.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ORG Company</head><p>A Vocaloid voicebank developed and distributed by Yamaha Corporation for Vocaloid 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Education</head><p>Long volunteer coached the offensive line for Briarcrest Christian School for 9 seasons.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Government</head><p>It was constructed using the savings of the Quezon provincial government.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Media</head><p>He was the Editor in Chief of Grenada's national newspaper "The Free West Indian".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Political/party</head><p>Stanley Norman Evans was a British industrialist and Labour Party politician.</p><p>Religion D'Souza was born on 10 November 1985 into a Goan Catholic family in Goa, India.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sports League</head><p>His strong performances convinced him that he was ready for the NBA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sports Team</head><p>The Pirates won the game and the World Series with Oldham on the mound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Show ORG</head><p>Standing in the Way of Control is the third studio album by American indie rock band Gossip.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other</head><p>He is the Creative Director of the Oliver Sacks Foundation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Building Airport</head><p>The city is served by the Sir Seretse Khama International Airport.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hospital</head><p>Then he did residency in ophthalmology at Farabi Eye Hospital from 1979 to 1982.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hotel</head><p>Nick also played at the regular Sunday evening sessions that were held at the Ramada Inn in Schenectady.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Library</head><p>RMIT University Library consists of six academic branch libraries in Australia and Vietnam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Restaurant</head><p>The first Panda Express restaurant opened in Galleria II in the same year, on level 3 near Bloomingdale's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sports Facility</head><p>This was the last year that the Razorbacks would play in Barnhill Arena.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theater</head><p>From 1954, she became a guest singer at the Vienna State Opera.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other</head><p>Eissler designated Masson to succeed him as Director of the Sigmund Freud Archives after his and Anna Freud's death.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Art</head><p>Music "Get Right" is a song recorded by American singer Jennifer Lopez for her fourth studio album.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Film</head><p>Margin for Error is a 1943 American drama film directed by Otto Preminger.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Written Art</head><p>The Count is a text adventure written by Scott Adams and published by Adventure International in 1979.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broadcast</head><p>In the fall of 1957, Mitchell starred in ABC's "The Guy Mitchell Show".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Painting</head><p>His painting 'Rooftops' has been in the collection of the City of London Corporation since 1989.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other</head><p>Kirwan appeared on stage at the Chichester Festival Theatre in a Jeremy Herrin production of Uncle Vanya.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Product Airplane</head><p>The Royal Norwegian Air Force's 330 Squadron operates a Westland Sea King search and rescue helicopter out of Flor?.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Car</head><p>The BYD Tang plug-in hybrid SUV was the top selling plug-in car with 31,405 units delivered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Food</head><p>The words "Time to make the donuts" are printed on the side of Dunkin' Donuts boxes in memory of Michael Vale/Fred the Baker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Game</head><p>Team Andromeda wanted to create a fully 3D arcade game, having worked on similar games such as "Out Run" which were not truly 3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ship</head><p>As night fell, Marine Corps General Holland Smith studied reports aboard the command ship "Eldorado".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Software</head><p>It allows communication between the Wolfram Mathematica kernel and front-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Train</head><p>On 9 June 1929, railcar No. 220 "Waterwitch" overran signals at Marshgate Junction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Weapon</head><p>Mannerheim gave Tibet's spiritual pontiff a Browning revolver and showed him how to reload the weapon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other</head><p>Rhinestone is as artificial and synthetic a concoction as has ever made its way to the screen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Event Attack</head><p>It was on this route that Tecumseh was killed at the Battle of the Thames on October 5, 1813.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Election</head><p>At the 1935 United Kingdom general election, McGleenan stood in Armagh as an Independent Republican.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Natural Disaster</head><p>He was originally from Chicago, but moved to Japan after the Second Great Kanto earthquake that all but decimated Japan's infrastructure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Protest</head><p>In 1832, following the failed Polish November Uprising, the Dominican monastery was sequestrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sports Event</head><p>Carle received a new defense partner when the Flyers traded for Chris Pronger at the 2009 NHL Entry Draft.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Other</head><p>One of TMG's first performances was in September 1972 at the Waitara Festival.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MISC Astronomy</head><p>He discovered a number of double stars and took many photographs of Mars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Award</head><p>He was awarded the Bialik Prize eight years later for these efforts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Biology</head><p>Estradiol valerate is rapidly hydrolyzed into estradiol in the intestines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chemistry</head><p>It was the first gas manufacturer in Kuwait to provide industrial gases such as oxygen and nitrogen to the local petroleum industry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Currency</head><p>Total investment has been 19 billion Norwegian krone.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disease</head><p>The 2020 competition was cancelled as part of the effort to minimize the COVID-19 pandemic.</p><p>Educational Degree Sigurlaug enrolled into the medical department of the University of Iceland and graduated as a Medical Doctor in 2010.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>God</head><p>Originally a farmer, Viking Ragnar Lothbrok claims to be descended from the god Odin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Language</head><p>The play was translated into English by Michael Hofmann and published in 1987 by Hamish Hamilton.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Law</head><p>Four of his five policy recommendations were incorporated into the U.S. Federal Financial Law of 1966.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Living Thing</head><p>Schistura horai is a species of ray-finned fish in the stone loach genus "Schistura".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Medical</head><p>Precious Blood Hospital offers specialist outpatient and inpatient services in General medicine. <ref type="table">Table 8</ref>: All the coarse-grained and fine-grained entity types in FEW-NERD, we only highlight the entities with the corresponding entity types in "Example".</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>) * equal contributions ? corresponding authors 1 The baselines are available at https://github. com/thunlp/Few-NERD</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>An overview of FEW-NERD. The inner circle represents the coarse-grained entity types and the outer circle represents the fine-grained entity types, some types are denoted by abbreviations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>A heat map to illustrate knowledge correlations among type in FEW-NERD, each small colored square represents the similarity of two entity types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>F1-scores of different entity types on FEW-NERD (SUP), we report the average performance of each coarse-grained entity type on the legends.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Screeshot of the interface used to annotate FEW-NERD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>An annotated case of FEW-NERD</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Statistics of FEW-NERD and multiple widely used NER datasets. For CoNLL'03, WikiGold, and I2B2, we report the statistics in the original paper. For OntoNotes 5.0 (LDC2013T19), we download and count all the data (English) annotated by the NER labels, some works use different split of OntoNotes 5.0 and may report different statistics. For WNUT'17, we download and count all the data. , all the fine-grained entity types belonging to Event, Building to E dev , and all the finegrained entity types belonging to ORG, LOC to E test , respectively. Based onFigure 2, in this setting, the training set, dev set and test set share little knowledge, making it a difficult benchmark.</figDesc><table><row><cell>Split</cell><cell>#Train</cell><cell>#Dev</cell><cell>#Test</cell></row><row><cell>FEW-NERD (SUP)</cell><cell cols="3">131,767 18,824 37,648</cell></row><row><cell cols="4">FEW-NERD (INTRA) 99,519 19,358 44,059</cell></row><row><cell cols="4">FEW-NERD (INTER) 130,112 18,817 14,007</cell></row></table><note>6.2 Few-shot NER The core intuition of few-shot learning is to learn new classes from few examples. Hence, we first split the overall entity set (denoted as E) into three mutually disjoint subsets, respectively denoted as E train , E dev , E test , and E train E dev E test = E, E train E dev E test = ?. Note that all the entity types are fine-grained types. Under this circum- stance, instances in train, dev and test datasets only consist of instances with entities in E train , E dev , E test respectively. However, NER is a sequence labeling problem, and it is possible that a sentence contains several different entities. To avoid the observation of new entity types in the training phase, we replace the labels of entities that belong to E test with O in the training set. Similarly, in the test set, entities that belongs to E train and E dev are also replaced by O. Based on this setting, we develop two few-shot NER tasks adopting different splitting strategies. FEW-NERD (INTRA) Firstly, we construct E train , E dev and E test according to the coarse-grained types. In other words, all the entities in differ- ent sets belong to different coarse-grained types. In the basis of the principle that we should re- place as few as possible entities with O, we assign all the fine-grained entity types belong- ing to People, MISC, Art, Product to EtrainFEW-NERD (INTER) In this task, although all the fine-grained entity types are mutually disjoint in E train , E dev , the coarse-grained types are shared. Specifically, we roughly assign 60% fine-grained types of all the 8 coarse-grained types to E train , 20% to E dev and 20% E test , respectively. The intuition of</note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Statistics of train, dev and test sets for three tasks of FEW-NERD. We remove the sentences with no entities for the few-shot benchmarks.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>Results of BERT-Tagger on previous NER datasets and the supervised setting of FEW-NERD.</figDesc><table /><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>97?0.61 29.66?1.39 20.76?0.84 36.34?1.33 51.32?0.45 42.54?0.94 11.33?0.57 22.47?0.49 15.05?0.44 29.39?0.27 44.51?1.00 35.40?0.13 NNShot 24.15?0.35 27.65?1.63 25.78?0.91 32.91?0.62 40.19?1.22 36.18?0.79 16.25?0.22 20.90?1.38 18.27?0.41 24.86?0.30 30.49?0.96 27.38?0.53 Struct 32.99?0.76 27.85?0.98 30.21?0.90 46.78?1.00 32.06?2.17 38.00?1.29 26.05?0.53 17.65?1.34 21.03?1.13 40.88?0.83 19.52?0.49 26.42?0.60</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">FEW-NERD(INTRA)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell>5 way 1?2 shot</cell><cell></cell><cell></cell><cell>5 way 5?10 shot</cell><cell></cell><cell cols="2">10 way 1?2 shot</cell><cell></cell><cell></cell><cell>10 way 5?10 shot</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Proto</cell><cell>15.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 5 :</head><label>5</label><figDesc>Performance of state-of-art models on FEW-NERD (INTRA). 04?1.75 49.30?0.68 38.83?1.49 52.54?1.32 66.76?1.01 58.79?0.44 26.02?1.32 43.17?0.92 32.45?0.79 46.38?0.42 61.60?0.36 52.92?0.37 NNShot 42.57?1.27 53.09?0.54 47.24?1.00 51.03?0.63 61.15?0.63 55.64?0.63 34.36?0.24 44.76?0.33 38.87?0.21 44.96?2.69 55.25?2.77 49.57?2.73 Struct 53.89?0.78 50.02?0.62 51.88?0.69 62.12?0.41 53.21?0.91 57.32?0.63 47.07?0.15 40.16?0.12 43.34?0.10 57.61?1.87 43.54?3.70 49.57?3.08</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">FEW-NERD(INTER)</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell></cell><cell>5 way 1?2 shot</cell><cell></cell><cell></cell><cell>5 way 5?10 shot</cell><cell></cell><cell cols="2">10 way 1?2 shot</cell><cell></cell><cell></cell><cell>10 way 5?10 shot</cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>Proto</cell><cell>32.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 6 :</head><label>6</label><figDesc>Performance of state-of-art models on FEW-NERD (INTER).</figDesc><table /><note></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://dumps.wikimedia.org/enwiki/ 3 https://github.com/attardi/ wikiextractor 4 https://www.nltk.org 5 https://lucene.apache.org/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>In this paper, we present a human-annotated dataset, FEW-NERD, for few-shot learning in NER. We describe the details of the collection process and conditions, the compensation of annotators, the measurements to ensure the quality in the main text. The corpus of the dataset is publicly obtained from Wikipedia and we have not modified or interfered with the content. FEW-NERD is likely to directly facilitate the research of few-shot NER, and further increase the progress of the construction of large-scale knowledge graphs (KGs). Models and systems built on FEW-NERD may contribute to construct KGs in various domains, including biomedical, financial, and legal fields, and further promote the development of NLP applications on specific domains. FEW-NERD is annotated in English, thus the dataset may mainly facilitate NLP research in English. For the sake of energy saving, we will not only open source the dataset and the code, but also release the checkpoints of our models from the experiments to reduce unnecessary carbon emission.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Association for Computational Linguistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dominic</forename><surname>Balasuriya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicky</forename><surname>Ringland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Nothman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2009 Workshop on The People&apos;s Web Meets NLP: Collaboratively Constructed Semantic Resources (People&apos;s Web)</title>
		<meeting>the 2009 Workshop on The People&apos;s Web Meets NLP: Collaboratively Constructed Semantic Resources (People&apos;s Web)<address><addrLine>Suntec, Singapore</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="10" to="18" />
		</imprint>
	</monogr>
	<note>Named entity recognition in Wikipedia</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">C</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nichols</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00104</idno>
		<title level="m">Named entity recognition with bidirectional LSTM-CNNs. Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="357" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ultra-fine entity typing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eunsol</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1009</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="87" to="96" />
		</imprint>
	</monogr>
	<note>Long Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A coefficient of agreement for nominal scales. Educational and psychological measurement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">https:/journals.sagepub.com/doi/abs/10.1177/001316446002000104?journalCode=epma</idno>
		<imprint>
			<date type="published" when="1960" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="37" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Kbqa: learning question answering over qa corpora and knowledge bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanyun</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Seung-Won</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of 43rd Very Large Data Base Conference Endowment</title>
		<meeting>43rd Very Large Data Base Conference Endowment</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Results of the WNUT2017 shared task on novel and emerging entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Derczynski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marieke</forename><surname>Van Erp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nut</forename><surname>Limsopatham</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-4418</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Noisy User-generated Text</title>
		<meeting>the 3rd Workshop on Noisy User-generated Text<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="140" to="147" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Long and Short Papers</publisher>
			<date type="published" when="2019" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Event detection with triggeraware lattice neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zibo</forename><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1033</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="347" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Prototypical representation learning for relation extraction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaobin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yao</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guangwei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Model-agnostic meta-learning for fast adaptation of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08-11" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Few-shot classification in named entity recognition task</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Fritzler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Varvara</forename><surname>Logacheva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maksim</forename><surname>Kretov</surname></persName>
		</author>
		<idno type="DOI">https:/dl.acm.org/doi/10.1145/3297280.3297378</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th ACM/SIGAPP Symposium on Applied Computing</title>
		<meeting>the 34th ACM/SIGAPP Symposium on Applied Computing</meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="993" to="1000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Contextdependent fine-grained entity type tagging</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Gillick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nevena</forename><surname>Lazic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuzman</forename><surname>Ganchev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jesse</forename><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Huynh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1820</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">FewRel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1514</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="4803" to="4809" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Few-shot learning for named entity recognition in medical text</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maximilian</forename><surname>Hofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrey</forename><surname>Kormilitzin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejo</forename><surname>Nevado-Holgado</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05468</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Few-shot slot tagging with collapsed dependency transfer and label-enhanced task-adaptive projection network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutai</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongkui</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhihan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yijia</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.128</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1381" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishan</forename><surname>Subudhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Damien</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shobana</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weizhu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baolin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14978</idno>
		<title level="m">Fewshot named entity recognition: A comprehensive study</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1030</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large-scale few-shot learning: Knowledge transfer with class hierarchy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tiange</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2019.00738</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-16" />
			<biblScope unit="page" from="7212" to="7220" />
		</imprint>
		<respStmt>
			<orgName>Computer Vision Foundation / IEEE</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Few-shot named entity recognition via metalearning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Billy</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shanshan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2020.3038670</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A unified MRC framework for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingrong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxian</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qinghong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="5849" to="5859" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Chinese relation extraction with multi-grained information and external linguistic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1430</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019" />
			<biblScope unit="page" from="4377" to="4386" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fine-grained entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Sixth AAAI Conference on Artificial Intelligence<address><addrLine>Toronto, Ontario, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2012-07-22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end sequence labeling via bi-directional LSTM-CNNs-CRF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuezhe</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P16-1101</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1064" to="1074" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>K?pf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>NeurIPS; Vancouver, BC, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-08" />
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">NNE: A dataset for nested named entity recognition in English newswire</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicky</forename><surname>Ringland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Hachey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sarvnaz</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cecile</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">R</forename><surname>Curran</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1510</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019" />
			<biblScope unit="page" from="5176" to="5181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Named entity recognition in tweets: An experimental study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><surname>Ritter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mausam</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Edinburgh, Scotland, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1524" to="1534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Modeling relation paths for knowledge graph completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ning</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai-Tao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-12-04" />
			<biblScope unit="page" from="4077" to="4087" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Annotating longitudinal clinical narratives for de-identification: The 2014 i2b2/uthealth corpus</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amber</forename><surname>Stubbs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Uzuner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="20" to="29" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Introduction to the CoNLL-2002 shared task: Language-independent named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><forename type="middle">F</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tjong Kim</forename><surname>Sang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING-02: The 6th Conference on Natural Language Learning</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">MAVEN: A Massive General Domain Event Detection Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wangyi</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juanzi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.129</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="1652" to="1671" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mitchell</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lance</forename><surname>Ramshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ann</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michelle</forename><surname>Franchini</surname></persName>
		</author>
		<title level="m">Ontonotes release 5.0 ldc2013t19. Linguistic Data Consortium, Philadelphia</title>
		<meeting><address><addrLine>PA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Teven</forename><forename type="middle">Le</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Scao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drame</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Quentin Lhoest, and Alexander Rush</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020" />
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simple and effective few-shot named entity recognition with structured nearest neighbor learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arzoo</forename><surname>Katiyar</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.516</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020" />
			<biblScope unit="page" from="6365" to="6375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">HotpotQA: A dataset for diverse, explainable multi-hop question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1259</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018" />
			<biblScope unit="page" from="2369" to="2380" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
